[
  {
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/README.md b/README.md\nindex 48b4b0fb3..724dc3569 100644\n--- a/README.md\n+++ b/README.md\n@@ -367,6 +367,7 @@ The following rules are enabled by default on specific platforms only:\n * `brew_update_formula` &ndash; turns `brew update <formula>` into `brew upgrade <formula>`;\n * `dnf_no_such_command` &ndash; fixes mistyped DNF commands;\n * `nixos_cmd_not_found` &ndash; installs apps on NixOS;\n+* `nix_shell` &ndash; re-runs your command in a `nix-shell`;\n * `pacman` &ndash; installs app with `pacman` if it is not installed (uses `yay`, `pikaur` or `yaourt` if available);\n * `pacman_invalid_option` &ndash; replaces lowercase `pacman` options with uppercase.\n * `pacman_not_found` &ndash; fixes package name with `pacman`, `yay`, `pikaur` or `yaourt`.\ndiff --git a/tests/rules/test_nix_shell.py b/tests/rules/test_nix_shell.py\nnew file mode 100644\nindex 000000000..69ba14872\n--- /dev/null\n+++ b/tests/rules/test_nix_shell.py\n@@ -0,0 +1,90 @@\n+import pytest\n+from thefuck.rules.nix_shell import get_nixpkgs_names, match, get_new_command\n+from thefuck.types import Command\n+from unittest.mock import patch, MagicMock\n+\n+\n+@pytest.mark.parametrize(\n+    \"script,output,nixpkgs_names\",\n+    [\n+        # output can be retrived by running `THEFUCK_DEBUG=true thefuck lsof`\n+        (\n+            \"lsof\",\n+            \"/nix/store/p6dlr3skfhxpyphipg2bqnj52999banh-bash-5.2-p15/bin/sh: line 1: lsof: command not found\",\n+            [\"lsof\"],\n+        ),\n+    ],\n+)\n+def test_match(script, output, nixpkgs_names):\n+    with patch(\"thefuck.rules.nix_shell.get_nixpkgs_names\") as mocked_get_nixpkgs_names:\n+        mocked_get_nixpkgs_names.return_value = nixpkgs_names\n+        command = Command(script, output)\n+        assert match(command)\n+\n+\n+@pytest.mark.parametrize(\n+    \"script,output,nixpkgs_names\",\n+    [\n+        # output can be retrived by running `THEFUCK_DEBUG=true thefuck foo`\n+        (\n+            \"foo\",\n+            \"/nix/store/p6dlr3skfhxpyphipg2bqnj52999banh-bash-5.2-p15/bin/sh: line 1: foo: command not found\",\n+            [],\n+        ),\n+    ],\n+)\n+def test_not_match(script, output, nixpkgs_names):\n+    with patch(\"thefuck.rules.nix_shell.get_nixpkgs_names\") as mocked_get_nixpkgs_names:\n+        mocked_get_nixpkgs_names.return_value = nixpkgs_names\n+        command = Command(script, output)\n+        assert not match(command)\n+\n+\n+@pytest.mark.parametrize(\n+    \"script,nixpkgs_names,new_command\",\n+    [\n+        (\n+            \"lsof -i :3000\",\n+            [\"busybox\", \"lsof\"],\n+            [\n+                'nix-shell -p busybox --run \"lsof -i :3000\"',\n+                'nix-shell -p lsof --run \"lsof -i :3000\"',\n+            ],\n+        ),\n+        (\"xev\", [\"xorg.xev\"], ['nix-shell -p xorg.xev --run \"xev\"']),\n+    ],\n+)\n+def test_get_new_command(script, nixpkgs_names, new_command):\n+    \"\"\"Check that flags and params are preserved in the new command\"\"\"\n+\n+    command = Command(script, \"\")\n+    with patch(\"thefuck.rules.nix_shell.get_nixpkgs_names\") as mocked_get_nixpkgs_names:\n+        mocked_get_nixpkgs_names.return_value = nixpkgs_names\n+        assert get_new_command(command) == new_command\n+\n+\n+# Mocks the stderr of `command-not-found QUERY`. Mock values are retrieved by\n+# running `THEFUCK_DEBUG=true thefuck command-not-found lsof`.\n+mocked_cnf_stderr = {\n+    \"lsof\": \"The program 'lsof' is not in your PATH. It is provided by several packages.\\nYou can make it available in an ephemeral shell by typing one of the following:\\n  nix-shell -p busybox\\n  nix-shell -p lsof\",\n+    \"xev\": \"The program 'xev' is not in your PATH. You can make it available in an ephemeral shell by typing:\\n  nix-shell -p xorg.xev\",\n+    \"foo\": \"foo: command not found\",\n+}\n+\n+\n+@pytest.mark.parametrize(\n+    \"bin,expected_nixpkgs_names,cnf_stderr\",\n+    [\n+        (\"lsof\", [\"busybox\", \"lsof\"], mocked_cnf_stderr[\"lsof\"]),\n+        (\"xev\", [\"xorg.xev\"], mocked_cnf_stderr[\"xev\"]),\n+        (\"foo\", [], mocked_cnf_stderr[\"foo\"]),\n+    ],\n+)\n+def test_get_nixpkgs_names(bin, expected_nixpkgs_names, cnf_stderr):\n+    \"\"\"Check that `get_nixpkgs_names` returns the correct names\"\"\"\n+\n+    with patch(\"subprocess.run\") as mocked_run:\n+        result = MagicMock()\n+        result.stderr = cnf_stderr\n+        mocked_run.return_value = result\n+        assert get_nixpkgs_names(bin) == expected_nixpkgs_names\ndiff --git a/thefuck/rules/nix_shell.py b/thefuck/rules/nix_shell.py\nnew file mode 100644\nindex 000000000..7bfba7eac\n--- /dev/null\n+++ b/thefuck/rules/nix_shell.py\n@@ -0,0 +1,51 @@\n+from thefuck.specific.nix import nix_available\n+import subprocess\n+\n+enabled_by_default = nix_available\n+\n+# Set the priority just ahead of `fix_file` rule, which can generate low quality matches due\n+# to the sheer amount of paths in the nix store.\n+priority = 999\n+\n+\n+def get_nixpkgs_names(bin):\n+    \"\"\"\n+    Returns the name of the Nix package that provides the given binary. It uses the\n+    `command-not-found` binary to do so, which is how nix-shell generates it's own suggestions.\n+    \"\"\"\n+\n+    result = subprocess.run(\n+        [\"command-not-found\", bin], stderr=subprocess.PIPE, universal_newlines=True\n+    )\n+\n+    # The suggestion, if any, will be found in stderr. Upstream definition: https://github.com/NixOS/nixpkgs/blob/b6fbd87328f8eabd82d65cc8f75dfb74341b0ace/nixos/modules/programs/command-not-found/command-not-found.nix#L48-L90\n+    text = result.stderr\n+\n+    # return early if binary is not available through nix\n+    if \"nix-shell\" not in text:\n+        return []\n+\n+    nixpkgs_names = [\n+        line.split()[-1] for line in text.splitlines() if \"nix-shell -p\" in line\n+    ]\n+    return nixpkgs_names\n+\n+\n+def match(command):\n+    bin = command.script_parts[0]\n+    return (\n+        \"command not found\" in command.output  # only match commands which had exit code: 127                   # noqa: E501\n+        and get_nixpkgs_names(bin)             # only match commands which could be made available through nix  # noqa: E501\n+    )\n+\n+\n+def get_new_command(command):\n+    bin = command.script_parts[0]\n+    nixpkgs_names = get_nixpkgs_names(bin)\n+\n+    # Construct a command for each package name\n+    commands = [\n+        'nix-shell -p {} --run \"{}\"'.format(name, command.script)\n+        for name in nixpkgs_names\n+    ]\n+    return commands\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "looks like you [can use this already](https://github.com/NixOS/nixpkgs/compare/master...KiaraGrouwstra:nixpkgs:thefuck-nix-shell) using e.g. an overlay, altho i had a bit of trouble getting it to work out of the box.\r\nspecifically, without adding `doCheck = false;`, i would run into this error:\r\n\r\n```\r\nerror: builder for '/nix/store/rl44gb6qd4x2myclj9i8cpkfrvw6ysqa-thefuck-3.32.drv' failed with exit code 2;\r\n       last 10 log lines:\r\n       > thefuck/system/unix.py:6\r\n       >   /build/source/thefuck/system/unix.py:6: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\r\n       >     from distutils.spawn import find_executable\r\n       >\r\n       > -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\r\n       > =========================== short test summary info ============================\r\n       > ERROR  - ModuleNotFoundError: No module named 'pytest_docker_pexpect'\r\n       > !!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\r\n       > ========================= 1 warning, 1 error in 0.09s ==========================\r\n       > /nix/store/bknngadwym46j65qs14ic2w79rpav888-stdenv-linux/setup: line 1582: pop_var_context: head of shell_variables not a function context\r\n```\r\n\r\ni had tried removing the added test, altho that appeared not to resolve the issue.\r\n;\nit would seem cool to similarly get an approach using `nix run`, i.e. go from suggesting `nix-shell -p ponysay --run \"ponysay moo\"` to `nix run nixpkgs#ponysay -- moo` - this might eventually help extend beyond just `nixpkgs`.\r\n\r\nedit: https://github.com/KiaraGrouwstra/thefuck/commit/81d6786c80b86f2cc80b3ea90adc214df8266643\r\n;\nI've been using a custom rule that supports the new [unified CLI](https://zero-to-nix.com/concepts/nix#unified-cli) for a while, and was planning on opening a PR once this one has been merged (I hesitate to update this current PR as it's already tested and ready to be merged). I don't know if that will happen soon, so in the meantime I've pushed the changes to [this](https://github.com/thenbe/thefuck/tree/nix-shell-new) new branch instead, which [builds](https://github.com/thenbe/thefuck/compare/nix-shell...thenbe:thefuck:nix-shell-new) on this here PR. You can use the updated rule by adding it as a [custom rule](https://github.com/nvbn/thefuck?tab=readme-ov-file#creating-your-own-rules) to your config.\r\n\r\nIn the new rule, three variants are suggested. Assuming I run `cowsay hello world`, I am presented with the following:\r\n\r\n1. `nix run nixpkgs#cowsay -- hello world`: This runs my command in a non-interactive shell. Uses the nix unified CLI.\r\n1. `nix shell nixpkgs#cowsay`: This enters an interactive shell with `cowsay` available, but does not run any command. This is useful if you'd rather run the command yourself after entering the shell because your command requires delicate massaging (e.g. running it with `sudo`, prefixing it with environment variable, juggling quote variants, etc).\r\n1. `nix-shell -p cowsay --run \"cowsay hello world\"`. This runs my command in a non-interactive shell. Uses the nix original CLI.\r\n1. `nix shell nixpkgs#cowsay --command cowsay hello world`: Very similar to the first one so I've personally disabled this one.\r\n\r\n### Thoughts on future updates:\r\n\r\n\r\n\r\n- It'd be nice if there was a variant that runs my command and then _keeps me_ in the shell.\r\n  - For the original CLI, we [can](https://nix.dev/manual/nix/2.19/command-ref/nix-shell#options) add a `--command \"echo hello; return\"` to our `nix-shell` invocation.\r\n  - For the unified CLI: not sure yet, we might need to do something like this example from the [docs](https://nix.dev/manual/nix/2.19/command-ref/new-cli/nix3-shell): ` nix shell nixpkgs#gnumake --command sh -c \"cd src && make\"`\r\n- We should expose a couple of flags for users to configure this.\r\n  - `disable_unified_cli` (boolean)\r\n  - `disable_original_cli` (boolean)\r\n- As far as I can tell, the `command-not-found` db doesn't really play nice if you use flakes to configure your system and might return stale results (unless you update it manually?). [`nix-index`](https://github.com/nix-community/nix-index) seems to be the go-to alternative. It'd be great if we could optionally use that instead (perhaps behind a flag `enable_nix_index` for users who have installed `nix-index` (`programs.nix-index.enable = true;` in home manager).;\n@thenbe i agree integrating with `nix-index`'s `command-not-found` replacement seems cool, as a flake user.\r\ni kinda wish we could have `command-not-found` (and this `thefuck` integration) [extend to flake inputs beyond nixpkgs](https://github.com/nix-community/nix-index/issues/244) as well, such as to packages from NUR for example. preferably this should be dynamic based on your inputs rather than hardcoded to specific ones like nixpkgs, or NUR for that matter.\r\ni'll admit i haven't really figured out how that might work tho.;\njust tried these with a command like `program_i_have | program_i_dont_have`, seems that may complicate the suggestions a bit;\n@thenbe hm, i'm not sure.\r\n\r\n```\r\nfortune | cowsay\r\nThe program 'cowsay' is not in your PATH. It is provided by several packages.\r\nYou can make it available in an ephemeral shell by typing one of the following:\r\n  nix-shell -p cowsay\r\n  nix-shell -p neo-cowsay\r\n$ fuck\r\nnix run nixpkgs#fortune | cowsay\r\n```\r\n\r\nfeels like it knows about the whole command given it's reproducing it?\r\n;\nanother common nix thing we might be able to address from `thefuck` would be errors about packages being unfree\r\n\r\nedit: https://github.com/KiaraGrouwstra/thefuck/commit/16d838bf6f63117b161a2f1e6572e06108b007eb\r\n;\nIf I'm only looking to execute a program (and don't need to be dropped into a shell) then I prefer `nix run` over `nix shell` as the [documentation](https://nix.dev/manual/nix/2.19/command-ref/new-cli/nix3-run) suggests `nix run` specifically for this use case.\r\n\r\nI also recall `nix run` being more performant (perhaps because we forego the overhead of launching a shell?). This last point is not derived from benchmarks, only anecdotal evidence.\r\n\r\n> i guess the latter seems a bit more generic in case of handling non-standard binaries at least\r\n\r\nI've added this variant (the 4th one in my [previous post](https://github.com/nvbn/thefuck/pull/1393#issuecomment-1961487094)), but disabled it after a while when I realized that I never reach for it. Do you find that you still need it over `nix run` (the 1st variant in my previous post)?;\n> another common nix thing we might be able to address from `thefuck` would be errors about packages being unfree\r\n> \r\n> edit: [KiaraGrouwstra@16d838b](https://github.com/KiaraGrouwstra/thefuck/commit/16d838bf6f63117b161a2f1e6572e06108b007eb)\r\n\r\nThis would be useful. Does it still complain about the `--impure` flag? Or do you use a workaround for that?;\nI just have it aliased to `f` for extra convenience.\r\n\r\nI opted not to package it for nix separately since `fuck` already exposes a method for easily adding custom rules. Instead, I placed the rule in `~/mydotfiles/thefuck/rules/nix-shell.py` then told home-manager to symlink it to the appropriate place in `.config`:\r\n\r\n```nix\r\n# home.nix\r\nhome.file.\".config/thefuck/rules/nix-shell.py\".source = config.lib.file.mkOutOfStoreSymlink \"${config.home.homeDirectory}/mydotfiles/thefuck/rules/nix-shell.py\";\r\n```\r\n\r\nThis way I don't need to rebuild every time I tweak the rule.\r\n\r\n> what was the --impure error?\r\n\r\nThe unified CLI commands (`nix shell`, `nix run`, etc) will not acknowledge environment variables unless the `--impure` flag is used.\r\n\r\n<details>\r\n  <summary> output </summary>\r\n\r\n```\r\n$ NIXPKGS_ALLOW_UNFREE=1 nix shell nixpkgs#github-copilot-cli\r\n\r\nerror:\r\n       â€¦ in the condition of the assert statement\r\n\r\n         at /nix/store/xwc3zfc544jg6zhr0wi6k8253s7mwlhi-source/lib/customisation.nix:267:17:\r\n\r\n          266|     in commonAttrs // {\r\n          267|       drvPath = assert condition; drv.drvPath;\r\n             |                 ^\r\n          268|       outPath = assert condition; drv.outPath;\r\n\r\n       â€¦ while evaluating the attribute 'handled'\r\n\r\n         at /nix/store/xwc3zfc544jg6zhr0wi6k8253s7mwlhi-source/pkgs/stdenv/generic/check-meta.nix:490:7:\r\n\r\n          489|       # or, alternatively, just output a warning message.\r\n          490|       handled =\r\n             |       ^\r\n          491|         (\r\n\r\n       (stack trace truncated; use '--show-trace' to show the full trace)\r\n\r\n       error: Package â€˜github-copilot-cli-0.1.36â€™ in /nix/store/xwc3zfc544jg6zhr0wi6k8253s7mwlhi-source/pkgs/tools/misc/github-copilot-cli/default.nix:21 has\r\n an unfree license (â€˜unfreeâ€™), refusing to evaluate.\r\n\r\n       a) To temporarily allow unfree packages, you can use an environment variable\r\n          for a single invocation of the nix tools.\r\n\r\n            $ export NIXPKGS_ALLOW_UNFREE=1\r\n\r\n          Note: When using `nix shell`, `nix build`, `nix develop`, etc with a flake,\r\n                then pass `--impure` in order to allow use of environment variables.\r\n\r\n       b) For `nixos-rebuild` you can set\r\n         { nixpkgs.config.allowUnfree = true; }\r\n       in configuration.nix to override this.\r\n\r\n       Alternatively you can configure a predicate to allow specific packages:\r\n         { nixpkgs.config.allowUnfreePredicate = pkg: builtins.elem (lib.getName pkg) [\r\n             \"github-copilot-cli-0.1.36\"\r\n           ];\r\n         }\r\n\r\n       c) For `nix-env`, `nix-build`, `nix-shell` or any other Nix command you can add\r\n         { allowUnfree = true; }\r\n       to ~/.config/nixpkgs/config.nix.\r\n\r\n\r\n```\r\n```bash\r\n# it wants this instead:\r\n$ NIXPKGS_ALLOW_UNFREE=1 nix shell nixpkgs#github-copilot-cli --impure\r\n```\r\n\r\n</details>\r\n"
      }
    ],
    "token_estimation": 4026.0
  },
  {
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/whisper/__init__.py b/whisper/__init__.py\nindex 2a1fb4ec6..4f45f9969 100644\n--- a/whisper/__init__.py\n+++ b/whisper/__init__.py\n@@ -92,7 +92,12 @@ def load_model(name: str, device: Optional[Union[str, torch.device]] = None, dow\n     \"\"\"\n \n     if device is None:\n-        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+        if torch.cuda.is_available():\n+            device = \"cuda\"\n+        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n+            device = \"mps\"\n+        else:\n+            device = \"cpu\"\n     if download_root is None:\n         download_root = os.getenv(\n             \"XDG_CACHE_HOME\", \ndiff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex d95d3336d..0d89a5d77 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -75,6 +75,8 @@ def transcribe(\n     if model.device == torch.device(\"cpu\"):\n         if torch.cuda.is_available():\n             warnings.warn(\"Performing inference on CPU when CUDA is available\")\n+        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n+            warnings.warn(\"Performing inference on CPU when MPS is available\")\n         if dtype == torch.float16:\n             warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n             dtype = torch.float32\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "I'm getting this error when try to use MPS\r\n\r\n/Users/diego/.pyenv/versions/3.10.6/lib/python3.10/site-packages/whisper-1.0-py3.10.egg/whisper/decoding.py:629: UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/diego/Projects/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/AppleInternal/Library/BuildRoots/2d9b4df9-4b93-11ed-b0fc-2e32217d8374/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:794: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 23200 bytes\r\n'\r\nAbort trap: 6\r\n/Users/diego/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n\r\nany clues?;\n@DiegoGiovany Not an expert on this but It looks like PyTorch itself is missing some operators for MPS. See for example\r\nhttps://github.com/pytorch/pytorch/issues/77764#issuecomment-1254352628\r\n(which refers to repeat_interleave)\r\n\r\nand\r\nhttps://github.com/pytorch/pytorch/issues/87219\r\n;\nThanks for your work. I just tried this. Unfortunately, it didn't work for me on my m1 max with 32GB.\r\nHere is what I did:\r\npip install git+https://github.com/openai/whisper.git@refs/pull/382/head\r\n\r\nNo errors on install and it works fine when run without mps: whisper audiofile_name --model medium \r\n\r\nWhen I run: whisper audiofile_name --model medium --device mps\r\n\r\nHere is the error I get:\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/810eba08-405a-11ed-86e9-6af958a02716/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x1024x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s). \r\n\r\nWhen I run:  whisper audiofile_name --model medium --device mps --fp16 False\r\n\r\nHere is the error I get:\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: English\r\n/anaconda3/lib/python3.9/site-packages/whisper/decoding.py:633: UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/AppleInternal/Library/BuildRoots/f0468ab4-4115-11ed-8edc-7ef33c48bc85/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:794: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 1007280 bytes\r\n\r\nBasically, same error as @DiegoGiovany.\r\n\r\nAny ideas on how to fix?;\n@dwarkeshsp \r\n\r\nnot workï¼Œwith mbp2015 pytorch 1.3 stableï¼Œegpu RX580, MacOS 12.3.\r\n\r\nchanged the code as the same as yours.\r\n\r\nchanged  to use --device mps but show error, maybe there is still somewhere to change or modify.\r\n\r\nuse --device cpu, it works.\r\n\r\nwith other pytorch-metal project, MPS works.;\nI also see the same errors as others mentioned above, on an M1 Mac running arm64 Python. ;\nOn an M1 16\" MBP with 16GB running MacOS 13.0.1, I'm seeing the following with `openai-whisper-20230117`:\r\n\r\nUsing this command:\r\n```(venv) whisper_ai_playground % whisper './test_file.mp3' --model tiny.en --output_dir ./output --device mps```\r\n\r\nI'm encountering the following errors:\r\n\r\n```loc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/810eba08-405a-11ed-86e9-6af958a02716/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x384x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible```\r\n\r\n```LLVM ERROR: Failed to infer result type(s).```\r\n\r\n```zsh: abort      whisper  --model tiny.en --output_dir ./output --device mps```\r\n\r\n```/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '```;\nSame problem with osx 13.2 in MacBook Pro M2 max:\r\n\r\n```\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x1280x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\nzsh: abort      whisper audio.wav --language en --model large\r\nm2@Render ~ % /opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```;\nI'm getting the same error as @renderpci using the M1 Base Model\r\n```bash\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x512x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\n[1]    3746 abort      python3 test.py\r\n```\r\n**test.py:**\r\n```py\r\nimport whisper\r\n\r\nmodel = whisper.load_model(\"base\")\r\nresult = model.transcribe(\"audio.mp3\")\r\nprint(result[\"text\"])\r\n```;\nFWIW I switched to the C++ port https://github.com/ggerganov/whisper.cpp/ and got a ~15x speedup compared to CPU pytorch on my M1 Pro. (But note that it doesn't have all the features/flags from the official whisper repo.);\n> FWIW I switched to the C++ port https://github.com/ggerganov/whisper.cpp/ \r\n\r\nFor us whisper.cpp is not an option:\r\n\r\n> **Should I use whisper.cpp in my project?**\r\n> \r\n> whisper.cpp is a hobby project. It does not strive to provide a production ready implementation. The main goals of the implementation is to be educational, minimalistic, portable, hackable and performant. There are no guarantees that the implementation is correct and bug-free and stuff can break at any point in the future. Support and updates will depend mostly on contributions, since with time I will move on and won't dedicate too much time on the project.\r\n> \r\n> If you plan to use whisper.cpp in your own project, keep in mind the above.\r\n> My advice is to not put all your eggs into the whisper.cpp basket.;\nThe same error as @renderpci using the M2\r\n\r\n\r\nwhisper interview.mp4 --language en --model large --device mps\r\n\r\n```\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x1280x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\nzsh: abort      whisper interview.mp4 --language en --model large --device mps\r\npac@dd ~ % /opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```;\nHey @devpacdd  - this should be fixed in latest pytorch nightly (pip3 install --pre --force-reinstall torch --index-url https://download.pytorch.org/whl/nightly/cpu). Let me know if you still see any issues. Thanks;\nStill have the same error after updating\r\n\r\nEdit: After adding `--fp16 False` to the command, I now get a new error, as well as the old one:\r\n```\r\n/opt/homebrew/lib/python3.10/site-packages/whisper/decoding.py:633: UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/AppleInternal/Library/BuildRoots/5b8a32f9-5db2-11ed-8aeb-7ef33c48bc85/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:794: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 1007280 bytes\r\n'\r\nzsh: abort      whisper --model large --language de --task transcribe  --device mps --fp16\r\n/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```;\ni was able to get it to kinda work: https://github.com/davabase/whisper_real_time/issues/5#issue-1596258783;\n> The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n>   audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n\r\n@manuthebyte could you please make sure you are on a recent nightly? `repeat_interleave` should be natively supported. If you could try grabbing today's nightly and give a try that would be awesome! (You can get today's nightly with `pip3 install --pre --force-reinstall torch==2.0.0.dev20230224 --index-url https://download.pytorch.org/whl/nightly/cpu`)\r\n\r\n;\nWow! \r\n\r\nwhen running:\r\n`Python3 transcribe_demo.py --model medium` (from https://github.com/davabase/whisper_real_time)\r\n\r\nwith the following packages in my pipenv's requirements.txt\r\n```\r\ncertifi==2022.12.7\r\ncharset-normalizer==3.0.1\r\nffmpeg-python==0.2.0\r\nfilelock==3.9.0\r\nfuture==0.18.3\r\nhuggingface-hub==0.12.1\r\nidna==3.4\r\nmore-itertools==9.0.0\r\nmpmath==1.2.1\r\nnetworkx==3.0rc1\r\nnumpy==1.24.2\r\nopenai-whisper @ git+https://github.com/openai/whisper.git@51c785f7c91b8c032a1fa79c0e8f862dea81b860\r\npackaging==23.0\r\nPillow==9.4.0\r\nPyAudio==0.2.13\r\nPyYAML==6.0\r\nregex==2022.10.31\r\nrequests==2.28.2\r\nSpeechRecognition==3.9.0\r\nsympy==1.11.1\r\ntokenizers==0.13.2\r\ntorch==2.0.0.dev20230224\r\ntorchaudio==0.13.1\r\ntorchvision==0.14.1\r\ntqdm==4.64.1\r\ntransformers==4.26.1\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.14\r\n```\r\n\r\nit gets every word! while i was singing! in realtime, with maybe 50%~ gpu usage on the apple M2 Pro Max.;\n> > The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n> > audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n> \r\n> @manuthebyte could you please make sure you are on a recent nightly? `repeat_interleave` should be natively supported. If you could try grabbing today's nightly and give a try that would be awesome! (You can get today's nightly with `pip3 install --pre --force-reinstall torch==2.0.0.dev20230224 --index-url https://download.pytorch.org/whl/nightly/cpu`)\r\n\r\nWith my pip3 freeze being:\r\n```\r\nbeautifulsoup4==4.11.2\r\ncertifi==2022.12.7\r\ncharset-normalizer==3.0.1\r\ncolorama==0.4.6\r\ndnspython==2.3.0\r\nffmpeg-python==0.2.0\r\nfilelock==3.9.0\r\nfuture==0.18.3\r\nhuggingface-hub==0.12.1\r\nidna==3.4\r\nmore-itertools==9.0.0\r\nmpmath==1.2.1\r\nnetworkx==3.0rc1\r\nnumpy==1.24.2\r\nopenai-whisper @ git+https://github.com/openai/whisper.git@7858aa9c08d98f75575035ecd6481f462d66ca27\r\npackaging==23.0\r\nprotobuf==4.21.12\r\nPyYAML==6.0\r\nregex==2022.10.31\r\nrequests==2.28.2\r\nsix==1.16.0\r\nsoupsieve==2.4\r\nsympy==1.11.1\r\ntokenizers==0.13.2\r\ntorch==2.0.0.dev20230224\r\ntqdm==4.64.1\r\ntransformers==4.26.1\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.14\r\n```\r\n\r\nIt now seems to use the GPU but I now get these errors:\r\n```\r\n/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py:636: UserWarning: 0MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py:443: UserWarning: 1MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:143.)\r\n  timestamp_logprob = logprobs[k, self.tokenizer.timestamp_begin :].logsumexp(dim=-1)\r\n/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py:444: UserWarning: 1MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1269.)\r\n  max_text_token_logprob = logprobs[k, : self.tokenizer.timestamp_begin].max()\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/bin/whisper\", line 8, in <module>\r\n    sys.exit(cli())\r\n             ^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py\", line 314, in cli\r\n    result = transcribe(model, audio_path, temperature=temperature, **args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py\", line 183, in transcribe\r\n    result: DecodingResult = decode_with_fallback(segment)\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py\", line 118, in decode_with_fallback\r\n    decode_result = model.decode(segment, options)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 707, in decode\r\n    result = DecodingTask(model, options).run(mel)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 640, in run\r\n    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\r\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 609, in _main_loop\r\n    tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 258, in update\r\n    next_tokens = Categorical(logits=logits / self.temperature).sample()\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/distributions/categorical.py\", line 66, in __init__\r\n    super().__init__(batch_shape, validate_args=validate_args)\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/distributions/distribution.py\", line 62, in __init__\r\n    raise ValueError(\r\nValueError: Expected parameter logits (Tensor of shape (5, 51865)) of distribution Categorical(logits: torch.Size([5, 51865])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\r\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan]], device='mps:0')\r\n```\r\n\r\nWhen running the command `whisper --model small --language en --task transcribe ***.wav --device mps`;\n> Hey @devpacdd - this should be fixed in latest pytorch nightly (pip3 install --pre --force-reinstall torch --index-url https://download.pytorch.org/whl/nightly/cpu). Let me know if you still see any issues. Thanks\r\n\r\nGeart! it works!\r\nBut.. In my test the GPU is slow than CPU... ??? \r\n\r\nAudio to transcribe: 1 minute with model large, language catalan\r\n\r\nCPU  : 2m : 33 s\r\nGPU (--device mps): 4m : 54 s\r\n\r\nI tried with different files and the result was the same; +/- double time with GPU enable.\r\n\r\nIt's normal? I expected less time for GPU than CPU.\r\n\r\nBest;\nI get this error while trying to use MPS\r\n\r\nHere is the command I am running: `whisper --model large --language en --task transcribe test.mp3 --device mps`\r\n\r\n```\r\n$ whisper --model large --language en --task transcribe test.mp3 --device mps\r\nTraceback (most recent call last):\r\n  File \"/Users/mukul/miniconda3/envs/ml/bin/whisper\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/whisper/transcribe.py\", line 433, in cli\r\n    model = load_model(model_name, device=device, download_root=model_dir)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/whisper/__init__.py\", line 159, in load_model\r\n    return model.to(device)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1170, in to\r\n    return self._apply(convert)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 869, in _apply\r\n    self._buffers[key] = fn(buf)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1168, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\nNotImplementedError: Could not run 'aten::empty.memory_format' with arguments from the 'SparseMPS' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, MPS, Meta, QuantizedCPU, QuantizedMeta, MkldnnCPU, SparseCPU, SparseMeta, SparseCsrCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\r\n\r\nCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31085 [kernel]\r\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMPS.cpp:24065 [kernel]\r\nMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26824 [kernel]\r\nQuantizedCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\r\nQuantizedMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterQuantizedMeta.cpp:105 [kernel]\r\nMkldnnCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMkldnnCPU.cpp:507 [kernel]\r\nSparseCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1379 [kernel]\r\nSparseMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseMeta.cpp:249 [kernel]\r\nSparseCsrCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1128 [kernel]\r\nBackendSelect: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:734 [kernel]\r\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\r\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\r\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\r\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\r\nConjugate: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\r\nNegative: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:23 [kernel]\r\nZeroTensor: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:90 [kernel]\r\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\r\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16872 [kernel]\r\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\r\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\r\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\r\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\r\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\r\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\r\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\r\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\r\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\r\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\r\n```;\n@mukulpatnaik \r\nMy device is M1 MacBook Pro, I got the same error with the latest version of whisper([v20230314](https://github.com/openai/whisper/releases/tag/v20230314)), then I switch to [v20230124](https://github.com/openai/whisper/releases/tag/v20230124), every thing works fine. (torch nightly version)\r\n\r\nBut, seems like mps is slower than cpu like @renderpci reported, for my task\r\n* cpu 3.26 s\r\n* mps 5.25 s\r\n* cpu+torch2 compile 3.31 s\r\n* mps+torch2 compile 4.94 s\r\n\r\nðŸ« "
      }
    ],
    "token_estimation": 6973.0
  },
  {
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/whisper/timing.py b/whisper/timing.py\nindex befcf464e..b695ead0a 100644\n--- a/whisper/timing.py\n+++ b/whisper/timing.py\n@@ -299,6 +299,7 @@ def add_word_timestamps(\n     word_durations = np.array([t.end - t.start for t in alignment])\n     word_durations = word_durations[word_durations.nonzero()]\n     median_duration = np.median(word_durations) if len(word_durations) > 0 else 0.0\n+    median_duration = min(0.7, float(median_duration))\n     max_duration = median_duration * 2\n \n     # hack: truncate long words at sentence boundaries.\ndiff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex e80bede1d..1c075a201 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -2,7 +2,7 @@\n import os\n import traceback\n import warnings\n-from typing import TYPE_CHECKING, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -23,6 +23,7 @@\n from .utils import (\n     exact_div,\n     format_timestamp,\n+    get_end,\n     get_writer,\n     make_safe,\n     optional_float,\n@@ -48,6 +49,8 @@ def transcribe(\n     word_timestamps: bool = False,\n     prepend_punctuations: str = \"\\\"'â€œÂ¿([{-\",\n     append_punctuations: str = \"\\\"'.ã€‚,ï¼Œ!ï¼?ï¼Ÿ:ï¼šâ€)]}ã€\",\n+    clip_timestamps: Union[str, List[float]] = \"0\",\n+    hallucination_silence_threshold: Optional[float] = None,\n     **decode_options,\n ):\n     \"\"\"\n@@ -102,6 +105,14 @@ def transcribe(\n     decode_options: dict\n         Keyword arguments to construct `DecodingOptions` instances\n \n+    clip_timestamps: Union[str, List[float]]\n+        Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process.\n+        The last end timestamp defaults to the end of the file.\n+\n+    hallucination_silence_threshold: Optional[float]\n+        When word_timestamps is True, skip silent periods longer than this threshold (in seconds)\n+        when a possible hallucination is detected\n+\n     Returns\n     -------\n     A dictionary containing the resulting text (\"text\") and segment-level details (\"segments\"), and\n@@ -121,6 +132,7 @@ def transcribe(\n     # Pad 30-seconds of silence to the input audio, for slicing\n     mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n     content_frames = mel.shape[-1] - N_FRAMES\n+    content_duration = float(content_frames * HOP_LENGTH / SAMPLE_RATE)\n \n     if decode_options.get(\"language\", None) is None:\n         if not model.is_multilingual:\n@@ -147,6 +159,19 @@ def transcribe(\n         task=task,\n     )\n \n+    if isinstance(clip_timestamps, str):\n+        clip_timestamps = [\n+            float(ts) for ts in (clip_timestamps.split(\",\") if clip_timestamps else [])\n+        ]\n+    seek_points: List[int] = [round(ts * FRAMES_PER_SECOND) for ts in clip_timestamps]\n+    if len(seek_points) == 0:\n+        seek_points.append(0)\n+    if len(seek_points) % 2 == 1:\n+        seek_points.append(content_frames)\n+    seek_clips: List[Tuple[int, int]] = list(zip(seek_points[::2], seek_points[1::2]))\n+\n+    punctuation = \"\\\"'â€œÂ¿([{-\\\"'.ã€‚,ï¼Œ!ï¼?ï¼Ÿ:ï¼šâ€)]}ã€\"\n+\n     if word_timestamps and task == \"translate\":\n         warnings.warn(\"Word-level timestamps on translations may not be reliable.\")\n \n@@ -190,7 +215,8 @@ def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n \n         return decode_result\n \n-    seek = 0\n+    clip_idx = 0\n+    seek = seek_clips[clip_idx][0]\n     input_stride = exact_div(\n         N_FRAMES, model.dims.n_audio_ctx\n     )  # mel frames per output token: 2\n@@ -229,10 +255,23 @@ def new_segment(\n         total=content_frames, unit=\"frames\", disable=verbose is not False\n     ) as pbar:\n         last_speech_timestamp = 0.0\n-        while seek < content_frames:\n+        # NOTE: This loop is obscurely flattened to make the diff readable.\n+        # A later commit should turn this into a simpler nested loop.\n+        # for seek_clip_start, seek_clip_end in seek_clips:\n+        #     while seek < seek_clip_end\n+        while clip_idx < len(seek_clips):\n+            seek_clip_start, seek_clip_end = seek_clips[clip_idx]\n+            if seek < seek_clip_start:\n+                seek = seek_clip_start\n+            if seek >= seek_clip_end:\n+                clip_idx += 1\n+                if clip_idx < len(seek_clips):\n+                    seek = seek_clips[clip_idx][0]\n+                continue\n             time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n-            mel_segment = mel[:, seek : seek + N_FRAMES]\n-            segment_size = min(N_FRAMES, content_frames - seek)\n+            window_end_time = float((seek + N_FRAMES) * HOP_LENGTH / SAMPLE_RATE)\n+            segment_size = min(N_FRAMES, content_frames - seek, seek_clip_end - seek)\n+            mel_segment = mel[:, seek : seek + segment_size]\n             segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n             mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n \n@@ -257,6 +296,30 @@ def new_segment(\n             previous_seek = seek\n             current_segments = []\n \n+            # anomalous words are very long/short/improbable\n+            def word_anomaly_score(word: dict) -> float:\n+                probability = word.get(\"probability\", 0.0)\n+                duration = word[\"end\"] - word[\"start\"]\n+                score = 0.0\n+                if probability < 0.15:\n+                    score += 1.0\n+                if duration < 0.133:\n+                    score += (0.133 - duration) * 15\n+                if duration > 2.0:\n+                    score += duration - 2.0\n+                return score\n+\n+            def is_segment_anomaly(segment: Optional[dict]) -> bool:\n+                if segment is None or not segment[\"words\"]:\n+                    return False\n+                words = [w for w in segment[\"words\"] if w[\"word\"] not in punctuation]\n+                words = words[:8]\n+                score = sum(word_anomaly_score(w) for w in words)\n+                return score >= 3 or score + 0.01 >= len(words)\n+\n+            def next_words_segment(segments: List[dict]) -> Optional[dict]:\n+                return next((s for s in segments if s[\"words\"]), None)\n+\n             timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n             single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n \n@@ -330,17 +393,71 @@ def new_segment(\n                     append_punctuations=append_punctuations,\n                     last_speech_timestamp=last_speech_timestamp,\n                 )\n-                word_end_timestamps = [\n-                    w[\"end\"] for s in current_segments for w in s[\"words\"]\n-                ]\n-                if len(word_end_timestamps) > 0:\n-                    last_speech_timestamp = word_end_timestamps[-1]\n-                if not single_timestamp_ending and len(word_end_timestamps) > 0:\n-                    seek_shift = round(\n-                        (word_end_timestamps[-1] - time_offset) * FRAMES_PER_SECOND\n-                    )\n-                    if seek_shift > 0:\n-                        seek = previous_seek + seek_shift\n+\n+                if not single_timestamp_ending:\n+                    last_word_end = get_end(current_segments)\n+                    if last_word_end is not None and last_word_end > time_offset:\n+                        seek = round(last_word_end * FRAMES_PER_SECOND)\n+\n+                # skip silence before possible hallucinations\n+                if hallucination_silence_threshold is not None:\n+                    threshold = hallucination_silence_threshold\n+                    if not single_timestamp_ending:\n+                        last_word_end = get_end(current_segments)\n+                        if last_word_end is not None and last_word_end > time_offset:\n+                            remaining_duration = window_end_time - last_word_end\n+                            if remaining_duration > threshold:\n+                                seek = round(last_word_end * FRAMES_PER_SECOND)\n+                            else:\n+                                seek = previous_seek + segment_size\n+\n+                    # if first segment might be a hallucination, skip leading silence\n+                    first_segment = next_words_segment(current_segments)\n+                    if first_segment is not None and is_segment_anomaly(first_segment):\n+                        gap = first_segment[\"start\"] - time_offset\n+                        if gap > threshold:\n+                            seek = previous_seek + round(gap * FRAMES_PER_SECOND)\n+                            continue\n+\n+                    # skip silence before any possible hallucination that is surrounded\n+                    # by silence or more hallucinations\n+                    hal_last_end = last_speech_timestamp\n+                    for si in range(len(current_segments)):\n+                        segment = current_segments[si]\n+                        if not segment[\"words\"]:\n+                            continue\n+                        if is_segment_anomaly(segment):\n+                            next_segment = next_words_segment(\n+                                current_segments[si + 1 :]\n+                            )\n+                            if next_segment is not None:\n+                                hal_next_start = next_segment[\"words\"][0][\"start\"]\n+                            else:\n+                                hal_next_start = time_offset + segment_duration\n+                            silence_before = (\n+                                segment[\"start\"] - hal_last_end > threshold\n+                                or segment[\"start\"] < threshold\n+                                or segment[\"start\"] - time_offset < 2.0\n+                            )\n+                            silence_after = (\n+                                hal_next_start - segment[\"end\"] > threshold\n+                                or is_segment_anomaly(next_segment)\n+                                or window_end_time - segment[\"end\"] < 2.0\n+                            )\n+                            if silence_before and silence_after:\n+                                seek = round(\n+                                    max(time_offset + 1, segment[\"start\"])\n+                                    * FRAMES_PER_SECOND\n+                                )\n+                                if content_duration - segment[\"end\"] < threshold:\n+                                    seek = content_frames\n+                                current_segments[si:] = []\n+                                break\n+                        hal_last_end = segment[\"end\"]\n+\n+                last_word_end = get_end(current_segments)\n+                if last_word_end is not None:\n+                    last_speech_timestamp = last_word_end\n \n             if verbose:\n                 for segment in current_segments:\n@@ -427,6 +544,8 @@ def valid_model_name(name):\n     parser.add_argument(\"--max_line_count\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of lines in a segment\")\n     parser.add_argument(\"--max_words_per_line\", type=optional_int, default=None, help=\"(requires --word_timestamps True, no effect with --max_line_width) the maximum number of words in a segment\")\n     parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n+    parser.add_argument(\"--clip_timestamps\", type=str, default=\"0\", help=\"comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process, where the last end timestamp defaults to the end of the file\")\n+    parser.add_argument(\"--hallucination_silence_threshold\", type=optional_float, help=\"(requires --word_timestamps True) skip silent periods longer than this threshold (in seconds) when a possible hallucination is detected\")\n     # fmt: on\n \n     args = parser.parse_args().__dict__\ndiff --git a/whisper/utils.py b/whisper/utils.py\nindex 7a172c401..9b9b13862 100644\n--- a/whisper/utils.py\n+++ b/whisper/utils.py\n@@ -3,7 +3,7 @@\n import re\n import sys\n import zlib\n-from typing import Callable, Optional, TextIO\n+from typing import Callable, List, Optional, TextIO\n \n system_encoding = sys.getdefaultencoding()\n \n@@ -68,6 +68,20 @@ def format_timestamp(\n     )\n \n \n+def get_start(segments: List[dict]) -> Optional[float]:\n+    return next(\n+        (w[\"start\"] for s in segments for w in s[\"words\"]),\n+        segments[0][\"start\"] if segments else None,\n+    )\n+\n+\n+def get_end(segments: List[dict]) -> Optional[float]:\n+    return next(\n+        (w[\"end\"] for s in reversed(segments) for w in reversed(s[\"words\"])),\n+        segments[-1][\"end\"] if segments else None,\n+    )\n+\n+\n class ResultWriter:\n     extension: str\n \n@@ -129,8 +143,8 @@ def iterate_subtitles():\n             line_len = 0\n             line_count = 1\n             # the next subtitle to yield (a list of word timings with whitespace)\n-            subtitle: list[dict] = []\n-            last = result[\"segments\"][0][\"words\"][0][\"start\"]\n+            subtitle: List[dict] = []\n+            last: float = get_start(result[\"segments\"]) or 0.0\n             for segment in result[\"segments\"]:\n                 chunk_index = 0\n                 words_count = max_words_per_line\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Testing on another example from https://github.com/openai/whisper/discussions/679#discussioncomment-7649183\r\n\r\n<details>\r\n<summary>Output</summary>\r\n\r\n```\r\nv2 runs:\r\n\r\n[00:00.000 --> 00:05.660]  spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo\r\nDETECTED HALLUCINATION:  non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho\r\nDETECTED HALLUCINATION:  no\r\nDETECTED HALLUCINATION:  no\r\n\r\n\r\n\r\n[00:00.000 --> 00:05.660]  spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo\r\nDETECTED HALLUCINATION:  non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho\r\nDETECTED HALLUCINATION:  .....\r\nDETECTED HALLUCINATION:  .....\r\n\r\n\r\n\r\n[00:00.000 --> 00:05.660]  spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo\r\nDETECTED HALLUCINATION:  non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho\r\nDETECTED HALLUCINATION:  uh\r\nDETECTED HALLUCINATION:  uh\r\n\r\n\r\n\r\nv3 run:\r\n\r\n[00:00.000 --> 00:04.240]  Spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo.\r\nDETECTED HALLUCINATION:  Grazie a tutti.\r\nDETECTED HALLUCINATION:  E' un attimo che non dovevo.\r\n[00:54.440 --> 00:55.700]  Ehm, ehm.\r\n```;\nI don't have the exact code anymore, but you could try temporarily inserting these two lines:\r\n\r\n```python\r\n                if score >= 3 or score + 0.01 >= len(words):\r\n                    print(f\"DETECTED HALLUCINATION: {segment['text']}\")\r\n```\r\n\r\nbefore the return in this function:\r\n\r\n```python\r\n            def is_segment_anomaly(segment: Optional[dict]) -> bool:\r\n                if segment is None or not segment[\"words\"]:\r\n                    return False\r\n                words = [w for w in segment[\"words\"] if w[\"word\"] not in punctuation]\r\n                words = words[:8]\r\n                score = sum(word_anomaly_score(w) for w in words)\r\n                return score >= 3 or score + 0.01 >= len(words)\r\n```;\n@ryanheise\r\nSometimes `--hallucination_silence_threshold` makes whole non-hallucinating segments or part of segments disappear.\r\n\r\nBelow is example with disappeared `orange pigmentation.` segment.\r\n\r\nI'm using faster-whisper, but you should be able to reproduce it with whisper too as implementation is same.\r\nAudio file -> https://we.tl/t-U5a6Al5bRs\r\n\r\n\r\n`--language en --model=base --beam_size=5 --word_timestamps=True --hallucination_silence_threshold=None`:\r\n\r\n```\r\n[02:06.620 --> 02:11.120]  White tigers carry a mutated version of this gene, which prevents them from producing\r\n  Processing segment at 02:11.120\r\n[02:11.120 --> 02:12.460]  orange pigmentation.\r\n[02:15.360 --> 02:18.340]  Fewer than 4,000 tigers remain in the wild.\r\n```\r\n\r\n`--language en --model=base --beam_size=5 --word_timestamps=True --hallucination_silence_threshold=2`:\r\n\r\n```\r\n[02:06.620 --> 02:11.120]  White tigers carry a mutated version of this gene, which prevents them from producing\r\n  Processing segment at 02:12.380\r\n* HST_1: Skipping silence before possible hallucinations.\r\n* HST_3: DETECTED HALLUCINATION:  oxygen.\r\n  Processing segment at 02:13.380\r\n* HST_1: Skipping silence before possible hallucinations.\r\n[02:14.680 --> 02:18.360]  fewer than 4,000 tigers remain in the wild.\r\n```\r\n\r\nEDIT:\r\nfloat32 was in use;\nI think, I've noticed a pattern, it happens when `if remaining_duration > threshold:` is not triggered, in there:\r\n`seek = previous_seek + segment_size`\r\n\r\nThen chunk go exactly by 30 secs cutting off the word.\r\n\r\nChunking when `--hallucination_silence_threshold=None`:\r\n\r\n```\r\n  Processing segment at 00:00.000\r\n  Processing segment at 00:26.040\r\n  Processing segment at 00:48.280\r\n  Processing segment at 01:14.400\r\n  Processing segment at 01:42.380\r\n  Processing segment at 02:11.120\r\n  Processing segment at 02:35.400\r\n  Processing segment at 03:05.400\r\n```\r\nChunking by setting high threshold `--hallucination_silence_threshold=40`:\r\n```\r\n  Processing segment at 00:00.000\r\n  Processing segment at 00:30.000\r\n  Processing segment at 01:00.000\r\n  Processing segment at 01:30.000\r\n  Processing segment at 02:00.000\r\n  Processing segment at 02:30.000\r\n  Processing segment at 03:00.000\r\n```;\nAnother thing, this PR affects transcription even if both new parameters are not enabled, I meant comparing vs without this PR.\r\n\r\nThis happens sometimes, but when it happens the discrepancy is always in the last chunk.\r\n\r\nAnd sometimes when discrepancy happens it tries to process additional micro chunk after it which produces some hallucination or fails because no-speech threshold is met, not sure if this is related to PR or to a discrepancy.\r\n\r\nExample of such discrepancy [audio is `05:05.877` long]:\r\n\r\nWithout this PR [perfect transcription]:\r\n```\r\nProcessing segment at 04:48.000\r\n[04:58.120 --> 05:05.260]  I just...\r\n[05:05.260 --> 05:05.760]  I...\r\n```\r\n\r\nWith this PR [all goes exactly same till the last chunk]:\r\n\r\n```\r\n  Processing segment at 04:48.000\r\n* Compression ratio threshold is not met with temperature 0.0 (3.523810 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.2 (3.523810 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.4 (8.038462 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.6 (3.523810 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.8 (2.423077 > 2.400000)\r\n[05:01.940 --> 05:02.900]  Okay.\r\n[05:02.900 --> 05:04.000]  I just-\r\n[05:04.940 --> 05:05.740]  I-\r\n[05:05.740 --> 05:05.840]  I-\r\n* Reset prompt. prompt_reset_on_temperature threshold is met 1.000000 > 0.500000\r\n  Processing segment at 05:05.840\r\n* Log probability threshold is not met with temperature 0.0 (-1.105777 < -1.000000)\r\n* No speech threshold is met (0.772002 > 0.600000)\r\n```;\n> Sometimes `--hallucination_silence_threshold` makes whole non-hallucinating segments or part of segments disappear.\r\n\r\nThis logic is part of the original Whisper strategy of advancing by the full 30 seconds to the next window whenever the current segment is unfinished. So basically, if the segment finishes before the end of the 30 second window, then Whisper will crop the window to the exact end timestamp of the last word in that segment. But if the segment does not finish by the end of the 30 second window, the window is not cropped, the speech is assumed to run all the way to the end of the window.\r\n\r\nThis logic exists whether or not the `hallucination_silence_threshold` is enabled, and I have seen it cause problems in both cases, however the larger models tend to be better at picking up the words across the window boundary.\r\n\r\nIn your case, the sentence in question is:\r\n\r\n> White tigers carry a mutated version of this gene, which prevents them from producing orange pigmentation.\r\n\r\nThis sentence does not fit within the 30 second window, and the word \"orange\" is right on the boundary. In fact, the word \"orange\" is slightly before the boundary and the human ear can pick it up (as can the larger models) but the smaller models fail to pick it up.\r\n\r\nAnd given Whisper's logic in this case, it will assume the speech went right up to the end of the 30 second window and will resume the next window from there.\r\n\r\nSo although yes the large models would probably resolve this, I think it would still be better to change Whisper's strategy and crop the window to the end timestamp of the last word even in this case where we have an unfinished segment.;\n> This logic is part of the original Whisper strategy of advancing by the full 30 seconds to the next window whenever the current segment is unfinished.\r\n\r\nI can't connect the dots...\r\nThen why it's \"unfinished\" when using `hallucination_silence_threshold` and it's \"finished\" without it?\r\n\r\nHow `remaining_duration <= hallucination_silence_threshold` means an \"unfinished\" segment? The option doesn't read as \"finished/unfinished segment threshold\"....\r\n;\nApologies, my explanation of that was around the wrong way. The original Whisper behaviour was that if the last segment in the window is \"complete\", THEN it skips to the end of the full 30 second window. If the last segment is incomplete, then it crops the window to end timestamp of the last word.\r\n\r\nBut when `hallucination_silence_threshold` is set, it still applies this logic in most cases except that it also includes a misfired heuristic that skips to the end of the full 30 second window if the end of the speech is close enough to the end of the window:\r\n\r\n```python\r\n                # skip silence before possible hallucinations\r\n                if hallucination_silence_threshold is not None:\r\n                    threshold = hallucination_silence_threshold\r\n                    if not single_timestamp_ending:\r\n                        last_word_end = get_end(current_segments)\r\n                        if last_word_end is not None and last_word_end > time_offset:\r\n                            remaining_duration = window_end_time - last_word_end\r\n                            if remaining_duration > threshold:  # <--- misfired heuristic\r\n                                seek = round(last_word_end * FRAMES_PER_SECOND)\r\n                            else:\r\n                                seek = previous_seek + segment_size\r\n````\r\n\r\nThe goal was to skip over as much silence as safely possible.\r\n\r\nHowever, in hindsight, this was a bit opportunistic, since after all `single_timestamp_ending` was `False` for good reason. You should find your example will work if you remove that heuristic. i.e. Delete this entire section:\r\n\r\n```python\r\n                    if not single_timestamp_ending:\r\n                        last_word_end = get_end(current_segments)\r\n                        if last_word_end is not None and last_word_end > time_offset:\r\n                            remaining_duration = window_end_time - last_word_end\r\n                            if remaining_duration > threshold:  # <--- misfired heuristic\r\n                                seek = round(last_word_end * FRAMES_PER_SECOND)\r\n                            else:\r\n                                seek = previous_seek + segment_size\r\n```\r\n\r\n(It's OK, the other parts of this code block are already handled elsewhere.);\nThanks for explanation, now this part of code makes sense.\r\nDo you have idea why seek in the last window can be affected by PR? -> https://github.com/openai/whisper/pull/1838#issuecomment-1960581637\r\n\r\n> The goal was to skip over as much silence as safely possible.\r\n\r\nImho, skipping to full 30s window is pretty unsafe.  ðŸ˜†\r\nAnd it contradicted the description: \"skip silent periods longer than this threshold (in seconds) **when a possible hallucination is detected**\";\n> Do you have an audio file to reproduce?\r\n\r\nThis file has discrepancy in the last window/chunk:\r\nt-001.mka -> https://we.tl/t-ecd6U1QaZp\r\n`--language en --model=base --beam_size 1 --word_timestamps=True`\r\n\r\nWhisper without this PR:\r\n```\r\n[01:53.920 --> 01:54.500]  I'll give you some advice.\r\n[01:59.500 --> 02:00.080]  I'll give you some advice.\r\n[02:00.080 --> 02:00.080]  I'll give you some advice.\r\n[02:00.080 --> 02:00.980]  Say the word, General.\r\n[02:02.300 --> 02:03.320]  Let him go.\r\n```\r\nWhisper with this PR:\r\n```\r\n[01:53.920 --> 01:55.200]  I'll give you some advice.\r\n[01:59.500 --> 02:00.980]  Say the word, General.\r\n[02:02.280 --> 02:03.320]  Let him go.\r\n```\r\n;\n> I'll test tomorrow, but does this also happen on PR #2043 ?\r\n\r\nYes, because `hallucination_silence_threshold` option is not relevant for the issue.\r\n\r\nCulprit affecting only the last window is found. it happens because of this:\r\n```python\r\n            mel_segment = mel[:, seek : seek + segment_size]\r\n```\r\n\r\nThis is the fix [that's how it was before this PR]:\r\n```python\r\n            mel_segment = mel[:, seek : seek + N_FRAMES]\r\n```\r\n\r\nNot sure why you changed it, on my observation it makes more hallucinations [probably it's random].\r\nAnyway, the fix brings back the previous behavior.;\nI've confirmed the discrepancy, which seems to be a consequence of slightly different mel spectrograms. Although in the two examples you gave (only the latter of which I have tested with the supplied audio file), the PR actually removed a hallucination on one example and introduced a hallucination on the other example. So on balance, it's hard to say whether this discrepancy it better or worse or about the same.\r\n\r\nSo if it's not clear whether it's better or worse, do you see anything incorrect in the clipping logic? I think the difference is that I am always clipping exactly to the stretch of audio being examined, and then padding it. But originally, there was padding on the end that was added immediately when the mel spectrogram was first generated, and then (in the original code), it is also possible that due to the dynamic shifting of the window starts, it could end up padding the last part of the audio twice, because there is no guarantee that that initial padding Whisper added at the start of the process was enough to reflect where this last window ended up actually starting.\r\n\r\nBut it's possible I've done something wrong which I can't see, so let me know if you do spot something incorrect in the logic.;\nAfter plotting the mel spectrograms, I noticed the padding when the audio is first loaded (as a whole) contains all -1.0's, while the padding in the main loop for each 30 second window contains all 0.0's. Not sure why that is, but there are two different padding algorithms in the code, and weirdly they are producing different padding results.\r\n\r\nSo in your example, the PR ends up always using the padding algorithm that pads to 0.0's whereas originally the end of file padding had -1.0's. ;\nThere's still a chance that a hallucination will be produced.\r\nFor me it was:\r\n```\r\n[02:15:58.100 --> 02:16:05.380]  Ñ Ð²Ð°Ð¼ Ð²Ñ‹ÑˆÐ»ÑŽ. Ð’ÑÐµÐ³Ð¾ Ð´Ð¾Ð±Ñ€Ð¾Ð³Ð¾, Ð´Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ.\r\n[02:16:28.100 --> 02:16:30.100]  Ð ÐµÐ´Ð°ÐºÑ‚Ð¾Ñ€ ÑÑƒÐ±Ñ‚Ð¸Ñ‚Ñ€Ð¾Ð² Ð˜.Ð‘Ð¾Ð¹ÐºÐ¾Ð²Ð°\r\n```\r\ni. e.\r\n```\r\n....\r\n[02:16:28.100 --> 02:16:30.100] Subtitle Editor I. Boykova\r\n```\r\nNotably, this timestamp belongs to the end of the audio.\r\n\r\nModel size: small. Also there are some results in google, if you search for this phrase. One of them:\r\n```\r\n[24:26.800 --> 24:30.160]  Ð¡Ð¼Ð¾Ñ‚Ñ€Ð¸Ñ‚Ðµ Ñ‚ÐµÐ»ÐµÐ±Ð°Ñ€Ð¾Ð¼ÐµÑ‚Ñ€ Ð½Ð° Ð½Ð°ÑˆÐµÐ¼ Ñ‚ÐµÐ»ÐµÐºÐ°Ð½Ð°Ð»Ðµ.\r\n[24:30.160 --> 24:32.160]  Ð ÐµÐ´Ð°ÐºÑ‚Ð¾Ñ€ ÑÑƒÐ±Ñ‚Ð¸Ñ‚Ñ€Ð¾Ð² Ð˜.Ð‘Ð¾Ð¹ÐºÐ¾Ð²Ð°\r\n[24:32.160 --> 24:39.160]  ÐšÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¾Ñ€ Ð.ÐšÑƒÐ»Ð°ÐºÐ¾Ð²Ð°\r\n```\r\nfrom https://storage.googleapis.com/data.gdeltproject.org/blog/2022-tv-news-whisperasr/BELARUSTV_20221005_161500.small.transcribe.run1.txt;\nThat's certainly possible, and unfortunately there is no single choice of parameters that will be perfect in all scenarios. You can tweak the silence threshold, which is exposed on the command line. You can also try tweaking the other thresholds that were built into the code (like how long a word must be before it is flagged as an abnormality). If we can gather a large enough dataset of audio samples that produce hallucinations, we should be able to come up with better default settings that work well across a variety of scenarios and languages.;\n@ryanheise \r\nI was using a bit tweaked segment anomaly heuristics to reduce false-positives, didn't noticed increase of false-negatives:\r\n\r\nchanged\r\n `if duration < 0.133:`\r\nto:\r\n`if duration < 0.133 and probability < 0.8:`\r\n\r\nchanged\r\n`return score >= 3 or score + 0.01 >= len(words)`\r\nto:\r\n`return score >= 3 or score + 0.001 >= len(words)`\r\n\r\nWhat you think about this tweak?;\nUnfortunately I'm between computers right now (my old computer died 2 weeks ago, and I'm just in the process of installing everything on the recently arrived replacement...)\r\n\r\n>  return score >= 3 or score + 0.001 >= len(words)\r\n\r\nI don't see any problem with that change.\r\n\r\n>  `if duration < 0.133 and probability < 0.8:`\r\n\r\nDo you have an example audio for this one? I'd be interested to analyse this correlation between duration < 0.133 and probability < 0.8.\r\n\r\nThe alternative is to take into account more observations (like your audio example) and try to fit a new curve to the data. I initially fitted a simple linear curve, and maybe exponential could help because it could model a slower initial gradient."
      }
    ],
    "token_estimation": 7799.25
  },
  {
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/whisper/audio.py b/whisper/audio.py\nindex a19b7ab0d..513ab7c9d 100644\n--- a/whisper/audio.py\n+++ b/whisper/audio.py\n@@ -1,6 +1,6 @@\n import os\n from functools import lru_cache\n-from typing import Union\n+from typing import Optional, Union\n \n import ffmpeg\n import numpy as np\n@@ -15,10 +15,8 @@\n N_MELS = 80\n HOP_LENGTH = 160\n CHUNK_LENGTH = 30\n-N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000: number of samples in a chunk\n-N_FRAMES = exact_div(\n-    N_SAMPLES, HOP_LENGTH\n-)  # 3000: number of frames in a mel spectrogram input\n+N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n+N_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n \n N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n FRAMES_PER_SECOND = exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n@@ -100,7 +98,10 @@ def mel_filters(device, n_mels: int = N_MELS) -> torch.Tensor:\n \n \n def log_mel_spectrogram(\n-    audio: Union[str, np.ndarray, torch.Tensor], n_mels: int = N_MELS\n+    audio: Union[str, np.ndarray, torch.Tensor],\n+    n_mels: int = N_MELS,\n+    padding: int = 0,\n+    device: Optional[Union[str, torch.device]] = None,\n ):\n     \"\"\"\n     Compute the log-Mel spectrogram of\n@@ -113,6 +114,12 @@ def log_mel_spectrogram(\n     n_mels: int\n         The number of Mel-frequency filters, only 80 is supported\n \n+    padding: int\n+        Number of zero samples to pad to the right\n+\n+    device: Optional[Union[str, torch.device]]\n+        If given, the audio tensor is moved to this device before STFT\n+\n     Returns\n     -------\n     torch.Tensor, shape = (80, n_frames)\n@@ -123,6 +130,10 @@ def log_mel_spectrogram(\n             audio = load_audio(audio)\n         audio = torch.from_numpy(audio)\n \n+    if device is not None:\n+        audio = audio.to(device)\n+    if padding > 0:\n+        audio = F.pad(audio, (0, padding))\n     window = torch.hann_window(N_FFT).to(audio.device)\n     stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)\n     magnitudes = stft[..., :-1].abs() ** 2\ndiff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex 20f01477e..773e6365e 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -11,6 +11,7 @@\n     FRAMES_PER_SECOND,\n     HOP_LENGTH,\n     N_FRAMES,\n+    N_SAMPLES,\n     SAMPLE_RATE,\n     log_mel_spectrogram,\n     pad_or_trim,\n@@ -116,7 +117,9 @@ def transcribe(\n     if dtype == torch.float32:\n         decode_options[\"fp16\"] = False\n \n-    mel = log_mel_spectrogram(audio)\n+    # Pad 30-seconds of silence to the input audio, for slicing\n+    mel = log_mel_spectrogram(audio, padding=N_SAMPLES)\n+    content_frames = mel.shape[-1] - N_FRAMES\n \n     if decode_options.get(\"language\", None) is None:\n         if not model.is_multilingual:\n@@ -212,14 +215,13 @@ def new_segment(\n         }\n \n     # show the progress bar when verbose is False (if True, transcribed text will be printed)\n-    num_frames = mel.shape[-1]\n     with tqdm.tqdm(\n-        total=num_frames, unit=\"frames\", disable=verbose is not False\n+        total=content_frames, unit=\"frames\", disable=verbose is not False\n     ) as pbar:\n-        while seek < num_frames:\n+        while seek < content_frames:\n             time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n-            mel_segment = mel[:, seek:]\n-            segment_size = min(mel_segment.shape[-1], N_FRAMES)\n+            mel_segment = mel[:, seek : seek + N_FRAMES]\n+            segment_size = min(N_FRAMES, content_frames - seek)\n             segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n             mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n \n@@ -246,20 +248,18 @@ def new_segment(\n             current_tokens = []\n \n             timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n-            consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[\n-                0\n-            ].add_(1)\n-            if (\n-                len(consecutive) > 0\n-            ):  # if the output contains two consecutive timestamp tokens\n-                if ended_with_single_timestamp := timestamp_tokens[-2:].tolist() == [\n-                    False,\n-                    True,\n-                ]:\n-                    consecutive = consecutive.tolist() + [len(tokens)]\n+            single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n+\n+            consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n+            consecutive.add_(1)\n+            if len(consecutive) > 0:\n+                # if the output contains two consecutive timestamp tokens\n+                slices = consecutive.tolist()\n+                if single_timestamp_ending:\n+                    slices.append(len(tokens))\n \n                 last_slice = 0\n-                for current_slice in consecutive:\n+                for current_slice in slices:\n                     sliced_tokens = tokens[last_slice:current_slice]\n                     start_timestamp_pos = (\n                         sliced_tokens[0].item() - tokenizer.timestamp_begin\n@@ -278,7 +278,7 @@ def new_segment(\n                     current_tokens.append(sliced_tokens.tolist())\n                     last_slice = current_slice\n \n-                if ended_with_single_timestamp:\n+                if single_timestamp_ending:\n                     # single timestamp at the end means no speech after the last timestamp.\n                     seek += segment_size\n                 else:\n@@ -329,7 +329,7 @@ def new_segment(\n                 word_end_timestamps = [\n                     w[\"end\"] for s in current_segments for w in s[\"words\"]\n                 ]\n-                if len(consecutive) > 0 and len(word_end_timestamps) > 0:\n+                if not single_timestamp_ending and len(word_end_timestamps) > 0:\n                     seek_shift = round(\n                         (word_end_timestamps[-1] - time_offset) * FRAMES_PER_SECOND\n                     )\n@@ -356,7 +356,7 @@ def new_segment(\n             )\n \n             # update progress bar\n-            pbar.update(min(num_frames, seek) - previous_seek)\n+            pbar.update(min(content_frames, seek) - previous_seek)\n \n     return dict(\n         text=tokenizer.decode(all_tokens[len(initial_prompt_tokens) :]),\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The problem triggered by the test data from @ryanheise is model sensitive. I see the problem with `small` but using either `small.en` or `medium.en` looks ok although the timing of the last few words is off. Below is the mp3 fragment converted to video to show the English subtitles.\r\n\r\nhttps://user-images.githubusercontent.com/3035114/223597998-74a8ec7f-da0b-4948-9f6a-75712820eb15.mp4\r\n\r\n\r\n;\nThanks all! The incorrect zero-padding of Mel spectrograms as identified in #730 and #838 was contributing to this error. The fix in 477f0be appears to fix the repetition issue.;\nBtw have you guys tried with longer audio, e.g. 5 mins long? I am still getting a lot of repetition even with this fix.\r\nE.g. on the TEDLIUM test set \"AimeeMullins_2009P.wav\"\r\n>[02:10.440 --> 02:14.720]  and needless to say, thank God, I wasn't using a thesaurus back then.\r\n[02:14.720 --> 02:14.720]  and needless to say, thank God, I wasn't using a thesaurus back then.\r\n[02:15.460 --> 02:18.580]  I mean from this entry, it would seem that\r\n[02:18.580 --> 02:22.800]  I was born into a world that perceived someone like me\r\n[02:22.800 --> 02:23.340]  I was born into a world that perceived someone like me\r\n[02:23.340 --> 02:27.540]  to have nothing positive, whatsoever, going for them\r\n[02:27.540 --> 02:27.540]  to have nothing positive, whatsoever, going for them\r\n[02:27.540 --> 02:35.340]  When in fact today, I'm celebrated for the opportunities and adventures my life has procured\r\n[02:35.340 --> 02:35.960]  When in fact today, I'm celebrated for the opportunities and adventures my life has procured\r\n[02:35.960 --> 02:42.140]  So I immediately went to look up the 2009 online edition\r\n[02:42.140 --> 02:42.160]  So I immediately went to look up the 2009 online edition\r\n[02:42.160 --> 02:42.160]  So I immediately went to look up the 2009 online edition\r\n\r\nI was hoping to update word segmentation results for whisper-only word timestamps in our paper https://arxiv.org/abs/2303.00747\r\n\r\nBut currently i am getting better results with our implementation which is similar to https://github.com/linto-ai/whisper-timestamped\r\n;\n@jongwook Note from @m-bain example above the repetition occurring with verbose print. The repetitions in this example are all \"instantaneous\" ; eg same start and end time\r\n> [02:14.720 --> 02:14.720] and needless to say, thank God, I wasn't using a thesaurus back then.\r\n\r\nthey are printed but then immediately cleared by this code, which looks like a bug unique to `--verbose True`\r\n\r\nhttps://github.com/openai/whisper/blob/aac47c98349b98cec5ca7b1be53960fb59f4436b/whisper/transcribe.py#L345;\nThis is not a verbose error, and the start times and end times of repetition are not always instantaneous, see output for the .srt file without verbose:\r\n\r\n271\r\n00:02:14,440 --> 00:02:14,720\r\nand needless to say, thank God, I wasn't using a thesaurus back<u> then.</u>\r\n\r\n272\r\n00:02:14,720 --> 00:02:14,720\r\n\r\n\r\n273\r\n00:02:15,460 --> 00:02:16,180\r\n<u>I</u> mean from this entry, it would seem that\r\n\r\n274\r\n00:02:16,180 --> 00:02:16,360\r\nI<u> mean</u> from this entry, it would seem that\r\n\r\n275\r\n00:02:16,360 --> 00:02:16,960\r\nI mean<u> from</u> this entry, it would seem that\r\n\r\n276\r\n00:02:16,960 --> 00:02:17,220\r\nI mean from<u> this</u> entry, it would seem that\r\n\r\n277\r\n00:02:17,220 --> 00:02:17,620\r\nI mean from this<u> entry,</u> it would seem that\r\n\r\n278\r\n00:02:17,620 --> 00:02:17,800\r\nI mean from this entry, it would seem that\r\n>;\nSo there are at least two problems then\r\n* verbose mode can print cleared segments\r\n* something else triggered by word_timestamps\r\n\r\nGiven how close the start/end times are it feels like something related to `seek_shift` is still off\r\nhttps://github.com/openai/whisper/blob/aac47c98349b98cec5ca7b1be53960fb59f4436b/whisper/transcribe.py#L337\r\n\r\n@m-bain Do the same repetitions happen with `word_timestamps False` or no? ;\nUpdate, I realise there is some specific underline formatting in the word_timestamps, was able to get it working in the end. See here for comparison on word-level timestamp accuracy\r\n\r\n![image](https://user-images.githubusercontent.com/36994049/224011580-4782f2ad-a178-4b2d-80c3-4baa8ca54ab9.png)\r\n\r\n@jongwook could you share the evaluation for long-form transcription WER? I am unable to reproduce whisper results, right now I report in the vanilla setting -- greedy/beam5 decoding without the heuristic tricks\r\n"
      }
    ],
    "token_estimation": 2680.25
  },
  {
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/.gitignore b/.gitignore\nindex 7ae8fabc6..9f5ed862b 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -9,3 +9,6 @@ thumbs.db\n .DS_Store\n .idea\n \n+.venv/\n+\n+samples/\ndiff --git a/README.md b/README.md\nindex eba82ce22..9ea3a38e5 100644\n--- a/README.md\n+++ b/README.md\n@@ -144,4 +144,4 @@ Please use the [ðŸ™Œ Show and tell](https://github.com/openai/whisper/discussion\n \n ## License\n \n-Whisper's code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.\n+Whisper's code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.\n\\ No newline at end of file\ndiff --git a/examples/confidence_per_token.py b/examples/confidence_per_token.py\nnew file mode 100644\nindex 000000000..0bf2a30d7\n--- /dev/null\n+++ b/examples/confidence_per_token.py\n@@ -0,0 +1,59 @@\n+# IMPORTANT: This is just for using the local whisper dir as the package directly. Delete until next comment when just installing whisper normally.\n+import sys\n+from pathlib import Path\n+sys.path.insert(0, str(Path(__file__).resolve().parents[1]))\n+# end of dev import\n+import whisper\n+\n+import colorsys\n+from typing import List\n+from whisper.tokenizer import get_tokenizer\n+from colorama import init, Style\n+\n+\n+print('Loading model')\n+model = whisper.load_model(\"large\")\n+\n+\n+print('Loading audio') # load audio and pad/trim it to fit 30 seconds\n+audio = whisper.load_audio(\"samples/your_audio.wav\")\n+audio = whisper.pad_or_trim(audio)\n+\n+\n+mel = whisper.log_mel_spectrogram(audio).to(model.device) # make log-Mel spectrogram and move to the same device as the model\n+\n+\n+detect_lang = False\n+language = \"en\"\n+if detect_lang: # detect the spoken language\n+    print('Detecting language')\n+    _, probs = model.detect_language(mel)\n+    print(f\"Detected language: {max(probs, key=probs.get)}\")\n+    language=max(probs, key=probs.get)\n+\n+\n+print('Decoding audio') # decode the audio\n+options = whisper.DecodingOptions()\n+result = whisper.decode(model, mel, options)\n+\n+\n+def get_colored_text(tokens: List[int], token_probs: List[float], tokenizer, prompt: str=\"\"):\n+    init(autoreset=False)  # Initialize colorama\n+    text_tokens = [tokenizer.decode([t]) for t in tokens]\n+\n+    output_text = \"\"\n+    for i, (token, prob) in enumerate(zip(text_tokens, token_probs)):\n+        # Interpolate between red and green in the HSV color space\n+        r, g, b = colorsys.hsv_to_rgb(prob * (1/3), 1, 1)\n+        r, g, b = int(r * 255), int(g * 255), int(b * 255)\n+        color_code = f\"\\033[38;2;{r};{g};{b}m\"\n+\n+        colored_token = f\"{color_code}{Style.BRIGHT}{token}{Style.RESET_ALL}\"\n+        output_text += colored_token\n+\n+    return output_text\n+\n+\n+tokenizer = get_tokenizer(multilingual=model.is_multilingual, language=language, task=options.task)\n+print(get_colored_text(result.tokens, result.token_probs, tokenizer))  # print text with fancy confidence colors\n+# HINT: when using a prompt, you must provide it in the get_colored_text as well\ndiff --git a/whisper/decoding.py b/whisper/decoding.py\nindex 81cd8452b..4ded71bd7 100644\n--- a/whisper/decoding.py\n+++ b/whisper/decoding.py\n@@ -118,6 +118,7 @@ class DecodingResult:\n     language: str\n     language_probs: Optional[Dict[str, float]] = None\n     tokens: List[int] = field(default_factory=list)\n+    token_probs: List[float] = field(default_factory=list)\n     text: str = \"\"\n     avg_logprob: float = np.nan\n     no_speech_prob: float = np.nan\n@@ -211,7 +212,7 @@ def reset(self):\n         \"\"\"Initialize any stateful variables for decoding a new sequence\"\"\"\n \n     def update(\n-        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n+        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor, token_probs: Tensor\n     ) -> Tuple[Tensor, bool]:\n         \"\"\"Specify how to select the next token, based on the current trace and logits\n \n@@ -238,7 +239,7 @@ def update(\n         raise NotImplementedError\n \n     def finalize(\n-        self, tokens: Tensor, sum_logprobs: Tensor\n+        self, tokens: Tensor, sum_logprobs: Tensor, token_probs: Tensor\n     ) -> Tuple[Sequence[Sequence[Tensor]], List[List[float]]]:\n         \"\"\"Finalize search and return the final candidate sequences\n \n@@ -268,7 +269,7 @@ def __init__(self, temperature: float, eot: int):\n         self.eot = eot\n \n     def update(\n-        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n+        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor, token_probs: Tensor\n     ) -> Tuple[Tensor, bool]:\n         if self.temperature == 0:\n             next_tokens = logits.argmax(dim=-1)\n@@ -276,19 +277,28 @@ def update(\n             next_tokens = Categorical(logits=logits / self.temperature).sample()\n \n         logprobs = F.log_softmax(logits.float(), dim=-1)\n+        probs = torch.exp(logprobs)\n         current_logprobs = logprobs[torch.arange(logprobs.shape[0]), next_tokens]\n         sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)\n \n         next_tokens[tokens[:, -1] == self.eot] = self.eot\n         tokens = torch.cat([tokens, next_tokens[:, None]], dim=-1)\n \n+        current_token_probs = probs[torch.arange(probs.shape[0]), next_tokens]\n+        token_probs = torch.cat([token_probs, current_token_probs[:, None]], dim=-1)\n+\n+        # token_logits = torch.stack([logits[k, next_tokens[k]] for k in range(next_tokens .shape[0])], dim=0)\n+        # or use logprobs, the log softmax of the logits\n+        # return it along with tokens and completed\n+\n         completed = (tokens[:, -1] == self.eot).all()\n-        return tokens, completed\n+        return tokens, completed, token_probs\n \n-    def finalize(self, tokens: Tensor, sum_logprobs: Tensor):\n+    def finalize(self, tokens: Tensor, sum_logprobs: Tensor, token_probs: Tensor):\n         # make sure each sequence has at least one EOT token at the end\n         tokens = F.pad(tokens, (0, 1), value=self.eot)\n-        return tokens, sum_logprobs.tolist()\n+        token_probs = F.pad(token_probs, (0, 1), value=0) # 0 ok?\n+        return tokens, sum_logprobs.tolist(), token_probs.tolist()\n \n \n class BeamSearchDecoder(TokenDecoder):\n@@ -374,7 +384,7 @@ def update(\n         )\n         return tokens, completed\n \n-    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor):\n+    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor, token_probs: Tensor):\n         # collect all finished sequences, including patience, and add unfinished ones if not enough\n         sum_logprobs = sum_logprobs.cpu()\n         for i, sequences in enumerate(self.finished_sequences):\n@@ -668,6 +678,8 @@ def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n         sum_logprobs: Tensor = torch.zeros(n_batch, device=audio_features.device)\n         no_speech_probs = [np.nan] * n_batch\n \n+        token_probs = torch.zeros_like(tokens).float()\n+\n         try:\n             for i in range(self.sample_len):\n                 logits = self.inference.logits(tokens, audio_features)\n@@ -686,14 +698,14 @@ def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n                     logit_filter.apply(logits, tokens)\n \n                 # expand the tokens tensor with the selected next tokens\n-                tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\n+                tokens, completed, token_probs = self.decoder.update(tokens, logits, sum_logprobs, token_probs)\n \n                 if completed or tokens.shape[-1] > self.n_ctx:\n                     break\n         finally:\n             self.inference.cleanup_caching()\n \n-        return tokens, sum_logprobs, no_speech_probs\n+        return tokens, sum_logprobs, no_speech_probs, token_probs\n \n     @torch.no_grad()\n     def run(self, mel: Tensor) -> List[DecodingResult]:\n@@ -721,7 +733,7 @@ def run(self, mel: Tensor) -> List[DecodingResult]:\n         tokens = tokens.repeat_interleave(self.n_group, dim=0).to(audio_features.device)\n \n         # call the main sampling loop\n-        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n+        tokens, sum_logprobs, no_speech_probs, token_probs = self._main_loop(audio_features, tokens)\n \n         # reshape the tensors to have (n_audio, n_group) as the first two dimensions\n         audio_features = audio_features[:: self.n_group]\n@@ -732,7 +744,7 @@ def run(self, mel: Tensor) -> List[DecodingResult]:\n         sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)\n \n         # get the final candidates for each group, and slice between the first sampled token and EOT\n-        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)\n+        tokens, sum_logprobs, token_probs = self.decoder.finalize(tokens, sum_logprobs, token_probs)\n         tokens: List[List[Tensor]] = [\n             [t[self.sample_begin : (t == tokenizer.eot).nonzero()[0, 0]] for t in s]\n             for s in tokens\n@@ -755,6 +767,7 @@ def run(self, mel: Tensor) -> List[DecodingResult]:\n             audio_features,\n             avg_logprobs,\n             no_speech_probs,\n+            token_probs\n         )\n         if len(set(map(len, fields))) != 1:\n             raise RuntimeError(f\"inconsistent result lengths: {list(map(len, fields))}\")\n@@ -769,8 +782,9 @@ def run(self, mel: Tensor) -> List[DecodingResult]:\n                 no_speech_prob=no_speech_prob,\n                 temperature=self.options.temperature,\n                 compression_ratio=compression_ratio(text),\n+                token_probs=token_probs[-len(tokens):]\n             )\n-            for text, language, tokens, features, avg_logprob, no_speech_prob in zip(\n+            for text, language, tokens, features, avg_logprob, no_speech_prob, token_probs in zip(\n                 *fields\n             )\n         ]\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This can be really useful for proofing the output via something like Subtitle Edit.  \r\n\r\nWould really need an command line option to output an additional subtitle though, right?  \r\n\r\nI get the impression @jongwook doesn't want to stuff too many features in though, so how does such a useful feature get added without having a fork?;\nHello!\r\n\r\n> Although the colour terminal stuff might be questionable\r\n\r\nI implemented the per-token confidence as is and implemented the colorful CLI output only in an example.\r\nThe main whisper code does not contain anything with color\r\n\r\n@jongwook is there anything I should modify or change for you to accept the PR? ;\nI'm hesitant to add this because the incremental utility of this compared to the probabilities returned by `word_timestamps=True` is quite niche, versus the added complexity & latency due to the additional GPU operations. The decoding logic is already taking as much as the forward pass, and I'm hoping to reduce this overhead. The subword token probabilities are not very useful anyway, because it's usually influenced more by language modeling than from speech recognition.\r\n\r\nFor the case you need per-token probs, you can add another forward pass without modifying decoding.py (similar to how it's done in [timing.py](https://github.com/openai/whisper/blob/76c901ab8d4558992c44138479c4d69eb52fadcb/whisper/timing.py#L197)) without incurring too much additional latency. It may even be faster than adding GPU operations for every autoregressive step.\r\n\r\nThe example script looks nifty, but i'd prefer it in the [show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) section.;\n@SinanAkkoyun thanks for your contribution. Not sure, but seems it works incorrect, \r\nI made distorted speech example https://drive.google.com/file/d/12zGWllJg6edftcnwuHX_ZHMuwk7PlVjg/view?usp=sharing .\r\nIf I don't set the language of decoding i.e. `options = whisper.DecodingOptions() `,  the output is correct in terms of locating mispronounce (I can read this slavic) though it translates it to random language.\r\n![Screenshot from 2023-08-09 10-58-49](https://github.com/openai/whisper/assets/54935496/cf45eaab-1799-45d0-a386-2fc2e3076b1a)\r\n\r\nBut if I set 'en' for decoding  `options = whisper.DecodingOptions(language=\"en\")` the picture is wrong.\r\n![Screenshot from 2023-08-09 10-58-53](https://github.com/openai/whisper/assets/54935496/8c009a46-ec4d-4a47-abe3-786408580857)\r\n The rest of the code is the same as in your PR except I used \"small\" model.\r\n;\n@Rtut654 Hi, I don't quite understand the issue you are having, the \"I like to play badminton and football.\" seems to be correct, the football especially sounds vague in the audio you provided. Could you please tell me more about your issue?\r\n\r\nDespite that, the PR is not going to get merged, so I stopped working on it and use that modification in my own work which does not include translation\r\n\r\nIf the random translation is the problem you are referring to, I believe that my PR did not modify nor change the output prediction by any means, it just grabbed the logits and displays them as confidence;\n@SinanAkkoyun \r\nThe issue is in the accuracy of token_probs. The first version (with translation to Ukrainian) gives very accurate result since \"like\" was also mispronounced very much. Also the word \"football \" was mispronounced in the last part which is correctly shown in the first picture. \r\n\r\nI did the same test with other audio, setting language of decoding to English.  The picture was same. Somehow it is lowering the prob of the last word even when it is pronounced correctly. At the same time probs of mispronounciations were high which is strange. So something is wrong in the way it predicts probs when language is set to English.;\nIn case it's of interest, I created a small web component to view the Whisper JSON file when `--word_timestamps` has been used. Ideas for improving it would be welcome!\r\n\r\nhttps://edsu.github.io/whisper-transcript/"
      }
    ],
    "token_estimation": 3457.5
  },
  {
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/core/cache/backends/redis.py b/django/core/cache/backends/redis.py\nnew file mode 100644\nindex 000000000000..16556b1ded5c\n--- /dev/null\n+++ b/django/core/cache/backends/redis.py\n@@ -0,0 +1,224 @@\n+\"\"\"Redis cache backend.\"\"\"\n+\n+import random\n+import re\n+\n+from django.core.cache.backends.base import DEFAULT_TIMEOUT, BaseCache\n+from django.core.serializers.base import PickleSerializer\n+from django.utils.functional import cached_property\n+from django.utils.module_loading import import_string\n+\n+\n+class RedisSerializer(PickleSerializer):\n+    def dumps(self, obj):\n+        if isinstance(obj, int):\n+            return obj\n+        return super().dumps(obj)\n+\n+    def loads(self, data):\n+        try:\n+            return int(data)\n+        except ValueError:\n+            return super().loads(data)\n+\n+\n+class RedisCacheClient:\n+    def __init__(\n+        self,\n+        servers,\n+        serializer=None,\n+        db=None,\n+        pool_class=None,\n+        parser_class=None,\n+    ):\n+        import redis\n+\n+        self._lib = redis\n+        self._servers = servers\n+        self._pools = {}\n+\n+        self._client = self._lib.Redis\n+\n+        if isinstance(pool_class, str):\n+            pool_class = import_string(pool_class)\n+        self._pool_class = pool_class or self._lib.ConnectionPool\n+\n+        if isinstance(serializer, str):\n+            serializer = import_string(serializer)\n+        if callable(serializer):\n+            serializer = serializer()\n+        self._serializer = serializer or RedisSerializer()\n+\n+        if isinstance(parser_class, str):\n+            parser_class = import_string(parser_class)\n+        parser_class = parser_class or self._lib.connection.DefaultParser\n+\n+        self._pool_options = {'parser_class': parser_class, 'db': db}\n+\n+    def _get_connection_pool_index(self, write):\n+        # Write to the first server. Read from other servers if there are more,\n+        # otherwise read from the first server.\n+        if write or len(self._servers) == 1:\n+            return 0\n+        return random.randint(1, len(self._servers) - 1)\n+\n+    def _get_connection_pool(self, write):\n+        index = self._get_connection_pool_index(write)\n+        if index not in self._pools:\n+            self._pools[index] = self._pool_class.from_url(\n+                self._servers[index], **self._pool_options,\n+            )\n+        return self._pools[index]\n+\n+    def get_client(self, key=None, *, write=False):\n+        # key is used so that the method signature remains the same and custom\n+        # cache client can be implemented which might require the key to select\n+        # the server, e.g. sharding.\n+        pool = self._get_connection_pool(write)\n+        return self._client(connection_pool=pool)\n+\n+    def add(self, key, value, timeout):\n+        client = self.get_client(key, write=True)\n+        value = self._serializer.dumps(value)\n+\n+        if timeout == 0:\n+            if ret := bool(client.set(key, value, nx=True)):\n+                client.delete(key)\n+            return ret\n+        else:\n+            return bool(client.set(key, value, ex=timeout, nx=True))\n+\n+    def get(self, key, default):\n+        client = self.get_client(key)\n+        value = client.get(key)\n+        return default if value is None else self._serializer.loads(value)\n+\n+    def set(self, key, value, timeout):\n+        client = self.get_client(key, write=True)\n+        value = self._serializer.dumps(value)\n+        if timeout == 0:\n+            client.delete(key)\n+        else:\n+            client.set(key, value, ex=timeout)\n+\n+    def touch(self, key, timeout):\n+        client = self.get_client(key, write=True)\n+        if timeout is None:\n+            return bool(client.persist(key))\n+        else:\n+            return bool(client.expire(key, timeout))\n+\n+    def delete(self, key):\n+        client = self.get_client(key, write=True)\n+        return bool(client.delete(key))\n+\n+    def get_many(self, keys):\n+        client = self.get_client(None)\n+        ret = client.mget(keys)\n+        return {\n+            k: self._serializer.loads(v) for k, v in zip(keys, ret) if v is not None\n+        }\n+\n+    def has_key(self, key):\n+        client = self.get_client(key)\n+        return bool(client.exists(key))\n+\n+    def incr(self, key, delta):\n+        client = self.get_client(key)\n+        if not client.exists(key):\n+            raise ValueError(\"Key '%s' not found.\" % key)\n+        return client.incr(key, delta)\n+\n+    def set_many(self, data, timeout):\n+        client = self.get_client(None, write=True)\n+        pipeline = client.pipeline()\n+        pipeline.mset({k: self._serializer.dumps(v) for k, v in data.items()})\n+\n+        if timeout is not None:\n+            # Setting timeout for each key as redis does not support timeout\n+            # with mset().\n+            for key in data:\n+                pipeline.expire(key, timeout)\n+        pipeline.execute()\n+\n+    def delete_many(self, keys):\n+        client = self.get_client(None, write=True)\n+        client.delete(*keys)\n+\n+    def clear(self):\n+        client = self.get_client(None, write=True)\n+        return bool(client.flushdb())\n+\n+\n+class RedisCache(BaseCache):\n+    def __init__(self, server, params):\n+        super().__init__(params)\n+        if isinstance(server, str):\n+            self._servers = re.split('[;,]', server)\n+        else:\n+            self._servers = server\n+\n+        self._class = RedisCacheClient\n+        self._options = params.get('OPTIONS', {})\n+\n+    @cached_property\n+    def _cache(self):\n+        return self._class(self._servers, **self._options)\n+\n+    def get_backend_timeout(self, timeout=DEFAULT_TIMEOUT):\n+        if timeout == DEFAULT_TIMEOUT:\n+            timeout = self.default_timeout\n+        # The key will be made persistent if None used as a timeout.\n+        # Non-positive values will cause the key to be deleted.\n+        return None if timeout is None else max(0, int(timeout))\n+\n+    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n+\n+    def get(self, key, default=None, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.get(key, default)\n+\n+    def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        self._cache.set(key, value, self.get_backend_timeout(timeout))\n+\n+    def touch(self, key, timeout=DEFAULT_TIMEOUT, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.touch(key, self.get_backend_timeout(timeout))\n+\n+    def delete(self, key, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.delete(key)\n+\n+    def get_many(self, keys, version=None):\n+        key_map = {self.make_and_validate_key(key, version=version): key for key in keys}\n+        ret = self._cache.get_many(key_map.keys())\n+        return {key_map[k]: v for k, v in ret.items()}\n+\n+    def has_key(self, key, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.has_key(key)\n+\n+    def incr(self, key, delta=1, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.incr(key, delta)\n+\n+    def set_many(self, data, timeout=DEFAULT_TIMEOUT, version=None):\n+        safe_data = {}\n+        for key, value in data.items():\n+            key = self.make_and_validate_key(key, version=version)\n+            safe_data[key] = value\n+        self._cache.set_many(safe_data, self.get_backend_timeout(timeout))\n+        return []\n+\n+    def delete_many(self, keys, version=None):\n+        safe_keys = []\n+        for key in keys:\n+            key = self.make_and_validate_key(key, version=version)\n+            safe_keys.append(key)\n+        self._cache.delete_many(safe_keys)\n+\n+    def clear(self):\n+        return self._cache.clear()\ndiff --git a/docs/internals/contributing/writing-code/unit-tests.txt b/docs/internals/contributing/writing-code/unit-tests.txt\nindex 6a5bd5ab8f05..9bba72c451af 100644\n--- a/docs/internals/contributing/writing-code/unit-tests.txt\n+++ b/docs/internals/contributing/writing-code/unit-tests.txt\n@@ -285,6 +285,7 @@ dependencies:\n *  PyYAML_\n *  pytz_ (required)\n *  pywatchman_\n+*  redis_\n *  setuptools_\n *  memcached_, plus a :ref:`supported Python binding <memcached>`\n *  gettext_ (:ref:`gettext_on_windows`)\n@@ -308,8 +309,9 @@ encounter.\n You can also install the database adapter(s) of your choice using\n ``oracle.txt``, ``mysql.txt``, or ``postgres.txt``.\n \n-If you want to test the memcached cache backend, you'll also need to define\n-a :setting:`CACHES` setting that points at your memcached instance.\n+If you want to test the memcached or Redis cache backends, you'll also need to\n+define a :setting:`CACHES` setting that points at your memcached or Redis\n+instance respectively.\n \n To run the GeoDjango tests, you will need to :doc:`set up a spatial database\n and install the Geospatial libraries</ref/contrib/gis/install/index>`.\n@@ -332,6 +334,7 @@ service.\n .. _PyYAML: https://pyyaml.org/wiki/PyYAML\n .. _pytz: https://pypi.org/project/pytz/\n .. _pywatchman: https://pypi.org/project/pywatchman/\n+.. _redis: https://pypi.org/project/redis/\n .. _setuptools: https://pypi.org/project/setuptools/\n .. _memcached: https://memcached.org/\n .. _gettext: https://www.gnu.org/software/gettext/manual/gettext.html\ndiff --git a/docs/ref/settings.txt b/docs/ref/settings.txt\nindex 12d89142d103..0c5b3fe3072d 100644\n--- a/docs/ref/settings.txt\n+++ b/docs/ref/settings.txt\n@@ -153,6 +153,7 @@ The cache backend to use. The built-in cache backends are:\n * ``'django.core.cache.backends.locmem.LocMemCache'``\n * ``'django.core.cache.backends.memcached.PyMemcacheCache'``\n * ``'django.core.cache.backends.memcached.PyLibMCCache'``\n+* ``'django.core.cache.backends.redis.RedisCache'``\n \n You can use a cache backend that doesn't ship with Django by setting\n :setting:`BACKEND <CACHES-BACKEND>` to a fully-qualified path of a cache\n@@ -162,6 +163,10 @@ backend class (i.e. ``mypackage.backends.whatever.WhateverCache``).\n \n     The ``PyMemcacheCache`` backend was added.\n \n+.. versionchanged:: 4.0\n+\n+    The ``RedisCache`` backend was added.\n+\n .. setting:: CACHES-KEY_FUNCTION\n \n ``KEY_FUNCTION``\ndiff --git a/docs/releases/4.0.txt b/docs/releases/4.0.txt\nindex 709363a08f76..540500af4734 100644\n--- a/docs/releases/4.0.txt\n+++ b/docs/releases/4.0.txt\n@@ -65,6 +65,16 @@ The new :ref:`scrypt password hasher <scrypt-usage>` is more secure and\n recommended over PBKDF2. However, it's not the default as it requires OpenSSL\n 1.1+ and more memory.\n \n+Redis cache backend\n+-------------------\n+\n+The new ``django.core.cache.backends.redis.RedisCache`` cache backend provides\n+built-in support for caching with Redis. `redis-py`_ 3.0.0 or higher is\n+required. For more details, see the :ref:`documentation on caching with Redis\n+in Django <redis>`.\n+\n+.. _`redis-py`: https://pypi.org/project/redis/\n+\n Minor features\n --------------\n \ndiff --git a/docs/spelling_wordlist b/docs/spelling_wordlist\nindex bd8785cf67f6..8d9b17a43d1b 100644\n--- a/docs/spelling_wordlist\n+++ b/docs/spelling_wordlist\n@@ -423,6 +423,7 @@ recomputation\n recursed\n redeclare\n redirections\n+redis\n redisplay\n redisplayed\n redisplaying\ndiff --git a/docs/topics/cache.txt b/docs/topics/cache.txt\nindex 0f25260672ce..3bc35fd51d06 100644\n--- a/docs/topics/cache.txt\n+++ b/docs/topics/cache.txt\n@@ -62,7 +62,6 @@ settings file. Here's an explanation of all available values for\n Memcached\n ---------\n \n-The fastest, most efficient type of cache supported natively by Django,\n Memcached__ is an entirely memory-based cache server, originally developed\n to handle high loads at LiveJournal.com and subsequently open-sourced by\n Danga Interactive. It is used by sites such as Facebook and Wikipedia to\n@@ -169,6 +168,71 @@ particularly temporary.\n     some problems and seems to be unmaintained. Use ``PyMemcacheCache`` or\n     ``PyLibMCCache`` instead.\n \n+.. _redis:\n+\n+Redis\n+-----\n+\n+.. versionadded:: 4.0\n+\n+Redis__ is an in-memory database that can be used for caching. To begin you'll\n+need a Redis server running either locally or on a remote machine.\n+\n+__ https://redis.io/\n+\n+After setting up the Redis server, you'll need to install Python bindings for\n+Redis. `redis-py`_ is the binding supported natively by Django. Installing the\n+additional `hiredis-py`_ package is also recommended.\n+\n+.. _`redis-py`: https://pypi.org/project/redis/\n+.. _`hiredis-py`: https://pypi.org/project/hiredis/\n+\n+To use Redis as your cache backend with Django:\n+\n+* Set :setting:`BACKEND <CACHES-BACKEND>` to\n+  ``django.core.cache.backends.redis.RedisCache``.\n+\n+* Set :setting:`LOCATION <CACHES-LOCATION>` to the URL pointing to your Redis\n+  instance, using the appropriate scheme. See the ``redis-py`` docs for\n+  `details on the available schemes\n+  <https://redis-py.readthedocs.io/en/stable/#redis.ConnectionPool.from_url>`_.\n+\n+For example, if Redis is running on localhost (127.0.0.1) port 6379::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': 'redis://127.0.0.1:6379',\n+        }\n+    }\n+\n+Often Redis servers are protected with authentication. In order to supply a\n+username and password, add them in the ``LOCATION`` along with the URL::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': 'redis://username:password@127.0.0.1:6379',\n+        }\n+    }\n+\n+If you have multiple Redis servers set up in the replication mode, you can\n+specify the servers either as a semicolon or comma delimited string, or as a\n+list. While using multiple servers, write operations are performed on the first\n+server (leader). Read operations are performed on the other servers (replicas)\n+chosen at random::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': [\n+                'redis://127.0.0.1:6379', # leader\n+                'redis://127.0.0.1:6378', # read-replica 1\n+                'redis://127.0.0.1:6377', # read-replica 2\n+            ],\n+        }\n+    }\n+\n .. _database-caching:\n \n Database caching\n@@ -422,9 +486,9 @@ behavior. These arguments are provided as additional keys in the\n     On some backends (``database`` in particular) this makes culling *much*\n     faster at the expense of more cache misses.\n \n-  Memcached backends pass the contents of :setting:`OPTIONS <CACHES-OPTIONS>`\n-  as keyword arguments to the client constructors, allowing for more advanced\n-  control of client behavior. For example usage, see below.\n+  The Memcached and Redis backends pass the contents of :setting:`OPTIONS\n+  <CACHES-OPTIONS>` as keyword arguments to the client constructors, allowing\n+  for more advanced control of client behavior. For example usage, see below.\n \n * :setting:`KEY_PREFIX <CACHES-KEY_PREFIX>`: A string that will be\n   automatically included (prepended by default) to all cache keys\n@@ -496,6 +560,27 @@ flag on the connection's socket::\n         }\n     }\n \n+Here's an example configuration for a ``redis`` based backend that selects\n+database ``10`` (by default Redis ships with 16 logical databases), specifies a\n+`parser class`_ (``redis.connection.HiredisParser`` will be used by default if\n+the ``hiredis-py`` package is installed), and sets a custom `connection pool\n+class`_ (``redis.ConnectionPool`` is used by default)::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': 'redis://127.0.0.1:6379',\n+            'OPTIONS': {\n+                'db': '10',\n+                'parser_class': 'redis.connection.PythonParser',\n+                'pool_class': 'redis.BlockingConnectionPool',\n+            }\n+        }\n+    }\n+\n+.. _`parser class`: https://github.com/andymccurdy/redis-py#parsers\n+.. _`connection pool class`: https://github.com/andymccurdy/redis-py#connection-pools\n+\n .. _the-per-site-cache:\n \n The per-site cache\ndiff --git a/tests/cache/tests.py b/tests/cache/tests.py\nindex 0e2f5f7d1f04..b88066285801 100644\n--- a/tests/cache/tests.py\n+++ b/tests/cache/tests.py\n@@ -22,6 +22,7 @@\n     caches,\n )\n from django.core.cache.backends.base import InvalidCacheBackendError\n+from django.core.cache.backends.redis import RedisCacheClient\n from django.core.cache.utils import make_template_fragment_key\n from django.db import close_old_connections, connection, connections\n from django.db.backends.utils import CursorWrapper\n@@ -1373,10 +1374,9 @@ def test_lru_incr(self):\n         self.assertEqual(cache.get(9), 9)\n \n \n-# memcached backend isn't guaranteed to be available.\n-# To check the memcached backend, the test settings file will\n-# need to contain at least one cache backend setting that points at\n-# your memcache server.\n+# memcached and redis backends aren't guaranteed to be available.\n+# To check the backends, the test settings file will need to contain at least\n+# one cache backend setting that points at your cache server.\n configured_caches = {}\n for _cache_params in settings.CACHES.values():\n     configured_caches[_cache_params['BACKEND']] = _cache_params\n@@ -1387,6 +1387,11 @@ def test_lru_incr(self):\n # The memcached backends don't support cull-related options like `MAX_ENTRIES`.\n memcached_excluded_caches = {'cull', 'zero_cull'}\n \n+RedisCache_params = configured_caches.get('django.core.cache.backends.redis.RedisCache')\n+\n+# The redis backend does not support cull-related options like `MAX_ENTRIES`.\n+redis_excluded_caches = {'cull', 'zero_cull'}\n+\n \n class BaseMemcachedTests(BaseCacheTests):\n \n@@ -1727,6 +1732,60 @@ def test_empty_cache_file_considered_expired(self):\n             self.assertIs(cache._is_expired(fh), True)\n \n \n+@unittest.skipUnless(RedisCache_params, \"Redis backend not configured\")\n+@override_settings(CACHES=caches_setting_for_tests(\n+    base=RedisCache_params,\n+    exclude=redis_excluded_caches,\n+))\n+class RedisCacheTests(BaseCacheTests, TestCase):\n+\n+    def setUp(self):\n+        import redis\n+        super().setUp()\n+        self.lib = redis\n+\n+    @property\n+    def incr_decr_type_error(self):\n+        return self.lib.ResponseError\n+\n+    def test_cache_client_class(self):\n+        self.assertIs(cache._class, RedisCacheClient)\n+        self.assertIsInstance(cache._cache, RedisCacheClient)\n+\n+    def test_get_backend_timeout_method(self):\n+        positive_timeout = 10\n+        positive_backend_timeout = cache.get_backend_timeout(positive_timeout)\n+        self.assertEqual(positive_backend_timeout, positive_timeout)\n+\n+        negative_timeout = -5\n+        negative_backend_timeout = cache.get_backend_timeout(negative_timeout)\n+        self.assertEqual(negative_backend_timeout, 0)\n+\n+        none_timeout = None\n+        none_backend_timeout = cache.get_backend_timeout(none_timeout)\n+        self.assertIsNone(none_backend_timeout)\n+\n+    def test_get_connection_pool_index(self):\n+        pool_index = cache._cache._get_connection_pool_index(write=True)\n+        self.assertEqual(pool_index, 0)\n+        pool_index = cache._cache._get_connection_pool_index(write=False)\n+        if len(cache._cache._servers) == 1:\n+            self.assertEqual(pool_index, 0)\n+        else:\n+            self.assertGreater(pool_index, 0)\n+            self.assertLess(pool_index, len(cache._cache._servers))\n+\n+    def test_get_connection_pool(self):\n+        pool = cache._cache._get_connection_pool(write=True)\n+        self.assertIsInstance(pool, self.lib.ConnectionPool)\n+\n+        pool = cache._cache._get_connection_pool(write=False)\n+        self.assertIsInstance(pool, self.lib.ConnectionPool)\n+\n+    def test_get_client(self):\n+        self.assertIsInstance(cache._cache.get_client(), self.lib.Redis)\n+\n+\n class FileBasedCachePathLibTests(FileBasedCacheTests):\n     def mkdtemp(self):\n         tmp_dir = super().mkdtemp()\ndiff --git a/tests/requirements/py3.txt b/tests/requirements/py3.txt\nindex af06c4e5f620..893e47a914a7 100644\n--- a/tests/requirements/py3.txt\n+++ b/tests/requirements/py3.txt\n@@ -15,6 +15,7 @@ python-memcached >= 1.59\n pytz\n pywatchman; sys.platform != 'win32'\n PyYAML\n+redis >= 3.0.0\n selenium\n sqlparse >= 0.2.2\n tblib >= 1.5.0\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Hey @carltongibson! \r\nSo I was going trying to work on the coverages of each backend. However, one tests keeps failing. \r\n`cache.tests.CreateCacheTableForDBCacheTests.test_createcachetable_observes_database_router`\r\n```\r\n======================================================================\r\nFAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n    return func(*args, **kwargs)\r\n  File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n    management.call_command('createcachetable', database='other', verbosity=0)\r\n  File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n    self.test_case.assertEqual(\r\nAssertionError: 1 != 5 : 1 queries executed, 5 expected\r\nCaptured queries were:\r\n1. \r\n            SELECT c.relname,\r\n            CASE WHEN c.relispartition THEN 'p' WHEN c.relkind IN ('m', 'v') THEN 'v' ELSE 't' END\r\n            FROM pg_catalog.pg_class c\r\n            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\r\n            WHERE c.relkind IN ('f', 'm', 'p', 'r', 'v')\r\n                AND n.nspname NOT IN ('pg_catalog', 'pg_toast')\r\n                AND pg_catalog.pg_table_is_visible(c.oid)\r\n        \r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nI did not completely understand the reason for this. \r\nMy test settings were\r\n```\r\nDATABASES = {\r\n    'default': {\r\n        'ENGINE': 'django.db.backends.postgresql',\r\n        'NAME': 'mydb_default',\r\n        'USER': 'myuser',\r\n        'PASSWORD': 'password',\r\n        'HOST': 'localhost',\r\n        'PORT': '5432',\r\n    },\r\n    'other': {\r\n        'ENGINE': 'django.db.backends.postgresql',\r\n        'NAME': 'mydb_other',\r\n        'USER': 'myuser',\r\n        'PASSWORD': 'password',\r\n        'HOST': 'localhost',\r\n        'PORT': '5432',\r\n    }\r\n}\r\n\r\nCACHES = {\r\n    \"default\": {\r\n        \"BACKEND\": \"django.core.cache.backends.db.DatabaseCache\",\r\n        \"LOCATION\": \"my_cache_table\",\r\n    },\r\n}\r\n```\r\nTried and tested with SQLite as well, and got the same results. \r\nError with SQLite\r\n```\r\n======================================================================\r\nFAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n    return func(*args, **kwargs)\r\n  File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n    management.call_command('createcachetable', database='other', verbosity=0)\r\n  File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n    self.test_case.assertEqual(\r\nAssertionError: 1 != 5 : 1 queries executed, 5 expected\r\nCaptured queries were:\r\n1. \r\n            SELECT name, type FROM sqlite_master\r\n            WHERE type in ('table', 'view') AND NOT name='sqlite_sequence'\r\n            ORDER BY name\r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nHowever, when I comment out this line, which call `createcachetable`, the test passes.\r\nhttps://github.com/django/django/blob/d270dd584e0af12fe6229fb712d0704c232dc7e5/django/db/backends/base/creation.py#L92\r\n;\nHey @carltongibson !\r\nI've just pushed the lastest update that I have. I've adapted the existsing tests for the new backend. The tests are failing at two instance\r\n- Culling\r\n- zero and negative timeout handling : Redis-py does not support 0 or negative timeouts. I have implemented the `get_backend_timeout` similar to the memcache backend but I'm still not sure about how to handle 0 timeout. Ideally it should not store the key in the first place.;\n> Hey @carltongibson!\r\n> So I was going trying to work on the coverages of each backend. However, one tests keeps failing.\r\n> `cache.tests.CreateCacheTableForDBCacheTests.test_createcachetable_observes_database_router`\r\n> \r\n> ```\r\n> ======================================================================\r\n> FAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n> ----------------------------------------------------------------------\r\n> Traceback (most recent call last):\r\n>   File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n>     return func(*args, **kwargs)\r\n>   File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n>     management.call_command('createcachetable', database='other', verbosity=0)\r\n>   File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n>     self.test_case.assertEqual(\r\n> AssertionError: 1 != 5 : 1 queries executed, 5 expected\r\n> Captured queries were:\r\n> 1. \r\n>             SELECT c.relname,\r\n>             CASE WHEN c.relispartition THEN 'p' WHEN c.relkind IN ('m', 'v') THEN 'v' ELSE 't' END\r\n>             FROM pg_catalog.pg_class c\r\n>             LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\r\n>             WHERE c.relkind IN ('f', 'm', 'p', 'r', 'v')\r\n>                 AND n.nspname NOT IN ('pg_catalog', 'pg_toast')\r\n>                 AND pg_catalog.pg_table_is_visible(c.oid)\r\n>         \r\n> \r\n> ----------------------------------------------------------------------\r\n> ```\r\n> \r\n> I did not completely understand the reason for this.\r\n> My test settings were\r\n> \r\n> ```\r\n> DATABASES = {\r\n>     'default': {\r\n>         'ENGINE': 'django.db.backends.postgresql',\r\n>         'NAME': 'mydb_default',\r\n>         'USER': 'myuser',\r\n>         'PASSWORD': 'password',\r\n>         'HOST': 'localhost',\r\n>         'PORT': '5432',\r\n>     },\r\n>     'other': {\r\n>         'ENGINE': 'django.db.backends.postgresql',\r\n>         'NAME': 'mydb_other',\r\n>         'USER': 'myuser',\r\n>         'PASSWORD': 'password',\r\n>         'HOST': 'localhost',\r\n>         'PORT': '5432',\r\n>     }\r\n> }\r\n> \r\n> CACHES = {\r\n>     \"default\": {\r\n>         \"BACKEND\": \"django.core.cache.backends.db.DatabaseCache\",\r\n>         \"LOCATION\": \"my_cache_table\",\r\n>     },\r\n> }\r\n> ```\r\n> \r\n> Tried and tested with SQLite as well, and got the same results.\r\n> Error with SQLite\r\n> \r\n> ```\r\n> ======================================================================\r\n> FAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n> ----------------------------------------------------------------------\r\n> Traceback (most recent call last):\r\n>   File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n>     return func(*args, **kwargs)\r\n>   File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n>     management.call_command('createcachetable', database='other', verbosity=0)\r\n>   File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n>     self.test_case.assertEqual(\r\n> AssertionError: 1 != 5 : 1 queries executed, 5 expected\r\n> Captured queries were:\r\n> 1. \r\n>             SELECT name, type FROM sqlite_master\r\n>             WHERE type in ('table', 'view') AND NOT name='sqlite_sequence'\r\n>             ORDER BY name\r\n> \r\n> ----------------------------------------------------------------------\r\n> ```\r\n> \r\n> However, when I comment out this line, which call `createcachetable`, the test passes.\r\n> https://github.com/django/django/blob/d270dd584e0af12fe6229fb712d0704c232dc7e5/django/db/backends/base/creation.py#L92\r\n\r\n@carltongibson I was able to figure this one out. I was following the [documentation](https://docs.djangoproject.com/en/3.2/topics/cache/#database-caching) to setup the DatabaseCache.\r\n```\r\nCACHES = {\r\n    \"default\": {\r\n        \"BACKEND\": \"django.core.cache.backends.db.DatabaseCache\",\r\n        \"LOCATION\": \"my_cache_table\",\r\n    },\r\n}\r\n```\r\nHowever, I believe `my_cache_table` was conflicting with this\r\nhttps://github.com/django/django/blob/225d96533a8e05debd402a2bfe566487cc27d95f/tests/cache/tests.py#L1213-L1220\r\n\r\nSetting the \"LOCATION\" to some other table name leads to the test passing. \r\n\r\nMaybe we could mention it in the docs somewhere or update the test to check if duplicate table names exists? This is a little off-topic from this PR. Should I create a separate ticket for this? Or should we let it be for now?\r\n;\nHey!\r\nI've made the changes suggested by @pope1ni. I've updated the tests and added\r\n```\r\nredis_excluded_caches = {'cull', 'zero_cull'}\r\n...\r\n@override_settings(CACHES=caches_setting_for_tests(\r\n    base=RedisCache_params,\r\n    exclude=redis_excluded_caches\r\n))\r\nclass RedisCacheTests(BaseCacheTests, TestCase):\r\n    ...\r\n```\r\nNow only on test fails. Handling zero timeout. Redis-py does not support it natively and django expects to no set a key with zero timeout. I'm not sure at which level should this be handled. I was wondering if we perform the check in `get_backend_timeout` method and return a value (eg: None) or raise a suitable exception. \r\n;\n> If `redis-py` isn't installed, or if Redis isn't running we get a couple of errors:\r\n> \r\n> ```\r\n> django.core.cache.backends.base.InvalidCacheBackendError: Could not find backend 'django.core.cache.backends.redis.RedisCache': No module named 'redis'\r\n> ```\r\n> \r\n\r\nShould I move the `import redis` line inside the `__init__` method of the RedisCache class? All memcache backend do that and the error raised when a binding is not installed is like this\r\n```\r\nModuleNotFoundError: No module named 'pymemcache'\r\n```\r\nIncluding `import redis` at the top leads to an error message as you mentioned above.\r\nI wanted to move the import command like this\r\n```\r\nclass RedisCache(BaseCache):\r\n    def __init__(self, server, params):\r\n        import redis\r\n        super().__init__(params)\r\n        if isinstance(server, str):\r\n            self._servers = re.split('[;,]', server)\r\n        else:\r\n            self._servers = server\r\n\r\n        self._class = RedisCacheClient\r\n        self._options = params.get('OPTIONS') or {}\r\n```\r\nHowever, this would lead to some refactoring of code where `redis.ConnectionPool` etc are used.\r\nWhat do you think about this?\r\n;\n> Should I move the `import redis` line inside the `__init__` method of the RedisCache class? All memcache backend do that and the error raised when a binding is not installed is like this\r\n\r\nYes, we should do something like this. Although with `pymemcache` it is done in `PyMemcacheCache.__init__()` because `pymemcache` provides the client class. We're writing the client class ourselves, so we want `import redis` in `RedisCacheClient.__init__()`.;\nHey @pope1ni and @carltongibson \r\n\r\nAccording to the comments https://github.com/django/django/pull/14437#pullrequestreview-691776531, if we want to use the `redis.incr` or `redis.decr`, we would need to stop serializing the valus which are integers. I think this will create a mess as there would be too much to manual handling of values based on their types. As we are pickling the values, we can not directly use the `redis.incr` or `redis.decr` methods.\r\n\r\nI'm not sure how useful it is to support milli-second timeouts. `django-redis` has migrated to an approach which supports both seconds and milliseconds ( [refs](https://github.com/jazzband/django-redis/pull/508/files) ). \r\n\r\nWe can make the logical databases configurable via the url as well as a parameter in the options. I'll work on it.\r\n\r\n;\nWhile using the `from_url` method, we can not provide `username` and `password` in the `OPTIONS`. Even if we pass them in the kwargs, [kwargs.update(...)](https://redis-py.readthedocs.io/en/stable/_modules/redis/connection.html#ConnectionPool.from_url) overrides it with the `username` and `password` from the URL, else it sets it to None. One solution is that we only allow username and password to be set using the \"LOCATION\" key only. Let me know what you feel. @pope1ni @carltongibson ;\nThe master branch of redis-py has updated the implementation of the `from_url` method. \r\nhttps://github.com/andymccurdy/redis-py/blob/627db540acd1f1f36db88290d74cbcd75f6bda0c/redis/connection.py#L951-L955\r\n\r\nHowever, the latest stable branch (3.5.3) still uses the old implementation.\r\n```\r\nif decode_components:\r\n    username = unquote(url.username) if url.username else None\r\n    password = unquote(url.password) if url.password else None\r\n    path = unquote(url.path) if url.path else None\r\n    hostname = unquote(url.hostname) if url.hostname else None\r\nelse:\r\n    username = url.username or None\r\n    password = url.password or None\r\n    path = url.path\r\n    hostname = url.hostname\r\n\r\nif url.scheme == 'unix':\r\n    url_options.update({\r\n                'username': username,\r\n                'password': password,\r\n                'path': path,\r\n                'connection_class': UnixDomainSocketConnection,    \r\n})\r\n\r\nelif url.scheme in ('redis', 'rediss'):\r\n    url_options.update({\r\n                'host': hostname,\r\n                'port': int(url.port or 6379),\r\n                'username': username,\r\n                'password': password,\r\n            })\r\n...\r\nkwargs.update(url_options)\r\n```\r\nThis will always override the username and password in the kwargs. I think for now, we can only support giving the `username` and `password` via the url and once redis-py's latest implementation is stable, we can add supplying  `username` and `password` via the `OPTIONS`.\r\n;\nHey @smithdc1! Thank you for the review.\r\n\r\n\r\n> I'm not so confident to comment on the main part with all the different options. I think my main observation there is about the structure, I find it a little hard to follow. There's many options here but here's one idea of how it could be structured.\r\n> \r\n> * Required settings\r\n> * settings which are available to all backends\r\n> * Redis specific items (optional)\r\n\r\nYes we could do that but that'll require restructing the whole documentation as there are some memcached specific items that are currently laid out in the examples section. In the future, I think such a restructuring would be nice where we can talk about cache specific arguements in a separate section.\r\n"
      }
    ],
    "token_estimation": 8736.75
  }
]