[
  {
    "index": 1,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/autogpt/app.py b/autogpt/app.py\nindex 58d9f7164ddf..cf8e29a36dd3 100644\n--- a/autogpt/app.py\n+++ b/autogpt/app.py\n@@ -119,105 +103,29 @@ def execute_command(command_name: str, arguments):\n         str: The result of the command\n     \"\"\"\n     try:\n+        cmd = command_registry.commands.get(command_name)\n+\n+        # If the command is found, call it with the provided arguments\n+        if cmd:\n+            return cmd(**arguments)\n+\n+        # TODO: Remove commands below after they are moved to the command registry.\n         command_name = map_command_synonyms(command_name.lower())\n-        if command_name == \"google\":\n-            # Check if the Google API key is set and use the official search method\n-            # If the API key is not set or has only whitespaces, use the unofficial\n-            # search method\n-            key = CFG.google_api_key\n-            if key and key.strip() and key != \"your-google-api-key\":\n-                google_result = google_official_search(arguments[\"input\"])\n-                return google_result\n-            else:\n-                google_result = google_search(arguments[\"input\"])\n-\n-            # google_result can be a list or a string depending on the search results\n-            if isinstance(google_result, list):\n-                safe_message = [\n-                    google_result_single.encode(\"utf-8\", \"ignore\")\n-                    for google_result_single in google_result\n-                ]\n-            else:\n-                safe_message = google_result.encode(\"utf-8\", \"ignore\")\n-\n-            return safe_message.decode(\"utf-8\")\n-        elif command_name == \"memory_add\":\n-            memory = get_memory(CFG)\n-            return memory.add(arguments[\"string\"])\n-        elif command_name == \"start_agent\":\n-            return start_agent(\n-                arguments[\"name\"], arguments[\"task\"], arguments[\"prompt\"]\n-            )\n-        elif command_name == \"message_agent\":\n-            return message_agent(arguments[\"key\"], arguments[\"message\"])\n-        elif command_name == \"list_agents\":\n-            return list_agents()\n-        elif command_name == \"delete_agent\":\n-            return delete_agent(arguments[\"key\"])\n-        elif command_name == \"get_text_summary\":\n-            return get_text_summary(arguments[\"url\"], arguments[\"question\"])\n-        elif command_name == \"get_hyperlinks\":\n-            return get_hyperlinks(arguments[\"url\"])\n-        elif command_name == \"clone_repository\":\n-            return clone_repository(\n-                arguments[\"repository_url\"], arguments[\"clone_path\"]\n-            )\n-        elif command_name == \"read_file\":\n-            return read_file(arguments[\"file\"])\n-        elif command_name == \"write_to_file\":\n-            return write_to_file(arguments[\"file\"], arguments[\"text\"])\n-        elif command_name == \"append_to_file\":\n-            return append_to_file(arguments[\"file\"], arguments[\"text\"])\n-        elif command_name == \"delete_file\":\n-            return delete_file(arguments[\"file\"])\n-        elif command_name == \"search_files\":\n-            return search_files(arguments[\"directory\"])\n-        elif command_name == \"download_file\":\n-            if not CFG.allow_downloads:\n-                return \"Error: You do not have user authorization to download files locally.\"\n-            return download_file(arguments[\"url\"], arguments[\"file\"])\n-        elif command_name == \"browse_website\":\n-            return browse_website(arguments[\"url\"], arguments[\"question\"])\n+\n+        if command_name == \"memory_add\":\n+            return get_memory(CFG).add(arguments[\"string\"])\n+\n         # TODO: Change these to take in a file rather than pasted code, if\n         # non-file is given, return instructions \"Input should be a python\n-        # filepath, write your code to file and try again\"\n-        elif command_name == \"analyze_code\":\n-            return analyze_code(arguments[\"code\"])\n-        elif command_name == \"improve_code\":\n-            return improve_code(arguments[\"suggestions\"], arguments[\"code\"])\n-        elif command_name == \"write_tests\":\n-            return write_tests(arguments[\"code\"], arguments.get(\"focus\"))\n-        elif command_name == \"execute_python_file\":  # Add this command\n-            return execute_python_file(arguments[\"file\"])\n-        elif command_name == \"execute_shell\":\n-            if CFG.execute_local_commands:\n-                return execute_shell(arguments[\"command_line\"])\n-            else:\n-                return (\n-                    \"You are not allowed to run local shell commands. To execute\"\n-                    \" shell commands, EXECUTE_LOCAL_COMMANDS must be set to 'True' \"\n-                    \"in your config. Do not attempt to bypass the restriction.\"\n-                )\n-        elif command_name == \"execute_shell_popen\":\n-            if CFG.execute_local_commands:\n-                return execute_shell_popen(arguments[\"command_line\"])\n-            else:\n-                return (\n-                    \"You are not allowed to run local shell commands. To execute\"\n-                    \" shell commands, EXECUTE_LOCAL_COMMANDS must be set to 'True' \"\n-                    \"in your config. Do not attempt to bypass the restriction.\"\n-                )\n-        elif command_name == \"read_audio_from_file\":\n-            return read_audio_from_file(arguments[\"file\"])\n-        elif command_name == \"generate_image\":\n-            return generate_image(arguments[\"prompt\"])\n-        elif command_name == \"send_tweet\":\n-            return send_tweet(arguments[\"text\"])\n+        # filepath, write your code to file and try again\n         elif command_name == \"do_nothing\":\n             return \"No action performed.\"\n         elif command_name == \"task_complete\":\n             shutdown()\n         else:\n+            for command in prompt.commands:\n+                if command_name == command[\"label\"] or command_name == command[\"name\"]:\n+                    return command[\"function\"](*arguments.values())\n             return (\n                 f\"Unknown command '{command_name}'. Please refer to the 'COMMANDS'\"\n                 \" list for available commands and only respond in the specified JSON\"\n\ndiff --git a/autogpt/commands/command.py b/autogpt/commands/command.py\nnew file mode 100644\nindex 000000000000..e97af008a440\n--- /dev/null\n+++ b/autogpt/commands/command.py\n@@ -0,0 +1,153 @@\n+import importlib\n+import inspect\n+from typing import Any, Callable, Optional\n+\n+# Unique identifier for auto-gpt commands\n+AUTO_GPT_COMMAND_IDENTIFIER = \"auto_gpt_command\"\n+\n+\n+class Command:\n+    \"\"\"A class representing a command.\n+\n+    Attributes:\n+        name (str): The name of the command.\n+        description (str): A brief description of what the command does.\n+        signature (str): The signature of the function that the command executes. Defaults to None.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        description: str,\n+        method: Callable[..., Any],\n+        signature: str = \"\",\n+        enabled: bool = True,\n+        disabled_reason: Optional[str] = None,\n+    ):\n+        self.name = name\n+        self.description = description\n+        self.method = method\n+        self.signature = signature if signature else str(inspect.signature(self.method))\n+        self.enabled = enabled\n+        self.disabled_reason = disabled_reason\n+\n+    def __call__(self, *args, **kwargs) -> Any:\n+        if not self.enabled:\n+            return f\"Command '{self.name}' is disabled: {self.disabled_reason}\"\n+        return self.method(*args, **kwargs)\n+\n+    def __str__(self) -> str:\n+        return f\"{self.name}: {self.description}, args: {self.signature}\"\n+\n+\n+class CommandRegistry:\n+    \"\"\"\n+    The CommandRegistry class is a manager for a collection of Command objects.\n+    It allows the registration, modification, and retrieval of Command objects,\n+    as well as the scanning and loading of command plugins from a specified\n+    directory.\n+    \"\"\"\n+\n+    def __init__(self):\n+        self.commands = {}\n+\n+    def _import_module(self, module_name: str) -> Any:\n+        return importlib.import_module(module_name)\n+\n+    def _reload_module(self, module: Any) -> Any:\n+        return importlib.reload(module)\n+\n+    def register(self, cmd: Command) -> None:\n+        self.commands[cmd.name] = cmd\n+\n+    def unregister(self, command_name: str):\n+        if command_name in self.commands:\n+            del self.commands[command_name]\n+        else:\n+            raise KeyError(f\"Command '{command_name}' not found in registry.\")\n+\n+    def reload_commands(self) -> None:\n+        \"\"\"Reloads all loaded command plugins.\"\"\"\n+        for cmd_name in self.commands:\n+            cmd = self.commands[cmd_name]\n+            module = self._import_module(cmd.__module__)\n+            reloaded_module = self._reload_module(module)\n+            if hasattr(reloaded_module, \"register\"):\n+                reloaded_module.register(self)\n+\n+    def get_command(self, name: str) -> Callable[..., Any]:\n+        return self.commands[name]\n+\n+    def call(self, command_name: str, **kwargs) -> Any:\n+        if command_name not in self.commands:\n+            raise KeyError(f\"Command '{command_name}' not found in registry.\")\n+        command = self.commands[command_name]\n+        return command(**kwargs)\n+\n+    def command_prompt(self) -> str:\n+        \"\"\"\n+        Returns a string representation of all registered `Command` objects for use in a prompt\n+        \"\"\"\n+        commands_list = [\n+            f\"{idx + 1}. {str(cmd)}\" for idx, cmd in enumerate(self.commands.values())\n+        ]\n+        return \"\\n\".join(commands_list)\n+\n+    def import_commands(self, module_name: str) -> None:\n+        \"\"\"\n+        Imports the specified Python module containing command plugins.\n+\n+        This method imports the associated module and registers any functions or\n+        classes that are decorated with the `AUTO_GPT_COMMAND_IDENTIFIER` attribute\n+        as `Command` objects. The registered `Command` objects are then added to the\n+        `commands` dictionary of the `CommandRegistry` object.\n+\n+        Args:\n+            module_name (str): The name of the module to import for command plugins.\n+        \"\"\"\n+\n+        module = importlib.import_module(module_name)\n+\n+        for attr_name in dir(module):\n+            attr = getattr(module, attr_name)\n+            # Register decorated functions\n+            if hasattr(attr, AUTO_GPT_COMMAND_IDENTIFIER) and getattr(\n+                attr, AUTO_GPT_COMMAND_IDENTIFIER\n+            ):\n+                self.register(attr.command)\n+            # Register command classes\n+            elif (\n+                inspect.isclass(attr) and issubclass(attr, Command) and attr != Command\n+            ):\n+                cmd_instance = attr()\n+                self.register(cmd_instance)\n+\n+\n+def command(\n+    name: str,\n+    description: str,\n+    signature: str = \"\",\n+    enabled: bool = True,\n+    disabled_reason: Optional[str] = None,\n+) -> Callable[..., Any]:\n+    \"\"\"The command decorator is used to create Command objects from ordinary functions.\"\"\"\n+\n+    def decorator(func: Callable[..., Any]) -> Command:\n+        cmd = Command(\n+            name=name,\n+            description=description,\n+            method=func,\n+            signature=signature,\n+            enabled=enabled,\n+            disabled_reason=disabled_reason,\n+        )\n+\n+        def wrapper(*args, **kwargs) -> Any:\n+            return func(*args, **kwargs)\n+\n+        wrapper.command = cmd\n+\n+        setattr(wrapper, AUTO_GPT_COMMAND_IDENTIFIER, True)\n+        return wrapper\n+\n+    return decorator\ndiff --git a/autogpt/models/base_open_ai_plugin.py b/autogpt/models/base_open_ai_plugin.py\nnew file mode 100644\nindex 000000000000..046295c0dcbe\n--- /dev/null\n+++ b/autogpt/models/base_open_ai_plugin.py\n@@ -0,0 +1,199 @@\n+\"\"\"Handles loading of plugins.\"\"\"\n+from typing import Any, Dict, List, Optional, Tuple, TypedDict, TypeVar\n+\n+from auto_gpt_plugin_template import AutoGPTPluginTemplate\n+\n+PromptGenerator = TypeVar(\"PromptGenerator\")\n+\n+\n+class Message(TypedDict):\n+    role: str\n+    content: str\n+\n+\n+class BaseOpenAIPlugin(AutoGPTPluginTemplate):\n+    \"\"\"\n+    This is a BaseOpenAIPlugin class for generating Auto-GPT plugins.\n+    \"\"\"\n+\n+    def __init__(self, manifests_specs_clients: dict):\n+        # super().__init__()\n+        self._name = manifests_specs_clients[\"manifest\"][\"name_for_model\"]\n+        self._version = manifests_specs_clients[\"manifest\"][\"schema_version\"]\n+        self._description = manifests_specs_clients[\"manifest\"][\"description_for_model\"]\n+        self._client = manifests_specs_clients[\"client\"]\n+        self._manifest = manifests_specs_clients[\"manifest\"]\n+        self._openapi_spec = manifests_specs_clients[\"openapi_spec\"]\n+\n+    def can_handle_on_response(self) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+        handle the on_response method.\n+        Returns:\n+            bool: True if the plugin can handle the on_response method.\"\"\"\n+        return False\n+\n+    def on_response(self, response: str, *args, **kwargs) -> str:\n+        \"\"\"This method is called when a response is received from the model.\"\"\"\n+        return response\n+\n+    def can_handle_post_prompt(self) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+        handle the post_prompt method.\n+        Returns:\n+            bool: True if the plugin can handle the post_prompt method.\"\"\"\n+        return False\n+\n+    def post_prompt(self, prompt: PromptGenerator) -> PromptGenerator:\n+        \"\"\"This method is called just after the generate_prompt is called,\n+            but actually before the prompt is generated.\n+        Args:\n+            prompt (PromptGenerator): The prompt generator.\n+        Returns:\n+            PromptGenerator: The prompt generator.\n+        \"\"\"\n+        return prompt\n+\n+    def can_handle_on_planning(self) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+        handle the on_planning method.\n+        Returns:\n+            bool: True if the plugin can handle the on_planning method.\"\"\"\n+        return False\n+\n+    def on_planning(\n+        self, prompt: PromptGenerator, messages: List[Message]\n+    ) -> Optional[str]:\n+        \"\"\"This method is called before the planning chat completion is done.\n+        Args:\n+            prompt (PromptGenerator): The prompt generator.\n+            messages (List[str]): The list of messages.\n+        \"\"\"\n+        pass\n+\n+    def can_handle_post_planning(self) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+        handle the post_planning method.\n+        Returns:\n+            bool: True if the plugin can handle the post_planning method.\"\"\"\n+        return False\n+\n+    def post_planning(self, response: str) -> str:\n+        \"\"\"This method is called after the planning chat completion is done.\n+        Args:\n+            response (str): The response.\n+        Returns:\n+            str: The resulting response.\n+        \"\"\"\n+        return response\n+\n+    def can_handle_pre_instruction(self) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+        handle the pre_instruction method.\n+        Returns:\n+            bool: True if the plugin can handle the pre_instruction method.\"\"\"\n+        return False\n+\n+    def pre_instruction(self, messages: List[Message]) -> List[Message]:\n+        \"\"\"This method is called before the instruction chat is done.\n+        Args:\n+            messages (List[Message]): The list of context messages.\n+        Returns:\n+            List[Message]: The resulting list of messages.\n+        \"\"\"\n+        return messages\n+\n+    def can_handle_on_instruction(self) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+        handle the on_instruction method.\n+        Returns:\n+            bool: True if the plugin can handle the on_instruction method.\"\"\"\n+        return False\n+\n+    def on_instruction(self, messages: List[Message]) -> Optional[str]:\n+        \"\"\"This method is called when the instruction chat is done.\n+        Args:\n+            messages (List[Message]): The list of context messages.\n+        Returns:\n+            Optional[str]: The resulting message.\n+        \"\"\"\n+        pass\n+\n+    def can_handle_post_instruction(self) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+        handle the post_instruction method.\n+        Returns:\n+            bool: True if the plugin can handle the post_instruction method.\"\"\"\n+        return False\n+\n+    def post_instruction(self, response: str) -> str:\n+        \"\"\"This method is called after the instruction chat is done.\n+        Args:\n+            response (str): The response.\n+        Returns:\n+            str: The resulting response.\n+        \"\"\"\n+        return response\n+\n+    def can_handle_pre_command(self) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+        handle the pre_command method.\n+        Returns:\n+            bool: True if the plugin can handle the pre_command method.\"\"\"\n+        return False\n+\n+    def pre_command(\n+        self, command_name: str, arguments: Dict[str, Any]\n+    ) -> Tuple[str, Dict[str, Any]]:\n+        \"\"\"This method is called before the command is executed.\n+        Args:\n+            command_name (str): The command name.\n+            arguments (Dict[str, Any]): The arguments.\n+        Returns:\n+            Tuple[str, Dict[str, Any]]: The command name and the arguments.\n+        \"\"\"\n+        return command_name, arguments\n+\n+    def can_handle_post_command(self) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+        handle the post_command method.\n+        Returns:\n+            bool: True if the plugin can handle the post_command method.\"\"\"\n+        return False\n+\n+    def post_command(self, command_name: str, response: str) -> str:\n+        \"\"\"This method is called after the command is executed.\n+        Args:\n+            command_name (str): The command name.\n+            response (str): The response.\n+        Returns:\n+            str: The resulting response.\n+        \"\"\"\n+        return response\n+\n+    def can_handle_chat_completion(\n+        self, messages: Dict[Any, Any], model: str, temperature: float, max_tokens: int\n+    ) -> bool:\n+        \"\"\"This method is called to check that the plugin can\n+          handle the chat_completion method.\n+        Args:\n+            messages (List[Message]): The messages.\n+            model (str): The model name.\n+            temperature (float): The temperature.\n+            max_tokens (int): The max tokens.\n+          Returns:\n+              bool: True if the plugin can handle the chat_completion method.\"\"\"\n+        return False\n+\n+    def handle_chat_completion(\n+        self, messages: List[Message], model: str, temperature: float, max_tokens: int\n+    ) -> str:\n+        \"\"\"This method is called when the chat completion is done.\n+        Args:\n+            messages (List[Message]): The messages.\n+            model (str): The model name.\n+            temperature (float): The temperature.\n+            max_tokens (int): The max tokens.\n+        Returns:\n+            str: The resulting response.\n+        \"\"\"\n+        pass\ndiff --git a/autogpt/plugins.py b/autogpt/plugins.py\nnew file mode 100644\nindex 000000000000..b536acbd5701\n--- /dev/null\n+++ b/autogpt/plugins.py\n@@ -0,0 +1,265 @@\n+\"\"\"Handles loading of plugins.\"\"\"\n+\n+import importlib\n+import json\n+import os\n+import zipfile\n+from pathlib import Path\n+from typing import List, Optional, Tuple\n+from urllib.parse import urlparse\n+from zipimport import zipimporter\n+\n+import openapi_python_client\n+import requests\n+from auto_gpt_plugin_template import AutoGPTPluginTemplate\n+from openapi_python_client.cli import Config as OpenAPIConfig\n+\n+from autogpt.config import Config\n+from autogpt.models.base_open_ai_plugin import BaseOpenAIPlugin\n+\n+\n+def inspect_zip_for_module(zip_path: str, debug: bool = False) -> Optional[str]:\n+    \"\"\"\n+    Inspect a zipfile for a module.\n+\n+    Args:\n+        zip_path (str): Path to the zipfile.\n+        debug (bool, optional): Enable debug logging. Defaults to False.\n+\n+    Returns:\n+        Optional[str]: The name of the module if found, else None.\n+    \"\"\"\n+    with zipfile.ZipFile(zip_path, \"r\") as zfile:\n+        for name in zfile.namelist():\n+            if name.endswith(\"__init__.py\"):\n+                if debug:\n+                    print(f\"Found module '{name}' in the zipfile at: {name}\")\n+                return name\n+    if debug:\n+        print(f\"Module '__init__.py' not found in the zipfile @ {zip_path}.\")\n+    return None\n+\n+\n+def write_dict_to_json_file(data: dict, file_path: str) -> None:\n+    \"\"\"\n+    Write a dictionary to a JSON file.\n+    Args:\n+        data (dict): Dictionary to write.\n+        file_path (str): Path to the file.\n+    \"\"\"\n+    with open(file_path, \"w\") as file:\n+        json.dump(data, file, indent=4)\n+\n+\n+def fetch_openai_plugins_manifest_and_spec(cfg: Config) -> dict:\n+    \"\"\"\n+    Fetch the manifest for a list of OpenAI plugins.\n+        Args:\n+        urls (List): List of URLs to fetch.\n+    Returns:\n+        dict: per url dictionary of manifest and spec.\n+    \"\"\"\n+    # TODO add directory scan\n+    manifests = {}\n+    for url in cfg.plugins_openai:\n+        openai_plugin_client_dir = f\"{cfg.plugins_dir}/openai/{urlparse(url).netloc}\"\n+        create_directory_if_not_exists(openai_plugin_client_dir)\n+        if not os.path.exists(f\"{openai_plugin_client_dir}/ai-plugin.json\"):\n+            try:\n+                response = requests.get(f\"{url}/.well-known/ai-plugin.json\")\n+                if response.status_code == 200:\n+                    manifest = response.json()\n+                    if manifest[\"schema_version\"] != \"v1\":\n+                        print(\n+                            f\"Unsupported manifest version: {manifest['schem_version']} for {url}\"\n+                        )\n+                        continue\n+                    if manifest[\"api\"][\"type\"] != \"openapi\":\n+                        print(\n+                            f\"Unsupported API type: {manifest['api']['type']} for {url}\"\n+                        )\n+                        continue\n+                    write_dict_to_json_file(\n+                        manifest, f\"{openai_plugin_client_dir}/ai-plugin.json\"\n+                    )\n+                else:\n+                    print(f\"Failed to fetch manifest for {url}: {response.status_code}\")\n+            except requests.exceptions.RequestException as e:\n+                print(f\"Error while requesting manifest from {url}: {e}\")\n+        else:\n+            print(f\"Manifest for {url} already exists\")\n+            manifest = json.load(open(f\"{openai_plugin_client_dir}/ai-plugin.json\"))\n+        if not os.path.exists(f\"{openai_plugin_client_dir}/openapi.json\"):\n+            openapi_spec = openapi_python_client._get_document(\n+                url=manifest[\"api\"][\"url\"], path=None, timeout=5\n+            )\n+            write_dict_to_json_file(\n+                openapi_spec, f\"{openai_plugin_client_dir}/openapi.json\"\n+            )\n+        else:\n+            print(f\"OpenAPI spec for {url} already exists\")\n+            openapi_spec = json.load(open(f\"{openai_plugin_client_dir}/openapi.json\"))\n+        manifests[url] = {\"manifest\": manifest, \"openapi_spec\": openapi_spec}\n+    return manifests\n+\n+\n+def create_directory_if_not_exists(directory_path: str) -> bool:\n+    \"\"\"\n+    Create a directory if it does not exist.\n+    Args:\n+        directory_path (str): Path to the directory.\n+    Returns:\n+        bool: True if the directory was created, else False.\n+    \"\"\"\n+    if not os.path.exists(directory_path):\n+        try:\n+            os.makedirs(directory_path)\n+            print(f\"Created directory: {directory_path}\")\n+            return True\n+        except OSError as e:\n+            print(f\"Error creating directory {directory_path}: {e}\")\n+            return False\n+    else:\n+        print(f\"Directory {directory_path} already exists\")\n+        return True\n+\n+\n+def initialize_openai_plugins(\n+    manifests_specs: dict, cfg: Config, debug: bool = False\n+) -> dict:\n+    \"\"\"\n+    Initialize OpenAI plugins.\n+    Args:\n+        manifests_specs (dict): per url dictionary of manifest and spec.\n+        cfg (Config): Config instance including plugins config\n+        debug (bool, optional): Enable debug logging. Defaults to False.\n+    Returns:\n+        dict: per url dictionary of manifest, spec and client.\n+    \"\"\"\n+    openai_plugins_dir = f\"{cfg.plugins_dir}/openai\"\n+    if create_directory_if_not_exists(openai_plugins_dir):\n+        for url, manifest_spec in manifests_specs.items():\n+            openai_plugin_client_dir = f\"{openai_plugins_dir}/{urlparse(url).hostname}\"\n+            _meta_option = (openapi_python_client.MetaType.SETUP,)\n+            _config = OpenAPIConfig(\n+                **{\n+                    \"project_name_override\": \"client\",\n+                    \"package_name_override\": \"client\",\n+                }\n+            )\n+            prev_cwd = Path.cwd()\n+            os.chdir(openai_plugin_client_dir)\n+            Path(\"ai-plugin.json\")\n+            if not os.path.exists(\"client\"):\n+                client_results = openapi_python_client.create_new_client(\n+                    url=manifest_spec[\"manifest\"][\"api\"][\"url\"],\n+                    path=None,\n+                    meta=_meta_option,\n+                    config=_config,\n+                )\n+                if client_results:\n+                    print(\n+                        f\"Error creating OpenAPI client: {client_results[0].header} \\n\"\n+                        f\" details: {client_results[0].detail}\"\n+                    )\n+                    continue\n+            spec = importlib.util.spec_from_file_location(\n+                \"client\", \"client/client/client.py\"\n+            )\n+            module = importlib.util.module_from_spec(spec)\n+            spec.loader.exec_module(module)\n+            client = module.Client(base_url=url)\n+            os.chdir(prev_cwd)\n+            manifest_spec[\"client\"] = client\n+    return manifests_specs\n+\n+\n+def instantiate_openai_plugin_clients(\n+    manifests_specs_clients: dict, cfg: Config, debug: bool = False\n+) -> dict:\n+    \"\"\"\n+    Instantiates BaseOpenAIPlugin instances for each OpenAI plugin.\n+    Args:\n+        manifests_specs_clients (dict): per url dictionary of manifest, spec and client.\n+        cfg (Config): Config instance including plugins config\n+        debug (bool, optional): Enable debug logging. Defaults to False.\n+    Returns:\n+          plugins (dict): per url dictionary of BaseOpenAIPlugin instances.\n+\n+    \"\"\"\n+    plugins = {}\n+    for url, manifest_spec_client in manifests_specs_clients.items():\n+        plugins[url] = BaseOpenAIPlugin(manifest_spec_client)\n+    return plugins\n+\n+\n+def scan_plugins(cfg: Config, debug: bool = False) -> List[AutoGPTPluginTemplate]:\n+    \"\"\"Scan the plugins directory for plugins and loads them.\n+\n+    Args:\n+        cfg (Config): Config instance including plugins config\n+        debug (bool, optional): Enable debug logging. Defaults to False.\n+\n+    Returns:\n+        List[Tuple[str, Path]]: List of plugins.\n+    \"\"\"\n+    loaded_plugins = []\n+    # Generic plugins\n+    plugins_path_path = Path(cfg.plugins_dir)\n+    for plugin in plugins_path_path.glob(\"*.zip\"):\n+        if module := inspect_zip_for_module(str(plugin), debug):\n+            plugin = Path(plugin)\n+            module = Path(module)\n+            if debug:\n+                print(f\"Plugin: {plugin} Module: {module}\")\n+            zipped_package = zipimporter(str(plugin))\n+            zipped_module = zipped_package.load_module(str(module.parent))\n+            for key in dir(zipped_module):\n+                if key.startswith(\"__\"):\n+                    continue\n+                a_module = getattr(zipped_module, key)\n+                a_keys = dir(a_module)\n+                if (\n+                    \"_abc_impl\" in a_keys\n+                    and a_module.__name__ != \"AutoGPTPluginTemplate\"\n+                    and blacklist_whitelist_check(a_module.__name__, cfg)\n+                ):\n+                    loaded_plugins.append(a_module())\n+    # OpenAI plugins\n+    if cfg.plugins_openai:\n+        manifests_specs = fetch_openai_plugins_manifest_and_spec(cfg)\n+        if manifests_specs.keys():\n+            manifests_specs_clients = initialize_openai_plugins(\n+                manifests_specs, cfg, debug\n+            )\n+            for url, openai_plugin_meta in manifests_specs_clients.items():\n+                if blacklist_whitelist_check(url, cfg):\n+                    plugin = BaseOpenAIPlugin(openai_plugin_meta)\n+                    loaded_plugins.append(plugin)\n+\n+    if loaded_plugins:\n+        print(f\"\\nPlugins found: {len(loaded_plugins)}\\n\" \"--------------------\")\n+    for plugin in loaded_plugins:\n+        print(f\"{plugin._name}: {plugin._version} - {plugin._description}\")\n+    return loaded_plugins\n+\n+\n+def blacklist_whitelist_check(plugin_name: str, cfg: Config) -> bool:\n+    \"\"\"Check if the plugin is in the whitelist or blacklist.\n+\n+    Args:\n+        plugin_name (str): Name of the plugin.\n+        cfg (Config): Config object.\n+\n+    Returns:\n+        True or False\n+    \"\"\"\n+    if plugin_name in cfg.plugins_blacklist:\n+        return False\n+    if plugin_name in cfg.plugins_whitelist:\n+        return True\n+    ack = input(\n+        f\"WARNNG Plugin {plugin_name} found. But not in the\"\n+        \" whitelist... Load? (y/n): \"\n+    )\n+    return ack.lower() == \"y\"\ndiff --git a/autogpt/prompt.py b/autogpt/prompts/prompt.py\nsimilarity index 50%\nrename from autogpt/prompt.py\nrename to autogpt/prompts/prompt.py\nindex 08754605fbdf..79de04ea8f4b 100644\n--- a/autogpt/prompt.py\n+++ b/autogpt/prompts/prompt.py\n@@ -39,96 +35,12 @@ def get_prompt() -> str:\n     prompt_generator.add_constraint(\n         'Exclusively use the commands listed in double quotes e.g. \"command name\"'\n     )\n-    prompt_generator.add_constraint(\n-        \"Use subprocesses for commands that will not terminate within a few minutes\"\n-    )\n \n     # Define the command list\n     commands = [\n-        (\"Google Search\", \"google\", {\"input\": \"<search>\"}),\n-        (\n-            \"Browse Website\",\n-            \"browse_website\",\n-            {\"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"},\n-        ),\n-        (\n-            \"Start GPT Agent\",\n-            \"start_agent\",\n-            {\"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"},\n-        ),\n-        (\n-            \"Message GPT Agent\",\n-            \"message_agent\",\n-            {\"key\": \"<key>\", \"message\": \"<message>\"},\n-        ),\n-        (\"List GPT Agents\", \"list_agents\", {}),\n-        (\"Delete GPT Agent\", \"delete_agent\", {\"key\": \"<key>\"}),\n-        (\n-            \"Clone Repository\",\n-            \"clone_repository\",\n-            {\"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"},\n-        ),\n-        (\"Write to file\", \"write_to_file\", {\"file\": \"<file>\", \"text\": \"<text>\"}),\n-        (\"Read file\", \"read_file\", {\"file\": \"<file>\"}),\n-        (\"Append to file\", \"append_to_file\", {\"file\": \"<file>\", \"text\": \"<text>\"}),\n-        (\"Delete file\", \"delete_file\", {\"file\": \"<file>\"}),\n-        (\"Search Files\", \"search_files\", {\"directory\": \"<directory>\"}),\n-        (\"Analyze Code\", \"analyze_code\", {\"code\": \"<full_code_string>\"}),\n-        (\n-            \"Get Improved Code\",\n-            \"improve_code\",\n-            {\"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"},\n-        ),\n-        (\n-            \"Write Tests\",\n-            \"write_tests\",\n-            {\"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"},\n-        ),\n-        (\"Execute Python File\", \"execute_python_file\", {\"file\": \"<file>\"}),\n-        (\"Generate Image\", \"generate_image\", {\"prompt\": \"<prompt>\"}),\n-        (\"Send Tweet\", \"send_tweet\", {\"text\": \"<text>\"}),\n-    ]\n-\n-    # Only add the audio to text command if the model is specified\n-    if cfg.huggingface_audio_to_text_model:\n-        commands.append(\n-            (\"Convert Audio to text\", \"read_audio_from_file\", {\"file\": \"<file>\"}),\n-        )\n-\n-    # Only add shell command to the prompt if the AI is allowed to execute it\n-    if cfg.execute_local_commands:\n-        commands.append(\n-            (\n-                \"Execute Shell Command, non-interactive commands only\",\n-                \"execute_shell\",\n-                {\"command_line\": \"<command_line>\"},\n-            ),\n-        )\n-        commands.append(\n-            (\n-                \"Execute Shell Command Popen, non-interactive commands only\",\n-                \"execute_shell_popen\",\n-                {\"command_line\": \"<command_line>\"},\n-            ),\n-        )\n-\n-    # Only add the download file command if the AI is allowed to execute it\n-    if cfg.allow_downloads:\n-        commands.append(\n-            (\n-                \"Downloads a file from the internet, and stores it locally\",\n-                \"download_file\",\n-                {\"url\": \"<file_url>\", \"file\": \"<saved_filename>\"},\n-            ),\n-        )\n-\n-    # Add these command last.\n-    commands.append(\n         (\"Do Nothing\", \"do_nothing\", {}),\n-    )\n-    commands.append(\n         (\"Task Complete (Shutdown)\", \"task_complete\", {\"reason\": \"<reason>\"}),\n-    )\n+    ]\n \n     # Add commands to the PromptGenerator object\n     for command_label, command_name, args in commands:\n\ndiff --git a/tests/test_commands.py b/tests/test_commands.py\nnew file mode 100644\nindex 000000000000..8a7771f67745\n--- /dev/null\n+++ b/tests/test_commands.py\n@@ -0,0 +1,177 @@\n+import os\n+import shutil\n+import sys\n+from pathlib import Path\n+\n+import pytest\n+\n+from autogpt.commands.command import Command, CommandRegistry\n+\n+\n+class TestCommand:\n+    @staticmethod\n+    def example_function(arg1: int, arg2: str) -> str:\n+        return f\"{arg1} - {arg2}\"\n+\n+    def test_command_creation(self):\n+        cmd = Command(\n+            name=\"example\", description=\"Example command\", method=self.example_function\n+        )\n+\n+        assert cmd.name == \"example\"\n+        assert cmd.description == \"Example command\"\n+        assert cmd.method == self.example_function\n+        assert cmd.signature == \"(arg1: int, arg2: str) -> str\"\n+\n+    def test_command_call(self):\n+        cmd = Command(\n+            name=\"example\", description=\"Example command\", method=self.example_function\n+        )\n+\n+        result = cmd(arg1=1, arg2=\"test\")\n+        assert result == \"1 - test\"\n+\n+    def test_command_call_with_invalid_arguments(self):\n+        cmd = Command(\n+            name=\"example\", description=\"Example command\", method=self.example_function\n+        )\n+\n+        with pytest.raises(TypeError):\n+            cmd(arg1=\"invalid\", does_not_exist=\"test\")\n+\n+    def test_command_default_signature(self):\n+        cmd = Command(\n+            name=\"example\", description=\"Example command\", method=self.example_function\n+        )\n+\n+        assert cmd.signature == \"(arg1: int, arg2: str) -> str\"\n+\n+    def test_command_custom_signature(self):\n+        custom_signature = \"custom_arg1: int, custom_arg2: str\"\n+        cmd = Command(\n+            name=\"example\",\n+            description=\"Example command\",\n+            method=self.example_function,\n+            signature=custom_signature,\n+        )\n+\n+        assert cmd.signature == custom_signature\n+\n+\n+class TestCommandRegistry:\n+    @staticmethod\n+    def example_function(arg1: int, arg2: str) -> str:\n+        return f\"{arg1} - {arg2}\"\n+\n+    def test_register_command(self):\n+        \"\"\"Test that a command can be registered to the registry.\"\"\"\n+        registry = CommandRegistry()\n+        cmd = Command(\n+            name=\"example\", description=\"Example command\", method=self.example_function\n+        )\n+\n+        registry.register(cmd)\n+\n+        assert cmd.name in registry.commands\n+        assert registry.commands[cmd.name] == cmd\n+\n+    def test_unregister_command(self):\n+        \"\"\"Test that a command can be unregistered from the registry.\"\"\"\n+        registry = CommandRegistry()\n+        cmd = Command(\n+            name=\"example\", description=\"Example command\", method=self.example_function\n+        )\n+\n+        registry.register(cmd)\n+        registry.unregister(cmd.name)\n+\n+        assert cmd.name not in registry.commands\n+\n+    def test_get_command(self):\n+        \"\"\"Test that a command can be retrieved from the registry.\"\"\"\n+        registry = CommandRegistry()\n+        cmd = Command(\n+            name=\"example\", description=\"Example command\", method=self.example_function\n+        )\n+\n+        registry.register(cmd)\n+        retrieved_cmd = registry.get_command(cmd.name)\n+\n+        assert retrieved_cmd == cmd\n+\n+    def test_get_nonexistent_command(self):\n+        \"\"\"Test that attempting to get a nonexistent command raises a KeyError.\"\"\"\n+        registry = CommandRegistry()\n+\n+        with pytest.raises(KeyError):\n+            registry.get_command(\"nonexistent_command\")\n+\n+    def test_call_command(self):\n+        \"\"\"Test that a command can be called through the registry.\"\"\"\n+        registry = CommandRegistry()\n+        cmd = Command(\n+            name=\"example\", description=\"Example command\", method=self.example_function\n+        )\n+\n+        registry.register(cmd)\n+        result = registry.call(\"example\", arg1=1, arg2=\"test\")\n+\n+        assert result == \"1 - test\"\n+\n+    def test_call_nonexistent_command(self):\n+        \"\"\"Test that attempting to call a nonexistent command raises a KeyError.\"\"\"\n+        registry = CommandRegistry()\n+\n+        with pytest.raises(KeyError):\n+            registry.call(\"nonexistent_command\", arg1=1, arg2=\"test\")\n+\n+    def test_get_command_prompt(self):\n+        \"\"\"Test that the command prompt is correctly formatted.\"\"\"\n+        registry = CommandRegistry()\n+        cmd = Command(\n+            name=\"example\", description=\"Example command\", method=self.example_function\n+        )\n+\n+        registry.register(cmd)\n+        command_prompt = registry.command_prompt()\n+\n+        assert f\"(arg1: int, arg2: str)\" in command_prompt\n+\n+    def test_import_mock_commands_module(self):\n+        \"\"\"Test that the registry can import a module with mock command plugins.\"\"\"\n+        registry = CommandRegistry()\n+        mock_commands_module = \"tests.mocks.mock_commands\"\n+\n+        registry.import_commands(mock_commands_module)\n+\n+        assert \"function_based\" in registry.commands\n+        assert registry.commands[\"function_based\"].name == \"function_based\"\n+        assert (\n+            registry.commands[\"function_based\"].description\n+            == \"Function-based test command\"\n+        )\n+\n+    def test_import_temp_command_file_module(self, tmp_path):\n+        \"\"\"Test that the registry can import a command plugins module from a temp file.\"\"\"\n+        registry = CommandRegistry()\n+\n+        # Create a temp command file\n+        src = Path(os.getcwd()) / \"tests/mocks/mock_commands.py\"\n+        temp_commands_file = tmp_path / \"mock_commands.py\"\n+        shutil.copyfile(src, temp_commands_file)\n+\n+        # Add the temp directory to sys.path to make the module importable\n+        sys.path.append(str(tmp_path))\n+\n+        temp_commands_module = \"mock_commands\"\n+        registry.import_commands(temp_commands_module)\n+\n+        # Remove the temp directory from sys.path\n+        sys.path.remove(str(tmp_path))\n+\n+        assert \"function_based\" in registry.commands\n+        assert registry.commands[\"function_based\"].name == \"function_based\"\n+        assert (\n+            registry.commands[\"function_based\"].description\n+            == \"Function-based test command\"\n+        )\ndiff --git a/tests/unit/models/test_base_open_api_plugin.py b/tests/unit/models/test_base_open_api_plugin.py\nnew file mode 100644\nindex 000000000000..950a32662383\n--- /dev/null\n+++ b/tests/unit/models/test_base_open_api_plugin.py\n@@ -0,0 +1,79 @@\n+from typing import Any, Dict, List, Optional, Tuple\n+\n+import pytest\n+\n+from autogpt.models.base_open_ai_plugin import (\n+    BaseOpenAIPlugin,\n+    Message,\n+    PromptGenerator,\n+)\n+\n+\n+class DummyPlugin(BaseOpenAIPlugin):\n+    pass\n+\n+\n+@pytest.fixture\n+def dummy_plugin():\n+    manifests_specs_clients = {\n+        \"manifest\": {\n+            \"name_for_model\": \"Dummy\",\n+            \"schema_version\": \"1.0\",\n+            \"description_for_model\": \"A dummy plugin for testing purposes\",\n+        },\n+        \"client\": None,\n+        \"openapi_spec\": None,\n+    }\n+    return DummyPlugin(manifests_specs_clients)\n+\n+\n+def test_dummy_plugin_inheritance(dummy_plugin):\n+    assert isinstance(dummy_plugin, BaseOpenAIPlugin)\n+\n+\n+def test_dummy_plugin_name(dummy_plugin):\n+    assert dummy_plugin._name == \"Dummy\"\n+\n+\n+def test_dummy_plugin_version(dummy_plugin):\n+    assert dummy_plugin._version == \"1.0\"\n+\n+\n+def test_dummy_plugin_description(dummy_plugin):\n+    assert dummy_plugin._description == \"A dummy plugin for testing purposes\"\n+\n+\n+def test_dummy_plugin_default_methods(dummy_plugin):\n+    assert not dummy_plugin.can_handle_on_response()\n+    assert not dummy_plugin.can_handle_post_prompt()\n+    assert not dummy_plugin.can_handle_on_planning()\n+    assert not dummy_plugin.can_handle_post_planning()\n+    assert not dummy_plugin.can_handle_pre_instruction()\n+    assert not dummy_plugin.can_handle_on_instruction()\n+    assert not dummy_plugin.can_handle_post_instruction()\n+    assert not dummy_plugin.can_handle_pre_command()\n+    assert not dummy_plugin.can_handle_post_command()\n+    assert not dummy_plugin.can_handle_chat_completion(None, None, None, None)\n+\n+    assert dummy_plugin.on_response(\"hello\") == \"hello\"\n+    assert dummy_plugin.post_prompt(None) is None\n+    assert dummy_plugin.on_planning(None, None) is None\n+    assert dummy_plugin.post_planning(\"world\") == \"world\"\n+    pre_instruction = dummy_plugin.pre_instruction(\n+        [{\"role\": \"system\", \"content\": \"Beep, bop, boop\"}]\n+    )\n+    assert isinstance(pre_instruction, list)\n+    assert len(pre_instruction) == 1\n+    assert pre_instruction[0][\"role\"] == \"system\"\n+    assert pre_instruction[0][\"content\"] == \"Beep, bop, boop\"\n+    assert dummy_plugin.on_instruction(None) is None\n+    assert dummy_plugin.post_instruction(\"I'm a robot\") == \"I'm a robot\"\n+    pre_command = dummy_plugin.pre_command(\"evolve\", {\"continuously\": True})\n+    assert isinstance(pre_command, tuple)\n+    assert len(pre_command) == 2\n+    assert pre_command[0] == \"evolve\"\n+    assert pre_command[1][\"continuously\"] == True\n+    post_command = dummy_plugin.post_command(\"evolve\", \"upgraded successfully!\")\n+    assert isinstance(post_command, str)\n+    assert post_command == \"upgraded successfully!\"\n+    assert dummy_plugin.handle_chat_completion(None, None, None, None) is None\ndiff --git a/tests/unit/test_plugins.py b/tests/unit/test_plugins.py\nnew file mode 100644\nindex 000000000000..739e69bbe7e4\n--- /dev/null\n+++ b/tests/unit/test_plugins.py\n@@ -0,0 +1,112 @@\n+import pytest\n+\n+from autogpt.config import Config\n+from autogpt.plugins import (\n+    blacklist_whitelist_check,\n+    inspect_zip_for_module,\n+    scan_plugins,\n+)\n+\n+PLUGINS_TEST_DIR = \"tests/unit/data/test_plugins\"\n+PLUGIN_TEST_ZIP_FILE = \"Auto-GPT-Plugin-Test-master.zip\"\n+PLUGIN_TEST_INIT_PY = \"Auto-GPT-Plugin-Test-master/src/auto_gpt_vicuna/__init__.py\"\n+PLUGIN_TEST_OPENAI = \"https://weathergpt.vercel.app/\"\n+\n+\n+def test_inspect_zip_for_module():\n+    result = inspect_zip_for_module(str(f\"{PLUGINS_TEST_DIR}/{PLUGIN_TEST_ZIP_FILE}\"))\n+    assert result == PLUGIN_TEST_INIT_PY\n+\n+\n+@pytest.fixture\n+def mock_config_blacklist_whitelist_check():\n+    class MockConfig:\n+        plugins_blacklist = [\"BadPlugin\"]\n+        plugins_whitelist = [\"GoodPlugin\"]\n+\n+    return MockConfig()\n+\n+\n+def test_blacklist_whitelist_check_blacklist(\n+    mock_config_blacklist_whitelist_check, monkeypatch\n+):\n+    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n+    assert not blacklist_whitelist_check(\n+        \"BadPlugin\", mock_config_blacklist_whitelist_check\n+    )\n+\n+\n+def test_blacklist_whitelist_check_whitelist(\n+    mock_config_blacklist_whitelist_check, monkeypatch\n+):\n+    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n+    assert blacklist_whitelist_check(\n+        \"GoodPlugin\", mock_config_blacklist_whitelist_check\n+    )\n+\n+\n+def test_blacklist_whitelist_check_user_input_yes(\n+    mock_config_blacklist_whitelist_check, monkeypatch\n+):\n+    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n+    assert blacklist_whitelist_check(\n+        \"UnknownPlugin\", mock_config_blacklist_whitelist_check\n+    )\n+\n+\n+def test_blacklist_whitelist_check_user_input_no(\n+    mock_config_blacklist_whitelist_check, monkeypatch\n+):\n+    monkeypatch.setattr(\"builtins.input\", lambda _: \"n\")\n+    assert not blacklist_whitelist_check(\n+        \"UnknownPlugin\", mock_config_blacklist_whitelist_check\n+    )\n+\n+\n+def test_blacklist_whitelist_check_user_input_invalid(\n+    mock_config_blacklist_whitelist_check, monkeypatch\n+):\n+    monkeypatch.setattr(\"builtins.input\", lambda _: \"invalid\")\n+    assert not blacklist_whitelist_check(\n+        \"UnknownPlugin\", mock_config_blacklist_whitelist_check\n+    )\n+\n+\n+@pytest.fixture\n+def config_with_plugins():\n+    cfg = Config()\n+    cfg.plugins_dir = PLUGINS_TEST_DIR\n+    cfg.plugins_openai = [\"https://weathergpt.vercel.app/\"]\n+    return cfg\n+\n+\n+@pytest.fixture\n+def mock_config_openai_plugin():\n+    class MockConfig:\n+        plugins_dir = PLUGINS_TEST_DIR\n+        plugins_openai = [PLUGIN_TEST_OPENAI]\n+        plugins_blacklist = [\"AutoGPTPVicuna\"]\n+        plugins_whitelist = [PLUGIN_TEST_OPENAI]\n+\n+    return MockConfig()\n+\n+\n+def test_scan_plugins_openai(mock_config_openai_plugin):\n+    result = scan_plugins(mock_config_openai_plugin, debug=True)\n+    assert len(result) == 1\n+\n+\n+@pytest.fixture\n+def mock_config_generic_plugin():\n+    class MockConfig:\n+        plugins_dir = PLUGINS_TEST_DIR\n+        plugins_openai = []\n+        plugins_blacklist = []\n+        plugins_whitelist = [\"AutoGPTPVicuna\"]\n+\n+    return MockConfig()\n+\n+\n+def test_scan_plugins_generic(mock_config_generic_plugin):\n+    result = scan_plugins(mock_config_generic_plugin, debug=True)\n+    assert len(result) == 1\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Great discussion here. We need to refine our plugin hooking approach, considering both flexibility and security. The current whitelist checks only by name, which opens up potential abuse. Implementing more checks could improve safety. The suggestion to manage plugins through `requirements.txt` is promising, especially with specific commits for version control. However, manually authorizing plugins unless bypassed by an environment variable would keep things simple yet secure. The proposed API for prompts looks solid, allowing for customization without breaking existing functionalities."
      }
    ],
    "codediff_length": 46397
  },
  {
    "index": 2,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py b/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\nindex 01e7b3a49edf..b4e87d5080f6 100644\n--- a/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\n+++ b/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\n@@ -96,105 +100,25 @@ async def on_node_activate(\n ) -> \"NodeModel\":\n     \"\"\"Hook to be called when the node is activated/created\"\"\"\n \n-    block = node.block\n-\n-    if not block.webhook_config:\n-        return node\n-\n-    provider = block.webhook_config.provider\n-    if not supports_webhooks(provider):\n-        raise ValueError(\n-            f\"Block #{block.id} has webhook_config for provider {provider} \"\n-            \"which does not support webhooks\"\n+    if node.block.webhook_config:\n+        new_webhook, feedback = await setup_webhook_for_block(\n+            user_id=user_id,\n+            trigger_block=node.block,\n+            trigger_config=node.input_default,\n+            for_graph_id=node.graph_id,\n         )\n-\n-    logger.debug(\n-        f\"Activating webhook node #{node.id} with config {block.webhook_config}\"\n-    )\n-\n-    webhooks_manager = get_webhook_manager(provider)\n-\n-    if auto_setup_webhook := isinstance(block.webhook_config, BlockWebhookConfig):\n-        try:\n-            resource = block.webhook_config.resource_format.format(**node.input_default)\n-        except KeyError:\n-            resource = None\n-        logger.debug(\n-            f\"Constructed resource string {resource} from input {node.input_default}\"\n-        )\n-    else:\n-        resource = \"\"  # not relevant for manual webhooks\n-\n-    block_input_schema = cast(BlockSchema, block.input_schema)\n-    credentials_field_name = next(iter(block_input_schema.get_credentials_fields()), \"\")\n-    credentials_meta = (\n-        node.input_default.get(credentials_field_name)\n-        if credentials_field_name\n-        else None\n-    )\n-    event_filter_input_name = block.webhook_config.event_filter_input\n-    has_everything_for_webhook = (\n-        resource is not None\n-        and (credentials_meta or not credentials_field_name)\n-        and (\n-            not event_filter_input_name\n-            or (\n-                event_filter_input_name in node.input_default\n-                and any(\n-                    is_on\n-                    for is_on in node.input_default[event_filter_input_name].values()\n-                )\n-            )\n-        )\n-    )\n-\n-    if has_everything_for_webhook and resource is not None:\n-        logger.debug(f\"Node #{node} has everything for a webhook!\")\n-        if credentials_meta and not credentials:\n-            raise ValueError(\n-                f\"Cannot set up webhook for node #{node.id}: \"\n-                f\"credentials #{credentials_meta['id']} not available\"\n-            )\n-\n-        if event_filter_input_name:\n-            # Shape of the event filter is enforced in Block.__init__\n-            event_filter = cast(dict, node.input_default[event_filter_input_name])\n-            events = [\n-                block.webhook_config.event_format.format(event=event)\n-                for event, enabled in event_filter.items()\n-                if enabled is True\n-            ]\n-            logger.debug(f\"Webhook events to subscribe to: {', '.join(events)}\")\n+        if new_webhook:\n+            node = await set_node_webhook(node.id, new_webhook.id)\n         else:\n-            events = []\n-\n-        # Find/make and attach a suitable webhook to the node\n-        if auto_setup_webhook:\n-            assert credentials is not None\n-            new_webhook = await webhooks_manager.get_suitable_auto_webhook(\n-                user_id,\n-                credentials,\n-                block.webhook_config.webhook_type,\n-                resource,\n-                events,\n+            logger.debug(\n+                f\"Node #{node.id} does not have everything for a webhook: {feedback}\"\n             )\n-        else:\n-            # Manual webhook -> no credentials -> don't register but do create\n-            new_webhook = await webhooks_manager.get_manual_webhook(\n-                user_id,\n-                node.graph_id,\n-                block.webhook_config.webhook_type,\n-                events,\n-            )\n-        logger.debug(f\"Acquired webhook: {new_webhook}\")\n-        return await set_node_webhook(node.id, new_webhook.id)\n-    else:\n-        logger.debug(f\"Node #{node.id} does not have everything for a webhook\")\n \n     return node\n \n \n async def on_node_deactivate(\n+    user_id: str,\n     node: \"NodeModel\",\n     *,\n     credentials: Optional[\"Credentials\"] = None,\n\ndiff --git a/autogpt_platform/backend/backend/integrations/webhooks/utils.py b/autogpt_platform/backend/backend/integrations/webhooks/utils.py\nindex e53a18f0fb1c..d7c66efe2901 100644\n--- a/autogpt_platform/backend/backend/integrations/webhooks/utils.py\n+++ b/autogpt_platform/backend/backend/integrations/webhooks/utils.py\n@@ -10,3 +25,122 @@ def webhook_ingress_url(provider_name: ProviderName, webhook_id: str) -> str:\n         f\"{app_config.platform_base_url}/api/integrations/{provider_name.value}\"\n         f\"/webhooks/{webhook_id}/ingress\"\n     )\n+\n+\n+async def setup_webhook_for_block(\n+    user_id: str,\n+    trigger_block: \"Block[BlockSchema, BlockSchema]\",\n+    trigger_config: dict[str, JsonValue],  # = Trigger block inputs\n+    for_graph_id: Optional[str] = None,\n+    for_preset_id: Optional[str] = None,\n+    credentials: Optional[\"Credentials\"] = None,\n+) -> tuple[\"Webhook\", None] | tuple[None, str]:\n+    \"\"\"\n+    Utility function to create (and auto-setup if possible) a webhook for a given provider.\n+\n+    Returns:\n+        Webhook: The created or found webhook object, if successful.\n+        str: A feedback message, if any required inputs are missing.\n+    \"\"\"\n+    from backend.data.block import BlockWebhookConfig\n+\n+    if not (trigger_base_config := trigger_block.webhook_config):\n+        raise ValueError(f\"Block #{trigger_block.id} does not have a webhook_config\")\n+\n+    provider = trigger_base_config.provider\n+    if not supports_webhooks(provider):\n+        raise NotImplementedError(\n+            f\"Block #{trigger_block.id} has webhook_config for provider {provider} \"\n+            \"for which we do not have a WebhooksManager\"\n+        )\n+\n+    logger.debug(\n+        f\"Setting up webhook for block #{trigger_block.id} with config {trigger_config}\"\n+    )\n+\n+    # Check & parse the event filter input, if any\n+    events: list[str] = []\n+    if event_filter_input_name := trigger_base_config.event_filter_input:\n+        if not (event_filter := trigger_config.get(event_filter_input_name)):\n+            return None, (\n+                f\"Cannot set up {provider.value} webhook without event filter input: \"\n+                f\"missing input for '{event_filter_input_name}'\"\n+            )\n+        elif not (\n+            # Shape of the event filter is enforced in Block.__init__\n+            any((event_filter := cast(dict[str, bool], event_filter)).values())\n+        ):\n+            return None, (\n+                f\"Cannot set up {provider.value} webhook without any enabled events \"\n+                f\"in event filter input '{event_filter_input_name}'\"\n+            )\n+\n+        events = [\n+            trigger_base_config.event_format.format(event=event)\n+            for event, enabled in event_filter.items()\n+            if enabled is True\n+        ]\n+        logger.debug(f\"Webhook events to subscribe to: {', '.join(events)}\")\n+\n+    # Check & process prerequisites for auto-setup webhooks\n+    if auto_setup_webhook := isinstance(trigger_base_config, BlockWebhookConfig):\n+        try:\n+            resource = trigger_base_config.resource_format.format(**trigger_config)\n+        except KeyError as missing_key:\n+            return None, (\n+                f\"Cannot auto-setup {provider.value} webhook without resource: \"\n+                f\"missing input for '{missing_key}'\"\n+            )\n+        logger.debug(\n+            f\"Constructed resource string {resource} from input {trigger_config}\"\n+        )\n+\n+        creds_field_name = next(\n+            # presence of this field is enforced in Block.__init__\n+            iter(trigger_block.input_schema.get_credentials_fields())\n+        )\n+\n+        if not (\n+            credentials_meta := cast(dict, trigger_config.get(creds_field_name, None))\n+        ):\n+            return None, f\"Cannot set up {provider.value} webhook without credentials\"\n+        elif not (\n+            credentials := credentials\n+            or await credentials_manager.get(user_id, credentials_meta[\"id\"])\n+        ):\n+            raise ValueError(\n+                f\"Cannot set up {provider.value} webhook without credentials: \"\n+                f\"credentials #{credentials_meta['id']} not found for user #{user_id}\"\n+            )\n+        elif credentials.provider != provider:\n+            raise ValueError(\n+                f\"Credentials #{credentials.id} do not match provider {provider.value}\"\n+            )\n+    else:\n+        # not relevant for manual webhooks:\n+        resource = \"\"\n+        credentials = None\n+\n+    webhooks_manager = get_webhook_manager(provider)\n+\n+    # Find/make and attach a suitable webhook to the node\n+    if auto_setup_webhook:\n+        assert credentials is not None\n+        webhook = await webhooks_manager.get_suitable_auto_webhook(\n+            user_id=user_id,\n+            credentials=credentials,\n+            webhook_type=trigger_base_config.webhook_type,\n+            resource=resource,\n+            events=events,\n+        )\n+    else:\n+        # Manual webhook -> no credentials -> don't register but do create\n+        webhook = await webhooks_manager.get_manual_webhook(\n+            user_id=user_id,\n+            webhook_type=trigger_base_config.webhook_type,\n+            events=events,\n+            graph_id=for_graph_id,\n+            preset_id=for_preset_id,\n+        )\n+    logger.debug(f\"Acquired webhook: {webhook}\")\n+    return webhook, None\ndiff --git a/autogpt_platform/backend/backend/server/v2/library/routes/agents.py b/autogpt_platform/backend/backend/server/v2/library/routes/agents.py\nindex 54a0cd235fe6..97580ac39197 100644\n--- a/autogpt_platform/backend/backend/server/v2/library/routes/agents.py\n+++ b/autogpt_platform/backend/backend/server/v2/library/routes/agents.py\n@@ -245,3 +243,81 @@ async def fork_library_agent(\n         library_agent_id=library_agent_id,\n         user_id=user_id,\n     )\n+\n+\n+class TriggeredPresetSetupParams(BaseModel):\n+    name: str\n+    description: str = \"\"\n+\n+    trigger_config: dict[str, Any]\n+    agent_credentials: dict[str, CredentialsMetaInput] = Field(default_factory=dict)\n+\n+\n+@router.post(\"/{library_agent_id}/setup-trigger\")\n+async def setup_trigger(\n+    library_agent_id: str = Path(..., description=\"ID of the library agent\"),\n+    params: TriggeredPresetSetupParams = Body(),\n+    user_id: str = Depends(autogpt_auth_lib.depends.get_user_id),\n+) -> library_model.LibraryAgentPreset:\n+    \"\"\"\n+    Sets up a webhook-triggered `LibraryAgentPreset` for a `LibraryAgent`.\n+    Returns the correspondingly created `LibraryAgentPreset` with `webhook_id` set.\n+    \"\"\"\n+    library_agent = await library_db.get_library_agent(\n+        id=library_agent_id, user_id=user_id\n+    )\n+    if not library_agent:\n+        raise HTTPException(\n+            status_code=status.HTTP_404_NOT_FOUND,\n+            detail=f\"Library agent #{library_agent_id} not found\",\n+        )\n+\n+    graph = await get_graph(\n+        library_agent.graph_id, version=library_agent.graph_version, user_id=user_id\n+    )\n+    if not graph:\n+        raise HTTPException(\n+            status.HTTP_410_GONE,\n+            f\"Graph #{library_agent.graph_id} not accessible (anymore)\",\n+        )\n+    if not (trigger_node := graph.webhook_input_node):\n+        raise HTTPException(\n+            status_code=status.HTTP_400_BAD_REQUEST,\n+            detail=f\"Graph #{library_agent.graph_id} does not have a webhook node\",\n+        )\n+\n+    trigger_config_with_credentials = {\n+        **params.trigger_config,\n+        **(\n+            make_node_credentials_input_map(graph, params.agent_credentials).get(\n+                trigger_node.id\n+            )\n+            or {}\n+        ),\n+    }\n+\n+    new_webhook, feedback = await setup_webhook_for_block(\n+        user_id=user_id,\n+        trigger_block=trigger_node.block,\n+        trigger_config=trigger_config_with_credentials,\n+    )\n+    if not new_webhook:\n+        raise HTTPException(\n+            status_code=status.HTTP_400_BAD_REQUEST,\n+            detail=f\"Could not set up webhook: {feedback}\",\n+        )\n+\n+    new_preset = await library_db.create_preset(\n+        user_id=user_id,\n+        preset=library_model.LibraryAgentPresetCreatable(\n+            graph_id=library_agent.graph_id,\n+            graph_version=library_agent.graph_version,\n+            name=params.name,\n+            description=params.description,\n+            inputs=trigger_config_with_credentials,\n+            credentials=params.agent_credentials,\n+            webhook_id=new_webhook.id,\n+            is_active=True,\n+        ),\n+    )\n+    return new_preset\ndiff --git a/autogpt_platform/backend/backend/server/v2/library/routes/presets.py b/autogpt_platform/backend/backend/server/v2/library/routes/presets.py\nindex 23fc80d23e6e..35bbb9f59124 100644\n--- a/autogpt_platform/backend/backend/server/v2/library/routes/presets.py\n+++ b/autogpt_platform/backend/backend/server/v2/library/routes/presets.py\n@@ -163,17 +162,85 @@ async def update_preset(\n     Raises:\n         HTTPException: If an error occurs while updating the preset.\n     \"\"\"\n+    current = await get_preset(preset_id, user_id=user_id)\n+    if not current:\n+        raise HTTPException(status.HTTP_404_NOT_FOUND, f\"Preset #{preset_id} not found\")\n+\n+    graph = await get_graph(\n+        current.graph_id,\n+        current.graph_version,\n+        user_id=user_id,\n+    )\n+    if not graph:\n+        raise HTTPException(\n+            status.HTTP_410_GONE,\n+            f\"Graph #{current.graph_id} not accessible (anymore)\",\n+        )\n+\n+    trigger_inputs_updated, new_webhook, feedback = False, None, None\n+    if (trigger_node := graph.webhook_input_node) and (\n+        preset.inputs is not None and preset.credentials is not None\n+    ):\n+        trigger_config_with_credentials = {\n+            **preset.inputs,\n+            **(\n+                make_node_credentials_input_map(graph, preset.credentials).get(\n+                    trigger_node.id\n+                )\n+                or {}\n+            ),\n+        }\n+        new_webhook, feedback = await setup_webhook_for_block(\n+            user_id=user_id,\n+            trigger_block=graph.webhook_input_node.block,\n+            trigger_config=trigger_config_with_credentials,\n+            for_preset_id=preset_id,\n+        )\n+        trigger_inputs_updated = True\n+        if not new_webhook:\n+            raise HTTPException(\n+                status_code=status.HTTP_400_BAD_REQUEST,\n+                detail=f\"Could not update trigger configuration: {feedback}\",\n+            )\n+\n     try:\n-        return await db.update_preset(\n-            user_id=user_id, preset_id=preset_id, preset=preset\n+        updated = await db.update_preset(\n+            user_id=user_id,\n+            preset_id=preset_id,\n+            inputs=preset.inputs,\n+            credentials=preset.credentials,\n+            name=preset.name,\n+            description=preset.description,\n+            is_active=preset.is_active,\n         )\n     except Exception as e:\n         logger.exception(\"Preset update failed for user %s: %s\", user_id, e)\n         raise HTTPException(\n-            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n-            detail={\"message\": str(e), \"hint\": \"Check preset data and try again.\"},\n+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e)\n+        )\n+\n+    # Update the webhook as well, if necessary\n+    if trigger_inputs_updated:\n+        updated = await db.set_preset_webhook(\n+            user_id, preset_id, new_webhook.id if new_webhook else None\n         )\n \n+        # Clean up webhook if it is now unused\n+        if current.webhook_id and (\n+            current.webhook_id != (new_webhook.id if new_webhook else None)\n+        ):\n+            current_webhook = await get_webhook(current.webhook_id)\n+            credentials = (\n+                await credentials_manager.get(user_id, current_webhook.credentials_id)\n+                if current_webhook.credentials_id\n+                else None\n+            )\n+            await get_webhook_manager(\n+                current_webhook.provider\n+            ).prune_webhook_if_dangling(user_id, current_webhook.id, credentials)\n+\n+    return updated\n+\n \n @router.delete(\n     \"/presets/{preset_id}\",\n\ndiff --git a/autogpt_platform/backend/schema.prisma b/autogpt_platform/backend/schema.prisma\nindex 58e724db9c5b..0282b51f988e 100644\n--- a/autogpt_platform/backend/schema.prisma\n+++ b/autogpt_platform/backend/schema.prisma\n@@index([userId])\n@@ -428,7 +432,8 @@ model IntegrationWebhook {\n \n   providerWebhookId String // Webhook ID assigned by the provider\n \n-  AgentNodes AgentNode[]\n+  AgentNodes    AgentNode[]\n+  AgentPresets  AgentPreset[]\n \n   @@index([userId])\n }\ndiff --git a/autogpt_platform/frontend/src/components/agents/agent-run-draft-view.tsx b/autogpt_platform/frontend/src/components/agents/agent-run-draft-view.tsx\nindex ab531bf002f8..752f887ec077 100644\n--- a/autogpt_platform/frontend/src/components/agents/agent-run-draft-view.tsx\n+++ b/autogpt_platform/frontend/src/components/agents/agent-run-draft-view.tsx\n@@ -1,73 +1,434 @@\n \"use client\";\n-import React, { useCallback, useMemo, useState } from \"react\";\n+import React, { useCallback, useEffect, useMemo, useState } from \"react\";\n \n import { useBackendAPI } from \"@/lib/autogpt-server-api/context\";\n-import { GraphExecutionID, GraphMeta } from \"@/lib/autogpt-server-api\";\n+import {\n+  CredentialsMetaInput,\n+  GraphExecutionID,\n+  LibraryAgent,\n+  LibraryAgentPreset,\n+  LibraryAgentPresetID,\n+  LibraryAgentPresetUpdatable,\n+} from \"@/lib/autogpt-server-api\";\n \n import type { ButtonAction } from \"@/components/agptui/types\";\n import { Card, CardContent, CardHeader, CardTitle } from \"@/components/ui/card\";\n+import { IconCross, IconPlay, IconSave } from \"@/components/ui/icons\";\n import { CredentialsInput } from \"@/components/integrations/credentials-input\";\n import { TypeBasedInput } from \"@/components/type-based-input\";\n import { useToastOnFail } from \"@/components/ui/use-toast\";\n import ActionButtonGroup from \"@/components/agptui/action-button-group\";\n+import { useOnboarding } from \"@/components/onboarding/onboarding-provider\";\n+import { Trash2Icon } from \"lucide-react\";\n import SchemaTooltip from \"@/components/SchemaTooltip\";\n-import { IconPlay } from \"@/components/ui/icons\";\n-import { useOnboarding } from \"../onboarding/onboarding-provider\";\n+import { useToast } from \"@/components/ui/use-toast\";\n+import { isEmpty } from \"lodash\";\n+import { Input } from \"@/components/ui/input\";\n \n export default function AgentRunDraftView({\n-  graph,\n+  agent,\n+  agentPreset,\n   onRun,\n+  onCreatePreset,\n+  onUpdatePreset,\n+  doDeletePreset,\n   agentActions,\n }: {\n-  graph: GraphMeta;\n-  onRun: (runID: GraphExecutionID) => void;\n+  agent: LibraryAgent;\n   agentActions: ButtonAction[];\n-}): React.ReactNode {\n+  onRun: (runID: GraphExecutionID) => void;\n+} & (\n+  | {\n+      onCreatePreset: (preset: LibraryAgentPreset) => void;\n+      agentPreset?: never;\n+      onUpdatePreset?: never;\n+      doDeletePreset?: never;\n+    }\n+  | {\n+      onCreatePreset?: never;\n+      agentPreset: LibraryAgentPreset;\n+      onUpdatePreset: (preset: LibraryAgentPreset) => void;\n+      doDeletePreset: (presetID: LibraryAgentPresetID) => void;\n+    }\n+)): React.ReactNode {\n   const api = useBackendAPI();\n+  const { toast } = useToast();\n   const toastOnFail = useToastOnFail();\n \n-  const agentInputs = graph.input_schema.properties;\n-  const agentCredentialsInputs = graph.credentials_input_schema.properties;\n   const [inputValues, setInputValues] = useState<Record<string, any>>({});\n-  const [inputCredentials, setInputCredentials] = useState<Record<string, any>>(\n-    {},\n+  const [inputCredentials, setInputCredentials] = useState<\n+    Record<string, CredentialsMetaInput>\n+  >({});\n+  const [presetName, setPresetName] = useState<string>(\"\");\n+  const [presetDescription, setPresetDescription] = useState<string>(\"\");\n+  const [changedPresetAttributes, setChangedPresetAttributes] = useState<\n+    Set<keyof LibraryAgentPresetUpdatable>\n+  >(new Set());\n+  const { state: onboardingState, completeStep: completeOnboardingStep } =\n+    useOnboarding();\n+\n+  // Update values if agentPreset parameter is changed\n+  useEffect(() => {\n+    setInputValues(agentPreset?.inputs ?? {});\n+    setInputCredentials(agentPreset?.credentials ?? {});\n+    setPresetName(agentPreset?.name ?? \"\");\n+    setPresetDescription(agentPreset?.description ?? \"\");\n+    setChangedPresetAttributes(new Set());\n+  }, [agentPreset]);\n+\n+  const agentInputSchema = useMemo(\n+    () =>\n+      agent.has_external_trigger\n+        ? agent.trigger_setup_info.config_schema\n+        : agent.input_schema,\n+    [agent],\n+  );\n+  const agentInputFields = useMemo(\n+    () =>\n+      Object.fromEntries(\n+        Object.entries(agentInputSchema.properties).filter(\n+          ([_, subSchema]) => !subSchema.hidden,\n+        ),\n+      ),\n+    [agentInputSchema],\n+  );\n+  const agentCredentialsInputFields = useMemo(\n+    () => agent.credentials_input_schema.properties,\n+    [agent],\n+  );\n+\n+  const [allRequiredInputsAreSet, missingInputs] = useMemo(() => {\n+    const nonEmptyInputs = new Set(\n+      Object.keys(inputValues).filter((k) => !isEmpty(inputValues[k])),\n+    );\n+    const requiredInputs = new Set(\n+      agentInputSchema.required as string[] | undefined,\n+    );\n+    return [\n+      nonEmptyInputs.isSupersetOf(requiredInputs),\n+      [...requiredInputs.difference(nonEmptyInputs)],\n+    ];\n+  }, [agentInputSchema.required, inputValues]);\n+  const [allCredentialsAreSet, missingCredentials] = useMemo(() => {\n+    const availableCredentials = new Set(Object.keys(inputCredentials));\n+    const allCredentials = new Set(Object.keys(agentCredentialsInputFields));\n+    return [\n+      availableCredentials.isSupersetOf(allCredentials),\n+      [...allCredentials.difference(availableCredentials)],\n+    ];\n+  }, [agentCredentialsInputFields, inputCredentials]);\n+  const notifyMissingInputs = useCallback(\n+    (needPresetName: boolean = true) => {\n+      const allMissingFields = (\n+        needPresetName && !presetName\n+          ? [agent.has_external_trigger ? \"trigger_name\" : \"preset_name\"]\n+          : []\n+      )\n+        .concat(missingInputs)\n+        .concat(missingCredentials);\n+      toast({\n+        title: \" Not all required inputs are set\",\n+        description: `Please set ${allMissingFields.map((k) => `\\`${k}\\``).join(\", \")}`,\n+      });\n+    },\n+    [missingInputs, missingCredentials],\n   );\n-  const { state, completeStep } = useOnboarding();\n \n   const doRun = useCallback(() => {\n-    api\n-      .executeGraph(graph.id, graph.version, inputValues, inputCredentials)\n-      .then((newRun) => onRun(newRun.graph_exec_id))\n-      .catch(toastOnFail(\"execute agent\"));\n+    // Manually running webhook-triggered agents is not supported\n+    if (agent.has_external_trigger) return;\n+\n+    if (!agentPreset || changedPresetAttributes.size > 0) {\n+      if (!allRequiredInputsAreSet || !allCredentialsAreSet) {\n+        notifyMissingInputs(false);\n+        return;\n+      }\n+      // TODO: on executing preset with changes, ask for confirmation and offer save+run\n+      api\n+        .executeGraph(\n+          agent.graph_id,\n+          agent.graph_version,\n+          inputValues,\n+          inputCredentials,\n+        )\n+        .then((newRun) => onRun(newRun.graph_exec_id))\n+        .catch(toastOnFail(\"execute agent\"));\n+    } else {\n+      api\n+        .executeLibraryAgentPreset(agentPreset.id)\n+        .then((newRun) => onRun(newRun.id))\n+        .catch(toastOnFail(\"execute agent preset\"));\n+    }\n     // Mark run agent onboarding step as completed\n-    if (state?.completedSteps.includes(\"MARKETPLACE_ADD_AGENT\")) {\n-      completeStep(\"MARKETPLACE_RUN_AGENT\");\n+    if (onboardingState?.completedSteps.includes(\"MARKETPLACE_ADD_AGENT\")) {\n+      completeOnboardingStep(\"MARKETPLACE_RUN_AGENT\");\n     }\n   }, [\n     api,\n-    graph,\n+    agent,\n     inputValues,\n     inputCredentials,\n     onRun,\n     toastOnFail,\n-    state,\n-    completeStep,\n+    onboardingState,\n+    completeOnboardingStep,\n+  ]);\n+\n+  const doCreatePreset = useCallback(() => {\n+    if (!onCreatePreset) return;\n+\n+    if (!presetName || !allRequiredInputsAreSet || !allCredentialsAreSet) {\n+      notifyMissingInputs();\n+      return;\n+    }\n+\n+    api\n+      .createLibraryAgentPreset({\n+        name: presetName,\n+        description: presetDescription,\n+        graph_id: agent.graph_id,\n+        graph_version: agent.graph_version,\n+        inputs: inputValues,\n+        credentials: inputCredentials,\n+      })\n+      .then((newPreset) => {\n+        onCreatePreset(newPreset);\n+        setChangedPresetAttributes(new Set()); // reset change tracker\n+      })\n+      .catch(toastOnFail(\"save agent preset\"));\n+  }, [\n+    api,\n+    agent,\n+    presetName,\n+    presetDescription,\n+    inputValues,\n+    inputCredentials,\n+    onCreatePreset,\n+    toast,\n+    toastOnFail,\n+    onboardingState,\n+    completeOnboardingStep,\n+  ]);\n+\n+  const doUpdatePreset = useCallback(() => {\n+    if (!agentPreset || changedPresetAttributes.size == 0) return;\n+\n+    if (!presetName || !allRequiredInputsAreSet || !allCredentialsAreSet) {\n+      notifyMissingInputs();\n+      return;\n+    }\n+\n+    const updatePreset: LibraryAgentPresetUpdatable = {};\n+    if (changedPresetAttributes.has(\"name\")) updatePreset[\"name\"] = presetName;\n+    if (changedPresetAttributes.has(\"description\"))\n+      updatePreset[\"description\"] = presetDescription;\n+    if (\n+      changedPresetAttributes.has(\"inputs\") ||\n+      changedPresetAttributes.has(\"credentials\")\n+    ) {\n+      updatePreset[\"inputs\"] = inputValues;\n+      updatePreset[\"credentials\"] = inputCredentials;\n+    }\n+    api\n+      .updateLibraryAgentPreset(agentPreset.id, updatePreset)\n+      .then((updatedPreset) => {\n+        onUpdatePreset(updatedPreset);\n+        setChangedPresetAttributes(new Set()); // reset change tracker\n+      })\n+      .catch(toastOnFail(\"update agent preset\"));\n+  }, [\n+    api,\n+    agent,\n+    presetName,\n+    presetDescription,\n+    inputValues,\n+    inputCredentials,\n+    onUpdatePreset,\n+    toast,\n+    toastOnFail,\n+    onboardingState,\n+    completeOnboardingStep,\n+  ]);\n+\n+  const doSetPresetActive = useCallback(\n+    async (active: boolean) => {\n+      if (!agentPreset) return;\n+      const updatedPreset = await api.updateLibraryAgentPreset(agentPreset.id, {\n+        is_active: active,\n+      });\n+      onUpdatePreset(updatedPreset);\n+    },\n+    [agentPreset, api, onUpdatePreset],\n+  );\n+\n+  const doSetupTrigger = useCallback(() => {\n+    // Setting up a trigger for non-webhook-triggered agents is not supported\n+    if (!agent.has_external_trigger || !onCreatePreset) return;\n+\n+    if (!presetName || !allRequiredInputsAreSet || !allCredentialsAreSet) {\n+      notifyMissingInputs();\n+      return;\n+    }\n+\n+    const credentialsInputName =\n+      agent.trigger_setup_info.credentials_input_name;\n+\n+    if (!credentialsInputName) {\n+      // FIXME: implement support for manual-setup webhooks\n+      toast({\n+        variant: \"destructive\",\n+        title: \" Feature under construction\",\n+        description: \"Setting up non-auto-setup triggers is not yet supported.\",\n+      });\n+      return;\n+    }\n+\n+    api\n+      .setupAgentTrigger(agent.id, {\n+        name: presetName,\n+        description: presetDescription,\n+        trigger_config: inputValues,\n+        agent_credentials: inputCredentials,\n+      })\n+      .then((newPreset) => {\n+        onCreatePreset(newPreset);\n+        setChangedPresetAttributes(new Set()); // reset change tracker\n+      })\n+      .catch(toastOnFail(\"set up agent trigger\"));\n+\n+    // Mark run agent onboarding step as completed(?)\n+    if (onboardingState?.completedSteps.includes(\"MARKETPLACE_ADD_AGENT\")) {\n+      completeOnboardingStep(\"MARKETPLACE_RUN_AGENT\");\n+    }\n+  }, [\n+    api,\n+    agent,\n+    presetName,\n+    presetDescription,\n+    inputValues,\n+    inputCredentials,\n+    onCreatePreset,\n+    toast,\n+    toastOnFail,\n+    onboardingState,\n+    completeOnboardingStep,\n   ]);\n \n   const runActions: ButtonAction[] = useMemo(\n     () => [\n-      {\n-        label: (\n-          <>\n-            <IconPlay className=\"mr-2 size-5\" />\n-            Run\n-          </>\n-        ),\n-        variant: \"accent\",\n-        callback: doRun,\n-      },\n+      // \"Regular\" agent: [run] + [save as preset] buttons\n+      ...(!agent.has_external_trigger\n+        ? ([\n+            {\n+              label: (\n+                <>\n+                  <IconPlay className=\"mr-2 size-4\" /> Run\n+                </>\n+              ),\n+              variant: \"accent\",\n+              callback: doRun,\n+            },\n+            // {\n+            //   label: (\n+            //     <>\n+            //       <IconSave className=\"mr-2 size-4\" /> Save as a preset\n+            //     </>\n+            //   ),\n+            //   callback: doCreatePreset,\n+            //   disabled: !(\n+            //     presetName &&\n+            //     allRequiredInputsAreSet &&\n+            //     allCredentialsAreSet\n+            //   ),\n+            // },\n+          ] satisfies ButtonAction[])\n+        : []),\n+      // Triggered agent: [setup] button\n+      ...(agent.has_external_trigger && !agentPreset?.webhook_id\n+        ? ([\n+            {\n+              label: (\n+                <>\n+                  <IconPlay className=\"mr-2 size-4\" /> Set up trigger\n+                </>\n+              ),\n+              variant: \"accent\",\n+              callback: doSetupTrigger,\n+              disabled: !(\n+                presetName &&\n+                allRequiredInputsAreSet &&\n+                allCredentialsAreSet\n+              ),\n+            },\n+          ] satisfies ButtonAction[])\n+        : []),\n+      // Existing agent trigger: [enable]/[disable] button\n+      ...(agentPreset?.webhook_id\n+        ? ([\n+            agentPreset.is_active\n+              ? {\n+                  label: (\n+                    <>\n+                      <IconCross className=\"mr-2.5 size-3.5\" /> Disable trigger\n+                    </>\n+                  ),\n+                  variant: \"destructive\",\n+                  callback: () => doSetPresetActive(false),\n+                }\n+              : {\n+                  label: (\n+                    <>\n+                      <IconPlay className=\"mr-2 size-4\" /> Enable trigger\n+                    </>\n+                  ),\n+                  variant: \"accent\",\n+                  callback: () => doSetPresetActive(true),\n+                },\n+          ] satisfies ButtonAction[])\n+        : []),\n+      // Existing agent preset/trigger: [save] and [delete] buttons\n+      ...(agentPreset\n+        ? ([\n+            {\n+              label: (\n+                <>\n+                  <IconSave className=\"mr-2 size-4\" /> Save changes\n+                </>\n+              ),\n+              callback: doUpdatePreset,\n+              disabled: !(\n+                changedPresetAttributes.size > 0 &&\n+                presetName &&\n+                allRequiredInputsAreSet &&\n+                allCredentialsAreSet\n+              ),\n+            },\n+            {\n+              label: (\n+                <>\n+                  <Trash2Icon className=\"mr-2 size-4\" />\n+                  Delete {agent.has_external_trigger ? \"trigger\" : \"preset\"}\n+                </>\n+              ),\n+              callback: () => doDeletePreset(agentPreset.id),\n+            },\n+          ] satisfies ButtonAction[])\n+        : []),\n+    ],\n+    [\n+      agent.has_external_trigger,\n+      agentPreset,\n+      api,\n+      doRun,\n+      doSetupTrigger,\n+      doCreatePreset,\n+      doUpdatePreset,\n+      doDeletePreset,\n+      changedPresetAttributes,\n+      presetName,\n+      allRequiredInputsAreSet,\n+      allCredentialsAreSet,\n     ],\n-    [doRun],\n   );\n \n   return (\n\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Thank you for your PR on webhook-triggered agent support! Since it's marked as WIP, several key areas need attention before merging:\n\n1. Expand the PR description to clearly explain the feature and specific changes made.\n2. Complete the checklist items.\n3. The scope in the title (platform/library) doesnt match the extensive changes across backend and frontend. Consider updating it.\n4. Ensure user_id checks are handled in backend/data modifications.\n5. Include a test plan for verifying webhook functionality.\n\nOnce these are addressed, the PR will be ready for review. Looking forward to the completion!"
      }
    ],
    "codediff_length": 32608
  },
  {
    "index": 3,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/autogpt_platform/backend/backend/server/v2/store/admin_routes.py b/autogpt_platform/backend/backend/server/v2/store/admin_routes.py\nnew file mode 100644\nindex 000000000000..2c9273ffe4fc\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/server/v2/store/admin_routes.py\n@@ -0,0 +1,207 @@\n+import logging\n+import typing\n+\n+import autogpt_libs.auth.depends\n+import fastapi\n+import fastapi.responses\n+import prisma.enums\n+\n+import backend.server.v2.store.db\n+import backend.server.v2.store.exceptions\n+import backend.server.v2.store.model\n+\n+logger = logging.getLogger(__name__)\n+\n+router = fastapi.APIRouter(prefix=\"/admin\", tags=[\"store\", \"admin\"])\n+\n+\n+@router.get(\n+    \"/submissions\",\n+    response_model=backend.server.v2.store.model.StoreSubmissionsResponse,\n+    dependencies=[fastapi.Depends(autogpt_libs.auth.depends.requires_admin_user)],\n+)\n+async def get_submissions(\n+    status: typing.Optional[prisma.enums.SubmissionStatus] = None,\n+    search: typing.Optional[str] = None,\n+    page: int = 1,\n+    page_size: int = 20,\n+):\n+    \"\"\"\n+    Get all store submissions with filtering options.\n+\n+    Admin only endpoint for managing agent submissions.\n+\n+    Args:\n+        status: Filter by submission status (ALL, PENDING, APPROVED, REJECTED)\n+        search: Search by name, creator, or description\n+        page: Page number for pagination\n+        page_size: Number of items per page\n+\n+    Returns:\n+        StoreSubmissionsResponse with filtered submissions\n+    \"\"\"\n+    try:\n+        submissions = await backend.server.v2.store.db.get_admin_submissions(\n+            status=status,\n+            search_query=search,\n+            page=page,\n+            page_size=page_size,\n+        )\n+        return submissions\n+    except Exception as e:\n+        logger.exception(\"Error getting admin submissions: %s\", e)\n+        return fastapi.responses.JSONResponse(\n+            status_code=500,\n+            content={\"detail\": \"An error occurred while retrieving submissions\"},\n+        )\n+\n+\n+@router.get(\n+    \"/submissions/pending\",\n+    response_model=backend.server.v2.store.model.StoreSubmissionsResponse,\n+    dependencies=[fastapi.Depends(autogpt_libs.auth.depends.requires_admin_user)],\n+)\n+async def get_pending_submissions(\n+    page: int = 1,\n+    page_size: int = 20,\n+):\n+    \"\"\"\n+    Get pending submissions that need admin review.\n+\n+    Convenience endpoint for the admin dashboard.\n+\n+    Args:\n+        page: Page number for pagination\n+        page_size: Number of items per page\n+\n+    Returns:\n+        StoreSubmissionsResponse with pending submissions\n+    \"\"\"\n+    try:\n+        submissions = await backend.server.v2.store.db.get_pending_submissions(\n+            page=page,\n+            page_size=page_size,\n+        )\n+        return submissions\n+    except Exception as e:\n+        logger.exception(\"Error getting pending submissions: %s\", e)\n+        return fastapi.responses.JSONResponse(\n+            status_code=500,\n+            content={\n+                \"detail\": \"An error occurred while retrieving pending submissions\"\n+            },\n+        )\n+\n+\n+@router.get(\n+    \"/submissions/{store_listing_version_id}\",\n+    response_model=backend.server.v2.store.model.StoreSubmission,\n+    dependencies=[fastapi.Depends(autogpt_libs.auth.depends.requires_admin_user)],\n+)\n+async def get_submission_details(\n+    store_listing_version_id: str,\n+):\n+    \"\"\"\n+    Get detailed information about a specific submission.\n+\n+    Args:\n+        store_listing_version_id: ID of the submission version\n+\n+    Returns:\n+        StoreSubmission with full details including internal comments\n+    \"\"\"\n+    try:\n+        submission = await backend.server.v2.store.db.get_submission_details(\n+            store_listing_version_id=store_listing_version_id,\n+        )\n+        return submission\n+    except backend.server.v2.store.exceptions.SubmissionNotFoundError:\n+        return fastapi.responses.JSONResponse(\n+            status_code=404,\n+            content={\"detail\": \"Submission not found\"},\n+        )\n+    except Exception as e:\n+        logger.exception(\"Error getting submission details: %s\", e)\n+        return fastapi.responses.JSONResponse(\n+            status_code=500,\n+            content={\"detail\": \"An error occurred while retrieving submission details\"},\n+        )\n+\n+\n+@router.get(\n+    \"/submissions/listing/{listing_id}/history\",\n+    response_model=backend.server.v2.store.model.StoreSubmissionsResponse,\n+    dependencies=[fastapi.Depends(autogpt_libs.auth.depends.requires_admin_user)],\n+)\n+async def get_listing_history(\n+    listing_id: str,\n+    page: int = 1,\n+    page_size: int = 20,\n+):\n+    \"\"\"\n+    Get all submissions for a specific listing.\n+\n+    This shows the version history of a listing over time.\n+\n+    Args:\n+        listing_id: The ID of the store listing\n+        page: Page number for pagination\n+        page_size: Number of items per page\n+\n+    Returns:\n+        StoreSubmissionsResponse with all versions of a specific listing\n+    \"\"\"\n+    try:\n+        submissions = await backend.server.v2.store.db.get_listing_submissions_history(\n+            listing_id=listing_id,\n+            page=page,\n+            page_size=page_size,\n+        )\n+        return submissions\n+    except Exception as e:\n+        logger.exception(\"Error getting listing history: %s\", e)\n+        return fastapi.responses.JSONResponse(\n+            status_code=500,\n+            content={\"detail\": \"An error occurred while retrieving listing history\"},\n+        )\n+\n+\n+@router.post(\n+    \"/submissions/{store_listing_version_id}/review\",\n+    response_model=backend.server.v2.store.model.StoreSubmission,\n+    dependencies=[fastapi.Depends(autogpt_libs.auth.depends.requires_admin_user)],\n+)\n+async def review_submission(\n+    store_listing_version_id: str,\n+    request: backend.server.v2.store.model.ReviewSubmissionRequest,\n+    user: typing.Annotated[\n+        autogpt_libs.auth.models.User,\n+        fastapi.Depends(autogpt_libs.auth.depends.requires_admin_user),\n+    ],\n+):\n+    \"\"\"\n+    Review a store listing submission.\n+\n+    Args:\n+        store_listing_version_id: ID of the submission to review\n+        request: Review details including approval status and comments\n+        user: Authenticated admin user performing the review\n+\n+    Returns:\n+        StoreSubmission with updated review information\n+    \"\"\"\n+    try:\n+        submission = await backend.server.v2.store.db.review_store_submission(\n+            store_listing_version_id=store_listing_version_id,\n+            is_approved=request.is_approved,\n+            external_comments=request.comments,\n+            internal_comments=request.internal_comments or \"\",\n+            reviewer_id=user.user_id,\n+        )\n+        return submission\n+    except Exception as e:\n+        logger.exception(\"Error reviewing submission: %s\", e)\n+        return fastapi.responses.JSONResponse(\n+            status_code=500,\n+            content={\"detail\": \"An error occurred while reviewing the submission\"},\n+        )\ndiff --git a/autogpt_platform/backend/backend/server/v2/store/db.py b/autogpt_platform/backend/backend/server/v2/store/db.py\nindex c6cf64031966..3b97dee395c7 100644\n--- a/autogpt_platform/backend/backend/server/v2/store/db.py\n+++ b/autogpt_platform/backend/backend/server/v2/store/db.py\n@@ -810,36 +920,355 @@ async def review_store_submission(\n             else prisma.enums.SubmissionStatus.REJECTED\n         )\n \n-        update_data: prisma.types.StoreListingSubmissionUpdateInput = {\n-            \"Status\": submission_status,\n-            \"reviewComments\": comments,\n+        # Update the version with review information\n+        update_data: prisma.types.StoreListingVersionUpdateInput = {\n+            \"submissionStatus\": submission_status,\n+            \"reviewComments\": external_comments,\n+            \"internalComments\": internal_comments,\n             \"Reviewer\": {\"connect\": {\"id\": reviewer_id}},\n             \"StoreListing\": {\"connect\": {\"id\": store_listing_version.StoreListing.id}},\n+            \"reviewedAt\": datetime.now(tz=timezone.utc),\n         }\n \n-        create_data: prisma.types.StoreListingSubmissionCreateInput = {\n-            **update_data,\n-            \"StoreListingVersion\": {\"connect\": {\"id\": store_listing_version_id}},\n-        }\n-\n-        submission = await prisma.models.StoreListingSubmission.prisma().upsert(\n-            where={\"storeListingVersionId\": store_listing_version_id},\n-            data={\n-                \"create\": create_data,\n-                \"update\": update_data,\n-            },\n+        # Update the version\n+        submission = await prisma.models.StoreListingVersion.prisma().update(\n+            where={\"id\": store_listing_version_id},\n+            data=update_data,\n+            include={\"StoreListing\": True},\n         )\n \n         if not submission:\n-            raise fastapi.HTTPException(  # FIXME: don't return HTTP exceptions here\n-                status_code=404,\n-                detail=f\"Store listing submission {store_listing_version_id} not found\",\n+            raise backend.server.v2.store.exceptions.DatabaseError(\n+                f\"Failed to update store listing version {store_listing_version_id}\"\n             )\n \n-        return submission\n+        # Convert to Pydantic model for consistency\n+        return backend.server.v2.store.model.StoreSubmission(\n+            agent_id=submission.agentId,\n+            agent_version=submission.agentVersion,\n+            name=submission.name,\n+            sub_heading=submission.subHeading,\n+            slug=(\n+                submission.StoreListing.slug\n+                if hasattr(submission, \"storeListing\") and submission.StoreListing\n+                else \"\"\n+            ),\n+            description=submission.description,\n+            image_urls=submission.imageUrls or [],\n+            date_submitted=submission.submittedAt or submission.createdAt,\n+            status=submission.submissionStatus,\n+            runs=0,  # Default values since we don't have this data here\n+            rating=0.0,\n+            store_listing_version_id=submission.id,\n+            reviewer_id=submission.reviewerId,\n+            review_comments=submission.reviewComments,\n+            internal_comments=submission.internalComments,\n+            reviewed_at=submission.reviewedAt,\n+            changes_summary=submission.changesSummary,\n+        )\n \n     except Exception as e:\n         logger.error(f\"Could not create store submission review: {e}\")\n         raise backend.server.v2.store.exceptions.DatabaseError(\n             \"Failed to create store submission review\"\n         ) from e\n+\n+\n+async def get_admin_submissions(\n+    status: prisma.enums.SubmissionStatus | None = None,\n+    search_query: str | None = None,\n+    page: int = 1,\n+    page_size: int = 20,\n+) -> backend.server.v2.store.model.StoreSubmissionsResponse:\n+    \"\"\"Get store submissions for admins with filtering options.\n+\n+    Args:\n+        status: Filter by submission status (PENDING, APPROVED, REJECTED)\n+        search_query: Search by name, description, or user email\n+        page: Page number for pagination\n+        page_size: Number of items per page\n+\n+    Returns:\n+        StoreSubmissionsResponse with submissions including internal comments\n+    \"\"\"\n+    logger.debug(\n+        f\"Getting admin store submissions with status={status}, search={search_query}, page={page}\"\n+    )\n+\n+    try:\n+        # Build the where clause\n+        where = prisma.types.StoreSubmissionWhereInput()\n+        if status:\n+            where[\"status\"] = status\n+\n+        sanitized_query = sanitize_query(search_query)\n+        if sanitized_query:\n+            # Find users with matching email first\n+            matching_users = await prisma.models.User.prisma().find_many(\n+                where={\"email\": {\"contains\": sanitized_query, \"mode\": \"insensitive\"}},\n+            )\n+\n+            user_ids = [user.id for user in matching_users]\n+\n+            # Set up the OR conditions to search across multiple fields\n+            where[\"OR\"] = [\n+                {\"name\": {\"contains\": sanitized_query, \"mode\": \"insensitive\"}},\n+                {\"description\": {\"contains\": sanitized_query, \"mode\": \"insensitive\"}},\n+                {\"sub_heading\": {\"contains\": sanitized_query, \"mode\": \"insensitive\"}},\n+            ]\n+\n+            # Add user_id condition if any users matched\n+            if user_ids:\n+                where[\"OR\"].append({\"user_id\": {\"in\": user_ids}})\n+                logger.debug(\n+                    f\"Found {len(user_ids)} users matching query: {sanitized_query}\"\n+                )\n+\n+        # Calculate pagination\n+        skip = (page - 1) * page_size\n+\n+        # Query submissions from database - use the StoreSubmission view\n+        submissions = await prisma.models.StoreSubmission.prisma().find_many(\n+            where=where,\n+            skip=skip,\n+            take=page_size,\n+            order=[{\"date_submitted\": \"desc\"}],\n+        )\n+\n+        # Get total count for pagination\n+        total = await prisma.models.StoreSubmission.prisma().count(where=where)\n+        total_pages = (total + page_size - 1) // page_size\n+\n+        # Convert to response models - using all fields for admins\n+        submission_models = []\n+        for sub in submissions:\n+            submission_model = backend.server.v2.store.model.StoreSubmission(\n+                agent_id=sub.agent_id,\n+                agent_version=sub.agent_version,\n+                name=sub.name,\n+                sub_heading=sub.sub_heading,\n+                slug=sub.slug,\n+                description=sub.description,\n+                image_urls=sub.image_urls or [],\n+                date_submitted=sub.date_submitted or datetime.now(tz=timezone.utc),\n+                status=sub.status,\n+                runs=sub.runs or 0,\n+                rating=sub.rating or 0.0,\n+                store_listing_version_id=sub.store_listing_version_id,\n+                reviewer_id=sub.reviewer_id,\n+                review_comments=sub.review_comments,\n+                internal_comments=sub.internal_comments,\n+                reviewed_at=sub.reviewed_at,\n+                changes_summary=sub.changes_summary,\n+            )\n+            submission_models.append(submission_model)\n+\n+        logger.debug(f\"Found {len(submission_models)} submissions for admin\")\n+        return backend.server.v2.store.model.StoreSubmissionsResponse(\n+            submissions=submission_models,\n+            pagination=backend.server.v2.store.model.Pagination(\n+                current_page=page,\n+                total_items=total,\n+                total_pages=total_pages,\n+                page_size=page_size,\n+            ),\n+        )\n+    except Exception as e:\n+        logger.error(f\"Error fetching admin store submissions: {e}\")\n+        # Return empty response rather than exposing internal errors\n+        return backend.server.v2.store.model.StoreSubmissionsResponse(\n+            submissions=[],\n+            pagination=backend.server.v2.store.model.Pagination(\n+                current_page=page,\n+                total_items=0,\n+                total_pages=0,\n+                page_size=page_size,\n+            ),\n+        )\n+\n+\n+async def get_listing_submissions_history(\n+    listing_id: str,\n+    page: int = 1,\n+    page_size: int = 20,\n+) -> backend.server.v2.store.model.StoreSubmissionsResponse:\n+    \"\"\"Get all submissions for a specific listing for admins.\n+\n+    Args:\n+        listing_id: The ID of the store listing\n+        page: Page number for pagination\n+        page_size: Number of items per page\n+\n+    Returns:\n+        StoreSubmissionsResponse with all versions of a specific listing\n+    \"\"\"\n+    logger.debug(f\"Getting submission history for listing {listing_id}\")\n+\n+    try:\n+        # Query StoreListingVersion directly since we need all versions for a listing\n+        versions = await prisma.models.StoreListingVersion.prisma().find_many(\n+            where={\"storeListingId\": listing_id},\n+            order=[{\"version\": \"desc\"}],\n+            skip=(page - 1) * page_size,\n+            take=page_size,\n+            include={\"StoreListing\": True},\n+        )\n+\n+        # Get total count for pagination\n+        total = await prisma.models.StoreListingVersion.prisma().count(\n+            where={\"storeListingId\": listing_id}\n+        )\n+        total_pages = (total + page_size - 1) // page_size\n+\n+        # Convert to StoreSubmission models\n+        submissions = []\n+        for version in versions:\n+            if not version.StoreListing:\n+                continue\n+\n+            submissions.append(\n+                backend.server.v2.store.model.StoreSubmission(\n+                    agent_id=version.agentId,\n+                    agent_version=version.agentVersion,\n+                    name=version.name,\n+                    sub_heading=version.subHeading,\n+                    slug=version.StoreListing.slug,\n+                    description=version.description,\n+                    image_urls=version.imageUrls or [],\n+                    date_submitted=version.submittedAt or version.createdAt,\n+                    status=version.submissionStatus,\n+                    runs=0,  # We don't have this data here\n+                    rating=0.0,  # We don't have this data here\n+                    store_listing_version_id=version.id,\n+                    reviewer_id=version.reviewerId,\n+                    review_comments=version.reviewComments,\n+                    internal_comments=version.internalComments,\n+                    reviewed_at=version.reviewedAt,\n+                    changes_summary=version.changesSummary,\n+                )\n+            )\n+\n+        logger.debug(\n+            f\"Found {len(submissions)} version history for listing {listing_id}\"\n+        )\n+        return backend.server.v2.store.model.StoreSubmissionsResponse(\n+            submissions=submissions,\n+            pagination=backend.server.v2.store.model.Pagination(\n+                current_page=page,\n+                total_items=total,\n+                total_pages=total_pages,\n+                page_size=page_size,\n+            ),\n+        )\n+    except Exception as e:\n+        logger.error(f\"Error fetching listing submission history: {e}\")\n+        return backend.server.v2.store.model.StoreSubmissionsResponse(\n+            submissions=[],\n+            pagination=backend.server.v2.store.model.Pagination(\n+                current_page=page,\n+                total_items=0,\n+                total_pages=0,\n+                page_size=page_size,\n+            ),\n+        )\n+\n+\n+async def get_pending_submissions(\n+    page: int = 1,\n+    page_size: int = 20,\n+) -> backend.server.v2.store.model.StoreSubmissionsResponse:\n+    \"\"\"Convenience function to get pending submissions for the admin dashboard.\n+\n+    Args:\n+        page: Page number for pagination\n+        page_size: Number of items per page\n+\n+    Returns:\n+        StoreSubmissionsResponse with submissions that need review\n+    \"\"\"\n+    return await get_admin_submissions(\n+        status=prisma.enums.SubmissionStatus.PENDING,\n+        page=page,\n+        page_size=page_size,\n+    )\n+\n+\n+async def get_submission_details(\n+    store_listing_version_id: str,\n+) -> backend.server.v2.store.model.StoreSubmission:\n+    \"\"\"Get detailed information about a specific submission for admins.\n+\n+    Args:\n+        store_listing_version_id: ID of the submission version\n+\n+    Returns:\n+        StoreSubmission with full details including internal comments\n+    \"\"\"\n+    logger.debug(f\"Getting submission details for {store_listing_version_id}\")\n+\n+    try:\n+        # First try getting from the view for efficiency\n+        submission = await prisma.models.StoreSubmission.prisma().find_first(\n+            where={\"store_listing_version_id\": store_listing_version_id}\n+        )\n+\n+        if submission:\n+            return backend.server.v2.store.model.StoreSubmission(\n+                agent_id=submission.agent_id,\n+                agent_version=submission.agent_version,\n+                name=submission.name,\n+                sub_heading=submission.sub_heading,\n+                slug=submission.slug,\n+                description=submission.description,\n+                image_urls=submission.image_urls or [],\n+                date_submitted=submission.date_submitted\n+                or datetime.now(tz=timezone.utc),\n+                status=submission.status,\n+                runs=submission.runs or 0,\n+                rating=submission.rating or 0.0,\n+                store_listing_version_id=submission.store_listing_version_id,\n+                reviewer_id=submission.reviewer_id,\n+                review_comments=submission.review_comments,\n+                internal_comments=submission.internal_comments,\n+                reviewed_at=submission.reviewed_at,\n+                changes_summary=submission.changes_summary,\n+            )\n+\n+        # If not found in the view, try getting it directly\n+        version = await prisma.models.StoreListingVersion.prisma().find_unique(\n+            where={\"id\": store_listing_version_id},\n+            include={\"StoreListing\": True},\n+        )\n+\n+        if not version or not version.StoreListing:\n+            raise backend.server.v2.store.exceptions.SubmissionNotFoundError(\n+                f\"Submission {store_listing_version_id} not found\"\n+            )\n+\n+        return backend.server.v2.store.model.StoreSubmission(\n+            agent_id=version.agentId,\n+            agent_version=version.agentVersion,\n+            name=version.name,\n+            sub_heading=version.subHeading,\n+            slug=version.StoreListing.slug,\n+            description=version.description,\n+            image_urls=version.imageUrls or [],\n+            date_submitted=version.submittedAt or version.createdAt,\n+            status=version.submissionStatus,\n+            runs=0,  # We don't have this data here\n+            rating=0.0,  # We don't have this data here\n+            store_listing_version_id=version.id,\n+            reviewer_id=version.reviewerId,\n+            review_comments=version.reviewComments,\n+            internal_comments=version.internalComments,\n+            reviewed_at=version.reviewedAt,\n+            changes_summary=version.changesSummary,\n+        )\n+\n+    except backend.server.v2.store.exceptions.SubmissionNotFoundError:\n+        raise\n+    except Exception as e:\n+        logger.error(f\"Error fetching submission details: {e}\")\n+        raise backend.server.v2.store.exceptions.DatabaseError(\n+            \"Failed to fetch submission details\"\n+        ) from e\ndiff --git a/autogpt_platform/backend/migrations/20250318043016_update_store_submissions_format/migration.sql b/autogpt_platform/backend/migrations/20250318043016_update_store_submissions_format/migration.sql\nnew file mode 100644\nindex 000000000000..c8c7edac7087\n--- /dev/null\n+++ b/autogpt_platform/backend/migrations/20250318043016_update_store_submissions_format/migration.sql\n@@ -0,0 +1,372 @@\n+/*\n+  Warnings:\n+\n+  - The enum type \"SubmissionStatus\" will be replaced. The 'DAFT' value is removed, so any data using 'DAFT' will be updated to 'DRAFT'. If there are rows still expecting 'DAFT' after this change, it will fail.\n+  - You are about to drop the column \"isApproved\" on the \"StoreListing\" table. All the data in that column will be lost.\n+  - You are about to drop the column \"slug\" on the \"StoreListingVersion\" table. All the data in that column will be lost.\n+  - You are about to drop the \"StoreListingSubmission\" table. Data in that table (beyond what is copied over) will be permanently lost.\n+  - A unique constraint covering the column \"activeVersionId\" on the \"StoreListing\" table will be added. If duplicates already exist, this will fail.\n+  - A unique constraint covering the columns (\"storeListingId\",\"version\") on \"StoreListingVersion\" will be added. If duplicates already exist, this will fail.\n+  - The \"storeListingId\" column on \"StoreListingVersion\" is set to NOT NULL. If any rows currently have a NULL value, this step will fail.\n+  - The views \"StoreSubmission\", \"StoreAgent\", and \"Creator\" are dropped and recreated. Any usage or references to them will be momentarily disrupted until the views are recreated.\n+*/\n+\n+BEGIN;\n+\n+-- First, drop all views that depend on the columns and types we're modifying\n+DROP VIEW IF EXISTS \"StoreSubmission\";\n+DROP VIEW IF EXISTS \"StoreAgent\";\n+DROP VIEW IF EXISTS \"Creator\";\n+\n+-- Create the new enum type\n+CREATE TYPE \"SubmissionStatus_new\" AS ENUM ('DRAFT', 'PENDING', 'APPROVED', 'REJECTED');\n+\n+-- Modify the column with the correct casing (Status with capital S)\n+ALTER TABLE \"StoreListingSubmission\" ALTER COLUMN \"Status\" DROP DEFAULT;\n+ALTER TABLE \"StoreListingSubmission\"\n+    ALTER COLUMN \"Status\" TYPE \"SubmissionStatus_new\"\n+    USING (\n+      CASE WHEN \"Status\"::text = 'DAFT' THEN 'DRAFT'::text\n+           ELSE \"Status\"::text\n+      END\n+    )::\"SubmissionStatus_new\";\n+\n+-- Rename the enum types\n+ALTER TYPE \"SubmissionStatus\" RENAME TO \"SubmissionStatus_old\";\n+ALTER TYPE \"SubmissionStatus_new\" RENAME TO \"SubmissionStatus\";\n+DROP TYPE \"SubmissionStatus_old\";\n+\n+-- Set default back\n+ALTER TABLE \"StoreListingSubmission\" ALTER COLUMN \"Status\" SET DEFAULT 'PENDING';\n+\n+-- Drop constraints\n+ALTER TABLE \"StoreListingSubmission\" DROP CONSTRAINT IF EXISTS \"StoreListingSubmission_reviewerId_fkey\";\n+\n+-- Drop indexes\n+DROP INDEX IF EXISTS \"StoreListing_isDeleted_isApproved_idx\";\n+DROP INDEX IF EXISTS \"StoreListingSubmission_storeListingVersionId_key\";\n+\n+-- Modify StoreListing\n+ALTER TABLE \"StoreListing\"\n+    DROP COLUMN IF EXISTS \"isApproved\",\n+    ADD COLUMN IF NOT EXISTS \"activeVersionId\" TEXT,\n+    ADD COLUMN IF NOT EXISTS \"hasApprovedVersion\" BOOLEAN NOT NULL DEFAULT false,\n+    ADD COLUMN IF NOT EXISTS \"slug\" TEXT;\n+\n+-- First add ALL columns to StoreListingVersion (including the submissionStatus column)\n+ALTER TABLE \"StoreListingVersion\"\n+    ADD COLUMN IF NOT EXISTS \"reviewerId\" TEXT,\n+    ADD COLUMN IF NOT EXISTS \"reviewComments\" TEXT,\n+    ADD COLUMN IF NOT EXISTS \"internalComments\" TEXT,\n+    ADD COLUMN IF NOT EXISTS \"reviewedAt\" TIMESTAMP(3),\n+    ADD COLUMN IF NOT EXISTS \"changesSummary\" TEXT,\n+    ADD COLUMN IF NOT EXISTS \"submissionStatus\" \"SubmissionStatus\" NOT NULL DEFAULT 'DRAFT',\n+    ADD COLUMN IF NOT EXISTS \"submittedAt\" TIMESTAMP(3),\n+    ALTER COLUMN \"storeListingId\" SET NOT NULL;\n+\n+-- NOW copy data from StoreListingSubmission to StoreListingVersion\n+DO $$\n+BEGIN\n+    -- First, check what columns actually exist in the StoreListingSubmission table\n+    DECLARE\n+        has_reviewerId BOOLEAN := (\n+            SELECT EXISTS (\n+                SELECT FROM information_schema.columns\n+                WHERE table_name = 'StoreListingSubmission'\n+                AND column_name = 'reviewerId'\n+            )\n+        );\n+\n+        has_reviewComments BOOLEAN := (\n+            SELECT EXISTS (\n+                SELECT FROM information_schema.columns\n+                WHERE table_name = 'StoreListingSubmission'\n+                AND column_name = 'reviewComments'\n+            )\n+        );\n+\n+        has_changesSummary BOOLEAN := (\n+            SELECT EXISTS (\n+                SELECT FROM information_schema.columns\n+                WHERE table_name = 'StoreListingSubmission'\n+                AND column_name = 'changesSummary'\n+            )\n+        );\n+    BEGIN\n+        -- Only copy fields that we know exist\n+        IF has_reviewerId THEN\n+            UPDATE \"StoreListingVersion\" AS v\n+            SET \"reviewerId\" = s.\"reviewerId\"\n+            FROM \"StoreListingSubmission\" AS s\n+            WHERE v.\"id\" = s.\"storeListingVersionId\";\n+        END IF;\n+\n+        IF has_reviewComments THEN\n+            UPDATE \"StoreListingVersion\" AS v\n+            SET \"reviewComments\" = s.\"reviewComments\"\n+            FROM \"StoreListingSubmission\" AS s\n+            WHERE v.\"id\" = s.\"storeListingVersionId\";\n+        END IF;\n+\n+        IF has_changesSummary THEN\n+            UPDATE \"StoreListingVersion\" AS v\n+            SET \"changesSummary\" = s.\"changesSummary\"\n+            FROM \"StoreListingSubmission\" AS s\n+            WHERE v.\"id\" = s.\"storeListingVersionId\";\n+        END IF;\n+    END;\n+\n+    -- Update submission status based on StoreListingSubmission status\n+    UPDATE \"StoreListingVersion\" AS v\n+    SET \"submissionStatus\" = s.\"Status\"\n+    FROM \"StoreListingSubmission\" AS s\n+    WHERE v.\"id\" = s.\"storeListingVersionId\";\n+\n+    -- Update reviewedAt timestamps for versions with APPROVED or REJECTED status\n+    UPDATE \"StoreListingVersion\" AS v\n+    SET \"reviewedAt\" = s.\"updatedAt\"\n+    FROM \"StoreListingSubmission\" AS s\n+    WHERE v.\"id\" = s.\"storeListingVersionId\"\n+      AND s.\"Status\" IN ('APPROVED', 'REJECTED');\n+END;\n+$$;\n+\n+-- Drop the StoreListingSubmission table\n+DROP TABLE IF EXISTS \"StoreListingSubmission\";\n+\n+-- Copy slugs from StoreListingVersion to StoreListing\n+WITH latest_versions AS (\n+    SELECT\n+        \"storeListingId\",\n+        \"slug\",\n+        ROW_NUMBER() OVER (PARTITION BY \"storeListingId\" ORDER BY \"version\" DESC) as rn\n+    FROM \"StoreListingVersion\"\n+)\n+UPDATE \"StoreListing\" sl\n+SET \"slug\" = lv.\"slug\"\n+FROM latest_versions lv\n+WHERE sl.\"id\" = lv.\"storeListingId\"\n+  AND lv.rn = 1;\n+\n+-- Make StoreListing.slug required and unique\n+ALTER TABLE \"StoreListing\" ALTER COLUMN \"slug\" SET NOT NULL;\n+CREATE UNIQUE INDEX \"StoreListing_owningUserId_slug_key\" ON \"StoreListing\"(\"owningUserId\", \"slug\");\n+DROP INDEX \"StoreListing_owningUserId_idx\";\n+\n+-- Drop the slug column from StoreListingVersion since it's now on StoreListing\n+ALTER TABLE \"StoreListingVersion\" DROP COLUMN \"slug\";\n+\n+-- Update both sides of the relation from one-to-one to one-to-many\n+-- The AgentGraph->StoreListingVersion relationship is now one-to-many\n+\n+-- Drop the unique constraint but add a non-unique index for query performance\n+ALTER TABLE \"StoreListingVersion\" DROP CONSTRAINT IF EXISTS \"StoreListingVersion_agentId_agentVersion_key\";\n+CREATE INDEX IF NOT EXISTS \"StoreListingVersion_agentId_agentVersion_idx\"\n+    ON \"StoreListingVersion\"(\"agentId\", \"agentVersion\");\n+\n+-- Set isApproved based on submissionStatus before removing it\n+UPDATE \"StoreListingVersion\"\n+SET \"submissionStatus\" = 'APPROVED'\n+WHERE \"isApproved\" = true;\n+\n+-- Drop the isApproved column from StoreListingVersion since it's redundant with submissionStatus\n+ALTER TABLE \"StoreListingVersion\" DROP COLUMN \"isApproved\";\n+\n+-- Initialize hasApprovedVersion for existing StoreListing rows ***\n+-- This sets \"hasApprovedVersion\" = TRUE for any StoreListing\n+-- that has at least one corresponding version with \"APPROVED\" status.\n+UPDATE \"StoreListing\" sl\n+SET \"hasApprovedVersion\" = (\n+  SELECT COUNT(*) > 0\n+  FROM \"StoreListingVersion\" slv\n+  WHERE slv.\"storeListingId\" = sl.id\n+    AND slv.\"submissionStatus\" = 'APPROVED'\n+    AND sl.\"agentId\" = slv.\"agentId\"\n+    AND sl.\"agentVersion\" = slv.\"agentVersion\"\n+);\n+\n+-- Create new indexes\n+CREATE UNIQUE INDEX IF NOT EXISTS \"StoreListing_activeVersionId_key\"\n+    ON \"StoreListing\"(\"activeVersionId\");\n+\n+CREATE INDEX IF NOT EXISTS \"StoreListing_isDeleted_hasApprovedVersion_idx\"\n+    ON \"StoreListing\"(\"isDeleted\", \"hasApprovedVersion\");\n+\n+CREATE INDEX IF NOT EXISTS \"StoreListingVersion_storeListingId_submissionStatus_isAvailable_idx\"\n+    ON \"StoreListingVersion\"(\"storeListingId\", \"submissionStatus\", \"isAvailable\");\n+\n+CREATE INDEX IF NOT EXISTS \"StoreListingVersion_submissionStatus_idx\"\n+    ON \"StoreListingVersion\"(\"submissionStatus\");\n+\n+CREATE UNIQUE INDEX IF NOT EXISTS \"StoreListingVersion_storeListingId_version_key\"\n+    ON \"StoreListingVersion\"(\"storeListingId\", \"version\");\n+\n+-- Add foreign keys\n+ALTER TABLE \"StoreListing\"\n+ADD CONSTRAINT \"StoreListing_activeVersionId_fkey\"\n+FOREIGN KEY (\"activeVersionId\") REFERENCES \"StoreListingVersion\"(\"id\")\n+ON DELETE SET NULL ON UPDATE CASCADE;\n+\n+-- Add reviewer foreign key\n+ALTER TABLE \"StoreListingVersion\"\n+ADD CONSTRAINT \"StoreListingVersion_reviewerId_fkey\"\n+FOREIGN KEY (\"reviewerId\") REFERENCES \"User\"(\"id\")\n+ON DELETE SET NULL ON UPDATE CASCADE;\n+\n+-- Add index for reviewer\n+CREATE INDEX IF NOT EXISTS \"StoreListingVersion_reviewerId_idx\"\n+    ON \"StoreListingVersion\"(\"reviewerId\");\n+\n+-- DropIndex\n+DROP INDEX \"StoreListingVersion_agentId_agentVersion_key\";\n+\n+-- RenameIndex\n+ALTER INDEX \"StoreListingVersion_storeListingId_submissionStatus_isAvailable_idx\"\n+RENAME TO \"StoreListingVersion_storeListingId_submissionStatus_isAvail_idx\";\n+\n+-- Recreate the views with updated column references\n+\n+-- 1. Recreate StoreSubmission view\n+CREATE VIEW \"StoreSubmission\" AS\n+SELECT\n+    sl.id AS listing_id,\n+    sl.\"owningUserId\" AS user_id,\n+    slv.\"agentId\" AS agent_id,\n+    slv.version AS agent_version,\n+    sl.slug,\n+    COALESCE(slv.name, '') AS name,\n+    slv.\"subHeading\" AS sub_heading,\n+    slv.description,\n+    slv.\"imageUrls\" AS image_urls,\n+    slv.\"submittedAt\" AS date_submitted,\n+    slv.\"submissionStatus\" AS status,\n+    COALESCE(ar.run_count, 0::bigint) AS runs,\n+    COALESCE(avg(sr.score::numeric), 0.0)::double precision AS rating,\n+    -- Add the additional fields needed by the Pydantic model\n+    slv.id AS store_listing_version_id,\n+    slv.\"reviewerId\" AS reviewer_id,\n+    slv.\"reviewComments\" AS review_comments,\n+    slv.\"internalComments\" AS internal_comments,\n+    slv.\"reviewedAt\" AS reviewed_at,\n+    slv.\"changesSummary\" AS changes_summary\n+FROM \"StoreListing\" sl\n+    JOIN \"StoreListingVersion\" slv ON slv.\"storeListingId\" = sl.id\n+    LEFT JOIN \"StoreListingReview\" sr ON sr.\"storeListingVersionId\" = slv.id\n+    LEFT JOIN (\n+        SELECT \"AgentGraphExecution\".\"agentGraphId\", count(*) AS run_count\n+        FROM \"AgentGraphExecution\"\n+        GROUP BY \"AgentGraphExecution\".\"agentGraphId\"\n+    ) ar ON ar.\"agentGraphId\" = slv.\"agentId\"\n+WHERE sl.\"isDeleted\" = false\n+GROUP BY sl.id, sl.\"owningUserId\", slv.id, slv.\"agentId\", slv.version, sl.slug, slv.name,\n+         slv.\"subHeading\", slv.description, slv.\"imageUrls\", slv.\"submittedAt\",\n+         slv.\"submissionStatus\", slv.\"reviewerId\", slv.\"reviewComments\", slv.\"internalComments\",\n+         slv.\"reviewedAt\", slv.\"changesSummary\", ar.run_count;\n+\n+-- 2. Recreate StoreAgent view\n+CREATE VIEW \"StoreAgent\" AS\n+WITH reviewstats AS (\n+    SELECT sl_1.id AS \"storeListingId\",\n+           count(sr.id) AS review_count,\n+           avg(sr.score::numeric) AS avg_rating\n+      FROM \"StoreListing\" sl_1\n+      JOIN \"StoreListingVersion\" slv_1\n+           ON slv_1.\"storeListingId\" = sl_1.id\n+      JOIN \"StoreListingReview\" sr\n+           ON sr.\"storeListingVersionId\" = slv_1.id\n+     WHERE sl_1.\"isDeleted\" = false\n+     GROUP BY sl_1.id\n+), agentruns AS (\n+    SELECT \"AgentGraphExecution\".\"agentGraphId\",\n+           count(*) AS run_count\n+      FROM \"AgentGraphExecution\"\n+     GROUP BY \"AgentGraphExecution\".\"agentGraphId\"\n+)\n+SELECT sl.id AS listing_id,\n+       slv.id AS \"storeListingVersionId\",\n+       slv.\"createdAt\" AS updated_at,\n+       sl.slug,\n+       COALESCE(slv.name, '') AS agent_name,\n+       slv.\"videoUrl\" AS agent_video,\n+       COALESCE(slv.\"imageUrls\", ARRAY[]::text[]) AS agent_image,\n+       slv.\"isFeatured\" AS featured,\n+       p.username AS creator_username,\n+       p.\"avatarUrl\" AS creator_avatar,\n+       slv.\"subHeading\" AS sub_heading,\n+       slv.description,\n+       slv.categories,\n+       COALESCE(ar.run_count, 0::bigint) AS runs,\n+       COALESCE(rs.avg_rating, 0.0)::double precision AS rating,\n+       array_agg(DISTINCT slv.version::text) AS versions\n+  FROM \"StoreListing\" sl\n+  JOIN \"AgentGraph\" a\n+       ON sl.\"agentId\" = a.id\n+      AND sl.\"agentVersion\" = a.version\n+  LEFT JOIN \"Profile\" p\n+       ON sl.\"owningUserId\" = p.\"userId\"\n+  LEFT JOIN \"StoreListingVersion\" slv\n+       ON slv.\"storeListingId\" = sl.id\n+  LEFT JOIN reviewstats rs\n+       ON sl.id = rs.\"storeListingId\"\n+  LEFT JOIN agentruns ar\n+       ON a.id = ar.\"agentGraphId\"\n+ WHERE sl.\"isDeleted\" = false\n+   AND sl.\"hasApprovedVersion\" = true\n+   AND slv.\"submissionStatus\" = 'APPROVED'\n+ GROUP BY sl.id, slv.id, sl.slug, slv.\"createdAt\", slv.name, slv.\"videoUrl\",\n+          slv.\"imageUrls\", slv.\"isFeatured\", p.username, p.\"avatarUrl\",\n+          slv.\"subHeading\", slv.description, slv.categories, ar.run_count,\n+          rs.avg_rating;\n+\n+-- 3. Recreate Creator view\n+CREATE VIEW \"Creator\" AS\n+WITH agentstats AS (\n+    SELECT p_1.username,\n+           count(DISTINCT sl.id) AS num_agents,\n+           avg(COALESCE(sr.score, 0)::numeric) AS agent_rating,\n+           sum(COALESCE(age.run_count, 0::bigint)) AS agent_runs\n+      FROM \"Profile\" p_1\n+ LEFT JOIN \"StoreListing\" sl\n+        ON sl.\"owningUserId\" = p_1.\"userId\"\n+ LEFT JOIN \"StoreListingVersion\" slv\n+        ON slv.\"storeListingId\" = sl.id\n+ LEFT JOIN \"StoreListingReview\" sr\n+        ON sr.\"storeListingVersionId\" = slv.id\n+ LEFT JOIN (\n+          SELECT \"AgentGraphExecution\".\"agentGraphId\",\n+                 count(*) AS run_count\n+            FROM \"AgentGraphExecution\"\n+        GROUP BY \"AgentGraphExecution\".\"agentGraphId\"\n+      ) age ON age.\"agentGraphId\" = sl.\"agentId\"\n+     WHERE sl.\"isDeleted\" = false\n+       AND sl.\"hasApprovedVersion\" = true\n+       AND slv.\"submissionStatus\" = 'APPROVED'\n+  GROUP BY p_1.username\n+)\n+SELECT p.username,\n+       p.name,\n+       p.\"avatarUrl\" AS avatar_url,\n+       p.description,\n+       array_agg(DISTINCT cats.c) FILTER (WHERE cats.c IS NOT NULL) AS top_categories,\n+       p.links,\n+       p.\"isFeatured\" AS is_featured,\n+       COALESCE(ast.num_agents, 0::bigint) AS num_agents,\n+       COALESCE(ast.agent_rating, 0.0) AS agent_rating,\n+       COALESCE(ast.agent_runs, 0::numeric) AS agent_runs\n+  FROM \"Profile\" p\n+  LEFT JOIN agentstats ast\n+         ON ast.username = p.username\n+  LEFT JOIN LATERAL (\n+          SELECT unnest(slv.categories) AS c\n+            FROM \"StoreListing\" sl\n+            JOIN \"StoreListingVersion\" slv\n+                 ON slv.\"storeListingId\" = sl.id\n+           WHERE sl.\"owningUserId\" = p.\"userId\"\n+             AND sl.\"isDeleted\" = false\n+             AND sl.\"hasApprovedVersion\" = true\n+             AND slv.\"submissionStatus\" = 'APPROVED'\n+       ) cats ON true\n+ GROUP BY p.username, p.name, p.\"avatarUrl\", p.description, p.links,\n+          p.\"isFeatured\", ast.num_agents, ast.agent_rating, ast.agent_runs;\n+\n+COMMIT;\ndiff --git a/autogpt_platform/backend/schema.prisma b/autogpt_platform/backend/schema.prisma\nindex 6defe1ed5e8a..5a34024eb9df 100644\n--- a/autogpt_platform/backend/schema.prisma\n+++ b/autogpt_platform/backend/schema.prisma\n@@index([id])\n   @@index([email])\n@@index([userId])\n }\n@@ -86,13 +86,13 @@ model AgentGraph {\n   name        String?\n   description String?\n \n-  isActive    Boolean @default(true)\n+  isActive Boolean @default(true)\n \n   // Link to User model\n   userId String\n   // FIX: Do not cascade delete the agent when the user is deleted\n   // This allows us to delete user data with deleting the agent which maybe in use by other users\n-  user   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\n+  User   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\n \n   AgentNodes          AgentNode[]\n   AgentGraphExecution AgentGraphExecution[]\n@@ -100,7 +100,7 @@ model AgentGraph {\n   AgentPreset         AgentPreset[]\n   LibraryAgent        LibraryAgent[]\n   StoreListing        StoreListing[]\n-  StoreListingVersion StoreListingVersion?\n+  StoreListingVersion StoreListingVersion[]\n \n   @@id(name: \"graphVersionId\", [id, version])\n   @@index([userId, isActive])\n@@ -175,11 +175,11 @@ model UserNotificationBatch {\n   updatedAt DateTime @default(now()) @updatedAt\n \n   userId String\n-  user   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\n+  User   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\n \n   type NotificationType\n \n-  notifications NotificationEvent[]\n+  Notifications NotificationEvent[]\n \n   // Each user can only have one batch of a notification type at a time\n   @@unique([userId, type])\n@@index([userId])\n }\n@@ -470,7 +470,7 @@ model CreditTransaction {\n   createdAt      DateTime @default(now())\n \n   userId String\n-  user   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\n+  User   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\n \n   amount Int\n   type   CreditTransactionType\n@@ -579,19 +579,25 @@ view StoreAgent {\n }\n \n view StoreSubmission {\n-  listing_id     String           @id\n-  user_id        String\n-  slug           String\n-  name           String\n-  sub_heading    String\n-  description    String\n-  image_urls     String[]\n-  date_submitted DateTime\n-  status         SubmissionStatus\n-  runs           Int\n-  rating         Float\n-  agent_id       String\n-  agent_version  Int\n+  listing_id               String           @id\n+  user_id                  String\n+  slug                     String\n+  name                     String\n+  sub_heading              String\n+  description              String\n+  image_urls               String[]\n+  date_submitted           DateTime\n+  status                   SubmissionStatus\n+  runs                     Int\n+  rating                   Float\n+  agent_id                 String\n+  agent_version            Int\n+  store_listing_version_id String\n+  reviewer_id              String?\n+  review_comments          String?\n+  internal_comments        String?\n+  reviewed_at              DateTime?\n+  changes_summary          String?\n \n   // Index or unique are not applied to views\n }\n@@ -601,11 +607,18 @@ model StoreListing {\n   createdAt DateTime @default(now())\n   updatedAt DateTime @default(now()) @updatedAt\n \n-  isDeleted  Boolean @default(false)\n-  // Not needed but makes lookups faster\n-  isApproved Boolean @default(false)\n+  isDeleted          Boolean @default(false)\n+  // Whether any version has been approved and is available for display\n+  hasApprovedVersion Boolean @default(false)\n+\n+  // URL-friendly identifier for this agent (moved from StoreListingVersion)\n+  slug String\n \n-  // The agent link here is only so we can do lookup on agentId, for the listing the StoreListingVersion is used.\n+  // The currently active version that should be shown to users\n+  activeVersionId String?              @unique\n+  ActiveVersion   StoreListingVersion? @relation(\"ActiveVersion\", fields: [activeVersionId], references: [id])\n+\n+  // The agent link here is only so we can do lookup on agentId\n   agentId      String\n   agentVersion Int\n   Agent        AgentGraph @relation(fields: [agentId, agentVersion], references: [id, version], onDelete: Cascade)\n@@ -613,14 +626,14 @@ model StoreListing {\n   owningUserId String\n   OwningUser   User   @relation(fields: [owningUserId], references: [id])\n \n-  StoreListingVersions   StoreListingVersion[]\n-  StoreListingSubmission StoreListingSubmission[]\n+  // Relations\n+  Versions StoreListingVersion[] @relation(\"ListingVersions\")\n \n   // Unique index on agentId to ensure only one listing per agent, regardless of number of versions the agent has.\n   @@unique([agentId])\n-  @@index([owningUserId])\n+  @@unique([owningUserId, slug])\n   // Used in the view query\n-  @@index([isDeleted, isApproved])\n+  @@index([isDeleted, hasApprovedVersion])\n }\n \n model StoreListingVersion {\n@@unique([agentId, agentVersion])\n+  // Reviews for this specific version\n+  Reviews StoreListingReview[]\n+\n+  @@unique([storeListingId, version])\n+  @@index([storeListingId, submissionStatus, isAvailable])\n+  @@index([submissionStatus])\n+  @@index([reviewerId])\n+  @@index([agentId, agentVersion]) // Non-unique index for efficient lookups\n }\n \n model StoreListingReview {\n@@unique([storeListingVersionId])\n-  @@index([storeListingId])\n+  DRAFT // Being prepared, not yet submitted\n+  PENDING // Submitted, awaiting review\n+  APPROVED // Reviewed and approved\n+  REJECTED // Reviewed and rejected\n }\n \n enum APIKeyPermission {\n@@index([key])\n   @@index([prefix])\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR generally adheres to guidelines, but several key issues need attention. The title includes a correct scope, and the changes are well-documented. However, the test plan references a private document, hindering verification and transparency. Additionally, the checklist remains partially completed without marking sections as not applicable, particularly in the configuration section. Lastly, while user_id handling seems appropriate in data files, specific details about these changes are lacking. Addressing these points will strengthen the PR's integrity before merging."
      }
    ],
    "codediff_length": 43994
  },
  {
    "index": 4,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/autogpt/scripts/llamafile/serve.py b/autogpt/scripts/llamafile/serve.py\nnew file mode 100755\nindex 000000000000..431fa253f19f\n--- /dev/null\n+++ b/autogpt/scripts/llamafile/serve.py\n@@ -0,0 +1,165 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Use llamafile to serve a (quantized) mistral-7b-instruct-v0.2 model\n+\n+Usage:\n+  cd <repo-root>/autogpt\n+  ./scripts/llamafile/serve.py\n+\"\"\"\n+\n+import os\n+import platform\n+import subprocess\n+from pathlib import Path\n+from typing import Optional\n+\n+import click\n+\n+LLAMAFILE = Path(\"mistral-7b-instruct-v0.2.Q5_K_M.llamafile\")\n+LLAMAFILE_URL = f\"https://huggingface.co/jartine/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/{LLAMAFILE.name}\"  # noqa\n+LLAMAFILE_EXE = Path(\"llamafile.exe\")\n+LLAMAFILE_EXE_URL = \"https://github.com/Mozilla-Ocho/llamafile/releases/download/0.8.6/llamafile-0.8.6\"  # noqa\n+\n+\n+@click.command()\n+@click.option(\n+    \"--llamafile\",\n+    type=click.Path(dir_okay=False, path_type=Path),\n+    help=f\"Name of the llamafile to serve. Default: {LLAMAFILE.name}\",\n+)\n+@click.option(\"--llamafile_url\", help=\"Download URL for the llamafile you want to use\")\n+@click.option(\n+    \"--host\", help=\"Specify the address for the llamafile server to listen on\"\n+)\n+@click.option(\n+    \"--port\", type=int, help=\"Specify the port for the llamafile server to listen on\"\n+)\n+@click.option(\n+    \"--force-gpu\",\n+    is_flag=True,\n+    hidden=platform.system() != \"Darwin\",\n+    help=\"Run the model using only the GPU (AMD or Nvidia). \"\n+    \"Otherwise, both CPU and GPU may be (partially) used.\",\n+)\n+def main(\n+    llamafile: Optional[Path] = None,\n+    llamafile_url: Optional[str] = None,\n+    host: Optional[str] = None,\n+    port: Optional[int] = None,\n+    force_gpu: bool = False,\n+):\n+    print(f\"type(llamafile) = {type(llamafile)}\")\n+    if not llamafile:\n+        if not llamafile_url:\n+            llamafile = LLAMAFILE\n+        else:\n+            llamafile = Path(llamafile_url.rsplit(\"/\", 1)[1])\n+            if llamafile.suffix != \".llamafile\":\n+                click.echo(\n+                    click.style(\n+                        \"The given URL does not end with '.llamafile' -> \"\n+                        \"can't get filename from URL. \"\n+                        \"Specify the filename using --llamafile.\",\n+                        fg=\"red\",\n+                    ),\n+                    err=True,\n+                )\n+                return\n+\n+    if llamafile == LLAMAFILE and not llamafile_url:\n+        llamafile_url = LLAMAFILE_URL\n+    elif llamafile_url != LLAMAFILE_URL:\n+        if not click.prompt(\n+            click.style(\n+                \"You seem to have specified a different URL for the default model \"\n+                f\"({llamafile.name}). Are you sure this is correct? \"\n+                \"If you want to use a different model, also specify --llamafile.\",\n+                fg=\"yellow\",\n+            ),\n+            type=bool,\n+        ):\n+            return\n+\n+    # Go to autogpt/scripts/llamafile/\n+    os.chdir(Path(__file__).resolve().parent)\n+\n+    on_windows = platform.system() == \"Windows\"\n+\n+    if not llamafile.is_file():\n+        if not llamafile_url:\n+            click.echo(\n+                click.style(\n+                    \"Please use --lamafile_url to specify a download URL for \"\n+                    f\"'{llamafile.name}'. \"\n+                    \"This will only be necessary once, so we can download the model.\",\n+                    fg=\"red\",\n+                ),\n+                err=True,\n+            )\n+            return\n+\n+        download_file(llamafile_url, llamafile)\n+\n+        if not on_windows:\n+            llamafile.chmod(0o755)\n+            subprocess.run([llamafile, \"--version\"], check=True)\n+\n+    if not on_windows:\n+        base_command = [f\"./{llamafile}\"]\n+    else:\n+        # Windows does not allow executables over 4GB, so we have to download a\n+        # model-less llamafile.exe and run that instead.\n+        if not LLAMAFILE_EXE.is_file():\n+            download_file(LLAMAFILE_EXE_URL, LLAMAFILE_EXE)\n+            LLAMAFILE_EXE.chmod(0o755)\n+            subprocess.run([f\".\\\\{LLAMAFILE_EXE}\", \"--version\"], check=True)\n+\n+        base_command = [f\".\\\\{LLAMAFILE_EXE}\", \"-m\", llamafile]\n+\n+    if host:\n+        base_command.extend([\"--host\", host])\n+    if port:\n+        base_command.extend([\"--port\", str(port)])\n+    if force_gpu:\n+        base_command.extend([\"-ngl\", \"9999\"])\n+\n+    subprocess.run(\n+        [\n+            *base_command,\n+            \"--server\",\n+            \"--nobrowser\",\n+            \"--ctx-size\",\n+            \"0\",\n+            \"--n-predict\",\n+            \"1024\",\n+        ],\n+        check=True,\n+    )\n+\n+    # note: --ctx-size 0 means the prompt context size will be set directly from the\n+    # underlying model configuration. This may cause slow response times or consume\n+    # a lot of memory.\n+\n+\n+def download_file(url: str, to_file: Path) -> None:\n+    print(f\"Downloading {to_file.name}...\")\n+    import urllib.request\n+\n+    urllib.request.urlretrieve(url, to_file, reporthook=report_download_progress)\n+    print()\n+\n+\n+def report_download_progress(chunk_number: int, chunk_size: int, total_size: int):\n+    if total_size != -1:\n+        downloaded_size = chunk_number * chunk_size\n+        percent = min(1, downloaded_size / total_size)\n+        bar = \"#\" * int(40 * percent)\n+        print(\n+            f\"\\rDownloading: [{bar:<40}] {percent:.0%}\"\n+            f\" - {downloaded_size/1e6:.1f}/{total_size/1e6:.1f} MB\",\n+            end=\"\",\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/docs/content/AutoGPT/setup/index.md b/docs/content/AutoGPT/setup/index.md\nindex 4fcdbf1cac44..78b9632ac19f 100644\n--- a/docs/content/AutoGPT/setup/index.md\n+++ b/docs/content/AutoGPT/setup/index.md\n@@ -198,3 +198,66 @@ If you don't know which to choose, you can safely go with OpenAI*.\n \n [groq/api-keys]: https://console.groq.com/keys\n [groq/models]: https://console.groq.com/docs/models\n+\n+\n+### Llamafile\n+\n+With llamafile you can run models locally, which means no need to set up billing,\n+and guaranteed data privacy.\n+\n+For more information and in-depth documentation, check out the [llamafile documentation].\n+\n+!!! warning\n+    At the moment, llamafile only serves one model at a time. This means you can not\n+    set `SMART_LLM` and `FAST_LLM` to two different llamafile models.\n+\n+!!! warning\n+    Due to the issues linked below, llamafiles don't work on WSL. To use a llamafile\n+    with AutoGPT in WSL, you will have to run the llamafile in Windows (outside WSL).\n+\n+    <details>\n+    <summary>Instructions</summary>\n+\n+    1. Get the `llamafile/serve.py` script through one of these two ways:\n+        1. Clone the AutoGPT repo somewhere in your Windows environment,\n+           with the script located at `autogpt/scripts/llamafile/serve.py`\n+        2. Download just the [serve.py] script somewhere in your Windows environment\n+    2. Make sure you have `click` installed: `pip install click`\n+    3. Run `ip route | grep default | awk '{print $3}'` *inside WSL* to get the address\n+       of the WSL host machine\n+    4. Run `python3 serve.py --host {WSL_HOST_ADDR}`, where `{WSL_HOST_ADDR}`\n+       is the address you found at step 3.\n+       If port 8080 is taken, also specify a different port using `--port {PORT}`.\n+    5. In WSL, set `LLAMAFILE_API_BASE=http://{WSL_HOST_ADDR}:8080/v1` in your `.env`.\n+    6. Follow the rest of the regular instructions below.\n+\n+    [serve.py]: https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpt/scripts/llamafile/serve.py\n+    </details>\n+\n+    * [Mozilla-Ocho/llamafile#356](https://github.com/Mozilla-Ocho/llamafile/issues/356)\n+    * [Mozilla-Ocho/llamafile#100](https://github.com/Mozilla-Ocho/llamafile/issues/100)\n+\n+!!! note\n+    These instructions will download and use `mistral-7b-instruct-v0.2.Q5_K_M.llamafile`.\n+    `mistral-7b-instruct-v0.2` is currently the only tested and supported model.\n+    If you want to try other models, you'll have to add them to `LlamafileModelName` in\n+    [`llamafile.py`][forge/llamafile.py].\n+    For optimal results, you may also have to add some logic to adapt the message format,\n+    like `LlamafileProvider._adapt_chat_messages_for_mistral_instruct(..)` does.\n+\n+1. Run the llamafile serve script:\n+   ```shell\n+   python3 ./scripts/llamafile/serve.py\n+   ```\n+   The first time this is run, it will download a file containing the model + runtime,\n+   which may take a while and a few gigabytes of disk space.\n+\n+   To force GPU acceleration, add `--use-gpu` to the command.\n+\n+3. In `.env`, set `SMART_LLM`/`FAST_LLM` or both to `mistral-7b-instruct-v0.2`\n+\n+4. If the server is running on different address than `http://localhost:8080/v1`,\n+   set `LLAMAFILE_API_BASE` in `.env` to the right base URL\n+\n+[llamafile documentation]: https://github.com/Mozilla-Ocho/llamafile#readme\n+[forge/llamafile.py]: https://github.com/Significant-Gravitas/AutoGPT/blob/master/forge/forge/llm/providers/llamafile/llamafile.py\ndiff --git a/forge/forge/llm/providers/llamafile/llamafile.py b/forge/forge/llm/providers/llamafile/llamafile.py\nnew file mode 100644\nindex 000000000000..7eb7afaafb10\n--- /dev/null\n+++ b/forge/forge/llm/providers/llamafile/llamafile.py\n@@ -0,0 +1,351 @@\n+import enum\n+import logging\n+import re\n+from pathlib import Path\n+from typing import Any, Iterator, Optional, Sequence\n+\n+import requests\n+from openai.types.chat import (\n+    ChatCompletionMessage,\n+    ChatCompletionMessageParam,\n+    CompletionCreateParams,\n+)\n+from pydantic import SecretStr\n+\n+from forge.json.parsing import json_loads\n+from forge.models.config import UserConfigurable\n+\n+from .._openai_base import BaseOpenAIChatProvider\n+from ..schema import (\n+    AssistantToolCall,\n+    AssistantToolCallDict,\n+    ChatMessage,\n+    ChatModelInfo,\n+    CompletionModelFunction,\n+    ModelProviderConfiguration,\n+    ModelProviderCredentials,\n+    ModelProviderName,\n+    ModelProviderSettings,\n+    ModelTokenizer,\n+)\n+\n+\n+class LlamafileModelName(str, enum.Enum):\n+    MISTRAL_7B_INSTRUCT = \"mistral-7b-instruct-v0.2\"\n+\n+\n+LLAMAFILE_CHAT_MODELS = {\n+    info.name: info\n+    for info in [\n+        ChatModelInfo(\n+            name=LlamafileModelName.MISTRAL_7B_INSTRUCT,\n+            provider_name=ModelProviderName.LLAMAFILE,\n+            prompt_token_cost=0.0,\n+            completion_token_cost=0.0,\n+            max_tokens=32768,\n+            has_function_call_api=False,\n+        ),\n+    ]\n+}\n+\n+LLAMAFILE_EMBEDDING_MODELS = {}\n+\n+\n+class LlamafileConfiguration(ModelProviderConfiguration):\n+    # TODO: implement 'seed' across forge.llm.providers\n+    seed: Optional[int] = None\n+\n+\n+class LlamafileCredentials(ModelProviderCredentials):\n+    api_key: Optional[SecretStr] = SecretStr(\"sk-no-key-required\")\n+    api_base: SecretStr = UserConfigurable(  # type: ignore\n+        default=SecretStr(\"http://localhost:8080/v1\"), from_env=\"LLAMAFILE_API_BASE\"\n+    )\n+\n+    def get_api_access_kwargs(self) -> dict[str, str]:\n+        return {\n+            k: v.get_secret_value()\n+            for k, v in {\n+                \"api_key\": self.api_key,\n+                \"base_url\": self.api_base,\n+            }.items()\n+            if v is not None\n+        }\n+\n+\n+class LlamafileSettings(ModelProviderSettings):\n+    configuration: LlamafileConfiguration  # type: ignore\n+    credentials: Optional[LlamafileCredentials] = None  # type: ignore\n+\n+\n+class LlamafileTokenizer(ModelTokenizer[int]):\n+    def __init__(self, credentials: LlamafileCredentials):\n+        self._credentials = credentials\n+\n+    @property\n+    def _tokenizer_base_url(self):\n+        # The OpenAI-chat-compatible base url should look something like\n+        # 'http://localhost:8080/v1' but the tokenizer endpoint is\n+        # 'http://localhost:8080/tokenize'. So here we just strip off the '/v1'.\n+        api_base = self._credentials.api_base.get_secret_value()\n+        return api_base.strip(\"/v1\")\n+\n+    def encode(self, text: str) -> list[int]:\n+        response = requests.post(\n+            url=f\"{self._tokenizer_base_url}/tokenize\", json={\"content\": text}\n+        )\n+        response.raise_for_status()\n+        return response.json()[\"tokens\"]\n+\n+    def decode(self, tokens: list[int]) -> str:\n+        response = requests.post(\n+            url=f\"{self._tokenizer_base_url}/detokenize\", json={\"tokens\": tokens}\n+        )\n+        response.raise_for_status()\n+        return response.json()[\"content\"]\n+\n+\n+class LlamafileProvider(\n+    BaseOpenAIChatProvider[LlamafileModelName, LlamafileSettings],\n+    # TODO: add and test support for embedding models\n+    # BaseOpenAIEmbeddingProvider[LlamafileModelName, LlamafileSettings],\n+):\n+    EMBEDDING_MODELS = LLAMAFILE_EMBEDDING_MODELS\n+    CHAT_MODELS = LLAMAFILE_CHAT_MODELS\n+    MODELS = {**CHAT_MODELS, **EMBEDDING_MODELS}\n+\n+    default_settings = LlamafileSettings(\n+        name=\"llamafile_provider\",\n+        description=(\n+            \"Provides chat completion and embedding services \"\n+            \"through a llamafile instance\"\n+        ),\n+        configuration=LlamafileConfiguration(),\n+    )\n+\n+    _settings: LlamafileSettings\n+    _credentials: LlamafileCredentials\n+    _configuration: LlamafileConfiguration\n+\n+    async def get_available_models(self) -> Sequence[ChatModelInfo[LlamafileModelName]]:\n+        _models = (await self._client.models.list()).data\n+        # note: at the moment, llamafile only serves one model at a time (so this\n+        # list will only ever have one value). however, in the future, llamafile\n+        # may support multiple models, so leaving this method as-is for now.\n+        self._logger.debug(f\"Retrieved llamafile models: {_models}\")\n+\n+        clean_model_ids = [clean_model_name(m.id) for m in _models]\n+        self._logger.debug(f\"Cleaned llamafile model IDs: {clean_model_ids}\")\n+\n+        return [\n+            LLAMAFILE_CHAT_MODELS[id]\n+            for id in clean_model_ids\n+            if id in LLAMAFILE_CHAT_MODELS\n+        ]\n+\n+    def get_tokenizer(self, model_name: LlamafileModelName) -> LlamafileTokenizer:\n+        return LlamafileTokenizer(self._credentials)\n+\n+    def count_message_tokens(\n+        self,\n+        messages: ChatMessage | list[ChatMessage],\n+        model_name: LlamafileModelName,\n+    ) -> int:\n+        if isinstance(messages, ChatMessage):\n+            messages = [messages]\n+\n+        if model_name == LlamafileModelName.MISTRAL_7B_INSTRUCT:\n+            # For mistral-instruct, num added tokens depends on if the message\n+            # is a prompt/instruction or an assistant-generated message.\n+            # - prompt gets [INST], [/INST] added and the first instruction\n+            # begins with '<s>' ('beginning-of-sentence' token).\n+            # - assistant-generated messages get '</s>' added\n+            # see: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n+            #\n+            prompt_added = 1  # one for '<s>' token\n+            assistant_num_added = 0\n+            ntokens = 0\n+            for message in messages:\n+                if (\n+                    message.role == ChatMessage.Role.USER\n+                    # note that 'system' messages will get converted\n+                    # to 'user' messages before being sent to the model\n+                    or message.role == ChatMessage.Role.SYSTEM\n+                ):\n+                    # 5 tokens for [INST], [/INST], which actually get\n+                    # tokenized into \"[, INST, ]\" and \"[, /, INST, ]\"\n+                    # by the mistral tokenizer\n+                    prompt_added += 5\n+                elif message.role == ChatMessage.Role.ASSISTANT:\n+                    assistant_num_added += 1  # for </s>\n+                else:\n+                    raise ValueError(\n+                        f\"{model_name} does not support role: {message.role}\"\n+                    )\n+\n+                ntokens += self.count_tokens(message.content, model_name)\n+\n+            total_token_count = prompt_added + assistant_num_added + ntokens\n+            return total_token_count\n+\n+        else:\n+            raise NotImplementedError(\n+                f\"count_message_tokens not implemented for model {model_name}\"\n+            )\n+\n+    def _get_chat_completion_args(\n+        self,\n+        prompt_messages: list[ChatMessage],\n+        model: LlamafileModelName,\n+        functions: list[CompletionModelFunction] | None = None,\n+        max_output_tokens: int | None = None,\n+        **kwargs,\n+    ) -> tuple[\n+        list[ChatCompletionMessageParam], CompletionCreateParams, dict[str, Any]\n+    ]:\n+        messages, completion_kwargs, parse_kwargs = super()._get_chat_completion_args(\n+            prompt_messages, model, functions, max_output_tokens, **kwargs\n+        )\n+\n+        if model == LlamafileModelName.MISTRAL_7B_INSTRUCT:\n+            messages = self._adapt_chat_messages_for_mistral_instruct(messages)\n+\n+        if \"seed\" not in kwargs and self._configuration.seed is not None:\n+            completion_kwargs[\"seed\"] = self._configuration.seed\n+\n+        # Convert all messages with content blocks to simple text messages\n+        for message in messages:\n+            if isinstance(content := message.get(\"content\"), list):\n+                message[\"content\"] = \"\\n\\n\".join(\n+                    b[\"text\"]\n+                    for b in content\n+                    if b[\"type\"] == \"text\"\n+                    # FIXME: add support for images through image_data completion kwarg\n+                )\n+\n+        return messages, completion_kwargs, parse_kwargs\n+\n+    def _adapt_chat_messages_for_mistral_instruct(\n+        self, messages: list[ChatCompletionMessageParam]\n+    ) -> list[ChatCompletionMessageParam]:\n+        \"\"\"\n+        Munge the messages to be compatible with the mistral-7b-instruct chat\n+        template, which:\n+        - only supports 'user' and 'assistant' roles.\n+        - expects messages to alternate between user/assistant roles.\n+\n+        See details here:\n+        https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2#instruction-format\n+        \"\"\"\n+        adapted_messages: list[ChatCompletionMessageParam] = []\n+        for message in messages:\n+            # convert 'system' role to 'user' role as mistral-7b-instruct does\n+            # not support 'system'\n+            if message[\"role\"] == ChatMessage.Role.SYSTEM:\n+                message[\"role\"] = ChatMessage.Role.USER\n+\n+            if (\n+                len(adapted_messages) == 0\n+                or message[\"role\"] != (last_message := adapted_messages[-1])[\"role\"]\n+            ):\n+                adapted_messages.append(message)\n+            else:\n+                if not message.get(\"content\"):\n+                    continue\n+\n+                # if the curr message has the same role as the previous one,\n+                # concat the current message content to the prev message\n+                if message[\"role\"] == \"user\" and last_message[\"role\"] == \"user\":\n+                    # user messages can contain other types of content blocks\n+                    if not isinstance(last_message[\"content\"], list):\n+                        last_message[\"content\"] = [\n+                            {\"type\": \"text\", \"text\": last_message[\"content\"]}\n+                        ]\n+\n+                    last_message[\"content\"].extend(\n+                        message[\"content\"]\n+                        if isinstance(message[\"content\"], list)\n+                        else [{\"type\": \"text\", \"text\": message[\"content\"]}]\n+                    )\n+                elif message[\"role\"] != \"user\" and last_message[\"role\"] != \"user\":\n+                    last_message[\"content\"] = (\n+                        (last_message.get(\"content\") or \"\")\n+                        + \"\\n\\n\"\n+                        + (message.get(\"content\") or \"\")\n+                    ).strip()\n+\n+        return adapted_messages\n+\n+    def _parse_assistant_tool_calls(\n+        self,\n+        assistant_message: ChatCompletionMessage,\n+        compat_mode: bool = False,\n+        **kwargs,\n+    ):\n+        tool_calls: list[AssistantToolCall] = []\n+        parse_errors: list[Exception] = []\n+\n+        if compat_mode and assistant_message.content:\n+            try:\n+                tool_calls = list(\n+                    _tool_calls_compat_extract_calls(assistant_message.content)\n+                )\n+            except Exception as e:\n+                parse_errors.append(e)\n+\n+        return tool_calls, parse_errors\n+\n+\n+def clean_model_name(model_file: str) -> str:\n+    \"\"\"\n+    Clean up model names:\n+    1. Remove file extension\n+    2. Remove quantization info\n+\n+    Examples:\n+    ```\n+    raw:   'mistral-7b-instruct-v0.2.Q5_K_M.gguf'\n+    clean: 'mistral-7b-instruct-v0.2'\n+\n+    raw: '/Users/kate/models/mistral-7b-instruct-v0.2.Q5_K_M.gguf'\n+    clean:                  'mistral-7b-instruct-v0.2'\n+\n+    raw:   'llava-v1.5-7b-q4.gguf'\n+    clean: 'llava-v1.5-7b'\n+    ```\n+    \"\"\"\n+    name_without_ext = Path(model_file).name.rsplit(\".\", 1)[0]\n+    name_without_Q = re.match(\n+        r\"^[a-zA-Z0-9]+([.\\-](?!([qQ]|B?F)\\d{1,2})[a-zA-Z0-9]+)*\",\n+        name_without_ext,\n+    )\n+    return name_without_Q.group() if name_without_Q else name_without_ext\n+\n+\n+def _tool_calls_compat_extract_calls(response: str) -> Iterator[AssistantToolCall]:\n+    import re\n+    import uuid\n+\n+    logging.debug(f\"Trying to extract tool calls from response:\\n{response}\")\n+\n+    response = response.strip()  # strip off any leading/trailing whitespace\n+    if response.startswith(\"```\"):\n+        # attempt to remove any extraneous markdown artifacts like \"```json\"\n+        response = response.strip(\"```\")\n+        if response.startswith(\"json\"):\n+            response = response.strip(\"json\")\n+        response = response.strip()  # any remaining whitespace\n+\n+    if response[0] == \"[\":\n+        tool_calls: list[AssistantToolCallDict] = json_loads(response)\n+    else:\n+        block = re.search(r\"```(?:tool_calls)?\\n(.*)\\n```\\s*$\", response, re.DOTALL)\n+        if not block:\n+            raise ValueError(\"Could not find tool_calls block in response\")\n+        tool_calls: list[AssistantToolCallDict] = json_loads(block.group(1))\n+\n+    for t in tool_calls:\n+        t[\"id\"] = str(uuid.uuid4())\n+        # t[\"function\"][\"arguments\"] = str(t[\"function\"][\"arguments\"])  # HACK\n+\n+        yield AssistantToolCall.parse_obj(t)\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR introduces significant changes, including new model provider integrations and configuration adjustments. However, it lacks relevant tests, which raises concerns about potential issues. Notably, the `check_model_llamafile` method could expose sensitive information, and the hard-coded `seed` in `_create_chat_completion` limits model variability. I recommend masking sensitive data in error messages and making the seed configurable to enhance security and flexibility. Ensuring comprehensive testing and refining error messaging are also crucial for mitigating user confusion and potential errors."
      }
    ],
    "codediff_length": 22425
  },
  {
    "index": 5,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/scripts/json_parser.py b/scripts/json_parser.py\nnew file mode 100644\nindex 000000000000..8154b584a12e\n--- /dev/null\n+++ b/scripts/json_parser.py\n@@ -0,0 +1,77 @@\n+import dirtyjson\n+from call_ai_function import call_ai_function\n+from config import Config\n+cfg = Config()\n+\n+def fix_and_parse_json(json_str: str, try_to_fix_with_gpt: bool = True):\n+    json_schema = \"\"\"\n+    {\n+    \"command\": {\n+        \"name\": \"command name\",\n+        \"args\":{\n+            \"arg name\": \"value\"\n+        }\n+    },\n+    \"thoughts\":\n+    {\n+        \"text\": \"thought\",\n+        \"reasoning\": \"reasoning\",\n+        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n+        \"criticism\": \"constructive self-criticism\",\n+        \"speak\": \"thoughts summary to say to user\"\n+    }\n+    }\n+    \"\"\"\n+\n+    try:\n+        return dirtyjson.loads(json_str)\n+    except Exception as e:\n+        # Let's do something manually - sometimes GPT responds with something BEFORE the braces:\n+        # \"I'm sorry, I don't understand. Please try again.\"{\"text\": \"I'm sorry, I don't understand. Please try again.\", \"confidence\": 0.0}\n+        # So let's try to find the first brace and then parse the rest of the string\n+        try:\n+          brace_index = json_str.index(\"{\")\n+          json_str = json_str[brace_index:]\n+          last_brace_index = json_str.rindex(\"}\")\n+          json_str = json_str[:last_brace_index+1]\n+          return dirtyjson.loads(json_str)\n+        except Exception as e:\n+          if try_to_fix_with_gpt:\n+            print(f\"Warning: Failed to parse AI output, attempting to fix.\\n If you see this warning frequently, it's likely that your prompt is confusing the AI. Try changing it up slightly.\")\n+            # Now try to fix this up using the ai_functions\n+            ai_fixed_json = fix_json(json_str, json_schema, False)\n+            if ai_fixed_json != \"failed\":\n+              return dirtyjson.loads(ai_fixed_json)\n+            else:\n+              print(f\"Failed to fix ai output, telling the AI.\") # This allows the AI to react to the error message, which usually results in it correcting its ways.\n+              return json_str\n+          else:\n+            raise e\n+        \n+# TODO: Make debug a global config var\n+def fix_json(json_str: str, schema: str, debug=False) -> str:\n+    # Try to fix the JSON using gpt:\n+    function_string = \"def fix_json(json_str: str, schema:str=None) -> str:\"\n+    args = [json_str, schema]\n+    description_string = \"\"\"Fixes the provided JSON string to make it parseable and fully complient with the provided schema.\\n If an object or field specifed in the schema isn't contained within the correct JSON, it is ommited.\\n This function is brilliant at guessing when the format is incorrect.\"\"\"\n+\n+    # If it doesn't already start with a \"`\", add one:\n+    if not json_str.startswith(\"`\"):\n+      json_str = \"```json\\n\" + json_str + \"\\n```\"\n+    result_string = call_ai_function(\n+        function_string, args, description_string, model=cfg.fast_llm_model\n+    )\n+    if debug:\n+        print(\"------------ JSON FIX ATTEMPT ---------------\")\n+        print(f\"Original JSON: {json_str}\")\n+        print(\"-----------\")\n+        print(f\"Fixed JSON: {result_string}\")\n+        print(\"----------- END OF FIX ATTEMPT ----------------\")\n+    try:\n+        return dirtyjson.loads(result_string)\n+    except:\n+        # Get the call stack:\n+        # import traceback\n+        # call_stack = traceback.format_exc()\n+        # print(f\"Failed to fix JSON: '{json_str}' \"+call_stack)\n+        return \"failed\"\ndiff --git a/scripts/main.py b/scripts/main.py\nindex e80bf5d08a14..48093cf458e1 100644\n--- a/scripts/main.py\n+++ b/scripts/main.py\n@@ -103,11 +110,93 @@ def print_assistant_thoughts(assistant_reply):\n         print_to_console(\"Error: Invalid JSON\\n\", Fore.RED, assistant_reply)\n     # All other errors, return \"Error: + error message\"\n     except Exception as e:\n-        print_to_console(\"Error: \\n\", Fore.RED, str(e))\n+        call_stack = traceback.format_exc()\n+        print_to_console(\"Error: \\n\", Fore.RED, call_stack)\n+\n+def load_variables(config_file=\"config.yaml\"):\n+    # Load variables from yaml file if it exists\n+    try:\n+        with open(config_file) as file:\n+            config = yaml.load(file, Loader=yaml.FullLoader)\n+        ai_name = config.get(\"ai_name\")\n+        ai_role = config.get(\"ai_role\")\n+        ai_goals = config.get(\"ai_goals\")\n+    except FileNotFoundError:\n+        ai_name = \"\"\n+        ai_role = \"\"\n+        ai_goals = []\n+\n+    # Prompt the user for input if config file is missing or empty values\n+    if not ai_name:\n+        ai_name = input(\"Name your AI: \")\n+        if ai_name == \"\":\n+            ai_name = \"Entrepreneur-GPT\"\n+\n+    if not ai_role:        \n+        ai_role = input(f\"{ai_name} is: \")\n+        if ai_role == \"\":\n+            ai_role = \"an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.\"\n+\n+    if not ai_goals:\n+        print(\"Enter up to 5 goals for your AI: \")\n+        print(\"For example: \\nIncrease net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'\")\n+        print(\"Enter nothing to load defaults, enter nothing when finished.\")\n+        ai_goals = []\n+        for i in range(5):\n+            ai_goal = input(f\"Goal {i+1}: \")\n+            if ai_goal == \"\":\n+                break\n+            ai_goals.append(ai_goal)\n+        if len(ai_goals) == 0:\n+            ai_goals = [\"Increase net worth\", \"Grow Twitter Account\", \"Develop and manage multiple businesses autonomously\"]\n+         \n+    # Save variables to yaml file\n+    config = {\"ai_name\": ai_name, \"ai_role\": ai_role, \"ai_goals\": ai_goals}\n+    with open(config_file, \"w\") as file:\n+        documents = yaml.dump(config, file)\n+\n+    prompt = data.load_prompt()\n+    prompt_start = \"\"\"Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\"\"\"\n+\n+    # Construct full prompt\n+    full_prompt = f\"You are {ai_name}, {ai_role}\\n{prompt_start}\\n\\nGOALS:\\n\\n\"\n+    for i, goal in enumerate(ai_goals):\n+        full_prompt += f\"{i+1}. {goal}\\n\"\n+\n+    full_prompt += f\"\\n\\n{prompt}\"\n+    return full_prompt\n \n \n def construct_prompt():\n+    config = AIConfig.load()\n+    if config.ai_name:\n+        print_to_console(\n+            f\"Welcome back! \",\n+            Fore.GREEN,\n+            f\"Would you like me to return to being {config.ai_name}?\",\n+            speak_text=True)\n+        should_continue = input(f\"\"\"Continue with the last settings? \n+Name:  {config.ai_name}\n+Role:  {config.ai_role}\n+Goals: {config.ai_goals}  \n+Continue (y/n): \"\"\")\n+        if should_continue.lower() == \"n\":\n+            config = AIConfig()\n+\n+    if not config.ai_name:         \n+        config = prompt_user()\n+        config.save()\n+\n+    # Get rid of this global:\n     global ai_name\n+    ai_name = config.ai_name\n+    \n+    full_prompt = config.construct_full_prompt()\n+    return full_prompt\n+\n+\n+def prompt_user():\n+    ai_name = \"\"\n     # Construct the prompt\n     print_to_console(\n         \"Welcome to Auto-GPT! \",\n\ndiff --git a/scripts/token_counter.py b/scripts/token_counter.py\nnew file mode 100644\nindex 000000000000..a28a9868ed8e\n--- /dev/null\n+++ b/scripts/token_counter.py\n@@ -0,0 +1,57 @@\n+import tiktoken\n+from typing import List, Dict\n+\n+def count_message_tokens(messages : List[Dict[str, str]], model : str = \"gpt-3.5-turbo-0301\") -> int:\n+    \"\"\"\n+    Returns the number of tokens used by a list of messages.\n+\n+    Args:\n+    messages (list): A list of messages, each of which is a dictionary containing the role and content of the message.\n+    model (str): The name of the model to use for tokenization. Defaults to \"gpt-3.5-turbo-0301\".\n+\n+    Returns:\n+    int: The number of tokens used by the list of messages.\n+    \"\"\"\n+    try:\n+        encoding = tiktoken.encoding_for_model(model)\n+    except KeyError:\n+        print(\"Warning: model not found. Using cl100k_base encoding.\")\n+        encoding = tiktoken.get_encoding(\"cl100k_base\")\n+    if model == \"gpt-3.5-turbo\":\n+        # !Node: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n+        return count_message_tokens(messages, model=\"gpt-3.5-turbo-0301\")\n+    elif model == \"gpt-4\":\n+        # !Note: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n+        return count_message_tokens(messages, model=\"gpt-4-0314\")\n+    elif model == \"gpt-3.5-turbo-0301\":\n+        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n+        tokens_per_name = -1  # if there's a name, the role is omitted\n+    elif model == \"gpt-4-0314\":\n+        tokens_per_message = 3\n+        tokens_per_name = 1\n+    else:\n+        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n+    num_tokens = 0\n+    for message in messages:\n+        num_tokens += tokens_per_message\n+        for key, value in message.items():\n+            num_tokens += len(encoding.encode(value))\n+            if key == \"name\":\n+                num_tokens += tokens_per_name\n+    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n+    return num_tokens\n+\n+def count_string_tokens(string: str, model_name: str) -> int:\n+    \"\"\"\n+    Returns the number of tokens in a text string.\n+\n+    Args:\n+    string (str): The text string.\n+    model_name (str): The name of the encoding to use. (e.g., \"gpt-3.5-turbo\")\n+\n+    Returns:\n+    int: The number of tokens in the text string.\n+    \"\"\"\n+    encoding = tiktoken.encoding_for_model(model_name)\n+    num_tokens = len(encoding.encode(string))\n+    return num_tokens\ndiff --git a/tests/json_tests.py b/tests/json_tests.py\nnew file mode 100644\nindex 000000000000..459da610b872\n--- /dev/null\n+++ b/tests/json_tests.py\n@@ -0,0 +1,115 @@\n+import unittest\n+import os\n+import sys\n+# Probably a better way:\n+sys.path.append(os.path.abspath('../scripts'))\n+from json_parser import fix_and_parse_json\n+\n+class TestParseJson(unittest.TestCase):\n+    def test_valid_json(self):\n+        # Test that a valid JSON string is parsed correctly\n+        json_str = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'\n+        obj = fix_and_parse_json(json_str)\n+        self.assertEqual(obj, {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"})\n+    \n+    def test_invalid_json_minor(self):\n+        # Test that an invalid JSON string can be fixed with gpt\n+        json_str = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\",}'\n+        self.assertEqual(fix_and_parse_json(json_str, try_to_fix_with_gpt=False), {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"})\n+    \n+    def test_invalid_json_major_with_gpt(self):\n+        # Test that an invalid JSON string raises an error when try_to_fix_with_gpt is False\n+        json_str = 'BEGIN: \"name\": \"John\" - \"age\": 30 - \"city\": \"New York\" :END'\n+        self.assertEqual(fix_and_parse_json(json_str, try_to_fix_with_gpt=True), {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"})\n+\n+    def test_invalid_json_major_without_gpt(self):\n+        # Test that a REALLY invalid JSON string raises an error when try_to_fix_with_gpt is False\n+        json_str = 'BEGIN: \"name\": \"John\" - \"age\": 30 - \"city\": \"New York\" :END'\n+        # Assert that this raises an exception:\n+        with self.assertRaises(Exception):\n+            fix_and_parse_json(json_str, try_to_fix_with_gpt=False)\n+\n+    def test_invalid_json_leading_sentence_with_gpt(self):\n+        # Test that a REALLY invalid JSON string raises an error when try_to_fix_with_gpt is False\n+        json_str = \"\"\"I suggest we start by browsing the repository to find any issues that we can fix.\n+\n+{\n+    \"command\": {\n+        \"name\": \"browse_website\",\n+        \"args\":{\n+            \"url\": \"https://github.com/Torantulino/Auto-GPT\"\n+        }\n+    },\n+    \"thoughts\":\n+    {\n+        \"text\": \"I suggest we start browsing the repository to find any issues that we can fix.\",\n+        \"reasoning\": \"Browsing the repository will give us an idea of the current state of the codebase and identify any issues that we can address to improve the repo.\",\n+        \"plan\": \"- Look through the repository to find any issues.\\n- Investigate any issues to determine what needs to be fixed\\n- Identify possible solutions to fix the issues\\n- Open Pull Requests with fixes\",\n+        \"criticism\": \"I should be careful while browsing so as not to accidentally introduce any new bugs or issues.\",\n+        \"speak\": \"I will start browsing the repository to find any issues we can fix.\"\n+    }\n+}\"\"\"\n+        good_obj = {\n+          \"command\": {\n+              \"name\": \"browse_website\",\n+              \"args\":{\n+                  \"url\": \"https://github.com/Torantulino/Auto-GPT\"\n+              }\n+          },\n+          \"thoughts\":\n+          {\n+              \"text\": \"I suggest we start browsing the repository to find any issues that we can fix.\",\n+              \"reasoning\": \"Browsing the repository will give us an idea of the current state of the codebase and identify any issues that we can address to improve the repo.\",\n+              \"plan\": \"- Look through the repository to find any issues.\\n- Investigate any issues to determine what needs to be fixed\\n- Identify possible solutions to fix the issues\\n- Open Pull Requests with fixes\",\n+              \"criticism\": \"I should be careful while browsing so as not to accidentally introduce any new bugs or issues.\",\n+              \"speak\": \"I will start browsing the repository to find any issues we can fix.\"\n+          }\n+      }\n+        # Assert that this raises an exception:\n+        self.assertEqual(fix_and_parse_json(json_str, try_to_fix_with_gpt=False), good_obj)\n+\n+\n+\n+    def test_invalid_json_leading_sentence_with_gpt(self):\n+        # Test that a REALLY invalid JSON string raises an error when try_to_fix_with_gpt is False\n+        json_str = \"\"\"I will first need to browse the repository (https://github.com/Torantulino/Auto-GPT) and identify any potential bugs that need fixing. I will use the \"browse_website\" command for this.\n+\n+{\n+    \"command\": {\n+        \"name\": \"browse_website\",\n+        \"args\":{\n+            \"url\": \"https://github.com/Torantulino/Auto-GPT\"\n+        }\n+    },\n+    \"thoughts\":\n+    {\n+        \"text\": \"Browsing the repository to identify potential bugs\",\n+        \"reasoning\": \"Before fixing bugs, I need to identify what needs fixing. I will use the 'browse_website' command to analyze the repository.\",\n+        \"plan\": \"- Analyze the repository for potential bugs and areas of improvement\",\n+        \"criticism\": \"I need to ensure I am thorough and pay attention to detail while browsing the repository.\",\n+        \"speak\": \"I am browsing the repository to identify potential bugs.\"\n+    }\n+}\"\"\"\n+        good_obj = {\n+    \"command\": {\n+        \"name\": \"browse_website\",\n+        \"args\":{\n+            \"url\": \"https://github.com/Torantulino/Auto-GPT\"\n+        }\n+    },\n+    \"thoughts\":\n+    {\n+        \"text\": \"Browsing the repository to identify potential bugs\",\n+        \"reasoning\": \"Before fixing bugs, I need to identify what needs fixing. I will use the 'browse_website' command to analyze the repository.\",\n+        \"plan\": \"- Analyze the repository for potential bugs and areas of improvement\",\n+        \"criticism\": \"I need to ensure I am thorough and pay attention to detail while browsing the repository.\",\n+        \"speak\": \"I am browsing the repository to identify potential bugs.\"\n+    }\n+}\n+        # Assert that this raises an exception:\n+        self.assertEqual(fix_and_parse_json(json_str, try_to_fix_with_gpt=False), good_obj)\n+\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\\ No newline at end of file\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR highlights several key issues. First, the `playsound` library still relies on Python 2; consider transitioning to `PyObjC` for better efficiency. Theres an unhandled `AttributeError` regarding `smart_llm_model` in the `Config` object, suggesting incomplete initialization. The `yaml` module was missing but resolved by installing `PyYAML`, which should have been in `requirements.txt`. Lastly, a suggestion to modify `load_prompt` to use absolute paths is solid, ensuring script compatibility regardless of the working directory. Streamlining context limits will also be crucial for performance."
      }
    ],
    "codediff_length": 15945
  },
  {
    "index": 6,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/autogpt_platform/backend/backend/server/v2/library/routes/agents.py b/autogpt_platform/backend/backend/server/v2/library/routes/agents.py\nindex 899a6f87d444..60f94f519bb7 100644\n--- a/autogpt_platform/backend/backend/server/v2/library/routes/agents.py\n+++ b/autogpt_platform/backend/backend/server/v2/library/routes/agents.py\n@@ -295,81 +289,3 @@ async def fork_library_agent(\n         library_agent_id=library_agent_id,\n         user_id=user_id,\n     )\n-\n-\n-class TriggeredPresetSetupParams(BaseModel):\n-    name: str\n-    description: str = \"\"\n-\n-    trigger_config: dict[str, Any]\n-    agent_credentials: dict[str, CredentialsMetaInput] = Field(default_factory=dict)\n-\n-\n-@router.post(\"/{library_agent_id}/setup-trigger\")\n-async def setup_trigger(\n-    library_agent_id: str = Path(..., description=\"ID of the library agent\"),\n-    params: TriggeredPresetSetupParams = Body(),\n-    user_id: str = Depends(autogpt_auth_lib.depends.get_user_id),\n-) -> library_model.LibraryAgentPreset:\n-    \"\"\"\n-    Sets up a webhook-triggered `LibraryAgentPreset` for a `LibraryAgent`.\n-    Returns the correspondingly created `LibraryAgentPreset` with `webhook_id` set.\n-    \"\"\"\n-    library_agent = await library_db.get_library_agent(\n-        id=library_agent_id, user_id=user_id\n-    )\n-    if not library_agent:\n-        raise HTTPException(\n-            status_code=status.HTTP_404_NOT_FOUND,\n-            detail=f\"Library agent #{library_agent_id} not found\",\n-        )\n-\n-    graph = await get_graph(\n-        library_agent.graph_id, version=library_agent.graph_version, user_id=user_id\n-    )\n-    if not graph:\n-        raise HTTPException(\n-            status.HTTP_410_GONE,\n-            f\"Graph #{library_agent.graph_id} not accessible (anymore)\",\n-        )\n-    if not (trigger_node := graph.webhook_input_node):\n-        raise HTTPException(\n-            status_code=status.HTTP_400_BAD_REQUEST,\n-            detail=f\"Graph #{library_agent.graph_id} does not have a webhook node\",\n-        )\n-\n-    trigger_config_with_credentials = {\n-        **params.trigger_config,\n-        **(\n-            make_node_credentials_input_map(graph, params.agent_credentials).get(\n-                trigger_node.id\n-            )\n-            or {}\n-        ),\n-    }\n-\n-    new_webhook, feedback = await setup_webhook_for_block(\n-        user_id=user_id,\n-        trigger_block=trigger_node.block,\n-        trigger_config=trigger_config_with_credentials,\n-    )\n-    if not new_webhook:\n-        raise HTTPException(\n-            status_code=status.HTTP_400_BAD_REQUEST,\n-            detail=f\"Could not set up webhook: {feedback}\",\n-        )\n-\n-    new_preset = await library_db.create_preset(\n-        user_id=user_id,\n-        preset=library_model.LibraryAgentPresetCreatable(\n-            graph_id=library_agent.graph_id,\n-            graph_version=library_agent.graph_version,\n-            name=params.name,\n-            description=params.description,\n-            inputs=trigger_config_with_credentials,\n-            credentials=params.agent_credentials,\n-            webhook_id=new_webhook.id,\n-            is_active=True,\n-        ),\n-    )\n-    return new_preset\ndiff --git a/autogpt_platform/backend/backend/server/v2/library/routes/presets.py b/autogpt_platform/backend/backend/server/v2/library/routes/presets.py\nindex 35bbb9f59124..4a64ec425ee0 100644\n--- a/autogpt_platform/backend/backend/server/v2/library/routes/presets.py\n+++ b/autogpt_platform/backend/backend/server/v2/library/routes/presets.py\n@@ -138,6 +138,66 @@ async def create_preset(\n         )\n \n \n+@router.post(\"/presets/setup-trigger\")\n+async def setup_trigger(\n+    params: models.TriggeredPresetSetupRequest = Body(),\n+    user_id: str = Depends(autogpt_auth_lib.depends.get_user_id),\n+) -> models.LibraryAgentPreset:\n+    \"\"\"\n+    Sets up a webhook-triggered `LibraryAgentPreset` for a `LibraryAgent`.\n+    Returns the correspondingly created `LibraryAgentPreset` with `webhook_id` set.\n+    \"\"\"\n+    graph = await get_graph(\n+        params.graph_id, version=params.graph_version, user_id=user_id\n+    )\n+    if not graph:\n+        raise HTTPException(\n+            status.HTTP_410_GONE,\n+            f\"Graph #{params.graph_id} not accessible (anymore)\",\n+        )\n+    if not (trigger_node := graph.webhook_input_node):\n+        raise HTTPException(\n+            status_code=status.HTTP_400_BAD_REQUEST,\n+            detail=f\"Graph #{params.graph_id} does not have a webhook node\",\n+        )\n+\n+    trigger_config_with_credentials = {\n+        **params.trigger_config,\n+        **(\n+            make_node_credentials_input_map(graph, params.agent_credentials).get(\n+                trigger_node.id\n+            )\n+            or {}\n+        ),\n+    }\n+\n+    new_webhook, feedback = await setup_webhook_for_block(\n+        user_id=user_id,\n+        trigger_block=trigger_node.block,\n+        trigger_config=trigger_config_with_credentials,\n+    )\n+    if not new_webhook:\n+        raise HTTPException(\n+            status_code=status.HTTP_400_BAD_REQUEST,\n+            detail=f\"Could not set up webhook: {feedback}\",\n+        )\n+\n+    new_preset = await db.create_preset(\n+        user_id=user_id,\n+        preset=models.LibraryAgentPresetCreatable(\n+            graph_id=params.graph_id,\n+            graph_version=params.graph_version,\n+            name=params.name,\n+            description=params.description,\n+            inputs=trigger_config_with_credentials,\n+            credentials=params.agent_credentials,\n+            webhook_id=new_webhook.id,\n+            is_active=True,\n+        ),\n+    )\n+    return new_preset\n+\n+\n @router.patch(\n     \"/presets/{preset_id}\",\n     summary=\"Update an existing preset\",\ndiff --git a/autogpt_platform/frontend/src/components/RunnerUIWrapper.tsx b/autogpt_platform/frontend/src/components/RunnerUIWrapper.tsx\nindex 37ead841481a..deae14d0d6e1 100644\n--- a/autogpt_platform/frontend/src/components/RunnerUIWrapper.tsx\n+++ b/autogpt_platform/frontend/src/components/RunnerUIWrapper.tsx\n@@ -1,215 +1,108 @@\n import React, {\n   useState,\n-  useCallback,\n   forwardRef,\n   useImperativeHandle,\n+  useMemo,\n } from \"react\";\n-import RunnerOutputUI, { BlockOutput } from \"./runner-ui/RunnerOutputUI\";\n-import RunnerInputUI from \"./runner-ui/RunnerInputUI\";\n import { Node } from \"@xyflow/react\";\n-import { filterBlocksByType } from \"@/lib/utils\";\n+import { CustomNodeData } from \"@/components/CustomNode\";\n+import { RunnerInputDialog } from \"@/components/runner-ui/RunnerInputUI\";\n import {\n-  BlockIOObjectSubSchema,\n-  BlockIORootSchema,\n   BlockUIType,\n+  CredentialsMetaInput,\n+  GraphMeta,\n } from \"@/lib/autogpt-server-api/types\";\n-import { CustomNode } from \"./CustomNode\";\n-\n-interface HardcodedValues {\n-  name: any;\n-  description: any;\n-  value: any;\n-  placeholder_values: any;\n-}\n-\n-export interface InputItem {\n-  id: string;\n-  type: \"input\";\n-  inputSchema: BlockIORootSchema;\n-  hardcodedValues: HardcodedValues;\n-}\n+import RunnerOutputUI, {\n+  OutputNodeInfo,\n+} from \"@/components/runner-ui/RunnerOutputUI\";\n \n interface RunnerUIWrapperProps {\n-  nodes: Node[];\n-  setNodes: React.Dispatch<React.SetStateAction<CustomNode[]>>;\n-  setIsScheduling: React.Dispatch<React.SetStateAction<boolean>>;\n-  isRunning: boolean;\n-  isScheduling: boolean;\n-  requestSaveAndRun: () => void;\n-  scheduleRunner: (\n+  graph: GraphMeta;\n+  nodes: Node<CustomNodeData>[];\n+  saveAndRun: (\n+    inputs: Record<string, any>,\n+    credentialsInputs: Record<string, CredentialsMetaInput>,\n+  ) => void;\n+  createRunSchedule: (\n     cronExpression: string,\n-    input: InputItem[],\n     scheduleName: string,\n+    inputs: Record<string, any>,\n+    credentialsInputs: Record<string, CredentialsMetaInput>,\n   ) => Promise<void>;\n }\n \n export interface RunnerUIWrapperRef {\n-  openRunnerInput: () => void;\n+  openRunInputDialog: () => void;\n   openRunnerOutput: () => void;\n   runOrOpenInput: () => void;\n-  collectInputsForScheduling: (\n-    cronExpression: string,\n-    scheduleName: string,\n-  ) => void;\n }\n \n const RunnerUIWrapper = forwardRef<RunnerUIWrapperRef, RunnerUIWrapperProps>(\n-  (\n-    {\n-      nodes,\n-      setIsScheduling,\n-      setNodes,\n-      isScheduling,\n-      isRunning,\n-      requestSaveAndRun,\n-      scheduleRunner,\n-    },\n-    ref,\n-  ) => {\n-    const [isRunnerInputOpen, setIsRunnerInputOpen] = useState(false);\n+  ({ graph, nodes, saveAndRun, createRunSchedule }, ref) => {\n+    const [isRunInputDialogOpen, setIsRunInputDialogOpen] = useState(false);\n     const [isRunnerOutputOpen, setIsRunnerOutputOpen] = useState(false);\n-    const [scheduledInput, setScheduledInput] = useState(false);\n-    const [cronExpression, setCronExpression] = useState(\"\");\n-    const [scheduleName, setScheduleName] = useState(\"\");\n \n-    const getBlockInputsAndOutputs = useCallback((): {\n-      inputs: InputItem[];\n-      outputs: BlockOutput[];\n-    } => {\n-      const inputBlocks = filterBlocksByType(\n-        nodes,\n-        (node) => node.data.uiType === BlockUIType.INPUT,\n-      );\n+    const graphInputs = graph.input_schema.properties;\n \n-      const outputBlocks = filterBlocksByType(\n-        nodes,\n+    const graphOutputs = useMemo((): OutputNodeInfo[] => {\n+      const outputNodes = nodes.filter(\n         (node) => node.data.uiType === BlockUIType.OUTPUT,\n       );\n \n-      const inputs = inputBlocks.map(\n-        (node) =>\n-          ({\n-            id: node.id,\n-            type: \"input\" as const,\n-            inputSchema: (node.data.inputSchema as BlockIOObjectSubSchema)\n-              .properties.value as BlockIORootSchema,\n-            hardcodedValues: {\n-              name: (node.data.hardcodedValues as any).name || \"\",\n-              description: (node.data.hardcodedValues as any).description || \"\",\n-              value: (node.data.hardcodedValues as any).value,\n-              placeholder_values:\n-                (node.data.hardcodedValues as any).placeholder_values || [],\n-            },\n-          }) satisfies InputItem,\n-      );\n-\n-      const outputs = outputBlocks.map(\n+      return outputNodes.map(\n         (node) =>\n           ({\n             metadata: {\n-              name: (node.data.hardcodedValues as any).name || \"Output\",\n+              name: node.data.hardcodedValues.name || \"Output\",\n               description:\n-                (node.data.hardcodedValues as any).description ||\n+                node.data.hardcodedValues.description ||\n                 \"Output from the agent\",\n             },\n             result:\n               (node.data.executionResults as any)\n                 ?.map((result: any) => result?.data?.output)\n                 .join(\"\\n--\\n\") || \"No output yet\",\n-          }) satisfies BlockOutput,\n+          }) satisfies OutputNodeInfo,\n       );\n-\n-      return { inputs, outputs };\n     }, [nodes]);\n \n-    const handleInputChange = useCallback(\n-      (nodeId: string, field: string, value: any) => {\n-        setNodes((nds) =>\n-          nds.map((node) => {\n-            if (node.id === nodeId) {\n-              return {\n-                ...node,\n-                data: {\n-                  ...node.data,\n-                  hardcodedValues: {\n-                    ...(node.data.hardcodedValues as any),\n-                    [field]: value,\n-                  },\n-                },\n-              };\n-            }\n-            return node;\n-          }),\n-        );\n-      },\n-      [setNodes],\n-    );\n-\n-    const openRunnerInput = () => setIsRunnerInputOpen(true);\n+    const openRunInputDialog = () => setIsRunInputDialogOpen(true);\n     const openRunnerOutput = () => setIsRunnerOutputOpen(true);\n \n     const runOrOpenInput = () => {\n-      const { inputs } = getBlockInputsAndOutputs();\n-      if (inputs.length > 0) {\n-        openRunnerInput();\n+      if (\n+        Object.keys(graphInputs).length > 0 ||\n+        Object.keys(graph.credentials_input_schema.properties).length > 0\n+      ) {\n+        openRunInputDialog();\n       } else {\n-        requestSaveAndRun();\n+        saveAndRun({}, {});\n       }\n     };\n \n-    const collectInputsForScheduling = (\n-      cronExpression: string,\n-      scheduleName: string,\n-    ) => {\n-      const { inputs } = getBlockInputsAndOutputs();\n-      setCronExpression(cronExpression);\n-      setScheduleName(scheduleName);\n-\n-      if (inputs.length > 0) {\n-        setScheduledInput(true);\n-        setIsRunnerInputOpen(true);\n-      } else {\n-        scheduleRunner(cronExpression, [], scheduleName);\n-      }\n-    };\n-\n-    useImperativeHandle(ref, () => ({\n-      openRunnerInput,\n-      openRunnerOutput,\n-      runOrOpenInput,\n-      collectInputsForScheduling,\n-    }));\n+    useImperativeHandle(\n+      ref,\n+      () =>\n+        ({\n+          openRunInputDialog,\n+          openRunnerOutput,\n+          runOrOpenInput,\n+        }) satisfies RunnerUIWrapperRef,\n+    );\n \n     return (\n       <>\n-        <RunnerInputUI\n-          isOpen={isRunnerInputOpen}\n-          onClose={() => setIsRunnerInputOpen(false)}\n-          blockInputs={getBlockInputsAndOutputs().inputs}\n-          onInputChange={handleInputChange}\n-          onRun={() => {\n-            setIsRunnerInputOpen(false);\n-            requestSaveAndRun();\n-          }}\n-          scheduledInput={scheduledInput}\n-          isScheduling={isScheduling}\n-          onSchedule={async () => {\n-            setIsScheduling(true);\n-            await scheduleRunner(\n-              cronExpression,\n-              getBlockInputsAndOutputs().inputs,\n-              scheduleName,\n-            );\n-            setIsScheduling(false);\n-            setIsRunnerInputOpen(false);\n-            setScheduledInput(false);\n-          }}\n-          isRunning={isRunning}\n+        <RunnerInputDialog\n+          isOpen={isRunInputDialogOpen}\n+          doClose={() => setIsRunInputDialogOpen(false)}\n+          graph={graph}\n+          doRun={saveAndRun}\n+          doCreateSchedule={createRunSchedule}\n         />\n         <RunnerOutputUI\n           isOpen={isRunnerOutputOpen}\n-          onClose={() => setIsRunnerOutputOpen(false)}\n-          blockOutputs={getBlockInputsAndOutputs().outputs}\n+          doClose={() => setIsRunnerOutputOpen(false)}\n+          outputs={graphOutputs}\n         />\n       </>\n     );\ndiff --git a/autogpt_platform/frontend/src/components/edit/control/BlocksControl.tsx b/autogpt_platform/frontend/src/components/edit/control/BlocksControl.tsx\nindex dacc896ec661..e5c3a3315fc7 100644\n--- a/autogpt_platform/frontend/src/components/edit/control/BlocksControl.tsx\n+++ b/autogpt_platform/frontend/src/components/edit/control/BlocksControl.tsx\n@@ -336,4 +327,57 @@ export const BlocksControl: React.FC<BlocksControlProps> = ({\n       </PopoverContent>\n     </Popover>\n   );\n-};\n+}\n+\n+/**\n+ * Evaluates how well a block matches the search query and returns a relevance score.\n+ * The scoring algorithm works as follows:\n+ * - Returns 1 if no query (all blocks match equally)\n+ * - Normalized query for case-insensitive matching\n+ * - Returns 3 for exact substring matches in block name (highest priority)\n+ * - Returns 2 when all query words appear in the block name (regardless of order)\n+ * - Returns 1.X for blocks with names similar to query using Jaro-Winkler distance (X is similarity score)\n+ * - Returns 0.5 when all query words appear in the block description (lowest priority)\n+ * - Returns 0 for no match\n+ *\n+ * Higher scores will appear first in search results.\n+ */\n+function blockScoreForQuery(block: _Block, query: string): number {\n+  if (!query) return 1;\n+  const normalizedQuery = query.toLowerCase().trim();\n+  const queryWords = normalizedQuery.split(/\\s+/);\n+\n+  // Use cached values for performance\n+  const { blockName, beautifiedName, description } = block._cached!;\n+\n+  // 1. Exact match in name (highest priority)\n+  if (\n+    blockName.includes(normalizedQuery) ||\n+    beautifiedName.includes(normalizedQuery)\n+  ) {\n+    return 3;\n+  }\n+\n+  // 2. All query words in name (regardless of order)\n+  const allWordsInName = queryWords.every(\n+    (word) => blockName.includes(word) || beautifiedName.includes(word),\n+  );\n+  if (allWordsInName) return 2;\n+\n+  // 3. Similarity with name (Jaro-Winkler)\n+  const similarityThreshold = 0.65;\n+  const nameSimilarity = jaro(blockName, normalizedQuery);\n+  const beautifiedSimilarity = jaro(beautifiedName, normalizedQuery);\n+  const maxSimilarity = Math.max(nameSimilarity, beautifiedSimilarity);\n+  if (maxSimilarity > similarityThreshold) {\n+    return 1 + maxSimilarity; // Score between 1 and 2\n+  }\n+\n+  // 4. All query words in description (lower priority)\n+  const allWordsInDescription = queryWords.every((word) =>\n+    description.includes(word),\n+  );\n+  if (allWordsInDescription) return 0.5;\n+\n+  return 0;\n+}\ndiff --git a/autogpt_platform/frontend/src/components/monitor/FlowInfo.tsx b/autogpt_platform/frontend/src/components/monitor/FlowInfo.tsx\nindex fb9aae5d8cf9..8dd4b2d14c34 100644\n--- a/autogpt_platform/frontend/src/components/monitor/FlowInfo.tsx\n+++ b/autogpt_platform/frontend/src/components/monitor/FlowInfo.tsx\n@@ -62,90 +64,30 @@ export const FlowInfo: React.FC<\n       (selectedVersion == \"all\" ? flow.graph_version : selectedVersion),\n   );\n \n+  const hasInputs = Object.keys(flow.input_schema.properties).length > 0;\n+  const hasCredentialsInputs =\n+    Object.keys(flow.credentials_input_schema.properties).length > 0;\n+\n   const [isDeleteModalOpen, setIsDeleteModalOpen] = useState(false);\n-  const [isRunnerInputOpen, setIsRunnerInputOpen] = useState(false);\n+  const [isRunDialogOpen, setIsRunDialogOpen] = useState(false);\n   const isDisabled = !selectedFlowVersion;\n \n-  const getBlockInputsAndOutputs = useCallback(() => {\n-    const inputBlocks = filterBlocksByType(\n-      nodes,\n-      (node) => node.data.uiType === BlockUIType.INPUT,\n-    );\n-\n-    const outputBlocks = filterBlocksByType(\n-      nodes,\n-      (node) => node.data.uiType === BlockUIType.OUTPUT,\n-    );\n-\n-    const inputs = inputBlocks.map((node) => ({\n-      id: node.id,\n-      type: \"input\" as const,\n-      inputSchema: node.data.inputSchema as BlockIORootSchema,\n-      hardcodedValues: {\n-        name: (node.data.hardcodedValues as any).name || \"\",\n-        description: (node.data.hardcodedValues as any).description || \"\",\n-        value: (node.data.hardcodedValues as any).value,\n-        placeholder_values:\n-          (node.data.hardcodedValues as any).placeholder_values || [],\n-      },\n-    }));\n-\n-    const outputs = outputBlocks.map((node) => ({\n-      id: node.id,\n-      type: \"output\" as const,\n-      hardcodedValues: {\n-        name: (node.data.hardcodedValues as any).name || \"Output\",\n-        description:\n-          (node.data.hardcodedValues as any).description ||\n-          \"Output from the agent\",\n-        value: (node.data.hardcodedValues as any).value,\n-      },\n-      result: (node.data.executionResults as any)?.at(-1)?.data?.output,\n-    }));\n-\n-    return { inputs, outputs };\n-  }, [nodes]);\n-\n   useEffect(() => {\n     api\n       .getGraphAllVersions(flow.graph_id)\n       .then((result) => setFlowVersions(result));\n   }, [flow.graph_id, api]);\n \n-  const openRunnerInput = () => setIsRunnerInputOpen(true);\n+  const openRunDialog = () => setIsRunDialogOpen(true);\n \n   const runOrOpenInput = () => {\n-    const { inputs } = getBlockInputsAndOutputs();\n-    if (inputs.length > 0) {\n-      openRunnerInput();\n+    if (hasInputs || hasCredentialsInputs) {\n+      openRunDialog();\n     } else {\n-      requestSaveAndRun();\n+      saveAndRun({}, {});\n     }\n   };\n \n-  const handleInputChange = useCallback(\n-    (nodeId: string, field: string, value: any) => {\n-      setNodes((nds) =>\n-        nds.map((node) => {\n-          if (node.id === nodeId) {\n-            return {\n-              ...node,\n-              data: {\n-                ...node.data,\n-                hardcodedValues: {\n-                  ...(node.data.hardcodedValues as any),\n-                  [field]: value,\n-                },\n-              },\n-            };\n-          }\n-          return node;\n-        }),\n-      );\n-    },\n-    [setNodes],\n-  );\n-\n   return (\n     <Card {...props}>\n       <CardHeader className=\"\">\n\ndiff --git a/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx b/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx\nindex f5ce1a8b62c7..43293c6db3f8 100644\n--- a/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx\n+++ b/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx\n@@ -456,234 +451,6 @@ export default function useAgentGraph(\n     });\n   }, [updateQueue, nodesSyncedWithSavedAgent, updateNodesWithExecutionData]);\n \n-  const validateNodes = useCallback((): string | null => {\n-    let errorMessage = null;\n-\n-    nodes.forEach((node) => {\n-      const validate = ajv.compile(node.data.inputSchema);\n-      const errors = {} as { [key: string]: string };\n-\n-      // Validate values against schema using AJV\n-      const inputData =\n-        node.data.uiType === BlockUIType.AGENT\n-          ? node.data.hardcodedValues?.data || {}\n-          : node.data.hardcodedValues || {};\n-      const valid = validate(inputData);\n-      if (!valid) {\n-        // Populate errors if validation fails\n-        validate.errors?.forEach((error) => {\n-          // Skip error if there's an edge connected\n-          const path =\n-            \"dataPath\" in error\n-              ? (error.dataPath as string)\n-              : error.instancePath;\n-          const handle = path.split(/[\\/.]/)[0];\n-          if (\n-            node.data.connections.some(\n-              (conn) => conn.target === node.id || conn.targetHandle === handle,\n-            )\n-          ) {\n-            return;\n-          }\n-          console.warn(`Error in ${node.data.blockType}: ${error}`, {\n-            data: inputData,\n-            schema: node.data.inputSchema,\n-          });\n-          errorMessage = error.message || \"Invalid input\";\n-          if (path && error.message) {\n-            const key = path.slice(1);\n-            setNestedProperty(\n-              errors,\n-              key,\n-              error.message[0].toUpperCase() + error.message.slice(1),\n-            );\n-          } else if (error.keyword === \"required\") {\n-            const key = error.params.missingProperty;\n-            setNestedProperty(errors, key, \"This field is required\");\n-          }\n-        });\n-      }\n-\n-      Object.entries(node.data.inputSchema.properties || {}).forEach(\n-        ([key, schema]) => {\n-          if (schema.depends_on) {\n-            const dependencies = schema.depends_on;\n-\n-            // Check if dependent field has value\n-            const hasValue =\n-              inputData[key] != null ||\n-              (\"default\" in schema && schema.default != null);\n-\n-            const mustHaveValue = node.data.inputSchema.required?.includes(key);\n-\n-            // Check for missing dependencies when dependent field is present\n-            const missingDependencies = dependencies.filter(\n-              (dep) =>\n-                !inputData[dep as keyof typeof inputData] ||\n-                String(inputData[dep as keyof typeof inputData]).trim() === \"\",\n-            );\n-\n-            if ((hasValue || mustHaveValue) && missingDependencies.length > 0) {\n-              setNestedProperty(\n-                errors,\n-                key,\n-                `Requires ${missingDependencies.join(\", \")} to be set`,\n-              );\n-              errorMessage = `Field ${key} requires ${missingDependencies.join(\", \")} to be set`;\n-            }\n-\n-            // Check if field is required when dependencies are present\n-            const hasAllDependencies = dependencies.every(\n-              (dep) =>\n-                inputData[dep as keyof typeof inputData] &&\n-                String(inputData[dep as keyof typeof inputData]).trim() !== \"\",\n-            );\n-\n-            if (hasAllDependencies && !hasValue) {\n-              setNestedProperty(\n-                errors,\n-                key,\n-                `${key} is required when ${dependencies.join(\", \")} are set`,\n-              );\n-              errorMessage = `${key} is required when ${dependencies.join(\", \")} are set`;\n-            }\n-          }\n-        },\n-      );\n-\n-      // Set errors\n-      setNodes((nodes) => {\n-        return nodes.map((n) => {\n-          if (n.id === node.id) {\n-            return {\n-              ...n,\n-              data: {\n-                ...n.data,\n-                errors,\n-              },\n-            };\n-          }\n-          return n;\n-        });\n-      });\n-    });\n-\n-    return errorMessage;\n-  }, [nodes]);\n-\n-  // Handle user requests\n-  useEffect(() => {\n-    // Ignore none request\n-    if (saveRunRequest.request === \"none\") {\n-      return;\n-    }\n-    // Display error message\n-    if (saveRunRequest.state === \"error\") {\n-      if (saveRunRequest.request === \"save\") {\n-        toast({\n-          variant: \"destructive\",\n-          title: `Error saving agent`,\n-          duration: 2000,\n-        });\n-      } else if (saveRunRequest.request === \"run\") {\n-        toast({\n-          variant: \"destructive\",\n-          title: `Error saving&running agent`,\n-          duration: 2000,\n-        });\n-      } else if (saveRunRequest.request === \"stop\") {\n-        toast({\n-          variant: \"destructive\",\n-          title: `Error stopping agent`,\n-          duration: 2000,\n-        });\n-      }\n-      // Reset request\n-      setSaveRunRequest({\n-        request: \"none\",\n-        state: \"none\",\n-      });\n-      return;\n-    }\n-    // When saving request is done\n-    if (\n-      saveRunRequest.state === \"saving\" &&\n-      savedAgent &&\n-      nodesSyncedWithSavedAgent\n-    ) {\n-      // Reset request if only save was requested\n-      if (saveRunRequest.request === \"save\") {\n-        setSaveRunRequest({\n-          request: \"none\",\n-          state: \"none\",\n-        });\n-        // If run was requested, run the agent\n-      } else if (saveRunRequest.request === \"run\") {\n-        const validationError = validateNodes();\n-        if (validationError) {\n-          toast({\n-            title: `Validation failed: ${validationError}`,\n-            variant: \"destructive\",\n-            duration: 2000,\n-          });\n-          setSaveRunRequest({\n-            request: \"none\",\n-            state: \"none\",\n-          });\n-          return;\n-        }\n-        setSaveRunRequest({ request: \"run\", state: \"running\" });\n-        api\n-          .executeGraph(savedAgent.id, savedAgent.version)\n-          .then((graphExecution) => {\n-            setSaveRunRequest({\n-              request: \"run\",\n-              state: \"running\",\n-              activeExecutionID: graphExecution.graph_exec_id,\n-            });\n-\n-            // Update URL params\n-            const path = new URLSearchParams(searchParams);\n-            path.set(\"flowID\", savedAgent.id);\n-            path.set(\"flowVersion\", savedAgent.version.toString());\n-            path.set(\"flowExecutionID\", graphExecution.graph_exec_id);\n-            router.push(`${pathname}?${path.toString()}`);\n-            if (state?.completedSteps.includes(\"BUILDER_SAVE_AGENT\")) {\n-              completeStep(\"BUILDER_RUN_AGENT\");\n-            }\n-          })\n-          .catch((error) => {\n-            const errorMessage =\n-              error instanceof Error ? error.message : String(error);\n-            toast({\n-              variant: \"destructive\",\n-              title: \"Error saving agent\",\n-              description: errorMessage,\n-            });\n-            setSaveRunRequest({ request: \"run\", state: \"error\" });\n-          });\n-\n-        processedUpdates.current = [];\n-      }\n-    }\n-    // Handle stop request\n-    if (\n-      saveRunRequest.request === \"stop\" &&\n-      saveRunRequest.state != \"stopping\" &&\n-      savedAgent &&\n-      saveRunRequest.activeExecutionID\n-    ) {\n-      api.stopGraphExecution(savedAgent.id, saveRunRequest.activeExecutionID);\n-    }\n-  }, [\n-    api,\n-    toast,\n-    saveRunRequest,\n-    savedAgent,\n-    nodesSyncedWithSavedAgent,\n-    validateNodes,\n-  ]);\n-\n   useEffect(() => {\n     if (!flowID || !flowExecutionID) {\n       return;\n@@ -790,93 +535,15 @@ export default function useAgentGraph(\n         return {};\n       }\n \n-      const getNestedData = (\n-        schema: BlockIOSubSchema,\n-        values: { [key: string]: any },\n-      ): { [key: string]: any } => {\n-        let inputData: { [key: string]: any } = {};\n-\n-        if (\"properties\" in schema) {\n-          Object.keys(schema.properties).forEach((key) => {\n-            if (values[key] !== undefined) {\n-              if (\n-                \"properties\" in schema.properties[key] ||\n-                \"additionalProperties\" in schema.properties[key]\n-              ) {\n-                inputData[key] = getNestedData(\n-                  schema.properties[key],\n-                  values[key],\n-                );\n-              } else {\n-                inputData[key] = values[key];\n-              }\n-            }\n-          });\n-        }\n-\n-        if (\"additionalProperties\" in schema) {\n-          inputData = { ...inputData, ...values };\n-        }\n-\n-        return inputData;\n-      };\n-\n-      const inputData = getNestedData(blockSchema, node.data.hardcodedValues);\n-\n-      console.debug(\n-        `Final prepared input for ${node.data.blockType} (${node.id}):`,\n-        inputData,\n-      );\n-      return inputData;\n+      return rebuildObjectUsingSchema(blockSchema, node.data.hardcodedValues);\n     },\n-    [availableNodes],\n+    [availableBlocks],\n   );\n \n-  const _saveAgent = useCallback(async () => {\n-    //FIXME frontend ids should be resolved better (e.g. returned from the server)\n-    // currently this relays on block_id and position\n-    const blockIdToNodeIdMap: Record<string, string> = {};\n-\n-    nodes.forEach((node) => {\n-      const key = `${node.data.block_id}_${node.position.x}_${node.position.y}`;\n-      blockIdToNodeIdMap[key] = node.id;\n-    });\n-\n-    const formattedNodes = nodes.map((node) => {\n-      const inputDefault = prepareNodeInputData(node);\n-      const inputNodes = edges\n-        .filter((edge) => edge.target === node.id)\n-        .map((edge) => ({\n-          name: edge.targetHandle || \"\",\n-          node_id: edge.source,\n-        }));\n-\n-      const outputNodes = edges\n-        .filter((edge) => edge.source === node.id)\n-        .map((edge) => ({\n-          name: edge.sourceHandle || \"\",\n-          node_id: edge.target,\n-        }));\n-\n-      return {\n-        id: node.id,\n-        block_id: node.data.block_id,\n-        input_default: inputDefault,\n-        input_nodes: inputNodes,\n-        output_nodes: outputNodes,\n-        data: {\n-          ...node.data,\n-          hardcodedValues: removeEmptyStringsAndNulls(\n-            node.data.hardcodedValues,\n-          ),\n-        },\n-        metadata: { position: node.position },\n-      };\n-    });\n-\n-    const links = edges.map((edge) => {\n+  const prepareSaveableGraph = useCallback((): GraphCreatable => {\n+    const links = xyEdges.map((edge): LinkCreatable => {\n       let sourceName = edge.sourceHandle || \"\";\n-      const sourceNode = nodes.find((node) => node.id === edge.source);\n+      const sourceNode = xyNodes.find((node) => node.id === edge.source);\n \n       // Special case for SmartDecisionMakerBlock\n       if (\n@@ -893,51 +560,181 @@ export default function useAgentGraph(\n       };\n     });\n \n-    const payload = {\n+    return {\n       name: agentName || `New Agent ${new Date().toISOString()}`,\n       description: agentDescription || \"\",\n-      nodes: formattedNodes,\n+      nodes: xyNodes.map(\n+        (node): NodeCreatable => ({\n+          id: node.id,\n+          block_id: node.data.block_id,\n+          input_default: prepareNodeInputData(node),\n+          metadata: { position: node.position },\n+        }),\n+      ),\n       links: links,\n     };\n+  }, [\n+    xyNodes,\n+    xyEdges,\n+    agentName,\n+    agentDescription,\n+    prepareNodeInputData,\n+    getToolFuncName,\n+  ]);\n+\n+  const validateGraph = useCallback(\n+    (graph?: GraphCreatable): string | null => {\n+      let errorMessage = null;\n+\n+      if (!graph) {\n+        graph = prepareSaveableGraph();\n+      }\n+\n+      graph.nodes.forEach((node) => {\n+        const block = availableBlocks.find(\n+          (block) => block.id === node.block_id,\n+        );\n+        if (!block) {\n+          console.error(\n+            `Node ${node.id} is invalid: unknown block ID ${node.block_id}`,\n+          );\n+          return;\n+        }\n+        const inputSchema = block.inputSchema;\n+        const validate = ajv.compile(inputSchema);\n+        const errors: Record<string, string> = {};\n+        const errorPrefix = `${block.name} [${node.id.split(\"-\")[0]}]`;\n+\n+        // Validate values against schema using AJV\n+        const inputData = node.input_default;\n+        const valid = validate(inputData);\n+        if (!valid) {\n+          // Populate errors if validation fails\n+          validate.errors?.forEach((error) => {\n+            const path =\n+              \"dataPath\" in error\n+                ? (error.dataPath as string)\n+                : error.instancePath || error.params.missingProperty;\n+            const handle = path.split(/[\\/.]/)[0];\n+            // Skip error if there's an edge connected\n+            if (\n+              graph.links.some(\n+                (link) => link.sink_id == node.id && link.sink_name == handle,\n+              )\n+            ) {\n+              return;\n+            }\n+            console.warn(`Error in ${block.name} input: ${error}`, {\n+              data: inputData,\n+              schema: inputSchema,\n+            });\n+            errorMessage =\n+              `${errorPrefix}: ` + (error.message || \"Invalid input\");\n+            if (path && error.message) {\n+              const key = path.slice(1);\n+              setNestedProperty(\n+                errors,\n+                key,\n+                error.message[0].toUpperCase() + error.message.slice(1),\n+              );\n+            } else if (error.keyword === \"required\") {\n+              const key = error.params.missingProperty;\n+              setNestedProperty(errors, key, \"This field is required\");\n+            }\n+          });\n+        }\n+\n+        Object.entries(inputSchema.properties).forEach(([key, schema]) => {\n+          if (schema.depends_on) {\n+            const dependencies = schema.depends_on;\n+\n+            // Check if dependent field has value\n+            const hasValue =\n+              inputData[key] != null ||\n+              (\"default\" in schema && schema.default != null);\n+\n+            const mustHaveValue = inputSchema.required?.includes(key);\n+\n+            // Check for missing dependencies when dependent field is present\n+            const missingDependencies = dependencies.filter(\n+              (dep) =>\n+                !inputData[dep as keyof typeof inputData] ||\n+                String(inputData[dep as keyof typeof inputData]).trim() === \"\",\n+            );\n+\n+            if ((hasValue || mustHaveValue) && missingDependencies.length > 0) {\n+              setNestedProperty(\n+                errors,\n+                key,\n+                `Requires ${missingDependencies.join(\", \")} to be set`,\n+              );\n+              errorMessage = `${errorPrefix}: field ${key} requires ${missingDependencies.join(\", \")} to be set`;\n+            }\n+\n+            // Check if field is required when dependencies are present\n+            const hasAllDependencies = dependencies.every(\n+              (dep) =>\n+                inputData[dep as keyof typeof inputData] &&\n+                String(inputData[dep as keyof typeof inputData]).trim() !== \"\",\n+            );\n+\n+            if (hasAllDependencies && !hasValue) {\n+              setNestedProperty(\n+                errors,\n+                key,\n+                `${key} is required when ${dependencies.join(\", \")} are set`,\n+              );\n+              errorMessage = `${errorPrefix}: ${key} is required when ${dependencies.join(\", \")} are set`;\n+            }\n+          }\n+        });\n+\n+        // Set errors\n+        setXYNodes((nodes) => {\n+          return nodes.map((n) => {\n+            if (n.id === node.id) {\n+              return {\n+                ...n,\n+                data: {\n+                  ...n.data,\n+                  errors,\n+                },\n+              };\n+            }\n+            return n;\n+          });\n+        });\n+      });\n+\n+      return errorMessage;\n+    },\n+    [prepareSaveableGraph, availableBlocks],\n+  );\n+\n+  const _saveAgent = useCallback(async () => {\n+    // FIXME: frontend IDs should be resolved better (e.g. returned from the server)\n+    // currently this relays on block_id and position\n+    const blockIDToNodeIDMap = Object.fromEntries(\n+      xyNodes.map((node) => [\n+        `${node.data.block_id}_${node.position.x}_${node.position.y}`,\n+        node.id,\n+      ]),\n+    );\n+\n+    const payload = prepareSaveableGraph();\n \n     // To avoid saving the same graph, we compare the payload with the saved agent.\n     // Differences in IDs are ignored.\n-    const comparedPayload = {\n-      name: payload.name,\n-      description: payload.description,\n-      nodes: payload.nodes.map(\n-        ({ id: _, data: __, input_nodes: ___, output_nodes: ____, ...rest }) =>\n-          rest,\n-      ),\n-      links: payload.links.map(\n-        ({ source_id: _, sink_id: __, ...rest }) => rest,\n-      ),\n-    };\n-    const comparedSavedAgent = {\n-      name: savedAgent?.name,\n-      description: savedAgent?.description,\n-      nodes: savedAgent?.nodes.map((v) => ({\n-        block_id: v.block_id,\n-        input_default: v.input_default,\n-        metadata: v.metadata,\n-      })),\n-      links: savedAgent?.links.map((v) => ({\n-        sink_name: v.sink_name,\n-        source_name: v.source_name,\n-      })),\n-    };\n-\n-    let newSavedAgent = null;\n-    if (savedAgent && deepEquals(comparedPayload, comparedSavedAgent)) {\n+    let newSavedAgent: Graph;\n+    if (savedAgent && graphsEquivalent(savedAgent, payload)) {\n       console.warn(\"No need to save: Graph is the same as version on server\");\n       newSavedAgent = savedAgent;\n     } else {\n       console.debug(\n         \"Saving new Graph version; old vs new:\",\n-        comparedPayload,\n-        comparedSavedAgent,\n+        savedAgent,\n+        payload,\n       );\n-      setNodesSyncedWithSavedAgent(false);\n \n       newSavedAgent = savedAgent\n         ? await api.updateGraph(savedAgent.id, {\n@@ -1024,80 +825,148 @@ export default function useAgentGraph(\n         title: \"Error saving agent\",\n         description: errorMessage,\n       });\n-      setSaveRunRequest({ request: \"save\", state: \"error\" });\n+    } finally {\n+      setIsSaving(false);\n     }\n-  }, [_saveAgent, toast]);\n+  }, [_saveAgent, toast, completeStep]);\n \n-  const requestSave = useCallback(() => {\n-    if (saveRunRequest.state !== \"none\") {\n-      return;\n-    }\n-    saveAgent();\n-    setSaveRunRequest({\n-      request: \"save\",\n-      state: \"saving\",\n-    });\n-  }, [saveAgent, saveRunRequest.state]);\n+  const saveAndRun = useCallback(\n+    async (\n+      inputs: Record<string, any>,\n+      credentialsInputs: Record<string, CredentialsMetaInput>,\n+    ) => {\n+      if (isSaving || isRunning) {\n+        return;\n+      }\n \n-  const requestSaveAndRun = useCallback(() => {\n-    saveAgent();\n-    setSaveRunRequest({\n-      request: \"run\",\n-      state: \"saving\",\n-    });\n-  }, [saveAgent]);\n+      setIsSaving(true);\n+      let savedAgent: Graph;\n+      try {\n+        savedAgent = await _saveAgent();\n+        completeStep(\"BUILDER_SAVE_AGENT\");\n+      } catch (error) {\n+        const errorMessage =\n+          error instanceof Error ? error.message : String(error);\n+        toast({\n+          variant: \"destructive\",\n+          title: \"Error saving agent\",\n+          description: errorMessage,\n+        });\n+        return;\n+      } finally {\n+        setIsSaving(false);\n+      }\n+\n+      const validationError = validateGraph(savedAgent);\n+      if (validationError) {\n+        toast({\n+          title: `Graph validation failed: ${validationError}`,\n+          variant: \"destructive\",\n+        });\n+        return;\n+      }\n+\n+      setIsRunning(true);\n+      processedUpdates.current = [];\n+\n+      try {\n+        const graphExecution = await api.executeGraph(\n+          savedAgent.id,\n+          savedAgent.version,\n+          inputs,\n+          credentialsInputs,\n+        );\n+\n+        setActiveExecutionID(graphExecution.graph_exec_id);\n+\n+        // Update URL params\n+        const path = new URLSearchParams(searchParams);\n+        path.set(\"flowID\", savedAgent.id);\n+        path.set(\"flowVersion\", savedAgent.version.toString());\n+        path.set(\"flowExecutionID\", graphExecution.graph_exec_id);\n+        router.push(`${pathname}?${path.toString()}`);\n \n-  const requestStopRun = useCallback(() => {\n-    if (saveRunRequest.state != \"running\") {\n+        if (state?.completedSteps.includes(\"BUILDER_SAVE_AGENT\")) {\n+          completeStep(\"BUILDER_RUN_AGENT\");\n+        }\n+      } catch (error) {\n+        const errorMessage =\n+          error instanceof Error ? error.message : String(error);\n+        toast({\n+          variant: \"destructive\",\n+          title: \"Error running agent\",\n+          description: errorMessage,\n+        });\n+        setIsRunning(false);\n+        setActiveExecutionID(null);\n+      }\n+    },\n+    [\n+      _saveAgent,\n+      toast,\n+      completeStep,\n+      savedAgent,\n+      prepareSaveableGraph,\n+      nodesSyncedWithSavedAgent,\n+      validateGraph,\n+      api,\n+      searchParams,\n+      pathname,\n+      router,\n+      state,\n+      isSaving,\n+      isRunning,\n+      processedUpdates,\n+    ],\n+  );\n+\n+  const stopRun = useCallback(async () => {\n+    if (!isRunning || !activeExecutionID || isStopping || !savedAgent) {\n       return;\n     }\n-    if (!saveRunRequest.activeExecutionID) {\n-      console.warn(\n-        \"Stop requested but execution ID is unknown; state:\",\n-        saveRunRequest,\n-      );\n-    }\n-    setSaveRunRequest((prev) => ({\n-      ...prev,\n-      request: \"stop\",\n-      state: \"running\",\n-    }));\n-  }, [saveRunRequest]);\n+\n+    setIsStopping(true);\n+    await api\n+      .stopGraphExecution(savedAgent.id, activeExecutionID)\n+      .catch((error) => {\n+        const errorMessage =\n+          error instanceof Error ? error.message : String(error);\n+        toast({\n+          variant: \"destructive\",\n+          title: \"Error stopping agent\",\n+          description: errorMessage,\n+        });\n+      });\n+    setIsStopping(false);\n+  }, [api, savedAgent, activeExecutionID, isRunning, isStopping, toast]);\n \n   // runs after saving cron expression and inputs (if exists)\n-  const scheduleRunner = useCallback(\n+  const createRunSchedule = useCallback(\n     async (\n       cronExpression: string,\n-      inputs: InputItem[],\n       scheduleName: string,\n+      inputs: Record<string, any>,\n+      credentialsInputs: Record<string, CredentialsMetaInput>,\n     ) => {\n-      await saveAgent();\n+      if (!savedAgent || isScheduling) return;\n+\n+      setIsScheduling(true);\n       try {\n-        if (flowID) {\n-          await api.createGraphExecutionSchedule({\n-            graph_id: flowID,\n-            // flowVersion is always defined here because scheduling is opened for a specific version\n-            graph_version: flowVersion!,\n-            name: scheduleName,\n-            cron: cronExpression,\n-            inputs: inputs.reduce(\n-              (acc, input) => ({\n-                ...acc,\n-                [input.hardcodedValues.name]: input.hardcodedValues.value,\n-              }),\n-              {},\n-            ),\n-          });\n-          toast({\n-            title: \"Agent scheduling successful\",\n-          });\n+        await api.createGraphExecutionSchedule({\n+          graph_id: savedAgent.id,\n+          graph_version: savedAgent.version,\n+          name: scheduleName,\n+          cron: cronExpression,\n+          inputs: inputs,\n+          credentials: credentialsInputs,\n+        });\n+        toast({\n+          title: \"Agent scheduling successful\",\n+        });\n \n-          // if scheduling is done from the monitor page, then redirect to monitor page after successful scheduling\n-          if (searchParams.get(\"open_scheduling\") === \"true\") {\n-            router.push(\"/\");\n-          }\n-        } else {\n-          return;\n+        // if scheduling is done from the monitor page, then redirect to monitor page after successful scheduling\n+        if (searchParams.get(\"open_scheduling\") === \"true\") {\n+          router.push(\"/monitoring\");\n         }\n       } catch (error) {\n         console.error(error);\n@@ -1117,21 +988,74 @@ export default function useAgentGraph(\n     agentDescription,\n     setAgentDescription,\n     savedAgent,\n-    availableNodes,\n+    availableBlocks,\n     availableFlows,\n     getOutputType,\n-    requestSave,\n-    requestSaveAndRun,\n-    requestStopRun,\n-    scheduleRunner,\n-    isSaving: saveRunRequest.state == \"saving\",\n-    isRunning: saveRunRequest.state == \"running\",\n-    isStopping: saveRunRequest.state == \"stopping\",\n+    saveAgent,\n+    saveAndRun,\n+    stopRun,\n+    createRunSchedule,\n+    isSaving,\n+    isRunning,\n+    isStopping,\n     isScheduling,\n-    setIsScheduling,\n-    nodes,\n-    setNodes,\n-    edges,\n-    setEdges,\n+    nodes: xyNodes,\n+    setNodes: setXYNodes,\n+    edges: xyEdges,\n+    setEdges: setXYEdges,\n+  };\n+}\n+\n+function graphsEquivalent(saved: Graph, current: GraphCreatable): boolean {\n+  const _saved = {\n+    name: saved.name,\n+    description: saved.description,\n+    nodes: saved.nodes.map((v) => ({\n+      block_id: v.block_id,\n+      input_default: v.input_default,\n+      metadata: v.metadata,\n+    })),\n+    links: saved.links.map((v) => ({\n+      sink_name: v.sink_name,\n+      source_name: v.source_name,\n+    })),\n   };\n+  const _current = {\n+    name: current.name,\n+    description: current.description,\n+    nodes: current.nodes.map(({ id: _, ...rest }) => rest),\n+    links: current.links.map(({ source_id: _, sink_id: __, ...rest }) => rest),\n+  };\n+  return deepEquals(_saved, _current);\n+}\n+\n+function rebuildObjectUsingSchema(\n+  schema: BlockIOSubSchema,\n+  object: { [key: string]: any },\n+): Record<string, any> {\n+  let inputData: Record<string, any> = {};\n+\n+  if (\"properties\" in schema) {\n+    Object.keys(schema.properties).forEach((key) => {\n+      if (object[key] !== undefined) {\n+        if (\n+          \"properties\" in schema.properties[key] ||\n+          \"additionalProperties\" in schema.properties[key]\n+        ) {\n+          inputData[key] = rebuildObjectUsingSchema(\n+            schema.properties[key],\n+            object[key],\n+          );\n+        } else {\n+          inputData[key] = object[key];\n+        }\n+      }\n+    });\n+  }\n+\n+  if (\"additionalProperties\" in schema) {\n+    inputData = { ...inputData, ...object };\n+  }\n+\n+  return inputData;\n }\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Thanks for the PR on improving Builder credentials support and UX! The overall structure looks solid, and the refactoring you've done enhances both maintainability and user experience.\n\nHowever, before merging, please ensure the checklist is completespecifically, check the section confirming that you've tested your changes according to the plan. \n\nAlso, your work on `GraphMeta` and the UX improvements for smoother canvas navigation are commendable. Once the checklist is fully updated, this should be ready for approval!"
      }
    ],
    "codediff_length": 46940
  },
  {
    "index": 7,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/tests/rules/test_nix_shell.py b/tests/rules/test_nix_shell.py\nnew file mode 100644\nindex 000000000..69ba14872\n--- /dev/null\n+++ b/tests/rules/test_nix_shell.py\n@@ -0,0 +1,90 @@\n+import pytest\n+from thefuck.rules.nix_shell import get_nixpkgs_names, match, get_new_command\n+from thefuck.types import Command\n+from unittest.mock import patch, MagicMock\n+\n+\n+@pytest.mark.parametrize(\n+    \"script,output,nixpkgs_names\",\n+    [\n+        # output can be retrived by running `THEFUCK_DEBUG=true thefuck lsof`\n+        (\n+            \"lsof\",\n+            \"/nix/store/p6dlr3skfhxpyphipg2bqnj52999banh-bash-5.2-p15/bin/sh: line 1: lsof: command not found\",\n+            [\"lsof\"],\n+        ),\n+    ],\n+)\n+def test_match(script, output, nixpkgs_names):\n+    with patch(\"thefuck.rules.nix_shell.get_nixpkgs_names\") as mocked_get_nixpkgs_names:\n+        mocked_get_nixpkgs_names.return_value = nixpkgs_names\n+        command = Command(script, output)\n+        assert match(command)\n+\n+\n+@pytest.mark.parametrize(\n+    \"script,output,nixpkgs_names\",\n+    [\n+        # output can be retrived by running `THEFUCK_DEBUG=true thefuck foo`\n+        (\n+            \"foo\",\n+            \"/nix/store/p6dlr3skfhxpyphipg2bqnj52999banh-bash-5.2-p15/bin/sh: line 1: foo: command not found\",\n+            [],\n+        ),\n+    ],\n+)\n+def test_not_match(script, output, nixpkgs_names):\n+    with patch(\"thefuck.rules.nix_shell.get_nixpkgs_names\") as mocked_get_nixpkgs_names:\n+        mocked_get_nixpkgs_names.return_value = nixpkgs_names\n+        command = Command(script, output)\n+        assert not match(command)\n+\n+\n+@pytest.mark.parametrize(\n+    \"script,nixpkgs_names,new_command\",\n+    [\n+        (\n+            \"lsof -i :3000\",\n+            [\"busybox\", \"lsof\"],\n+            [\n+                'nix-shell -p busybox --run \"lsof -i :3000\"',\n+                'nix-shell -p lsof --run \"lsof -i :3000\"',\n+            ],\n+        ),\n+        (\"xev\", [\"xorg.xev\"], ['nix-shell -p xorg.xev --run \"xev\"']),\n+    ],\n+)\n+def test_get_new_command(script, nixpkgs_names, new_command):\n+    \"\"\"Check that flags and params are preserved in the new command\"\"\"\n+\n+    command = Command(script, \"\")\n+    with patch(\"thefuck.rules.nix_shell.get_nixpkgs_names\") as mocked_get_nixpkgs_names:\n+        mocked_get_nixpkgs_names.return_value = nixpkgs_names\n+        assert get_new_command(command) == new_command\n+\n+\n+# Mocks the stderr of `command-not-found QUERY`. Mock values are retrieved by\n+# running `THEFUCK_DEBUG=true thefuck command-not-found lsof`.\n+mocked_cnf_stderr = {\n+    \"lsof\": \"The program 'lsof' is not in your PATH. It is provided by several packages.\\nYou can make it available in an ephemeral shell by typing one of the following:\\n  nix-shell -p busybox\\n  nix-shell -p lsof\",\n+    \"xev\": \"The program 'xev' is not in your PATH. You can make it available in an ephemeral shell by typing:\\n  nix-shell -p xorg.xev\",\n+    \"foo\": \"foo: command not found\",\n+}\n+\n+\n+@pytest.mark.parametrize(\n+    \"bin,expected_nixpkgs_names,cnf_stderr\",\n+    [\n+        (\"lsof\", [\"busybox\", \"lsof\"], mocked_cnf_stderr[\"lsof\"]),\n+        (\"xev\", [\"xorg.xev\"], mocked_cnf_stderr[\"xev\"]),\n+        (\"foo\", [], mocked_cnf_stderr[\"foo\"]),\n+    ],\n+)\n+def test_get_nixpkgs_names(bin, expected_nixpkgs_names, cnf_stderr):\n+    \"\"\"Check that `get_nixpkgs_names` returns the correct names\"\"\"\n+\n+    with patch(\"subprocess.run\") as mocked_run:\n+        result = MagicMock()\n+        result.stderr = cnf_stderr\n+        mocked_run.return_value = result\n+        assert get_nixpkgs_names(bin) == expected_nixpkgs_names\ndiff --git a/thefuck/rules/nix_shell.py b/thefuck/rules/nix_shell.py\nnew file mode 100644\nindex 000000000..7bfba7eac\n--- /dev/null\n+++ b/thefuck/rules/nix_shell.py\n@@ -0,0 +1,51 @@\n+from thefuck.specific.nix import nix_available\n+import subprocess\n+\n+enabled_by_default = nix_available\n+\n+# Set the priority just ahead of `fix_file` rule, which can generate low quality matches due\n+# to the sheer amount of paths in the nix store.\n+priority = 999\n+\n+\n+def get_nixpkgs_names(bin):\n+    \"\"\"\n+    Returns the name of the Nix package that provides the given binary. It uses the\n+    `command-not-found` binary to do so, which is how nix-shell generates it's own suggestions.\n+    \"\"\"\n+\n+    result = subprocess.run(\n+        [\"command-not-found\", bin], stderr=subprocess.PIPE, universal_newlines=True\n+    )\n+\n+    # The suggestion, if any, will be found in stderr. Upstream definition: https://github.com/NixOS/nixpkgs/blob/b6fbd87328f8eabd82d65cc8f75dfb74341b0ace/nixos/modules/programs/command-not-found/command-not-found.nix#L48-L90\n+    text = result.stderr\n+\n+    # return early if binary is not available through nix\n+    if \"nix-shell\" not in text:\n+        return []\n+\n+    nixpkgs_names = [\n+        line.split()[-1] for line in text.splitlines() if \"nix-shell -p\" in line\n+    ]\n+    return nixpkgs_names\n+\n+\n+def match(command):\n+    bin = command.script_parts[0]\n+    return (\n+        \"command not found\" in command.output  # only match commands which had exit code: 127                   # noqa: E501\n+        and get_nixpkgs_names(bin)             # only match commands which could be made available through nix  # noqa: E501\n+    )\n+\n+\n+def get_new_command(command):\n+    bin = command.script_parts[0]\n+    nixpkgs_names = get_nixpkgs_names(bin)\n+\n+    # Construct a command for each package name\n+    commands = [\n+        'nix-shell -p {} --run \"{}\"'.format(name, command.script)\n+        for name in nixpkgs_names\n+    ]\n+    return commands\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "There are some solid ideas here, particularly regarding integrating with `nix run` for smoother command execution. However, the build errors due to missing modules and deprecation warnings indicate issues with the current setup that need resolution. The suggestion to use an updated rule via a custom configuration is promising, but we should ensure that it doesn't complicate the user experience. Lastly, adding flags for configuration is a good approach, but consider the usability impact of adding multiple options. Let's keep refining this for clarity and stability."
      }
    ],
    "codediff_length": 5539
  },
  {
    "index": 8,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/docs/en/docs/advanced/settings.md b/docs/en/docs/advanced/settings.md\nindex 706a71173e76f..d1428b6ec9b84 100644\n--- a/docs/en/docs/advanced/settings.md\n+++ b/docs/en/docs/advanced/settings.md\n@@ -293,85 +288,17 @@ config.Settings()\n \n a new `Settings` object would be created, and at creation it would read the `.env` file again.\n \n-If the dependency function was just like:\n+If we declared our dependency without `Depends(..., use_cache=\"app\")`, we would create that object for each request, and we would be reading the `.env` file for each request. \n \n-```Python\n-def get_settings():\n-    return config.Settings()\n-```\n-\n-we would create that object for each request, and we would be reading the `.env` file for each request. \n-\n-But as we are using the `@lru_cache()` decorator on top, the `Settings` object will be created only once, the first time it's called. \n+But by setting `use_cache=\"app\"`, the `Settings` object will be created only once, the first time it's called. \n \n ```Python hl_lines=\"1  10\"\n {!../../../docs_src/settings/app03/main.py!}\n ```\n \n-Then for any subsequent calls of `get_settings()` in the dependencies for the next requests, instead of executing the internal code of `get_settings()` and creating a new `Settings` object, it will return the same object that was returned on the first call, again and again.\n-\n-#### `lru_cache` Technical Details\n-\n-`@lru_cache()` modifies the function it decorates to return the same value that was returned the first time, instead of computing it again, executing the code of the function every time.\n-\n-So, the function below it will be executed once for each combination of arguments. And then the values returned by each of those combinations of arguments will be used again and again whenever the function is called with exactly the same combination of arguments.\n-\n-For example, if you have a function:\n-\n-```Python\n-@lru_cache()\n-def say_hi(name: str, salutation: str = \"Ms.\"):\n-    return f\"Hello {salutation} {name}\"\n-```\n-\n-your program could execute like this:\n-\n-```mermaid\n-sequenceDiagram\n-\n-participant code as Code\n-participant function as say_hi()\n-participant execute as Execute function\n-\n-    rect rgba(0, 255, 0, .1)\n-        code ->> function: say_hi(name=\"Camila\")\n-        function ->> execute: execute function code\n-        execute ->> code: return the result\n-    end\n-\n-    rect rgba(0, 255, 255, .1)\n-        code ->> function: say_hi(name=\"Camila\")\n-        function ->> code: return stored result\n-    end\n-\n-    rect rgba(0, 255, 0, .1)\n-        code ->> function: say_hi(name=\"Rick\")\n-        function ->> execute: execute function code\n-        execute ->> code: return the result\n-    end\n-\n-    rect rgba(0, 255, 0, .1)\n-        code ->> function: say_hi(name=\"Rick\", salutation=\"Mr.\")\n-        function ->> execute: execute function code\n-        execute ->> code: return the result\n-    end\n-\n-    rect rgba(0, 255, 255, .1)\n-        code ->> function: say_hi(name=\"Rick\")\n-        function ->> code: return stored result\n-    end\n-\n-    rect rgba(0, 255, 255, .1)\n-        code ->> function: say_hi(name=\"Camila\")\n-        function ->> code: return stored result\n-    end\n-```\n-\n-In the case of our dependency `get_settings()`, the function doesn't even take any arguments, so it always returns the same value.\n-\n-That way, it behaves almost as if it was just a global variable. But as it uses a dependency function, then we can override it easily for testing.\n+Then for any subsequent requests to `/info`, instead of executing the internal code of `get_settings()` and creating a new `Settings` object, the same object will be returned again and again.\n \n-`@lru_cache()` is part of `functools` which is part of Python's standard library, you can read more about it in the <a href=\"https://docs.python.org/3/library/functools.html#functools.lru_cache\" class=\"external-link\" target=\"_blank\">Python docs for `@lru_cache()`</a>.\n+For more details on dependency caching scopes, see [Dependency Cache Scopes](../tutorial/dependencies/dependency-cache-scopes.md){.internal-link target=_blank}\n \n ## Recap\n \n\ndiff --git a/docs/en/docs/tutorial/dependencies/dependency-cache-scopes.md b/docs/en/docs/tutorial/dependencies/dependency-cache-scopes.md\nnew file mode 100644\nindex 0000000000000..baf9b8633e8a4\n--- /dev/null\n+++ b/docs/en/docs/tutorial/dependencies/dependency-cache-scopes.md\n@@ -0,0 +1,128 @@\n+# Dependency Cache Scopes\n+\n+In FastAPI, if one of your dependencies is declared multiple times for the same *path operation*, for example, multiple dependencies have a common sub-dependency, **FastAPI** will know to call that sub-dependency only once per request.\n+\n+And it will save the returned value in a <abbr title=\"A utility/system to store computed/generated values, to re-use them instead of computing them again.\">\"cache\"</abbr> and pass it to all the \"dependants\" that need it in that specific request, instead of calling the dependency multiple times for the same request.\n+\n+But you may not want this behavior. FastAPI let's you control this via *cache scopes*. You can set the cache scope using the `use_cache` parameter to `Depends()`. For example, to disable the cache for a dependency:\n+\n+```Python hl_lines=\"1\"\n+def needy_dependency(fresh_value: str = Depends(get_value, use_cache=False)):\n+    return {\"fresh_value\": fresh_value}\n+```\n+\n+You may also find situations where you only want your dependencies to be created *once*.\n+For example, if you are loading a config from enviroment variables using [Pydantic's settings management](../../advanced/settings.md){.internal-link target=_blank}, you probably only want that to happen once, and be cached therafter.\n+For this situation, set `use_cache=\"app\"` to cache the dependencies value for the lifetime of your application:\n+\n+```Python hl_lines=\"4\"\n+def load_config():\n+    return Config()\n+\n+def dependent(cfg: Config = Depends(load_config, use_cache=\"app\")):\n+    return cfg.some_value\n+```\n+\n+!!! note \"Technical Details\"\n+    FastAPI caches your dependencies based on the callable's identity.\n+    If your dependency is a class or function, the cache will be based on `id(ClassName)` or `id(function_name)`.\n+    However if your dependency is an _instance_ of a class implementing `__call__`, only that particular _instance_ will be cached.\n+\n+## DependencyCacheScopes\n+\n+To be more explicit, you can also use the `fastapi.dependencies.cache.DependencyCacheScopes` enumeration to set values:\n+\n+```Python\n+from fastapi.dependencies.cache import DependencyCacheScopes\n+\n+def dependent(cfg: Config = Depends(load_config, use_cache=DependencyCacheScopes.app)):\n+    return cfg.some_value\n+```\n+\n+!!! note \"Technical Details\"\n+    FastAPI uses `fastapi.dependencies.cache.DependencyCacheScopes` internally.\n+    When you pass in `use_cache={True,False,\"app\"}` it gets converted to a value from `DependencyCacheScopes`.\n+    The scalar values of `{True,False,\"app\"}` are supported so that you don't have to import `fastapi.dependencies.cache.DependencyCacheScopes` all over the place, but if you are using dependency scopes extensively it may be clearer to use `DependencyCacheScopes` directly.\n+\n+## Dependencies in multiple scopes\n+\n+If a dependency is cached in the `\"app\"` scope, it will be used for any dependencies declared with the default scope (`use_cache=True`, scoped to a single request).\n+\n+If you need to use a dependency both in the `\"app\"` scope and seperately at the individual request scope, you have two options.\n+\n+### Disable cache for the shared dependency\n+\n+You can mark the dependencies at the request scope with `Depends(..., use_cache=False)`, but note that they will not be cached at all in the request scope:\n+\n+```Python hl_lines=\"6\"\n+def random_num() -> float:\n+    return random.random()\n+\n+async def dependent(\n+    app_value: float = Depends(random_num, use_cache=\"app\"),\n+    request_value_1: float = Depends(random_num, use_cache=False),\n+    request_value_2: float = Depends(random_num, use_cache=False),\n+):\n+    assert request_value_1 != request_value_2\n+```\n+\n+However, now there will be no caching done at the request level.\n+In this example, `app_value` will always have the same value while `request_value_1` and `request_value_2` will each have a new unique value every call.\n+\n+### Specialize the dependency\n+\n+If you want to have caching at *both* the `\"app\"` scope _and_ the `\"request\"` scope, you can specialize your dependency itself.\n+If your dependency is a class, you can sublcass it:\n+\n+```Python\n+class Dependency:\n+    ...\n+\n+\n+class DependencyApp(Dependency):\n+    ...\n+\n+class DependencyRequest(Dependency):\n+    ...\n+```\n+\n+Or if it's a function, you can wrap it:\n+\n+```Python\n+def random_num_app()\n+    return random_num()\n+\n+def random_num_request():\n+    return random_num()\n+\n+async def dependent(\n+    app_value: float = Depends(random_num_app, use_cache=\"app\"),\n+    request_value_1: float = Depends(random_num_request),\n+    request_value_2: float = Depends(random_num_request),\n+):\n+    assert request_value_1 == request_value_2\n+```\n+\n+Now `app_value` will still be the same every call while `request_value_1` and `request_value_2` will be different every call.\n+But `request_value_1` and `request_value_2` will share the same value, and `random_num_request` will only be called once per request.\n+\n+## Recap\n+\n+In total, **FastAPI** supports **3** _cache scopes_:\n+\n+- `True`: this is the default. The dependency's value is cached for that individual request.\n+- `False`: this completely disables the cache that dependency.\n+- `\"app\"`: the dependency's value is cached for the lifetime of the app.\n+\n+These cache scopes are set via the `use_cache` parameter to `Depends()`.\n+\n+You can also use the values of `fastapi.dependencies.cache.DependencyCacheScopes` directly to be more explicit:\n+\n+- `DependencyCacheScopes.request`: equivalent to `True`.\n+- `DependencyCacheScopes.nocache`: equivalent to `False`.\n+- `DependencyCacheScopes.app`: equivalent to `\"app\"`.\n+\n+!!! note \"Info\"\n+    You may be asking yourself what happens if you ask **FastAPI** to cache your dependency's value for the lifetime of your app when using [Dependencies with yield](dependencies-with-yield.md){.internal-link target=_blank}\n+    The handling of teardown for dependencies with yield can be controlled indpendently of the caching of the dependency's yielded value.\n+    Read onto [Dependency lifetimes (the next section)](dependency-lifetimes.md){.internal-link target=_blank} to find out more!\ndiff --git a/fastapi/dependencies/utils.py b/fastapi/dependencies/utils.py\nindex 923669b94ba85..6176ea39e8925 100644\n--- a/fastapi/dependencies/utils.py\n+++ b/fastapi/dependencies/utils.py\n@@ -462,21 +473,107 @@ async def solve_generator(\n     return await stack.enter_async_context(cm)\n \n \n+async def solve_lifespan_dependencies(\n+    *,\n+    dependant: Dependant,\n+    app_dependency_cache: Dict[DependencyCacheKey, Any],\n+    dependency_overrides_provider: Optional[Any] = None,\n+    dependency_cache: Optional[Dict[DependencyCacheKey, Any]] = None,\n+    stack: Union[None, AsyncExitStack],\n+) -> Tuple[Dict[str, Any], Dict[DependencyCacheKey, Any]]:\n+    values: Dict[str, Any] = {}\n+    sub_dependant: Dependant\n+    dependency_cache = dependency_cache or app_dependency_cache.copy()\n+    for sub_dependant in dependant.dependencies:\n+        sub_dependant.call = cast(Callable[..., Any], sub_dependant.call)\n+        sub_dependant.cache_key = cast(\n+            Tuple[Callable[..., Any], Tuple[str]], sub_dependant.cache_key\n+        )\n+        call = sub_dependant.call\n+        use_sub_dependant = sub_dependant\n+        if (\n+            dependency_overrides_provider\n+            and dependency_overrides_provider.dependency_overrides\n+        ):\n+            original_call = sub_dependant.call\n+            call = getattr(\n+                dependency_overrides_provider, \"dependency_overrides\", {}\n+            ).get(original_call, original_call)\n+            use_sub_dependant = get_dependant(\n+                call=call,\n+                name=sub_dependant.name,\n+            )\n+        solved_result = await solve_lifespan_dependencies(\n+            dependant=use_sub_dependant,\n+            dependency_overrides_provider=dependency_overrides_provider,\n+            app_dependency_cache=app_dependency_cache,\n+            dependency_cache=dependency_cache,\n+            stack=stack,\n+        )\n+        sub_values, sub_dependency_cache = solved_result\n+        dependency_cache.update(sub_dependency_cache)\n+        if (\n+            sub_dependant.use_cache != DependencyCacheScope.nocache\n+            and sub_dependant.cache_key in dependency_cache\n+        ):\n+            solved = dependency_cache[sub_dependant.cache_key]\n+        elif is_gen_callable(call) or is_async_gen_callable(call):\n+            if stack is None:\n+                raise RuntimeError(\n+                    async_contextmanager_dependencies_error\n+                )  # pragma: no cover\n+            solved = await solve_generator(\n+                call=call, stack=stack, sub_values=sub_values\n+            )\n+        elif is_coroutine_callable(call):\n+            solved = await call(**sub_values)\n+        else:\n+            solved = await run_in_threadpool(call, **sub_values)\n+        if sub_dependant.name is not None:\n+            values[sub_dependant.name] = solved\n+        if sub_dependant.cache_key not in dependency_cache:\n+            dependency_cache[sub_dependant.cache_key] = solved\n+        if (\n+            sub_dependant.use_cache == DependencyCacheScope.app\n+            and sub_dependant.cache_key not in app_dependency_cache\n+        ):\n+            app_dependency_cache[sub_dependant.cache_key] = solved\n+    if any(\n+        (\n+            dependant.request_param_name,\n+            dependant.query_params,\n+            dependant.header_params,\n+            dependant.cookie_params,\n+            dependant.body_params,\n+            dependant.request_param_name,\n+            dependant.websocket_param_name,\n+            dependant.http_connection_param_name,\n+            dependant.response_param_name,\n+            dependant.path,\n+        )\n+    ):\n+        raise DependencyResolutionError(\n+            \"Lifespan Dependencies cannot depend on connection parameters\"\n+        )\n+    return values, dependency_cache\n+\n+\n async def solve_dependencies(\n     *,\n     request: Union[Request, WebSocket],\n     dependant: Dependant,\n+    app_dependency_cache: Dict[DependencyCacheKey, Any],\n     body: Optional[Union[Dict[str, Any], FormData]] = None,\n     background_tasks: Optional[BackgroundTasks] = None,\n     response: Optional[Response] = None,\n     dependency_overrides_provider: Optional[Any] = None,\n-    dependency_cache: Optional[Dict[Tuple[Callable[..., Any], Tuple[str]], Any]] = None,\n+    dependency_cache: Optional[Dict[DependencyCacheKey, Any]] = None,\n ) -> Tuple[\n     Dict[str, Any],\n     List[ErrorWrapper],\n     Optional[BackgroundTasks],\n     Response,\n-    Dict[Tuple[Callable[..., Any], Tuple[str]], Any],\n+    Dict[DependencyCacheKey, Any],\n ]:\n     values: Dict[str, Any] = {}\n     errors: List[ErrorWrapper] = []\n\ndiff --git a/tests/test_dependency_lifetime.py b/tests/test_dependency_lifetime.py\nnew file mode 100644\nindex 0000000000000..5b0d16fa64ff3\n--- /dev/null\n+++ b/tests/test_dependency_lifetime.py\n@@ -0,0 +1,221 @@\n+import asyncio\n+from threading import Event\n+from time import sleep\n+from typing import AsyncGenerator, Callable, Generator, List\n+\n+import pytest\n+from fastapi import Depends, FastAPI, HTTPException\n+from fastapi.dependencies.lifetime import DependencyLifetime\n+from fastapi.testclient import TestClient\n+\n+DESTROY_TIME = 0.1  # simulate work\n+\n+\n+class CallableDependency:\n+    def __init__(self) -> None:\n+        self.counter = 0\n+\n+\n+class SyncCallableDep(CallableDependency):\n+    def __call__(self) -> \"SyncCallableDep\":\n+        self.counter += 1\n+        return self\n+\n+\n+class AsyncCallableDep(CallableDependency):\n+    async def __call__(self) -> \"AsyncCallableDep\":\n+        self.counter += 1\n+        return self\n+\n+\n+class LifespanEvents:\n+    def __init__(self) -> None:\n+        self.destroying = Event()\n+        self.destroyed = Event()\n+\n+\n+class GeneratorDependency:\n+    def __init__(self):\n+        self.counter = 0\n+        self.lifespan_events: List[LifespanEvents] = []\n+\n+\n+class SyncGeneratorDep(GeneratorDependency):\n+    def __call__(self) -> Generator[\"SyncGeneratorDep\", None, None]:\n+        self.counter += 1\n+        events = LifespanEvents()\n+        self.lifespan_events.append(events)\n+        yield self\n+        events.destroying.set()\n+        sleep(DESTROY_TIME)  # simulate work\n+        events.destroyed.set()\n+\n+\n+class AsyncGeneratorDep(GeneratorDependency):\n+    async def __call__(self) -> AsyncGenerator[\"AsyncGeneratorDep\", None]:\n+        self.counter += 1\n+        events = LifespanEvents()\n+        self.lifespan_events.append(events)\n+        yield self\n+        events.destroying.set()\n+        await asyncio.sleep(DESTROY_TIME)  # simulate work\n+        events.destroyed.set()\n+\n+\n+@pytest.mark.parametrize(\"dep_cls\", (SyncGeneratorDep, AsyncGeneratorDep))\n+def test_request_lifetime_context_manager(dep_cls: Callable[[], GeneratorDependency]):\n+\n+    dep = dep_cls()\n+\n+    app = FastAPI()\n+\n+    @app.get(\"/\", dependencies=[Depends(dep, lifetime=DependencyLifetime.request)])\n+    def root():\n+        assert not dep.lifespan_events[-1].destroying.is_set()\n+\n+    with TestClient(app) as client:\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 1\n+        assert dep.lifespan_events[0].destroying.is_set()\n+        assert dep.lifespan_events[0].destroyed.wait(DESTROY_TIME)\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 2\n+        assert dep.lifespan_events[1].destroyed.wait(DESTROY_TIME)\n+\n+\n+@pytest.mark.parametrize(\"dep_cls\", (SyncCallableDep, AsyncCallableDep))\n+def test_request_lifetime_callable(dep_cls: Callable[[], CallableDependency]):\n+\n+    dep = dep_cls()\n+\n+    app = FastAPI()\n+\n+    @app.get(\"/\", dependencies=[Depends(dep, lifetime=DependencyLifetime.request)])\n+    def root():\n+        ...\n+\n+    with TestClient(app) as client:\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 1\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 2\n+\n+\n+@pytest.mark.parametrize(\"dep_cls\", (SyncGeneratorDep, AsyncGeneratorDep))\n+def test_app_lifetime_context_manager(dep_cls: Callable[[], GeneratorDependency]):\n+\n+    dep = dep_cls()\n+\n+    app = FastAPI()\n+\n+    @app.get(\"/\", dependencies=[Depends(dep, lifetime=DependencyLifetime.app)])\n+    def root():\n+        assert not dep.lifespan_events[-1].destroying.is_set()\n+\n+    with TestClient(app) as client:\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 1\n+        assert not dep.lifespan_events[0].destroying.is_set()\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 2\n+        assert not dep.lifespan_events[1].destroying.is_set()\n+    assert all(e.destroyed.is_set() for e in dep.lifespan_events)\n+\n+\n+@pytest.mark.parametrize(\"dep_cls\", (SyncCallableDep, AsyncCallableDep))\n+def test_app_lifetime_callable(dep_cls: Callable[[], CallableDependency]):\n+\n+    dep = dep_cls()\n+\n+    app = FastAPI()\n+\n+    @app.get(\"/\", dependencies=[Depends(dep, lifetime=DependencyLifetime.app)])\n+    def root():\n+        ...\n+\n+    with TestClient(app) as client:\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 1\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 2\n+\n+\n+@pytest.mark.parametrize(\"dep_cls\", (SyncGeneratorDep, AsyncGeneratorDep))\n+def test_endpoint_lifetime_context_manager(dep_cls: Callable[[], GeneratorDependency]):\n+\n+    dep = dep_cls()\n+\n+    app = FastAPI()\n+\n+    @app.get(\"/\", dependencies=[Depends(dep, lifetime=DependencyLifetime.endpoint)])\n+    def root():\n+        assert not dep.lifespan_events[-1].destroying.is_set()\n+\n+    with TestClient(app) as client:\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 1\n+        assert dep.lifespan_events[0].destroyed.is_set()\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 2\n+        assert dep.lifespan_events[1].destroyed.is_set()\n+\n+\n+def test_endpoint_lifetime_context_manager_cleanup_raises_HTTPException():\n+    def error_gen():\n+        yield\n+        raise HTTPException(400)  # should set status code\n+\n+    app = FastAPI()\n+\n+    called = False\n+\n+    @app.get(\n+        \"/\", dependencies=[Depends(error_gen, lifetime=DependencyLifetime.endpoint)]\n+    )\n+    def root():\n+        nonlocal called\n+        called = True\n+\n+    with TestClient(app) as client:\n+        res = client.get(\"/\")\n+        assert res.status_code == 400\n+        assert called\n+\n+\n+def test_endpoint_lifetime_context_manager_cleanup_raises_Exception():\n+    def error_gen():\n+        yield\n+        raise Exception()  # results in 500 status code\n+\n+    app = FastAPI()\n+\n+    called = False\n+\n+    @app.get(\n+        \"/\", dependencies=[Depends(error_gen, lifetime=DependencyLifetime.endpoint)]\n+    )\n+    def root():\n+        nonlocal called\n+        called = True\n+\n+    with TestClient(app, raise_server_exceptions=False) as client:\n+        assert client.get(\"/\").status_code == 500\n+        assert called\n+\n+\n+@pytest.mark.parametrize(\"dep_cls\", (SyncCallableDep, AsyncCallableDep))\n+def test_endpoint_lifetime_callable(dep_cls: Callable[[], CallableDependency]):\n+\n+    dep = dep_cls()\n+\n+    app = FastAPI()\n+\n+    @app.get(\"/\", dependencies=[Depends(dep, lifetime=DependencyLifetime.endpoint)])\n+    def root():\n+        ...\n+\n+    with TestClient(app) as client:\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 1\n+        assert client.get(\"/\").status_code == 200\n+        assert dep.counter == 2\ndiff --git a/tests/test_lifespan_dependencies.py b/tests/test_lifespan_dependencies.py\nnew file mode 100644\nindex 0000000000000..7f8198b3d6520\n--- /dev/null\n+++ b/tests/test_lifespan_dependencies.py\n@@ -0,0 +1,185 @@\n+from typing import Callable\n+\n+import pytest\n+from fastapi import Depends, FastAPI, Request\n+from fastapi.exceptions import DependencyResolutionError\n+from fastapi.testclient import TestClient\n+\n+\n+class BaseDependency:\n+    def __init__(self) -> None:\n+        self.counter = 0\n+        self.constructed, self.destructed = False, False\n+\n+\n+class SyncCallable(BaseDependency):\n+    def __call__(self) -> \"SyncCallable\":\n+        self.counter += 1\n+        self.constructed, self.destructed = True, True\n+        return self\n+\n+\n+class AsyncCallable(BaseDependency):\n+    async def __call__(self) -> \"AsyncCallable\":\n+        self.counter += 1\n+        self.constructed, self.destructed = True, True\n+        return self\n+\n+\n+class SyncGenerator(BaseDependency):\n+    def __call__(self) -> \"SyncGenerator\":\n+        self.counter += 1\n+        self.constructed = True\n+        yield self\n+        self.destructed = True\n+\n+\n+class AsyncGenerator(BaseDependency):\n+    async def __call__(self) -> \"AsyncGenerator\":\n+        self.counter += 1\n+        self.constructed = True\n+        yield self\n+        self.destructed = True\n+\n+\n+@pytest.mark.parametrize(\n+    \"dependency_cls, stateless\",\n+    [\n+        (SyncCallable, True),\n+        (AsyncCallable, True),\n+        (SyncGenerator, False),\n+        (AsyncGenerator, False),\n+    ],\n+)\n+def test_startup_dependencies(\n+    dependency_cls: Callable[[], BaseDependency], stateless: bool\n+):\n+\n+    dependency = dependency_cls()\n+\n+    def startup(placeholder: None = Depends(dependency)):\n+        ...\n+\n+    app = FastAPI(on_startup=[startup])\n+\n+    with TestClient(app):\n+        assert dependency.constructed\n+        assert dependency.destructed == stateless\n+    assert dependency.destructed\n+\n+\n+@pytest.mark.parametrize(\n+    \"dependency_cls, stateless\",\n+    [\n+        (SyncCallable, True),\n+        (AsyncCallable, True),\n+        (SyncGenerator, False),\n+        (AsyncGenerator, False),\n+    ],\n+)\n+def test_shutdown_dependencies(\n+    dependency_cls: Callable[[], BaseDependency], stateless: bool\n+):\n+\n+    dependency = dependency_cls()\n+\n+    def shutdown(placeholder: dependency_cls = Depends(dependency)):\n+        ...\n+\n+    app = FastAPI(on_shutdown=[shutdown])\n+\n+    with TestClient(app):\n+        assert not dependency.constructed\n+    assert dependency.destructed\n+\n+\n+def test_dependency_reset_on_shutdown():\n+    \"\"\"lifespan dependencies should be reset when the app shuts down\"\"\"\n+    dependency = SyncCallable()\n+\n+    def startup(placeholder: SyncCallable = Depends(dependency)):\n+        ...\n+\n+    app = FastAPI(on_startup=[startup])\n+\n+    for lifecycle in range(2):\n+        with TestClient(app):\n+            assert dependency.constructed\n+            assert dependency.counter == lifecycle + 1\n+        assert dependency.destructed\n+\n+\n+@pytest.mark.parametrize(\"use_cache\", (True, False))\n+def test_dependency_caching(use_cache: bool):\n+    \"\"\"Startup/shutdown dependencies are cached, unless `use_cache=False` is passed\"\"\"\n+\n+    def child_dep():\n+        return object()\n+\n+    class ParentDep(BaseDependency):\n+        def __call__(self, child: object = Depends(child_dep)) -> \"ParentDep\":\n+            self.counter += 1\n+            self.child = child\n+            return self\n+\n+    parent_dep = ParentDep()\n+\n+    def startup(\n+        parent: ParentDep = Depends(parent_dep),\n+        child: object = Depends(child_dep, use_cache=use_cache),\n+    ):\n+        assert (parent.child is child) == use_cache\n+\n+    app = FastAPI(on_startup=[startup])\n+\n+    with TestClient(app):\n+        ...\n+\n+\n+def test_overrides():\n+    \"\"\"Dependency overrides set BEFORE startup should be honored\"\"\"\n+\n+    def real() -> int:\n+        return 1\n+\n+    def fake() -> int:\n+        return 2\n+\n+    class StartupRecorder:\n+        v = None\n+\n+        def __call__(self, v: int = Depends(real)):\n+            self.v = v\n+\n+    startup = StartupRecorder()\n+\n+    app = FastAPI(on_startup=[startup])\n+\n+    with TestClient(app):\n+        assert startup.v == 1  # from real\n+\n+    app.dependency_overrides[real] = fake\n+    with TestClient(app):\n+        assert startup.v == 2  # from fake\n+\n+\n+def test_startup_requests_request():\n+    \"\"\"Startup events cannot depend on Request, even indirectly\"\"\"\n+\n+    def dep(placeholder: Request):\n+        ...  # pragma: no cover\n+\n+    def startup_direct(placeholder: Request):\n+        ...  # pragma: no cover\n+\n+    def startup_indirect(placeholder: None = Depends(dep)):\n+        ...  # pragma: no cover\n+\n+    for startup in (startup_direct, startup_indirect):\n+        app = FastAPI(on_startup=[startup])\n+\n+        with pytest.raises(\n+            DependencyResolutionError, match=\"cannot depend on connection parameters\"\n+        ):\n+            with TestClient(app):\n+                ...  # pragma: no cover\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR implements a robust system for handling synchronous and asynchronous dependency lifespans in FastAPI. Key decisions include leveraging `AsyncContextStack` for managing app lifetimes and caching dependencies efficiently. The change redefines the lifecycle events, allowing users to specify dependencies' lifespan flexibly. However, compatibility with Starlette's existing lifespan hooks remains a concern; integrating this feature risks conflicts with existing startup hooks. Some questions about caching behavior and thread-safety remain, suggesting the need for clear documentation and possibly future adjustments to ensure seamless integration. Great work overall!"
      }
    ],
    "codediff_length": 27083
  },
  {
    "index": 9,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/docs/en/docs/advanced/sql-databases-sqlalchemy.md b/docs/en/docs/advanced/sql-databases-sqlalchemy.md\nnew file mode 100644\nindex 0000000000000..ab22087d31f3a\n--- /dev/null\n+++ b/docs/en/docs/advanced/sql-databases-sqlalchemy.md\n@@ -0,0 +1,85 @@\n+# Async Databases with SQLAlchemy\n+\n+This tutorial will cover the usage of SQLAlchemy async parts as an ORM.\n+\n+!!! tip\n+    We strongly suggest going through the non async tutorial first ([SQL (Relational) Databases](../tutorial/sql-databases.md){.internal-link target=_blank}), since this tutorial will only cover the async parts of SQLAlchemy.\n+\n+!!! warning\n+    This tutorial expects SQLAlchemy >= 1.4.\n+\n+## File structure\n+\n+For this example, let's say you have a directory named `my_super_project` that contains a sub-directory called `sql_app` with a structure like this:\n+\n+```\n+.\n+ sql_app\n+     __init__.py\n+     crud.py\n+     database.py\n+     main.py\n+     models.py\n+     schemas.py\n+```\n+\n+The file `__init__.py` is just an empty file, but it tells Python that `sql_app` with all its modules (Python files) is a package.\n+\n+Now let's see what each file/module does.\n+\n+## Create the `SQLAlchemy` parts\n+\n+To use the async parts you will need to import `create_async_engine` instead of the base engine.\n+\n+```Python hl_lines=\"1  4  7\"\n+{!../../../docs_src/async_sql_databases/sql_app/database.py!}\n+```\n+\n+!!! tip\n+    If your using PostgreSQL you need to append `asyncpg` to the database string, as shown above.\n+\n+    You will also need to `pip install asyncpg`\n+\n+## Create models and schemas\n+\n+This step in unchanged from the non-async way, so we'll just copy the code in, if you want more detail on how it works ([SQL (Relational) Databases](../tutorial/sql-databases.md){.internal-link target=_blank}).\n+\n+```Python\n+{!../../../docs_src/async_sql_databases/sql_app/models.py!}\n+```\n+\n+```Python\n+{!../../../docs_src/async_sql_databases/sql_app/schemas.py!}\n+```\n+\n+## CRUD utils\n+\n+```Python\n+{!../../../docs_src/async_sql_databases/sql_app/crud.py!}\n+```\n+\n+!!! Note\n+    Notice that as we communicate with the database using `await`, the *function* is declared with `async`.\n+\n+## Main FastAPI app\n+\n+On startup we run the engine to create all the declarative_base models, you can also drop all of them on each start.\n+\n+Notice how we are creating an instance of `AsyncSession` instead of the usual `Session`, this is also what we pass as the db type.\n+\n+```Python\n+{!../../../docs_src/async_sql_databases/sql_app/main.py!}\n+```\n+\n+## Check it\n+\n+You can copy this code as is, and see the docs at <a href=\"http://127.0.0.1:8000/docs\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/docs</a>.\n+\n+There you can see all your API documented and interact with it:\n+\n+<img src=\"/img/tutorial/sql-databases/image01.png\">\n+\n+\n+## More info\n+\n+You can read more about <a href=\"https://docs.sqlalchemy.org/en/14/orm/extensions/asyncio.html\" class=\"external-link\" target=\"_blank\">`SQLAlchemy` at its Docs page</a>.\ndiff --git a/docs_src/async_sql_databases/sql_app/crud.py b/docs_src/async_sql_databases/sql_app/crud.py\nnew file mode 100644\nindex 0000000000000..89620e4ddc9bc\n--- /dev/null\n+++ b/docs_src/async_sql_databases/sql_app/crud.py\n@@ -0,0 +1,50 @@\n+from sqlalchemy import select\n+from sqlalchemy.ext.asyncio import AsyncSession\n+\n+from . import models, schemas\n+\n+\n+async def get_user(db: AsyncSession, user_id: int):\n+    db_execute = await db.execute(select(models.User).where(models.User.id == user_id))\n+    return db_execute.scalars().first()\n+\n+\n+async def get_user_by_email(db: AsyncSession, email: str):\n+    db_execute = await db.execute(select(models.User).where(models.User.email == email))\n+    return db_execute.scalars().first()\n+\n+\n+async def get_users(db: AsyncSession, skip: int = 0, limit: int = 100):\n+    db_execute = await db.execute(select(models.User).offset(skip).limit(limit))\n+    return db_execute.scalars().all()\n+\n+\n+async def create_user(db: AsyncSession, user: schemas.UserCreate):\n+    fake_hashed_password = user.password + \"notreallyhashed\"\n+    db_user = models.User(email=user.email, hashed_password=fake_hashed_password)\n+    db.add(db_user)\n+    await db.commit()\n+    await db.refresh(db_user)\n+    return db_user\n+\n+\n+async def delete_user(db: AsyncSession, user_id: int):\n+    db_execute = await db.execute(select(models.User).where(models.User.id == user_id))\n+    db_user = db_execute.scalars().first()\n+    if db_user is None:\n+        raise HTTPException(status_code=404, detail=\"User not found\")\n+    await db.delete(db_user)\n+    await db.commit()\n+\n+\n+async def get_items(db: AsyncSession, skip: int = 0, limit: int = 100):\n+    db_execute = await db.execute(select(models.Item).offset(skip).limit(limit))\n+    return db_execute.scalars().all()\n+\n+\n+async def create_user_item(db: AsyncSession, item: schemas.ItemCreate, user_id: int):\n+    db_item = models.Item(**item.dict(), owner_id=user_id)\n+    db.add(db_item)\n+    await db.commit()\n+    await db.refresh(db_item)\n+    return db_item\ndiff --git a/docs_src/async_sql_databases/sql_app/main.py b/docs_src/async_sql_databases/sql_app/main.py\nnew file mode 100644\nindex 0000000000000..87438cf6d5532\n--- /dev/null\n+++ b/docs_src/async_sql_databases/sql_app/main.py\n@@ -0,0 +1,67 @@\n+from typing import List\n+\n+from fastapi import Depends, FastAPI, HTTPException\n+from sqlalchemy.ext.asyncio import AsyncSession\n+\n+from . import crud, schemas\n+from .database import Base, engine\n+\n+app = FastAPI()\n+\n+\n+@app.on_event(\"startup\")\n+async def start_db():\n+    async with engine.begin() as conn:\n+        # await conn.run_sync(Base.metadata.drop_all)\n+        await conn.run_sync(Base.metadata.create_all)\n+\n+\n+# Dependency\n+async def get_db():\n+    session = AsyncSession(engine, expire_on_commit=True)\n+    try:\n+        yield session\n+    finally:\n+        await session.close()\n+\n+\n+@app.post(\"/users/\", response_model=schemas.User)\n+async def create_user(user: schemas.UserCreate, db: AsyncSession = Depends(get_db)):\n+    db_user = await crud.get_user_by_email(db, email=user.email)\n+    if db_user:\n+        raise HTTPException(status_code=400, detail=\"Email already registered\")\n+    return await crud.create_user(db=db, user=user)\n+\n+\n+@app.get(\"/users/\", response_model=List[schemas.User])\n+async def read_users(\n+    skip: int = 0, limit: int = 100, db: AsyncSession = Depends(get_db)\n+):\n+    return await crud.get_users(db, skip=skip, limit=limit)\n+\n+\n+@app.get(\"/users/{user_id}\", response_model=schemas.User)\n+async def read_user(user_id: int, db: AsyncSession = Depends(get_db)):\n+    db_user = await crud.get_user(db, user_id=user_id)\n+    if db_user is None:\n+        raise HTTPException(status_code=404, detail=\"User not found\")\n+    return db_user\n+\n+\n+@app.delete(\"/users/{user_id}\", response_model=schemas.User)\n+async def delete_user(user_id: int, db: AsyncSession = Depends(get_db)):\n+    await crud.delete_user(db, user_id=user_id)\n+\n+\n+@app.post(\"/users/{user_id}/items/\", response_model=schemas.Item)\n+async def create_item_for_user(\n+    user_id: int, item: schemas.ItemCreate, db: AsyncSession = Depends(get_db)\n+):\n+    return await crud.create_user_item(db=db, item=item, user_id=user_id)\n+\n+\n+@app.get(\"/items/\", response_model=List[schemas.Item])\n+async def read_items(\n+    skip: int = 0, limit: int = 100, db: AsyncSession = Depends(get_db)\n+):\n+    return await crud.get_items(db, skip=skip, limit=limit)\ndiff --git a/docs_src/async_sql_databases/sql_app/tests/test_sql_app.py b/docs_src/async_sql_databases/sql_app/tests/test_sql_app.py\nnew file mode 100644\nindex 0000000000000..f5710f23d5520\n--- /dev/null\n+++ b/docs_src/async_sql_databases/sql_app/tests/test_sql_app.py\n@@ -0,0 +1,50 @@\n+from fastapi.testclient import TestClient\n+from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\n+\n+from ..database import Base\n+from ..main import app, get_db\n+\n+SQLALCHEMY_DATABASE_URL = \"sqlite+aiosqlite:///./test.db\"\n+\n+# connect_args is ONLY for sqlite, remove if using PostgreSQL or other.\n+engine = create_async_engine(\n+    SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False}\n+)\n+\n+\n+@app.on_event(\"startup\")\n+async def start_db():\n+    async with engine.begin() as conn:\n+        await conn.run_sync(Base.metadata.drop_all)\n+        await conn.run_sync(Base.metadata.create_all)\n+\n+\n+# Dependency\n+async def override_get_db():\n+    session = AsyncSession(engine)\n+    try:\n+        yield session\n+    finally:\n+        await session.close()\n+\n+\n+app.dependency_overrides[get_db] = override_get_db\n+\n+\n+def test_create_user():\n+    with TestClient(app) as client:\n+        response = client.post(\n+            \"/users/\",\n+            json={\"email\": \"deadpool@example.com\", \"password\": \"chimichangas4life\"},\n+        )\n+        assert response.status_code == 200, response.text\n+        data = response.json()\n+        assert data[\"email\"] == \"deadpool@example.com\"\n+        assert \"id\" in data\n+        user_id = data[\"id\"]\n+\n+        response = client.get(f\"/users/{user_id}\")\n+        assert response.status_code == 200, response.text\n+        data = response.json()\n+        assert data[\"email\"] == \"deadpool@example.com\"\n+        assert data[\"id\"] == user_id\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR discusses an unresolved issue with `pydantic`'s `response_model`, highlighting that the test fails due to this. You provided workarounds and noted the nuances of async SQLAlchemy, particularly around relationships and session management. You've identified the importance of `lazy` loading options for better performance and suggested removing the `\"check_same_thread\"` parameter for PostgreSQL. Additionally, the `get_db` dependency is causing a `TypeError`, which needs addressing, while a working solution is shared using `AsyncGenerator`. Further documentation on these async behaviors would be beneficial."
      }
    ],
    "codediff_length": 9237
  },
  {
    "index": 10,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/core/cache/backends/redis.py b/django/core/cache/backends/redis.py\nnew file mode 100644\nindex 000000000000..16556b1ded5c\n--- /dev/null\n+++ b/django/core/cache/backends/redis.py\n@@ -0,0 +1,224 @@\n+\"\"\"Redis cache backend.\"\"\"\n+\n+import random\n+import re\n+\n+from django.core.cache.backends.base import DEFAULT_TIMEOUT, BaseCache\n+from django.core.serializers.base import PickleSerializer\n+from django.utils.functional import cached_property\n+from django.utils.module_loading import import_string\n+\n+\n+class RedisSerializer(PickleSerializer):\n+    def dumps(self, obj):\n+        if isinstance(obj, int):\n+            return obj\n+        return super().dumps(obj)\n+\n+    def loads(self, data):\n+        try:\n+            return int(data)\n+        except ValueError:\n+            return super().loads(data)\n+\n+\n+class RedisCacheClient:\n+    def __init__(\n+        self,\n+        servers,\n+        serializer=None,\n+        db=None,\n+        pool_class=None,\n+        parser_class=None,\n+    ):\n+        import redis\n+\n+        self._lib = redis\n+        self._servers = servers\n+        self._pools = {}\n+\n+        self._client = self._lib.Redis\n+\n+        if isinstance(pool_class, str):\n+            pool_class = import_string(pool_class)\n+        self._pool_class = pool_class or self._lib.ConnectionPool\n+\n+        if isinstance(serializer, str):\n+            serializer = import_string(serializer)\n+        if callable(serializer):\n+            serializer = serializer()\n+        self._serializer = serializer or RedisSerializer()\n+\n+        if isinstance(parser_class, str):\n+            parser_class = import_string(parser_class)\n+        parser_class = parser_class or self._lib.connection.DefaultParser\n+\n+        self._pool_options = {'parser_class': parser_class, 'db': db}\n+\n+    def _get_connection_pool_index(self, write):\n+        # Write to the first server. Read from other servers if there are more,\n+        # otherwise read from the first server.\n+        if write or len(self._servers) == 1:\n+            return 0\n+        return random.randint(1, len(self._servers) - 1)\n+\n+    def _get_connection_pool(self, write):\n+        index = self._get_connection_pool_index(write)\n+        if index not in self._pools:\n+            self._pools[index] = self._pool_class.from_url(\n+                self._servers[index], **self._pool_options,\n+            )\n+        return self._pools[index]\n+\n+    def get_client(self, key=None, *, write=False):\n+        # key is used so that the method signature remains the same and custom\n+        # cache client can be implemented which might require the key to select\n+        # the server, e.g. sharding.\n+        pool = self._get_connection_pool(write)\n+        return self._client(connection_pool=pool)\n+\n+    def add(self, key, value, timeout):\n+        client = self.get_client(key, write=True)\n+        value = self._serializer.dumps(value)\n+\n+        if timeout == 0:\n+            if ret := bool(client.set(key, value, nx=True)):\n+                client.delete(key)\n+            return ret\n+        else:\n+            return bool(client.set(key, value, ex=timeout, nx=True))\n+\n+    def get(self, key, default):\n+        client = self.get_client(key)\n+        value = client.get(key)\n+        return default if value is None else self._serializer.loads(value)\n+\n+    def set(self, key, value, timeout):\n+        client = self.get_client(key, write=True)\n+        value = self._serializer.dumps(value)\n+        if timeout == 0:\n+            client.delete(key)\n+        else:\n+            client.set(key, value, ex=timeout)\n+\n+    def touch(self, key, timeout):\n+        client = self.get_client(key, write=True)\n+        if timeout is None:\n+            return bool(client.persist(key))\n+        else:\n+            return bool(client.expire(key, timeout))\n+\n+    def delete(self, key):\n+        client = self.get_client(key, write=True)\n+        return bool(client.delete(key))\n+\n+    def get_many(self, keys):\n+        client = self.get_client(None)\n+        ret = client.mget(keys)\n+        return {\n+            k: self._serializer.loads(v) for k, v in zip(keys, ret) if v is not None\n+        }\n+\n+    def has_key(self, key):\n+        client = self.get_client(key)\n+        return bool(client.exists(key))\n+\n+    def incr(self, key, delta):\n+        client = self.get_client(key)\n+        if not client.exists(key):\n+            raise ValueError(\"Key '%s' not found.\" % key)\n+        return client.incr(key, delta)\n+\n+    def set_many(self, data, timeout):\n+        client = self.get_client(None, write=True)\n+        pipeline = client.pipeline()\n+        pipeline.mset({k: self._serializer.dumps(v) for k, v in data.items()})\n+\n+        if timeout is not None:\n+            # Setting timeout for each key as redis does not support timeout\n+            # with mset().\n+            for key in data:\n+                pipeline.expire(key, timeout)\n+        pipeline.execute()\n+\n+    def delete_many(self, keys):\n+        client = self.get_client(None, write=True)\n+        client.delete(*keys)\n+\n+    def clear(self):\n+        client = self.get_client(None, write=True)\n+        return bool(client.flushdb())\n+\n+\n+class RedisCache(BaseCache):\n+    def __init__(self, server, params):\n+        super().__init__(params)\n+        if isinstance(server, str):\n+            self._servers = re.split('[;,]', server)\n+        else:\n+            self._servers = server\n+\n+        self._class = RedisCacheClient\n+        self._options = params.get('OPTIONS', {})\n+\n+    @cached_property\n+    def _cache(self):\n+        return self._class(self._servers, **self._options)\n+\n+    def get_backend_timeout(self, timeout=DEFAULT_TIMEOUT):\n+        if timeout == DEFAULT_TIMEOUT:\n+            timeout = self.default_timeout\n+        # The key will be made persistent if None used as a timeout.\n+        # Non-positive values will cause the key to be deleted.\n+        return None if timeout is None else max(0, int(timeout))\n+\n+    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n+\n+    def get(self, key, default=None, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.get(key, default)\n+\n+    def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        self._cache.set(key, value, self.get_backend_timeout(timeout))\n+\n+    def touch(self, key, timeout=DEFAULT_TIMEOUT, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.touch(key, self.get_backend_timeout(timeout))\n+\n+    def delete(self, key, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.delete(key)\n+\n+    def get_many(self, keys, version=None):\n+        key_map = {self.make_and_validate_key(key, version=version): key for key in keys}\n+        ret = self._cache.get_many(key_map.keys())\n+        return {key_map[k]: v for k, v in ret.items()}\n+\n+    def has_key(self, key, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.has_key(key)\n+\n+    def incr(self, key, delta=1, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.incr(key, delta)\n+\n+    def set_many(self, data, timeout=DEFAULT_TIMEOUT, version=None):\n+        safe_data = {}\n+        for key, value in data.items():\n+            key = self.make_and_validate_key(key, version=version)\n+            safe_data[key] = value\n+        self._cache.set_many(safe_data, self.get_backend_timeout(timeout))\n+        return []\n+\n+    def delete_many(self, keys, version=None):\n+        safe_keys = []\n+        for key in keys:\n+            key = self.make_and_validate_key(key, version=version)\n+            safe_keys.append(key)\n+        self._cache.delete_many(safe_keys)\n+\n+    def clear(self):\n+        return self._cache.clear()\ndiff --git a/docs/topics/cache.txt b/docs/topics/cache.txt\nindex 0f25260672ce..3bc35fd51d06 100644\n--- a/docs/topics/cache.txt\n+++ b/docs/topics/cache.txt\n@@ -169,6 +168,71 @@ particularly temporary.\n     some problems and seems to be unmaintained. Use ``PyMemcacheCache`` or\n     ``PyLibMCCache`` instead.\n \n+.. _redis:\n+\n+Redis\n+-----\n+\n+.. versionadded:: 4.0\n+\n+Redis__ is an in-memory database that can be used for caching. To begin you'll\n+need a Redis server running either locally or on a remote machine.\n+\n+__ https://redis.io/\n+\n+After setting up the Redis server, you'll need to install Python bindings for\n+Redis. `redis-py`_ is the binding supported natively by Django. Installing the\n+additional `hiredis-py`_ package is also recommended.\n+\n+.. _`redis-py`: https://pypi.org/project/redis/\n+.. _`hiredis-py`: https://pypi.org/project/hiredis/\n+\n+To use Redis as your cache backend with Django:\n+\n+* Set :setting:`BACKEND <CACHES-BACKEND>` to\n+  ``django.core.cache.backends.redis.RedisCache``.\n+\n+* Set :setting:`LOCATION <CACHES-LOCATION>` to the URL pointing to your Redis\n+  instance, using the appropriate scheme. See the ``redis-py`` docs for\n+  `details on the available schemes\n+  <https://redis-py.readthedocs.io/en/stable/#redis.ConnectionPool.from_url>`_.\n+\n+For example, if Redis is running on localhost (127.0.0.1) port 6379::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': 'redis://127.0.0.1:6379',\n+        }\n+    }\n+\n+Often Redis servers are protected with authentication. In order to supply a\n+username and password, add them in the ``LOCATION`` along with the URL::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': 'redis://username:password@127.0.0.1:6379',\n+        }\n+    }\n+\n+If you have multiple Redis servers set up in the replication mode, you can\n+specify the servers either as a semicolon or comma delimited string, or as a\n+list. While using multiple servers, write operations are performed on the first\n+server (leader). Read operations are performed on the other servers (replicas)\n+chosen at random::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': [\n+                'redis://127.0.0.1:6379', # leader\n+                'redis://127.0.0.1:6378', # read-replica 1\n+                'redis://127.0.0.1:6377', # read-replica 2\n+            ],\n+        }\n+    }\n+\n .. _database-caching:\n \n Database caching\n\ndiff --git a/tests/cache/tests.py b/tests/cache/tests.py\nindex 0e2f5f7d1f04..b88066285801 100644\n--- a/tests/cache/tests.py\n+++ b/tests/cache/tests.py\n@@ -1727,6 +1732,60 @@ def test_empty_cache_file_considered_expired(self):\n             self.assertIs(cache._is_expired(fh), True)\n \n \n+@unittest.skipUnless(RedisCache_params, \"Redis backend not configured\")\n+@override_settings(CACHES=caches_setting_for_tests(\n+    base=RedisCache_params,\n+    exclude=redis_excluded_caches,\n+))\n+class RedisCacheTests(BaseCacheTests, TestCase):\n+\n+    def setUp(self):\n+        import redis\n+        super().setUp()\n+        self.lib = redis\n+\n+    @property\n+    def incr_decr_type_error(self):\n+        return self.lib.ResponseError\n+\n+    def test_cache_client_class(self):\n+        self.assertIs(cache._class, RedisCacheClient)\n+        self.assertIsInstance(cache._cache, RedisCacheClient)\n+\n+    def test_get_backend_timeout_method(self):\n+        positive_timeout = 10\n+        positive_backend_timeout = cache.get_backend_timeout(positive_timeout)\n+        self.assertEqual(positive_backend_timeout, positive_timeout)\n+\n+        negative_timeout = -5\n+        negative_backend_timeout = cache.get_backend_timeout(negative_timeout)\n+        self.assertEqual(negative_backend_timeout, 0)\n+\n+        none_timeout = None\n+        none_backend_timeout = cache.get_backend_timeout(none_timeout)\n+        self.assertIsNone(none_backend_timeout)\n+\n+    def test_get_connection_pool_index(self):\n+        pool_index = cache._cache._get_connection_pool_index(write=True)\n+        self.assertEqual(pool_index, 0)\n+        pool_index = cache._cache._get_connection_pool_index(write=False)\n+        if len(cache._cache._servers) == 1:\n+            self.assertEqual(pool_index, 0)\n+        else:\n+            self.assertGreater(pool_index, 0)\n+            self.assertLess(pool_index, len(cache._cache._servers))\n+\n+    def test_get_connection_pool(self):\n+        pool = cache._cache._get_connection_pool(write=True)\n+        self.assertIsInstance(pool, self.lib.ConnectionPool)\n+\n+        pool = cache._cache._get_connection_pool(write=False)\n+        self.assertIsInstance(pool, self.lib.ConnectionPool)\n+\n+    def test_get_client(self):\n+        self.assertIsInstance(cache._cache.get_client(), self.lib.Redis)\n+\n+\n class FileBasedCachePathLibTests(FileBasedCacheTests):\n     def mkdtemp(self):\n         tmp_dir = super().mkdtemp()\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The failing test `test_createcachetable_observes_database_router` indicates a mismatch in expected vs. actual database queries during the cache table creation. This issue could stem from naming conflicts with `my_cache_table`, which aligns with existing test tables. Consider renaming to avoid conflicts, as you noted that changing the name resolved the issue. Regarding zero timeout handling for Redis, ensure proper checks in `get_backend_timeout`, as Redis-py doesn't support it. Refactoring the import of `redis` into the `__init__` method is also a solid approach to prevent import errors when the library isn't available."
      }
    ],
    "codediff_length": 13149
  },
  {
    "index": 11,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/db/models/expressions.py b/django/db/models/expressions.py\nindex 1937ca16c7ce..49ca80192438 100644\n--- a/django/db/models/expressions.py\n+++ b/django/db/models/expressions.py\n@@ -1118,3 +1151,168 @@ def asc(self):\n \n     def desc(self):\n         self.descending = True\n+\n+\n+class Window(Expression):\n+    template = '%(expression)s OVER (%(window)s)'\n+    # Although the main expression may either be an aggregate or an\n+    # expression with an aggregate function, the GROUP BY that will\n+    # be introduced in the query as a result is not desired.\n+    contains_aggregate = False\n+    contains_over_clause = True\n+    filterable = False\n+\n+    def __init__(self, expression, partition_by=None, order_by=None, frame=None, output_field=None):\n+        self.partition_by = partition_by\n+        self.order_by = order_by\n+        self.frame = frame\n+\n+        if not getattr(expression, 'window_compatible', False):\n+            raise ValueError(\n+                \"Expression '%s' isn't compatible with OVER clauses.\" %\n+                expression.__class__.__name__\n+            )\n+\n+        if self.partition_by is not None:\n+            if not isinstance(self.partition_by, (tuple, list)):\n+                self.partition_by = (self.partition_by,)\n+            self.partition_by = ExpressionList(*self.partition_by)\n+\n+        if self.order_by is not None:\n+            if isinstance(self.order_by, (list, tuple)):\n+                self.order_by = ExpressionList(*self.order_by)\n+            elif not isinstance(self.order_by, BaseExpression):\n+                raise ValueError(\n+                    'order_by must be either an Expression or a sequence of '\n+                    'expressions.'\n+                )\n+        super().__init__(output_field=output_field)\n+        self.source_expression = self._parse_expressions(expression)[0]\n+\n+    def _resolve_output_field(self):\n+        return self.source_expression.output_field\n+\n+    def get_source_expressions(self):\n+        return [self.source_expression, self.partition_by, self.order_by, self.frame]\n+\n+    def set_source_expressions(self, exprs):\n+        self.source_expression, self.partition_by, self.order_by, self.frame = exprs\n+\n+    def as_sql(self, compiler, connection, function=None, template=None):\n+        connection.ops.check_expression_support(self)\n+        expr_sql, params = compiler.compile(self.source_expression)\n+        window_sql, window_params = [], []\n+\n+        if self.partition_by is not None:\n+            sql_expr, sql_params = self.partition_by.as_sql(\n+                compiler=compiler, connection=connection,\n+                template='PARTITION BY %(expressions)s',\n+            )\n+            window_sql.extend(sql_expr)\n+            window_params.extend(sql_params)\n+\n+        if self.order_by is not None:\n+            window_sql.append(' ORDER BY ')\n+            order_sql, order_params = compiler.compile(self.order_by)\n+            window_sql.extend(''.join(order_sql))\n+            window_params.extend(order_params)\n+\n+        if self.frame:\n+            frame_sql, frame_params = compiler.compile(self.frame)\n+            window_sql.extend(' ' + frame_sql)\n+            window_params.extend(frame_params)\n+\n+        params.extend(window_params)\n+        template = template or self.template\n+\n+        return template % {\n+            'expression': expr_sql,\n+            'window': ''.join(window_sql).strip()\n+        }, params\n+\n+    def __str__(self):\n+        return '{} OVER ({}{}{})'.format(\n+            str(self.source_expression),\n+            'PARTITION BY ' + str(self.partition_by) if self.partition_by else '',\n+            'ORDER BY ' + str(self.order_by) if self.order_by else '',\n+            str(self.frame or ''),\n+        )\n+\n+    def __repr__(self):\n+        return '<%s: %s>' % (self.__class__.__name__, self)\n+\n+    def get_group_by_cols(self):\n+        return []\n+\n+\n+class WindowFrame(Expression):\n+    \"\"\"\n+    Model the frame clause in window expressions. There are two types of frame\n+    clauses which are subclasses, however, all processing and validation (by no\n+    means intended to be complete) is done here. Thus, providing an end for a\n+    frame is optional (the default is UNBOUNDED FOLLOWING, which is the last\n+    row in the frame).\n+    \"\"\"\n+    template = '%(frame_type)s BETWEEN %(start)s AND %(end)s'\n+\n+    def __init__(self, start=None, end=None):\n+        self.start = start\n+        self.end = end\n+\n+    def set_source_expressions(self, exprs):\n+        self.start, self.end = exprs\n+\n+    def get_source_expressions(self):\n+        return [Value(self.start), Value(self.end)]\n+\n+    def as_sql(self, compiler, connection):\n+        connection.ops.check_expression_support(self)\n+        start, end = self.window_frame_start_end(connection, self.start.value, self.end.value)\n+        return self.template % {\n+            'frame_type': self.frame_type,\n+            'start': start,\n+            'end': end,\n+        }, []\n+\n+    def __repr__(self):\n+        return '<%s: %s>' % (self.__class__.__name__, self)\n+\n+    def get_group_by_cols(self):\n+        return []\n+\n+    def __str__(self):\n+        if self.start is not None and self.start < 0:\n+            start = '%d %s' % (abs(self.start), connection.ops.PRECEDING)\n+        elif self.start is not None and self.start == 0:\n+            start = connection.ops.CURRENT_ROW\n+        else:\n+            start = connection.ops.UNBOUNDED_PRECEDING\n+\n+        if self.end is not None and self.end > 0:\n+            end = '%d %s' % (self.end, connection.ops.FOLLOWING)\n+        elif self.end is not None and self.end == 0:\n+            end = connection.ops.CURRENT_ROW\n+        else:\n+            end = connection.ops.UNBOUNDED_FOLLOWING\n+        return self.template % {\n+            'frame_type': self.frame_type,\n+            'start': start,\n+            'end': end,\n+        }\n+\n+    def window_frame_start_end(self, connection, start, end):\n+        raise NotImplementedError('Subclasses must implement window_frame_start_end().')\n+\n+\n+class RowRange(WindowFrame):\n+    frame_type = 'ROWS'\n+\n+    def window_frame_start_end(self, connection, start, end):\n+        return connection.ops.window_frame_rows_start_end(start, end)\n+\n+\n+class ValueRange(WindowFrame):\n+    frame_type = 'RANGE'\n+\n+    def window_frame_start_end(self, connection, start, end):\n+        return connection.ops.window_frame_range_start_end(start, end)\ndiff --git a/django/db/models/functions/window.py b/django/db/models/functions/window.py\nnew file mode 100644\nindex 000000000000..3719dfca8863\n--- /dev/null\n+++ b/django/db/models/functions/window.py\n@@ -0,0 +1,118 @@\n+from django.db.models import FloatField, Func, IntegerField\n+\n+__all__ = [\n+    'CumeDist', 'DenseRank', 'FirstValue', 'Lag', 'LastValue', 'Lead',\n+    'NthValue', 'Ntile', 'PercentRank', 'Rank', 'RowNumber',\n+]\n+\n+\n+class CumeDist(Func):\n+    function = 'CUME_DIST'\n+    name = 'CumeDist'\n+    output_field = FloatField()\n+    window_compatible = True\n+\n+\n+class DenseRank(Func):\n+    function = 'DENSE_RANK'\n+    name = 'DenseRank'\n+    output_field = IntegerField()\n+    window_compatible = True\n+\n+\n+class FirstValue(Func):\n+    arity = 1\n+    function = 'FIRST_VALUE'\n+    name = 'FirstValue'\n+    window_compatible = True\n+\n+\n+class LagLeadFunction(Func):\n+    window_compatible = True\n+\n+    def __init__(self, expression, offset=1, default=None, **extra):\n+        if expression is None:\n+            raise ValueError(\n+                '%s requires a non-null source expression.' %\n+                self.__class__.__name__\n+            )\n+        if offset is None or offset <= 0:\n+            raise ValueError(\n+                '%s requires a positive integer for the offset.' %\n+                self.__class__.__name__\n+            )\n+        args = (expression, offset)\n+        if default is not None:\n+            args += (default,)\n+        super().__init__(*args, **extra)\n+\n+    def _resolve_output_field(self):\n+        sources = self.get_source_expressions()\n+        return sources[0].output_field\n+\n+\n+class Lag(LagLeadFunction):\n+    function = 'LAG'\n+    name = 'Lag'\n+\n+\n+class LastValue(Func):\n+    arity = 1\n+    function = 'LAST_VALUE'\n+    name = 'LastValue'\n+    window_compatible = True\n+\n+\n+class Lead(LagLeadFunction):\n+    function = 'LEAD'\n+    name = 'Lead'\n+\n+\n+class NthValue(Func):\n+    function = 'NTH_VALUE'\n+    name = 'NthValue'\n+    window_compatible = True\n+\n+    def __init__(self, expression, nth=1, **extra):\n+        if expression is None:\n+            raise ValueError('%s requires a non-null source expression.' % self.__class__.__name__)\n+        if nth is None or nth <= 0:\n+            raise ValueError('%s requires a positive integer as for nth.' % self.__class__.__name__)\n+        super().__init__(expression, nth, **extra)\n+\n+    def _resolve_output_field(self):\n+        sources = self.get_source_expressions()\n+        return sources[0].output_field\n+\n+\n+class Ntile(Func):\n+    function = 'NTILE'\n+    name = 'Ntile'\n+    output_field = IntegerField()\n+    window_compatible = True\n+\n+    def __init__(self, num_buckets=1, **extra):\n+        if num_buckets <= 0:\n+            raise ValueError('num_buckets must be greater than 0.')\n+        super().__init__(num_buckets, **extra)\n+\n+\n+class PercentRank(Func):\n+    function = 'PERCENT_RANK'\n+    name = 'PercentRank'\n+    output_field = FloatField()\n+    window_compatible = True\n+\n+\n+class Rank(Func):\n+    function = 'RANK'\n+    name = 'Rank'\n+    output_field = IntegerField()\n+    window_compatible = True\n+\n+\n+class RowNumber(Func):\n+    function = 'ROW_NUMBER'\n+    name = 'RowNumber'\n+    output_field = IntegerField()\n+    window_compatible = True\ndiff --git a/docs/ref/models/database-functions.txt b/docs/ref/models/database-functions.txt\nindex 875786c6402a..f5a9af3a059d 100644\n--- a/docs/ref/models/database-functions.txt\n+++ b/docs/ref/models/database-functions.txt\n@@ -819,3 +819,132 @@ Usage example::\n      'minute': 'minute': datetime.datetime(2014, 6, 15, 14, 30, tzinfo=<UTC>),\n      'second': datetime.datetime(2014, 6, 15, 14, 30, 50, tzinfo=<UTC>)\n     }\n+\n+.. _window-functions:\n+\n+Window functions\n+================\n+\n+.. versionadded:: 2.0\n+\n+There are a number of functions to use in a\n+:class:`~django.db.models.expressions.Window` expression for computing the rank\n+of elements or the :class:`Ntile` of some rows.\n+\n+``CumeDist``\n+------------\n+\n+.. class:: CumeDist(*expressions, **extra)\n+\n+Calculates the cumulative distribution of a value within a window or partition.\n+The cumulative distribution is defined as the number of rows preceding or\n+peered with the current row divided by the total number of rows in the frame.\n+\n+``DenseRank``\n+-------------\n+\n+.. class:: DenseRank(*expressions, **extra)\n+\n+Equivalent to :class:`Rank` but does not have gaps.\n+\n+``FirstValue``\n+--------------\n+\n+.. class:: FirstValue(expression, **extra)\n+\n+Returns the value evaluated at the row that's the first row of the window\n+frame, or ``None`` if no such value exists.\n+\n+``Lag``\n+-------\n+\n+.. class:: Lag(expression, offset=1, default=None, **extra)\n+\n+Calculates the value offset by ``offset``, and if no row exists there, returns\n+``default``.\n+\n+``default`` must have the same type as the ``expression``, however, this is\n+only validated by the database and not in Python.\n+\n+``LastValue``\n+-------------\n+\n+.. class:: LastValue(expression, **extra)\n+\n+Comparable to :class:`FirstValue`, it calculates the last value in a given\n+frame clause.\n+\n+``Lead``\n+--------\n+\n+.. class:: Lead(expression, offset=1, default=None, **extra)\n+\n+Calculates the leading value in a given :ref:`frame <window-frames>`. Both\n+``offset`` and ``default`` are evaluated with respect to the current row.\n+\n+``default`` must have the same type as the ``expression``, however, this is\n+only validated by the database and not in Python.\n+\n+``NthValue``\n+------------\n+\n+.. class:: NthValue(expression, nth=1, **extra)\n+\n+Computes the row relative to the offset ``nth`` (must be a positive value)\n+within the window. Returns ``None`` if no row exists.\n+\n+Some databases may handle a nonexistent nth-value differently. For example,\n+Oracle returns an empty string rather than ``None`` for character-based\n+expressions. Django doesn't do any conversions in these cases.\n+\n+``Ntile``\n+---------\n+\n+.. class:: Ntile(num_buckets=1, **extra)\n+\n+Calculates a partition for each of the rows in the frame clause, distributing\n+numbers as evenly as possible between 1 and ``num_buckets``. If the rows don't\n+divide evenly into a number of buckets, one or more buckets will be represented\n+more frequently.\n+\n+``PercentRank``\n+---------------\n+\n+.. class:: PercentRank(*expressions, **extra)\n+\n+Computes the percentile rank of the rows in the frame clause. This\n+computation is equivalent to evaluating::\n+\n+    (rank - 1) / (total rows - 1)\n+\n+The following table explains the calculation for the percentile rank of a row:\n+\n+===== ===== ==== ============ ============\n+Row # Value Rank Calculation  Percent Rank\n+===== ===== ==== ============ ============\n+1     15    1    (1-1)/(7-1)  0.0000\n+2     20    2    (2-1)/(7-1)  0.1666\n+3     20    2    (2-1)/(7-1)  0.1666\n+4     20    2    (2-1)/(7-1)  0.1666\n+5     30    5    (5-1)/(7-1)  0.6666\n+6     30    5    (5-1)/(7-1)  0.6666\n+7     40    7    (7-1)/(7-1)  1.0000\n+===== ===== ==== ============ ============\n+\n+``Rank``\n+--------\n+\n+.. class:: Rank(*expressions, **extra)\n+\n+Comparable to ``RowNumber``, this function ranks rows in the window. The\n+computed rank contains gaps. Use :class:`DenseRank` to compute rank without\n+gaps.\n+\n+``RowNumber``\n+-------------\n+\n+.. class:: RowNumber(*expressions, **extra)\n+\n+Computes the row number according to the ordering of either the frame clause\n+or the ordering of the whole query if there is no partitioning of the\n+:ref:`window frame <window-frames>`.\ndiff --git a/docs/ref/models/expressions.txt b/docs/ref/models/expressions.txt\nindex 0974a9dd5101..6ef1001a9ffb 100644\n--- a/docs/ref/models/expressions.txt\n+++ b/docs/ref/models/expressions.txt\n@@ -649,6 +656,184 @@ should avoid them if possible.\n     force you to acknowledge that you're not interpolating your SQL with user\n     provided data.\n \n+Window functions\n+----------------\n+\n+.. versionadded:: 2.0\n+\n+Window functions provide a way to apply functions on partitions. Unlike a\n+normal aggregation function which computes a final result for each set defined\n+by the group by, window functions operate on :ref:`frames <window-frames>` and\n+partitions, and compute the result for each row.\n+\n+You can specify multiple windows in the same query which in Django ORM would be\n+equivalent to including multiple expressions in a :doc:`QuerySet.annotate()\n+</topics/db/aggregation>` call. The ORM doesn't make use of named windows,\n+instead they are part of the selected columns.\n+\n+.. class:: Window(expression, partition_by=None, order_by=None, frame=None, output_field=None)\n+\n+    .. attribute:: filterable\n+\n+        Defaults to ``False``. The SQL standard disallows referencing window\n+        functions in the ``WHERE`` clause and Django raises an exception when\n+        constructing a ``QuerySet`` that would do that.\n+\n+    .. attribute:: template\n+\n+        Defaults to ``%(expression)s OVER (%(window)s)'``. If only the\n+        ``expression`` argument is provided, the window clause will be blank.\n+\n+The ``Window`` class is the main expression for an ``OVER`` clause.\n+\n+The ``expression`` argument is either a :ref:`window function\n+<window-functions>`, an :ref:`aggregate function <aggregation-functions>`, or\n+an expression that's compatible in a window clause.\n+\n+The ``partition_by`` argument is a list of expressions (column names should be\n+wrapped in an ``F``-object) that control the partitioning of the rows.\n+Partitioning narrows which rows are used to compute the result set.\n+\n+The ``output_field`` is specified either as an argument or by the expression.\n+\n+The ``order_by`` argument accepts a sequence of expressions on which you can\n+call :meth:`~django.db.models.Expression.asc` and\n+:meth:`~django.db.models.Expression.desc`. The ordering controls the order in\n+which the expression is applied. For example, if you sum over the rows in a\n+partition, the first result is just the value of the first row, the second is\n+the sum of first and second row.\n+\n+The ``frame`` parameter specifies which other rows that should be used in the\n+computation. See :ref:`window-frames` for details.\n+\n+For example, to annotate each movie with the average rating for the movies by\n+the same studio in the same genre and release year::\n+\n+    >>> from django.db.models import Avg, ExtractYear, F, Window\n+    >>> Movie.objects.annotate(\n+    >>>     avg_rating=Window(\n+    >>>         expression=Avg('rating'),\n+    >>>         partition_by=[F('studio'), F('genre')],\n+    >>>         order_by=ExtractYear('released').asc(),\n+    >>>     ),\n+    >>> )\n+\n+This makes it easy to check if a movie is rated better or worse than its peers.\n+\n+You may want to apply multiple expressions over the same window, i.e., the\n+same partition and frame. For example, you could modify the previous example\n+to also include the best and worst rating in each movie's group (same studio,\n+genre, and release year) by using three window functions in the same query. The\n+partition and ordering from the previous example is extracted into a dictionary\n+to reduce repetition::\n+\n+    >>> from django.db.models import Avg, ExtractYear, F, Max, Min, Window\n+    >>> window = {\n+    >>>    'partition': [F('studio'), F('genre')],\n+    >>>    'order_by': ExtractYear('released').asc(),\n+    >>> }\n+    >>> Movie.objects.annotate(\n+    >>>     avg_rating=Window(\n+    >>>         expression=Avg('rating'), **window,\n+    >>>     ),\n+    >>>     best=Window(\n+    >>>         expression=Max('rating'), **window,\n+    >>>     ),\n+    >>>     worst=Window(\n+    >>>         expression=Min('rating'), **window,\n+    >>>     ),\n+    >>> )\n+\n+Among Django's built-in database backends, MySQL 8.0.2+, PostgreSQL, and Oracle\n+support window expressions. Support for different window expression features\n+varies among the different databases. For example, the options in\n+:meth:`~django.db.models.Expression.asc` and\n+:meth:`~django.db.models.Expression.desc` may not be supported. Consult the\n+documentation for your database as needed.\n+\n+.. _window-frames:\n+\n+Frames\n+~~~~~~\n+\n+For a window frame, you can choose either a range-based sequence of rows or an\n+ordinary sequence of rows.\n+\n+.. class:: ValueRange(start=None, end=None)\n+\n+    .. attribute:: frame_type\n+\n+        This attribute is set to ``'RANGE'``.\n+\n+    PostgreSQL has limited support for ``ValueRange`` and only supports use of\n+    the standard start and end points, such as ``CURRENT ROW`` and ``UNBOUNDED\n+    FOLLOWING``.\n+\n+.. class:: RowRange(start=None, end=None)\n+\n+    .. attribute:: frame_type\n+\n+        This attribute is set to ``'ROWS'``.\n+\n+Both classes return SQL with the template::\n+\n+    %(frame_type)s BETWEEN %(start)s AND %(end)s\n+\n+Frames narrow the rows that are used for computing the result. They shift from\n+some start point to some specified end point. Frames can be used with and\n+without partitions, but it's often a good idea to specify an ordering of the\n+window to ensure a deterministic result. In a frame, a peer in a frame is a row\n+with an equivalent value, or all rows if an ordering clause isn't present.\n+\n+The default starting point for a frame is ``UNBOUNDED PRECEDING`` which is the\n+first row of the partition. The end point is always explicitly included in the\n+SQL generated by the ORM and is by default ``UNBOUNDED FOLLOWING``. The default\n+frame includes all rows from the partition to the last row in the set.\n+\n+The accepted values for the ``start`` and ``end`` arguments are ``None``, an\n+integer, or zero. A negative integer for ``start`` results in ``N preceding``,\n+while ``None`` yields ``UNBOUNDED PRECEDING``. For both ``start`` and ``end``,\n+zero will return ``CURRENT ROW``. Positive integers are accepted for ``end``.\n+\n+There's a difference in what ``CURRENT ROW`` includes. When specified in\n+``ROWS`` mode, the frame starts or ends with the current row. When specified in\n+``RANGE`` mode, the frame starts or ends at the first or last peer according to\n+the ordering clause. Thus, ``RANGE CURRENT ROW`` evaluates the expression for\n+rows which have the same value specified by the ordering. Because the template\n+includes both the ``start`` and ``end`` points, this may be expressed with::\n+\n+    ValueRange(start=0, end=0)\n+\n+If a movie's \"peers\" are described as movies released by the same studio in the\n+same genre in the same year, this ``RowRange`` example annotates each movie\n+with the average rating of a movie's two prior and two following peers::\n+\n+    >>> from django.db.models import Avg, ExtractYear, F, RowRange, Window\n+    >>> Movie.objects.annotate(\n+    >>>     avg_rating=Window(\n+    >>>         expression=Avg('rating'),\n+    >>>         partition_by=[F('studio'), F('genre')],\n+    >>>         order_by=ExtractYear('released').asc(),\n+    >>>         frame=RowRange(start=-2, end=2),\n+    >>>     ),\n+    >>> )\n+\n+If the database supports it, you can specify the start and end points based on\n+values of an expression in the partition. If the ``released`` field of the\n+``Movie`` model stores the release month of each movies, this ``ValueRange``\n+example annotates each movie with the average rating of a movie's peers\n+released between twelve months before and twelve months after the each movie.\n+\n+    >>> from django.db.models import Avg, ExpressionList, F, ValueRange, Window\n+    >>> Movie.objects.annotate(\n+    >>>     avg_rating=Window(\n+    >>>         expression=Avg('rating'),\n+    >>>         partition_by=[F('studio'), F('genre')],\n+    >>>         order_by=F('released').asc(),\n+    >>>         frame=ValueRange(start=-12, end=12),\n+    >>>     ),\n+    >>> )\n+\n .. currentmodule:: django.db.models\n \n Technical Information\n\ndiff --git a/tests/expressions_window/tests.py b/tests/expressions_window/tests.py\nnew file mode 100644\nindex 000000000000..e61516b4b647\n--- /dev/null\n+++ b/tests/expressions_window/tests.py\n@@ -0,0 +1,783 @@\n+import datetime\n+from unittest import skipIf, skipUnless\n+\n+from django.core.exceptions import FieldError\n+from django.db import NotSupportedError, connection\n+from django.db.models import (\n+    F, RowRange, Value, ValueRange, Window, WindowFrame,\n+)\n+from django.db.models.aggregates import Avg, Max, Min, Sum\n+from django.db.models.functions import (\n+    CumeDist, DenseRank, ExtractYear, FirstValue, Lag, LastValue, Lead,\n+    NthValue, Ntile, PercentRank, Rank, RowNumber, Upper,\n+)\n+from django.test import SimpleTestCase, TestCase, skipUnlessDBFeature\n+\n+from .models import Employee\n+\n+\n+@skipUnlessDBFeature('supports_over_clause')\n+class WindowFunctionTests(TestCase):\n+    @classmethod\n+    def setUpTestData(cls):\n+        Employee.objects.bulk_create([\n+            Employee(name=e[0], salary=e[1], department=e[2], hire_date=e[3])\n+            for e in [\n+                ('Jones', 45000, 'Accounting', datetime.datetime(2005, 11, 1)),\n+                ('Williams', 37000, 'Accounting', datetime.datetime(2009, 6, 1)),\n+                ('Jenson', 45000, 'Accounting', datetime.datetime(2008, 4, 1)),\n+                ('Adams', 50000, 'Accounting', datetime.datetime(2013, 7, 1)),\n+                ('Smith', 55000, 'Sales', datetime.datetime(2007, 6, 1)),\n+                ('Brown', 53000, 'Sales', datetime.datetime(2009, 9, 1)),\n+                ('Johnson', 40000, 'Marketing', datetime.datetime(2012, 3, 1)),\n+                ('Smith', 38000, 'Marketing', datetime.datetime(2009, 10, 1)),\n+                ('Wilkinson', 60000, 'IT', datetime.datetime(2011, 3, 1)),\n+                ('Moore', 34000, 'IT', datetime.datetime(2013, 8, 1)),\n+                ('Miller', 100000, 'Management', datetime.datetime(2005, 6, 1)),\n+                ('Johnson', 80000, 'Management', datetime.datetime(2005, 7, 1)),\n+            ]\n+        ])\n+\n+    def test_dense_rank(self):\n+        qs = Employee.objects.annotate(rank=Window(\n+            expression=DenseRank(),\n+            order_by=ExtractYear(F('hire_date')).asc(),\n+        ))\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 1),\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), 1),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), 1),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), 2),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 3),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), 4),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), 4),\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), 4),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), 5),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), 6),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), 7),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), 7),\n+        ], lambda entry: (entry.name, entry.salary, entry.department, entry.hire_date, entry.rank), ordered=False)\n+\n+    def test_department_salary(self):\n+        qs = Employee.objects.annotate(department_sum=Window(\n+            expression=Sum('salary'),\n+            partition_by=F('department'),\n+            order_by=[F('hire_date').asc()],\n+        )).order_by('department', 'department_sum')\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 'Accounting', 45000, 45000),\n+            ('Jenson', 'Accounting', 45000, 90000),\n+            ('Williams', 'Accounting', 37000, 127000),\n+            ('Adams', 'Accounting', 50000, 177000),\n+            ('Wilkinson', 'IT', 60000, 60000),\n+            ('Moore', 'IT', 34000, 94000),\n+            ('Miller', 'Management', 100000, 100000),\n+            ('Johnson', 'Management', 80000, 180000),\n+            ('Smith', 'Marketing', 38000, 38000),\n+            ('Johnson', 'Marketing', 40000, 78000),\n+            ('Smith', 'Sales', 55000, 55000),\n+            ('Brown', 'Sales', 53000, 108000),\n+        ], lambda entry: (entry.name, entry.department, entry.salary, entry.department_sum))\n+\n+    def test_rank(self):\n+        \"\"\"\n+        Rank the employees based on the year they're were hired. Since there\n+        are multiple employees hired in different years, this will contain\n+        gaps.\n+        \"\"\"\n+        qs = Employee.objects.annotate(rank=Window(\n+            expression=Rank(),\n+            order_by=ExtractYear(F('hire_date')).asc(),\n+        ))\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 1),\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), 1),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), 1),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), 4),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 5),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), 6),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), 6),\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), 6),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), 9),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), 10),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), 11),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), 11),\n+        ], lambda entry: (entry.name, entry.salary, entry.department, entry.hire_date, entry.rank), ordered=False)\n+\n+    def test_row_number(self):\n+        \"\"\"\n+        The row number window function computes the number based on the order\n+        in which the tuples were inserted. Depending on the backend,\n+\n+        Oracle requires an ordering-clause in the Window expression.\n+        \"\"\"\n+        qs = Employee.objects.annotate(row_number=Window(\n+            expression=RowNumber(),\n+            order_by=F('pk').asc(),\n+        )).order_by('pk')\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 'Accounting', 1),\n+            ('Williams', 'Accounting', 2),\n+            ('Jenson', 'Accounting', 3),\n+            ('Adams', 'Accounting', 4),\n+            ('Smith', 'Sales', 5),\n+            ('Brown', 'Sales', 6),\n+            ('Johnson', 'Marketing', 7),\n+            ('Smith', 'Marketing', 8),\n+            ('Wilkinson', 'IT', 9),\n+            ('Moore', 'IT', 10),\n+            ('Miller', 'Management', 11),\n+            ('Johnson', 'Management', 12),\n+        ], lambda entry: (entry.name, entry.department, entry.row_number))\n+\n+    @skipIf(connection.vendor == 'oracle', \"Oracle requires ORDER BY in row_number, ANSI:SQL doesn't\")\n+    def test_row_number_no_ordering(self):\n+        \"\"\"\n+        The row number window function computes the number based on the order\n+        in which the tuples were inserted.\n+        \"\"\"\n+        # Add a default ordering for consistent results across databases.\n+        qs = Employee.objects.annotate(row_number=Window(\n+            expression=RowNumber(),\n+        )).order_by('pk')\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 'Accounting', 1),\n+            ('Williams', 'Accounting', 2),\n+            ('Jenson', 'Accounting', 3),\n+            ('Adams', 'Accounting', 4),\n+            ('Smith', 'Sales', 5),\n+            ('Brown', 'Sales', 6),\n+            ('Johnson', 'Marketing', 7),\n+            ('Smith', 'Marketing', 8),\n+            ('Wilkinson', 'IT', 9),\n+            ('Moore', 'IT', 10),\n+            ('Miller', 'Management', 11),\n+            ('Johnson', 'Management', 12),\n+        ], lambda entry: (entry.name, entry.department, entry.row_number))\n+\n+    def test_avg_salary_department(self):\n+        qs = Employee.objects.annotate(avg_salary=Window(\n+            expression=Avg('salary'),\n+            order_by=F('department').asc(),\n+            partition_by='department',\n+        )).order_by('department', '-salary', 'name')\n+        self.assertQuerysetEqual(qs, [\n+            ('Adams', 50000, 'Accounting', 44250.00),\n+            ('Jenson', 45000, 'Accounting', 44250.00),\n+            ('Jones', 45000, 'Accounting', 44250.00),\n+            ('Williams', 37000, 'Accounting', 44250.00),\n+            ('Wilkinson', 60000, 'IT', 47000.00),\n+            ('Moore', 34000, 'IT', 47000.00),\n+            ('Miller', 100000, 'Management', 90000.00),\n+            ('Johnson', 80000, 'Management', 90000.00),\n+            ('Johnson', 40000, 'Marketing', 39000.00),\n+            ('Smith', 38000, 'Marketing', 39000.00),\n+            ('Smith', 55000, 'Sales', 54000.00),\n+            ('Brown', 53000, 'Sales', 54000.00),\n+        ], transform=lambda row: (row.name, row.salary, row.department, row.avg_salary))\n+\n+    def test_lag(self):\n+        \"\"\"\n+        Compute the difference between an employee's salary and the next\n+        highest salary in the employee's department. Return None if the\n+        employee has the lowest salary.\n+        \"\"\"\n+        qs = Employee.objects.annotate(lag=Window(\n+            expression=Lag(expression='salary', offset=1),\n+            partition_by=F('department'),\n+            order_by=[F('salary').asc(), F('name').asc()],\n+        )).order_by('department')\n+        self.assertQuerysetEqual(qs, [\n+            ('Williams', 37000, 'Accounting', None),\n+            ('Jenson', 45000, 'Accounting', 37000),\n+            ('Jones', 45000, 'Accounting', 45000),\n+            ('Adams', 50000, 'Accounting', 45000),\n+            ('Moore', 34000, 'IT', None),\n+            ('Wilkinson', 60000, 'IT', 34000),\n+            ('Johnson', 80000, 'Management', None),\n+            ('Miller', 100000, 'Management', 80000),\n+            ('Smith', 38000, 'Marketing', None),\n+            ('Johnson', 40000, 'Marketing', 38000),\n+            ('Brown', 53000, 'Sales', None),\n+            ('Smith', 55000, 'Sales', 53000),\n+        ], transform=lambda row: (row.name, row.salary, row.department, row.lag))\n+\n+    def test_first_value(self):\n+        qs = Employee.objects.annotate(first_value=Window(\n+            expression=FirstValue('salary'),\n+            partition_by=F('department'),\n+            order_by=F('hire_date').asc(),\n+        )).order_by('department', 'hire_date')\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 45000),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 45000),\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), 45000),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), 45000),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), 60000),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), 60000),\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), 100000),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), 100000),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), 38000),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), 38000),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), 55000),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), 55000),\n+        ], lambda row: (row.name, row.salary, row.department, row.hire_date, row.first_value))\n+\n+    def test_last_value(self):\n+        qs = Employee.objects.annotate(last_value=Window(\n+            expression=LastValue('hire_date'),\n+            partition_by=F('department'),\n+            order_by=F('hire_date').asc(),\n+        ))\n+        self.assertQuerysetEqual(qs, [\n+            ('Adams', 'Accounting', datetime.date(2013, 7, 1), 50000, datetime.date(2013, 7, 1)),\n+            ('Jenson', 'Accounting', datetime.date(2008, 4, 1), 45000, datetime.date(2008, 4, 1)),\n+            ('Jones', 'Accounting', datetime.date(2005, 11, 1), 45000, datetime.date(2005, 11, 1)),\n+            ('Williams', 'Accounting', datetime.date(2009, 6, 1), 37000, datetime.date(2009, 6, 1)),\n+            ('Moore', 'IT', datetime.date(2013, 8, 1), 34000, datetime.date(2013, 8, 1)),\n+            ('Wilkinson', 'IT', datetime.date(2011, 3, 1), 60000, datetime.date(2011, 3, 1)),\n+            ('Miller', 'Management', datetime.date(2005, 6, 1), 100000, datetime.date(2005, 6, 1)),\n+            ('Johnson', 'Management', datetime.date(2005, 7, 1), 80000, datetime.date(2005, 7, 1)),\n+            ('Johnson', 'Marketing', datetime.date(2012, 3, 1), 40000, datetime.date(2012, 3, 1)),\n+            ('Smith', 'Marketing', datetime.date(2009, 10, 1), 38000, datetime.date(2009, 10, 1)),\n+            ('Brown', 'Sales', datetime.date(2009, 9, 1), 53000, datetime.date(2009, 9, 1)),\n+            ('Smith', 'Sales', datetime.date(2007, 6, 1), 55000, datetime.date(2007, 6, 1)),\n+        ], transform=lambda row: (row.name, row.department, row.hire_date, row.salary, row.last_value), ordered=False)\n+\n+    def test_function_list_of_values(self):\n+        qs = Employee.objects.annotate(lead=Window(\n+            expression=Lead(expression='salary'),\n+            order_by=[F('hire_date').asc(), F('name').desc()],\n+            partition_by='department',\n+        )).values_list('name', 'salary', 'department', 'hire_date', 'lead')\n+        self.assertNotIn('GROUP BY', str(qs.query))\n+        self.assertSequenceEqual(qs, [\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 45000),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 37000),\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), 50000),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), None),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), 34000),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), None),\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), 80000),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), None),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), 40000),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), None),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), 53000),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), None),\n+        ])\n+\n+    def test_min_department(self):\n+        \"\"\"An alternative way to specify a query for FirstValue.\"\"\"\n+        qs = Employee.objects.annotate(min_salary=Window(\n+            expression=Min('salary'),\n+            partition_by=F('department'),\n+            order_by=[F('salary').asc(), F('name').asc()]\n+        )).order_by('department', 'salary', 'name')\n+        self.assertQuerysetEqual(qs, [\n+            ('Williams', 'Accounting', 37000, 37000),\n+            ('Jenson', 'Accounting', 45000, 37000),\n+            ('Jones', 'Accounting', 45000, 37000),\n+            ('Adams', 'Accounting', 50000, 37000),\n+            ('Moore', 'IT', 34000, 34000),\n+            ('Wilkinson', 'IT', 60000, 34000),\n+            ('Johnson', 'Management', 80000, 80000),\n+            ('Miller', 'Management', 100000, 80000),\n+            ('Smith', 'Marketing', 38000, 38000),\n+            ('Johnson', 'Marketing', 40000, 38000),\n+            ('Brown', 'Sales', 53000, 53000),\n+            ('Smith', 'Sales', 55000, 53000),\n+        ], lambda row: (row.name, row.department, row.salary, row.min_salary))\n+\n+    def test_max_per_year(self):\n+        \"\"\"\n+        Find the maximum salary awarded in the same year as the\n+        employee was hired, regardless of the department.\n+        \"\"\"\n+        qs = Employee.objects.annotate(max_salary_year=Window(\n+            expression=Max('salary'),\n+            order_by=ExtractYear('hire_date').asc(),\n+            partition_by=ExtractYear('hire_date')\n+        )).order_by(ExtractYear('hire_date'), 'salary')\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 'Accounting', 45000, 2005, 100000),\n+            ('Johnson', 'Management', 80000, 2005, 100000),\n+            ('Miller', 'Management', 100000, 2005, 100000),\n+            ('Smith', 'Sales', 55000, 2007, 55000),\n+            ('Jenson', 'Accounting', 45000, 2008, 45000),\n+            ('Williams', 'Accounting', 37000, 2009, 53000),\n+            ('Smith', 'Marketing', 38000, 2009, 53000),\n+            ('Brown', 'Sales', 53000, 2009, 53000),\n+            ('Wilkinson', 'IT', 60000, 2011, 60000),\n+            ('Johnson', 'Marketing', 40000, 2012, 40000),\n+            ('Moore', 'IT', 34000, 2013, 50000),\n+            ('Adams', 'Accounting', 50000, 2013, 50000),\n+        ], lambda row: (row.name, row.department, row.salary, row.hire_date.year, row.max_salary_year))\n+\n+    def test_cume_dist(self):\n+        \"\"\"\n+        Compute the cumulative distribution for the employees based on the\n+        salary in increasing order. Equal to rank/total number of rows (12).\n+        \"\"\"\n+        qs = Employee.objects.annotate(cume_dist=Window(\n+            expression=CumeDist(),\n+            order_by=F('salary').asc(),\n+        )).order_by('salary', 'name')\n+        # Round result of cume_dist because Oracle uses greater precision.\n+        self.assertQuerysetEqual(qs, [\n+            ('Moore', 'IT', 34000, 0.0833333333),\n+            ('Williams', 'Accounting', 37000, 0.1666666667),\n+            ('Smith', 'Marketing', 38000, 0.25),\n+            ('Johnson', 'Marketing', 40000, 0.3333333333),\n+            ('Jenson', 'Accounting', 45000, 0.5),\n+            ('Jones', 'Accounting', 45000, 0.5),\n+            ('Adams', 'Accounting', 50000, 0.5833333333),\n+            ('Brown', 'Sales', 53000, 0.6666666667),\n+            ('Smith', 'Sales', 55000, 0.75),\n+            ('Wilkinson', 'IT', 60000, 0.8333333333),\n+            ('Johnson', 'Management', 80000, 0.9166666667),\n+            ('Miller', 'Management', 100000, 1),\n+        ], lambda row: (row.name, row.department, row.salary, round(row.cume_dist, 10)))\n+\n+    def test_nthvalue(self):\n+        qs = Employee.objects.annotate(\n+            nth_value=Window(expression=NthValue(\n+                expression='salary', nth=2),\n+                order_by=[F('hire_date').asc(), F('name').desc()],\n+                partition_by=F('department'),\n+            )\n+        ).order_by('department', 'hire_date', 'name')\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 'Accounting', datetime.date(2005, 11, 1), 45000, None),\n+            ('Jenson', 'Accounting', datetime.date(2008, 4, 1), 45000, 45000),\n+            ('Williams', 'Accounting', datetime.date(2009, 6, 1), 37000, 45000),\n+            ('Adams', 'Accounting', datetime.date(2013, 7, 1), 50000, 45000),\n+            ('Wilkinson', 'IT', datetime.date(2011, 3, 1), 60000, None),\n+            ('Moore', 'IT', datetime.date(2013, 8, 1), 34000, 34000),\n+            ('Miller', 'Management', datetime.date(2005, 6, 1), 100000, None),\n+            ('Johnson', 'Management', datetime.date(2005, 7, 1), 80000, 80000),\n+            ('Smith', 'Marketing', datetime.date(2009, 10, 1), 38000, None),\n+            ('Johnson', 'Marketing', datetime.date(2012, 3, 1), 40000, 40000),\n+            ('Smith', 'Sales', datetime.date(2007, 6, 1), 55000, None),\n+            ('Brown', 'Sales', datetime.date(2009, 9, 1), 53000, 53000),\n+        ], lambda row: (row.name, row.department, row.hire_date, row.salary, row.nth_value))\n+\n+    def test_lead(self):\n+        \"\"\"\n+        Determine what the next person hired in the same department makes.\n+        Because the dataset is ambiguous, the name is also part of the\n+        ordering clause. No default is provided, so None/NULL should be\n+        returned.\n+        \"\"\"\n+        qs = Employee.objects.annotate(lead=Window(\n+            expression=Lead(expression='salary'),\n+            order_by=[F('hire_date').asc(), F('name').desc()],\n+            partition_by='department',\n+        )).order_by('department')\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 45000),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 37000),\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), 50000),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), None),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), 34000),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), None),\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), 80000),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), None),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), 40000),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), None),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), 53000),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), None),\n+        ], transform=lambda row: (row.name, row.salary, row.department, row.hire_date, row.lead))\n+\n+    def test_lead_offset(self):\n+        \"\"\"\n+        Determine what the person hired after someone makes. Due to\n+        ambiguity, the name is also included in the ordering.\n+        \"\"\"\n+        qs = Employee.objects.annotate(lead=Window(\n+            expression=Lead('salary', offset=2),\n+            partition_by='department',\n+            order_by=F('hire_date').asc(),\n+        ))\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 37000),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 50000),\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), None),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), None),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), None),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), None),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), None),\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), None),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), None),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), None),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), None),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), None),\n+        ], transform=lambda row: (row.name, row.salary, row.department, row.hire_date, row.lead),\n+            ordered=False\n+        )\n+\n+    def test_lead_default(self):\n+        qs = Employee.objects.annotate(lead_default=Window(\n+            expression=Lead(expression='salary', offset=5, default=60000),\n+            partition_by=F('department'),\n+            order_by=F('department').asc(),\n+        ))\n+        self.assertEqual(list(qs.values_list('lead_default', flat=True).distinct()), [60000])\n+\n+    def test_ntile(self):\n+        \"\"\"\n+        Compute the group for each of the employees across the entire company,\n+        based on how high the salary is for them. There are twelve employees\n+        so it divides evenly into four groups.\n+        \"\"\"\n+        qs = Employee.objects.annotate(ntile=Window(\n+            expression=Ntile(num_buckets=4),\n+            order_by=F('salary').desc(),\n+        )).order_by('ntile', '-salary', 'name')\n+        self.assertQuerysetEqual(qs, [\n+            ('Miller', 'Management', 100000, 1),\n+            ('Johnson', 'Management', 80000, 1),\n+            ('Wilkinson', 'IT', 60000, 1),\n+            ('Smith', 'Sales', 55000, 2),\n+            ('Brown', 'Sales', 53000, 2),\n+            ('Adams', 'Accounting', 50000, 2),\n+            ('Jenson', 'Accounting', 45000, 3),\n+            ('Jones', 'Accounting', 45000, 3),\n+            ('Johnson', 'Marketing', 40000, 3),\n+            ('Smith', 'Marketing', 38000, 4),\n+            ('Williams', 'Accounting', 37000, 4),\n+            ('Moore', 'IT', 34000, 4),\n+        ], lambda x: (x.name, x.department, x.salary, x.ntile))\n+\n+    def test_percent_rank(self):\n+        \"\"\"\n+        Calculate the percentage rank of the employees across the entire\n+        company based on salary and name (in case of ambiguity).\n+        \"\"\"\n+        qs = Employee.objects.annotate(percent_rank=Window(\n+            expression=PercentRank(),\n+            order_by=[F('salary').asc(), F('name').asc()],\n+        )).order_by('percent_rank')\n+        # Round to account for precision differences among databases.\n+        self.assertQuerysetEqual(qs, [\n+            ('Moore', 'IT', 34000, 0.0),\n+            ('Williams', 'Accounting', 37000, 0.0909090909),\n+            ('Smith', 'Marketing', 38000, 0.1818181818),\n+            ('Johnson', 'Marketing', 40000, 0.2727272727),\n+            ('Jenson', 'Accounting', 45000, 0.3636363636),\n+            ('Jones', 'Accounting', 45000, 0.4545454545),\n+            ('Adams', 'Accounting', 50000, 0.5454545455),\n+            ('Brown', 'Sales', 53000, 0.6363636364),\n+            ('Smith', 'Sales', 55000, 0.7272727273),\n+            ('Wilkinson', 'IT', 60000, 0.8181818182),\n+            ('Johnson', 'Management', 80000, 0.9090909091),\n+            ('Miller', 'Management', 100000, 1.0),\n+        ], transform=lambda row: (row.name, row.department, row.salary, round(row.percent_rank, 10)))\n+\n+    def test_nth_returns_null(self):\n+        \"\"\"\n+        Find the nth row of the data set. None is returned since there are\n+        fewer than 20 rows in the test data.\n+        \"\"\"\n+        qs = Employee.objects.annotate(nth_value=Window(\n+            expression=NthValue('salary', nth=20),\n+            order_by=F('salary').asc()\n+        ))\n+        self.assertEqual(list(qs.values_list('nth_value', flat=True).distinct()), [None])\n+\n+    def test_multiple_partitioning(self):\n+        \"\"\"\n+        Find the maximum salary for each department for people hired in the\n+        same year.\n+        \"\"\"\n+        qs = Employee.objects.annotate(max=Window(\n+            expression=Max('salary'),\n+            partition_by=[F('department'), ExtractYear(F('hire_date'))],\n+        )).order_by('department', 'hire_date', 'name')\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 45000),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 45000),\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), 37000),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), 50000),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), 60000),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), 34000),\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), 100000),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), 100000),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), 38000),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), 40000),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), 55000),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), 53000),\n+        ], transform=lambda row: (row.name, row.salary, row.department, row.hire_date, row.max))\n+\n+    def test_multiple_ordering(self):\n+        \"\"\"\n+        Accumulate the salaries over the departments based on hire_date.\n+        If two people were hired on the same date in the same department, the\n+        ordering clause will render a different result for those people.\n+        \"\"\"\n+        qs = Employee.objects.annotate(sum=Window(\n+            expression=Sum('salary'),\n+            partition_by='department',\n+            order_by=[F('hire_date').asc(), F('name').asc()],\n+        )).order_by('department', 'sum')\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 45000),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 90000),\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), 127000),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), 177000),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), 60000),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), 94000),\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), 100000),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), 180000),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), 38000),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), 78000),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), 55000),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), 108000),\n+        ], transform=lambda row: (row.name, row.salary, row.department, row.hire_date, row.sum))\n+\n+    @skipIf(connection.vendor == 'postgresql', 'n following/preceding not supported by PostgreSQL')\n+    def test_range_n_preceding_and_following(self):\n+        qs = Employee.objects.annotate(sum=Window(\n+            expression=Sum('salary'),\n+            order_by=F('salary').asc(),\n+            partition_by='department',\n+            frame=ValueRange(start=-2, end=2),\n+        ))\n+        self.assertIn('RANGE BETWEEN 2 PRECEDING AND 2 FOLLOWING', str(qs.query))\n+        self.assertQuerysetEqual(qs, [\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), 37000),\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 90000),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 90000),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), 50000),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), 53000),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), 55000),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), 40000),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), 38000),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), 60000),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), 34000),\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), 100000),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), 80000),\n+        ], transform=lambda row: (row.name, row.salary, row.department, row.hire_date, row.sum), ordered=False)\n+\n+    def test_range_unbound(self):\n+        \"\"\"A query with RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING.\"\"\"\n+        qs = Employee.objects.annotate(sum=Window(\n+            expression=Sum('salary'),\n+            partition_by='department',\n+            order_by=[F('hire_date').asc(), F('name').asc()],\n+            frame=ValueRange(start=None, end=None),\n+        )).order_by('department', 'hire_date', 'name')\n+        self.assertIn('RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING', str(qs.query))\n+        self.assertQuerysetEqual(qs, [\n+            ('Jones', 'Accounting', 45000, datetime.date(2005, 11, 1), 177000),\n+            ('Jenson', 'Accounting', 45000, datetime.date(2008, 4, 1), 177000),\n+            ('Williams', 'Accounting', 37000, datetime.date(2009, 6, 1), 177000),\n+            ('Adams', 'Accounting', 50000, datetime.date(2013, 7, 1), 177000),\n+            ('Wilkinson', 'IT', 60000, datetime.date(2011, 3, 1), 94000),\n+            ('Moore', 'IT', 34000, datetime.date(2013, 8, 1), 94000),\n+            ('Miller', 'Management', 100000, datetime.date(2005, 6, 1), 180000),\n+            ('Johnson', 'Management', 80000, datetime.date(2005, 7, 1), 180000),\n+            ('Smith', 'Marketing', 38000, datetime.date(2009, 10, 1), 78000),\n+            ('Johnson', 'Marketing', 40000, datetime.date(2012, 3, 1), 78000),\n+            ('Smith', 'Sales', 55000, datetime.date(2007, 6, 1), 108000),\n+            ('Brown', 'Sales', 53000, datetime.date(2009, 9, 1), 108000),\n+        ], transform=lambda row: (row.name, row.department, row.salary, row.hire_date, row.sum))\n+\n+    def test_row_range_rank(self):\n+        \"\"\"\n+        A query with ROWS BETWEEN UNBOUNDED PRECEDING AND 3 FOLLOWING.\n+        The resulting sum is the sum of the three next (if they exist) and all\n+        previous rows according to the ordering clause.\n+        \"\"\"\n+        qs = Employee.objects.annotate(sum=Window(\n+            expression=Sum('salary'),\n+            order_by=[F('hire_date').asc(), F('name').desc()],\n+            frame=RowRange(start=None, end=3),\n+        )).order_by('sum', 'hire_date')\n+        self.assertIn('ROWS BETWEEN UNBOUNDED PRECEDING AND 3 FOLLOWING', str(qs.query))\n+        self.assertQuerysetEqual(qs, [\n+            ('Miller', 100000, 'Management', datetime.date(2005, 6, 1), 280000),\n+            ('Johnson', 80000, 'Management', datetime.date(2005, 7, 1), 325000),\n+            ('Jones', 45000, 'Accounting', datetime.date(2005, 11, 1), 362000),\n+            ('Smith', 55000, 'Sales', datetime.date(2007, 6, 1), 415000),\n+            ('Jenson', 45000, 'Accounting', datetime.date(2008, 4, 1), 453000),\n+            ('Williams', 37000, 'Accounting', datetime.date(2009, 6, 1), 513000),\n+            ('Brown', 53000, 'Sales', datetime.date(2009, 9, 1), 553000),\n+            ('Smith', 38000, 'Marketing', datetime.date(2009, 10, 1), 603000),\n+            ('Wilkinson', 60000, 'IT', datetime.date(2011, 3, 1), 637000),\n+            ('Johnson', 40000, 'Marketing', datetime.date(2012, 3, 1), 637000),\n+            ('Adams', 50000, 'Accounting', datetime.date(2013, 7, 1), 637000),\n+            ('Moore', 34000, 'IT', datetime.date(2013, 8, 1), 637000),\n+        ], transform=lambda row: (row.name, row.salary, row.department, row.hire_date, row.sum))\n+\n+    @skipUnlessDBFeature('can_distinct_on_fields')\n+    def test_distinct_window_function(self):\n+        \"\"\"\n+        Window functions are not aggregates, and hence a query to filter out\n+        duplicates may be useful.\n+        \"\"\"\n+        qs = Employee.objects.annotate(\n+            sum=Window(\n+                expression=Sum('salary'),\n+                partition_by=ExtractYear('hire_date'),\n+                order_by=ExtractYear('hire_date')\n+            ),\n+            year=ExtractYear('hire_date'),\n+        ).values('year', 'sum').distinct('year').order_by('year')\n+        results = [\n+            {'year': 2005, 'sum': 225000}, {'year': 2007, 'sum': 55000},\n+            {'year': 2008, 'sum': 45000}, {'year': 2009, 'sum': 128000},\n+            {'year': 2011, 'sum': 60000}, {'year': 2012, 'sum': 40000},\n+            {'year': 2013, 'sum': 84000},\n+        ]\n+        for idx, val in zip(range(len(results)), results):\n+            with self.subTest(result=val):\n+                self.assertEqual(qs[idx], val)\n+\n+    def test_fail_update(self):\n+        \"\"\"Window expressions can't be used in an UPDATE statement.\"\"\"\n+        msg = 'Window expressions are not allowed in this query'\n+        with self.assertRaisesMessage(FieldError, msg):\n+            Employee.objects.filter(department='Management').update(\n+                salary=Window(expression=Max('salary'), partition_by='department'),\n+            )\n+\n+    def test_fail_insert(self):\n+        \"\"\"Window expressions can't be used in an INSERT statement.\"\"\"\n+        msg = 'Window expressions are not allowed in this query'\n+        with self.assertRaisesMessage(FieldError, msg):\n+            Employee.objects.create(\n+                name='Jameson', department='Management', hire_date=datetime.date(2007, 7, 1),\n+                salary=Window(expression=Sum(Value(10000), order_by=F('pk').asc())),\n+            )\n+\n+    def test_invalid_start_value_range(self):\n+        msg = \"start argument must be a negative integer, zero, or None, but got '3'.\"\n+        with self.assertRaisesMessage(ValueError, msg):\n+            list(Employee.objects.annotate(test=Window(\n+                expression=Sum('salary'),\n+                order_by=F('hire_date').asc(),\n+                frame=ValueRange(start=3),\n+            )))\n+\n+    def test_invalid_end_value_range(self):\n+        msg = \"end argument must be a positive integer, zero, or None, but got '-3'.\"\n+        with self.assertRaisesMessage(ValueError, msg):\n+            list(Employee.objects.annotate(test=Window(\n+                expression=Sum('salary'),\n+                order_by=F('hire_date').asc(),\n+                frame=ValueRange(end=-3),\n+            )))\n+\n+    def test_invalid_type_end_value_range(self):\n+        msg = \"end argument must be a positive integer, zero, or None, but got 'a'.\"\n+        with self.assertRaisesMessage(ValueError, msg):\n+            list(Employee.objects.annotate(test=Window(\n+                expression=Sum('salary'),\n+                order_by=F('hire_date').asc(),\n+                frame=ValueRange(end='a'),\n+            )))\n+\n+    def test_invalid_type_start_value_range(self):\n+        msg = \"start argument must be a negative integer, zero, or None, but got 'a'.\"\n+        with self.assertRaisesMessage(ValueError, msg):\n+            list(Employee.objects.annotate(test=Window(\n+                expression=Sum('salary'),\n+                frame=ValueRange(start='a'),\n+            )))\n+\n+    def test_invalid_type_end_row_range(self):\n+        msg = \"end argument must be a positive integer, zero, or None, but got 'a'.\"\n+        with self.assertRaisesMessage(ValueError, msg):\n+            list(Employee.objects.annotate(test=Window(\n+                expression=Sum('salary'),\n+                frame=RowRange(end='a'),\n+            )))\n+\n+    @skipUnless(connection.vendor == 'postgresql', 'Frame construction not allowed on PostgreSQL')\n+    def test_postgresql_illegal_range_frame_start(self):\n+        msg = 'PostgreSQL only supports UNBOUNDED together with PRECEDING and FOLLOWING.'\n+        with self.assertRaisesMessage(NotSupportedError, msg):\n+            list(Employee.objects.annotate(test=Window(\n+                expression=Sum('salary'),\n+                order_by=F('hire_date').asc(),\n+                frame=ValueRange(start=-1),\n+            )))\n+\n+    @skipUnless(connection.vendor == 'postgresql', 'Frame construction not allowed on PostgreSQL')\n+    def test_postgresql_illegal_range_frame_end(self):\n+        msg = 'PostgreSQL only supports UNBOUNDED together with PRECEDING and FOLLOWING.'\n+        with self.assertRaisesMessage(NotSupportedError, msg):\n+            list(Employee.objects.annotate(test=Window(\n+                expression=Sum('salary'),\n+                order_by=F('hire_date').asc(),\n+                frame=ValueRange(end=1),\n+            )))\n+\n+    def test_invalid_type_start_row_range(self):\n+        msg = \"start argument must be a negative integer, zero, or None, but got 'a'.\"\n+        with self.assertRaisesMessage(ValueError, msg):\n+            list(Employee.objects.annotate(test=Window(\n+                expression=Sum('salary'),\n+                order_by=F('hire_date').asc(),\n+                frame=RowRange(start='a'),\n+            )))\n+\n+\n+class NonQueryWindowTests(SimpleTestCase):\n+    def test_window_repr(self):\n+        self.assertEqual(\n+            repr(Window(expression=Sum('salary'), partition_by='department')),\n+            '<Window: Sum(F(salary)) OVER (PARTITION BY F(department))>'\n+        )\n+        self.assertEqual(\n+            repr(Window(expression=Avg('salary'), order_by=F('department').asc())),\n+            '<Window: Avg(F(salary)) OVER (ORDER BY OrderBy(F(department), descending=False))>'\n+        )\n+\n+    def test_window_frame_repr(self):\n+        self.assertEqual(\n+            repr(RowRange(start=-1)),\n+            '<RowRange: ROWS BETWEEN 1 PRECEDING AND UNBOUNDED FOLLOWING>'\n+        )\n+        self.assertEqual(\n+            repr(ValueRange(start=None, end=1)),\n+            '<ValueRange: RANGE BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING>'\n+        )\n+        self.assertEqual(\n+            repr(ValueRange(start=0, end=0)),\n+            '<ValueRange: RANGE BETWEEN CURRENT ROW AND CURRENT ROW>'\n+        )\n+        self.assertEqual(\n+            repr(RowRange(start=0, end=0)),\n+            '<RowRange: ROWS BETWEEN CURRENT ROW AND CURRENT ROW>'\n+        )\n+\n+    def test_empty_group_by_cols(self):\n+        window = Window(expression=Sum('pk'))\n+        self.assertEqual(window.get_group_by_cols(), [])\n+        self.assertFalse(window.contains_aggregate)\n+\n+    def test_frame_empty_group_by_cols(self):\n+        frame = WindowFrame()\n+        self.assertEqual(frame.get_group_by_cols(), [])\n+\n+    def test_frame_window_frame_notimplemented(self):\n+        frame = WindowFrame()\n+        msg = 'Subclasses must implement window_frame_start_end().'\n+        with self.assertRaisesMessage(NotImplementedError, msg):\n+            frame.window_frame_start_end(None, None, None)\n+\n+    def test_invalid_filter(self):\n+        msg = 'Window is disallowed in the filter clause'\n+        with self.assertRaisesMessage(NotSupportedError, msg):\n+            Employee.objects.annotate(dense_rank=Window(expression=DenseRank())).filter(dense_rank__gte=1)\n+\n+    def test_invalid_order_by(self):\n+        msg = 'order_by must be either an Expression or a sequence of expressions'\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Window(expression=Sum('power'), order_by='-horse')\n+\n+    def test_invalid_source_expression(self):\n+        msg = \"Expression 'Upper' isn't compatible with OVER clauses.\"\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Window(expression=Upper('name'))\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Great work on this PR! However, consider disallowing filtering by annotated window expressions to prevent exceptions. Additionally, the `GROUP BY` issue needs addressing to prevent unintended additions to the query. Aggregates shouldn't inherit from `WindowFunc`; instead, integrate the `window_compatible` property into the base aggregate type. Minimal changes to documentation around this are essential. Lastly, itd be beneficial to enhance test coverage with diverse sample data to fully illustrate concepts, while ensuring redundancy in tests is minimized."
      }
    ],
    "codediff_length": 63637
  },
  {
    "index": 12,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/tests/migrations/test_autodetector.py b/tests/migrations/test_autodetector.py\nindex d2f9be75f7d9..82ddb1754323 100644\n--- a/tests/migrations/test_autodetector.py\n+++ b/tests/migrations/test_autodetector.py\n@@ -2349,6 +2357,58 @@ def test_alter_db_table_with_model_change(self):\n             changes, \"testapp\", 0, 1, name=\"newauthor\", table=\"author_three\"\n         )\n \n+    def test_alter_db_table_comment_add(self):\n+        changes = self.get_changes(\n+            [self.author_empty], [self.author_with_db_table_comment]\n+        )\n+        self.assertNumberMigrations(changes, \"testapp\", 1)\n+        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n+        self.assertOperationAttributes(\n+            changes, \"testapp\", 0, 0, name=\"author\", table_comment=\"Table comment\"\n+        )\n+\n+    def test_alter_db_table_comment_change(self):\n+        author_with_new_db_table_comment = ModelState(\n+            \"testapp\",\n+            \"Author\",\n+            [\n+                (\"id\", models.AutoField(primary_key=True)),\n+            ],\n+            {\"db_table_comment\": \"New table comment\"},\n+        )\n+        changes = self.get_changes(\n+            [self.author_with_db_table_comment],\n+            [author_with_new_db_table_comment],\n+        )\n+        self.assertNumberMigrations(changes, \"testapp\", 1)\n+        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n+        self.assertOperationAttributes(\n+            changes,\n+            \"testapp\",\n+            0,\n+            0,\n+            name=\"author\",\n+            table_comment=\"New table comment\",\n+        )\n+\n+    def test_alter_db_table_comment_remove(self):\n+        changes = self.get_changes(\n+            [self.author_with_db_table_comment],\n+            [self.author_empty],\n+        )\n+        self.assertNumberMigrations(changes, \"testapp\", 1)\n+        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n+        self.assertOperationAttributes(\n+            changes, \"testapp\", 0, 0, name=\"author\", db_table_comment=None\n+        )\n+\n+    def test_alter_db_table_comment_no_changes(self):\n+        changes = self.get_changes(\n+            [self.author_with_db_table_comment],\n+            [self.author_with_db_table_comment],\n+        )\n+        self.assertNumberMigrations(changes, \"testapp\", 0)\n+\n     def test_identical_regex_doesnt_alter(self):\n         from_state = ModelState(\n             \"testapp\",\ndiff --git a/tests/schema/tests.py b/tests/schema/tests.py\nindex 9b2b512c1b1f..e325fa0c8846 100644\n--- a/tests/schema/tests.py\n+++ b/tests/schema/tests.py\n@@ -4390,6 +4411,186 @@ def test_add_unique_charfield(self):\n             ],\n         )\n \n+    @skipUnlessDBFeature(\"supports_comments\")\n+    def test_add_db_comment_charfield(self):\n+        comment = \"Custom comment\"\n+        field = CharField(max_length=255, db_comment=comment)\n+        field.set_attributes_from_name(\"name_with_comment\")\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+            editor.add_field(Author, field)\n+        self.assertEqual(\n+            self.get_column_comment(Author._meta.db_table, \"name_with_comment\"),\n+            comment,\n+        )\n+\n+    @skipUnlessDBFeature(\"supports_comments\")\n+    def test_add_db_comment_and_default_charfield(self):\n+        comment = \"Custom comment with default\"\n+        field = CharField(max_length=255, default=\"Joe Doe\", db_comment=comment)\n+        field.set_attributes_from_name(\"name_with_comment_default\")\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+            Author.objects.create(name=\"Before adding a new field\")\n+            editor.add_field(Author, field)\n+\n+        self.assertEqual(\n+            self.get_column_comment(Author._meta.db_table, \"name_with_comment_default\"),\n+            comment,\n+        )\n+        with connection.cursor() as cursor:\n+            cursor.execute(\n+                f\"SELECT name_with_comment_default FROM {Author._meta.db_table};\"\n+            )\n+            for row in cursor.fetchall():\n+                self.assertEqual(row[0], \"Joe Doe\")\n+\n+    @skipUnlessDBFeature(\"supports_comments\")\n+    def test_alter_db_comment(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        # Add comment.\n+        old_field = Author._meta.get_field(\"name\")\n+        new_field = CharField(max_length=255, db_comment=\"Custom comment\")\n+        new_field.set_attributes_from_name(\"name\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(Author, old_field, new_field, strict=True)\n+        self.assertEqual(\n+            self.get_column_comment(Author._meta.db_table, \"name\"),\n+            \"Custom comment\",\n+        )\n+        # Alter comment.\n+        old_field = new_field\n+        new_field = CharField(max_length=255, db_comment=\"New custom comment\")\n+        new_field.set_attributes_from_name(\"name\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(Author, old_field, new_field, strict=True)\n+        self.assertEqual(\n+            self.get_column_comment(Author._meta.db_table, \"name\"),\n+            \"New custom comment\",\n+        )\n+        # Remove comment.\n+        old_field = new_field\n+        new_field = CharField(max_length=255)\n+        new_field.set_attributes_from_name(\"name\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(Author, old_field, new_field, strict=True)\n+        self.assertIn(\n+            self.get_column_comment(Author._meta.db_table, \"name\"),\n+            [None, \"\"],\n+        )\n+\n+    @skipUnlessDBFeature(\"supports_comments\", \"supports_foreign_keys\")\n+    def test_alter_db_comment_foreign_key(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+            editor.create_model(Book)\n+\n+        comment = \"FK custom comment\"\n+        old_field = Book._meta.get_field(\"author\")\n+        new_field = ForeignKey(Author, CASCADE, db_comment=comment)\n+        new_field.set_attributes_from_name(\"author\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(Book, old_field, new_field, strict=True)\n+        self.assertEqual(\n+            self.get_column_comment(Book._meta.db_table, \"author_id\"),\n+            comment,\n+        )\n+\n+    @skipUnlessDBFeature(\"supports_comments\")\n+    def test_alter_field_type_preserve_comment(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+\n+        comment = \"This is the name.\"\n+        old_field = Author._meta.get_field(\"name\")\n+        new_field = CharField(max_length=255, db_comment=comment)\n+        new_field.set_attributes_from_name(\"name\")\n+        new_field.model = Author\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(Author, old_field, new_field, strict=True)\n+        self.assertEqual(\n+            self.get_column_comment(Author._meta.db_table, \"name\"),\n+            comment,\n+        )\n+        # Changing a field type should preserve the comment.\n+        old_field = new_field\n+        new_field = CharField(max_length=511, db_comment=comment)\n+        new_field.set_attributes_from_name(\"name\")\n+        new_field.model = Author\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(Author, new_field, old_field, strict=True)\n+        # Comment is preserved.\n+        self.assertEqual(\n+            self.get_column_comment(Author._meta.db_table, \"name\"),\n+            comment,\n+        )\n+\n+    @isolate_apps(\"schema\")\n+    @skipUnlessDBFeature(\"supports_comments\")\n+    def test_db_comment_table(self):\n+        class ModelWithDbTableComment(Model):\n+            class Meta:\n+                app_label = \"schema\"\n+                db_table_comment = \"Custom table comment\"\n+\n+        with connection.schema_editor() as editor:\n+            editor.create_model(ModelWithDbTableComment)\n+        self.isolated_local_models = [ModelWithDbTableComment]\n+        self.assertEqual(\n+            self.get_table_comment(ModelWithDbTableComment._meta.db_table),\n+            \"Custom table comment\",\n+        )\n+        # Alter table comment.\n+        old_db_table_comment = ModelWithDbTableComment._meta.db_table_comment\n+        with connection.schema_editor() as editor:\n+            editor.alter_db_table_comment(\n+                ModelWithDbTableComment, old_db_table_comment, \"New table comment\"\n+            )\n+        self.assertEqual(\n+            self.get_table_comment(ModelWithDbTableComment._meta.db_table),\n+            \"New table comment\",\n+        )\n+        # Remove table comment.\n+        old_db_table_comment = ModelWithDbTableComment._meta.db_table_comment\n+        with connection.schema_editor() as editor:\n+            editor.alter_db_table_comment(\n+                ModelWithDbTableComment, old_db_table_comment, None\n+            )\n+        self.assertIn(\n+            self.get_table_comment(ModelWithDbTableComment._meta.db_table),\n+            [None, \"\"],\n+        )\n+\n+    @isolate_apps(\"schema\")\n+    @skipUnlessDBFeature(\"supports_comments\", \"supports_foreign_keys\")\n+    def test_db_comments_from_abstract_model(self):\n+        class AbstractModelWithDbComments(Model):\n+            name = CharField(\n+                max_length=255, db_comment=\"Custom comment\", null=True, blank=True\n+            )\n+\n+            class Meta:\n+                app_label = \"schema\"\n+                abstract = True\n+                db_table_comment = \"Custom table comment\"\n+\n+        class ModelWithDbComments(AbstractModelWithDbComments):\n+            pass\n+\n+        with connection.schema_editor() as editor:\n+            editor.create_model(ModelWithDbComments)\n+        self.isolated_local_models = [ModelWithDbComments]\n+\n+        self.assertEqual(\n+            self.get_column_comment(ModelWithDbComments._meta.db_table, \"name\"),\n+            \"Custom comment\",\n+        )\n+        self.assertEqual(\n+            self.get_table_comment(ModelWithDbComments._meta.db_table),\n+            \"Custom table comment\",\n+        )\n+\n     @unittest.skipUnless(connection.vendor == \"postgresql\", \"PostgreSQL specific\")\n     def test_alter_field_add_index_to_charfield(self):\n         # Create the table and verify no initial indexes.\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR discusses the addition of `db_comment` support in Django, highlighting the limitations of existing test utilities like `assertTableExists()`. It suggests adding a new assertion method for better coverage and proposes modifying `get_table_description()` to include `db_comment`, although this raises concerns about targeting test-specific changes. Several test cases were added, particularly for ManyToManyFields and proxy models, emphasizing the need for careful handling of db comments in those scenarios. The collaborative approach to feedback is commendable, and the author is proactive in considering edge cases and documentation needs."
      }
    ],
    "codediff_length": 10419
  },
  {
    "index": 13,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/core/management/commands/shell.py b/django/core/management/commands/shell.py\nindex f55b346406aa..1619561fea2e 100644\n--- a/django/core/management/commands/shell.py\n+++ b/django/core/management/commands/shell.py\n@@ -111,10 +127,68 @@ def python(self, options):\n         # Start the interactive interpreter.\n         code.interact(local=imported_objects)\n \n+    def get_and_report_namespace(self, verbosity, no_imports=False):\n+        if no_imports:\n+            return {}\n+\n+        namespace = self.get_namespace()\n+\n+        if verbosity < 1:\n+            return namespace\n+\n+        amount = len(namespace)\n+        msg = f\"{amount} objects imported automatically\"\n+\n+        if verbosity < 2:\n+            self.stdout.write(f\"{msg} (use -v 2 for details).\", self.style.SUCCESS)\n+            return namespace\n+\n+        imports_by_module = defaultdict(list)\n+        for obj_name, obj in namespace.items():\n+            if hasattr(obj, \"__module__\") and (\n+                (hasattr(obj, \"__qualname__\") and obj.__qualname__.find(\".\") == -1)\n+                or not hasattr(obj, \"__qualname__\")\n+            ):\n+                imports_by_module[obj.__module__].append(obj_name)\n+            if not hasattr(obj, \"__module__\") and hasattr(obj, \"__name__\"):\n+                tokens = obj.__name__.split(\".\")\n+                if obj_name in tokens:\n+                    module = \".\".join(t for t in tokens if t != obj_name)\n+                    imports_by_module[module].append(obj_name)\n+\n+        import_string = \"\\n\".join(\n+            [\n+                f\"  from {module} import {objects}\"\n+                for module, imported_objects in imports_by_module.items()\n+                if (objects := \", \".join(imported_objects))\n+            ]\n+        )\n+\n+        try:\n+            import isort\n+        except ImportError:\n+            pass\n+        else:\n+            import_string = isort.code(import_string)\n+\n+        self.stdout.write(\n+            f\"{msg}, including:\\n\\n{import_string}\", self.style.SUCCESS, ending=\"\\n\\n\"\n+        )\n+\n+        return namespace\n+\n+    def get_namespace(self):\n+        apps_models = apps.get_models()\n+        namespace = {}\n+        for model in reversed(apps_models):\n+            if model.__module__:\n+                namespace[model.__name__] = model\n+        return namespace\n+\n     def handle(self, **options):\n         # Execute the command and exit.\n         if options[\"command\"]:\n-            exec(options[\"command\"], globals())\n+            exec(options[\"command\"], {**globals(), **self.get_namespace()})\n             return\n \n         # Execute stdin if it has anything to read and exit.\n\ndiff --git a/docs/howto/custom-shell.txt b/docs/howto/custom-shell.txt\nnew file mode 100644\nindex 000000000000..07034dd197d4\n--- /dev/null\n+++ b/docs/howto/custom-shell.txt\n@@ -0,0 +1,57 @@\n+======================================\n+How to customize the ``shell`` command\n+======================================\n+\n+The Django :djadmin:`shell` is an interactive Python environment that provides\n+access to models and settings, making it useful for testing code, experimenting\n+with queries, and interacting with application data.\n+\n+Customizing the :djadmin:`shell` command allows adding extra functionality or\n+pre-loading specific modules. To do this, create a new management command that subclasses\n+``django.core.management.commands.shell.Command`` and overrides the existing\n+``shell`` management command. For more details, refer to the guide on\n+:ref:`overriding commands <overriding-commands>`.\n+\n+.. _customizing-shell-auto-imports:\n+\n+Customize automatic imports\n+===========================\n+\n+.. versionadded:: 5.2\n+\n+To customize the automatic import behavior of the :djadmin:`shell` management\n+command, override the ``get_namespace()`` method. For example:\n+\n+.. code-block:: python\n+    :caption: ``polls/management/commands/shell.py``\n+\n+    from django.core.management.commands import shell\n+\n+\n+    class Command(shell.Command):\n+        def get_namespace(self):\n+            from django.urls.base import resolve, reverse\n+\n+            return {\n+                **super().get_namespace(),\n+                \"resolve\": resolve,\n+                \"reverse\": reverse,\n+            }\n+\n+The above customization adds :func:`~django.urls.resolve` and\n+:func:`~django.urls.reverse` to the default namespace, which includes all\n+models from all apps. These two functions will then be available when the\n+shell opens, without a manual import statement.\n+\n+If you prefer to not have models automatically imported, create a custom\n+``get_namespace()`` that excludes the ``super().get_namespace()`` call:\n+\n+.. code-block:: python\n+    :caption: ``polls/management/commands/shell.py``\n+\n+    from django.core.management.commands import shell\n+\n+\n+    class Command(shell.Command):\n+        def get_namespace(self):\n+            return {}\ndiff --git a/tests/shell/tests.py b/tests/shell/tests.py\nindex ca823f6290a6..49528cca8e6d 100644\n--- a/tests/shell/tests.py\n+++ b/tests/shell/tests.py\n@@ -122,3 +141,166 @@ def test_python(self):\n     # returns EOF and so select always shows that sys.stdin is ready to read.\n     # This causes problems because of the call to select.select() toward the\n     # end of shell's handle() method.\n+\n+\n+class ShellCommandAutoImportsTestCase(SimpleTestCase):\n+\n+    @override_settings(\n+        INSTALLED_APPS=[\"shell\", \"django.contrib.auth\", \"django.contrib.contenttypes\"]\n+    )\n+    def test_get_namespace(self):\n+        namespace = shell.Command().get_namespace()\n+\n+        self.assertEqual(\n+            namespace,\n+            {\n+                \"Marker\": Marker,\n+                \"Phone\": Phone,\n+                \"ContentType\": ContentType,\n+                \"Group\": Group,\n+                \"Permission\": Permission,\n+                \"User\": User,\n+            },\n+        )\n+\n+    @override_settings(INSTALLED_APPS=[\"basic\", \"shell\"])\n+    @isolate_apps(\"basic\", \"shell\", kwarg_name=\"apps\")\n+    def test_get_namespace_precedence(self, apps):\n+        class Article(models.Model):\n+            class Meta:\n+                app_label = \"basic\"\n+\n+        winner_article = Article\n+\n+        class Article(models.Model):\n+            class Meta:\n+                app_label = \"shell\"\n+\n+        with mock.patch(\"django.apps.apps.get_models\", return_value=apps.get_models()):\n+            namespace = shell.Command().get_namespace()\n+            self.assertEqual(namespace, {\"Article\": winner_article})\n+\n+    @override_settings(\n+        INSTALLED_APPS=[\"shell\", \"django.contrib.auth\", \"django.contrib.contenttypes\"]\n+    )\n+    def test_get_namespace_overridden(self):\n+        class TestCommand(shell.Command):\n+            def get_namespace(self):\n+                from django.urls.base import resolve, reverse\n+\n+                return {\n+                    **super().get_namespace(),\n+                    \"resolve\": resolve,\n+                    \"reverse\": reverse,\n+                }\n+\n+        namespace = TestCommand().get_namespace()\n+\n+        self.assertEqual(\n+            namespace,\n+            {\n+                \"resolve\": resolve,\n+                \"reverse\": reverse,\n+                \"Marker\": Marker,\n+                \"Phone\": Phone,\n+                \"ContentType\": ContentType,\n+                \"Group\": Group,\n+                \"Permission\": Permission,\n+                \"User\": User,\n+            },\n+        )\n+\n+    @override_settings(\n+        INSTALLED_APPS=[\"shell\", \"django.contrib.auth\", \"django.contrib.contenttypes\"]\n+    )\n+    def test_no_imports_flag(self):\n+        for verbosity in (0, 1, 2, 3):\n+            with self.subTest(verbosity=verbosity), captured_stdout() as stdout:\n+                namespace = shell.Command().get_and_report_namespace(\n+                    verbosity=verbosity, no_imports=True\n+                )\n+            self.assertEqual(namespace, {})\n+            self.assertEqual(stdout.getvalue().strip(), \"\")\n+\n+    @override_settings(\n+        INSTALLED_APPS=[\"shell\", \"django.contrib.auth\", \"django.contrib.contenttypes\"]\n+    )\n+    def test_verbosity_zero(self):\n+        with captured_stdout() as stdout:\n+            cmd = shell.Command()\n+            namespace = cmd.get_and_report_namespace(verbosity=0)\n+        self.assertEqual(namespace, cmd.get_namespace())\n+        self.assertEqual(stdout.getvalue().strip(), \"\")\n+\n+    @override_settings(\n+        INSTALLED_APPS=[\"shell\", \"django.contrib.auth\", \"django.contrib.contenttypes\"]\n+    )\n+    def test_verbosity_one(self):\n+        with captured_stdout() as stdout:\n+            cmd = shell.Command()\n+            namespace = cmd.get_and_report_namespace(verbosity=1)\n+        self.assertEqual(namespace, cmd.get_namespace())\n+        self.assertEqual(\n+            stdout.getvalue().strip(),\n+            \"6 objects imported automatically (use -v 2 for details).\",\n+        )\n+\n+    @override_settings(INSTALLED_APPS=[\"shell\", \"django.contrib.contenttypes\"])\n+    @mock.patch.dict(sys.modules, {\"isort\": None})\n+    def test_message_with_stdout_listing_objects_with_isort_not_installed(self):\n+        class TestCommand(shell.Command):\n+            def get_namespace(self):\n+                class MyClass:\n+                    pass\n+\n+                constant = \"constant\"\n+\n+                return {\n+                    **super().get_namespace(),\n+                    \"MyClass\": MyClass,\n+                    \"constant\": constant,\n+                }\n+\n+        with captured_stdout() as stdout:\n+            TestCommand().get_and_report_namespace(verbosity=2)\n+\n+        self.assertEqual(\n+            stdout.getvalue().strip(),\n+            \"5 objects imported automatically, including:\\n\\n\"\n+            \"  from django.contrib.contenttypes.models import ContentType\\n\"\n+            \"  from shell.models import Phone, Marker\",\n+        )\n+\n+    @override_settings(INSTALLED_APPS=[\"shell\", \"django.contrib.contenttypes\"])\n+    def test_message_with_stdout_listing_objects_with_isort(self):\n+        sorted_imports = (\n+            \"  from shell.models import Marker, Phone\\n\\n\"\n+            \"  from django.contrib.contenttypes.models import ContentType\"\n+        )\n+        mock_isort_code = mock.Mock(code=mock.MagicMock(return_value=sorted_imports))\n+\n+        class TestCommand(shell.Command):\n+            def get_namespace(self):\n+                class MyClass:\n+                    pass\n+\n+                constant = \"constant\"\n+\n+                return {\n+                    **super().get_namespace(),\n+                    \"MyClass\": MyClass,\n+                    \"constant\": constant,\n+                }\n+\n+        with (\n+            mock.patch.dict(sys.modules, {\"isort\": mock_isort_code}),\n+            captured_stdout() as stdout,\n+        ):\n+            TestCommand().get_and_report_namespace(verbosity=2)\n+\n+        self.assertEqual(\n+            stdout.getvalue().strip(),\n+            \"5 objects imported automatically, including:\\n\\n\"\n+            \"  from shell.models import Marker, Phone\\n\\n\"\n+            \"  from django.contrib.contenttypes.models import ContentType\",\n+        )\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The tests need to be added within `ShellCommandTestCase`. Unit-test `get_namespace()` directly and complement it with integration tests similar to current ones. Existing tests lack coverage for different shell launches; adding tests for the default `python` shell would be beneficial, though be cautious about interaction with startup files. It's worth addressing the `--no-startup` flag's behavior regarding auto-imports, and filtering out None values from `locals()` aligns with clean coding practices. Lastly, sorting imports using isort is recommended to maintain consistency."
      }
    ],
    "codediff_length": 11129
  },
  {
    "index": 14,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/db/models/indexes.py b/django/db/models/indexes.py\nindex fddde3b16f34..5530d0b66128 100644\n--- a/django/db/models/indexes.py\n+++ b/django/db/models/indexes.py\n@@ -137,3 +184,84 @@ def __eq__(self, other):\n         if self.__class__ == other.__class__:\n             return self.deconstruct() == other.deconstruct()\n         return NotImplemented\n+\n+\n+class IndexExpression(Func):\n+    \"\"\"Order and wrap expressions for CREATE INDEX statements.\"\"\"\n+    template = '%(expressions)s'\n+    wrapper_classes = (OrderBy, Collate)\n+\n+    def set_wrapper_classes(self, connection=None):\n+        # Some databases (e.g. MySQL) treats COLLATE as an indexed expression.\n+        if connection and connection.features.collate_as_index_expression:\n+            self.wrapper_classes = tuple([\n+                wrapper_cls\n+                for wrapper_cls in self.wrapper_classes\n+                if wrapper_cls is not Collate\n+            ])\n+\n+    @classmethod\n+    def register_wrappers(cls, *wrapper_classes):\n+        cls.wrapper_classes = wrapper_classes\n+\n+    def resolve_expression(\n+        self,\n+        query=None,\n+        allow_joins=True,\n+        reuse=None,\n+        summarize=False,\n+        for_save=False,\n+    ):\n+        expressions = list(self.flatten())\n+        # Split expressions and wrappers.\n+        index_expressions, wrappers = partition(\n+            lambda e: isinstance(e, self.wrapper_classes),\n+            expressions,\n+        )\n+        wrapper_types = [type(wrapper) for wrapper in wrappers]\n+        if len(wrapper_types) != len(set(wrapper_types)):\n+            raise ValueError(\n+                \"Multiple references to %s can't be used in an indexed \"\n+                \"expression.\" % ', '.join([\n+                    wrapper_cls.__qualname__ for wrapper_cls in self.wrapper_classes\n+                ])\n+            )\n+        if expressions[1:len(wrappers) + 1] != wrappers:\n+            raise ValueError(\n+                '%s must be topmost expressions in an indexed expression.'\n+                % ', '.join([\n+                    wrapper_cls.__qualname__ for wrapper_cls in self.wrapper_classes\n+                ])\n+            )\n+        # Wrap expressions in parentheses if they are not column references.\n+        root_expression = index_expressions[1]\n+        resolve_root_expression = root_expression.resolve_expression(\n+            query,\n+            allow_joins,\n+            reuse,\n+            summarize,\n+            for_save,\n+        )\n+        if not isinstance(resolve_root_expression, Col):\n+            root_expression = Func(root_expression, template='(%(expressions)s)')\n+\n+        if wrappers:\n+            # Order wrappers and set their expressions.\n+            wrappers = sorted(\n+                wrappers,\n+                key=lambda w: self.wrapper_classes.index(type(w)),\n+            )\n+            wrappers = [wrapper.copy() for wrapper in wrappers]\n+            for i, wrapper in enumerate(wrappers[:-1]):\n+                wrapper.set_source_expressions([wrappers[i + 1]])\n+            # Set the root expression on the deepest wrapper.\n+            wrappers[-1].set_source_expressions([root_expression])\n+            self.set_source_expressions([wrappers[0]])\n+        else:\n+            # Use the root expression, if there are no wrappers.\n+            self.set_source_expressions([root_expression])\n+        return super().resolve_expression(query, allow_joins, reuse, summarize, for_save)\n+\n+    def as_sqlite(self, compiler, connection, **extra_context):\n+        # Casting to numeric is unnecessary.\n+        return self.as_sql(compiler, connection, **extra_context)\ndiff --git a/tests/backends/test_ddl_references.py b/tests/backends/test_ddl_references.py\nindex d96ebcb57f08..bd4036ee3358 100644\n--- a/tests/backends/test_ddl_references.py\n+++ b/tests/backends/test_ddl_references.py\n@@ -181,3 +187,66 @@ def test_str(self):\n         reference = MockReference('reference', {}, {})\n         statement = Statement(\"%(reference)s - %(non_reference)s\", reference=reference, non_reference='non_reference')\n         self.assertEqual(str(statement), 'reference - non_reference')\n+\n+\n+class ExpressionsTests(TransactionTestCase):\n+    available_apps = []\n+\n+    def setUp(self):\n+        compiler = Person.objects.all().query.get_compiler(connection.alias)\n+        self.editor = connection.schema_editor()\n+        self.expressions = Expressions(\n+            table=Person._meta.db_table,\n+            expressions=ExpressionList(\n+                IndexExpression(F('first_name')),\n+                IndexExpression(F('last_name').desc()),\n+                IndexExpression(Upper('last_name')),\n+            ).resolve_expression(compiler.query),\n+            compiler=compiler,\n+            quote_value=self.editor.quote_value,\n+        )\n+\n+    def test_references_table(self):\n+        self.assertIs(self.expressions.references_table(Person._meta.db_table), True)\n+        self.assertIs(self.expressions.references_table('other'), False)\n+\n+    def test_references_column(self):\n+        table = Person._meta.db_table\n+        self.assertIs(self.expressions.references_column(table, 'first_name'), True)\n+        self.assertIs(self.expressions.references_column(table, 'last_name'), True)\n+        self.assertIs(self.expressions.references_column(table, 'other'), False)\n+\n+    def test_rename_table_references(self):\n+        table = Person._meta.db_table\n+        self.expressions.rename_table_references(table, 'other')\n+        self.assertIs(self.expressions.references_table(table), False)\n+        self.assertIs(self.expressions.references_table('other'), True)\n+        self.assertIn(\n+            '%s.%s' % (\n+                self.editor.quote_name('other'),\n+                self.editor.quote_name('first_name'),\n+            ),\n+            str(self.expressions),\n+        )\n+\n+    def test_rename_column_references(self):\n+        table = Person._meta.db_table\n+        self.expressions.rename_column_references(table, 'first_name', 'other')\n+        self.assertIs(self.expressions.references_column(table, 'other'), True)\n+        self.assertIs(self.expressions.references_column(table, 'first_name'), False)\n+        self.assertIn(\n+            '%s.%s' % (self.editor.quote_name(table), self.editor.quote_name('other')),\n+            str(self.expressions),\n+        )\n+\n+    def test_str(self):\n+        table_name = self.editor.quote_name(Person._meta.db_table)\n+        expected_str = '%s.%s, %s.%s DESC, (UPPER(%s.%s))' % (\n+            table_name,\n+            self.editor.quote_name('first_name'),\n+            table_name,\n+            self.editor.quote_name('last_name'),\n+            table_name,\n+            self.editor.quote_name('last_name'),\n+        )\n+        self.assertEqual(str(self.expressions), expected_str)\ndiff --git a/tests/migrations/test_operations.py b/tests/migrations/test_operations.py\nindex 38ad6a2d1818..897808f75bbc 100644\n--- a/tests/migrations/test_operations.py\n+++ b/tests/migrations/test_operations.py\n@@ -1939,6 +1940,76 @@ def test_remove_index_state_forwards(self):\n         new_model = new_state.apps.get_model('test_rminsf', 'Pony')\n         self.assertIsNot(old_model, new_model)\n \n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_add_func_index(self):\n+        app_label = 'test_addfuncin'\n+        index_name = f'{app_label}_pony_abs_idx'\n+        table_name = f'{app_label}_pony'\n+        project_state = self.set_up_test_model(app_label)\n+        index = models.Index(Abs('weight'), name=index_name)\n+        operation = migrations.AddIndex('Pony', index)\n+        self.assertEqual(\n+            operation.describe(),\n+            'Create index test_addfuncin_pony_abs_idx on Abs(F(weight)) on model Pony',\n+        )\n+        self.assertEqual(\n+            operation.migration_name_fragment,\n+            'pony_test_addfuncin_pony_abs_idx',\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+        self.assertEqual(len(new_state.models[app_label, 'pony'].options['indexes']), 1)\n+        self.assertIndexNameNotExists(table_name, index_name)\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+        self.assertIndexNameExists(table_name, index_name)\n+        # Reversal.\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+        self.assertIndexNameNotExists(table_name, index_name)\n+        # Deconstruction.\n+        definition = operation.deconstruct()\n+        self.assertEqual(definition[0], 'AddIndex')\n+        self.assertEqual(definition[1], [])\n+        self.assertEqual(definition[2], {'model_name': 'Pony', 'index': index})\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_remove_func_index(self):\n+        app_label = 'test_rmfuncin'\n+        index_name = f'{app_label}_pony_abs_idx'\n+        table_name = f'{app_label}_pony'\n+        project_state = self.set_up_test_model(app_label, indexes=[\n+            models.Index(Abs('weight'), name=index_name),\n+        ])\n+        self.assertTableExists(table_name)\n+        self.assertIndexNameExists(table_name, index_name)\n+        operation = migrations.RemoveIndex('Pony', index_name)\n+        self.assertEqual(\n+            operation.describe(),\n+            'Remove index test_rmfuncin_pony_abs_idx from Pony',\n+        )\n+        self.assertEqual(\n+            operation.migration_name_fragment,\n+            'remove_pony_test_rmfuncin_pony_abs_idx',\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+        self.assertEqual(len(new_state.models[app_label, 'pony'].options['indexes']), 0)\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+        self.assertIndexNameNotExists(table_name, index_name)\n+        # Reversal.\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+        self.assertIndexNameExists(table_name, index_name)\n+        # Deconstruction.\n+        definition = operation.deconstruct()\n+        self.assertEqual(definition[0], 'RemoveIndex')\n+        self.assertEqual(definition[1], [])\n+        self.assertEqual(definition[2], {'model_name': 'Pony', 'name': index_name})\n+\n     def test_alter_field_with_index(self):\n         \"\"\"\n         Test AlterField operation with an index to ensure indexes created via\ndiff --git a/tests/postgres_tests/test_indexes.py b/tests/postgres_tests/test_indexes.py\nindex b9888f48431a..49646feb97c8 100644\n--- a/tests/postgres_tests/test_indexes.py\n+++ b/tests/postgres_tests/test_indexes.py\n@@ -455,3 +527,83 @@ def test_spgist_parameters(self):\n         with connection.schema_editor() as editor:\n             editor.remove_index(CharFieldModel, index)\n         self.assertNotIn(index_name, self.get_constraints(CharFieldModel._meta.db_table))\n+\n+    def test_op_class(self):\n+        index_name = 'test_op_class'\n+        index = Index(\n+            OpClass(Lower('field'), name='text_pattern_ops'),\n+            name=index_name,\n+        )\n+        with connection.schema_editor() as editor:\n+            editor.add_index(TextFieldModel, index)\n+        with editor.connection.cursor() as cursor:\n+            cursor.execute(self.get_opclass_query, [index_name])\n+            self.assertCountEqual(cursor.fetchall(), [('text_pattern_ops', index_name)])\n+\n+    def test_op_class_descending_collation(self):\n+        collation = connection.features.test_collations.get('non_default')\n+        if not collation:\n+            self.skipTest(\n+                'This backend does not support case-insensitive collations.'\n+            )\n+        index_name = 'test_op_class_descending_collation'\n+        index = Index(\n+            Collate(\n+                OpClass(Lower('field'), name='text_pattern_ops').desc(nulls_last=True),\n+                collation=collation,\n+            ),\n+            name=index_name,\n+        )\n+        with connection.schema_editor() as editor:\n+            editor.add_index(TextFieldModel, index)\n+            self.assertIn(\n+                'COLLATE %s' % editor.quote_name(collation),\n+                str(index.create_sql(TextFieldModel, editor)),\n+            )\n+        with editor.connection.cursor() as cursor:\n+            cursor.execute(self.get_opclass_query, [index_name])\n+            self.assertCountEqual(cursor.fetchall(), [('text_pattern_ops', index_name)])\n+        table = TextFieldModel._meta.db_table\n+        constraints = self.get_constraints(table)\n+        self.assertIn(index_name, constraints)\n+        self.assertEqual(constraints[index_name]['orders'], ['DESC'])\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(TextFieldModel, index)\n+        self.assertNotIn(index_name, self.get_constraints(table))\n+\n+    def test_op_class_descending_partial(self):\n+        index_name = 'test_op_class_descending_partial'\n+        index = Index(\n+            OpClass(Lower('field'), name='text_pattern_ops').desc(),\n+            name=index_name,\n+            condition=Q(field__contains='China'),\n+        )\n+        with connection.schema_editor() as editor:\n+            editor.add_index(TextFieldModel, index)\n+        with editor.connection.cursor() as cursor:\n+            cursor.execute(self.get_opclass_query, [index_name])\n+            self.assertCountEqual(cursor.fetchall(), [('text_pattern_ops', index_name)])\n+        constraints = self.get_constraints(TextFieldModel._meta.db_table)\n+        self.assertIn(index_name, constraints)\n+        self.assertEqual(constraints[index_name]['orders'], ['DESC'])\n+\n+    def test_op_class_descending_partial_tablespace(self):\n+        index_name = 'test_op_class_descending_partial_tablespace'\n+        index = Index(\n+            OpClass(Lower('field').desc(), name='text_pattern_ops'),\n+            name=index_name,\n+            condition=Q(field__contains='China'),\n+            db_tablespace='pg_default',\n+        )\n+        with connection.schema_editor() as editor:\n+            editor.add_index(TextFieldModel, index)\n+            self.assertIn(\n+                'TABLESPACE \"pg_default\" ',\n+                str(index.create_sql(TextFieldModel, editor))\n+            )\n+        with editor.connection.cursor() as cursor:\n+            cursor.execute(self.get_opclass_query, [index_name])\n+            self.assertCountEqual(cursor.fetchall(), [('text_pattern_ops', index_name)])\n+        constraints = self.get_constraints(TextFieldModel._meta.db_table)\n+        self.assertIn(index_name, constraints)\n+        self.assertEqual(constraints[index_name]['orders'], ['DESC'])\ndiff --git a/tests/schema/tests.py b/tests/schema/tests.py\nindex 7740065d311e..b994c252d969 100644\n--- a/tests/schema/tests.py\n+++ b/tests/schema/tests.py\n@@ -2481,6 +2488,366 @@ def test_text_field_with_db_index(self):\n         assertion = self.assertIn if connection.features.supports_index_on_text_field else self.assertNotIn\n         assertion('text_field', self.get_indexes(AuthorTextFieldWithIndex._meta.db_table))\n \n+    def _index_expressions_wrappers(self):\n+        index_expression = IndexExpression()\n+        index_expression.set_wrapper_classes(connection)\n+        return ', '.join([\n+            wrapper_cls.__qualname__ for wrapper_cls in index_expression.wrapper_classes\n+        ])\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index_multiple_wrapper_references(self):\n+        index = Index(OrderBy(F('name').desc(), descending=True), name='name')\n+        msg = (\n+            \"Multiple references to %s can't be used in an indexed expression.\"\n+            % self._index_expressions_wrappers()\n+        )\n+        with connection.schema_editor() as editor:\n+            with self.assertRaisesMessage(ValueError, msg):\n+                editor.add_index(Author, index)\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index_invalid_topmost_expressions(self):\n+        index = Index(Upper(F('name').desc()), name='name')\n+        msg = (\n+            '%s must be topmost expressions in an indexed expression.'\n+            % self._index_expressions_wrappers()\n+        )\n+        with connection.schema_editor() as editor:\n+            with self.assertRaisesMessage(ValueError, msg):\n+                editor.add_index(Author, index)\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        index = Index(Lower('name').desc(), name='func_lower_idx')\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            editor.add_index(Author, index)\n+            sql = index.create_sql(Author, editor)\n+        table = Author._meta.db_table\n+        if connection.features.supports_index_column_ordering:\n+            self.assertIndexOrder(table, index.name, ['DESC'])\n+        # SQL contains a database function.\n+        self.assertIs(sql.references_column(table, 'name'), True)\n+        self.assertIn('LOWER(%s)' % editor.quote_name('name'), str(sql))\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Author, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index_f(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Tag)\n+        index = Index('slug', F('title').desc(), name='func_f_idx')\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            editor.add_index(Tag, index)\n+            sql = index.create_sql(Tag, editor)\n+        table = Tag._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        if connection.features.supports_index_column_ordering:\n+            self.assertIndexOrder(Tag._meta.db_table, index.name, ['ASC', 'DESC'])\n+        # SQL contains columns.\n+        self.assertIs(sql.references_column(table, 'slug'), True)\n+        self.assertIs(sql.references_column(table, 'title'), True)\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Tag, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index_lookups(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        with register_lookup(CharField, Lower), register_lookup(IntegerField, Abs):\n+            index = Index(\n+                F('name__lower'),\n+                F('weight__abs'),\n+                name='func_lower_abs_lookup_idx',\n+            )\n+            # Add index.\n+            with connection.schema_editor() as editor:\n+                editor.add_index(Author, index)\n+                sql = index.create_sql(Author, editor)\n+        table = Author._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        # SQL contains columns.\n+        self.assertIs(sql.references_column(table, 'name'), True)\n+        self.assertIs(sql.references_column(table, 'weight'), True)\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Author, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_composite_func_index(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        index = Index(Lower('name'), Upper('name'), name='func_lower_upper_idx')\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            editor.add_index(Author, index)\n+            sql = index.create_sql(Author, editor)\n+        table = Author._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        # SQL contains database functions.\n+        self.assertIs(sql.references_column(table, 'name'), True)\n+        sql = str(sql)\n+        self.assertIn('LOWER(%s)' % editor.quote_name('name'), sql)\n+        self.assertIn('UPPER(%s)' % editor.quote_name('name'), sql)\n+        self.assertLess(sql.index('LOWER'), sql.index('UPPER'))\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Author, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_composite_func_index_field_and_expression(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+            editor.create_model(Book)\n+        index = Index(\n+            F('author').desc(),\n+            Lower('title').asc(),\n+            'pub_date',\n+            name='func_f_lower_field_idx',\n+        )\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            editor.add_index(Book, index)\n+            sql = index.create_sql(Book, editor)\n+        table = Book._meta.db_table\n+        constraints = self.get_constraints(table)\n+        if connection.features.supports_index_column_ordering:\n+            self.assertIndexOrder(table, index.name, ['DESC', 'ASC', 'ASC'])\n+        self.assertEqual(len(constraints[index.name]['columns']), 3)\n+        self.assertEqual(constraints[index.name]['columns'][2], 'pub_date')\n+        # SQL contains database functions and columns.\n+        self.assertIs(sql.references_column(table, 'author_id'), True)\n+        self.assertIs(sql.references_column(table, 'title'), True)\n+        self.assertIs(sql.references_column(table, 'pub_date'), True)\n+        self.assertIn('LOWER(%s)' % editor.quote_name('title'), str(sql))\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Book, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    @isolate_apps('schema')\n+    def test_func_index_f_decimalfield(self):\n+        class Node(Model):\n+            value = DecimalField(max_digits=5, decimal_places=2)\n+\n+            class Meta:\n+                app_label = 'schema'\n+\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Node)\n+        index = Index(F('value'), name='func_f_decimalfield_idx')\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            editor.add_index(Node, index)\n+            sql = index.create_sql(Node, editor)\n+        table = Node._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        self.assertIs(sql.references_column(table, 'value'), True)\n+        # SQL doesn't contain casting.\n+        self.assertNotIn('CAST', str(sql))\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Node, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index_cast(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        index = Index(Cast('weight', FloatField()), name='func_cast_idx')\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            editor.add_index(Author, index)\n+            sql = index.create_sql(Author, editor)\n+        table = Author._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        self.assertIs(sql.references_column(table, 'weight'), True)\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Author, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index_collate(self):\n+        collation = connection.features.test_collations.get('non_default')\n+        if not collation:\n+            self.skipTest(\n+                'This backend does not support case-insensitive collations.'\n+            )\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+            editor.create_model(BookWithSlug)\n+        index = Index(\n+            Collate(F('title'), collation=collation).desc(),\n+            Collate('slug', collation=collation),\n+            name='func_collate_idx',\n+        )\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            editor.add_index(BookWithSlug, index)\n+            sql = index.create_sql(BookWithSlug, editor)\n+        table = Book._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        if connection.features.supports_index_column_ordering:\n+            self.assertIndexOrder(table, index.name, ['DESC', 'ASC'])\n+        # SQL contains columns and a collation.\n+        self.assertIs(sql.references_column(table, 'title'), True)\n+        self.assertIs(sql.references_column(table, 'slug'), True)\n+        self.assertIn('COLLATE %s' % editor.quote_name(collation), str(sql))\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Book, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    @skipIfDBFeature('collate_as_index_expression')\n+    def test_func_index_collate_f_ordered(self):\n+        collation = connection.features.test_collations.get('non_default')\n+        if not collation:\n+            self.skipTest(\n+                'This backend does not support case-insensitive collations.'\n+            )\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        index = Index(\n+            Collate(F('name').desc(), collation=collation),\n+            name='func_collate_f_desc_idx',\n+        )\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            editor.add_index(Author, index)\n+            sql = index.create_sql(Author, editor)\n+        table = Author._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        if connection.features.supports_index_column_ordering:\n+            self.assertIndexOrder(table, index.name, ['DESC'])\n+        # SQL contains columns and a collation.\n+        self.assertIs(sql.references_column(table, 'name'), True)\n+        self.assertIn('COLLATE %s' % editor.quote_name(collation), str(sql))\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Author, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index_calc(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        index = Index(F('height') / (F('weight') + Value(5)), name='func_calc_idx')\n+        # Add index.\n+        with connection.schema_editor() as editor:\n+            editor.add_index(Author, index)\n+            sql = index.create_sql(Author, editor)\n+        table = Author._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        # SQL contains columns and expressions.\n+        self.assertIs(sql.references_column(table, 'height'), True)\n+        self.assertIs(sql.references_column(table, 'weight'), True)\n+        sql = str(sql)\n+        self.assertIs(\n+            sql.index(editor.quote_name('height')) <\n+            sql.index('/') <\n+            sql.index(editor.quote_name('weight')) <\n+            sql.index('+') <\n+            sql.index('5'),\n+            True,\n+        )\n+        # Remove index.\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(Author, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes', 'supports_json_field')\n+    @isolate_apps('schema')\n+    def test_func_index_json_key_transform(self):\n+        class JSONModel(Model):\n+            field = JSONField()\n+\n+            class Meta:\n+                app_label = 'schema'\n+\n+        with connection.schema_editor() as editor:\n+            editor.create_model(JSONModel)\n+        self.isolated_local_models = [JSONModel]\n+        index = Index('field__some_key', name='func_json_key_idx')\n+        with connection.schema_editor() as editor:\n+            editor.add_index(JSONModel, index)\n+            sql = index.create_sql(JSONModel, editor)\n+        table = JSONModel._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        self.assertIs(sql.references_column(table, 'field'), True)\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(JSONModel, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes', 'supports_json_field')\n+    @isolate_apps('schema')\n+    def test_func_index_json_key_transform_cast(self):\n+        class JSONModel(Model):\n+            field = JSONField()\n+\n+            class Meta:\n+                app_label = 'schema'\n+\n+        with connection.schema_editor() as editor:\n+            editor.create_model(JSONModel)\n+        self.isolated_local_models = [JSONModel]\n+        index = Index(\n+            Cast(KeyTextTransform('some_key', 'field'), IntegerField()),\n+            name='func_json_key_cast_idx',\n+        )\n+        with connection.schema_editor() as editor:\n+            editor.add_index(JSONModel, index)\n+            sql = index.create_sql(JSONModel, editor)\n+        table = JSONModel._meta.db_table\n+        self.assertIn(index.name, self.get_constraints(table))\n+        self.assertIs(sql.references_column(table, 'field'), True)\n+        with connection.schema_editor() as editor:\n+            editor.remove_index(JSONModel, index)\n+        self.assertNotIn(index.name, self.get_constraints(table))\n+\n+    @skipIfDBFeature('supports_expression_indexes')\n+    def test_func_index_unsupported(self):\n+        # Index is ignored on databases that don't support indexes on\n+        # expressions.\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        index = Index(F('name'), name='random_idx')\n+        with connection.schema_editor() as editor, self.assertNumQueries(0):\n+            self.assertIsNone(editor.add_index(Author, index))\n+            self.assertIsNone(editor.remove_index(Author, index))\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index_nonexistent_field(self):\n+        index = Index(Lower('nonexistent'), name='func_nonexistent_idx')\n+        msg = (\n+            \"Cannot resolve keyword 'nonexistent' into field. Choices are: \"\n+            \"height, id, name, uuid, weight\"\n+        )\n+        with self.assertRaisesMessage(FieldError, msg):\n+            with connection.schema_editor() as editor:\n+                editor.add_index(Author, index)\n+\n+    @skipUnlessDBFeature('supports_expression_indexes')\n+    def test_func_index_nondeterministic(self):\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        index = Index(Random(), name='func_random_idx')\n+        with connection.schema_editor() as editor:\n+            with self.assertRaises(DatabaseError):\n+                editor.add_index(Author, index)\n+\n     def test_primary_key(self):\n         \"\"\"\n         Tests altering of the primary key\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Good job on revising this PR! A couple of critical points to address: \n\nFirst, ensure the API for partial functional indexes is clear and supports the `Q` conditions properly. The inclusion of covering indexes could enhance functionality but remember that it's specific to Postgres 11. Also, consider implementing an option for automatic conversion of string fields to `F()` in the `__init__` method to improve usability.\n\nLastly, the `Index` ordering needs refactoring to prevent confusion with composite indexes. Let's keep pushing for clarity and compatibility across databases."
      }
    ],
    "codediff_length": 31972
  },
  {
    "index": 15,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/tests/bulk_create/tests.py b/tests/bulk_create/tests.py\nindex 7b86a2def54d..56e749f05def 100644\n--- a/tests/bulk_create/tests.py\n+++ b/tests/bulk_create/tests.py\n@@ -840,3 +840,172 @@ def test_update_conflicts_unique_fields_update_fields_db_column(self):\n                 {\"rank\": 2, \"name\": \"d\"},\n             ],\n         )\n+\n+    @skipUnlessDBFeature(\"supports_update_conflicts_with_condition\")\n+    def test_bulk_create_with_q_expression(self):\n+        initial_items = [\n+            UpsertConflict(number=1, rank=1, name=\"John\"),\n+            UpsertConflict(number=2, rank=3, name=\"Mary\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(initial_items)\n+        updated_items = [\n+            UpsertConflict(number=1, rank=3, name=\"Steve\"),\n+            UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(\n+            updated_items,\n+            update_conflicts=True,\n+            update_fields=[\"name\", \"rank\"],\n+            unique_fields=[\"number\"],\n+            condition=Q(rank__lt=3),\n+        )\n+        self.assertEqual(UpsertConflict.objects.get(number=1).name, \"Steve\")\n+        self.assertEqual(UpsertConflict.objects.get(number=2).name, \"Mary\")\n+\n+    @skipUnlessDBFeature(\"supports_update_conflicts_with_condition\")\n+    def test_bulk_create_with_f_expression(self):\n+        initial_items = [\n+            UpsertConflict(number=1, rank=1, name=\"John\"),\n+            UpsertConflict(number=2, rank=4, name=\"Mary\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(initial_items)\n+        updated_items = [\n+            UpsertConflict(number=1, rank=3, name=\"Steve\"),\n+            UpsertConflict(number=2, rank=4, name=\"Olivia\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(\n+            updated_items,\n+            update_conflicts=True,\n+            update_fields=[\"name\", \"rank\"],\n+            unique_fields=[\"number\"],\n+            condition=Q(rank__lt=F(\"number\") + 1),\n+        )\n+        self.assertEqual(UpsertConflict.objects.get(number=1).name, \"Steve\")\n+        self.assertEqual(UpsertConflict.objects.get(number=2).name, \"Mary\")\n+\n+    @skipUnlessDBFeature(\"supports_update_conflicts_with_condition\")\n+    def test_bulk_create_with_complex_condition(self):\n+        initial_items = [\n+            UpsertConflict(number=1, rank=3, name=\"John\"),\n+            UpsertConflict(number=2, rank=2, name=\"Mary\"),\n+            UpsertConflict(number=3, rank=6, name=\"Alice\"),\n+            UpsertConflict(number=4, rank=7, name=\"Bob\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(initial_items)\n+        updated_items = [\n+            UpsertConflict(number=1, rank=1, name=\"Steve\"),\n+            UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n+            UpsertConflict(number=3, rank=3, name=\"Emma\"),\n+            UpsertConflict(number=4, rank=4, name=\"Lucas\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(\n+            updated_items,\n+            update_conflicts=True,\n+            update_fields=[\"name\", \"rank\"],\n+            unique_fields=[\"number\"],\n+            condition=Q(rank__gt=F(\"number\") + 1) & ~Q(name__startswith=\"A\")\n+            | Q(number__gt=3),\n+        )\n+        self.assertEqual(UpsertConflict.objects.get(number=1).name, \"Steve\")\n+        self.assertEqual(UpsertConflict.objects.get(number=2).name, \"Mary\")\n+        self.assertEqual(UpsertConflict.objects.get(number=3).name, \"Alice\")\n+        self.assertEqual(UpsertConflict.objects.get(number=4).name, \"Lucas\")\n+\n+    @skipUnlessDBFeature(\n+        \"supports_update_conflicts_with_condition\",\n+        \"supports_excluded_expression\",\n+    )\n+    def test_bulk_create_with_excluded_expression(self):\n+        initial_items = [\n+            UpsertConflict(number=1, rank=3, name=\"John\"),\n+            UpsertConflict(number=2, rank=2, name=\"Mary\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(initial_items)\n+        updated_items = [\n+            UpsertConflict(number=1, rank=4, name=\"Steve\"),\n+            UpsertConflict(number=2, rank=1, name=\"Olivia\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(\n+            updated_items,\n+            update_conflicts=True,\n+            update_fields=[\"name\", \"rank\"],\n+            unique_fields=[\"number\"],\n+            condition=Q(rank__lt=Excluded(\"rank\")),\n+        )\n+        self.assertEqual(UpsertConflict.objects.get(number=1).name, \"Steve\")\n+        self.assertEqual(UpsertConflict.objects.get(number=2).name, \"Mary\")\n+\n+    @skipUnlessDBFeature(\n+        \"supports_update_conflicts_with_condition\",\n+        \"supports_excluded_expression\",\n+    )\n+    def test_bulk_create_with_complex_excluded_expression(self):\n+        initial_items = [\n+            UpsertConflict(number=1, rank=3, name=\"John\"),\n+            UpsertConflict(number=2, rank=5, name=\"Mary\"),\n+            UpsertConflict(number=3, rank=4, name=\"EmmaAlice\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(initial_items)\n+        updated_items = [\n+            UpsertConflict(number=1, rank=5, name=\"Steve\"),\n+            UpsertConflict(number=2, rank=4, name=\"Olivia\"),\n+            UpsertConflict(number=3, rank=6, name=\"Emma\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(\n+            updated_items,\n+            update_conflicts=True,\n+            update_fields=[\"name\", \"rank\"],\n+            unique_fields=[\"number\"],\n+            condition=(\n+                Q(rank__gt=F(\"number\") + 1) & Q(rank__lt=Excluded(\"rank\"))\n+                | Q(name__startswith=Excluded(\"name\"))\n+            ),\n+        )\n+        self.assertEqual(UpsertConflict.objects.get(number=1).name, \"Steve\")\n+        self.assertEqual(UpsertConflict.objects.get(number=2).name, \"Mary\")\n+        self.assertEqual(UpsertConflict.objects.get(number=3).name, \"Emma\")\n+\n+    @skipUnlessDBFeature(\n+        \"supports_update_conflicts_with_condition\",\n+        \"supports_excluded_expression\",\n+    )\n+    def test_bulk_create_with_arithmetic_excluded_expression(self):\n+        initial_items = [\n+            UpsertConflict(number=1, rank=5, name=\"John\"),\n+            UpsertConflict(number=2, rank=2, name=\"Mary\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(initial_items)\n+        updated_items = [\n+            UpsertConflict(number=1, rank=3, name=\"Steve\"),\n+            UpsertConflict(number=2, rank=4, name=\"Olivia\"),\n+        ]\n+        UpsertConflict.objects.bulk_create(\n+            updated_items,\n+            update_conflicts=True,\n+            update_fields=[\"name\", \"rank\"],\n+            unique_fields=[\"number\"],\n+            condition=Q(rank__gt=Excluded(\"rank\") + 1),\n+        )\n+        self.assertEqual(UpsertConflict.objects.get(number=1).name, \"Steve\")\n+        self.assertEqual(UpsertConflict.objects.get(number=2).name, \"Mary\")\n+\n+    @skipUnlessDBFeature(\n+        \"supports_update_conflicts\", \"supports_update_conflicts_with_condition\"\n+    )\n+    @skipIfDBFeature(\"supports_excluded_expression\")\n+    def test_excluded_not_supported(self):\n+        bulk_create_kwargs = {\n+            \"update_conflicts\": True,\n+            \"update_fields\": [\"name\", \"rank\"],\n+            \"condition\": Q(rank__lt=Excluded(\"rank\")),\n+        }\n+        if connection.features.supports_update_conflicts_with_target:\n+            bulk_create_kwargs[\"unique_fields\"] = [\"number\"]\n+        msg = (\n+            f\"{connection.vendor} doesn't support partial updating rows on \"\n+            \"conflicts during INSERT\"\n+        )\n+        with self.assertRaisesMessage(NotSupportedError, msg):\n+            UpsertConflict.objects.bulk_create(\n+                [UpsertConflict(number=1, rank=1, name=\"Steve\")], **bulk_create_kwargs\n+            )\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Thanks for the work on this feature; its valuable. However, the current approach still relies too heavily on backend-specific SQL, which raises security risks, particularly around SQL injection. The primary focus should be enhancing the `where_clause` support for passing expressions rather than hardcoding for `F` and `Q`. Addressing PostgreSQLs `EXCLUDED` is also keyconsider subclassing `F` to manage this better. Lastly, ensure all comments in the previous reviews are addressed before moving forward."
      }
    ],
    "codediff_length": 7694
  },
  {
    "index": 16,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/docs/ref/models/querysets.txt b/docs/ref/models/querysets.txt\nindex c7ab413f5337..e46e57c6643c 100644\n--- a/docs/ref/models/querysets.txt\n+++ b/docs/ref/models/querysets.txt\n@@ -264,6 +264,78 @@ control the name of the annotation::\n For an in-depth discussion of aggregation, see :doc:`the topic guide on\n Aggregation </topics/db/aggregation>`.\n \n+\n+.. currentmodule:: django.db.models.query\n+\n+.. class:: FilteredRelation(relation_name, condition=Q())\n+\n+    .. versionadded:: 2.0\n+    .. attribute:: FilteredRelation.relation_name\n+\n+        The name of the field on which you'd like to filter the relation.\n+    .. attribute:: FilteredRelation.condition\n+\n+        A :class:`~django.db.models.Q` object to control the filtering.\n+\n+Annotate supports also a new kind of argument that is an instance of\n+:class:`.FilteredRelation`.\n+\n+This kind of parameter allows to pass filtering options inside the the ``ON``\n+clause when a ``JOIN`` is performed.\n+It does not act on the default relationship but on the given ``alias`` given\n+as name of the parameter (``pizzas_vegetarian`` in example below).\n+\n+Example::\n+\n+    >>> from django.db.models import FilteredRelation\n+    >>> Restaurant.objects.annotate(\n+    ...    pizzas_vegetarian=FilteredRelation(\n+    ...        'pizzas', condition=Q(pizzas__vegetarian=True),\n+    ...    )).filter(pizzas_vegetarian__name__icontains='Mozzarella')\n+\n+This query will join the model Restaurant only to Pizzas that have vegetarian\n+flag to True. This results into a performance improvement, noticeable if the\n+join is performed through a large number of rows. Because the filtering from\n+the ``WHERE`` clause will only operate on a subset of Pizzas (the vegetarian\n+ones).\n+\n+.. admonition:: You need to be aware of some limitations\n+\n+    The :class:`.FilteredRelation` can not contain\n+    conditions that span over deep relations.\n+    The condition can only apply on fields that satisfy the constraint\n+    ``Field.is_relation is False``.\n+\n+    Example::\n+\n+        >>> Restaurant.objects.annotate(\n+        ...    pizzas_with_toppings_startswith_n=FilteredRelation(\n+        ...        'pizzas__toppings',\n+        ...        condition=Q(pizzas__toppings__name__startswith='n'),\n+        ...     )).filter(pizzas_with_toppings_startswith_n__isnull=False)\n+\n+    Will raise ``ValueError`` \"Filtered relation\n+    'pizzas_with_toppings_startswith_n' cannot operate on foreign key\n+    'pizzas__toppings'\".\n+\n+.. admonition:: :meth:`~.QuerySet.prefetch_related()` is not supported\n+\n+    If you try to use :meth:`~.QuerySet.prefetch_related()` with a\n+    FilteredRelation alias, ``ValueError`` will be raised, with the following\n+    message::\n+\n+        \"prefetch_related() is not supported with filtered relation 'alias'\"\n+\n+\n+.. admonition:: :meth:`~.QuerySet.only()` is not supported\n+\n+    If you try to use :meth:`~.QuerySet.only()` with a FilteredRelation alias,\n+    ``ValueError`` will be raised, with the following message::\n+\n+        \"only() is not supported with filtered relation 'alias'\"\n+\n+.. currentmodule:: django.db.models.query.QuerySet\n+\n ``order_by()``\n ~~~~~~~~~~~~~~\n \ndiff --git a/tests/filtered_relation/models.py b/tests/filtered_relation/models.py\nnew file mode 100644\nindex 000000000000..c4233a69f8a3\n--- /dev/null\n+++ b/tests/filtered_relation/models.py\n@@ -0,0 +1,100 @@\n+from django.db import models\n+\n+\n+class Author(models.Model):\n+    name = models.CharField(max_length=50, unique=True)\n+    favourite_books = models.ManyToManyField(\n+        'Book',\n+        related_name='preferred_by_authors',\n+        related_query_name='preferred_by_authors',\n+    )\n+\n+    def __str__(self):\n+        return self.name\n+\n+\n+class Editor(models.Model):\n+    name = models.CharField(max_length=255)\n+\n+    def __str__(self):\n+        return self.name\n+\n+\n+class Book(models.Model):\n+    AVAILABLE = 'available'\n+    RESERVED = 'reserved'\n+    RENTED = 'rented'\n+    STATES = (\n+        (AVAILABLE, 'Available'),\n+        (RESERVED, 'reserved'),\n+        (RENTED, 'Rented'),\n+    )\n+    title = models.CharField(max_length=255)\n+    author = models.ForeignKey(\n+        Author,\n+        models.CASCADE,\n+        related_name='books',\n+        related_query_name='book',\n+    )\n+    editor = models.ForeignKey(Editor, models.CASCADE)\n+    state = models.CharField(max_length=9, choices=STATES, default=AVAILABLE)\n+\n+    def __str__(self):\n+        return self.title\n+\n+\n+class Borrower(models.Model):\n+    name = models.CharField(max_length=50, unique=True)\n+\n+    def __str__(self):\n+        return self.name\n+\n+\n+class Reservation(models.Model):\n+    NEW = 'new'\n+    STOPPED = 'stopped'\n+    STATES = (\n+        (NEW, 'New'),\n+        (STOPPED, 'Stopped'),\n+    )\n+    borrower = models.ForeignKey(\n+        Borrower,\n+        models.CASCADE,\n+        related_name='reservations',\n+        related_query_name='reservation',\n+    )\n+    book = models.ForeignKey(\n+        Book,\n+        models.CASCADE,\n+        related_name='reservations',\n+        related_query_name='reservation',\n+    )\n+    state = models.CharField(max_length=7, choices=STATES, default=NEW)\n+\n+    def __str__(self):\n+        return '-'.join((self.book.name, self.borrower.name, self.state))\n+\n+\n+class RentalSession(models.Model):\n+    NEW = 'new'\n+    STOPPED = 'stopped'\n+    STATES = (\n+        (NEW, 'New'),\n+        (STOPPED, 'Stopped'),\n+    )\n+    borrower = models.ForeignKey(\n+        Borrower,\n+        models.CASCADE,\n+        related_name='rental_sessions',\n+        related_query_name='rental_session',\n+    )\n+    book = models.ForeignKey(\n+        Book,\n+        models.CASCADE,\n+        related_name='rental_sessions',\n+        related_query_name='rental_session',\n+    )\n+    state = models.CharField(max_length=7, choices=STATES, default=NEW)\n+\n+    def __str__(self):\n+        return '-'.join((self.book.name, self.borrower.name, self.state))\ndiff --git a/tests/filtered_relation/tests.py b/tests/filtered_relation/tests.py\nnew file mode 100644\nindex 000000000000..9a11ceeab086\n--- /dev/null\n+++ b/tests/filtered_relation/tests.py\n@@ -0,0 +1,367 @@\n+from django.db import connection\n+from django.db.models import Case, Count, F, FilteredRelation, Q, When\n+from django.test import TestCase\n+from django.test.testcases import skipUnlessDBFeature\n+\n+from .models import Author, Book, Borrower, Editor, RentalSession, Reservation\n+\n+\n+class FilteredRelationTests(TestCase):\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.author1 = Author.objects.create(name='Alice')\n+        cls.author2 = Author.objects.create(name='Jane')\n+\n+        cls.editor_a = Editor.objects.create(name='a')\n+        cls.editor_b = Editor.objects.create(name='b')\n+\n+        cls.book1 = Book.objects.create(\n+            title='Poem by Alice',\n+            editor=cls.editor_a,\n+            author=cls.author1,\n+        )\n+\n+        cls.book2 = Book.objects.create(\n+            title='The book by Jane A',\n+            editor=cls.editor_b,\n+            author=cls.author2,\n+        )\n+\n+        cls.book3 = Book.objects.create(\n+            title='The book by Jane B',\n+            editor=cls.editor_b,\n+            author=cls.author2,\n+        )\n+\n+        cls.book4 = Book.objects.create(\n+            title='The book by Alice',\n+            editor=cls.editor_a,\n+            author=cls.author1,\n+        )\n+        cls.author1.favourite_books.add(cls.book2)\n+        cls.author1.favourite_books.add(cls.book3)\n+\n+    def test_filtered_relation_select_related(self):\n+        qs = Author.objects.annotate(\n+            book_join=FilteredRelation('book', condition=Q())\n+        ).select_related('book_join__editor').order_by('pk', 'book_join__pk')\n+        with self.assertNumQueries(1):\n+            expected = [\n+                (self.author1, self.book1, self.editor_a, self.author1),\n+                (self.author1, self.book4, self.editor_a, self.author1),\n+                (self.author2, self.book2, self.editor_b, self.author2),\n+                (self.author2, self.book3, self.editor_b, self.author2),\n+            ]\n+            self.assertQuerysetEqual(\n+                qs, expected, lambda x: (x, x.book_join, x.book_join.editor, x.book_join.author)\n+            )\n+\n+    def test_filtered_relation_select_related_foreign_key(self):\n+        # this doesn't make much sense, but let's have it anyways...\n+        qs = Book.objects.annotate(\n+            author_join=FilteredRelation('author', condition=Q())\n+        ).select_related('author_join').order_by('pk')\n+        with self.assertNumQueries(1):\n+            expected = [\n+                (self.book1, self.author1),\n+                (self.book2, self.author2),\n+                (self.book3, self.author2),\n+                (self.book4, self.author1),\n+            ]\n+            self.assertQuerysetEqual(\n+                qs, expected, lambda x: (x, x.author_join)\n+            )\n+\n+    def test_filtered_relation_wo_join(self):\n+        self.assertSequenceEqual(\n+            Author.objects.annotate(\n+                book_alice=FilteredRelation(\n+                    'book', condition=Q(book__title__iexact='poem by alice'),\n+                )), [self.author1, self.author2])\n+\n+    def test_filered_relation_with_join(self):\n+        self.assertQuerysetEqual(\n+            Author.objects.annotate(\n+                book_alice=FilteredRelation(\n+                    'book', condition=Q(book__title__iexact='poem by alice'),\n+                )).filter(book_alice__isnull=False), [\"<Author: Alice>\"])\n+\n+    def test_filtered_relation_alias_mapping_other(self):\n+        queryset = Author.objects.annotate(\n+            book_alice=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='poem by alice'),\n+            )).filter(book_alice__isnull=False)\n+        self.assertIn(\n+            'INNER JOIN {} book_alice ON'.format(\n+                connection.ops.quote_name('filtered_relation_book'),\n+            ),\n+            str(queryset.query))\n+\n+    def test_filtered_relation_with_multiple_filter(self):\n+        self.assertQuerysetEqual(\n+            Author.objects.annotate(\n+                book_editor_a=FilteredRelation(\n+                    'book', condition=Q(book__title__icontains='book',\n+                                        book__editor_id=self.editor_a.pk),\n+                )).filter(book_editor_a__isnull=False), [\"<Author: Alice>\"])\n+\n+    def test_filtered_relation_multiple_times(self):\n+        self.assertQuerysetEqual(\n+            Author.objects.annotate(\n+                book_title_alice=FilteredRelation(\n+                    'book', condition=Q(book__title__icontains='alice'),\n+                )).filter(book_title_alice__isnull=False).filter(book_title_alice__isnull=False).distinct(),\n+            [\"<Author: Alice>\"])\n+\n+    def test_exclude_relation_with_join(self):\n+        self.assertQuerysetEqual(\n+            Author.objects.annotate(\n+                book_alice=FilteredRelation(\n+                    'book', condition=~Q(book__title__icontains='alice'),\n+                )).filter(book_alice__isnull=False).distinct(),\n+            [\"<Author: Jane>\"])\n+\n+    def test_filtered_relation_with_m2m(self):\n+        qs = Author.objects.annotate(\n+            favourite_books_written_by_jane=FilteredRelation(\n+                'favourite_books', condition=Q(favourite_books__in=[self.book2]),\n+            )).filter(favourite_books_written_by_jane__isnull=False)\n+        self.assertQuerysetEqual(qs, [\"<Author: Alice>\"])\n+\n+    def test_filtered_relation_with_m2m_deep(self):\n+        qs = Author.objects.annotate(\n+            favourite_books_written_by_jane=FilteredRelation(\n+                'favourite_books', condition=Q(favourite_books__author=self.author2),\n+            )).filter(favourite_books_written_by_jane__title='The book by Jane B')\n+        self.assertQuerysetEqual(qs, [\"<Author: Alice>\"])\n+\n+    def test_filtered_relation_with_m2m_multijoin(self):\n+        qs = Author.objects.annotate(\n+            favourite_books_written_by_jane=FilteredRelation(\n+                'favourite_books', condition=Q(favourite_books__author=self.author2),\n+            )).filter(favourite_books_written_by_jane__editor__name='b').distinct()\n+        self.assertQuerysetEqual(qs, [\"<Author: Alice>\"])\n+\n+    def test_filtered_relation_values_list(self):\n+        self.assertSequenceEqual(\n+            Author.objects.annotate(\n+                book_alice=FilteredRelation(\n+                    'book', condition=Q(book__title__iexact='poem by alice'),\n+                )).filter(book_alice__isnull=False).values_list(\n+                    'book_alice__title', flat=True),\n+            [\"Poem by Alice\"])\n+\n+    def test_filtered_relation_values(self):\n+        self.assertSequenceEqual(Author.objects.annotate(\n+            book_alice=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='poem by alice'),\n+            )).filter(book_alice__isnull=False).values(),\n+            [{'id': 1, 'name': 'Alice'}])\n+\n+    def test_filtered_relation_extra(self):\n+        self.assertSequenceEqual(Author.objects.annotate(\n+            book_alice=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='poem by alice'),\n+            )).filter(book_alice__isnull=False).extra(\n+                where=[\"1 = 1\"]),\n+            [self.author1])\n+\n+    @skipUnlessDBFeature('supports_select_union')\n+    def test_filtered_relation_union(self):\n+        qs1 = Author.objects.annotate(\n+            book_alice=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='poem by alice'),\n+            )).filter(book_alice__isnull=False)\n+        qs2 = Author.objects.annotate(\n+            book_jane=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='the book by jane a'),\n+            )).filter(book_jane__isnull=False)\n+        self.assertSequenceEqual(qs1.union(qs2), [self.author1, self.author2])\n+\n+    @skipUnlessDBFeature('supports_select_intersection')\n+    def test_filtered_relation_intersection(self):\n+        qs1 = Author.objects.annotate(\n+            book_alice=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='poem by alice'),\n+            )).filter(book_alice__isnull=False)\n+        qs2 = Author.objects.annotate(\n+            book_jane=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='the book by jane a'),\n+            )).filter(book_jane__isnull=False)\n+        self.assertQuerysetEqual(qs1.intersection(qs2), [])\n+\n+    @skipUnlessDBFeature('supports_select_difference')\n+    def test_filtered_relation_difference(self):\n+        qs1 = Author.objects.annotate(\n+            book_alice=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='poem by alice'),\n+            )).filter(book_alice__isnull=False)\n+        qs2 = Author.objects.annotate(\n+            book_jane=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='the book by jane a'),\n+            )).filter(book_jane__isnull=False)\n+        self.assertSequenceEqual(qs1.difference(qs2), [self.author1])\n+\n+    def test_filtered_relation_select_for_update(self):\n+        self.assertQuerysetEqual(Author.objects.annotate(\n+            book_jane=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='the book by jane a'),\n+            )).filter(book_jane__isnull=False).select_for_update(),\n+            ['<Author: Jane>'])\n+\n+    def test_filtered_relation_defer(self):\n+        # One query for the list, and one query to fetch the\n+        # deferred title.\n+        with self.assertNumQueries(2):\n+            self.assertQuerysetEqual(Author.objects.annotate(\n+                book_alice=FilteredRelation(\n+                    'book', condition=Q(book__title__iexact='poem by alice'),\n+                )).filter(book_alice__isnull=False).select_related(\n+                    'book_alice').defer('book_alice__title'),\n+                [\"Poem by Alice\"], lambda author: author.book_alice.title)\n+\n+    def test_filtered_relation_only_not_supported(self):\n+        msg = \"only() is not supported with filtered relation 'book_alice'\"\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Author.objects.annotate(\n+                book_alice=FilteredRelation(\n+                    'book', condition=Q(book__title__iexact='poem by alice'),\n+                )).filter(book_alice__isnull=False).select_related('book_alice').only(\n+                    'book_alice__state')\n+\n+    def test_filtered_relation_as_subquery(self):\n+        inner_qs = Author.objects.annotate(\n+            book_alice=FilteredRelation(\n+                'book', condition=Q(book__title__iexact='poem by alice'),\n+            )).filter(book_alice__isnull=False)\n+        qs = Author.objects.filter(id__in=inner_qs)\n+        self.assertQuerysetEqual(qs, [\"<Author: Alice>\"])\n+\n+    def test_filtered_relation_with_foreign_key_error(self):\n+        msg = (\"Filtered relation 'alice_favourite_books' cannot operate on foreign key\"\n+               \" 'author__favourite_books__author'.\")\n+        with self.assertRaisesMessage(ValueError, msg):\n+            list(\n+                Book.objects.annotate(\n+                    alice_favourite_books=FilteredRelation(\n+                        'author__favourite_books',\n+                        condition=Q(author__favourite_books__author=self.author1),\n+                    )).filter(alice_favourite_books__title__icontains='poem')\n+            )\n+\n+    def test_filtered_relation_with_foreign_key_on_condition_error(self):\n+        msg = (\"Filtered relation 'book_edited_by_b' cannot operate on foreign key\"\n+               \" 'book__editor__name__icontains'.\")\n+        with self.assertRaisesMessage(ValueError, msg):\n+            list(\n+                Author.objects.annotate(\n+                    book_edited_by_b=FilteredRelation(\n+                        'book', condition=Q(book__editor__name__icontains='b'),\n+                    )).filter(book_edited_by_b__isnull=False)\n+            )\n+\n+    def test_filtered_relation_with_empty_relation_name_error(self):\n+        with self.assertRaisesMessage(ValueError, 'relation_name cannot be empty'):\n+            FilteredRelation('', condition=Q(blank=''))\n+\n+    def test_filtered_relation_with_prefetch_related(self):\n+        msg = (\"prefetch_related() is not supported with filtered relation 'book_title_contains_b'\")\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Author.objects.annotate(\n+                book_title_contains_b=FilteredRelation(\n+                    'book',\n+                    condition=Q(book__title__icontains='b'),\n+                )).filter(\n+                    book_title_contains_b__isnull=False\n+            ).prefetch_related('book_title_contains_b')\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Author.objects.annotate(\n+                book_title_contains_b=FilteredRelation(\n+                    'book',\n+                    condition=Q(book__title__icontains='b'),\n+                )).filter(\n+                    book_title_contains_b__isnull=False\n+            ).prefetch_related('book_title_contains_b__editor')\n+\n+\n+class FilteredRelationWithAggregationTests(TestCase):\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.author1 = Author.objects.create(name='Alice')\n+        cls.editor_a = Editor.objects.create(name='a')\n+        cls.book1 = Book.objects.create(\n+            title='Poem by Alice',\n+            editor=cls.editor_a,\n+            author=cls.author1,\n+        )\n+        cls.borrower1 = Borrower.objects.create(name='Jenny')\n+        cls.borrower2 = Borrower.objects.create(name='Kevin')\n+        # borrower 1 reserve, rent and return book1\n+        Reservation.objects.create(\n+            borrower=cls.borrower1,\n+            book=cls.book1,\n+            state=Reservation.STOPPED,\n+        )\n+        RentalSession.objects.create(\n+            borrower=cls.borrower1,\n+            book=cls.book1,\n+            state=RentalSession.STOPPED,\n+        )\n+        # borrower2 reserve, rent and return book1\n+        Reservation.objects.create(\n+            borrower=cls.borrower2,\n+            book=cls.book1,\n+            state=Reservation.STOPPED,\n+        )\n+        RentalSession.objects.create(\n+            borrower=cls.borrower2,\n+            book=cls.book1,\n+            state=RentalSession.STOPPED,\n+        )\n+\n+    def test_filtered_relation_enforce_aggregation_correctness(self):\n+        \"\"\"\n+        Test case that shows how filtered_relation not only improves\n+        performance, but also ensure correctness of results when aggregation\n+        and multiple LEFT JOIN are involved.\n+\n+        Use Case:\n+\n+            Books can be reserved then rented by a borrower.\n+            Each reservation and rental_session are recorded with\n+            Reservation and RentalSession models.\n+            Every time a reservation or a rental session is over,\n+            their state change to 'stopped'.\n+\n+        Goal: Count number of books that are either currently reserved or\n+              rented by borrower1 or available.\n+        \"\"\"\n+        qs = Book.objects.annotate(\n+            is_reserved_or_rented_by=Case(\n+                When(reservation__state=Reservation.NEW, then=F('reservation__borrower__pk')),\n+                When(rental_session__state=RentalSession.NEW, then=F('rental_session__borrower__pk')),\n+                default=None)\n+        ).filter(\n+            Q(is_reserved_or_rented_by=self.borrower1.pk) | Q(state=Book.AVAILABLE)\n+        ).distinct().order_by()  # disable implicit grouping\n+        self.assertEqual(qs.count(), 1)\n+        # If count is equal to 1 we expect also the same aggregation to return the same result\n+        # but it fails by returning 4.\n+        self.assertQuerysetEqual(qs.annotate(total=Count('pk')).values('total'), [\"{'total': 4}\"])\n+\n+        qs = Book.objects.annotate(\n+            active_reservations=FilteredRelation(\n+                'reservation', condition=Q(reservation__state=Reservation.NEW,\n+                                           reservation__borrower=self.borrower1),\n+            )).annotate(\n+                active_rental_sessions=FilteredRelation(\n+                    'rental_session', condition=Q(rental_session__state=RentalSession.NEW,\n+                                                  rental_session__borrower=self.borrower1),\n+                )).filter((Q(active_reservations__isnull=False) |\n+                           Q(active_rental_sessions__isnull=False)) |\n+                          Q(state=Book.AVAILABLE)).distinct().order_by()  # disable implicit grouping\n+        self.assertEqual(qs.count(), 1)\n+        # Thanks to FilteredRelation the aggregation is now correct\n+        self.assertQuerysetEqual(qs.annotate(total=Count('pk')).values('total'), [\"{'total': 1}\"])\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The suggestion to allow specifying join types in `filtered_relation` could enhance usability for common scenarios. However, we should aim for simplicity. Defaulting to LEFT joins makes sense, given the potential complexity with INNER joins via filters. Testing is crucial, especially for NOT NULL cases and split exclude behavior. Consider simplifying the API to promote a single join per relation. It might be best to leave `select_related` integration for future discussion once `filtered_relation` stabilizes. Let's ensure we cover necessary tests and document any limitations clearly."
      }
    ],
    "codediff_length": 22935
  },
  {
    "index": 17,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/db/models/query.py b/django/db/models/query.py\nindex 86b1631f6707..187441692868 100644\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -466,7 +466,69 @@ def _prepare_for_bulk_create(self, objs):\n                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n             obj._prepare_related_fields_for_save(operation_name='bulk_create')\n \n-    def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):\n+    def _check_bulk_create_options(self, ignore_conflicts, update_conflicts, update_fields, unique_fields):\n+        if ignore_conflicts and update_conflicts:\n+            raise ValueError(\n+                'ignore_conflicts and update_conflicts are mutually exclusive.'\n+            )\n+        db_features = connections[self.db].features\n+        if ignore_conflicts:\n+            if not db_features.supports_ignore_conflicts:\n+                raise NotSupportedError(\n+                    'This database backend does not support ignoring conflicts.'\n+                )\n+            return OnConflict.IGNORE\n+        elif update_conflicts:\n+            if not db_features.supports_update_conflicts:\n+                raise NotSupportedError(\n+                    'This database backend does not support updating conflicts.'\n+                )\n+            if not update_fields:\n+                raise ValueError(\n+                    'Fields that will be updated when a row insertion fails '\n+                    'on conflicts must be provided.'\n+                )\n+            if unique_fields and not db_features.supports_update_conflicts_with_target:\n+                raise NotSupportedError(\n+                    'This database backend does not support updating '\n+                    'conflicts with specifying unique fields that can trigger '\n+                    'the upsert.'\n+                )\n+            if not unique_fields and db_features.supports_update_conflicts_with_target:\n+                raise ValueError(\n+                    'Unique fields that can trigger the upsert must be '\n+                    'provided.'\n+                )\n+            # Updating primary keys and non-concrete fields is forbidden.\n+            update_fields = [self.model._meta.get_field(name) for name in update_fields]\n+            if any(not f.concrete or f.many_to_many for f in update_fields):\n+                raise ValueError(\n+                    'bulk_create() can only be used with concrete fields in '\n+                    'update_fields.'\n+                )\n+            if any(f.primary_key for f in update_fields):\n+                raise ValueError(\n+                    'bulk_create() cannot be used with primary keys in '\n+                    'update_fields.'\n+                )\n+            if unique_fields:\n+                # Primary key is allowed in unique_fields.\n+                unique_fields = [\n+                    self.model._meta.get_field(name)\n+                    for name in unique_fields if name != 'pk'\n+                ]\n+                if any(not f.concrete or f.many_to_many for f in unique_fields):\n+                    raise ValueError(\n+                        'bulk_create() can only be used with concrete fields '\n+                        'in unique_fields.'\n+                    )\n+            return OnConflict.UPDATE\n+        return None\n+\n+    def bulk_create(\n+        self, objs, batch_size=None, ignore_conflicts=False,\n+        update_conflicts=False, update_fields=None, unique_fields=None,\n+    ):\n         \"\"\"\n         Insert each of the instances into the database. Do *not* call\n         save() on each of the instances, do not send any pre/post_save\n\ndiff --git a/tests/bulk_create/tests.py b/tests/bulk_create/tests.py\nindex 2ee54c382fcf..7e5ff323804d 100644\n--- a/tests/bulk_create/tests.py\n+++ b/tests/bulk_create/tests.py\n@@ -352,3 +357,276 @@ def test_invalid_batch_size_exception(self):\n         msg = 'Batch size must be a positive integer.'\n         with self.assertRaisesMessage(ValueError, msg):\n             Country.objects.bulk_create([], batch_size=-1)\n+\n+    @skipIfDBFeature('supports_update_conflicts')\n+    def test_update_conflicts_unsupported(self):\n+        msg = 'This database backend does not support updating conflicts.'\n+        with self.assertRaisesMessage(NotSupportedError, msg):\n+            Country.objects.bulk_create(self.data, update_conflicts=True)\n+\n+    @skipUnlessDBFeature('supports_ignore_conflicts', 'supports_update_conflicts')\n+    def test_ignore_update_conflicts_exclusive(self):\n+        msg = 'ignore_conflicts and update_conflicts are mutually exclusive'\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Country.objects.bulk_create(\n+                self.data,\n+                ignore_conflicts=True,\n+                update_conflicts=True,\n+            )\n+\n+    @skipUnlessDBFeature('supports_update_conflicts')\n+    def test_update_conflicts_no_update_fields(self):\n+        msg = (\n+            'Fields that will be updated when a row insertion fails on '\n+            'conflicts must be provided.'\n+        )\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Country.objects.bulk_create(self.data, update_conflicts=True)\n+\n+    @skipUnlessDBFeature('supports_update_conflicts')\n+    @skipIfDBFeature('supports_update_conflicts_with_target')\n+    def test_update_conflicts_unique_field_unsupported(self):\n+        msg = (\n+            'This database backend does not support updating conflicts with '\n+            'specifying unique fields that can trigger the upsert.'\n+        )\n+        with self.assertRaisesMessage(NotSupportedError, msg):\n+            TwoFields.objects.bulk_create(\n+                [TwoFields(f1=1, f2=1), TwoFields(f1=2, f2=2)],\n+                update_conflicts=True,\n+                update_fields=['f2'],\n+                unique_fields=['f1'],\n+            )\n+\n+    @skipUnlessDBFeature('supports_update_conflicts')\n+    def test_update_conflicts_nonexistent_update_fields(self):\n+        unique_fields = None\n+        if connection.features.supports_update_conflicts_with_target:\n+            unique_fields = ['f1']\n+        msg = \"TwoFields has no field named 'nonexistent'\"\n+        with self.assertRaisesMessage(FieldDoesNotExist, msg):\n+            TwoFields.objects.bulk_create(\n+                [TwoFields(f1=1, f2=1), TwoFields(f1=2, f2=2)],\n+                update_conflicts=True,\n+                update_fields=['nonexistent'],\n+                unique_fields=unique_fields,\n+            )\n+\n+    @skipUnlessDBFeature(\n+        'supports_update_conflicts', 'supports_update_conflicts_with_target',\n+    )\n+    def test_update_conflicts_unique_fields_required(self):\n+        msg = 'Unique fields that can trigger the upsert must be provided.'\n+        with self.assertRaisesMessage(ValueError, msg):\n+            TwoFields.objects.bulk_create(\n+                [TwoFields(f1=1, f2=1), TwoFields(f1=2, f2=2)],\n+                update_conflicts=True,\n+                update_fields=['f1'],\n+            )\n+\n+    @skipUnlessDBFeature(\n+        'supports_update_conflicts', 'supports_update_conflicts_with_target',\n+    )\n+    def test_update_conflicts_invalid_update_fields(self):\n+        msg = (\n+            'bulk_create() can only be used with concrete fields in '\n+            'update_fields.'\n+        )\n+        # Reverse one-to-one relationship.\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Country.objects.bulk_create(\n+                self.data,\n+                update_conflicts=True,\n+                update_fields=['relatedmodel'],\n+                unique_fields=['pk'],\n+            )\n+        # Many-to-many relationship.\n+        with self.assertRaisesMessage(ValueError, msg):\n+            RelatedModel.objects.bulk_create(\n+                [RelatedModel(country=self.data[0])],\n+                update_conflicts=True,\n+                update_fields=['big_auto_fields'],\n+                unique_fields=['country'],\n+            )\n+\n+    @skipUnlessDBFeature(\n+        'supports_update_conflicts', 'supports_update_conflicts_with_target',\n+    )\n+    def test_update_conflicts_pk_in_update_fields(self):\n+        msg = 'bulk_create() cannot be used with primary keys in update_fields.'\n+        with self.assertRaisesMessage(ValueError, msg):\n+            BigAutoFieldModel.objects.bulk_create(\n+                [BigAutoFieldModel()],\n+                update_conflicts=True,\n+                update_fields=['id'],\n+                unique_fields=['id'],\n+            )\n+\n+    @skipUnlessDBFeature(\n+        'supports_update_conflicts', 'supports_update_conflicts_with_target',\n+    )\n+    def test_update_conflicts_invalid_unique_fields(self):\n+        msg = (\n+            'bulk_create() can only be used with concrete fields in '\n+            'unique_fields.'\n+        )\n+        # Reverse one-to-one relationship.\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Country.objects.bulk_create(\n+                self.data,\n+                update_conflicts=True,\n+                update_fields=['name'],\n+                unique_fields=['relatedmodel'],\n+            )\n+        # Many-to-many relationship.\n+        with self.assertRaisesMessage(ValueError, msg):\n+            RelatedModel.objects.bulk_create(\n+                [RelatedModel(country=self.data[0])],\n+                update_conflicts=True,\n+                update_fields=['name'],\n+                unique_fields=['big_auto_fields'],\n+            )\n+\n+    def _test_update_conflicts_two_fields(self, unique_fields):\n+        TwoFields.objects.bulk_create([\n+            TwoFields(f1=1, f2=1, name='a'),\n+            TwoFields(f1=2, f2=2, name='b'),\n+        ])\n+        self.assertEqual(TwoFields.objects.count(), 2)\n+\n+        conflicting_objects = [\n+            TwoFields(f1=1, f2=1, name='c'),\n+            TwoFields(f1=2, f2=2, name='d'),\n+        ]\n+        TwoFields.objects.bulk_create(\n+            conflicting_objects,\n+            update_conflicts=True,\n+            unique_fields=unique_fields,\n+            update_fields=['name'],\n+        )\n+        self.assertEqual(TwoFields.objects.count(), 2)\n+        self.assertCountEqual(TwoFields.objects.values('f1', 'f2', 'name'), [\n+            {'f1': 1, 'f2': 1, 'name': 'c'},\n+            {'f1': 2, 'f2': 2, 'name': 'd'},\n+        ])\n+\n+    @skipUnlessDBFeature('supports_update_conflicts', 'supports_update_conflicts_with_target')\n+    def test_update_conflicts_two_fields_unique_fields_first(self):\n+        self._test_update_conflicts_two_fields(['f1'])\n+\n+    @skipUnlessDBFeature('supports_update_conflicts', 'supports_update_conflicts_with_target')\n+    def test_update_conflicts_two_fields_unique_fields_second(self):\n+        self._test_update_conflicts_two_fields(['f2'])\n+\n+    @skipUnlessDBFeature('supports_update_conflicts', 'supports_update_conflicts_with_target')\n+    def test_update_conflicts_two_fields_unique_fields_both(self):\n+        with self.assertRaises((OperationalError, ProgrammingError)):\n+            self._test_update_conflicts_two_fields(['f1', 'f2'])\n+\n+    @skipUnlessDBFeature('supports_update_conflicts')\n+    @skipIfDBFeature('supports_update_conflicts_with_target')\n+    def test_update_conflicts_two_fields_no_unique_fields(self):\n+        self._test_update_conflicts_two_fields([])\n+\n+    def _test_update_conflicts_unique_two_fields(self, unique_fields):\n+        Country.objects.bulk_create(self.data)\n+        self.assertEqual(Country.objects.count(), 4)\n+\n+        new_data = [\n+            # Conflicting countries.\n+            Country(name='Germany', iso_two_letter='DE', description=(\n+                'Germany is a country in Central Europe.'\n+            )),\n+            Country(name='Czech Republic', iso_two_letter='CZ', description=(\n+                'The Czech Republic is a landlocked country in Central Europe.'\n+            )),\n+            # New countries.\n+            Country(name='Australia', iso_two_letter='AU'),\n+            Country(name='Japan', iso_two_letter='JP', description=(\n+                'Japan is an island country in East Asia.'\n+            )),\n+        ]\n+        Country.objects.bulk_create(\n+            new_data,\n+            update_conflicts=True,\n+            update_fields=['description'],\n+            unique_fields=unique_fields,\n+        )\n+        self.assertEqual(Country.objects.count(), 6)\n+        self.assertCountEqual(Country.objects.values('iso_two_letter', 'description'), [\n+            {'iso_two_letter': 'US', 'description': ''},\n+            {'iso_two_letter': 'NL', 'description': ''},\n+            {'iso_two_letter': 'DE', 'description': (\n+                'Germany is a country in Central Europe.'\n+            )},\n+            {'iso_two_letter': 'CZ', 'description': (\n+                'The Czech Republic is a landlocked country in Central Europe.'\n+            )},\n+            {'iso_two_letter': 'AU', 'description': ''},\n+            {'iso_two_letter': 'JP', 'description': (\n+                'Japan is an island country in East Asia.'\n+            )},\n+        ])\n+\n+    @skipUnlessDBFeature('supports_update_conflicts', 'supports_update_conflicts_with_target')\n+    def test_update_conflicts_unique_two_fields_unique_fields_both(self):\n+        self._test_update_conflicts_unique_two_fields(['iso_two_letter', 'name'])\n+\n+    @skipUnlessDBFeature('supports_update_conflicts', 'supports_update_conflicts_with_target')\n+    def test_update_conflicts_unique_two_fields_unique_fields_one(self):\n+        with self.assertRaises((OperationalError, ProgrammingError)):\n+            self._test_update_conflicts_unique_two_fields(['iso_two_letter'])\n+\n+    @skipUnlessDBFeature('supports_update_conflicts')\n+    @skipIfDBFeature('supports_update_conflicts_with_target')\n+    def test_update_conflicts_unique_two_fields_unique_no_unique_fields(self):\n+        self._test_update_conflicts_unique_two_fields([])\n+\n+    def _test_update_conflicts(self, unique_fields):\n+        UpsertConflict.objects.bulk_create([\n+            UpsertConflict(number=1, rank=1, name='John'),\n+            UpsertConflict(number=2, rank=2, name='Mary'),\n+            UpsertConflict(number=3, rank=3, name='Hannah'),\n+        ])\n+        self.assertEqual(UpsertConflict.objects.count(), 3)\n+\n+        conflicting_objects = [\n+            UpsertConflict(number=1, rank=4, name='Steve'),\n+            UpsertConflict(number=2, rank=2, name='Olivia'),\n+            UpsertConflict(number=3, rank=1, name='Hannah'),\n+        ]\n+        UpsertConflict.objects.bulk_create(\n+            conflicting_objects,\n+            update_conflicts=True,\n+            update_fields=['name', 'rank'],\n+            unique_fields=unique_fields,\n+        )\n+        self.assertEqual(UpsertConflict.objects.count(), 3)\n+        self.assertCountEqual(UpsertConflict.objects.values('number', 'rank', 'name'), [\n+            {'number': 1, 'rank': 4, 'name': 'Steve'},\n+            {'number': 2, 'rank': 2, 'name': 'Olivia'},\n+            {'number': 3, 'rank': 1, 'name': 'Hannah'},\n+        ])\n+\n+        UpsertConflict.objects.bulk_create(\n+            conflicting_objects + [UpsertConflict(number=4, rank=4, name='Mark')],\n+            update_conflicts=True,\n+            update_fields=['name', 'rank'],\n+            unique_fields=unique_fields,\n+        )\n+        self.assertEqual(UpsertConflict.objects.count(), 4)\n+        self.assertCountEqual(UpsertConflict.objects.values('number', 'rank', 'name'), [\n+            {'number': 1, 'rank': 4, 'name': 'Steve'},\n+            {'number': 2, 'rank': 2, 'name': 'Olivia'},\n+            {'number': 3, 'rank': 1, 'name': 'Hannah'},\n+            {'number': 4, 'rank': 4, 'name': 'Mark'},\n+        ])\n+\n+    @skipUnlessDBFeature('supports_update_conflicts', 'supports_update_conflicts_with_target')\n+    def test_update_conflicts_unique_fields(self):\n+        self._test_update_conflicts(unique_fields=['number'])\n+\n+    @skipUnlessDBFeature('supports_update_conflicts')\n+    @skipIfDBFeature('supports_update_conflicts_with_target')\n+    def test_update_conflicts_no_unique_fields(self):\n+        self._test_update_conflicts([])\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The proposed approach to the API needs careful consideration. Using a boolean flag for `update_conflicts` may not provide the necessary granularity for handling unique constraints across multiple fields. Instead, a more flexible design, akin to `QuerySet.update()`, could better support varied update behaviors, like incrementing fields on conflict. Additionally, clarity around how `ON CONFLICT` interacts with different databases is crucial. Overall, enhancing the conflict handling options could significantly improve usability while maintaining alignment with SQL standards."
      }
    ],
    "codediff_length": 16150
  },
  {
    "index": 18,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/template/base.py b/django/template/base.py\nindex 3e8a59fbe7b7..74b39874106f 100644\n--- a/django/template/base.py\n+++ b/django/template/base.py\n@@ -288,6 +293,57 @@ def get_exception_info(self, exception, token):\n         }\n \n \n+class PartialTemplate:\n+    \"\"\"\n+    A lightweight Template lookalike used for template partials.\n+\n+    Wraps nodelist as a partial, in order to be able to bind context.\n+    \"\"\"\n+\n+    def __init__(self, nodelist, origin, name):\n+        self.nodelist = nodelist\n+        self.origin = origin\n+        self.name = name\n+\n+    def get_exception_info(self, exception, token):\n+        template = self.origin.loader.get_template(self.origin.template_name)\n+        return template.get_exception_info(exception, token)\n+\n+    def find_partial_source(self, full_source, partial_name):\n+        start_match = None\n+        nesting = 0\n+\n+        for match in combined_partial_re.finditer(full_source):\n+            if name := match[\"name\"]:  # Opening tag.\n+                if start_match is None and name == partial_name:\n+                    start_match = match\n+                if start_match is not None:\n+                    nesting += 1\n+            elif start_match is not None:\n+                nesting -= 1\n+                if nesting == 0:\n+                    return full_source[start_match.start() : match.end()]\n+\n+        return \"\"\n+\n+    @property\n+    def source(self):\n+        template = self.origin.loader.get_template(self.origin.template_name)\n+        return self.find_partial_source(template.source, self.name)\n+\n+    def _render(self, context):\n+        return self.nodelist.render(context)\n+\n+    def render(self, context):\n+        with context.render_context.push_state(self):\n+            if context.template is None:\n+                with context.bind_template(self):\n+                    context.template_name = self.name\n+                    return self._render(context)\n+            else:\n+                return self._render(context)\n+\n+\n def linebreak_iter(template_source):\n     yield 0\n     p = template_source.find(\"\\n\")\ndiff --git a/django/template/defaulttags.py b/django/template/defaulttags.py\nindex a20598152c24..121d37605050 100644\n--- a/django/template/defaulttags.py\n+++ b/django/template/defaulttags.py\n@@ -1570,3 +1572,96 @@ def do_with(parser, token):\n     nodelist = parser.parse((\"endwith\",))\n     parser.delete_first_token()\n     return WithNode(None, None, nodelist, extra_context=extra_context)\n+\n+\n+class DefinePartialNode(Node):\n+    def __init__(self, partial_name, inline, nodelist):\n+        self.partial_name = partial_name\n+        self.inline = inline\n+        self.nodelist = nodelist\n+\n+    def render(self, context):\n+        return self.nodelist.render(context) if self.inline else \"\"\n+\n+\n+class RenderPartialNode(Node):\n+    def __init__(self, partial_name, partial_mapping):\n+        # Defer lookup of nodelist to runtime.\n+        self.partial_name = partial_name\n+        self.partial_mapping = partial_mapping\n+\n+    def render(self, context):\n+        try:\n+            return self.partial_mapping[self.partial_name].render(context)\n+        except KeyError:\n+            raise TemplateSyntaxError(\n+                f\"Partial '{self.partial_name}' is not defined in the current template.\"\n+            )\n+\n+\n+@register.tag(name=\"partialdef\")\n+def partialdef_func(parser, token):\n+    \"\"\"Declare a partial that can be used later in the template.\n+\n+    Usage::\n+\n+        {% partialdef partial_name %}\n+        Partial content goes here\n+        {% endpartialdef %}\n+\n+    Stores the nodelist in the context under the key \"partial_contents\" and can\n+    be retrieved using the {% partial %} tag.\n+\n+    The optional ``inline`` argument will render the contents of the partial\n+    where it is defined.\n+    \"\"\"\n+    match token.split_contents():\n+        case \"partialdef\", partial_name, \"inline\":\n+            inline = True\n+        case \"partialdef\", partial_name, _:\n+            raise TemplateSyntaxError(\n+                \"The 'inline' argument does not have any parameters; \"\n+                \"either use 'inline' or remove it completely.\"\n+            )\n+        case \"partialdef\", partial_name:\n+            inline = False\n+        case _:\n+            raise TemplateSyntaxError(\"'partialdef' tag requires 2-3 arguments\")\n+\n+    # Parse the content until the end tag.\n+    valid_endpartials = (\"endpartialdef\", f\"endpartialdef {partial_name}\")\n+    nodelist = parser.parse(valid_endpartials)\n+    endpartial = parser.next_token()\n+    if endpartial.contents not in valid_endpartials:\n+        parser.invalid_block_tag(endpartial, \"endpartialdef\", valid_endpartials)\n+\n+    # Store the partial nodelist in the parser.extra_data attribute.\n+    parser.extra_data.setdefault(\"template-partials\", {})\n+    if partial_name in parser.extra_data[\"template-partials\"]:\n+        raise TemplateSyntaxError(\n+            f\"Partial '{partial_name}' is already defined in the \"\n+            f\"'{parser.origin.name}' template.\"\n+        )\n+    parser.extra_data[\"template-partials\"][partial_name] = PartialTemplate(\n+        nodelist, parser.origin, partial_name\n+    )\n+\n+    return DefinePartialNode(partial_name, inline, nodelist)\n+\n+\n+@register.tag(name=\"partial\")\n+def partial_func(parser, token):\n+    \"\"\"\n+    Render a partial that was declared using the {% partialdef %} tag.\n+\n+    Usage::\n+\n+        {% partial partial_name %}\n+    \"\"\"\n+    match token.split_contents():\n+        case \"partial\", partial_name:\n+            extra_data = parser.extra_data\n+            partial_mapping = SubDictionaryWrapper(extra_data, \"template-partials\")\n+            return RenderPartialNode(partial_name, partial_mapping=partial_mapping)\n+        case _:\n+            raise TemplateSyntaxError(\"'partial' tag requires a single argument\")\ndiff --git a/docs/ref/templates/builtins.txt b/docs/ref/templates/builtins.txt\nindex 1eb391d8c0f2..9265243c6adc 100644\n--- a/docs/ref/templates/builtins.txt\n+++ b/docs/ref/templates/builtins.txt\n@@ -957,6 +957,76 @@ output (as a string) inside a variable. This is useful if you want to use\n     {% now \"Y\" as current_year %}\n     {% blocktranslate %}Copyright {{ current_year }}{% endblocktranslate %}\n \n+.. templatetag:: partial\n+\n+``partial``\n+-----------\n+\n+.. versionadded:: 6.0\n+\n+Renders a partial that was defined with :ttag:`partialdef`.\n+\n+Usage:\n+\n+.. code-block:: html+django\n+\n+    {% partial partial_name %}\n+\n+The ``partial_name`` argument is the identifier of the partial to render.\n+\n+Example:\n+\n+.. code-block:: html+django\n+\n+    {% partialdef button %}\n+        <button>Submit</button>\n+    {% endpartialdef %}\n+\n+    {% partial button %}\n+    {% partial button %}\n+    {% partial button %}\n+\n+This example will define a partial named ``button`` and render it three times.\n+\n+.. templatetag:: partialdef\n+\n+``partialdef``\n+--------------\n+\n+.. versionadded:: 6.0\n+\n+Defines a reusable template fragment that can be rendered multiple times within\n+the same template or accessed directly via template loading or inclusion.\n+\n+Usage:\n+\n+.. code-block:: html+django\n+\n+    {% partialdef partial_name %}\n+        <!-- partial content -->\n+    {% endpartialdef %}\n+\n+The ``{% partialdef %}`` tag takes a required ``partial_name`` argument, which\n+is an identifier for the partial:\n+\n+.. code-block:: html+django\n+\n+    {% partialdef card %}\n+        <div class=\"card\">\n+            <h3>{{ title }}</h3>\n+            <p>{{ content }}</p>\n+        </div>\n+    {% endpartialdef %}\n+\n+To define and render a partial immediately in the same place, use the ``inline``\n+option:\n+\n+.. code-block:: html+django\n+\n+    {% partialdef header inline %}\n+        <header>Site Header</header>\n+    {% endpartialdef %}\n+\n .. templatetag:: querystring\n \n ``querystring``\ndiff --git a/docs/ref/templates/language.txt b/docs/ref/templates/language.txt\nindex 16a3682086c4..09d4d4fe6750 100644\n--- a/docs/ref/templates/language.txt\n+++ b/docs/ref/templates/language.txt\n@@ -475,6 +475,143 @@ it also defines the content that fills the hole in the *parent*. If there were\n two similarly-named :ttag:`block` tags in a template, that template's parent\n wouldn't know which one of the blocks' content to use.\n \n+.. _template-partials:\n+\n+Template partials\n+=================\n+\n+.. versionadded:: 6.0\n+\n+Template partials define reusable template fragments within a template file.\n+Partials are self-contained components that you can use multiple times within\n+the same template.\n+\n+Basic syntax\n+------------\n+\n+Define a partial using the :ttag:`partialdef` tag:\n+\n+.. code-block:: html+django\n+\n+    {% partialdef card %}\n+        <div class=\"card\">\n+            <h3>{{ title }}</h3>\n+            <p>{{ content }}</p>\n+        </div>\n+    {% endpartialdef %}\n+\n+For extra readability, you can optionally add the name to your \n+``{% endpartialdef %}`` tag:\n+\n+.. code-block:: html+django\n+\n+    {% partialdef card %}\n+        <div class=\"card\">\n+            <h3>{{ title }}</h3>\n+            <p>{{ content }}</p>\n+        </div>\n+    {% endpartialdef card %}\n+\n+Render the partial using the :ttag:`partial` tag:\n+\n+.. code-block:: html+django\n+\n+    {% partial card %}\n+\n+Fragment reuse\n+--------------\n+\n+Once defined, you can reuse a partial multiple times:\n+\n+.. code-block:: html+django\n+\n+    {% partialdef user-info %}\n+        <div class=\"user\">\n+            <span>{{ user.username }}</span>\n+            <span>{{ user.email }}</span>\n+        </div>\n+    {% endpartialdef %}\n+\n+    {% block content %}\n+        <h2>Authors</h2>\n+        {% for user in authors %}\n+            {% partial user-info %}\n+        {% endfor %}\n+\n+        <h2>Editors</h2>\n+        {% for user in editors %}\n+            {% partial user-info %}\n+        {% endfor %}\n+    {% endblock %}\n+\n+The partial content will be rendered each time the named partial is used,\n+with the current template context.\n+\n+Inline partials\n+---------------\n+\n+Define and render a partial inline using the ``inline`` argument:\n+\n+.. code-block:: html+django\n+\n+    {% partialdef header inline %}\n+        <header class=\"site-header\">\n+            <h1>{{ site_name }}</h1>\n+        </header>\n+    {% endpartialdef %}\n+\n+This defines the partial and renders it immediately in place.\n+\n+Direct partial access\n+---------------------\n+\n+Access partials directly using the ``#`` syntax. In views, use the\n+:func:`~django.shortcuts.render` shortcut:\n+\n+.. code-block:: python\n+\n+    # In views.py\n+    from django.shortcuts import render\n+\n+\n+    def product_card_partial(request, product_id):\n+        product = get_object_or_404(Product, id=product_id)\n+        return render(request, \"products/list.html#product-card\", {\"product\": product})\n+\n+This approach is useful for AJAX responses, HTMX requests, or when you need\n+only a specific part of a template. Use partials with the :ttag:`include` tag:\n+\n+.. code-block:: html+django\n+\n+    {% include \"products/list.html#product-card\" %}\n+\n+Context handling\n+----------------\n+\n+Template partials render with the current template context. They work as\n+expected in loops and with context variables:\n+\n+.. code-block:: html+django\n+\n+    {% partialdef product-card %}\n+        <div class=\"product\" data-id=\"{{ product.id }}\">\n+            <h3>{{ product.name }}</h3>\n+            <p>${{ product.price }}</p>\n+        </div>\n+    {% endpartialdef %}\n+\n+    {% for product in products %}\n+        {% partial product-card %}\n+    {% endfor %}\n+\n+If you need to adjust the context, use the :ttag:`with` tag:\n+\n+.. code-block:: html+django\n+\n+    {% with title=\"Special Offer\" price=product.sale_price %}\n+        {% partial product-card %}\n+    {% endwith %}\n+\n .. _next section: #automatic-html-escaping\n .. _automatic-html-escaping:\n \ndiff --git a/tests/template_tests/syntax_tests/test_partials.py b/tests/template_tests/syntax_tests/test_partials.py\nnew file mode 100644\nindex 000000000000..7809d2c01895\n--- /dev/null\n+++ b/tests/template_tests/syntax_tests/test_partials.py\n@@ -0,0 +1,668 @@\n+from django.template import (\n+    Context,\n+    TemplateDoesNotExist,\n+    TemplateSyntaxError,\n+    VariableDoesNotExist,\n+)\n+from django.template.base import Token, TokenType\n+from django.test import SimpleTestCase\n+from django.views.debug import ExceptionReporter\n+\n+from ..utils import setup\n+\n+partial_templates = {\n+    \"partial_base.html\": (\n+        \"<main>{% block main %}Default main content.{% endblock main %}</main>\"\n+    ),\n+    \"partial_included.html\": (\n+        \"INCLUDED TEMPLATE START\\n\"\n+        \"{% partialdef included-partial %}\\n\"\n+        \"THIS IS CONTENT FROM THE INCLUDED PARTIAL\\n\"\n+        \"{% endpartialdef %}\\n\\n\"\n+        \"Now using the partial: {% partial included-partial %}\\n\"\n+        \"INCLUDED TEMPLATE END\\n\"\n+    ),\n+}\n+\n+valid_partialdef_names = (\n+    \"dot.in.name\",\n+    \"'space in name'\",\n+    \"exclamation!\",\n+    \"@at\",\n+    \"slash/something\",\n+    \"inline\",\n+    \"inline-inline\",\n+    \"INLINE\" \"with+plus\",\n+    \"with&amp\",\n+    \"with%percent\",\n+    \"with,comma\",\n+    \"with:colon\",\n+    \"with;semicolon\",\n+    \"[brackets]\",\n+    \"(parens)\",\n+    \"{curly}\",\n+)\n+\n+\n+def gen_partial_template(name, *args, **kwargs):\n+    if args or kwargs:\n+        extra = \" \".join((args, *(\"{k}={v}\" for k, v in kwargs.items()))) + \" \"\n+    else:\n+        extra = \"\"\n+    return (\n+        f\"{{% partialdef {name} {extra}%}}TEST with {name}!{{% endpartialdef %}}\"\n+        f\"{{% partial {name} %}}\"\n+    )\n+\n+\n+class PartialTagTests(SimpleTestCase):\n+    libraries = {\"bad_tag\": \"template_tests.templatetags.bad_tag\"}\n+\n+    @setup({name: gen_partial_template(name) for name in valid_partialdef_names})\n+    def test_valid_partialdef_names(self):\n+        for template_name in valid_partialdef_names:\n+            with self.subTest(template_name=template_name):\n+                output = self.engine.render_to_string(template_name)\n+                self.assertEqual(output, f\"TEST with {template_name}!\")\n+\n+    @setup(\n+        {\n+            \"basic\": (\n+                \"{% partialdef testing-name %}\"\n+                \"HERE IS THE TEST CONTENT\"\n+                \"{% endpartialdef %}\"\n+                \"{% partial testing-name %}\"\n+            ),\n+            \"basic_inline\": (\n+                \"{% partialdef testing-name inline %}\"\n+                \"HERE IS THE TEST CONTENT\"\n+                \"{% endpartialdef %}\"\n+            ),\n+            \"inline_inline\": (\n+                \"{% partialdef inline inline %}\"\n+                \"HERE IS THE TEST CONTENT\"\n+                \"{% endpartialdef %}\"\n+            ),\n+            \"with_newlines\": (\n+                \"{% partialdef testing-name %}\\n\"\n+                \"HERE IS THE TEST CONTENT\\n\"\n+                \"{% endpartialdef testing-name %}\\n\"\n+                \"{% partial testing-name %}\"\n+            ),\n+        }\n+    )\n+    def test_basic_usage(self):\n+        for template_name in (\n+            \"basic\",\n+            \"basic_inline\",\n+            \"inline_inline\",\n+            \"with_newlines\",\n+        ):\n+            with self.subTest(template_name=template_name):\n+                output = self.engine.render_to_string(template_name)\n+                self.assertEqual(output.strip(), \"HERE IS THE TEST CONTENT\")\n+\n+    @setup(\n+        {\n+            \"inline_partial_with_context\": (\n+                \"BEFORE\\n\"\n+                \"{% partialdef testing-name inline %}\"\n+                \"HERE IS THE TEST CONTENT\"\n+                \"{% endpartialdef %}\\n\"\n+                \"AFTER\"\n+            )\n+        }\n+    )\n+    def test_partial_inline_only_with_before_and_after_content(self):\n+        output = self.engine.render_to_string(\"inline_partial_with_context\")\n+        self.assertEqual(output.strip(), \"BEFORE\\nHERE IS THE TEST CONTENT\\nAFTER\")\n+\n+    @setup(\n+        {\n+            \"inline_partial_explicit_end\": (\n+                \"{% partialdef testing-name inline %}\"\n+                \"HERE IS THE TEST CONTENT\"\n+                \"{% endpartialdef testing-name %}\\n\"\n+                \"{% partial testing-name %}\"\n+            )\n+        }\n+    )\n+    def test_partial_inline_and_used_once(self):\n+        output = self.engine.render_to_string(\"inline_partial_explicit_end\")\n+        self.assertEqual(output, \"HERE IS THE TEST CONTENT\\nHERE IS THE TEST CONTENT\")\n+\n+    @setup(\n+        {\n+            \"inline_partial_with_usage\": (\n+                \"BEFORE\\n\"\n+                \"{% partialdef content_snippet inline %}\"\n+                \"HERE IS THE TEST CONTENT\"\n+                \"{% endpartialdef %}\\n\"\n+                \"AFTER\\n\"\n+                \"{% partial content_snippet %}\"\n+            )\n+        }\n+    )\n+    def test_partial_inline_and_used_once_with_before_and_after_content(self):\n+        output = self.engine.render_to_string(\"inline_partial_with_usage\")\n+        self.assertEqual(\n+            output.strip(),\n+            \"BEFORE\\nHERE IS THE TEST CONTENT\\nAFTER\\nHERE IS THE TEST CONTENT\",\n+        )\n+\n+    @setup(\n+        {\n+            \"partial_used_before_definition\": (\n+                \"TEMPLATE START\\n\"\n+                \"{% partial testing-name %}\\n\"\n+                \"MIDDLE CONTENT\\n\"\n+                \"{% partialdef testing-name %}\\n\"\n+                \"THIS IS THE PARTIAL CONTENT\\n\"\n+                \"{% endpartialdef %}\\n\"\n+                \"TEMPLATE END\"\n+            ),\n+        }\n+    )\n+    def test_partial_used_before_definition(self):\n+        output = self.engine.render_to_string(\"partial_used_before_definition\")\n+        expected = (\n+            \"TEMPLATE START\\n\\nTHIS IS THE PARTIAL CONTENT\\n\\n\"\n+            \"MIDDLE CONTENT\\n\\nTEMPLATE END\"\n+        )\n+        self.assertEqual(output, expected)\n+\n+    @setup(\n+        {\n+            \"partial_with_extends\": (\n+                \"{% extends 'partial_base.html' %}\"\n+                \"{% partialdef testing-name %}Inside Content{% endpartialdef %}\"\n+                \"{% block main %}\"\n+                \"Main content with {% partial testing-name %}\"\n+                \"{% endblock %}\"\n+            ),\n+        },\n+        partial_templates,\n+    )\n+    def test_partial_defined_outside_main_block(self):\n+        output = self.engine.render_to_string(\"partial_with_extends\")\n+        self.assertIn(\"<main>Main content with Inside Content</main>\", output)\n+\n+    @setup(\n+        {\n+            \"partial_with_extends_and_block_super\": (\n+                \"{% extends 'partial_base.html' %}\"\n+                \"{% partialdef testing-name %}Inside Content{% endpartialdef %}\"\n+                \"{% block main %}{{ block.super }} \"\n+                \"Main content with {% partial testing-name %}\"\n+                \"{% endblock %}\"\n+            ),\n+        },\n+        partial_templates,\n+    )\n+    def test_partial_used_with_block_super(self):\n+        output = self.engine.render_to_string(\"partial_with_extends_and_block_super\")\n+        self.assertIn(\n+            \"<main>Default main content. Main content with Inside Content</main>\",\n+            output,\n+        )\n+\n+    @setup(\n+        {\n+            \"partial_with_include\": (\n+                \"MAIN TEMPLATE START\\n\"\n+                \"{% include 'partial_included.html' %}\\n\"\n+                \"MAIN TEMPLATE END\"\n+            )\n+        },\n+        partial_templates,\n+    )\n+    def test_partial_in_included_template(self):\n+        output = self.engine.render_to_string(\"partial_with_include\")\n+        expected = (\n+            \"MAIN TEMPLATE START\\nINCLUDED TEMPLATE START\\n\\n\\n\"\n+            \"Now using the partial: \\n\"\n+            \"THIS IS CONTENT FROM THE INCLUDED PARTIAL\\n\\n\"\n+            \"INCLUDED TEMPLATE END\\n\\nMAIN TEMPLATE END\"\n+        )\n+        self.assertEqual(output, expected)\n+\n+    @setup(\n+        {\n+            \"partial_as_include_in_other_template\": (\n+                \"MAIN TEMPLATE START\\n\"\n+                \"{% include 'partial_included.html#included-partial' %}\\n\"\n+                \"MAIN TEMPLATE END\"\n+            )\n+        },\n+        partial_templates,\n+    )\n+    def test_partial_as_include_in_template(self):\n+        output = self.engine.render_to_string(\"partial_as_include_in_other_template\")\n+        expected = (\n+            \"MAIN TEMPLATE START\\n\\n\"\n+            \"THIS IS CONTENT FROM THE INCLUDED PARTIAL\\n\\n\"\n+            \"MAIN TEMPLATE END\"\n+        )\n+        self.assertEqual(output, expected)\n+\n+    @setup(\n+        {\n+            \"nested_simple\": (\n+                \"{% extends 'base.html' %}\"\n+                \"{% block content %}\"\n+                \"This is my main page.\"\n+                \"{% partialdef outer inline %}\"\n+                \"    It hosts a couple of partials.\"\n+                \"    {% partialdef inner inline %}\"\n+                \"        And an inner one.\"\n+                \"    {% endpartialdef inner %}\"\n+                \"{% endpartialdef outer %}\"\n+                \"{% endblock content %}\"\n+            ),\n+            \"use_outer\": \"{% include 'nested_simple#outer' %}\",\n+            \"use_inner\": \"{% include 'nested_simple#inner' %}\",\n+        }\n+    )\n+    def test_nested_partials(self):\n+        with self.subTest(template_name=\"use_outer\"):\n+            output = self.engine.render_to_string(\"use_outer\")\n+            self.assertEqual(\n+                output.strip(),\n+                \"It hosts a couple of partials.            And an inner one.\",\n+            )\n+        with self.subTest(template_name=\"use_inner\"):\n+            output = self.engine.render_to_string(\"use_inner\")\n+            self.assertEqual(output.strip(), \"And an inner one.\")\n+\n+    @setup(\n+        {\n+            \"partial_undefined_name\": \"{% partial undefined %}\",\n+            \"partial_missing_name\": \"{% partial %}\",\n+            \"partial_closing_tag\": (\n+                \"{% partialdef testing-name %}TEST{% endpartialdef %}\"\n+                \"{% partial testing-name %}{% endpartial %}\"\n+            ),\n+            \"partialdef_opening_closing_name_mismatch\": (\n+                \"{% partialdef testing-name %}TEST{% endpartialdef invalid %}\"\n+            ),\n+            \"duplicated_names\": (\n+                \"{% partialdef testing-name %}TEST{% endpartialdef %}\"\n+                \"{% partialdef testing-name %}TEST{% endpartialdef %}\"\n+                \"{% partial testing-name %}\"\n+            ),\n+            \"duplicated_nested_names\": (\n+                \"{% partialdef testing-name %}\"\n+                \"TEST\"\n+                \"{% partialdef testing-name %}TEST{% endpartialdef %}\"\n+                \"{% endpartialdef %}\"\n+                \"{% partial testing-name %}\"\n+            ),\n+            \"partialdef_invalid_name\": gen_partial_template(\"with\\nnewline\"),\n+        }\n+    )\n+    def test_basic_parse_errors(self):\n+        for template_name, error_msg in (\n+            (\n+                \"partial_undefined_name\",\n+                \"Partial 'undefined' is not defined in the current template.\",\n+            ),\n+            (\"partial_missing_name\", \"'partial' tag requires a single argument\"),\n+            (\"partial_closing_tag\", \"Invalid block tag on line 1: 'endpartial'\"),\n+            (\n+                \"partialdef_opening_closing_name_mismatch\",\n+                \"expected 'endpartialdef' or 'endpartialdef testing-name'.\",\n+            ),\n+            (\n+                \"duplicated_names\",\n+                \"Partial 'testing-name' is already defined in the 'duplicated_names' \"\n+                \"template.\",\n+            ),\n+            (\n+                \"duplicated_nested_names\",\n+                \"Partial 'testing-name' is already defined in the \"\n+                \"'duplicated_nested_names' template.\",\n+            ),\n+            (\"partialdef_invalid_name\", \"Invalid block tag on line 3: 'endpartialdef'\"),\n+        ):\n+            with (\n+                self.subTest(template_name=template_name),\n+                self.assertRaisesMessage(TemplateSyntaxError, error_msg),\n+            ):\n+                self.engine.render_to_string(template_name)\n+\n+    @setup(\n+        {\n+            \"missing_name\": \"{% partialdef %}TEST{% endpartialdef %}\",\n+            \"missing_close_tag\": \"{% partialdef %}\",\n+            \"inconsistent_name\": \"{% partialdef %}TEST{% endpartialdef name %}\",\n+            \"extra_params\": (\n+                \"{% partialdef testing-name inline extra %}TEST{% endpartialdef %}\"\n+            ),\n+        }\n+    )\n+    def test_partialdef_missing_arguments(self):\n+        error_msg = \"'partialdef' tag requires 2-3 arguments\"\n+        for template_name in (\n+            \"missing_name\",\n+            \"missing_close_tag\",\n+            \"inconsistent_name\",\n+            \"extra_params\",\n+        ):\n+            with (\n+                self.subTest(template_name=template_name),\n+                self.assertRaisesMessage(TemplateSyntaxError, error_msg),\n+            ):\n+                self.engine.render_to_string(template_name)\n+\n+    @setup(\n+        {\n+            \"with_params\": (\n+                \"{% partialdef testing-name inline=true %}TEST{% endpartialdef %}\"\n+            ),\n+            \"uppercase\": \"{% partialdef testing-name INLINE %}TEST{% endpartialdef %}\",\n+        }\n+    )\n+    def test_partialdef_invalid_inline(self):\n+        error_msg = \"The 'inline' argument does not have any parameters\"\n+        for template_name in (\"with_params\", \"uppercase\"):\n+            with (\n+                self.subTest(template_name=template_name),\n+                self.assertRaisesMessage(TemplateSyntaxError, error_msg),\n+            ):\n+                self.engine.render_to_string(template_name)\n+\n+    @setup(\n+        {\n+            \"partial_broken_unclosed\": (\n+                \"<div>Before partial</div>\"\n+                \"{% partialdef unclosed_partial %}\"\n+                \"<p>This partial has no closing tag</p>\"\n+                \"<div>After partial content</div>\"\n+            )\n+        }\n+    )\n+    def test_broken_partial_unclosed_exception_info(self):\n+        with self.assertRaises(TemplateSyntaxError) as cm:\n+            self.engine.get_template(\"partial_broken_unclosed\")\n+\n+        self.assertIn(\"endpartialdef\", str(cm.exception))\n+        self.assertIn(\"Unclosed tag\", str(cm.exception))\n+\n+        reporter = ExceptionReporter(None, cm.exception.__class__, cm.exception, None)\n+        traceback_data = reporter.get_traceback_data()\n+\n+        exception_value = str(traceback_data.get(\"exception_value\", \"\"))\n+        self.assertIn(\"Unclosed tag\", exception_value)\n+\n+    @setup(\n+        {\n+            \"partial_with_variable_error\": (\n+                \"<h1>Title</h1>\\n\"\n+                \"{% partialdef testing-name %}\\n\"\n+                \"<p>{{ nonexistent|default:alsonotthere }}</p>\\n\"\n+                \"{% endpartialdef %}\\n\"\n+                \"<h2>Sub Title</h2>\\n\"\n+                \"{% partial testing-name %}\\n\"\n+            ),\n+        }\n+    )\n+    def test_partial_runtime_exception_has_debug_info(self):\n+        template = self.engine.get_template(\"partial_with_variable_error\")\n+        context = Context({})\n+\n+        if hasattr(self.engine, \"string_if_invalid\") and self.engine.string_if_invalid:\n+            output = template.render(context)\n+            # The variable should be replaced with INVALID\n+            self.assertIn(\"INVALID\", output)\n+        else:\n+            with self.assertRaises(VariableDoesNotExist) as cm:\n+                template.render(context)\n+\n+            if self.engine.debug:\n+                exc_info = cm.exception.template_debug\n+\n+                self.assertEqual(\n+                    exc_info[\"during\"], \"{{ nonexistent|default:alsonotthere }}\"\n+                )\n+                self.assertEqual(exc_info[\"line\"], 3)\n+                self.assertEqual(exc_info[\"name\"], \"partial_with_variable_error\")\n+                self.assertIn(\"Failed lookup\", exc_info[\"message\"])\n+\n+    @setup(\n+        {\n+            \"partial_exception_info_test\": (\n+                \"<h1>Title</h1>\\n\"\n+                \"{% partialdef testing-name %}\\n\"\n+                \"<p>Content</p>\\n\"\n+                \"{% endpartialdef %}\\n\"\n+            ),\n+        }\n+    )\n+    def test_partial_template_get_exception_info_delegation(self):\n+        if self.engine.debug:\n+            template = self.engine.get_template(\"partial_exception_info_test\")\n+\n+            partial_template = template.extra_data[\"template-partials\"][\"testing-name\"]\n+\n+            test_exc = Exception(\"Test exception\")\n+            token = Token(\n+                token_type=TokenType.VAR,\n+                contents=\"test\",\n+                position=(0, 4),\n+            )\n+\n+            exc_info = partial_template.get_exception_info(test_exc, token)\n+            self.assertIn(\"message\", exc_info)\n+            self.assertIn(\"line\", exc_info)\n+            self.assertIn(\"name\", exc_info)\n+            self.assertEqual(exc_info[\"name\"], \"partial_exception_info_test\")\n+            self.assertEqual(exc_info[\"message\"], \"Test exception\")\n+\n+    @setup(\n+        {\n+            \"partial_with_undefined_reference\": (\n+                \"<h1>Header</h1>\\n\"\n+                \"{% partial undefined %}\\n\"\n+                \"<p>After undefined partial</p>\\n\"\n+            ),\n+        }\n+    )\n+    def test_undefined_partial_exception_info(self):\n+        template = self.engine.get_template(\"partial_with_undefined_reference\")\n+        with self.assertRaises(TemplateSyntaxError) as cm:\n+            template.render(Context())\n+\n+        self.assertIn(\"undefined\", str(cm.exception))\n+        self.assertIn(\"is not defined\", str(cm.exception))\n+\n+        if self.engine.debug:\n+            exc_debug = cm.exception.template_debug\n+\n+            self.assertEqual(exc_debug[\"during\"], \"{% partial undefined %}\")\n+            self.assertEqual(exc_debug[\"line\"], 2)\n+            self.assertEqual(exc_debug[\"name\"], \"partial_with_undefined_reference\")\n+            self.assertIn(\"undefined\", exc_debug[\"message\"])\n+\n+    @setup(\n+        {\n+            \"existing_template\": (\n+                \"<h1>Header</h1><p>This template has no partials defined</p>\"\n+            ),\n+        }\n+    )\n+    def test_undefined_partial_exception_info_template_does_not_exist(self):\n+        with self.assertRaises(TemplateDoesNotExist) as cm:\n+            self.engine.get_template(\"existing_template#undefined\")\n+\n+        self.assertIn(\"undefined\", str(cm.exception))\n+\n+    @setup(\n+        {\n+            \"partial_with_syntax_error\": (\n+                \"<h1>Title</h1>\\n\"\n+                \"{% partialdef syntax_error_partial %}\\n\"\n+                \"    {% if user %}\\n\"\n+                \"        <p>User: {{ user.name }}</p>\\n\"\n+                \"    {% endif\\n\"\n+                \"    <p>Missing closing tag above</p>\\n\"\n+                \"{% endpartialdef %}\\n\"\n+                \"{% partial syntax_error_partial %}\\n\"\n+            ),\n+        }\n+    )\n+    def test_partial_with_syntax_error_exception_info(self):\n+        with self.assertRaises(TemplateSyntaxError) as cm:\n+            self.engine.get_template(\"partial_with_syntax_error\")\n+\n+        self.assertIn(\"endif\", str(cm.exception).lower())\n+\n+        if self.engine.debug:\n+            exc_debug = cm.exception.template_debug\n+\n+            self.assertIn(\"endpartialdef\", exc_debug[\"during\"])\n+            self.assertEqual(exc_debug[\"name\"], \"partial_with_syntax_error\")\n+            self.assertIn(\"endif\", exc_debug[\"message\"].lower())\n+\n+    @setup(\n+        {\n+            \"partial_with_runtime_error\": (\n+                \"<h1>Title</h1>\\n\"\n+                \"{% load bad_tag %}\\n\"\n+                \"{% partialdef runtime_error_partial %}\\n\"\n+                \"    <p>This will raise an error:</p>\\n\"\n+                \"    {% badsimpletag %}\\n\"\n+                \"{% endpartialdef %}\\n\"\n+                \"{% partial runtime_error_partial %}\\n\"\n+            ),\n+        }\n+    )\n+    def test_partial_runtime_error_exception_info(self):\n+        template = self.engine.get_template(\"partial_with_runtime_error\")\n+        context = Context()\n+\n+        with self.assertRaises(RuntimeError) as cm:\n+            template.render(context)\n+\n+        if self.engine.debug:\n+            exc_debug = cm.exception.template_debug\n+\n+            self.assertIn(\"badsimpletag\", exc_debug[\"during\"])\n+            self.assertEqual(exc_debug[\"line\"], 5)  # Line 5 is where badsimpletag is\n+            self.assertEqual(exc_debug[\"name\"], \"partial_with_runtime_error\")\n+            self.assertIn(\"bad simpletag\", exc_debug[\"message\"])\n+\n+    @setup(\n+        {\n+            \"nested_partial_with_undefined_var\": (\n+                \"<h1>Title</h1>\\n\"\n+                \"{% partialdef outer_partial %}\\n\"\n+                '    <div class=\"outer\">\\n'\n+                \"        {% partialdef inner_partial %}\\n\"\n+                \"            <p>{{ undefined_var }}</p>\\n\"\n+                \"        {% endpartialdef %}\\n\"\n+                \"        {% partial inner_partial %}\\n\"\n+                \"    </div>\\n\"\n+                \"{% endpartialdef %}\\n\"\n+                \"{% partial outer_partial %}\\n\"\n+            ),\n+        }\n+    )\n+    def test_nested_partial_error_exception_info(self):\n+        template = self.engine.get_template(\"nested_partial_with_undefined_var\")\n+        context = Context()\n+        output = template.render(context)\n+\n+        # When string_if_invalid is set, it will show INVALID\n+        # When not set, undefined variables just render as empty string\n+        if hasattr(self.engine, \"string_if_invalid\") and self.engine.string_if_invalid:\n+            self.assertIn(\"INVALID\", output)\n+        else:\n+            self.assertIn(\"<p>\", output)\n+            self.assertIn(\"</p>\", output)\n+\n+    @setup(\n+        {\n+            \"parent.html\": (\n+                \"<!DOCTYPE html>\\n\"\n+                \"<html>\\n\"\n+                \"<head>{% block title %}Default Title{% endblock %}</head>\\n\"\n+                \"<body>\\n\"\n+                \"    {% block content %}{% endblock %}\\n\"\n+                \"</body>\\n\"\n+                \"</html>\\n\"\n+            ),\n+            \"child.html\": (\n+                \"{% extends 'parent.html' %}\\n\"\n+                \"{% block content %}\\n\"\n+                \"    {% partialdef content_partial %}\\n\"\n+                \"        <p>{{ missing_variable|undefined_filter }}</p>\\n\"\n+                \"    {% endpartialdef %}\\n\"\n+                \"    {% partial content_partial %}\\n\"\n+                \"{% endblock %}\\n\"\n+            ),\n+        }\n+    )\n+    def test_partial_in_extended_template_error(self):\n+        with self.assertRaises(TemplateSyntaxError) as cm:\n+            self.engine.get_template(\"child.html\")\n+\n+        self.assertIn(\"undefined_filter\", str(cm.exception))\n+\n+        if self.engine.debug:\n+            exc_debug = cm.exception.template_debug\n+\n+            self.assertIn(\"undefined_filter\", exc_debug[\"during\"])\n+            self.assertEqual(exc_debug[\"name\"], \"child.html\")\n+            self.assertIn(\"undefined_filter\", exc_debug[\"message\"])\n+\n+    @setup(\n+        {\n+            \"partial_broken_nesting\": (\n+                \"<div>Before partial</div>\\n\"\n+                \"{% partialdef outer %}\\n\"\n+                \"{% partialdef inner %}...{% endpartialdef outer %}\\n\"\n+                \"{% endpartialdef inner %}\\n\"\n+                \"<div>After partial content</div>\"\n+            )\n+        }\n+    )\n+    def test_broken_partial_nesting(self):\n+        with self.assertRaises(TemplateSyntaxError) as cm:\n+            self.engine.get_template(\"partial_broken_nesting\")\n+\n+        self.assertIn(\"endpartialdef\", str(cm.exception))\n+        self.assertIn(\"Invalid block tag\", str(cm.exception))\n+        self.assertIn(\"'endpartialdef inner'\", str(cm.exception))\n+\n+        reporter = ExceptionReporter(None, cm.exception.__class__, cm.exception, None)\n+        traceback_data = reporter.get_traceback_data()\n+\n+        exception_value = str(traceback_data.get(\"exception_value\", \"\"))\n+        self.assertIn(\"Invalid block tag\", exception_value)\n+        self.assertIn(\"'endpartialdef inner'\", str(cm.exception))\n+\n+    @setup(\n+        {\n+            \"partial_broken_nesting_mixed\": (\n+                \"<div>Before partial</div>\\n\"\n+                \"{% partialdef outer %}\\n\"\n+                \"{% partialdef inner %}...{% endpartialdef %}\\n\"\n+                \"{% endpartialdef inner %}\\n\"\n+                \"<div>After partial content</div>\"\n+            )\n+        }\n+    )\n+    def test_broken_partial_nesting_mixed(self):\n+        with self.assertRaises(TemplateSyntaxError) as cm:\n+            self.engine.get_template(\"partial_broken_nesting_mixed\")\n+\n+        self.assertIn(\"endpartialdef\", str(cm.exception))\n+        self.assertIn(\"Invalid block tag\", str(cm.exception))\n+        self.assertIn(\"'endpartialdef outer'\", str(cm.exception))\n+\n+        reporter = ExceptionReporter(None, cm.exception.__class__, cm.exception, None)\n+        traceback_data = reporter.get_traceback_data()\n+\n+        exception_value = str(traceback_data.get(\"exception_value\", \"\"))\n+        self.assertIn(\"Invalid block tag\", exception_value)\n+        self.assertIn(\"'endpartialdef outer'\", str(cm.exception))\ndiff --git a/tests/template_tests/test_partials.py b/tests/template_tests/test_partials.py\nnew file mode 100644\nindex 000000000000..fc6a988d61c0\n--- /dev/null\n+++ b/tests/template_tests/test_partials.py\n@@ -0,0 +1,413 @@\n+import os\n+from unittest import mock\n+\n+from django.http import HttpResponse\n+from django.template import (\n+    Context,\n+    Origin,\n+    Template,\n+    TemplateDoesNotExist,\n+    TemplateSyntaxError,\n+    engines,\n+)\n+from django.template.backends.django import DjangoTemplates\n+from django.template.loader import render_to_string\n+from django.test import TestCase, override_settings\n+from django.urls import path, reverse\n+\n+engine = engines[\"django\"]\n+\n+\n+class PartialTagsTests(TestCase):\n+\n+    def test_invalid_template_name_raises_template_does_not_exist(self):\n+        for template_name in [123, None, \"\", \"#\", \"#name\"]:\n+            with (\n+                self.subTest(template_name=template_name),\n+                self.assertRaisesMessage(TemplateDoesNotExist, str(template_name)),\n+            ):\n+                engine.get_template(template_name)\n+\n+    def test_template_source_is_correct(self):\n+        partial = engine.get_template(\"partial_examples.html#test-partial\")\n+        self.assertEqual(\n+            partial.template.source,\n+            \"{% partialdef test-partial %}\\nTEST-PARTIAL-CONTENT\\n{% endpartialdef %}\",\n+        )\n+\n+    def test_template_source_inline_is_correct(self):\n+        partial = engine.get_template(\"partial_examples.html#inline-partial\")\n+        self.assertEqual(\n+            partial.template.source,\n+            \"{% partialdef inline-partial inline %}\\nINLINE-CONTENT\\n\"\n+            \"{% endpartialdef %}\",\n+        )\n+\n+    def test_full_template_from_loader(self):\n+        template = engine.get_template(\"partial_examples.html\")\n+        rendered = template.render({})\n+\n+        # Check the partial was rendered twice\n+        self.assertEqual(2, rendered.count(\"TEST-PARTIAL-CONTENT\"))\n+        self.assertEqual(1, rendered.count(\"INLINE-CONTENT\"))\n+\n+    def test_chained_exception_forwarded(self):\n+        with self.assertRaises(TemplateDoesNotExist) as ctx:\n+            engine.get_template(\"not_there.html#not-a-partial\")\n+\n+        exception = ctx.exception\n+        self.assertGreater(len(exception.tried), 0)\n+        origin, _ = exception.tried[0]\n+        self.assertEqual(origin.template_name, \"not_there.html\")\n+\n+    def test_partials_use_cached_loader_when_configured(self):\n+        template_dir = os.path.join(os.path.dirname(__file__), \"templates\")\n+        backend = DjangoTemplates(\n+            {\n+                \"NAME\": \"django\",\n+                \"DIRS\": [template_dir],\n+                \"APP_DIRS\": False,\n+                \"OPTIONS\": {\n+                    \"loaders\": [\n+                        (\n+                            \"django.template.loaders.cached.Loader\",\n+                            [\"django.template.loaders.filesystem.Loader\"],\n+                        ),\n+                    ],\n+                },\n+            }\n+        )\n+        cached_loader = backend.engine.template_loaders[0]\n+        filesystem_loader = cached_loader.loaders[0]\n+\n+        with mock.patch.object(\n+            filesystem_loader, \"get_contents\", wraps=filesystem_loader.get_contents\n+        ) as mock_get_contents:\n+            full_template = backend.get_template(\"partial_examples.html\")\n+            self.assertIn(\"TEST-PARTIAL-CONTENT\", full_template.render({}))\n+\n+            partial_template = backend.get_template(\n+                \"partial_examples.html#test-partial\"\n+            )\n+            self.assertEqual(\n+                \"TEST-PARTIAL-CONTENT\", partial_template.render({}).strip()\n+            )\n+\n+            mock_get_contents.assert_called_once()\n+\n+    def test_context_available_in_response_for_partial_template(self):\n+        def sample_view(request):\n+            return HttpResponse(\n+                render_to_string(\"partial_examples.html#test-partial\", {\"foo\": \"bar\"})\n+            )\n+\n+        class PartialUrls:\n+            urlpatterns = [path(\"sample/\", sample_view, name=\"sample-view\")]\n+\n+        with override_settings(ROOT_URLCONF=PartialUrls):\n+            response = self.client.get(reverse(\"sample-view\"))\n+\n+        self.assertContains(response, \"TEST-PARTIAL-CONTENT\")\n+        self.assertEqual(response.context.get(\"foo\"), \"bar\")\n+\n+    def test_response_with_multiple_parts(self):\n+        context = {}\n+        template_partials = [\"partial_child.html\", \"partial_child.html#extra-content\"]\n+\n+        response_whole_content_at_once = HttpResponse(\n+            \"\".join(\n+                render_to_string(template_name, context)\n+                for template_name in template_partials\n+            )\n+        )\n+\n+        response_with_multiple_writes = HttpResponse()\n+        for template_name in template_partials:\n+            response_with_multiple_writes.write(\n+                render_to_string(template_name, context)\n+            )\n+\n+        response_with_generator = HttpResponse(\n+            render_to_string(template_name, context)\n+            for template_name in template_partials\n+        )\n+\n+        for label, response in [\n+            (\"response_whole_content_at_once\", response_whole_content_at_once),\n+            (\"response_with_multiple_writes\", response_with_multiple_writes),\n+            (\"response_with_generator\", response_with_generator),\n+        ]:\n+            with self.subTest(response=label):\n+                self.assertIn(b\"Main Content\", response.content)\n+                self.assertIn(b\"Extra Content\", response.content)\n+\n+    def test_partial_engine_assignment_with_real_template(self):\n+        template_with_partial = engine.get_template(\n+            \"partial_examples.html#test-partial\"\n+        )\n+        self.assertEqual(template_with_partial.template.engine, engine.engine)\n+        rendered_content = template_with_partial.render({})\n+        self.assertEqual(\"TEST-PARTIAL-CONTENT\", rendered_content.strip())\n+\n+\n+class RobustPartialHandlingTests(TestCase):\n+\n+    def override_get_template(self, **kwargs):\n+        class TemplateWithCustomAttrs:\n+            def __init__(self, **kwargs):\n+                for k, v in kwargs.items():\n+                    setattr(self, k, v)\n+\n+            def render(self, context):\n+                return \"rendered content\"\n+\n+        template = TemplateWithCustomAttrs(**kwargs)\n+        origin = mock.Mock()\n+        return mock.patch.object(\n+            engine.engine,\n+            \"find_template\",\n+            return_value=(template, origin),\n+        )\n+\n+    def test_template_without_extra_data_attribute(self):\n+        partial_name = \"some_partial_name\"\n+        with (\n+            self.override_get_template(),\n+            self.assertRaisesMessage(TemplateDoesNotExist, partial_name),\n+        ):\n+            engine.get_template(f\"some_template.html#{partial_name}\")\n+\n+    def test_template_extract_extra_data_robust(self):\n+        partial_name = \"some_partial_name\"\n+        for extra_data in (\n+            None,\n+            0,\n+            [],\n+            {},\n+            {\"partials\": {}},\n+            {\"template-partials\": None},\n+            {\"template-partials\": {}},\n+            {\"template-partials\": []},\n+            {\"template-partials\": 0},\n+        ):\n+            with (\n+                self.subTest(extra_data=extra_data),\n+                self.override_get_template(extra_data=extra_data),\n+                self.assertRaisesMessage(TemplateDoesNotExist, partial_name),\n+            ):\n+                engine.get_template(f\"template.html#{partial_name}\")\n+\n+    def test_nested_partials_rendering_with_context(self):\n+        template_source = \"\"\"\n+        {% partialdef outer inline %}\n+            Hello {{ name }}!\n+            {% partialdef inner inline %}\n+                Your age is {{ age }}.\n+            {% endpartialdef inner %}\n+            Nice to meet you.\n+        {% endpartialdef outer %}\n+        \"\"\"\n+        template = Template(template_source, origin=Origin(name=\"template.html\"))\n+\n+        context = Context({\"name\": \"Alice\", \"age\": 25})\n+        rendered = template.render(context)\n+\n+        self.assertIn(\"Hello Alice!\", rendered)\n+        self.assertIn(\"Your age is 25.\", rendered)\n+        self.assertIn(\"Nice to meet you.\", rendered)\n+\n+\n+class FindPartialSourceTests(TestCase):\n+\n+    def test_find_partial_source_success(self):\n+        template = engine.get_template(\"partial_examples.html\").template\n+        partial_proxy = template.extra_data[\"template-partials\"][\"test-partial\"]\n+\n+        expected = \"\"\"{% partialdef test-partial %}\n+TEST-PARTIAL-CONTENT\n+{% endpartialdef %}\"\"\"\n+        self.assertEqual(partial_proxy.source.strip(), expected.strip())\n+\n+    def test_find_partial_source_with_inline(self):\n+        template = engine.get_template(\"partial_examples.html\").template\n+        partial_proxy = template.extra_data[\"template-partials\"][\"inline-partial\"]\n+\n+        expected = \"\"\"{% partialdef inline-partial inline %}\n+INLINE-CONTENT\n+{% endpartialdef %}\"\"\"\n+        self.assertEqual(partial_proxy.source.strip(), expected.strip())\n+\n+    def test_find_partial_source_nonexistent_partial(self):\n+        template = engine.get_template(\"partial_examples.html\").template\n+        partial_proxy = template.extra_data[\"template-partials\"][\"test-partial\"]\n+\n+        result = partial_proxy.find_partial_source(\n+            template.source, \"nonexistent-partial\"\n+        )\n+        self.assertEqual(result, \"\")\n+\n+    def test_find_partial_source_empty_partial(self):\n+        template_source = \"{% partialdef empty %}{% endpartialdef %}\"\n+        template = Template(template_source)\n+        partial_proxy = template.extra_data[\"template-partials\"][\"empty\"]\n+\n+        result = partial_proxy.find_partial_source(template_source, \"empty\")\n+        self.assertEqual(result, \"{% partialdef empty %}{% endpartialdef %}\")\n+\n+    def test_find_partial_source_multiple_consecutive_partials(self):\n+\n+        template_source = (\n+            \"{% partialdef empty %}{% endpartialdef %}\"\n+            \"{% partialdef other %}...{% endpartialdef %}\"\n+        )\n+        template = Template(template_source)\n+\n+        empty_proxy = template.extra_data[\"template-partials\"][\"empty\"]\n+        other_proxy = template.extra_data[\"template-partials\"][\"other\"]\n+\n+        empty_result = empty_proxy.find_partial_source(template_source, \"empty\")\n+        self.assertEqual(empty_result, \"{% partialdef empty %}{% endpartialdef %}\")\n+\n+        other_result = other_proxy.find_partial_source(template_source, \"other\")\n+        self.assertEqual(other_result, \"{% partialdef other %}...{% endpartialdef %}\")\n+\n+    def test_partials_with_duplicate_names(self):\n+        test_cases = [\n+            (\n+                \"nested\",\n+                \"\"\"\n+                {% partialdef duplicate %}{% partialdef duplicate %}\n+                CONTENT\n+                {% endpartialdef %}{% endpartialdef %}\n+                \"\"\",\n+            ),\n+            (\n+                \"conditional\",\n+                \"\"\"\n+                {% if ... %}\n+                  {% partialdef duplicate %}\n+                  CONTENT\n+                  {% endpartialdef %}\n+                {% else %}\n+                  {% partialdef duplicate %}\n+                  OTHER-CONTENT\n+                  {% endpartialdef %}\n+                {% endif %}\n+                \"\"\",\n+            ),\n+        ]\n+\n+        for test_name, template_source in test_cases:\n+            with self.subTest(test_name=test_name):\n+                with self.assertRaisesMessage(\n+                    TemplateSyntaxError,\n+                    \"Partial 'duplicate' is already defined in the \"\n+                    \"'template.html' template.\",\n+                ):\n+                    Template(template_source, origin=Origin(name=\"template.html\"))\n+\n+    def test_find_partial_source_supports_named_end_tag(self):\n+        template_source = \"{% partialdef thing %}CONTENT{% endpartialdef thing %}\"\n+        template = Template(template_source)\n+        partial_proxy = template.extra_data[\"template-partials\"][\"thing\"]\n+\n+        result = partial_proxy.find_partial_source(template_source, \"thing\")\n+        self.assertEqual(\n+            result, \"{% partialdef thing %}CONTENT{% endpartialdef thing %}\"\n+        )\n+\n+    def test_find_partial_source_supports_nested_partials(self):\n+        template_source = (\n+            \"{% partialdef outer %}\"\n+            \"{% partialdef inner %}...{% endpartialdef %}\"\n+            \"{% endpartialdef %}\"\n+        )\n+        template = Template(template_source)\n+\n+        empty_proxy = template.extra_data[\"template-partials\"][\"outer\"]\n+        other_proxy = template.extra_data[\"template-partials\"][\"inner\"]\n+\n+        outer_result = empty_proxy.find_partial_source(template_source, \"outer\")\n+        self.assertEqual(\n+            outer_result,\n+            (\n+                \"{% partialdef outer %}{% partialdef inner %}\"\n+                \"...{% endpartialdef %}{% endpartialdef %}\"\n+            ),\n+        )\n+\n+        inner_result = other_proxy.find_partial_source(template_source, \"inner\")\n+        self.assertEqual(inner_result, \"{% partialdef inner %}...{% endpartialdef %}\")\n+\n+    def test_find_partial_source_supports_nested_partials_and_named_end_tags(self):\n+        template_source = (\n+            \"{% partialdef outer %}\"\n+            \"{% partialdef inner %}...{% endpartialdef inner %}\"\n+            \"{% endpartialdef outer %}\"\n+        )\n+        template = Template(template_source)\n+\n+        empty_proxy = template.extra_data[\"template-partials\"][\"outer\"]\n+        other_proxy = template.extra_data[\"template-partials\"][\"inner\"]\n+\n+        outer_result = empty_proxy.find_partial_source(template_source, \"outer\")\n+        self.assertEqual(\n+            outer_result,\n+            (\n+                \"{% partialdef outer %}{% partialdef inner %}\"\n+                \"...{% endpartialdef inner %}{% endpartialdef outer %}\"\n+            ),\n+        )\n+\n+        inner_result = other_proxy.find_partial_source(template_source, \"inner\")\n+        self.assertEqual(\n+            inner_result, \"{% partialdef inner %}...{% endpartialdef inner %}\"\n+        )\n+\n+    def test_find_partial_source_supports_nested_partials_and_mixed_end_tags_1(self):\n+        template_source = (\n+            \"{% partialdef outer %}\"\n+            \"{% partialdef inner %}...{% endpartialdef %}\"\n+            \"{% endpartialdef outer %}\"\n+        )\n+        template = Template(template_source)\n+\n+        empty_proxy = template.extra_data[\"template-partials\"][\"outer\"]\n+        other_proxy = template.extra_data[\"template-partials\"][\"inner\"]\n+\n+        outer_result = empty_proxy.find_partial_source(template_source, \"outer\")\n+        self.assertEqual(\n+            outer_result,\n+            (\n+                \"{% partialdef outer %}{% partialdef inner %}\"\n+                \"...{% endpartialdef %}{% endpartialdef outer %}\"\n+            ),\n+        )\n+\n+        inner_result = other_proxy.find_partial_source(template_source, \"inner\")\n+        self.assertEqual(inner_result, \"{% partialdef inner %}...{% endpartialdef %}\")\n+\n+    def test_find_partial_source_supports_nested_partials_and_mixed_end_tags_2(self):\n+        template_source = (\n+            \"{% partialdef outer %}\"\n+            \"{% partialdef inner %}...{% endpartialdef inner %}\"\n+            \"{% endpartialdef %}\"\n+        )\n+        template = Template(template_source)\n+\n+        empty_proxy = template.extra_data[\"template-partials\"][\"outer\"]\n+        other_proxy = template.extra_data[\"template-partials\"][\"inner\"]\n+\n+        outer_result = empty_proxy.find_partial_source(template_source, \"outer\")\n+        self.assertEqual(\n+            outer_result,\n+            (\n+                \"{% partialdef outer %}{% partialdef inner %}\"\n+                \"...{% endpartialdef inner %}{% endpartialdef %}\"\n+            ),\n+        )\n+\n+        inner_result = other_proxy.find_partial_source(template_source, \"inner\")\n+        self.assertEqual(\n+            inner_result, \"{% partialdef inner %}...{% endpartialdef inner %}\"\n+        )\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR introduces new syntax tests and aligns them with existing ones, particularly around partial definitions. The renaming of `TemplateProxy` to `PartialTemplate` and its relocation to `base.py` improves clarity. Shifting `SubDictionaryWrapper` to `utils.py` suggests a more organized structure. However, while you've identified template exceptions, ensure the test case for `get_exception_info` truly validates error handlingadding specific asserts will enhance clarity. Additionally, test performance with larger templates to catch potential regex inefficiencies. Great progress overall!"
      }
    ],
    "codediff_length": 53181
  },
  {
    "index": 19,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/tests/delete/models.py b/tests/delete/models.py\nindex 4b627712bb65..540fc1db01e1 100644\n--- a/tests/delete/models.py\n+++ b/tests/delete/models.py\n@@ -241,3 +252,99 @@ class GenericDeleteBottomParent(models.Model):\n     generic_delete_bottom = models.ForeignKey(\n         GenericDeleteBottom, on_delete=models.CASCADE\n     )\n+\n+\n+class ParentModel(models.Model):\n+    pass\n+\n+\n+class ChildModel(models.Model):\n+    \"\"\"First level foreignkey child for ParentModel\n+    Implemented using database level cascading\"\"\"\n+\n+    parent_model = models.ForeignKey(ParentModel, on_delete=models.DB_CASCADE)\n+\n+\n+class GrandChild(models.Model):\n+    \"\"\"Second level foreignkey child for ParentModel\n+    Implemented using in DB cascading\"\"\"\n+\n+    child_model = models.ForeignKey(ChildModel, on_delete=models.DB_CASCADE)\n+\n+\n+class ChildFKRestrict(models.Model):\n+    \"\"\"First level child of ParentModel with cascading set to restrict\"\"\"\n+\n+    parent_model = models.ForeignKey(ParentModel, on_delete=models.DB_RESTRICT)\n+\n+\n+class GrandChildFKRestrict(models.Model):\n+    \"\"\"Second level child of ParentModel with cascading set to restrict\"\"\"\n+\n+    child_model = models.ForeignKey(ChildModel, on_delete=models.DB_RESTRICT)\n+\n+\n+class ChildFKSetNull(models.Model):\n+    \"\"\"First level child of ParentModel with cascading set to null\"\"\"\n+\n+    parent_model = models.ForeignKey(\n+        ParentModel, on_delete=models.DB_SET_NULL, null=True\n+    )\n+    another_field = models.CharField(max_length=20)\n+\n+\n+class GrandChildFKSetNull(models.Model):\n+    \"\"\"Second level child of ParentModel with cascading set to null\"\"\"\n+\n+    child_model = models.ForeignKey(\n+        ChildModel,\n+        on_delete=models.DB_SET_NULL,\n+        null=True,\n+    )\n+    another_field = models.CharField(max_length=20)\n+\n+\n+class AnotherGrandChildFKSetNull(models.Model):\n+    \"\"\"Second level child of ParentModel with cascading set to null\"\"\"\n+\n+    child_fk_set_null = models.ForeignKey(\n+        ChildFKSetNull, on_delete=models.DB_SET_NULL, null=True\n+    )\n+    another_field = models.CharField(max_length=20)\n+\n+\n+class DBLevelChild(ParentModel):\n+    grandparent_ptr = models.OneToOneField(\n+        ParentModel, primary_key=True, parent_link=True, on_delete=models.DB_RESTRICT\n+    )\n+\n+\n+class NormalParent(ParentModel):\n+    grandparent_ptr = models.OneToOneField(\n+        ParentModel, primary_key=True, parent_link=True, on_delete=models.DB_CASCADE\n+    )\n+\n+\n+class DiamondParent(ParentModel):\n+    gp_ptr = models.OneToOneField(\n+        ParentModel, primary_key=True, parent_link=True, on_delete=models.DB_CASCADE\n+    )\n+\n+\n+class DiamondChild(NormalParent, DiamondParent):\n+    parent_ptr = models.OneToOneField(\n+        NormalParent, primary_key=True, parent_link=True, on_delete=models.DB_CASCADE\n+    )\n+\n+    diamondparent_ptr = models.OneToOneField(\n+        DiamondParent, parent_link=True, on_delete=models.DB_CASCADE\n+    )\n+\n+\n+class DBDefaultsFK(models.Model):\n+    parent_model = models.ForeignKey(\n+        ParentModel, db_default=1, on_delete=models.DB_SET_DEFAULT\n+    )\n+\n+    class Meta:\n+        required_db_features = (\"has_on_delete_db_default\",)\ndiff --git a/tests/delete/tests.py b/tests/delete/tests.py\nindex 01228631f4ba..f84831dea79e 100644\n--- a/tests/delete/tests.py\n+++ b/tests/delete/tests.py\n@@ -800,3 +838,189 @@ def test_fast_delete_full_match(self):\n         with self.assertNumQueries(1):\n             User.objects.filter(~Q(pk__in=[]) | Q(avatar__desc=\"foo\")).delete()\n         self.assertFalse(User.objects.exists())\n+\n+\n+class DatabaseLevelOnDeleteTests(TestCase):\n+    def test_deletion_on_nested_cascades(self):\n+        parent = ParentModel.objects.create()\n+        child = ChildModel.objects.create(parent_model=parent)\n+        grand_child = GrandChild.objects.create(child_model=child)\n+\n+        parent.delete()\n+\n+        self.assertIs(ChildModel.objects.filter(pk=child.pk).exists(), False)\n+        self.assertIs(GrandChild.objects.filter(pk=grand_child.pk).exists(), False)\n+\n+    def test_restricted_deletion(self):\n+        parent = ParentModel.objects.create()\n+        ChildFKRestrict.objects.create(parent_model=parent)\n+\n+        with self.assertRaises(IntegrityError):\n+            parent.delete()\n+\n+    def test_restricted_deletion_by_cascade(self):\n+        parent = ParentModel.objects.create()\n+        child = ChildModel.objects.create(parent_model=parent)\n+        GrandChildFKRestrict.objects.create(child_model=child)\n+        with self.assertRaises(IntegrityError):\n+            parent.delete()\n+\n+    def test_deletion_on_set_null(self):\n+        parent = ParentModel.objects.create()\n+        child = ChildFKSetNull.objects.create(\n+            parent_model=parent, another_field=\"Some Value\"\n+        )\n+        parent.delete()\n+        orphan_child = ChildFKSetNull.objects.get(pk=child.pk)\n+        self.assertEqual(child.pk, orphan_child.pk)\n+        self.assertEqual(child.another_field, orphan_child.another_field)\n+        self.assertNotEqual(child.parent_model, orphan_child.parent_model)\n+        self.assertIsNone(orphan_child.parent_model)\n+\n+    def test_set_null_on_cascade_deletion(self):\n+        parent = ParentModel.objects.create()\n+        child = ChildModel.objects.create(parent_model=parent)\n+        grand_child = GrandChildFKSetNull.objects.create(\n+            child_model=child, another_field=\"Some Value\"\n+        )\n+        parent.delete()\n+        orphan_grand_child = GrandChildFKSetNull.objects.get(pk=grand_child.pk)\n+        self.assertEqual(grand_child.pk, orphan_grand_child.pk)\n+        self.assertEqual(grand_child.another_field, orphan_grand_child.another_field)\n+        self.assertNotEqual(grand_child.child_model, orphan_grand_child.child_model)\n+        self.assertIsNone(orphan_grand_child.child_model)\n+\n+    def test_nested_set_null_on_deletion(self):\n+        parent = ParentModel.objects.create()\n+        child = ChildFKSetNull.objects.create(parent_model=parent)\n+        grand_child = AnotherGrandChildFKSetNull.objects.create(child_fk_set_null=child)\n+        parent.delete()\n+\n+        orphan_child = ChildFKSetNull.objects.get(pk=child.pk)\n+        self.assertEqual(child.pk, orphan_child.pk)\n+        self.assertEqual(child.another_field, orphan_child.another_field)\n+        self.assertNotEqual(child.parent_model, orphan_child.parent_model)\n+        self.assertIsNone(orphan_child.parent_model)\n+\n+        orphan_grand_child = AnotherGrandChildFKSetNull.objects.get(pk=grand_child.pk)\n+        self.assertEqual(grand_child.pk, orphan_grand_child.pk)\n+        self.assertEqual(grand_child.another_field, orphan_grand_child.another_field)\n+        self.assertEqual(\n+            grand_child.child_fk_set_null, orphan_grand_child.child_fk_set_null\n+        )\n+        self.assertIsNotNone(orphan_grand_child.child_fk_set_null)\n+\n+    @skipUnlessDBFeature(\"has_on_delete_db_default\")\n+    def test_foreign_key_db_default_exists(self):\n+        # Default parent exists\n+        default_parent = ParentModel.objects.create(pk=1)\n+        parent = ParentModel.objects.create(pk=2)\n+        child1 = DBDefaultsFK.objects.create(parent_model=parent)\n+        with self.assertNumQueries(1):\n+            parent.delete()\n+        child1.refresh_from_db()\n+        self.assertEqual(child1.parent_model, default_parent)\n+        child1.delete()\n+        default_parent.delete()\n+\n+\n+class DatabaseLevelOnDeleteTransactionTests(TransactionTestCase):\n+    available_apps = [\"delete\"]\n+\n+    @skipUnlessDBFeature(\"has_on_delete_db_default\")\n+    def test_foreign_key_default_parent_deleted_before_parent(self):\n+        # Default parent does not exists\n+        # Default parent deleted before parent\n+        default_parent = ParentModel.objects.create(pk=1)\n+        parent = ParentModel.objects.create(pk=2)\n+        child1 = DBDefaultsFK.objects.create(parent_model=parent)\n+        default_parent.delete()\n+        with self.assertRaises(IntegrityError):\n+            parent.delete()\n+            child1.refresh_from_db()\n+\n+    @skipUnlessDBFeature(\"has_on_delete_db_default\")\n+    def test_foreign_key_default_parent_deleted_after_parent(self):\n+        # Default parent does not exists\n+        # Default parent deleted after parent\n+        default_parent = ParentModel.objects.create(pk=1)\n+        parent = ParentModel.objects.create(pk=2)\n+        child1 = DBDefaultsFK.objects.create(parent_model=parent)\n+        parent.delete()\n+        with self.assertRaises(IntegrityError):\n+            default_parent.delete()\n+            child1.refresh_from_db()\n+\n+\n+class DatabaseLevelOnDeleteQueryAssertionTests(TestCase):\n+    def test_queries_on_nested_cascade(self):\n+        parent = ParentModel.objects.create()\n+\n+        for i in range(3):\n+            ChildModel.objects.create(parent_model=parent)\n+\n+        for child in ChildModel.objects.all():\n+            for i in range(3):\n+                GrandChild.objects.create(child_model=child)\n+\n+        # one is the deletion\n+        with self.assertNumQueries(1):\n+            parent.delete()\n+\n+    def test_queries_on_nested_set_null(self):\n+        parent = ParentModel.objects.create()\n+\n+        for i in range(3):\n+            ChildFKSetNull.objects.create(parent_model=parent)\n+\n+        for child_fk_set_null in ChildFKSetNull.objects.all():\n+            for i in range(3):\n+                AnotherGrandChildFKSetNull.objects.create(\n+                    child_fk_set_null=child_fk_set_null\n+                )\n+\n+        # one is the deletion\n+        with self.assertNumQueries(1):\n+            parent.delete()\n+\n+    def test_queries_on_nested_set_null_cascade(self):\n+        parent = ParentModel.objects.create()\n+\n+        for i in range(3):\n+            ChildModel.objects.create(parent_model=parent)\n+\n+        for child in ChildModel.objects.all():\n+            for i in range(3):\n+                GrandChildFKSetNull.objects.create(child_model=child)\n+\n+        # one is the deletion\n+        with self.assertNumQueries(1):\n+            parent.delete()\n+\n+    def test_queries_on_inherited_model(self):\n+        gp = ParentModel.objects.create()\n+        parent = NormalParent.objects.create(grandparent_ptr=gp)\n+        diamond_parent = DiamondParent.objects.create(gp_ptr=gp)\n+\n+        dc = DiamondChild.objects.create(\n+            parent_ptr=parent, diamondparent_ptr=diamond_parent\n+        )\n+\n+        with self.assertNumQueries(1):\n+            gp.delete()\n+\n+        self.assertIs(NormalParent.objects.filter(pk=parent.pk).exists(), False)\n+        self.assertIs(\n+            DiamondParent.objects.filter(pk=diamond_parent.pk).exists(), False\n+        )\n+        self.assertIs(DiamondChild.objects.filter(pk=dc.pk).exists(), False)\n+\n+    def test_restrict_on_inherited_model(self):\n+        gp = ParentModel.objects.create()\n+        child = DBLevelChild.objects.create(grandparent_ptr=gp)\n+\n+        with transaction.atomic():\n+            with self.assertRaises(IntegrityError):\n+                gp.delete()\n+\n+        self.assertIs(DBLevelChild.objects.filter(pk=child.pk).exists(), True)\ndiff --git a/tests/invalid_models_tests/test_relative_fields.py b/tests/invalid_models_tests/test_relative_fields.py\nindex 9b69ae415138..9f6874aa0084 100644\n--- a/tests/invalid_models_tests/test_relative_fields.py\n+++ b/tests/invalid_models_tests/test_relative_fields.py\n@@ -2056,3 +2058,174 @@ class Child(models.Model):\n                 ),\n             ],\n         )\n+\n+\n+@isolate_apps(\"invalid_models_tests\")\n+class DatabaseLevelCascadeCheckTests(SimpleTestCase):\n+    def test_system_check_on_nested_db_with_non_db_cascading(self):\n+        class Foo(models.Model):\n+            pass\n+\n+        class Bar(models.Model):\n+            foo = models.ForeignKey(\n+                Foo,\n+                on_delete=models.DB_CASCADE,\n+            )\n+\n+        class BadBar(models.Model):\n+            foo = models.ForeignKey(Foo, on_delete=models.CASCADE)\n+\n+        class Baz(models.Model):\n+            \"\"\"First level child\"\"\"\n+\n+            bar = models.ForeignKey(\n+                Bar,\n+                on_delete=models.CASCADE,\n+            )\n+\n+        baz_field = Baz._meta.get_field(\"bar\")\n+        related_model_status = {\"model\": Bar, \"field\": Bar._meta.get_field(\"foo\")}\n+        self.assertEqual(\n+            baz_field.check(),\n+            [\n+                Error(\n+                    \"Using python based on_delete with database \"\n+                    \"level on_delete referenced model is prohibited \"\n+                    f\"Related field is {related_model_status.get('field')}.\",\n+                    hint=\"Use database-level cascading for foreign keys\",\n+                    obj=baz_field,\n+                    id=\"fields.E323\",\n+                ),\n+            ],\n+        )\n+\n+        bad_bar_field = BadBar._meta.get_field(\"foo\")\n+        self.assertEqual(bad_bar_field.check(), [])\n+\n+        bar_field = Bar._meta.get_field(\"foo\")\n+        self.assertEqual(bar_field.check(), [])\n+\n+    def test_null_condition_with_set_null_db(self):\n+        class Foo(models.Model):\n+            pass\n+\n+        class SetNullDbNotNullModel(models.Model):\n+            foo = models.ForeignKey(\n+                Foo,\n+                on_delete=models.DB_SET_NULL,\n+            )\n+\n+            class Meta:\n+                managed = False\n+\n+        field = SetNullDbNotNullModel._meta.get_field(\"foo\")\n+        self.assertEqual(\n+            field.check(),\n+            [\n+                Error(\n+                    \"Field specifies on_delete=DB_SET_NULL, but cannot be null.\",\n+                    hint=(\n+                        \"Set null=True argument on the field, or change the \"\n+                        \"on_delete rule.\"\n+                    ),\n+                    obj=field,\n+                    id=\"fields.E320\",\n+                )\n+            ],\n+        )\n+\n+    def test_check_on_inherited_models(self):\n+        class GrandParent(models.Model):\n+            pass\n+\n+        class Parent(GrandParent):\n+            pass\n+\n+        class DiamondParent(GrandParent):\n+            gp_ptr = models.OneToOneField(\n+                GrandParent,\n+                primary_key=True,\n+                parent_link=True,\n+                on_delete=models.DB_CASCADE,\n+            )\n+\n+        class DiamondChild(Parent, DiamondParent):\n+            parent_ptr = models.OneToOneField(\n+                Parent, primary_key=True, parent_link=True, on_delete=models.DB_CASCADE\n+            )\n+\n+        field = DiamondChild._meta.get_field(\"diamondparent_ptr\")\n+        rel_field = DiamondParent._meta.get_field(\"gp_ptr\")\n+        # error for having DB_CASCADE over normal cascade\n+        self.assertEqual(\n+            field.check(),\n+            [\n+                Error(\n+                    \"Using python based on_delete with database \"\n+                    \"level on_delete referenced model is prohibited \"\n+                    f\"Related field is {rel_field}.\",\n+                    hint=\"Use database-level cascading for foreign keys\",\n+                    obj=field,\n+                    id=\"fields.E323\",\n+                )\n+            ],\n+        )\n+        # No error for entire django cascade\n+        field = Parent._meta.get_field(\"grandparent_ptr\")\n+        self.assertEqual(\n+            field.check(),\n+            [],\n+        )\n+\n+    def test_check_on_generic_foreign_key(self):\n+        class SomeModel(models.Model):\n+            some_fk = models.ForeignKey(\n+                ContentType,\n+                on_delete=models.DB_CASCADE,\n+                related_name=\"ctcmnt\",\n+            )\n+            object_id = models.PositiveIntegerField()\n+            content_object = GenericForeignKey(\"some_fk\", \"object_id\")\n+\n+            class Meta:\n+                abstract = True\n+\n+        comment_field = SomeModel._meta.get_field(\"some_fk\")\n+        self.assertIn(\n+            Error(\n+                \"Field specifies unsupported on_delete=DB_CASCADE on model \"\n+                \"declaring a GenericForeignKey.\",\n+                hint=\"Change the on_delete rule to a non DB_* method\",\n+                obj=comment_field,\n+                id=\"fields.E322\",\n+            ),\n+            comment_field.check(),\n+        )\n+\n+    def test_check_db_default_foreign_key(self):\n+        class DBDefaultsPK(models.Model):\n+            language_code = models.CharField(\n+                primary_key=True, max_length=2, db_default=\"en\"\n+            )\n+\n+        class DBDefaultsFK(models.Model):\n+            language_code = models.ForeignKey(\n+                DBDefaultsPK, on_delete=models.DB_SET_DEFAULT\n+            )\n+\n+            class Meta:\n+                abstract = True\n+\n+        fk_field = DBDefaultsFK._meta.get_field(\"language_code\")\n+        self.assertEqual(\n+            fk_field.check(),\n+            [\n+                Error(\n+                    \"Field specifies on_delete=DB_SET_DEFAULT, but has \"\n+                    \"no db_default value.\",\n+                    hint=\"Set a db_default value, or change the on_delete rule.\",\n+                    obj=fk_field,\n+                    id=\"fields.E324\",\n+                )\n+            ],\n+        )\ndiff --git a/tests/migrations/test_operations.py b/tests/migrations/test_operations.py\nindex f86550082995..4dc130975dcf 100644\n--- a/tests/migrations/test_operations.py\n+++ b/tests/migrations/test_operations.py\n@@ -6376,3 +6376,1265 @@ class BaseOperationTests(SimpleTestCase):\n     def test_formatted_description_no_category(self):\n         operation = Operation()\n         self.assertEqual(operation.formatted_description(), \"? Operation: ((), {})\")\n+\n+\n+class DatabaseLevelOnDeleteMigrationTests(OperationTestBase):\n+    def test_create_model_with_db_level_fk(self):\n+        app_label = \"test_cmwdblfk\"\n+        operations = [\n+            migrations.CreateModel(\n+                \"Pony\",\n+                [\n+                    (\n+                        \"id\",\n+                        models.CharField(\n+                            primary_key=True,\n+                            max_length=10,\n+                        ),\n+                    ),\n+                ],\n+            )\n+        ]\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+        # ForeignKey.\n+        new_state = project_state.clone()\n+        operation_list = [\n+            (\"id\", models.AutoField(primary_key=True)),\n+            (\"number\", models.IntegerField(default=1)),\n+            (\n+                \"pony_cascade\",\n+                models.ForeignKey(f\"{app_label}.Pony\", on_delete=models.DB_CASCADE),\n+            ),\n+            (\n+                \"pony_set_null\",\n+                models.ForeignKey(\n+                    f\"{app_label}.Pony\", null=True, on_delete=models.DB_SET_NULL\n+                ),\n+            ),\n+            (\n+                \"pony_restrict\",\n+                models.ForeignKey(f\"{app_label}.Pony\", on_delete=models.DB_RESTRICT),\n+            ),\n+        ]\n+        if connection.features.has_on_delete_db_default:\n+            operation_list.append(\n+                (\n+                    \"pony_default\",\n+                    models.ForeignKey(\n+                        f\"{app_label}.Pony\",\n+                        db_default=\"bn\",\n+                        on_delete=models.DB_SET_DEFAULT,\n+                    ),\n+                )\n+            )\n+        operation = migrations.CreateModel(\"Rider\", operation_list)\n+        operation.state_forwards(app_label, new_state)\n+        self.assertTableNotExists(f\"{app_label}_rider\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+        self.assertTableExists(f\"{app_label}_rider\")\n+        self.assertColumnExists(f\"{app_label}_rider\", \"pony_cascade_id\")\n+        self.assertColumnExists(f\"{app_label}_rider\", \"pony_set_null_id\")\n+        self.assertColumnExists(f\"{app_label}_rider\", \"pony_restrict_id\")\n+        if connection.features.has_on_delete_db_default:\n+            self.assertColumnExists(f\"{app_label}_rider\", \"pony_default_id\")\n+\n+    def test_add_field_with_db_cascade(self):\n+        app_label = \"test_add_field_with_db_level_fk\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fk_parent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\n+                        \"id\",\n+                        models.CharField(\n+                            primary_key=True,\n+                            max_length=10,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                ],\n+            ),\n+        ]\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+        operation = migrations.AddField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\", on_delete=models.DB_CASCADE\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+        self.assertColumnNotExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+        self.assertColumnExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance1 = fk_child_model.objects.create(**kwargs)\n+        fk_child_instance2 = fk_child_model.objects.create(**kwargs)\n+\n+        with self.assertNumQueries(1):\n+            fk_parent_instance.delete()\n+\n+        self.assertIs(\n+            fk_child_model.objects.filter(pk=fk_child_instance1.pk).exists(), False\n+        )\n+        self.assertIs(\n+            fk_child_model.objects.filter(pk=fk_child_instance2.pk).exists(), False\n+        )\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+        self.assertColumnNotExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+    def test_add_field_with_db_set_null(self):\n+        app_label = \"test_add_field_with_db_set_null\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fk_parent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\n+                        \"id\",\n+                        models.CharField(\n+                            primary_key=True,\n+                            max_length=10,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                ],\n+            ),\n+        ]\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+        operation = migrations.AddField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                null=True,\n+                on_delete=models.DB_SET_NULL,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+        self.assertColumnNotExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+        self.assertColumnExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance1 = fk_child_model.objects.create(**kwargs)\n+\n+        with self.assertNumQueries(1):\n+            fk_parent_instance.delete()\n+\n+        fk_child_instance1.refresh_from_db()\n+        self.assertIsNone(getattr(fk_child_instance1, fk_fieldname))\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+        self.assertColumnNotExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+    @skipUnlessDBFeature(\"has_on_delete_db_default\")\n+    def test_add_field_with_db_set_default(self):\n+        app_label = \"test_add_field_with_db_set_default\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fk_parent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\n+                        \"id\",\n+                        models.CharField(\n+                            primary_key=True,\n+                            max_length=10,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                ],\n+            ),\n+        ]\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+        operation = migrations.AddField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                db_default=1,\n+                on_delete=models.DB_SET_DEFAULT,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+        self.assertColumnNotExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+        self.assertColumnExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_default = fk_parent_model.objects.create(id=1)\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance = fk_child_model.objects.create(**kwargs)\n+\n+        fk_parent_instance.delete()\n+\n+        fk_child_instance.refresh_from_db()\n+        fk_parent_default.refresh_from_db()\n+        # After deletion it gets set to db default value\n+        self.assertEqual(getattr(fk_child_instance, fk_fieldname), fk_parent_default)\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+        self.assertColumnNotExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+    def test_add_field_with_db_restrict(self):\n+        app_label = \"test_add_field_with_db_restrict\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fk_parent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\n+                        \"id\",\n+                        models.CharField(\n+                            primary_key=True,\n+                            max_length=10,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                ],\n+            ),\n+        ]\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+        operation = migrations.AddField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\", on_delete=models.DB_RESTRICT\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+        self.assertColumnNotExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+        self.assertColumnExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance = fk_child_model.objects.create(**kwargs)\n+\n+        with self.assertRaises(IntegrityError):\n+            fk_parent_instance.delete()\n+\n+        self.assertIs(\n+            fk_child_model.objects.filter(pk=fk_child_instance.pk).exists(), True\n+        )\n+        self.assertIs(\n+            fk_parent_model.objects.filter(pk=fk_parent_instance.pk).exists(), True\n+        )\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+        self.assertColumnNotExists(\n+            f\"{app_label}_{fk_child_modelname.lower()}\", f\"{fk_fieldname}_id\"\n+        )\n+\n+    def change_to_db_set_null_test(\n+        self,\n+        app_label,\n+        fk_parent_modelname,\n+        fk_child_modelname,\n+        fk_fieldname,\n+        project_state,\n+    ):\n+        operation = migrations.AlterField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                null=True,\n+                on_delete=models.DB_SET_NULL,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance1 = fk_child_model.objects.create(**kwargs)\n+\n+        with self.assertNumQueries(1):\n+            fk_parent_instance.delete()\n+\n+        fk_child_instance1.refresh_from_db()\n+        self.assertIsNone(getattr(fk_child_instance1, fk_fieldname))\n+\n+        fk_parent_model.objects.all().delete()\n+        fk_child_model.objects.all().delete()\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+\n+    def change_to_db_cascade_test(\n+        self,\n+        app_label,\n+        fk_parent_modelname,\n+        fk_child_modelname,\n+        fk_fieldname,\n+        project_state,\n+    ):\n+        operation = migrations.AlterField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                on_delete=models.DB_CASCADE,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance1 = fk_child_model.objects.create(**kwargs)\n+        fk_child_instance2 = fk_child_model.objects.create(**kwargs)\n+\n+        with self.assertNumQueries(1):\n+            fk_parent_instance.delete()\n+\n+        self.assertIs(\n+            fk_child_model.objects.filter(pk=fk_child_instance1.pk).exists(), False\n+        )\n+        self.assertIs(\n+            fk_child_model.objects.filter(pk=fk_child_instance2.pk).exists(), False\n+        )\n+\n+        fk_parent_model.objects.all().delete()\n+        fk_child_model.objects.all().delete()\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+\n+    def change_to_db_restrict_test(\n+        self,\n+        app_label,\n+        fk_parent_modelname,\n+        fk_child_modelname,\n+        fk_fieldname,\n+        project_state,\n+    ):\n+        operation = migrations.AlterField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                on_delete=models.DB_RESTRICT,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance = fk_child_model.objects.create(**kwargs)\n+\n+        with self.assertRaises(IntegrityError):\n+            fk_parent_instance.delete()\n+\n+        self.assertIs(\n+            fk_child_model.objects.filter(pk=fk_child_instance.pk).exists(), True\n+        )\n+        self.assertIs(\n+            fk_parent_model.objects.filter(pk=fk_parent_instance.pk).exists(), True\n+        )\n+\n+        fk_child_model.objects.all().delete()\n+        fk_parent_model.objects.all().delete()\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+\n+    def change_to_db_set_default_test(\n+        self,\n+        app_label,\n+        fk_parent_modelname,\n+        fk_child_modelname,\n+        fk_fieldname,\n+        project_state,\n+    ):\n+        operation = migrations.AlterField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                db_default=1,\n+                on_delete=models.DB_SET_DEFAULT,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_default = fk_parent_model.objects.create(id=1)\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance = fk_child_model.objects.create(**kwargs)\n+\n+        fk_parent_instance.delete()\n+\n+        fk_child_instance.refresh_from_db()\n+        fk_parent_default.refresh_from_db()\n+        # After deletion it gets set to db default value\n+        self.assertEqual(getattr(fk_child_instance, fk_fieldname), fk_parent_default)\n+\n+        fk_child_model.objects.all().delete()\n+        fk_parent_model.objects.all().delete()\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+\n+    def change_to_non_db_set_null_test(\n+        self,\n+        app_label,\n+        fk_parent_modelname,\n+        fk_child_modelname,\n+        fk_fieldname,\n+        project_state,\n+    ):\n+        operation = migrations.AlterField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                null=True,\n+                on_delete=models.SET_NULL,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance1 = fk_child_model.objects.create(**kwargs)\n+\n+        with self.assertNumQueries(4):\n+            fk_parent_instance.delete()\n+\n+        fk_child_instance1.refresh_from_db()\n+        self.assertIsNone(getattr(fk_child_instance1, fk_fieldname))\n+\n+        fk_parent_model.objects.all().delete()\n+        fk_child_model.objects.all().delete()\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+\n+    def change_to_non_db_cascade_test(\n+        self,\n+        app_label,\n+        fk_parent_modelname,\n+        fk_child_modelname,\n+        fk_fieldname,\n+        project_state,\n+    ):\n+        operation = migrations.AlterField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                on_delete=models.CASCADE,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance1 = fk_child_model.objects.create(**kwargs)\n+        fk_child_instance2 = fk_child_model.objects.create(**kwargs)\n+\n+        with self.assertNumQueries(4):\n+            fk_parent_instance.delete()\n+\n+        self.assertIs(\n+            fk_child_model.objects.filter(pk=fk_child_instance1.pk).exists(), False\n+        )\n+        self.assertIs(\n+            fk_child_model.objects.filter(pk=fk_child_instance2.pk).exists(), False\n+        )\n+\n+        fk_parent_model.objects.all().delete()\n+        fk_child_model.objects.all().delete()\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+\n+    def change_to_non_db_restrict_test(\n+        self,\n+        app_label,\n+        fk_parent_modelname,\n+        fk_child_modelname,\n+        fk_fieldname,\n+        project_state,\n+    ):\n+        operation = migrations.AlterField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                on_delete=models.RESTRICT,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance = fk_child_model.objects.create(**kwargs)\n+\n+        with self.assertRaises(IntegrityError):\n+            fk_parent_instance.delete()\n+\n+        self.assertIs(\n+            fk_child_model.objects.filter(pk=fk_child_instance.pk).exists(), True\n+        )\n+        self.assertIs(\n+            fk_parent_model.objects.filter(pk=fk_parent_instance.pk).exists(), True\n+        )\n+\n+        fk_child_model.objects.all().delete()\n+        fk_parent_model.objects.all().delete()\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+\n+    def change_to_non_db_set_default_test(\n+        self,\n+        app_label,\n+        fk_parent_modelname,\n+        fk_child_modelname,\n+        fk_fieldname,\n+        project_state,\n+    ):\n+        operation = migrations.AlterField(\n+            fk_child_modelname,\n+            fk_fieldname,\n+            models.ForeignKey(\n+                f\"{app_label}.{fk_parent_modelname}\",\n+                default=1,\n+                on_delete=models.SET_DEFAULT,\n+            ),\n+        )\n+        new_state = project_state.clone()\n+        operation.state_forwards(app_label, new_state)\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(app_label, editor, project_state, new_state)\n+\n+        fk_child_model = new_state.apps.get_model(app_label, fk_child_modelname)\n+        fk_parent_model = new_state.apps.get_model(app_label, fk_parent_modelname)\n+\n+        fk_parent_default = fk_parent_model.objects.create(id=1)\n+        fk_parent_instance = fk_parent_model.objects.create()\n+        kwargs = {fk_fieldname: fk_parent_instance}\n+        fk_child_instance = fk_child_model.objects.create(**kwargs)\n+\n+        fk_parent_instance.delete()\n+\n+        fk_child_instance.refresh_from_db()\n+        fk_parent_default.refresh_from_db()\n+        # After deletion it gets set to db default value\n+        self.assertEqual(getattr(fk_child_instance, fk_fieldname), fk_parent_default)\n+\n+        fk_child_model.objects.all().delete()\n+        fk_parent_model.objects.all().delete()\n+\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(app_label, editor, new_state, project_state)\n+\n+    def test_alter_field_to_db_level_fk_from_non_db_cascade(self):\n+        app_label = \"test_alter_db_level_fk_from_non_db_cascade\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fkparent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                    (\n+                        fk_fieldname,\n+                        models.ForeignKey(\n+                            f\"{app_label}.{fk_parent_modelname}\",\n+                            on_delete=models.CASCADE,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+        ]\n+\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+\n+        self.change_to_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        if connection.features.has_on_delete_db_default:\n+            self.change_to_db_set_default_test(\n+                app_label,\n+                fk_parent_modelname,\n+                fk_child_modelname,\n+                fk_fieldname,\n+                project_state,\n+            )\n+\n+    def test_alter_field_to_db_level_fk_from_non_db_restrict(self):\n+        app_label = \"test_alter_db_level_fk_from_non_db_restrict\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fkparent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                    (\n+                        fk_fieldname,\n+                        models.ForeignKey(\n+                            f\"{app_label}.{fk_parent_modelname}\",\n+                            on_delete=models.RESTRICT,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+        ]\n+\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+\n+        self.change_to_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        if connection.features.has_on_delete_db_default:\n+            self.change_to_db_set_default_test(\n+                app_label,\n+                fk_parent_modelname,\n+                fk_child_modelname,\n+                fk_fieldname,\n+                project_state,\n+            )\n+\n+    def test_alter_field_to_db_level_fk_from_non_db_set_null(self):\n+        app_label = \"test_alter_db_level_fk_from_non_db_set_null\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fkparent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                    (\n+                        fk_fieldname,\n+                        models.ForeignKey(\n+                            f\"{app_label}.{fk_parent_modelname}\",\n+                            null=True,\n+                            on_delete=models.SET_NULL,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+        ]\n+\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+\n+        self.change_to_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        if connection.features.has_on_delete_db_default:\n+            self.change_to_db_set_default_test(\n+                app_label,\n+                fk_parent_modelname,\n+                fk_child_modelname,\n+                fk_fieldname,\n+                project_state,\n+            )\n+\n+    def test_alter_field_to_db_level_fk_from_non_db_set_default(self):\n+        app_label = \"test_alter_db_level_fk_from_non_db_set_default\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fkparent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                    (\n+                        fk_fieldname,\n+                        models.ForeignKey(\n+                            f\"{app_label}.{fk_parent_modelname}\",\n+                            default=1,\n+                            on_delete=models.SET_DEFAULT,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+        ]\n+\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+\n+        self.change_to_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        if connection.features.has_on_delete_db_default:\n+            self.change_to_db_set_default_test(\n+                app_label,\n+                fk_parent_modelname,\n+                fk_child_modelname,\n+                fk_fieldname,\n+                project_state,\n+            )\n+\n+    def test_alter_field_from_db_cascade(self):\n+        app_label = \"test_alter_field_from_db_cascade\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fkparent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                    (\n+                        fk_fieldname,\n+                        models.ForeignKey(\n+                            f\"{app_label}.{fk_parent_modelname}\",\n+                            on_delete=models.DB_CASCADE,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+        ]\n+\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+\n+        self.change_to_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        if connection.features.has_on_delete_db_default:\n+            self.change_to_db_set_default_test(\n+                app_label,\n+                fk_parent_modelname,\n+                fk_child_modelname,\n+                fk_fieldname,\n+                project_state,\n+            )\n+\n+        self.change_to_non_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_set_default_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+\n+    def test_alter_field_from_db_set_null(self):\n+        app_label = \"test_alter_field_from_db_set_null\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fkparent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                    (\n+                        fk_fieldname,\n+                        models.ForeignKey(\n+                            f\"{app_label}.{fk_parent_modelname}\",\n+                            null=True,\n+                            on_delete=models.DB_SET_NULL,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+        ]\n+\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+\n+        self.change_to_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        if connection.features.has_on_delete_db_default:\n+            self.change_to_db_set_default_test(\n+                app_label,\n+                fk_parent_modelname,\n+                fk_child_modelname,\n+                fk_fieldname,\n+                project_state,\n+            )\n+        self.change_to_non_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_set_default_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+\n+    def test_alter_field_from_db_restrict(self):\n+        app_label = \"test_alter_field_from_db_restrict\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fkparent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                    (\n+                        fk_fieldname,\n+                        models.ForeignKey(\n+                            f\"{app_label}.{fk_parent_modelname}\",\n+                            on_delete=models.DB_RESTRICT,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+        ]\n+\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+\n+        self.change_to_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        if connection.features.has_on_delete_db_default:\n+            self.change_to_db_set_default_test(\n+                app_label,\n+                fk_parent_modelname,\n+                fk_child_modelname,\n+                fk_fieldname,\n+                project_state,\n+            )\n+\n+        self.change_to_non_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_set_default_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+\n+    @skipUnlessDBFeature(\"has_on_delete_db_default\")\n+    def test_alter_field_from_db_set_default(self):\n+        app_label = \"test_alter_field_from_db_set_default\"\n+        fk_parent_modelname = \"FkParent\"\n+        fk_child_modelname = \"FkChild\"\n+        fk_fieldname = \"fkparent\"\n+        operations = [\n+            migrations.CreateModel(\n+                fk_parent_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+            migrations.CreateModel(\n+                fk_child_modelname,\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"number\", models.IntegerField(default=1)),\n+                    (\n+                        fk_fieldname,\n+                        models.ForeignKey(\n+                            f\"{app_label}.{fk_parent_modelname}\",\n+                            default=1,\n+                            on_delete=models.DB_SET_DEFAULT,\n+                        ),\n+                    ),\n+                ],\n+            ),\n+        ]\n+\n+        project_state = self.apply_operations(app_label, ProjectState(), operations)\n+\n+        self.change_to_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+\n+        self.change_to_non_db_set_null_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_cascade_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_restrict_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n+        self.change_to_non_db_set_default_test(\n+            app_label,\n+            fk_parent_modelname,\n+            fk_child_modelname,\n+            fk_fieldname,\n+            project_state,\n+        )\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The added tests for migrations are a positive step; however, you're facing two key issues. First, the MySQL error in `delete.tests.DatabaseLevelOnDeleteTests.test_foreign_key_db_default` likely stems from a table creation failure, potentially due to a foreign key type mismatch. Review the generated SQL closely. Secondly, for serializer testing, check `tests/migrations/test_writer.py`; it may contain relevant examples. Consider simplifying the `subTest` usage for more effective testing, particularly for backward migrations. Lets aim for clearer tests moving forward."
      }
    ],
    "codediff_length": 62309
  },
  {
    "index": 20,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/tests/queries/test_explain.py b/tests/queries/test_explain.py\nnew file mode 100644\nindex 000000000000..26baf6fb30da\n--- /dev/null\n+++ b/tests/queries/test_explain.py\n@@ -0,0 +1,99 @@\n+import unittest\n+\n+from django.db import NotSupportedError, connection, transaction\n+from django.db.models import Count\n+from django.test import TestCase, skipIfDBFeature, skipUnlessDBFeature\n+from django.test.utils import CaptureQueriesContext\n+\n+from .models import Tag\n+\n+\n+@skipUnlessDBFeature('supports_explaining_query_execution')\n+class ExplainTests(TestCase):\n+\n+    def test_basic(self):\n+        querysets = [\n+            Tag.objects.filter(name='test'),\n+            Tag.objects.filter(name='test').select_related('parent'),\n+            Tag.objects.filter(name='test').prefetch_related('children'),\n+            Tag.objects.filter(name='test').annotate(Count('children')),\n+            Tag.objects.filter(name='test').values_list('name'),\n+            Tag.objects.order_by().union(Tag.objects.order_by().filter(name='test')),\n+            Tag.objects.all().select_for_update().filter(name='test'),\n+        ]\n+        supported_formats = connection.features.supported_explain_formats\n+        all_formats = (None,) + tuple(supported_formats) + tuple(f.lower() for f in supported_formats)\n+        for idx, queryset in enumerate(querysets):\n+            for format in all_formats:\n+                with self.subTest(format=format, queryset=idx):\n+                    if connection.vendor == 'mysql':\n+                        # This does a query and caches the result.\n+                        connection.features.needs_explain_extended\n+                    with self.assertNumQueries(1), CaptureQueriesContext(connection) as captured_queries:\n+                        result = queryset.explain(format=format)\n+                        self.assertTrue(captured_queries[0]['sql'].startswith(connection.ops.explain_prefix))\n+                        self.assertIsInstance(result, str)\n+                        self.assertTrue(result)\n+\n+    @skipUnlessDBFeature('validates_explain_options')\n+    def test_unknown_options(self):\n+        with self.assertRaisesMessage(ValueError, 'Unknown options: test, test2'):\n+            Tag.objects.all().explain(test=1, test2=1)\n+\n+    def test_unknown_format(self):\n+        msg = 'DOES NOT EXIST is not a recognized format.'\n+        if connection.features.supported_explain_formats:\n+            msg += ' Allowed formats: %s' % ', '.join(sorted(connection.features.supported_explain_formats))\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Tag.objects.all().explain(format='does not exist')\n+\n+    @unittest.skipUnless(connection.vendor == 'postgresql', 'PostgreSQL specific')\n+    def test_postgres_options(self):\n+        qs = Tag.objects.filter(name='test')\n+        test_options = [\n+            {'COSTS': False, 'BUFFERS': True, 'ANALYZE': True},\n+            {'costs': False, 'buffers': True, 'analyze': True},\n+            {'verbose': True, 'timing': True, 'analyze': True},\n+            {'verbose': False, 'timing': False, 'analyze': True},\n+        ]\n+        if connection.pg_version >= 100000:\n+            test_options.append({'summary': True})\n+        for options in test_options:\n+            with self.subTest(**options), transaction.atomic():\n+                with CaptureQueriesContext(connection) as captured_queries:\n+                    qs.explain(format='text', **options)\n+                self.assertEqual(len(captured_queries), 1)\n+                for name, value in options.items():\n+                    option = '{} {}'.format(name.upper(), 'true' if value else 'false')\n+                    self.assertIn(option, captured_queries[0]['sql'])\n+\n+    @unittest.skipUnless(connection.vendor == 'mysql', 'MySQL specific')\n+    def test_mysql_text_to_traditional(self):\n+        with CaptureQueriesContext(connection) as captured_queries:\n+            Tag.objects.filter(name='test').explain(format='text')\n+        self.assertEqual(len(captured_queries), 1)\n+        self.assertIn('FORMAT=TRADITIONAL', captured_queries[0]['sql'])\n+\n+    @unittest.skipUnless(connection.vendor == 'mysql', 'MySQL < 5.7 specific')\n+    def test_mysql_extended(self):\n+        # Inner skip to avoid module level query for MySQL version.\n+        if not connection.features.needs_explain_extended:\n+            raise unittest.SkipTest('MySQL < 5.7 specific')\n+        qs = Tag.objects.filter(name='test')\n+        with CaptureQueriesContext(connection) as captured_queries:\n+            qs.explain(format='json')\n+        self.assertEqual(len(captured_queries), 1)\n+        self.assertNotIn('EXTENDED', captured_queries[0]['sql'])\n+        with CaptureQueriesContext(connection) as captured_queries:\n+            qs.explain(format='text')\n+        self.assertEqual(len(captured_queries), 1)\n+        self.assertNotIn('EXTENDED', captured_queries[0]['sql'])\n+\n+\n+@skipIfDBFeature('supports_explaining_query_execution')\n+class ExplainUnsupportedTests(TestCase):\n+\n+    def test_message(self):\n+        msg = 'This backend does not support explaining query execution.'\n+        with self.assertRaisesMessage(NotSupportedError, msg):\n+            Tag.objects.filter(name='test').explain()\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Overall, your implementation improvements are solid, but there are significant concerns regarding the MySQL and Oracle output formats. The MySQL explanation you propose could enhance usability, yet it might complicate the `SQLCompiler.explain` method with database-specific logic. The Oracle `EXPLAIN PLAN` challenges highlight the limitations of using parameters, leading to disabling support for Oracle completely. Make sure the unique identifier approach is well-documented, and consider alternative strategies for fetching query plans. Finally, ensure the test failures are addressed comprehensively."
      }
    ],
    "codediff_length": 5257
  },
  {
    "index": 21,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/contrib/postgres/constraints.py b/django/contrib/postgres/constraints.py\nnew file mode 100644\nindex 000000000000..2fcb076ecf29\n--- /dev/null\n+++ b/django/contrib/postgres/constraints.py\n@@ -0,0 +1,106 @@\n+from django.db.backends.ddl_references import Statement, Table\n+from django.db.models import F, Q\n+from django.db.models.constraints import BaseConstraint\n+from django.db.models.sql import Query\n+\n+__all__ = ['ExclusionConstraint']\n+\n+\n+class ExclusionConstraint(BaseConstraint):\n+    template = 'CONSTRAINT %(name)s EXCLUDE USING %(index_type)s (%(expressions)s)%(where)s'\n+\n+    def __init__(self, *, name, expressions, index_type=None, condition=None):\n+        if index_type and index_type.lower() not in {'gist', 'spgist'}:\n+            raise ValueError(\n+                'Exclusion constraints only support GiST or SP-GiST indexes.'\n+            )\n+        if not expressions:\n+            raise ValueError(\n+                'At least one expression is required to define an exclusion '\n+                'constraint.'\n+            )\n+        if not all(\n+            isinstance(expr, (list, tuple)) and len(expr) == 2\n+            for expr in expressions\n+        ):\n+            raise ValueError('The expressions must be a list of 2-tuples.')\n+        if not isinstance(condition, (type(None), Q)):\n+            raise ValueError(\n+                'ExclusionConstraint.condition must be a Q instance.'\n+            )\n+        self.expressions = expressions\n+        self.index_type = index_type or 'GIST'\n+        self.condition = condition\n+        super().__init__(name=name)\n+\n+    def _get_expression_sql(self, compiler, connection, query):\n+        expressions = []\n+        for expression, operator in self.expressions:\n+            if isinstance(expression, str):\n+                expression = F(expression)\n+            if isinstance(expression, F):\n+                expression = expression.resolve_expression(query=query, simple_col=True)\n+            else:\n+                expression = expression.resolve_expression(query=query)\n+            sql, params = expression.as_sql(compiler, connection)\n+            expressions.append('%s WITH %s' % (sql % params, operator))\n+        return expressions\n+\n+    def _get_condition_sql(self, compiler, schema_editor, query):\n+        if self.condition is None:\n+            return None\n+        where = query.build_where(self.condition)\n+        sql, params = where.as_sql(compiler, schema_editor.connection)\n+        return sql % tuple(schema_editor.quote_value(p) for p in params)\n+\n+    def constraint_sql(self, model, schema_editor):\n+        query = Query(model)\n+        compiler = query.get_compiler(connection=schema_editor.connection)\n+        expressions = self._get_expression_sql(compiler, schema_editor.connection, query)\n+        condition = self._get_condition_sql(compiler, schema_editor, query)\n+        return self.template % {\n+            'name': schema_editor.quote_name(self.name),\n+            'index_type': self.index_type,\n+            'expressions': ', '.join(expressions),\n+            'where': ' WHERE (%s)' % condition if condition else '',\n+        }\n+\n+    def create_sql(self, model, schema_editor):\n+        return Statement(\n+            'ALTER TABLE %(table)s ADD %(constraint)s',\n+            table=Table(model._meta.db_table, schema_editor.quote_name),\n+            constraint=self.constraint_sql(model, schema_editor),\n+        )\n+\n+    def remove_sql(self, model, schema_editor):\n+        return schema_editor._delete_constraint_sql(\n+            schema_editor.sql_delete_check,\n+            model,\n+            schema_editor.quote_name(self.name),\n+        )\n+\n+    def deconstruct(self):\n+        path, args, kwargs = super().deconstruct()\n+        kwargs['expressions'] = self.expressions\n+        if self.condition is not None:\n+            kwargs['condition'] = self.condition\n+        if self.index_type.lower() != 'gist':\n+            kwargs['index_type'] = self.index_type\n+        return path, args, kwargs\n+\n+    def __eq__(self, other):\n+        return (\n+            isinstance(other, self.__class__) and\n+            self.name == other.name and\n+            self.index_type == other.index_type and\n+            self.expressions == other.expressions and\n+            self.condition == other.condition\n+        )\n+\n+    def __repr__(self):\n+        return '<%s: index_type=%s, expressions=%s%s>' % (\n+            self.__class__.__qualname__,\n+            self.index_type,\n+            self.expressions,\n+            '' if self.condition is None else ', condition=%s' % self.condition,\n+        )\ndiff --git a/docs/ref/contrib/postgres/constraints.txt b/docs/ref/contrib/postgres/constraints.txt\nnew file mode 100644\nindex 000000000000..fe9e72e6055a\n--- /dev/null\n+++ b/docs/ref/contrib/postgres/constraints.txt\n@@ -0,0 +1,151 @@\n+========================================\n+PostgreSQL specific database constraints\n+========================================\n+\n+.. module:: django.contrib.postgres.constraints\n+   :synopsis: PostgreSQL specific database constraint\n+\n+PostgreSQL supports additional data integrity constraints available from the\n+``django.contrib.postgres.constraints`` module. They are added in the model\n+:attr:`Meta.constraints <django.db.models.Options.constraints>` option.\n+\n+``ExclusionConstraint``\n+=======================\n+\n+.. versionadded:: 3.0\n+\n+.. class:: ExclusionConstraint(*, name, expressions, index_type=None, condition=None)\n+\n+    Creates an exclusion constraint in the database. Internally, PostgreSQL\n+    implements exclusion constraints using indexes. The default index type is\n+    `GiST <https://www.postgresql.org/docs/current/gist.html>`_. To use them,\n+    you need to activate the `btree_gist extension\n+    <https://www.postgresql.org/docs/current/btree-gist.html>`_ on PostgreSQL.\n+    You can install it using the\n+    :class:`~django.contrib.postgres.operations.BtreeGistExtension` migration\n+    operation.\n+\n+    If you attempt to insert a new row that conflicts with an existing row, an\n+    :exc:`~django.db.IntegrityError` is raised. Similarly, when update\n+    conflicts with an existing row.\n+\n+``name``\n+--------\n+\n+.. attribute:: ExclusionConstraint.name\n+\n+The name of the constraint.\n+\n+``expressions``\n+---------------\n+\n+.. attribute:: ExclusionConstraint.expressions\n+\n+An iterable of 2-tuples. The first element is an expression or string. The\n+second element is a SQL operator represented as a string. To avoid typos, you\n+may use :class:`~django.contrib.postgres.fields.RangeOperators` which maps the\n+operators with strings. For example::\n+\n+    expressions=[\n+        ('timespan', RangeOperators.ADJACENT_TO),\n+        (F('room'), RangeOperators.EQUAL),\n+    ]\n+\n+.. admonition:: Restrictions on operators.\n+\n+    Only commutative operators can be used in exclusion constraints.\n+\n+``index_type``\n+--------------\n+\n+.. attribute:: ExclusionConstraint.index_type\n+\n+The index type of the constraint. Accepted values are ``GIST`` or ``SPGIST``.\n+Matching is case insensitive. If not provided, the default index type is\n+``GIST``.\n+\n+``condition``\n+-------------\n+\n+.. attribute:: ExclusionConstraint.condition\n+\n+A :class:`~django.db.models.Q` object that specifies the condition to restrict\n+a constraint to a subset of rows. For example,\n+``condition=Q(cancelled=False)``.\n+\n+These conditions have the same database restrictions as\n+:attr:`django.db.models.Index.condition`.\n+\n+Examples\n+--------\n+\n+The following example restricts overlapping reservations in the same room, not\n+taking canceled reservations into account::\n+\n+    from django.contrib.postgres.constraints import ExclusionConstraint\n+    from django.contrib.postgres.fields import DateTimeRangeField, RangeOperators\n+    from django.db import models\n+    from django.db.models import Q\n+\n+    class Room(models.Model):\n+        number = models.IntegerField()\n+\n+\n+    class Reservation(models.Model):\n+        room = models.ForeignKey('Room', on_delete=models.CASCADE)\n+        timespan = DateTimeRangeField()\n+        cancelled = models.BooleanField(default=False)\n+\n+        class Meta:\n+            constraints = [\n+                ExclusionConstraint(\n+                    name='exclude_overlapping_reservations',\n+                    expressions=[\n+                        ('timespan', RangeOperators.OVERLAPS),\n+                        ('room', RangeOperators.EQUAL),\n+                    ],\n+                    condition=Q(cancelled=False),\n+                ),\n+            ]\n+\n+In case your model defines a range using two fields, instead of the native\n+PostgreSQL range types, you should write an expression that uses the equivalent\n+function (e.g. ``TsTzRange()``), and use the delimiters for the field. Most\n+often, the delimiters will be ``'[)'``, meaning that the lower bound is\n+inclusive and the upper bound is exclusive. You may use the\n+:class:`~django.contrib.postgres.fields.RangeBoundary` that provides an\n+expression mapping for the `range boundaries <https://www.postgresql.org/docs/\n+current/rangetypes.html#RANGETYPES-INCLUSIVITY>`_. For example::\n+\n+    from django.contrib.postgres.constraints import ExclusionConstraint\n+    from django.contrib.postgres.fields import (\n+        DateTimeRangeField,\n+        RangeBoundary,\n+        RangeOperators,\n+    )\n+    from django.db import models\n+    from django.db.models import Func, Q\n+\n+\n+    class TsTzRange(Func):\n+        function = 'TSTZRANGE'\n+        output_field = DateTimeRangeField()\n+\n+\n+    class Reservation(models.Model):\n+        room = models.ForeignKey('Room', on_delete=models.CASCADE)\n+        start = models.DateTimeField()\n+        end = models.DateTimeField()\n+        cancelled = models.BooleanField(default=False)\n+\n+        class Meta:\n+            constraints = [\n+                ExclusionConstraint(\n+                    name='exclude_overlapping_reservations',\n+                    expressions=(\n+                        (TsTzRange('start', 'end', RangeBoundary()), RangeOperators.OVERLAPS),\n+                        ('room', RangeOperators.EQUAL),\n+                    ),\n+                    condition=Q(cancelled=False),\n+                ),\n+            ]\ndiff --git a/docs/ref/contrib/postgres/fields.txt b/docs/ref/contrib/postgres/fields.txt\nindex 34c4a4892d62..14cb1d00cbbb 100644\n--- a/docs/ref/contrib/postgres/fields.txt\n+++ b/docs/ref/contrib/postgres/fields.txt\n@@ -916,3 +916,54 @@ types.\n     .. attribute:: range_type\n \n         The psycopg2 range type to use.\n+\n+Range operators\n+---------------\n+\n+.. versionadded:: 3.0\n+\n+.. class:: RangeOperators\n+\n+PostgreSQL provides a set of SQL operators that can be used together with the\n+range data types (see `the PostgreSQL documentation for the full details of\n+range operators <https://www.postgresql.org/docs/current/\n+functions-range.html#RANGE-OPERATORS-TABLE>`_). This class is meant as a\n+convenient method to avoid typos. The operator names overlap with the names of\n+corresponding lookups.\n+\n+.. code-block:: python\n+\n+    class RangeOperators:\n+        EQUAL = '='\n+        NOT_EQUAL = '<>'\n+        CONTAINS = '@>'\n+        CONTAINED_BY = '<@'\n+        OVERLAPS = '&&'\n+        FULLY_LT = '<<'\n+        FULLY_GT = '>>'\n+        NOT_LT = '&>'\n+        NOT_GT = '&<'\n+        ADJACENT_TO = '-|-'\n+\n+RangeBoundary() expressions\n+---------------------------\n+\n+.. versionadded:: 3.0\n+\n+.. class:: RangeBoundary(inclusive_lower=True, inclusive_upper=False)\n+\n+    .. attribute:: inclusive_lower\n+\n+        If ``True`` (default), the lower bound is inclusive ``'['``, otherwise\n+        it's exclusive ``'('``.\n+\n+    .. attribute:: inclusive_upper\n+\n+        If ``False`` (default), the upper bound is exclusive ``')'``, otherwise\n+        it's inclusive ``']'``.\n+\n+A ``RangeBoundary()`` expression represents the range boundaries. It can be\n+used with a custom range functions that expected boundaries, for example to\n+define :class:`~django.contrib.postgres.constraints.ExclusionConstraint`. See\n+`the PostgreSQL documentation for the full details <https://www.postgresql.org/\n+docs/current/rangetypes.html#RANGETYPES-INCLUSIVITY>`_.\ndiff --git a/tests/postgres_tests/test_constraints.py b/tests/postgres_tests/test_constraints.py\nindex 3ecabcbd49f9..d8665f59f61f 100644\n--- a/tests/postgres_tests/test_constraints.py\n+++ b/tests/postgres_tests/test_constraints.py\n@@ -77,3 +81,249 @@ def test_check_constraint_datetimerange_contains(self):\n             timestamps=(datetime_1, datetime_2),\n             timestamps_inner=(datetime_1, datetime_2),\n         )\n+\n+\n+class ExclusionConstraintTests(PostgreSQLTestCase):\n+    def get_constraints(self, table):\n+        \"\"\"Get the constraints on the table using a new cursor.\"\"\"\n+        with connection.cursor() as cursor:\n+            return connection.introspection.get_constraints(cursor, table)\n+\n+    def test_invalid_condition(self):\n+        msg = 'ExclusionConstraint.condition must be a Q instance.'\n+        with self.assertRaisesMessage(ValueError, msg):\n+            ExclusionConstraint(\n+                index_type='GIST',\n+                name='exclude_invalid_condition',\n+                expressions=[(F('datespan'), RangeOperators.OVERLAPS)],\n+                condition=F('invalid'),\n+            )\n+\n+    def test_invalid_index_type(self):\n+        msg = 'Exclusion constraints only support GiST or SP-GiST indexes.'\n+        with self.assertRaisesMessage(ValueError, msg):\n+            ExclusionConstraint(\n+                index_type='gin',\n+                name='exclude_invalid_index_type',\n+                expressions=[(F('datespan'), RangeOperators.OVERLAPS)],\n+            )\n+\n+    def test_invalid_expressions(self):\n+        msg = 'The expressions must be a list of 2-tuples.'\n+        for expressions in (['foo'], [('foo')], [('foo_1', 'foo_2', 'foo_3')]):\n+            with self.subTest(expressions), self.assertRaisesMessage(ValueError, msg):\n+                ExclusionConstraint(\n+                    index_type='GIST',\n+                    name='exclude_invalid_expressions',\n+                    expressions=expressions,\n+                )\n+\n+    def test_empty_expressions(self):\n+        msg = 'At least one expression is required to define an exclusion constraint.'\n+        for empty_expressions in (None, []):\n+            with self.subTest(empty_expressions), self.assertRaisesMessage(ValueError, msg):\n+                ExclusionConstraint(\n+                    index_type='GIST',\n+                    name='exclude_empty_expressions',\n+                    expressions=empty_expressions,\n+                )\n+\n+    def test_repr(self):\n+        constraint = ExclusionConstraint(\n+            name='exclude_overlapping',\n+            expressions=[\n+                (F('datespan'), RangeOperators.OVERLAPS),\n+                (F('room'), RangeOperators.EQUAL),\n+            ],\n+        )\n+        self.assertEqual(\n+            repr(constraint),\n+            \"<ExclusionConstraint: index_type=GIST, expressions=[\"\n+            \"(F(datespan), '&&'), (F(room), '=')]>\",\n+        )\n+        constraint = ExclusionConstraint(\n+            name='exclude_overlapping',\n+            expressions=[(F('datespan'), RangeOperators.ADJACENT_TO)],\n+            condition=Q(cancelled=False),\n+            index_type='SPGiST',\n+        )\n+        self.assertEqual(\n+            repr(constraint),\n+            \"<ExclusionConstraint: index_type=SPGiST, expressions=[\"\n+            \"(F(datespan), '-|-')], condition=(AND: ('cancelled', False))>\",\n+        )\n+\n+    def test_eq(self):\n+        constraint_1 = ExclusionConstraint(\n+            name='exclude_overlapping',\n+            expressions=[\n+                (F('datespan'), RangeOperators.OVERLAPS),\n+                (F('room'), RangeOperators.EQUAL),\n+            ],\n+            condition=Q(cancelled=False),\n+        )\n+        constraint_2 = ExclusionConstraint(\n+            name='exclude_overlapping',\n+            expressions=[\n+                ('datespan', RangeOperators.OVERLAPS),\n+                ('room', RangeOperators.EQUAL),\n+            ],\n+        )\n+        constraint_3 = ExclusionConstraint(\n+            name='exclude_overlapping',\n+            expressions=[('datespan', RangeOperators.OVERLAPS)],\n+            condition=Q(cancelled=False),\n+        )\n+        self.assertEqual(constraint_1, constraint_1)\n+        self.assertNotEqual(constraint_1, constraint_2)\n+        self.assertNotEqual(constraint_1, constraint_3)\n+        self.assertNotEqual(constraint_2, constraint_3)\n+        self.assertNotEqual(constraint_1, object())\n+\n+    def test_deconstruct(self):\n+        constraint = ExclusionConstraint(\n+            name='exclude_overlapping',\n+            expressions=[('datespan', RangeOperators.OVERLAPS), ('room', RangeOperators.EQUAL)],\n+        )\n+        path, args, kwargs = constraint.deconstruct()\n+        self.assertEqual(path, 'django.contrib.postgres.constraints.ExclusionConstraint')\n+        self.assertEqual(args, ())\n+        self.assertEqual(kwargs, {\n+            'name': 'exclude_overlapping',\n+            'expressions': [('datespan', RangeOperators.OVERLAPS), ('room', RangeOperators.EQUAL)],\n+        })\n+\n+    def test_deconstruct_index_type(self):\n+        constraint = ExclusionConstraint(\n+            name='exclude_overlapping',\n+            index_type='SPGIST',\n+            expressions=[('datespan', RangeOperators.OVERLAPS), ('room', RangeOperators.EQUAL)],\n+        )\n+        path, args, kwargs = constraint.deconstruct()\n+        self.assertEqual(path, 'django.contrib.postgres.constraints.ExclusionConstraint')\n+        self.assertEqual(args, ())\n+        self.assertEqual(kwargs, {\n+            'name': 'exclude_overlapping',\n+            'index_type': 'SPGIST',\n+            'expressions': [('datespan', RangeOperators.OVERLAPS), ('room', RangeOperators.EQUAL)],\n+        })\n+\n+    def test_deconstruct_condition(self):\n+        constraint = ExclusionConstraint(\n+            name='exclude_overlapping',\n+            expressions=[('datespan', RangeOperators.OVERLAPS), ('room', RangeOperators.EQUAL)],\n+            condition=Q(cancelled=False),\n+        )\n+        path, args, kwargs = constraint.deconstruct()\n+        self.assertEqual(path, 'django.contrib.postgres.constraints.ExclusionConstraint')\n+        self.assertEqual(args, ())\n+        self.assertEqual(kwargs, {\n+            'name': 'exclude_overlapping',\n+            'expressions': [('datespan', RangeOperators.OVERLAPS), ('room', RangeOperators.EQUAL)],\n+            'condition': Q(cancelled=False),\n+        })\n+\n+    def _test_range_overlaps(self, constraint):\n+        # Create exclusion constraint.\n+        self.assertNotIn(constraint.name, self.get_constraints(HotelReservation._meta.db_table))\n+        with connection.schema_editor() as editor:\n+            editor.add_constraint(HotelReservation, constraint)\n+        self.assertIn(constraint.name, self.get_constraints(HotelReservation._meta.db_table))\n+        # Add initial reservations.\n+        room101 = Room.objects.create(number=101)\n+        room102 = Room.objects.create(number=102)\n+        datetimes = [\n+            timezone.datetime(2018, 6, 20),\n+            timezone.datetime(2018, 6, 24),\n+            timezone.datetime(2018, 6, 26),\n+            timezone.datetime(2018, 6, 28),\n+            timezone.datetime(2018, 6, 29),\n+        ]\n+        HotelReservation.objects.create(\n+            datespan=DateRange(datetimes[0].date(), datetimes[1].date()),\n+            start=datetimes[0],\n+            end=datetimes[1],\n+            room=room102,\n+        )\n+        HotelReservation.objects.create(\n+            datespan=DateRange(datetimes[1].date(), datetimes[3].date()),\n+            start=datetimes[1],\n+            end=datetimes[3],\n+            room=room102,\n+        )\n+        # Overlap dates.\n+        with self.assertRaises(IntegrityError), transaction.atomic():\n+            reservation = HotelReservation(\n+                datespan=(datetimes[1].date(), datetimes[2].date()),\n+                start=datetimes[1],\n+                end=datetimes[2],\n+                room=room102,\n+            )\n+            reservation.save()\n+        # Valid range.\n+        HotelReservation.objects.bulk_create([\n+            # Other room.\n+            HotelReservation(\n+                datespan=(datetimes[1].date(), datetimes[2].date()),\n+                start=datetimes[1],\n+                end=datetimes[2],\n+                room=room101,\n+            ),\n+            # Cancelled reservation.\n+            HotelReservation(\n+                datespan=(datetimes[1].date(), datetimes[1].date()),\n+                start=datetimes[1],\n+                end=datetimes[2],\n+                room=room102,\n+                cancelled=True,\n+            ),\n+            # Other adjacent dates.\n+            HotelReservation(\n+                datespan=(datetimes[3].date(), datetimes[4].date()),\n+                start=datetimes[3],\n+                end=datetimes[4],\n+                room=room102,\n+            ),\n+        ])\n+\n+    def test_range_overlaps_custom(self):\n+        class TsTzRange(Func):\n+            function = 'TSTZRANGE'\n+            output_field = DateTimeRangeField()\n+\n+        constraint = ExclusionConstraint(\n+            name='exclude_overlapping_reservations_custom',\n+            expressions=[\n+                (TsTzRange('start', 'end', RangeBoundary()), RangeOperators.OVERLAPS),\n+                ('room', RangeOperators.EQUAL)\n+            ],\n+            condition=Q(cancelled=False),\n+        )\n+        self._test_range_overlaps(constraint)\n+\n+    def test_range_overlaps(self):\n+        constraint = ExclusionConstraint(\n+            name='exclude_overlapping_reservations',\n+            expressions=[\n+                (F('datespan'), RangeOperators.OVERLAPS),\n+                ('room', RangeOperators.EQUAL)\n+            ],\n+            condition=Q(cancelled=False),\n+        )\n+        self._test_range_overlaps(constraint)\n+\n+    def test_range_adjacent(self):\n+        constraint_name = 'ints_adjacent'\n+        self.assertNotIn(constraint_name, self.get_constraints(RangesModel._meta.db_table))\n+        constraint = ExclusionConstraint(\n+            name=constraint_name,\n+            expressions=[('ints', RangeOperators.ADJACENT_TO)],\n+        )\n+        with connection.schema_editor() as editor:\n+            editor.add_constraint(RangesModel, constraint)\n+        self.assertIn(constraint_name, self.get_constraints(RangesModel._meta.db_table))\n+        RangesModel.objects.create(ints=(20, 50))\n+        with self.assertRaises(IntegrityError), transaction.atomic():\n+            RangesModel.objects.create(ints=(10, 20))\n+        RangesModel.objects.create(ints=(10, 19))\n+        RangesModel.objects.create(ints=(51, 60))\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR has made substantial progress, addressing most feedback except for discussions around `&&` versus constants. While introducing `F().with()` and `Expression().with()` is appealing, it complicates the PostgreSQL backend with subclasses. Documentation reliance on PostgreSQL's operators is reasonable, given target users' familiarity. However, tests are failing on unrelated backends, and the proposed solution using `try/except` isn't effective. There's a potential issue with the condition check returning `None`, which may be unnecessary. Overall, simplifications are strong, but further testing is essential for exclusion constraints."
      }
    ],
    "codediff_length": 22819
  },
  {
    "index": 22,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/db/models/constraints.py b/django/db/models/constraints.py\nindex 3577c638ef2f..9949b50b1e4e 100644\n--- a/django/db/models/constraints.py\n+++ b/django/db/models/constraints.py\n@@ -265,3 +289,61 @@ def deconstruct(self):\n         if self.opclasses:\n             kwargs[\"opclasses\"] = self.opclasses\n         return path, self.expressions, kwargs\n+\n+    def validate(self, model, instance, exclude=None, using=DEFAULT_DB_ALIAS):\n+        queryset = model._default_manager.using(using)\n+        if self.fields:\n+            lookup_kwargs = {}\n+            for field_name in self.fields:\n+                if exclude and field_name in exclude:\n+                    return\n+                field = model._meta.get_field(field_name)\n+                lookup_value = getattr(instance, field.attname)\n+                if lookup_value is None or (\n+                    lookup_value == \"\"\n+                    and connections[using].features.interprets_empty_strings_as_nulls\n+                ):\n+                    # A composite constraint containing NULL value cannot cause\n+                    # a violation since NULL != NULL in SQL.\n+                    return\n+                lookup_kwargs[field.name] = lookup_value\n+            queryset = queryset.filter(**lookup_kwargs)\n+        else:\n+            # Ignore constraints with excluded fields.\n+            if exclude:\n+                for expression in self.expressions:\n+                    for expr in expression.flatten():\n+                        if isinstance(expr, F) and expr.name in exclude:\n+                            return\n+            replacement_map = instance._get_field_value_map(\n+                meta=model._meta, exclude=exclude\n+            )\n+            expressions = [\n+                Exact(expr, expr.replace_references(replacement_map))\n+                for expr in self.expressions\n+            ]\n+            queryset = queryset.filter(*expressions)\n+        model_class_pk = instance._get_pk_val(model._meta)\n+        if not instance._state.adding and model_class_pk is not None:\n+            queryset = queryset.exclude(pk=model_class_pk)\n+        if not self.condition:\n+            if queryset.exists():\n+                if self.expressions:\n+                    raise ValidationError(self.get_violation_error_message())\n+                # When fields are defined, use the unique_error_message() for\n+                # backward compatibility.\n+                for model, constraints in instance.get_constraints():\n+                    for constraint in constraints:\n+                        if constraint is self:\n+                            raise ValidationError(\n+                                instance.unique_error_message(model, self.fields)\n+                            )\n+        else:\n+            against = instance._get_field_value_map(meta=model._meta, exclude=exclude)\n+            try:\n+                if (self.condition & Exists(queryset.filter(self.condition))).check(\n+                    against, using=using\n+                ):\n+                    raise ValidationError(self.get_violation_error_message())\n+            except FieldError:\n+                pass\ndiff --git a/tests/constraints/tests.py b/tests/constraints/tests.py\nindex 58960fa5a3c5..d9e377438e82 100644\n--- a/tests/constraints/tests.py\n+++ b/tests/constraints/tests.py\n@@ -415,15 +478,135 @@ def test_model_validation(self):\n         with self.assertRaisesMessage(ValidationError, msg):\n             UniqueConstraintProduct(\n                 name=self.p1.name, color=self.p1.color\n-            ).validate_unique()\n+            ).validate_constraints()\n \n     @skipUnlessDBFeature(\"supports_partial_indexes\")\n     def test_model_validation_with_condition(self):\n-        \"\"\"Partial unique constraints are ignored by Model.validate_unique().\"\"\"\n+        \"\"\"\n+        Partial unique constraints are not ignored by\n+        Model.validate_constraints().\n+        \"\"\"\n         obj1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\", color=\"red\")\n         obj2 = UniqueConstraintConditionProduct.objects.create(name=\"p2\")\n-        UniqueConstraintConditionProduct(name=obj1.name, color=\"blue\").validate_unique()\n-        UniqueConstraintConditionProduct(name=obj2.name).validate_unique()\n+        UniqueConstraintConditionProduct(\n+            name=obj1.name, color=\"blue\"\n+        ).validate_constraints()\n+        msg = \"Constraint name_without_color_uniq is violated.\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            UniqueConstraintConditionProduct(name=obj2.name).validate_constraints()\n+\n+    def test_validate(self):\n+        constraint = UniqueConstraintProduct._meta.constraints[0]\n+        msg = \"Unique constraint product with this Name and Color already exists.\"\n+        non_unique_product = UniqueConstraintProduct(\n+            name=self.p1.name, color=self.p1.color\n+        )\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(UniqueConstraintProduct, non_unique_product)\n+        # Null values are ignored.\n+        constraint.validate(\n+            UniqueConstraintProduct,\n+            UniqueConstraintProduct(name=self.p2.name, color=None),\n+        )\n+        # Existing instances have their existing row excluded.\n+        constraint.validate(UniqueConstraintProduct, self.p1)\n+        # Unique fields are excluded.\n+        constraint.validate(\n+            UniqueConstraintProduct,\n+            non_unique_product,\n+            exclude={\"name\"},\n+        )\n+        constraint.validate(\n+            UniqueConstraintProduct,\n+            non_unique_product,\n+            exclude={\"color\"},\n+        )\n+        constraint.validate(\n+            UniqueConstraintProduct,\n+            non_unique_product,\n+            exclude={\"name\", \"color\"},\n+        )\n+        # Validation on a child instance.\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(\n+                UniqueConstraintProduct,\n+                ChildUniqueConstraintProduct(name=self.p1.name, color=self.p1.color),\n+            )\n+\n+    @skipUnlessDBFeature(\"supports_partial_indexes\")\n+    def test_validate_condition(self):\n+        p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\")\n+        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n+        msg = \"Constraint name_without_color_uniq is violated.\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(\n+                UniqueConstraintConditionProduct,\n+                UniqueConstraintConditionProduct(name=p1.name, color=None),\n+            )\n+        # Values not matching condition are ignored.\n+        constraint.validate(\n+            UniqueConstraintConditionProduct,\n+            UniqueConstraintConditionProduct(name=p1.name, color=\"anything-but-none\"),\n+        )\n+        # Existing instances have their existing row excluded.\n+        constraint.validate(UniqueConstraintConditionProduct, p1)\n+        # Unique field is excluded.\n+        constraint.validate(\n+            UniqueConstraintConditionProduct,\n+            UniqueConstraintConditionProduct(name=p1.name, color=None),\n+            exclude={\"name\"},\n+        )\n+\n+    def test_validate_expression(self):\n+        constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n+        msg = \"Constraint name_lower_uniq is violated.\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(\n+                UniqueConstraintProduct,\n+                UniqueConstraintProduct(name=self.p1.name.upper()),\n+            )\n+        constraint.validate(\n+            UniqueConstraintProduct,\n+            UniqueConstraintProduct(name=\"another-name\"),\n+        )\n+        # Existing instances have their existing row excluded.\n+        constraint.validate(UniqueConstraintProduct, self.p1)\n+        # Unique field is excluded.\n+        constraint.validate(\n+            UniqueConstraintProduct,\n+            UniqueConstraintProduct(name=self.p1.name.upper()),\n+            exclude={\"name\"},\n+        )\n+\n+    def test_validate_expression_condition(self):\n+        constraint = models.UniqueConstraint(\n+            Lower(\"name\"),\n+            name=\"name_lower_without_color_uniq\",\n+            condition=models.Q(color__isnull=True),\n+        )\n+        non_unique_product = UniqueConstraintProduct(name=self.p2.name.upper())\n+        msg = \"Constraint name_lower_without_color_uniq is violated.\"\n+        with self.assertRaisesMessage(ValidationError, msg):\n+            constraint.validate(UniqueConstraintProduct, non_unique_product)\n+        # Values not matching condition are ignored.\n+        constraint.validate(\n+            UniqueConstraintProduct,\n+            UniqueConstraintProduct(name=self.p1.name, color=self.p1.color),\n+        )\n+        # Existing instances have their existing row excluded.\n+        constraint.validate(UniqueConstraintProduct, self.p2)\n+        # Unique field is excluded.\n+        constraint.validate(\n+            UniqueConstraintProduct,\n+            non_unique_product,\n+            exclude={\"name\"},\n+        )\n+        # Field from a condition is excluded.\n+        constraint.validate(\n+            UniqueConstraintProduct,\n+            non_unique_product,\n+            exclude={\"color\"},\n+        )\n \n     def test_name(self):\n         constraints = get_constraints(UniqueConstraintProduct._meta.db_table)\ndiff --git a/tests/invalid_models_tests/test_models.py b/tests/invalid_models_tests/test_models.py\nindex 5ea830d0ecf1..08aca62850b6 100644\n--- a/tests/invalid_models_tests/test_models.py\n+++ b/tests/invalid_models_tests/test_models.py\n@@ -2198,6 +2198,66 @@ class Meta:\n         ]\n         self.assertCountEqual(errors, expected_errors)\n \n+    def test_check_constraint_raw_sql_check(self):\n+        class Model(models.Model):\n+            class Meta:\n+                required_db_features = {\"supports_table_check_constraints\"}\n+                constraints = [\n+                    models.CheckConstraint(check=models.Q(id__gt=0), name=\"q_check\"),\n+                    models.CheckConstraint(\n+                        check=models.ExpressionWrapper(\n+                            models.Q(price__gt=20),\n+                            output_field=models.BooleanField(),\n+                        ),\n+                        name=\"expression_wrapper_check\",\n+                    ),\n+                    models.CheckConstraint(\n+                        check=models.expressions.RawSQL(\n+                            \"id = 0\",\n+                            params=(),\n+                            output_field=models.BooleanField(),\n+                        ),\n+                        name=\"raw_sql_check\",\n+                    ),\n+                    models.CheckConstraint(\n+                        check=models.Q(\n+                            models.ExpressionWrapper(\n+                                models.Q(\n+                                    models.expressions.RawSQL(\n+                                        \"id = 0\",\n+                                        params=(),\n+                                        output_field=models.BooleanField(),\n+                                    )\n+                                ),\n+                                output_field=models.BooleanField(),\n+                            )\n+                        ),\n+                        name=\"nested_raw_sql_check\",\n+                    ),\n+                ]\n+\n+        expected_warnings = (\n+            [\n+                Warning(\n+                    \"Check constraint 'raw_sql_check' contains RawSQL() expression and \"\n+                    \"won't be validated during the model full_clean().\",\n+                    hint=\"Silence this warning if you don't care about it.\",\n+                    obj=Model,\n+                    id=\"models.W045\",\n+                ),\n+                Warning(\n+                    \"Check constraint 'nested_raw_sql_check' contains RawSQL() \"\n+                    \"expression and won't be validated during the model full_clean().\",\n+                    hint=\"Silence this warning if you don't care about it.\",\n+                    obj=Model,\n+                    id=\"models.W045\",\n+                ),\n+            ]\n+            if connection.features.supports_table_check_constraints\n+            else []\n+        )\n+        self.assertEqual(Model.check(databases=self.databases), expected_warnings)\n+\n     def test_unique_constraint_with_condition(self):\n         class Model(models.Model):\n             age = models.IntegerField()\ndiff --git a/tests/validation/models.py b/tests/validation/models.py\nindex 6934fa017c53..8919a6931003 100644\n--- a/tests/validation/models.py\n+++ b/tests/validation/models.py\n@@ -161,3 +161,59 @@ class Meta:\n         constraints = [\n             models.UniqueConstraint(Lower(\"field\"), name=\"func_lower_field_uq\"),\n         ]\n+\n+\n+class Product(models.Model):\n+    price = models.IntegerField(null=True)\n+    discounted_price = models.IntegerField(null=True)\n+\n+    class Meta:\n+        required_db_features = {\n+            \"supports_table_check_constraints\",\n+        }\n+        constraints = [\n+            models.CheckConstraint(\n+                check=models.Q(price__gt=models.F(\"discounted_price\")),\n+                name=\"price_gt_discounted_price_validation\",\n+            ),\n+        ]\n+\n+\n+class ChildProduct(Product):\n+    class Meta:\n+        required_db_features = {\n+            \"supports_table_check_constraints\",\n+        }\n+\n+\n+class UniqueConstraintProduct(models.Model):\n+    name = models.CharField(max_length=255)\n+    color = models.CharField(max_length=32)\n+    rank = models.IntegerField()\n+\n+    class Meta:\n+        constraints = [\n+            models.UniqueConstraint(\n+                fields=[\"name\", \"color\"], name=\"name_color_uniq_validation\"\n+            ),\n+            models.UniqueConstraint(fields=[\"rank\"], name=\"rank_uniq_validation\"),\n+        ]\n+\n+\n+class ChildUniqueConstraintProduct(UniqueConstraintProduct):\n+    pass\n+\n+\n+class UniqueConstraintConditionProduct(models.Model):\n+    name = models.CharField(max_length=255)\n+    color = models.CharField(max_length=31, null=True, blank=True)\n+\n+    class Meta:\n+        required_db_features = {\"supports_partial_indexes\"}\n+        constraints = [\n+            models.UniqueConstraint(\n+                fields=[\"name\"],\n+                name=\"name_without_color_uniq_validation\",\n+                condition=models.Q(color__isnull=True),\n+            ),\n+        ]\ndiff --git a/tests/validation/test_constraints.py b/tests/validation/test_constraints.py\nnew file mode 100644\nindex 000000000000..0b1ee6518e28\n--- /dev/null\n+++ b/tests/validation/test_constraints.py\n@@ -0,0 +1,95 @@\n+from django.core.exceptions import ValidationError\n+from django.test import TestCase, skipUnlessDBFeature\n+\n+from .models import (\n+    ChildProduct,\n+    ChildUniqueConstraintProduct,\n+    Product,\n+    UniqueConstraintConditionProduct,\n+    UniqueConstraintProduct,\n+)\n+\n+\n+class PerformConstraintChecksTest(TestCase):\n+    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n+    def test_full_clean_with_check_constraints(self):\n+        product = Product(price=10, discounted_price=15)\n+        with self.assertRaises(ValidationError) as cm:\n+            product.full_clean()\n+        self.assertEqual(\n+            cm.exception.message_dict,\n+            {\n+                \"__all__\": [\n+                    \"Constraint price_gt_discounted_price_validation is violated.\"\n+                ]\n+            },\n+        )\n+\n+    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n+    def test_full_clean_with_check_constraints_on_child_model(self):\n+        product = ChildProduct(price=10, discounted_price=15)\n+        with self.assertRaises(ValidationError) as cm:\n+            product.full_clean()\n+        self.assertEqual(\n+            cm.exception.message_dict,\n+            {\n+                \"__all__\": [\n+                    \"Constraint price_gt_discounted_price_validation is violated.\"\n+                ]\n+            },\n+        )\n+\n+    @skipUnlessDBFeature(\"supports_table_check_constraints\")\n+    def test_full_clean_with_check_constraints_disabled(self):\n+        product = Product(price=10, discounted_price=15)\n+        product.full_clean(validate_constraints=False)\n+\n+    def test_full_clean_with_unique_constraints(self):\n+        UniqueConstraintProduct.objects.create(name=\"product\", color=\"yellow\", rank=1)\n+        tests = [\n+            UniqueConstraintProduct(name=\"product\", color=\"yellow\", rank=1),\n+            # Child model.\n+            ChildUniqueConstraintProduct(name=\"product\", color=\"yellow\", rank=1),\n+        ]\n+        for product in tests:\n+            with self.subTest(model=product.__class__.__name__):\n+                with self.assertRaises(ValidationError) as cm:\n+                    product.full_clean()\n+                self.assertEqual(\n+                    cm.exception.message_dict,\n+                    {\n+                        \"__all__\": [\n+                            \"Unique constraint product with this Name and Color \"\n+                            \"already exists.\"\n+                        ],\n+                        \"rank\": [\n+                            \"Unique constraint product with this Rank already exists.\"\n+                        ],\n+                    },\n+                )\n+\n+    def test_full_clean_with_unique_constraints_disabled(self):\n+        UniqueConstraintProduct.objects.create(name=\"product\", color=\"yellow\", rank=1)\n+        product = UniqueConstraintProduct(name=\"product\", color=\"yellow\", rank=1)\n+        product.full_clean(validate_constraints=False)\n+\n+    @skipUnlessDBFeature(\"supports_partial_indexes\")\n+    def test_full_clean_with_partial_unique_constraints(self):\n+        UniqueConstraintConditionProduct.objects.create(name=\"product\")\n+        product = UniqueConstraintConditionProduct(name=\"product\")\n+        with self.assertRaises(ValidationError) as cm:\n+            product.full_clean()\n+        self.assertEqual(\n+            cm.exception.message_dict,\n+            {\n+                \"__all__\": [\n+                    \"Constraint name_without_color_uniq_validation is violated.\"\n+                ]\n+            },\n+        )\n+\n+    @skipUnlessDBFeature(\"supports_partial_indexes\")\n+    def test_full_clean_with_partial_unique_constraints_disabled(self):\n+        UniqueConstraintConditionProduct.objects.create(name=\"product\")\n+        product = UniqueConstraintConditionProduct(name=\"product\")\n+        product.full_clean(validate_constraints=False)\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Moving all constraint validation to a dedicated `validate_constraint` method aligns with best practices and simplifies the logic in `Model._get_unique_checks`. The validation error messages should leverage `Model.unique_error_message` for consistency, especially to pass existing tests. The handling of exclusions in the `validate()` calls appears fragmented and could benefit from clearer comments. Additionally, introducing a model parameter for constraints would enhance usability but requires broader method refactoring, which might be beyond this PR's scope. Lastly, shifting to the `WHERE` clause in generated queries should streamline SQL generation, reducing unnecessary complexity."
      }
    ],
    "codediff_length": 18718
  },
  {
    "index": 23,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/utils/choices.py b/django/utils/choices.py\nnew file mode 100644\nindex 000000000000..fc8267af3431\n--- /dev/null\n+++ b/django/utils/choices.py\n@@ -0,0 +1,63 @@\n+from collections.abc import Callable, Iterable, Iterator, Mapping\n+\n+from django.db.models.enums import ChoicesMeta\n+from django.utils.functional import Promise\n+\n+\n+class ChoiceIterator:\n+    \"\"\"Base class for lazy iterators for choices.\"\"\"\n+\n+\n+class CallableChoiceIterator(ChoiceIterator):\n+    \"\"\"Iterator to lazily normalize choices generated by a callable.\"\"\"\n+\n+    def __init__(self, func):\n+        self.func = func\n+\n+    def __iter__(self):\n+        yield from normalize_choices(self.func())\n+\n+\n+def normalize_choices(value, *, depth=0):\n+    \"\"\"Normalize choices values consistently for fields and widgets.\"\"\"\n+\n+    match value:\n+        case ChoiceIterator() | Promise() | bytes() | str():\n+            # Avoid prematurely normalizing iterators that should be lazy.\n+            # Because string-like types are iterable, return early to avoid\n+            # iterating over them in the guard for the Iterable case below.\n+            return value\n+        case ChoicesMeta():\n+            # Choices enumeration helpers already output in canonical form.\n+            return value.choices\n+        case Mapping() if depth < 2:\n+            value = value.items()\n+        case Iterator() if depth < 2:\n+            # Although Iterator would be handled by the Iterable case below,\n+            # the iterator would be consumed prematurely while checking that\n+            # its elements are not string-like in the guard, so we handle it\n+            # separately.\n+            pass\n+        case Iterable() if depth < 2 and not any(\n+            isinstance(x, (Promise, bytes, str)) for x in value\n+        ):\n+            # String-like types are iterable, so the guard above ensures that\n+            # they're handled by the default case below.\n+            pass\n+        case Callable() if depth == 0:\n+            # If at the top level, wrap callables to be evaluated lazily.\n+            return CallableChoiceIterator(value)\n+        case Callable() if depth < 2:\n+            value = value()\n+        case _:\n+            return value\n+\n+    try:\n+        # Recursive call to convert any nested values to a list of 2-tuples.\n+        return [(k, normalize_choices(v, depth=depth + 1)) for k, v in value]\n+    except (TypeError, ValueError):\n+        # Return original value for the system check to raise if it has items\n+        # that are not iterable or not 2-tuples:\n+        # - TypeError: cannot unpack non-iterable <type> object\n+        # - ValueError: <not enough / too many> values to unpack\n+        return value\ndiff --git a/tests/utils_tests/test_choices.py b/tests/utils_tests/test_choices.py\nnew file mode 100644\nindex 000000000000..d96c3d49c4f4\n--- /dev/null\n+++ b/tests/utils_tests/test_choices.py\n@@ -0,0 +1,305 @@\n+from unittest import mock\n+\n+from django.db.models import TextChoices\n+from django.test import SimpleTestCase\n+from django.utils.choices import CallableChoiceIterator, normalize_choices\n+from django.utils.translation import gettext_lazy as _\n+\n+\n+class NormalizeFieldChoicesTests(SimpleTestCase):\n+    expected = [\n+        (\"C\", _(\"Club\")),\n+        (\"D\", _(\"Diamond\")),\n+        (\"H\", _(\"Heart\")),\n+        (\"S\", _(\"Spade\")),\n+    ]\n+    expected_nested = [\n+        (\"Audio\", [(\"vinyl\", _(\"Vinyl\")), (\"cd\", _(\"CD\"))]),\n+        (\"Video\", [(\"vhs\", _(\"VHS Tape\")), (\"dvd\", _(\"DVD\"))]),\n+        (\"unknown\", _(\"Unknown\")),\n+    ]\n+    invalid = [\n+        1j,\n+        123,\n+        123.45,\n+        \"invalid\",\n+        b\"invalid\",\n+        _(\"invalid\"),\n+        object(),\n+        None,\n+        True,\n+        False,\n+    ]\n+    invalid_iterable = [\n+        # Special cases of a string-likes which would unpack incorrectly.\n+        [\"ab\"],\n+        [b\"ab\"],\n+        [_(\"ab\")],\n+        # Non-iterable items or iterable items with incorrect number of\n+        # elements that cannot be unpacked.\n+        [123],\n+        [(\"value\",)],\n+        [(\"value\", \"label\", \"other\")],\n+    ]\n+    invalid_nested = [\n+        # Nested choices can only be two-levels deep, so return callables,\n+        # mappings, iterables, etc. at deeper levels unmodified.\n+        [(\"Group\", [(\"Value\", lambda: \"Label\")])],\n+        [(\"Group\", [(\"Value\", {\"Label 1?\": \"Label 2?\"})])],\n+        [(\"Group\", [(\"Value\", [(\"Label 1?\", \"Label 2?\")])])],\n+    ]\n+\n+    def test_empty(self):\n+        def generator():\n+            yield from ()\n+\n+        for choices in ({}, [], (), set(), frozenset(), generator()):\n+            with self.subTest(choices=choices):\n+                self.assertEqual(normalize_choices(choices), [])\n+\n+    def test_choices(self):\n+        class Medal(TextChoices):\n+            GOLD = \"GOLD\", _(\"Gold\")\n+            SILVER = \"SILVER\", _(\"Silver\")\n+            BRONZE = \"BRONZE\", _(\"Bronze\")\n+\n+        expected = [\n+            (\"GOLD\", _(\"Gold\")),\n+            (\"SILVER\", _(\"Silver\")),\n+            (\"BRONZE\", _(\"Bronze\")),\n+        ]\n+        self.assertEqual(normalize_choices(Medal), expected)\n+\n+    def test_callable(self):\n+        def get_choices():\n+            return {\n+                \"C\": _(\"Club\"),\n+                \"D\": _(\"Diamond\"),\n+                \"H\": _(\"Heart\"),\n+                \"S\": _(\"Spade\"),\n+            }\n+\n+        get_choices_spy = mock.Mock(wraps=get_choices)\n+        output = normalize_choices(get_choices_spy)\n+\n+        get_choices_spy.assert_not_called()\n+        self.assertIsInstance(output, CallableChoiceIterator)\n+        self.assertEqual(list(output), self.expected)\n+        get_choices_spy.assert_called_once()\n+\n+    def test_mapping(self):\n+        choices = {\n+            \"C\": _(\"Club\"),\n+            \"D\": _(\"Diamond\"),\n+            \"H\": _(\"Heart\"),\n+            \"S\": _(\"Spade\"),\n+        }\n+        self.assertEqual(normalize_choices(choices), self.expected)\n+\n+    def test_iterable(self):\n+        choices = [\n+            (\"C\", _(\"Club\")),\n+            (\"D\", _(\"Diamond\")),\n+            (\"H\", _(\"Heart\")),\n+            (\"S\", _(\"Spade\")),\n+        ]\n+        self.assertEqual(normalize_choices(choices), self.expected)\n+\n+    def test_iterator(self):\n+        def generator():\n+            yield \"C\", _(\"Club\")\n+            yield \"D\", _(\"Diamond\")\n+            yield \"H\", _(\"Heart\")\n+            yield \"S\", _(\"Spade\")\n+\n+        choices = generator()\n+        self.assertEqual(normalize_choices(choices), self.expected)\n+\n+    def test_nested_callable(self):\n+        def get_audio_choices():\n+            return [(\"vinyl\", _(\"Vinyl\")), (\"cd\", _(\"CD\"))]\n+\n+        def get_video_choices():\n+            return [(\"vhs\", _(\"VHS Tape\")), (\"dvd\", _(\"DVD\"))]\n+\n+        def get_media_choices():\n+            return [\n+                (\"Audio\", get_audio_choices),\n+                (\"Video\", get_video_choices),\n+                (\"unknown\", _(\"Unknown\")),\n+            ]\n+\n+        get_media_choices_spy = mock.Mock(wraps=get_media_choices)\n+        output = normalize_choices(get_media_choices_spy)\n+\n+        get_media_choices_spy.assert_not_called()\n+        self.assertIsInstance(output, CallableChoiceIterator)\n+        self.assertEqual(list(output), self.expected_nested)\n+        get_media_choices_spy.assert_called_once()\n+\n+    def test_nested_mapping(self):\n+        choices = {\n+            \"Audio\": {\"vinyl\": _(\"Vinyl\"), \"cd\": _(\"CD\")},\n+            \"Video\": {\"vhs\": _(\"VHS Tape\"), \"dvd\": _(\"DVD\")},\n+            \"unknown\": _(\"Unknown\"),\n+        }\n+        self.assertEqual(normalize_choices(choices), self.expected_nested)\n+\n+    def test_nested_iterable(self):\n+        choices = [\n+            (\"Audio\", [(\"vinyl\", _(\"Vinyl\")), (\"cd\", _(\"CD\"))]),\n+            (\"Video\", [(\"vhs\", _(\"VHS Tape\")), (\"dvd\", _(\"DVD\"))]),\n+            (\"unknown\", _(\"Unknown\")),\n+        ]\n+        self.assertEqual(normalize_choices(choices), self.expected_nested)\n+\n+    def test_nested_iterator(self):\n+        def generate_audio_choices():\n+            yield \"vinyl\", _(\"Vinyl\")\n+            yield \"cd\", _(\"CD\")\n+\n+        def generate_video_choices():\n+            yield \"vhs\", _(\"VHS Tape\")\n+            yield \"dvd\", _(\"DVD\")\n+\n+        def generate_media_choices():\n+            yield \"Audio\", generate_audio_choices()\n+            yield \"Video\", generate_video_choices()\n+            yield \"unknown\", _(\"Unknown\")\n+\n+        choices = generate_media_choices()\n+        self.assertEqual(normalize_choices(choices), self.expected_nested)\n+\n+    def test_callable_non_canonical(self):\n+        # Canonical form is list of 2-tuple, but nested lists should work.\n+        def get_choices():\n+            return [\n+                [\"C\", _(\"Club\")],\n+                [\"D\", _(\"Diamond\")],\n+                [\"H\", _(\"Heart\")],\n+                [\"S\", _(\"Spade\")],\n+            ]\n+\n+        get_choices_spy = mock.Mock(wraps=get_choices)\n+        output = normalize_choices(get_choices_spy)\n+\n+        get_choices_spy.assert_not_called()\n+        self.assertIsInstance(output, CallableChoiceIterator)\n+        self.assertEqual(list(output), self.expected)\n+        get_choices_spy.assert_called_once()\n+\n+    def test_iterable_non_canonical(self):\n+        # Canonical form is list of 2-tuple, but nested lists should work.\n+        choices = [\n+            [\"C\", _(\"Club\")],\n+            [\"D\", _(\"Diamond\")],\n+            [\"H\", _(\"Heart\")],\n+            [\"S\", _(\"Spade\")],\n+        ]\n+        self.assertEqual(normalize_choices(choices), self.expected)\n+\n+    def test_iterator_non_canonical(self):\n+        # Canonical form is list of 2-tuple, but nested lists should work.\n+        def generator():\n+            yield [\"C\", _(\"Club\")]\n+            yield [\"D\", _(\"Diamond\")]\n+            yield [\"H\", _(\"Heart\")]\n+            yield [\"S\", _(\"Spade\")]\n+\n+        choices = generator()\n+        self.assertEqual(normalize_choices(choices), self.expected)\n+\n+    def test_nested_callable_non_canonical(self):\n+        # Canonical form is list of 2-tuple, but nested lists should work.\n+\n+        def get_audio_choices():\n+            return [[\"vinyl\", _(\"Vinyl\")], [\"cd\", _(\"CD\")]]\n+\n+        def get_video_choices():\n+            return [[\"vhs\", _(\"VHS Tape\")], [\"dvd\", _(\"DVD\")]]\n+\n+        def get_media_choices():\n+            return [\n+                [\"Audio\", get_audio_choices],\n+                [\"Video\", get_video_choices],\n+                [\"unknown\", _(\"Unknown\")],\n+            ]\n+\n+        get_media_choices_spy = mock.Mock(wraps=get_media_choices)\n+        output = normalize_choices(get_media_choices_spy)\n+\n+        get_media_choices_spy.assert_not_called()\n+        self.assertIsInstance(output, CallableChoiceIterator)\n+        self.assertEqual(list(output), self.expected_nested)\n+        get_media_choices_spy.assert_called_once()\n+\n+    def test_nested_iterable_non_canonical(self):\n+        # Canonical form is list of 2-tuple, but nested lists should work.\n+        choices = [\n+            [\"Audio\", [[\"vinyl\", _(\"Vinyl\")], [\"cd\", _(\"CD\")]]],\n+            [\"Video\", [[\"vhs\", _(\"VHS Tape\")], [\"dvd\", _(\"DVD\")]]],\n+            [\"unknown\", _(\"Unknown\")],\n+        ]\n+        self.assertEqual(normalize_choices(choices), self.expected_nested)\n+\n+    def test_nested_iterator_non_canonical(self):\n+        # Canonical form is list of 2-tuple, but nested lists should work.\n+        def generator():\n+            yield [\"Audio\", [[\"vinyl\", _(\"Vinyl\")], [\"cd\", _(\"CD\")]]]\n+            yield [\"Video\", [[\"vhs\", _(\"VHS Tape\")], [\"dvd\", _(\"DVD\")]]]\n+            yield [\"unknown\", _(\"Unknown\")]\n+\n+        choices = generator()\n+        self.assertEqual(normalize_choices(choices), self.expected_nested)\n+\n+    def test_nested_mixed_mapping_and_iterable(self):\n+        # Although not documented, as it's better to stick to either mappings\n+        # or iterables, nesting of mappings within iterables and vice versa\n+        # works and is likely to occur in the wild. This is supported by the\n+        # recursive call to `normalize_choices()` which will normalize nested\n+        # choices.\n+        choices = {\n+            \"Audio\": [(\"vinyl\", _(\"Vinyl\")), (\"cd\", _(\"CD\"))],\n+            \"Video\": [(\"vhs\", _(\"VHS Tape\")), (\"dvd\", _(\"DVD\"))],\n+            \"unknown\": _(\"Unknown\"),\n+        }\n+        self.assertEqual(normalize_choices(choices), self.expected_nested)\n+        choices = [\n+            (\"Audio\", {\"vinyl\": _(\"Vinyl\"), \"cd\": _(\"CD\")}),\n+            (\"Video\", {\"vhs\": _(\"VHS Tape\"), \"dvd\": _(\"DVD\")}),\n+            (\"unknown\", _(\"Unknown\")),\n+        ]\n+        self.assertEqual(normalize_choices(choices), self.expected_nested)\n+\n+    def test_iterable_set(self):\n+        # Although not documented, as sets are unordered which results in\n+        # randomised order in form fields, passing a set of 2-tuples works.\n+        # Consistent ordering of choices on model fields in migrations is\n+        # enforced by the migrations serializer.\n+        choices = {\n+            (\"C\", _(\"Club\")),\n+            (\"D\", _(\"Diamond\")),\n+            (\"H\", _(\"Heart\")),\n+            (\"S\", _(\"Spade\")),\n+        }\n+        self.assertEqual(sorted(normalize_choices(choices)), sorted(self.expected))\n+\n+    def test_unsupported_values_returned_unmodified(self):\n+        # Unsupported values must be returned unmodified for model system check\n+        # to work correctly.\n+        for value in self.invalid + self.invalid_iterable + self.invalid_nested:\n+            with self.subTest(value=value):\n+                self.assertEqual(normalize_choices(value), value)\n+\n+    def test_unsupported_values_from_callable_returned_unmodified(self):\n+        for value in self.invalid_iterable + self.invalid_nested:\n+            with self.subTest(value=value):\n+                self.assertEqual(list(normalize_choices(lambda: value)), value)\n+\n+    def test_unsupported_values_from_iterator_returned_unmodified(self):\n+        for value in self.invalid_nested:\n+            with self.subTest(value=value):\n+                self.assertEqual(\n+                    list(normalize_choices((lambda: (yield from value))())),\n+                    value,\n+                )\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR introduces significant improvements to `normalize_choices()`, including explicit tests and renaming for clarity. However, the removal of callable support for model fields has led to regressions: passing a callable now bypasses validation checks, and `makemigrations` generates infinite migrations. To address this while preserving existing functionality, consider reintroducing handling for callable choices without full unrolling, potentially updating the `deconstruct` method to return wrapped functions. This preserves compatibility and avoids further complications related to ticket-24561. Let's prioritize stability before the feature freeze."
      }
    ],
    "codediff_length": 14144
  },
  {
    "index": 24,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/tests/csrf_tests/tests.py b/tests/csrf_tests/tests.py\nindex cd7ea8fbc606..a326df7f1afc 100644\n--- a/tests/csrf_tests/tests.py\n+++ b/tests/csrf_tests/tests.py\n@@ -107,77 +106,6 @@ def _check_token_present(self, response, csrf_id=None):\n             \"Could not find csrfmiddlewaretoken to match %s\" % csrf_token\n         )\n \n-    def test_process_view_token_too_long(self):\n-        \"\"\"\n-        If the token is longer than expected, it is ignored and a new token is\n-        created.\n-        \"\"\"\n-        req = self._get_GET_no_csrf_cookie_request()\n-        req.COOKIES[settings.CSRF_COOKIE_NAME] = 'x' * 100000\n-        CsrfViewMiddleware().process_view(req, token_view, (), {})\n-        resp = token_view(req)\n-        resp2 = CsrfViewMiddleware().process_response(req, resp)\n-        csrf_cookie = resp2.cookies.get(settings.CSRF_COOKIE_NAME, False)\n-        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n-\n-    def test_process_view_token_invalid_chars(self):\n-        \"\"\"\n-        If the token contains non-alphanumeric characters, it is ignored and a\n-        new token is created.\n-        \"\"\"\n-        token = ('!@#' + self._csrf_id)[:CSRF_TOKEN_LENGTH]\n-        req = self._get_GET_no_csrf_cookie_request()\n-        req.COOKIES[settings.CSRF_COOKIE_NAME] = token\n-        CsrfViewMiddleware().process_view(req, token_view, (), {})\n-        resp = token_view(req)\n-        resp2 = CsrfViewMiddleware().process_response(req, resp)\n-        csrf_cookie = resp2.cookies.get(settings.CSRF_COOKIE_NAME, False)\n-        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n-        self.assertNotEqual(csrf_cookie.value, token)\n-\n-    def test_process_view_token_invalid_bytes(self):\n-        \"\"\"\n-        If the token contains improperly encoded unicode, it is ignored and a\n-        new token is created.\n-        \"\"\"\n-        token = (b\"<1>\\xc2\\xa1\" + force_bytes(self._csrf_id, 'ascii'))[:CSRF_TOKEN_LENGTH]\n-        req = self._get_GET_no_csrf_cookie_request()\n-        req.COOKIES[settings.CSRF_COOKIE_NAME] = token\n-        # We expect a UnicodeWarning here, because we used broken utf-8 on purpose\n-        with warnings.catch_warnings():\n-            warnings.filterwarnings(\"ignore\", category=UnicodeWarning)\n-            CsrfViewMiddleware().process_view(req, token_view, (), {})\n-        resp = token_view(req)\n-        resp2 = CsrfViewMiddleware().process_response(req, resp)\n-        csrf_cookie = resp2.cookies.get(settings.CSRF_COOKIE_NAME, False)\n-        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n-        self.assertNotEqual(csrf_cookie.value, token)\n-\n-    def test_process_response_get_token_used(self):\n-        \"\"\"\n-        When get_token is used, check that the cookie is created and headers\n-        patched.\n-        \"\"\"\n-        req = self._get_GET_no_csrf_cookie_request()\n-\n-        # Put tests for CSRF_COOKIE_* settings here\n-        with self.settings(CSRF_COOKIE_NAME='myname',\n-                           CSRF_COOKIE_DOMAIN='.example.com',\n-                           CSRF_COOKIE_PATH='/test/',\n-                           CSRF_COOKIE_SECURE=True,\n-                           CSRF_COOKIE_HTTPONLY=True):\n-            # token_view calls get_token() indirectly\n-            CsrfViewMiddleware().process_view(req, token_view, (), {})\n-            resp = token_view(req)\n-            resp2 = CsrfViewMiddleware().process_response(req, resp)\n-        csrf_cookie = resp2.cookies.get('myname', False)\n-        self.assertIsNot(csrf_cookie, False)\n-        self.assertEqual(csrf_cookie['domain'], '.example.com')\n-        self.assertIs(csrf_cookie['secure'], True)\n-        self.assertIs(csrf_cookie['httponly'], True)\n-        self.assertEqual(csrf_cookie['path'], '/test/')\n-        self.assertIn('Cookie', resp2.get('Vary', ''))\n-\n     def test_process_response_get_token_not_used(self):\n         \"\"\"\n         If get_token() is not called, the view middleware does not\n@@ -535,21 +435,98 @@ def test_https_good_referer_matches_cookie_domain_with_different_port(self):\n         response = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n         self.assertIsNone(response)\n \n-    @override_settings(CSRF_COOKIE_DOMAIN='.example.com', DEBUG=True)\n-    def test_https_reject_insecure_referer(self):\n+    def test_ensures_csrf_cookie_no_logging(self):\n         \"\"\"\n-        A POST HTTPS request from an insecure referer should be rejected.\n+        ensure_csrf_cookie() doesn't log warnings (#19436).\n         \"\"\"\n-        req = self._get_POST_request_with_token()\n-        req._is_secure_override = True\n-        req.META['HTTP_REFERER'] = 'http://example.com/'\n-        req.META['SERVER_PORT'] = '443'\n-        response = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n-        self.assertContains(\n-            response,\n-            'Referer checking failed - Referer is insecure while host is secure.',\n-            status_code=403,\n-        )\n+        @ensure_csrf_cookie\n+        def view(request):\n+            # Doesn't insert a token or anything\n+            return HttpResponse(content=\"\")\n+\n+        class TestHandler(logging.Handler):\n+            def emit(self, record):\n+                raise Exception(\"This shouldn't have happened!\")\n+\n+        logger = logging.getLogger('django.request')\n+        test_handler = TestHandler()\n+        old_log_level = logger.level\n+        try:\n+            logger.addHandler(test_handler)\n+            logger.setLevel(logging.WARNING)\n+\n+            req = self._get_GET_no_csrf_cookie_request()\n+            view(req)\n+        finally:\n+            logger.removeHandler(test_handler)\n+            logger.setLevel(old_log_level)\n+\n+    def test_post_data_read_failure(self):\n+        \"\"\"\n+        #20128 -- IOErrors during POST data reading should be caught and\n+        treated as if the POST data wasn't there.\n+        \"\"\"\n+        class CsrfPostRequest(HttpRequest):\n+            \"\"\"\n+            HttpRequest that can raise an IOError when accessing POST data\n+            \"\"\"\n+            def __init__(self, token, raise_error):\n+                super(CsrfPostRequest, self).__init__()\n+                self.method = 'POST'\n+\n+                self.raise_error = False\n+                self.COOKIES[settings.CSRF_COOKIE_NAME] = token\n+\n+                # We handle both cases here to prevent a lot of duplicate code\n+                # in the session tests\n+                self.session = {}\n+                self.session[CSRF_SESSION_KEY] = token\n+\n+                self.POST['csrfmiddlewaretoken'] = token\n+                self.raise_error = raise_error\n+\n+            def _load_post_and_files(self):\n+                raise IOError('error reading input data')\n+\n+            def _get_post(self):\n+                if self.raise_error:\n+                    self._load_post_and_files()\n+                return self._post\n+\n+            def _set_post(self, post):\n+                self._post = post\n+\n+            POST = property(_get_post, _set_post)\n+\n+        token = ('ABC' + self._csrf_id)[:CSRF_TOKEN_LENGTH]\n+\n+        req = CsrfPostRequest(token, raise_error=False)\n+        resp = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n+        self.assertIsNone(resp)\n+\n+        req = CsrfPostRequest(token, raise_error=True)\n+        with patch_logger('django.security.csrf', 'warning') as logger_calls:\n+            resp = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n+            self.assertEqual(resp.status_code, 403)\n+            self.assertEqual(logger_calls[0], 'Forbidden (%s): ' % REASON_BAD_TOKEN)\n+\n+\n+class CsrfViewMiddlewareTests(CsrfViewMiddlewareTestMixin, SimpleTestCase):\n+\n+    def _get_GET_csrf_cookie_request(self):\n+        req = TestingHttpRequest()\n+        req.COOKIES[settings.CSRF_COOKIE_NAME] = self._csrf_id_cookie\n+        return req\n+\n+    def _get_POST_bare_secret_csrf_cookie_request(self):\n+        req = self._get_POST_no_csrf_cookie_request()\n+        req.COOKIES[settings.CSRF_COOKIE_NAME] = self._csrf_id_cookie[:32]\n+        return req\n+\n+    def _get_POST_bare_secret_csrf_cookie_request_with_token(self):\n+        req = self._get_POST_bare_secret_csrf_cookie_request()\n+        req.POST['csrfmiddlewaretoken'] = self._csrf_id_cookie[:32]\n+        return req\n \n     def test_ensures_csrf_cookie_no_middleware(self):\n         \"\"\"\n@@ -651,45 +602,206 @@ def test_csrf_cookie_age_none(self):\n             max_age = resp2.cookies.get('csrfcookie').get('max-age')\n             self.assertEqual(max_age, '')\n \n-    def test_post_data_read_failure(self):\n+    def test_process_view_token_too_long(self):\n         \"\"\"\n-        #20128 -- IOErrors during POST data reading should be caught and\n-        treated as if the POST data wasn't there.\n+        If the token is longer than expected, it is ignored and a new token is\n+        created.\n         \"\"\"\n-        class CsrfPostRequest(HttpRequest):\n-            \"\"\"\n-            HttpRequest that can raise an IOError when accessing POST data\n-            \"\"\"\n-            def __init__(self, token, raise_error):\n-                super(CsrfPostRequest, self).__init__()\n-                self.method = 'POST'\n+        req = self._get_GET_no_csrf_cookie_request()\n+        req.COOKIES[settings.CSRF_COOKIE_NAME] = 'x' * 100000\n+        CsrfViewMiddleware().process_view(req, token_view, (), {})\n+        resp = token_view(req)\n+        resp2 = CsrfViewMiddleware().process_response(req, resp)\n+        csrf_cookie = resp2.cookies.get(settings.CSRF_COOKIE_NAME, False)\n+        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n \n-                self.raise_error = False\n-                self.COOKIES[settings.CSRF_COOKIE_NAME] = token\n-                self.POST['csrfmiddlewaretoken'] = token\n-                self.raise_error = raise_error\n+    def test_process_view_token_invalid_bytes(self):\n+        \"\"\"\n+        If the token contains improperly encoded unicode, it is ignored and a\n+        new token is created.\n+        \"\"\"\n+        token = (b\"<1>\\xc2\\xa1\" + force_bytes(self._csrf_id, 'ascii'))[:CSRF_TOKEN_LENGTH]\n+        req = self._get_GET_no_csrf_cookie_request()\n+        req.COOKIES[settings.CSRF_COOKIE_NAME] = token\n+        # We expect a UnicodeWarning here, because we used broken utf-8 on purpose\n+        with warnings.catch_warnings():\n+            warnings.filterwarnings(\"ignore\", category=UnicodeWarning)\n+            CsrfViewMiddleware().process_view(req, token_view, (), {})\n+        resp = token_view(req)\n+        resp2 = CsrfViewMiddleware().process_response(req, resp)\n+        csrf_cookie = resp2.cookies.get(settings.CSRF_COOKIE_NAME, False)\n+        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n+        self.assertNotEqual(csrf_cookie.value, token)\n \n-            def _load_post_and_files(self):\n-                raise IOError('error reading input data')\n+    def test_process_view_token_invalid_chars(self):\n+        \"\"\"\n+        If the token contains non-alphanumeric characters, it is ignored and a\n+        new token is created.\n+        \"\"\"\n+        token = ('!@#' + self._csrf_id)[:CSRF_TOKEN_LENGTH]\n+        req = self._get_GET_no_csrf_cookie_request()\n+        req.COOKIES[settings.CSRF_COOKIE_NAME] = token\n+        CsrfViewMiddleware().process_view(req, token_view, (), {})\n+        resp = token_view(req)\n+        resp2 = CsrfViewMiddleware().process_response(req, resp)\n+        csrf_cookie = resp2.cookies.get(settings.CSRF_COOKIE_NAME, False)\n+        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n+        self.assertNotEqual(csrf_cookie.value, token)\n \n-            def _get_post(self):\n-                if self.raise_error:\n-                    self._load_post_and_files()\n-                return self._post\n+    def test_bare_secret_accepted_and_replaced(self):\n+        \"\"\"\n+        The csrf token is reset from a bare secret.\n+        \"\"\"\n+        req = self._get_POST_bare_secret_csrf_cookie_request_with_token()\n+        req2 = CsrfViewMiddleware().process_view(req, token_view, (), {})\n+        self.assertIsNone(req2)\n+        resp = token_view(req)\n+        resp = CsrfViewMiddleware().process_response(req, resp)\n+        self.assertIn(settings.CSRF_COOKIE_NAME, resp.cookies, \"Cookie was not reset from bare secret\")\n+        csrf_cookie = resp.cookies[settings.CSRF_COOKIE_NAME]\n+        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n+        self._check_token_present(resp, csrf_id=csrf_cookie.value)\n \n-            def _set_post(self, post):\n-                self._post = post\n+    @override_settings(ALLOWED_HOSTS=['www.example.com'], CSRF_COOKIE_DOMAIN='.example.com', USE_X_FORWARDED_PORT=True)\n+    def test_https_good_referer_behind_proxy(self):\n+        \"\"\"\n+        A POST HTTPS request is accepted when USE_X_FORWARDED_PORT=True.\n+        \"\"\"\n+        self._test_https_good_referer_behind_proxy()\n \n-            POST = property(_get_post, _set_post)\n+    @override_settings(ALLOWED_HOSTS=['www.example.com'], CSRF_COOKIE_DOMAIN='.example.com')\n+    def test_https_good_referer_matches_cookie_domain(self):\n+        \"\"\"\n+        A POST HTTPS request with a good referer should be accepted from a\n+        subdomain that's allowed by CSRF_COOKIE_DOMAIN.\n+        \"\"\"\n+        self._test_https_good_referer_matches_cookie_domain()\n \n-        token = ('ABC' + self._csrf_id)[:CSRF_TOKEN_LENGTH]\n+    @override_settings(ALLOWED_HOSTS=['www.example.com'], CSRF_COOKIE_DOMAIN='.example.com')\n+    def test_https_good_referer_matches_cookie_domain_with_different_port(self):\n+        \"\"\"\n+        A POST HTTPS request with a good referer should be accepted from a\n+        subdomain that's allowed by CSRF_COOKIE_DOMAIN and a non-443 port.\n+        \"\"\"\n+        self._test_https_good_referer_matches_cookie_domain_with_different_port()\n \n-        req = CsrfPostRequest(token, raise_error=False)\n-        resp = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n-        self.assertIsNone(resp)\n+    @override_settings(CSRF_COOKIE_DOMAIN='.example.com', DEBUG=True)\n+    def test_https_reject_insecure_referer(self):\n+        \"\"\"\n+        A POST HTTPS request from an insecure referer should be rejected.\n+        \"\"\"\n+        req = self._get_POST_request_with_token()\n+        req._is_secure_override = True\n+        req.META['HTTP_REFERER'] = 'http://example.com/'\n+        req.META['SERVER_PORT'] = '443'\n+        response = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n+        self.assertContains(\n+            response,\n+            'Referer checking failed - Referer is insecure while host is secure.',\n+            status_code=403,\n+        )\n \n-        req = CsrfPostRequest(token, raise_error=True)\n-        with patch_logger('django.security.csrf', 'warning') as logger_calls:\n-            resp = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n-            self.assertEqual(resp.status_code, 403)\n-            self.assertEqual(logger_calls[0], 'Forbidden (%s): ' % REASON_BAD_TOKEN)\n+\n+@override_settings(CSRF_USE_SESSIONS=True, CSRF_COOKIE_DOMAIN=None)\n+class CsrfViewMiddlewareUseSessionsTests(CsrfViewMiddlewareTestMixin, SimpleTestCase):\n+    \"\"\"\n+    CSRF tests with CSRF_USE_SESSIONS=True.\n+    \"\"\"\n+\n+    def _get_POST_bare_secret_csrf_cookie_request(self):\n+        req = self._get_POST_no_csrf_cookie_request()\n+        req.session[CSRF_SESSION_KEY] = self._csrf_id_cookie[:32]\n+        return req\n+\n+    def _get_GET_csrf_cookie_request(self):\n+        req = TestingHttpRequest()\n+        req.session[CSRF_SESSION_KEY] = self._csrf_id_cookie\n+        return req\n+\n+    def test_no_session_on_request(self):\n+        def view(request):\n+            # Doesn't insert a token or anything\n+            return HttpResponse(content=\"\")\n+\n+        request = HttpRequest()\n+        with self.assertRaises(ImproperlyConfigured):\n+            CsrfViewMiddleware().process_view(request, view, (), {})\n+\n+    def test_process_response_get_token_used(self):\n+        \"\"\"The ensure_csrf_cookie() decorator works without middleware.\"\"\"\n+        @ensure_csrf_cookie\n+        def view(request):\n+            # Doesn't insert a token or anything\n+            return HttpResponse(content=\"\")\n+\n+        req = self._get_GET_no_csrf_cookie_request()\n+        view(req)\n+        self.assertTrue(req.session.get(CSRF_SESSION_KEY, False))\n+\n+    def test_ensures_csrf_cookie_with_middleware(self):\n+        \"\"\"\n+        The ensure_csrf_cookie() decorator works with the CsrfViewMiddleware\n+        enabled.\n+        \"\"\"\n+        @ensure_csrf_cookie\n+        def view(request):\n+            # Doesn't insert a token or anything\n+            return HttpResponse(content=\"\")\n+\n+        req = self._get_GET_no_csrf_cookie_request()\n+        CsrfViewMiddleware().process_view(req, view, (), {})\n+        resp = view(req)\n+        CsrfViewMiddleware().process_response(req, resp)\n+        self.assertTrue(req.session.get(CSRF_SESSION_KEY, False))\n+\n+    def test_token_node_with_new_csrf_cookie(self):\n+        \"\"\"\n+        CsrfTokenNode works when a CSRF cookie is created by the middleware\n+        (when one was not already present).\n+        \"\"\"\n+        req = self._get_GET_no_csrf_cookie_request()\n+        CsrfViewMiddleware().process_view(req, token_view, (), {})\n+        resp = token_view(req)\n+        CsrfViewMiddleware().process_response(req, resp)\n+        csrf_cookie = req.session[CSRF_SESSION_KEY]\n+        self._check_token_present(resp, csrf_id=csrf_cookie)\n+\n+    @override_settings(ALLOWED_HOSTS=['www.example.com'], SESSION_COOKIE_DOMAIN='.example.com',\n+                       USE_X_FORWARDED_PORT=True, DEBUG=True)\n+    def test_https_good_referer_behind_proxy(self):\n+        \"\"\"\n+        A POST HTTPS request is accepted when USE_X_FORWARDED_PORT=True.\n+        \"\"\"\n+        self._test_https_good_referer_behind_proxy()\n+\n+    @override_settings(ALLOWED_HOSTS=['www.example.com'], SESSION_COOKIE_DOMAIN='.example.com')\n+    def test_https_good_referer_matches_cookie_domain(self):\n+        \"\"\"\n+        A POST HTTPS request with a good referer should be accepted from a\n+        subdomain that's allowed by SESSION_COOKIE_DOMAIN.\n+        \"\"\"\n+        self._test_https_good_referer_matches_cookie_domain()\n+\n+    @override_settings(ALLOWED_HOSTS=['www.example.com'], SESSION_COOKIE_DOMAIN='.example.com')\n+    def test_https_good_referer_matches_cookie_domain_with_different_port(self):\n+        \"\"\"\n+        A POST HTTPS request with a good referer should be accepted from a\n+        subdomain that's allowed by SESSION_COOKIE_DOMAIN and a non-443 port.\n+        \"\"\"\n+        self._test_https_good_referer_matches_cookie_domain_with_different_port()\n+\n+    @override_settings(SESSION_COOKIE_DOMAIN='.example.com', DEBUG=True)\n+    def test_https_reject_insecure_referer(self):\n+        \"\"\"\n+        A POST HTTPS request from an insecure referer should be rejected.\n+        \"\"\"\n+        req = self._get_POST_request_with_token()\n+        req._is_secure_override = True\n+        req.META['HTTP_REFERER'] = 'http://example.com/'\n+        req.META['SERVER_PORT'] = '443'\n+        response = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n+        self.assertContains(\n+            response,\n+            'Referer checking failed - Referer is insecure while host is secure.',\n+            status_code=403,\n+        )\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR addresses CSRF token handling and enhances compliance with common frameworks, which is beneficial for audits. The addition of `CSRF_SESSION_KEY` and adjustments to referer checks are solid improvements. However, the current AJAX examples are workarounds and fail with multiple formsconsider an enhanced solution like an optional argument for `csrf_token`. The testing strategy also needs more clarity; testing both session and cookie configurations is vital to avoid assumptions. Overall, thoughtful changes, but further refinement is necessary for wider applicability and robustness."
      }
    ],
    "codediff_length": 19394
  },
  {
    "index": 25,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/db/models/expressions.py b/django/db/models/expressions.py\nindex 1ff4cd77353d..36c2b969db12 100644\n--- a/django/db/models/expressions.py\n+++ b/django/db/models/expressions.py\n@@ -873,6 +900,128 @@ def as_sql(self, compiler, connection, template=None, case_joiner=None, **extra_\n         return sql, sql_params\n \n \n+class Subquery(Expression):\n+    \"\"\"\n+    An explicit subquery. It may contain OuterRef() references to the outer\n+    query which will be resolved when it is applied to that query.\n+    \"\"\"\n+    template = '(%(subquery)s)'\n+\n+    def __init__(self, queryset, output_field=None, **extra):\n+        self.queryset = queryset\n+        self.extra = extra\n+        if output_field is None and len(self.queryset.query.select) == 1:\n+            output_field = self.queryset.query.select[0].field\n+        super(Subquery, self).__init__(output_field)\n+\n+    def copy(self):\n+        clone = super(Subquery, self).copy()\n+        clone.queryset = clone.queryset.all()\n+        return clone\n+\n+    def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):\n+        clone = self.copy()\n+        clone.is_summary = summarize\n+        clone.queryset.query.bump_prefix(query)\n+\n+        # Need to recursively resolve these.\n+        def resolve_all(child):\n+            if hasattr(child, 'children'):\n+                [resolve_all(_child) for _child in child.children]\n+            if hasattr(child, 'rhs'):\n+                child.rhs = resolve(child.rhs)\n+\n+        def resolve(child):\n+            if hasattr(child, 'resolve_expression'):\n+                return child.resolve_expression(\n+                    query=query, allow_joins=allow_joins, reuse=reuse,\n+                    summarize=summarize, for_save=for_save,\n+                )\n+            return child\n+\n+        resolve_all(clone.queryset.query.where)\n+\n+        for key, value in clone.queryset.query.annotations.items():\n+            if isinstance(value, Subquery):\n+                clone.queryset.query.annotations[key] = resolve(value)\n+\n+        return clone\n+\n+    def get_source_expressions(self):\n+        return [\n+            x for x in [\n+                getattr(expr, 'lhs', None)\n+                for expr in self.queryset.query.where.children\n+            ] if x\n+        ]\n+\n+    def relabeled_clone(self, change_map):\n+        clone = self.copy()\n+        clone.queryset.query = clone.queryset.query.relabeled_clone(change_map)\n+        clone.queryset.query.external_aliases.update(\n+            alias for alias in change_map.values()\n+            if alias not in clone.queryset.query.tables\n+        )\n+        return clone\n+\n+    def as_sql(self, compiler, connection, template=None, **extra_context):\n+        connection.ops.check_expression_support(self)\n+        template_params = self.extra.copy()\n+        template_params.update(extra_context)\n+        template_params['subquery'], sql_params = self.queryset.query.get_compiler(connection=connection).as_sql()\n+\n+        template = template or template_params.get('template', self.template)\n+        sql = template % template_params\n+        sql = connection.ops.unification_cast_sql(self.output_field) % sql\n+        return sql, sql_params\n+\n+    def _prepare(self, output_field):\n+        # This method will only be called if this instance is the \"rhs\" in an\n+        # expression: the wrapping () must be removed (as the expression that\n+        # contains this will provide them). SQLite evaluates ((subquery))\n+        # differently than the other databases.\n+        if self.template == '(%(subquery)s)':\n+            clone = self.copy()\n+            clone.template = '%(subquery)s'\n+            return clone\n+        return self\n+\n+\n+class Exists(Subquery):\n+    template = 'EXISTS(%(subquery)s)'\n+\n+    def __init__(self, *args, **kwargs):\n+        self.negated = kwargs.pop('negated', False)\n+        super(Exists, self).__init__(*args, **kwargs)\n+\n+    def __invert__(self):\n+        return type(self)(self.queryset, self.output_field, negated=(not self.negated), **self.extra)\n+\n+    @property\n+    def output_field(self):\n+        return fields.BooleanField()\n+\n+    def resolve_expression(self, query=None, **kwargs):\n+        # As a performance optimization, remove ordering since EXISTS doesn't\n+        # care about it, just whether or not a row matches.\n+        self.queryset = self.queryset.order_by()\n+        return super(Exists, self).resolve_expression(query, **kwargs)\n+\n+    def as_sql(self, compiler, connection, template=None, **extra_context):\n+        sql, params = super(Exists, self).as_sql(compiler, connection, template, **extra_context)\n+        if self.negated:\n+            sql = 'NOT {}'.format(sql)\n+        return sql, params\n+\n+    def as_oracle(self, compiler, connection, template=None, **extra_context):\n+        # Oracle doesn't allow EXISTS() in the SELECT list, so wrap it with a\n+        # CASE WHEN expression. Change the template since the When expression\n+        # requires a left hand side (column) to compare against.\n+        sql, params = self.as_sql(compiler, connection, template, **extra_context)\n+        sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)\n+        return sql, params\n+\n+\n class OrderBy(BaseExpression):\n     template = '%(expression)s %(ordering)s'\n \ndiff --git a/docs/ref/models/expressions.txt b/docs/ref/models/expressions.txt\nindex e46df22f9828..01db10375878 100644\n--- a/docs/ref/models/expressions.txt\n+++ b/docs/ref/models/expressions.txt\n@@ -450,6 +450,178 @@ Conditional expressions allow you to use :keyword:`if` ... :keyword:`elif` ...\n :keyword:`else` logic in queries. Django natively supports SQL ``CASE``\n expressions. For more details see :doc:`conditional-expressions`.\n \n+``Subquery()`` expressions\n+--------------------------\n+\n+.. class:: Subquery(queryset, output_field=None)\n+\n+.. versionadded:: 1.11\n+\n+You can add an explicit subquery to a ``QuerySet`` using the ``Subquery``\n+expression.\n+\n+For example, to annotate each post with the email address of the author of the\n+newest comment on that post::\n+\n+    >>> from django.db.models import OuterRef, Subquery\n+    >>> newest = Comment.objects.filter(post=OuterRef('pk')).order_by('-created_at')\n+    >>> Post.objects.annotate(newest_commenter_email=Subquery(newest.values('email')[:1]))\n+\n+On PostgreSQL, the SQL looks like:\n+\n+.. code-block:: sql\n+\n+    SELECT \"post\".\"id\", (\n+        SELECT U0.\"email\"\n+        FROM \"comment\" U0\n+        WHERE U0.\"post_id\" = (\"post\".\"id\")\n+        ORDER BY U0.\"created_at\" DESC LIMIT 1\n+    ) AS \"newest_commenter_email\" FROM \"post\"\n+\n+.. note::\n+\n+    The examples in this section are designed to show how to force\n+    Django to execute a subquery. In some cases it may be possible to\n+    write an equivalent queryset that performs the same task more\n+    clearly or efficiently.\n+\n+Referencing columns from the outer queryset\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. class:: OuterRef(field)\n+\n+.. versionadded:: 1.11\n+\n+Use ``OuterRef`` when a queryset in a ``Subquery`` needs to refer to a field\n+from the outer query. It acts like an :class:`F` expression except that the\n+check to see if it refers to a valid field isn't made until the outer queryset\n+is resolved.\n+\n+Instances of ``OuterRef`` may be used in conjunction with nested instances\n+of ``Subquery`` to refer to a containing queryset that isn't the immediate\n+parent. For example, this queryset would need to be within a nested pair of\n+``Subquery`` instances to resolve correctly::\n+\n+    >>> Book.objects.filter(author=OuterRef(OuterRef('pk')))\n+\n+Limiting a subquery to a single column\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+There are times when a single column must be returned from a ``Subquery``, for\n+instance, to use a ``Subquery`` as the target of an ``__in`` lookup. To return\n+all comments for posts published within the last day::\n+\n+    >>> from datetime import timedelta\n+    >>> from django.utils import timezone\n+    >>> one_day_ago = timezone.now() - timedelta(days=1)\n+    >>> posts = Post.objects.filter(published_at__gte=one_day_ago)\n+    >>> Comment.objects.filter(post__in=Subquery(posts.values('pk')))\n+\n+In this case, the subquery must use :meth:`~.QuerySet.values`\n+to return only a single column: the primary key of the post.\n+\n+Limiting the subquery to a single row\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+To prevent a subquery from returning multiple rows, a slice (``[:1]``) of the\n+queryset is used::\n+\n+    >>> subquery = Subquery(newest.values('email')[:1])\n+    >>> Post.objects.annotate(newest_commenter_email=subquery)\n+\n+In this case, the subquery must only return a single column *and* a single\n+row: the email address of the most recently created comment.\n+\n+(Using :meth:`~.QuerySet.get` instead of a slice would fail because the\n+``OuterRef`` cannot be resolved until the queryset is used within a\n+``Subquery``.)\n+\n+``Exists()`` subqueries\n+~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. class:: Exists(queryset)\n+\n+.. versionadded:: 1.11\n+\n+``Exists`` is a ``Subquery`` subclass that uses an SQL ``EXISTS`` statement. In\n+many cases it will perform better than a subquery since the database is able to\n+stop evaluation of the subquery when a first matching row is found.\n+\n+For example, to annotate each post with whether or not it has a comment from\n+within the last day::\n+\n+    >>> from django.db.models import Exists, OuterRef\n+    >>> from datetime import timedelta\n+    >>> from django.utils import timezone\n+    >>> one_day_ago = timezone.now() - timedelta(days=1)\n+    >>> recent_comments = Comment.objects.filter(\n+    ...     post=OuterRef('pk'),\n+    ...     created_at__gte=one_day_ago,\n+    ... )\n+    >>> Post.objects.annotate(recent_comment=Exists(recent_comments)\n+\n+On PostgreSQL, the SQL looks like:\n+\n+.. code-block:: sql\n+\n+    SELECT \"post\".\"id\", \"post\".\"published_at\", EXISTS(\n+        SELECT U0.\"id\", U0.\"post_id\", U0.\"email\", U0.\"created_at\"\n+        FROM \"comment\" U0\n+        WHERE (\n+            U0.\"created_at\" >= YYYY-MM-DD HH:MM:SS AND\n+            U0.\"post_id\" = (\"post\".\"id\")\n+        )\n+    ) AS \"recent_comment\" FROM \"post\"\n+\n+It's unnecessary to force ``Exists`` to refer to a single column, since the\n+columns are discarded and a boolean result is returned. Similarly, since\n+ordering is unimportant within an SQL ``EXISTS`` subquery and would only\n+degrade performance, it's automatically removed.\n+\n+You can query using ``NOT EXISTS`` with ``~Exists()``.\n+\n+Filtering on a ``Subquery`` expression\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+It's not possible to filter directly using ``Subquery`` and ``Exists``, e.g.::\n+\n+    >>> Post.objects.filter(Exists(recent_comments))\n+    ...\n+    TypeError: 'Exists' object is not iterable\n+\n+\n+You must filter on a subquery expression by first annotating the queryset\n+and then filtering based on that annotation::\n+\n+    >>> Post.objects.annotate(\n+    ...     recent_comment=Exists(recent_comments),\n+    ... ).filter(recent_comment=True)\n+\n+Using aggregates within a ``Subquery`` expression\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+Aggregates may be used within a ``Subquery``, but they require a specific\n+combination of :meth:`~.QuerySet.filter`, :meth:`~.QuerySet.values`, and\n+:meth:`~.QuerySet.annotate` to get the subquery grouping correct.\n+\n+Assuming both models have a ``length`` field, to find posts where the post\n+length is greater than the total length of all combined comments::\n+\n+    >>> from django.db.models import OuterRef, Subquery, Sum\n+    >>> comments = Comment.objects.filter(post=OuterRef('pk')).values('post')\n+    >>> total_comments = comments.annotate(total=Sum('length')).values('total')\n+    >>> Post.objects.filter(length__gt=Subquery(total_comments))\n+\n+The initial ``filter(...)`` limits the subquery to the relevant parameters.\n+``values('post')`` aggregates comments by ``Post``. Finally, ``annotate(...)``\n+performs the aggregation. The order in which these queryset methods are applied\n+is important. In this case, since the subquery must be limited to a single\n+column, ``values('total')`` is required.\n+\n+This is the only way to perform an aggregation within a ``Subquery``, as\n+using :meth:`~.QuerySet.aggregate` attempts to evaluate the queryset (and if\n+there is an ``OuterRef``, this will not be possible to resolve).\n+\n Raw SQL expressions\n -------------------\n \ndiff --git a/tests/expressions/tests.py b/tests/expressions/tests.py\nindex 18d003d57d74..8399e3c0a92c 100644\n--- a/tests/expressions/tests.py\n+++ b/tests/expressions/tests.py\n@@ -387,6 +387,136 @@ def test_ticket_18375_chained_filters(self):\n         )\n         self.assertEqual(str(qs.query).count('JOIN'), 2)\n \n+    def test_outerref(self):\n+        inner = Company.objects.filter(point_of_contact=OuterRef('pk'))\n+        msg = (\n+            'This queryset contains a reference to an outer query and may only '\n+            'be used in a subquery.'\n+        )\n+        with self.assertRaisesMessage(ValueError, msg):\n+            inner.exists()\n+\n+        outer = Employee.objects.annotate(is_point_of_contact=Exists(inner))\n+        self.assertIs(outer.exists(), True)\n+\n+    def test_subquery(self):\n+        Company.objects.filter(name='Example Inc.').update(\n+            point_of_contact=Employee.objects.get(firstname='Joe', lastname='Smith'),\n+            ceo=Employee.objects.get(firstname='Max', lastname='Mustermann'),\n+        )\n+        Employee.objects.create(firstname='Bob', lastname='Brown', salary=40)\n+        qs = Employee.objects.annotate(\n+            is_point_of_contact=Exists(Company.objects.filter(point_of_contact=OuterRef('pk'))),\n+            is_not_point_of_contact=~Exists(Company.objects.filter(point_of_contact=OuterRef('pk'))),\n+            is_ceo_of_small_company=Exists(Company.objects.filter(num_employees__lt=200, ceo=OuterRef('pk'))),\n+            is_ceo_small_2=~~Exists(Company.objects.filter(num_employees__lt=200, ceo=OuterRef('pk'))),\n+            largest_company=Subquery(Company.objects.order_by('-num_employees').filter(\n+                models.Q(ceo=OuterRef('pk')) | models.Q(point_of_contact=OuterRef('pk'))\n+            ).values('name')[:1], output_field=models.CharField())\n+        ).values(\n+            'firstname',\n+            'is_point_of_contact',\n+            'is_not_point_of_contact',\n+            'is_ceo_of_small_company',\n+            'is_ceo_small_2',\n+            'largest_company',\n+        ).order_by('firstname')\n+\n+        results = list(qs)\n+        # Could use Coalesce(subq, Value('')) instead except for the bug in\n+        # cx_Oracle mentioned in #23843.\n+        bob = results[0]\n+        if bob['largest_company'] == '' and connection.features.interprets_empty_strings_as_nulls:\n+            bob['largest_company'] = None\n+\n+        self.assertEqual(results, [\n+            {\n+                'firstname': 'Bob',\n+                'is_point_of_contact': False,\n+                'is_not_point_of_contact': True,\n+                'is_ceo_of_small_company': False,\n+                'is_ceo_small_2': False,\n+                'largest_company': None,\n+            },\n+            {\n+                'firstname': 'Frank',\n+                'is_point_of_contact': False,\n+                'is_not_point_of_contact': True,\n+                'is_ceo_of_small_company': True,\n+                'is_ceo_small_2': True,\n+                'largest_company': 'Foobar Ltd.',\n+            },\n+            {\n+                'firstname': 'Joe',\n+                'is_point_of_contact': True,\n+                'is_not_point_of_contact': False,\n+                'is_ceo_of_small_company': False,\n+                'is_ceo_small_2': False,\n+                'largest_company': 'Example Inc.',\n+            },\n+            {\n+                'firstname': 'Max',\n+                'is_point_of_contact': False,\n+                'is_not_point_of_contact': True,\n+                'is_ceo_of_small_company': True,\n+                'is_ceo_small_2': True,\n+                'largest_company': 'Example Inc.'\n+            }\n+        ])\n+        # A less elegant way to write the same query: this uses a LEFT OUTER\n+        # JOIN and an IS NULL, inside a WHERE NOT IN which is probably less\n+        # efficient than EXISTS.\n+        self.assertCountEqual(\n+            qs.filter(is_point_of_contact=True).values('pk'),\n+            Employee.objects.exclude(company_point_of_contact_set=None).values('pk')\n+        )\n+\n+    def test_in_subquery(self):\n+        # This is a contrived test (and you really wouldn't write this query),\n+        # but it is a succinct way to test the __in=Subquery() construct.\n+        small_companies = Company.objects.filter(num_employees__lt=200).values('pk')\n+        subquery_test = Company.objects.filter(pk__in=Subquery(small_companies))\n+        self.assertCountEqual(subquery_test, [self.foobar_ltd, self.gmbh])\n+        subquery_test2 = Company.objects.filter(pk=Subquery(small_companies.filter(num_employees=3)))\n+        self.assertCountEqual(subquery_test2, [self.foobar_ltd])\n+\n+    def test_nested_subquery(self):\n+        inner = Company.objects.filter(point_of_contact=OuterRef('pk'))\n+        outer = Employee.objects.annotate(is_point_of_contact=Exists(inner))\n+        contrived = Employee.objects.annotate(\n+            is_point_of_contact=Subquery(\n+                outer.filter(pk=OuterRef('pk')).values('is_point_of_contact'),\n+                output_field=models.BooleanField(),\n+            ),\n+        )\n+        self.assertCountEqual(contrived.values_list(), outer.values_list())\n+\n+    def test_nested_subquery_outer_ref_2(self):\n+        first = Time.objects.create(time='09:00')\n+        second = Time.objects.create(time='17:00')\n+        third = Time.objects.create(time='21:00')\n+        SimulationRun.objects.bulk_create([\n+            SimulationRun(start=first, end=second, midpoint='12:00'),\n+            SimulationRun(start=first, end=third, midpoint='15:00'),\n+            SimulationRun(start=second, end=first, midpoint='00:00'),\n+        ])\n+        inner = Time.objects.filter(time=OuterRef(OuterRef('time')), pk=OuterRef('start')).values('time')\n+        middle = SimulationRun.objects.annotate(other=Subquery(inner)).values('other')[:1]\n+        outer = Time.objects.annotate(other=Subquery(middle, output_field=models.TimeField()))\n+        # This is a contrived example. It exercises the double OuterRef form.\n+        self.assertCountEqual(outer, [first, second, third])\n+\n+    def test_annotations_within_subquery(self):\n+        Company.objects.filter(num_employees__lt=50).update(ceo=Employee.objects.get(firstname='Frank'))\n+        inner = Company.objects.filter(\n+            ceo=OuterRef('pk')\n+        ).values('ceo').annotate(total_employees=models.Sum('num_employees')).values('total_employees')\n+        outer = Employee.objects.annotate(total_employees=Subquery(inner)).filter(salary__lte=Subquery(inner))\n+        self.assertSequenceEqual(\n+            outer.order_by('-total_employees').values('salary', 'total_employees'),\n+            [{'salary': 10, 'total_employees': 2300}, {'salary': 20, 'total_employees': 35}],\n+        )\n+\n \n class IterableLookupInnerExpressionsTests(TestCase):\n     @classmethod\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Good work on tackling the `EXISTS()` implementation! It's clear this enhances usability, especially with your idea of allowing boolean values for `__exists` lookups. However, your sentiment against `.exclude()` is valid; it complicates queries unnecessarily. The handling of negated filters is indeed problematic, particularly with multivalued relations. Your suggestion to revise query generation to avoid joins is crucial, and implementing `F('outerq__')` for subqueries could simplify complexity significantly. Consider adding `NotExists()` as a companion to streamline negated conditions. Overall, strong progress, but focus on refining the exclude handling."
      }
    ],
    "codediff_length": 19251
  },
  {
    "index": 26,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/db/backends/utils.py b/django/db/backends/utils.py\nindex 568f510a670e..608226c7de97 100644\n--- a/django/db/backends/utils.py\n+++ b/django/db/backends/utils.py\n@@ -114,6 +114,98 @@ def _executemany(self, sql, param_list, *ignored_wrapper_args):\n             return self.cursor.executemany(sql, param_list)\n \n \n+class AsyncCursorCtx:\n+    \"\"\"\n+    Asynchronous context manager to hold an async cursor.\n+    \"\"\"\n+\n+    def __init__(self, db, name=None):\n+        self.db = db\n+        self.name = name\n+        self.wrap_database_errors = self.db.wrap_database_errors\n+\n+    async def __aenter__(self):\n+        await self.db.aclose_if_health_check_failed()\n+        await self.db.aensure_connection()\n+        self.wrap_database_errors.__enter__()\n+        return self.db._aprepare_cursor(self.db.create_async_cursor(self.name))\n+\n+    async def __aexit__(self, type, value, traceback):\n+        self.wrap_database_errors.__exit__(type, value, traceback)\n+\n+\n+class AsyncCursorWrapper(CursorWrapper):\n+    async def _aexecute(self, sql, params, *ignored_wrapper_args):\n+        # Raise a warning during app initialization (stored_app_configs is only\n+        # ever set during testing).\n+        if not apps.ready and not apps.stored_app_configs:\n+            warnings.warn(self.APPS_NOT_READY_WARNING_MSG, category=RuntimeWarning)\n+        self.db.validate_no_broken_transaction()\n+        with self.db.wrap_database_errors:\n+            if params is None:\n+                # params default might be backend specific.\n+                return await self.cursor.execute(sql)\n+            else:\n+                return await self.cursor.execute(sql, params)\n+\n+    async def _aexecute_with_wrappers(self, sql, params, many, executor):\n+        context = {\"connection\": self.db, \"cursor\": self}\n+        for wrapper in reversed(self.db.execute_wrappers):\n+            executor = functools.partial(wrapper, executor)\n+        return await executor(sql, params, many, context)\n+\n+    async def aexecute(self, sql, params=None):\n+        return await self._aexecute_with_wrappers(\n+            sql, params, many=False, executor=self._aexecute\n+        )\n+\n+    async def _aexecutemany(self, sql, param_list, *ignored_wrapper_args):\n+        # Raise a warning during app initialization (stored_app_configs is only\n+        # ever set during testing).\n+        if not apps.ready and not apps.stored_app_configs:\n+            warnings.warn(self.APPS_NOT_READY_WARNING_MSG, category=RuntimeWarning)\n+        self.db.validate_no_broken_transaction()\n+        with self.db.wrap_database_errors:\n+            return await self.cursor.executemany(sql, param_list)\n+\n+    async def aexecutemany(self, sql, param_list):\n+        return await self._aexecute_with_wrappers(\n+            sql, param_list, many=True, executor=self._aexecutemany\n+        )\n+\n+    async def afetchone(self, *args, **kwargs):\n+        return await self.cursor.fetchone(*args, **kwargs)\n+\n+    async def afetchmany(self, *args, **kwargs):\n+        return await self.cursor.fetchmany(*args, **kwargs)\n+\n+    async def afetchall(self, *args, **kwargs):\n+        return await self.cursor.fetchall(*args, **kwargs)\n+\n+    def acopy(self, *args, **kwargs):\n+        return self.cursor.copy(*args, **kwargs)\n+\n+    def astream(self, *args, **kwargs):\n+        return self.cursor.stream(*args, **kwargs)\n+\n+    async def ascroll(self, *args, **kwargs):\n+        return await self.cursor.scroll(*args, **kwargs)\n+\n+    async def __aenter__(self):\n+        return self\n+\n+    async def __aexit__(self, type, value, traceback):\n+        try:\n+            await self.close()\n+        except self.db.Database.Error:\n+            pass\n+\n+    async def __aiter__(self):\n+        with self.db.wrap_database_errors:\n+            async for item in self.cursor:\n+                yield item\n+\n+\n class CursorDebugWrapper(CursorWrapper):\n     # XXX callproc isn't instrumented at this time.\n \n@@ -163,6 +255,57 @@ def debug_sql(\n             )\n \n \n+class AsyncCursorDebugWrapper(AsyncCursorWrapper):\n+    # XXX callproc isn't instrumented at this time.\n+\n+    async def aexecute(self, sql, params=None):\n+        with self.debug_sql(sql, params, use_last_executed_query=True):\n+            return await super().aexecute(sql, params)\n+\n+    async def aexecutemany(self, sql, param_list):\n+        with self.debug_sql(sql, param_list, many=True):\n+            return await super().aexecutemany(sql, param_list)\n+\n+    @contextmanager\n+    def debug_sql(\n+        self, sql=None, params=None, use_last_executed_query=False, many=False\n+    ):\n+        start = time.monotonic()\n+        try:\n+            yield\n+        finally:\n+            stop = time.monotonic()\n+            duration = stop - start\n+            if use_last_executed_query:\n+                sql = self.db.ops.last_executed_query(self.cursor, sql, params)\n+            try:\n+                times = len(params) if many else \"\"\n+            except TypeError:\n+                # params could be an iterator.\n+                times = \"?\"\n+            self.db.queries_log.append(\n+                {\n+                    \"sql\": \"%s times: %s\" % (times, sql) if many else sql,\n+                    \"time\": \"%.3f\" % duration,\n+                    \"async\": True,\n+                }\n+            )\n+            logger.debug(\n+                \"(%.3f) %s; args=%s; alias=%s; async=True\",\n+                duration,\n+                sql,\n+                params,\n+                self.db.alias,\n+                extra={\n+                    \"duration\": duration,\n+                    \"sql\": sql,\n+                    \"params\": params,\n+                    \"alias\": self.db.alias,\n+                    \"async\": True,\n+                },\n+            )\n+\n+\n @contextmanager\n def debug_transaction(connection, sql):\n     start = time.monotonic()\n\ndiff --git a/django/db/utils.py b/django/db/utils.py\nindex faaf3bf862fe..c767ed7b8542 100644\n--- a/django/db/utils.py\n+++ b/django/db/utils.py\n@@ -197,6 +199,89 @@ def create_connection(self, alias):\n         return backend.DatabaseWrapper(db, alias)\n \n \n+class AsyncAlias:\n+    \"\"\"\n+    A Context-aware list of connections.\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self._connections = Local()\n+        setattr(self._connections, \"_stack\", [])\n+\n+    @property\n+    def connections(self):\n+        return getattr(self._connections, \"_stack\", [])\n+\n+    def __len__(self):\n+        return len(self.connections)\n+\n+    def __iter__(self):\n+        return iter(self.connections)\n+\n+    def __str__(self):\n+        return \", \".join([str(id(conn)) for conn in self.connections])\n+\n+    def __repr__(self):\n+        return f\"<{self.__class__.__name__}: {len(self.connections)} connections>\"\n+\n+    def add_connection(self, connection):\n+        setattr(self._connections, \"_stack\", self.connections + [connection])\n+\n+    def pop(self):\n+        conns = self.connections\n+        conns.pop()\n+        setattr(self._connections, \"_stack\", conns)\n+\n+\n+class AsyncConnectionHandler:\n+    \"\"\"\n+    Context-aware class to store async connections, mapped by alias name.\n+    \"\"\"\n+\n+    _from_testcase = False\n+\n+    def __init__(self) -> None:\n+        self._aliases = Local()\n+        self._connection_count = Local()\n+        setattr(self._connection_count, \"value\", 0)\n+\n+    def __getitem__(self, alias):\n+        try:\n+            async_alias = getattr(self._aliases, alias)\n+        except AttributeError:\n+            async_alias = AsyncAlias()\n+            setattr(self._aliases, alias, async_alias)\n+        return async_alias\n+\n+    def __repr__(self) -> str:\n+        return f\"<{self.__class__.__name__}: {self.count} connections>\"\n+\n+    @property\n+    def count(self):\n+        return getattr(self._connection_count, \"value\", 0)\n+\n+    @property\n+    def empty(self):\n+        return self.count == 0\n+\n+    def add_connection(self, using, connection):\n+        self[using].add_connection(connection)\n+        setattr(self._connection_count, \"value\", self.count + 1)\n+\n+    async def pop_connection(self, using):\n+        await self[using].connections[-1].aclose_pool()\n+        self[using].connections.pop()\n+        setattr(self._connection_count, \"value\", self.count - 1)\n+\n+    def get_connection(self, using):\n+        alias = self[using]\n+        if len(alias.connections) == 0:\n+            raise ConnectionDoesNotExist(\n+                f\"There are no async connections using the '{using}' alias.\"\n+            )\n+        return alias.connections[-1]\n+\n+\n class ConnectionRouter:\n     def __init__(self, routers=None):\n         \"\"\"\ndiff --git a/tests/async/test_async_cursor.py b/tests/async/test_async_cursor.py\nnew file mode 100644\nindex 000000000000..b1beea856884\n--- /dev/null\n+++ b/tests/async/test_async_cursor.py\n@@ -0,0 +1,133 @@\n+from django.db import new_connection\n+from django.test import SimpleTestCase, skipUnlessDBFeature\n+\n+\n+@skipUnlessDBFeature(\"supports_async\")\n+class AsyncCursorTests(SimpleTestCase):\n+    databases = {\"default\", \"other\"}\n+\n+    async def test_aexecute(self):\n+        async with new_connection() as conn:\n+            async with conn.acursor() as cursor:\n+                await cursor.aexecute(\"SELECT 1\")\n+\n+    async def test_aexecutemany(self):\n+        async with new_connection() as conn:\n+            async with conn.acursor() as cursor:\n+                await cursor.aexecute(\"CREATE TABLE numbers (number SMALLINT)\")\n+                await cursor.aexecutemany(\n+                    \"INSERT INTO numbers VALUES (%s)\", [(1,), (2,), (3,)]\n+                )\n+                await cursor.aexecute(\"SELECT * FROM numbers\")\n+                result = await cursor.afetchall()\n+                self.assertEqual(result, [(1,), (2,), (3,)])\n+\n+    async def test_afetchone(self):\n+        async with new_connection() as conn:\n+            async with conn.acursor() as cursor:\n+                await cursor.aexecute(\"SELECT 1\")\n+                result = await cursor.afetchone()\n+            self.assertEqual(result, (1,))\n+\n+    async def test_afetchmany(self):\n+        async with new_connection() as conn:\n+            async with conn.acursor() as cursor:\n+                await cursor.aexecute(\n+                    \"\"\"\n+                    SELECT *\n+                    FROM (VALUES\n+                        ('BANANA'),\n+                        ('STRAWBERRY'),\n+                        ('MELON')\n+                    ) AS v (NAME)\"\"\"\n+                )\n+                result = await cursor.afetchmany(size=2)\n+            self.assertEqual(result, [(\"BANANA\",), (\"STRAWBERRY\",)])\n+\n+    async def test_afetchall(self):\n+        async with new_connection() as conn:\n+            async with conn.acursor() as cursor:\n+                await cursor.aexecute(\n+                    \"\"\"\n+                    SELECT *\n+                    FROM (VALUES\n+                        ('BANANA'),\n+                        ('STRAWBERRY'),\n+                        ('MELON')\n+                    ) AS v (NAME)\"\"\"\n+                )\n+                result = await cursor.afetchall()\n+            self.assertEqual(result, [(\"BANANA\",), (\"STRAWBERRY\",), (\"MELON\",)])\n+\n+    async def test_aiter(self):\n+        result = []\n+        async with new_connection() as conn:\n+            async with conn.acursor() as cursor:\n+                await cursor.aexecute(\n+                    \"\"\"\n+                    SELECT *\n+                    FROM (VALUES\n+                        ('BANANA'),\n+                        ('STRAWBERRY'),\n+                        ('MELON')\n+                    ) AS v (NAME)\"\"\"\n+                )\n+                async for record in cursor:\n+                    result.append(record)\n+            self.assertEqual(result, [(\"BANANA\",), (\"STRAWBERRY\",), (\"MELON\",)])\n+\n+    async def test_acopy(self):\n+        result = []\n+        async with new_connection() as conn:\n+            async with conn.acursor() as cursor:\n+                async with cursor.acopy(\n+                    \"\"\"\n+                    COPY (\n+                        SELECT *\n+                        FROM (VALUES\n+                            ('BANANA'),\n+                            ('STRAWBERRY'),\n+                            ('MELON')\n+                        ) AS v (NAME)\n+                    ) TO STDOUT\"\"\"\n+                ) as copy:\n+                    async for row in copy.rows():\n+                        result.append(row)\n+            self.assertEqual(result, [(\"BANANA\",), (\"STRAWBERRY\",), (\"MELON\",)])\n+\n+    async def test_astream(self):\n+        result = []\n+        async with new_connection() as conn:\n+            async with conn.acursor() as cursor:\n+                async for record in cursor.astream(\n+                    \"\"\"\n+                    SELECT *\n+                    FROM (VALUES\n+                        ('BANANA'),\n+                        ('STRAWBERRY'),\n+                        ('MELON')\n+                    ) AS v (NAME)\"\"\"\n+                ):\n+                    result.append(record)\n+            self.assertEqual(result, [(\"BANANA\",), (\"STRAWBERRY\",), (\"MELON\",)])\n+\n+    async def test_ascroll(self):\n+        result = []\n+        async with new_connection() as conn:\n+            async with conn.acursor() as cursor:\n+                await cursor.aexecute(\n+                    \"\"\"\n+                    SELECT *\n+                    FROM (VALUES\n+                        ('BANANA'),\n+                        ('STRAWBERRY'),\n+                        ('MELON')\n+                    ) AS v (NAME)\"\"\"\n+                )\n+                await cursor.ascroll(1, \"absolute\")\n+\n+                result = await cursor.afetchall()\n+                self.assertEqual(result, [(\"STRAWBERRY\",), (\"MELON\",)])\n+                await cursor.ascroll(0, \"absolute\")\n+                result = await cursor.afetchall()\n+                self.assertEqual(result, [(\"BANANA\",), (\"STRAWBERRY\",), (\"MELON\",)])\ndiff --git a/tests/db_utils/tests.py b/tests/db_utils/tests.py\nindex 4028a8acdf3e..aa1028690486 100644\n--- a/tests/db_utils/tests.py\n+++ b/tests/db_utils/tests.py\n@@ -90,3 +105,82 @@ def test_load_backend_invalid_name(self):\n         with self.assertRaisesMessage(ImproperlyConfigured, msg) as cm:\n             load_backend(\"foo\")\n         self.assertEqual(str(cm.exception.__cause__), \"No module named 'foo'\")\n+\n+\n+class AsyncConnectionTests(SimpleTestCase):\n+    databases = {\"default\", \"other\"}\n+\n+    def run_pool(self, coro, count=2):\n+        def fn():\n+            asyncio.run(coro())\n+\n+        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n+            futures = []\n+            for _ in range(count):\n+                futures.append(executor.submit(fn))\n+\n+            for future in concurrent.futures.as_completed(futures):\n+                exc = future.exception()\n+                if exc is not None:\n+                    raise exc\n+\n+    def test_async_alias(self):\n+        alias = AsyncAlias()\n+        assert len(alias) == 0\n+        assert alias.connections == []\n+\n+        async def coro():\n+            assert len(alias) == 0\n+            alias.add_connection(mock.Mock())\n+            alias.pop()\n+\n+        self.run_pool(coro)\n+\n+    def test_async_connection_handler(self):\n+        aconns = AsyncConnectionHandler()\n+        assert aconns.empty is True\n+        assert aconns[\"default\"].connections == []\n+\n+        async def coro():\n+            assert aconns[\"default\"].connections == []\n+            aconns.add_connection(\"default\", mock.Mock())\n+            aconns.pop_connection(\"default\")\n+\n+        self.run_pool(coro)\n+\n+    @skipUnlessDBFeature(\"supports_async\")\n+    def test_new_connection_threading(self):\n+        async def coro():\n+            assert async_connections.empty is True\n+            async with new_connection() as connection:\n+                async with connection.acursor() as c:\n+                    await c.execute(\"SELECT 1\")\n+\n+        self.run_pool(coro)\n+\n+    @skipUnlessDBFeature(\"supports_async\")\n+    async def test_new_connection(self):\n+        with self.assertRaises(ConnectionDoesNotExist):\n+            async_connections.get_connection(DEFAULT_DB_ALIAS)\n+\n+        async with new_connection():\n+            conn1 = async_connections.get_connection(DEFAULT_DB_ALIAS)\n+            self.assertIsNotNone(conn1.aconnection)\n+            async with new_connection():\n+                conn2 = async_connections.get_connection(DEFAULT_DB_ALIAS)\n+                self.assertIsNotNone(conn1.aconnection)\n+                self.assertIsNotNone(conn2.aconnection)\n+                self.assertNotEqual(conn1.aconnection, conn2.aconnection)\n+\n+            self.assertIsNotNone(conn1.aconnection)\n+            self.assertIsNone(conn2.aconnection)\n+        self.assertIsNone(conn1.aconnection)\n+\n+        with self.assertRaises(ConnectionDoesNotExist):\n+            async_connections.get_connection(DEFAULT_DB_ALIAS)\n+\n+    @skipUnlessDBFeature(\"supports_async\")\n+    async def test_new_connection_on_sync(self):\n+        with self.assertRaises(NotSupportedError):\n+            async with new_connection():\n+                async_connections.get_connection(DEFAULT_DB_ALIAS)\ndiff --git a/tests/transactions/tests.py b/tests/transactions/tests.py\nindex 9fe8c58593bb..6cf6d54348f5 100644\n--- a/tests/transactions/tests.py\n+++ b/tests/transactions/tests.py\n@@ -577,3 +578,93 @@ class DurableTransactionTests(DurableTestsBase, TransactionTestCase):\n \n class DurableTests(DurableTestsBase, TestCase):\n     pass\n+\n+\n+@skipUnlessDBFeature(\"uses_savepoints\", \"supports_async\")\n+class AsyncTransactionTestCase(TransactionTestCase):\n+    available_apps = [\"transactions\"]\n+\n+    async def test_new_connection_nested(self):\n+        async with new_connection() as connection:\n+            async with new_connection() as connection2:\n+                await connection2.aset_autocommit(False)\n+                async with connection2.acursor() as cursor2:\n+                    await cursor2.aexecute(\n+                        \"INSERT INTO transactions_reporter \"\n+                        \"(first_name, last_name, email) \"\n+                        \"VALUES (%s, %s, %s)\",\n+                        (\"Sarah\", \"Hatoff\", \"\"),\n+                    )\n+                    await cursor2.aexecute(\"SELECT * FROM transactions_reporter\")\n+                    result = await cursor2.afetchmany()\n+                    assert len(result) == 1\n+\n+            async with connection.acursor() as cursor:\n+                await cursor.aexecute(\"SELECT * FROM transactions_reporter\")\n+                result = await cursor.afetchmany()\n+                assert len(result) == 1\n+\n+    async def test_new_connection_nested2(self):\n+        async with new_connection() as connection:\n+            async with connection.acursor() as cursor:\n+                await cursor.aexecute(\n+                    \"INSERT INTO transactions_reporter (first_name, last_name, email) \"\n+                    \"VALUES (%s, %s, %s)\",\n+                    (\"Sarah\", \"Hatoff\", \"\"),\n+                )\n+                await cursor.aexecute(\"SELECT * FROM transactions_reporter\")\n+                result = await cursor.afetchmany()\n+                assert len(result) == 1\n+\n+            async with new_connection() as connection2:\n+                await connection2.aset_autocommit(False)\n+                async with connection2.acursor() as cursor2:\n+                    await cursor2.aexecute(\"SELECT * FROM transactions_reporter\")\n+                    result = await cursor2.afetchmany()\n+                    # This connection won't see any rows, because the outer one\n+                    # hasn't committed yet.\n+                    assert len(result) == 0\n+\n+    async def test_new_connection_nested3(self):\n+        async with new_connection() as connection:\n+            async with new_connection() as connection2:\n+                await connection2.aset_autocommit(False)\n+                assert id(connection) != id(connection2)\n+                async with connection2.acursor() as cursor2:\n+                    await cursor2.aexecute(\n+                        \"INSERT INTO transactions_reporter \"\n+                        \"(first_name, last_name, email) \"\n+                        \"VALUES (%s, %s, %s)\",\n+                        (\"Sarah\", \"Hatoff\", \"\"),\n+                    )\n+                    await cursor2.aexecute(\"SELECT * FROM transactions_reporter\")\n+                    result = await cursor2.afetchmany()\n+                    assert len(result) == 1\n+\n+                # Outermost connection doesn't see what the innermost did, because the\n+                # innermost connection hasn't exited yet.\n+                async with connection.acursor() as cursor:\n+                    await cursor.aexecute(\"SELECT * FROM transactions_reporter\")\n+                    result = await cursor.afetchmany()\n+                    assert len(result) == 0\n+\n+    async def test_asavepoint(self):\n+        async with new_connection() as connection:\n+            async with connection.acursor() as cursor:\n+                sid = await connection.asavepoint()\n+                assert sid is not None\n+\n+                await cursor.aexecute(\n+                    \"INSERT INTO transactions_reporter (first_name, last_name, email) \"\n+                    \"VALUES (%s, %s, %s)\",\n+                    (\"Archibald\", \"Haddock\", \"\"),\n+                )\n+                await cursor.aexecute(\"SELECT * FROM transactions_reporter\")\n+                result = await cursor.afetchmany(size=5)\n+                assert len(result) == 1\n+                assert result[0][1:] == (\"Archibald\", \"Haddock\", \"\")\n+\n+                await connection.asavepoint_rollback(sid)\n+                await cursor.aexecute(\"SELECT * FROM transactions_reporter\")\n+                result = await cursor.fetchmany(size=5)\n+                assert len(result) == 0\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR introduces code generation for converting async functions to sync versions, leveraging `libcst`. While it's promising, it parallels existing work in psycopg3, which benefits from existing patterns. Careful consideration is needed for the `ConnectionHandler`'s threading behavior, especially regarding sync and async context visibility. The suggestion to use context variables to manage active sync connections is intriguing but could complicate transaction handling. Furthermore, splitting the database wrappers for sync and async may introduce duplication, but prioritizes clarity. Its worth revisiting how deadlock prevention and connection management are handled in async contexts."
      }
    ],
    "codediff_length": 21875
  },
  {
    "index": 27,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/db/models/aggregates.py b/django/db/models/aggregates.py\nindex ea16cc440c46..a791593508f5 100644\n--- a/django/db/models/aggregates.py\n+++ b/django/db/models/aggregates.py\n@@ -218,6 +293,88 @@ def _get_repr_options(self):\n         return {**super()._get_repr_options(), \"sample\": self.function == \"STDDEV_SAMP\"}\n \n \n+class StringAggDelimiter(Func):\n+    arity = 1\n+    template = \"%(expressions)s\"\n+\n+    def __init__(self, value):\n+        self.value = value\n+        super().__init__(value)\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        template = \" SEPARATOR %(expressions)s\"\n+\n+        return self.as_sql(\n+            compiler,\n+            connection,\n+            template=template,\n+            **extra_context,\n+        )\n+\n+\n+class StringAgg(Aggregate):\n+    template = \"%(function)s(%(distinct)s%(expressions)s%(order_by)s)%(filter)s\"\n+    function = \"STRING_AGG\"\n+    name = \"StringAgg\"\n+    allow_distinct = True\n+    allow_order_by = True\n+    output_field = TextField()\n+\n+    def __init__(self, expression, delimiter, **extra):\n+        self.delimiter = StringAggDelimiter(delimiter)\n+        super().__init__(expression, self.delimiter, **extra)\n+\n+    def as_oracle(self, compiler, connection, **extra_context):\n+        if self.order_by:\n+            template = (\n+                \"%(function)s(%(distinct)s%(expressions)s) WITHIN GROUP (%(order_by)s)\"\n+                \"%(filter)s\"\n+            )\n+        else:\n+            template = \"%(function)s(%(distinct)s%(expressions)s)%(filter)s\"\n+\n+        return self.as_sql(\n+            compiler,\n+            connection,\n+            function=\"LISTAGG\",\n+            template=template,\n+            **extra_context,\n+        )\n+\n+    def as_mysql(self, compiler, connection, **extra_context):\n+        extra_context[\"function\"] = \"GROUP_CONCAT\"\n+\n+        template = \"%(function)s(%(distinct)s%(expressions)s%(order_by)s%(delimiter)s)\"\n+        extra_context[\"template\"] = template\n+\n+        c = self.copy()\n+        # The creation of the delimiter SQL and the ordering of the parameters must be\n+        # handled explicitly, as MySQL puts the delimiter at the end of the aggregate\n+        # using the `SEPARATOR` declaration (rather than treating as an expression like\n+        # other database backends).\n+        delimiter_params = []\n+        if c.delimiter:\n+            delimiter_sql, delimiter_params = compiler.compile(c.delimiter)\n+            # Drop the delimiter from the source expressions.\n+            c.source_expressions = c.source_expressions[:-1]\n+            extra_context[\"delimiter\"] = delimiter_sql\n+\n+        sql, params = c.as_sql(compiler, connection, **extra_context)\n+\n+        return sql, (*params, *delimiter_params)\n+\n+    def as_sqlite(self, compiler, connection, **extra_context):\n+        if connection.get_database_version() < (3, 44):\n+            return self.as_sql(\n+                compiler,\n+                connection,\n+                function=\"GROUP_CONCAT\",\n+                **extra_context,\n+            )\n+\n+        return self.as_sql(compiler, connection, **extra_context)\n+\n+\n class Sum(FixDurationInputMixin, Aggregate):\n     function = \"SUM\"\n     name = \"Sum\"\ndiff --git a/tests/aggregation/tests.py b/tests/aggregation/tests.py\nindex 86151000b52b..8caefc060c5a 100644\n--- a/tests/aggregation/tests.py\n+++ b/tests/aggregation/tests.py\n@@ -2202,6 +2250,167 @@ def test_aggregate_arity(self):\n                 with self.assertRaisesMessage(TypeError, msg):\n                     super(function, func_instance).__init__(Value(1), Value(2))\n \n+    def test_string_agg_requires_delimiter(self):\n+        with self.assertRaises(TypeError):\n+            Book.objects.aggregate(stringagg=StringAgg(\"name\"))\n+\n+    def test_string_agg_escapes_delimiter(self):\n+        values = Publisher.objects.aggregate(\n+            stringagg=StringAgg(\"name\", delimiter=Value(\"'\"))\n+        )\n+\n+        self.assertEqual(\n+            values,\n+            {\n+                \"stringagg\": \"Apress'Sams'Prentice Hall'Morgan Kaufmann'Jonno's House \"\n+                \"of Books\",\n+            },\n+        )\n+\n+    @skipUnlessDBFeature(\"supports_aggregate_order_by_clause\")\n+    def test_string_agg_order_by(self):\n+        order_by_test_cases = (\n+            (\n+                F(\"original_opening\").desc(),\n+                \"Books.com;Amazon.com;Mamma and Pappa's Books\",\n+            ),\n+            (\n+                F(\"original_opening\").asc(),\n+                \"Mamma and Pappa's Books;Amazon.com;Books.com\",\n+            ),\n+            (F(\"original_opening\"), \"Mamma and Pappa's Books;Amazon.com;Books.com\"),\n+            (\"original_opening\", \"Mamma and Pappa's Books;Amazon.com;Books.com\"),\n+            (\"-original_opening\", \"Books.com;Amazon.com;Mamma and Pappa's Books\"),\n+            (\n+                Concat(\"original_opening\", Value(\"@\")),\n+                \"Mamma and Pappa's Books;Amazon.com;Books.com\",\n+            ),\n+            (\n+                Concat(\"original_opening\", Value(\"@\")).desc(),\n+                \"Books.com;Amazon.com;Mamma and Pappa's Books\",\n+            ),\n+        )\n+        for order_by, expected_output in order_by_test_cases:\n+            with self.subTest(order_by=order_by, expected_output=expected_output):\n+                values = Store.objects.aggregate(\n+                    stringagg=StringAgg(\"name\", delimiter=Value(\";\"), order_by=order_by)\n+                )\n+                self.assertEqual(values, {\"stringagg\": expected_output})\n+\n+    @skipIfDBFeature(\"supports_aggregate_order_by_clause\")\n+    def test_string_agg_order_by_is_not_supported(self):\n+        message = (\n+            \"This database backend does not support specifying an order on aggregates.\"\n+        )\n+        with self.assertRaisesMessage(NotSupportedError, message):\n+            Store.objects.aggregate(\n+                stringagg=StringAgg(\n+                    \"name\",\n+                    delimiter=Value(\";\"),\n+                    order_by=\"original_opening\",\n+                )\n+            )\n+\n+    def test_string_agg_filter(self):\n+        values = Book.objects.aggregate(\n+            stringagg=StringAgg(\n+                \"name\",\n+                delimiter=Value(\";\"),\n+                filter=Q(name__startswith=\"P\"),\n+            )\n+        )\n+\n+        expected_values = {\n+            \"stringagg\": \"Practical Django Projects;\"\n+            \"Python Web Development with Django;Paradigms of Artificial \"\n+            \"Intelligence Programming: Case Studies in Common Lisp\",\n+        }\n+        self.assertEqual(values, expected_values)\n+\n+    @skipUnlessDBFeature(\"supports_json_field\", \"supports_aggregate_order_by_clause\")\n+    def test_string_agg_jsonfield_order_by(self):\n+        Employee.objects.bulk_create(\n+            [\n+                Employee(work_day_preferences={\"Monday\": \"morning\"}),\n+                Employee(work_day_preferences={\"Monday\": \"afternoon\"}),\n+            ]\n+        )\n+        values = Employee.objects.aggregate(\n+            stringagg=StringAgg(\n+                KeyTextTransform(\"Monday\", \"work_day_preferences\"),\n+                delimiter=Value(\",\"),\n+                order_by=KeyTextTransform(\n+                    \"Monday\",\n+                    \"work_day_preferences\",\n+                ),\n+                output_field=CharField(),\n+            ),\n+        )\n+        self.assertEqual(values, {\"stringagg\": \"afternoon,morning\"})\n+\n+    def test_string_agg_filter_in_subquery(self):\n+        aggregate = StringAgg(\n+            \"authors__name\",\n+            delimiter=Value(\";\"),\n+            filter=~Q(authors__name__startswith=\"J\"),\n+        )\n+        subquery = (\n+            Book.objects.filter(\n+                pk=OuterRef(\"pk\"),\n+            )\n+            .annotate(agg=aggregate)\n+            .values(\"agg\")\n+        )\n+        values = list(\n+            Book.objects.annotate(\n+                agg=Subquery(subquery),\n+            ).values_list(\"agg\", flat=True)\n+        )\n+\n+        expected_values = [\n+            \"Adrian Holovaty\",\n+            \"Brad Dayley\",\n+            \"Paul Bissex;Wesley J. Chun\",\n+            \"Peter Norvig;Stuart Russell\",\n+            \"Peter Norvig\",\n+            \"\" if connection.features.interprets_empty_strings_as_nulls else None,\n+        ]\n+\n+        self.assertQuerySetEqual(expected_values, values, ordered=False)\n+\n+    @skipUnlessDBFeature(\"supports_aggregate_order_by_clause\")\n+    def test_order_by_in_subquery(self):\n+        aggregate = StringAgg(\n+            \"authors__name\",\n+            delimiter=Value(\";\"),\n+            order_by=\"authors__name\",\n+        )\n+        subquery = (\n+            Book.objects.filter(\n+                pk=OuterRef(\"pk\"),\n+            )\n+            .annotate(agg=aggregate)\n+            .values(\"agg\")\n+        )\n+        values = list(\n+            Book.objects.annotate(\n+                agg=Subquery(subquery),\n+            )\n+            .order_by(\"agg\")\n+            .values_list(\"agg\", flat=True)\n+        )\n+\n+        expected_values = [\n+            \"Adrian Holovaty;Jacob Kaplan-Moss\",\n+            \"Brad Dayley\",\n+            \"James Bennett\",\n+            \"Jeffrey Forcier;Paul Bissex;Wesley J. Chun\",\n+            \"Peter Norvig\",\n+            \"Peter Norvig;Stuart Russell\",\n+        ]\n+\n+        self.assertEqual(expected_values, values)\n+\n \n class AggregateAnnotationPruningTests(TestCase):\n     @classmethod\ndiff --git a/tests/postgres_tests/test_aggregates.py b/tests/postgres_tests/test_aggregates.py\nindex ddd3b6bbd047..56544d99e5f7 100644\n--- a/tests/postgres_tests/test_aggregates.py\n+++ b/tests/postgres_tests/test_aggregates.py\n@@ -425,66 +439,6 @@ def test_bool_or_q_object(self):\n         )\n         self.assertEqual(values, {\"boolor\": False})\n \n-    def test_string_agg_requires_delimiter(self):\n-        with self.assertRaises(TypeError):\n-            AggregateTestModel.objects.aggregate(stringagg=StringAgg(\"char_field\"))\n-\n-    def test_string_agg_delimiter_escaping(self):\n-        values = AggregateTestModel.objects.aggregate(\n-            stringagg=StringAgg(\"char_field\", delimiter=\"'\")\n-        )\n-        self.assertEqual(values, {\"stringagg\": \"Foo1'Foo2'Foo4'Foo3\"})\n-\n-    def test_string_agg_charfield(self):\n-        values = AggregateTestModel.objects.aggregate(\n-            stringagg=StringAgg(\"char_field\", delimiter=\";\")\n-        )\n-        self.assertEqual(values, {\"stringagg\": \"Foo1;Foo2;Foo4;Foo3\"})\n-\n-    def test_string_agg_default_output_field(self):\n-        values = AggregateTestModel.objects.aggregate(\n-            stringagg=StringAgg(\"text_field\", delimiter=\";\"),\n-        )\n-        self.assertEqual(values, {\"stringagg\": \"Text1;Text2;Text4;Text3\"})\n-\n-    def test_string_agg_charfield_order_by(self):\n-        order_by_test_cases = (\n-            (F(\"char_field\").desc(), \"Foo4;Foo3;Foo2;Foo1\"),\n-            (F(\"char_field\").asc(), \"Foo1;Foo2;Foo3;Foo4\"),\n-            (F(\"char_field\"), \"Foo1;Foo2;Foo3;Foo4\"),\n-            (\"char_field\", \"Foo1;Foo2;Foo3;Foo4\"),\n-            (\"-char_field\", \"Foo4;Foo3;Foo2;Foo1\"),\n-            (Concat(\"char_field\", Value(\"@\")), \"Foo1;Foo2;Foo3;Foo4\"),\n-            (Concat(\"char_field\", Value(\"@\")).desc(), \"Foo4;Foo3;Foo2;Foo1\"),\n-        )\n-        for order_by, expected_output in order_by_test_cases:\n-            with self.subTest(order_by=order_by, expected_output=expected_output):\n-                values = AggregateTestModel.objects.aggregate(\n-                    stringagg=StringAgg(\"char_field\", delimiter=\";\", order_by=order_by)\n-                )\n-                self.assertEqual(values, {\"stringagg\": expected_output})\n-\n-    def test_string_agg_jsonfield_order_by(self):\n-        values = AggregateTestModel.objects.aggregate(\n-            stringagg=StringAgg(\n-                KeyTextTransform(\"lang\", \"json_field\"),\n-                delimiter=\";\",\n-                order_by=KeyTextTransform(\"lang\", \"json_field\"),\n-                output_field=CharField(),\n-            ),\n-        )\n-        self.assertEqual(values, {\"stringagg\": \"en;pl\"})\n-\n-    def test_string_agg_filter(self):\n-        values = AggregateTestModel.objects.aggregate(\n-            stringagg=StringAgg(\n-                \"char_field\",\n-                delimiter=\";\",\n-                filter=Q(char_field__endswith=\"3\") | Q(char_field__endswith=\"1\"),\n-            )\n-        )\n-        self.assertEqual(values, {\"stringagg\": \"Foo1;Foo3\"})\n-\n     def test_orderable_agg_alternative_fields(self):\n         values = AggregateTestModel.objects.aggregate(\n             arrayagg=ArrayAgg(\"integer_field\", order_by=F(\"char_field\").asc())\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR shows good progress, with commits structured by phase and documentation added. However, you encountered issues with SQLite's version constraints limiting ordering functionality and challenges with `distinct` behavior combined with delimiters. For SQLite, consider using `LD_PRELOAD` instead of recompiling Python. Regarding MySQL, the handling of delimiters and filtering complicates SQL generation; encapsulating logic in `as_mysql` could simplify this. You might need a feature toggle for unsupported behaviors on Oracle. Great work on enhancing cross-backend support; keep refining these areas."
      }
    ],
    "codediff_length": 12631
  },
  {
    "index": 28,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/django/db/migrations/autodetector.py b/django/db/migrations/autodetector.py\nindex 934dab20f028..516964c34154 100644\n--- a/django/db/migrations/autodetector.py\n+++ b/django/db/migrations/autodetector.py\n@@ -632,6 +653,137 @@ def generate_renamed_models(self):\n                             self.old_model_keys.add((app_label, model_name))\n                             break\n \n+    def generate_moved_models(self):\n+        \"\"\"\n+        Find any moved models, generate the operations for them, and remove\n+        the old entry from the model lists. Must be run before other\n+        model-level generation.\n+        \"\"\"\n+        added_models = self.new_model_keys - self.old_model_keys\n+        for app_label, model_name in added_models:\n+            model_state = self.to_state.models[app_label, model_name]\n+            model_fields_def = self.only_relation_agnostic_fields(model_state.fields)\n+            removed_models = self.old_model_keys - self.new_model_keys\n+            for rem_app_label, rem_model_name in removed_models:\n+                if rem_app_label == app_label:\n+                    continue\n+                rem_model_state = self.from_state.models[rem_app_label, rem_model_name]\n+                rem_model_fields_def = self.only_relation_agnostic_fields(\n+                    rem_model_state.fields\n+                )\n+                if (\n+                    model_fields_def == rem_model_fields_def\n+                    and not self.questioner.ask_move_model(\n+                        rem_model_state,\n+                        model_state,\n+                    )\n+                ):\n+                    continue\n+                dependencies = []\n+                old_table_name = rem_model_state.options.get(\n+                    \"db_table\", f\"{rem_app_label}_{rem_model_name}\"\n+                )\n+                new_table_name = model_state.options.get(\n+                    \"db_table\", f\"{app_label}_{model_name}\"\n+                )\n+                # If moved model has same table name as it was in old app,\n+                # then don't rename table.\n+                if old_table_name != new_table_name:\n+                    self.add_operation(\n+                        rem_app_label,\n+                        operations.AlterModelTable(\n+                            name=model_name,\n+                            table=new_table_name,\n+                        ),\n+                        beginning=True,\n+                    )\n+                    dependencies.append(\n+                        OperationDependency(\n+                            rem_app_label,\n+                            model_name,\n+                            None,\n+                            OperationDependency.Type.ALTER_MODEL_TABLE,\n+                        )\n+                    )\n+                model_state.options[\"managed\"] = False\n+                model_state.options[\"old_app_label\"] = rem_app_label\n+                self.add_operation(\n+                    app_label,\n+                    operations.CreateModel(\n+                        name=model_name,\n+                        fields=[d for d in model_state.fields.items()],\n+                        options=model_state.options,\n+                        bases=model_state.bases,\n+                        managers=model_state.managers,\n+                    ),\n+                    dependencies=dependencies,\n+                )\n+                dependencies = list(dependencies) + self._alter_options_for_moved_model(\n+                    rem_app_label, app_label, model_name, dependencies\n+                )\n+                # If there are any base models, then add dependencies for them\n+                # so that they are created before the model(s) inheriting it.\n+                if model_state.bases:\n+                    for base in model_state.bases:\n+                        if isinstance(base, str) and \".\" in base:\n+                            base_app_label, base_name = base.split(\".\", 1)\n+                            if (base_app_label, base_name) in added_models:\n+                                dependencies.append(\n+                                    OperationDependency(\n+                                        base_app_label,\n+                                        base_name,\n+                                        None,\n+                                        OperationDependency.Type.CREATE,\n+                                    )\n+                                )\n+                self.add_operation(\n+                    rem_app_label,\n+                    operations.DeleteModel(name=model_name),\n+                    dependencies=dependencies,\n+                )\n+                self.old_model_keys.remove((rem_app_label, rem_model_name))\n+                self.old_model_keys.add((app_label, model_name))\n+                self.from_state.add_model(model_state)\n+                break\n+\n+    def _alter_options_for_moved_model(\n+        self, rem_app_label, app_label, model_name, dependencies\n+    ):\n+        dependencies = list(dependencies) + [\n+            OperationDependency(\n+                app_label, model_name, None, OperationDependency.Type.CREATE\n+            )\n+        ]\n+        # Make a model unmanaged before deleting it.\n+        self.add_operation(\n+            rem_app_label,\n+            operations.AlterModelOptions(name=model_name, options={\"managed\": False}),\n+            dependencies=dependencies,\n+        )\n+        dependencies = list(dependencies) + [\n+            OperationDependency(\n+                rem_app_label,\n+                model_name,\n+                None,\n+                OperationDependency.Type.ALTER_MODEL_OPTIONS,\n+            )\n+        ]\n+        # Switch moved model (which was created un-managed) back to managed.\n+        self.add_operation(\n+            app_label,\n+            operations.AlterModelOptions(name=model_name, options={}),\n+            dependencies=dependencies,\n+        )\n+        dependencies = list(dependencies) + [\n+            OperationDependency(\n+                app_label,\n+                model_name,\n+                None,\n+                OperationDependency.Type.ALTER_MODEL_OPTIONS,\n+            )\n+        ]\n+        return dependencies\n+\n     def generate_created_models(self):\n         \"\"\"\n         Find all new models (both managed and unmanaged) and make create\ndiff --git a/tests/contenttypes_tests/test_operations.py b/tests/contenttypes_tests/test_operations.py\nindex d44648d9fe6e..16e4889e90cb 100644\n--- a/tests/contenttypes_tests/test_operations.py\n+++ b/tests/contenttypes_tests/test_operations.py\n@@ -189,3 +189,188 @@ def test_content_type_rename_conflict(self):\n                 app_label=\"contenttypes_tests\", model=\"renamedfoo\"\n             ).exists()\n         )\n+\n+\n+@override_settings(\n+    INSTALLED_APPS=[\n+        \"contenttypes_tests.target_app\",\n+        \"contenttypes_tests.source_app\",\n+        \"django.contrib.contenttypes\",\n+    ],\n+)\n+class MoveContentTypeOperationsTests(TransactionTestCase):\n+    databases = {\"default\", \"other\"}\n+    available_apps = [\n+        \"contenttypes_tests.source_app\",\n+        \"contenttypes_tests.target_app\",\n+        \"django.contrib.contenttypes\",\n+    ]\n+\n+    class TestRouter:\n+        def db_for_write(self, model, **hints):\n+            return \"default\"\n+\n+    def setUp(self):\n+        app_config = apps.get_app_config(\"source_app\")\n+        models.signals.post_migrate.connect(\n+            self.assertOperationsInjected, sender=app_config\n+        )\n+        self.addCleanup(\n+            models.signals.post_migrate.disconnect,\n+            self.assertOperationsInjected,\n+            sender=app_config,\n+        )\n+\n+    def assertOperationsInjected(self, plan, **kwargs):\n+        for migration, _backward in plan:\n+            operations = iter(migration.operations)\n+            for operation in operations:\n+                if isinstance(operation, migrations.CreateModel):\n+                    next_operation = next(operations)\n+                    self.assertIsInstance(\n+                        next_operation, contenttypes_management.MoveContentType\n+                    )\n+                    self.assertEqual(next_operation.new_app_label, migration.app_label)\n+                    self.assertEqual(\n+                        next_operation.old_app_label, operation.options[\"old_app_label\"]\n+                    )\n+                    self.assertEqual(next_operation.model_name, operation.name_lower)\n+\n+    def test_move_model_content_type_change(self):\n+        ContentType.objects.create(app_label=\"source_app\", model=\"simplebar\")\n+        call_command(\n+            \"migrate\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(\n+            ContentType.objects.filter(\n+                app_label=\"source_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+        self.assertTrue(\n+            ContentType.objects.filter(\n+                app_label=\"target_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+        call_command(\n+            \"migrate\",\n+            \"source_app\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(\n+            ContentType.objects.filter(\n+                app_label=\"source_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+        self.assertFalse(\n+            ContentType.objects.filter(\n+                app_label=\"target_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+\n+    @override_settings(DATABASE_ROUTERS=[TestRouter()])\n+    def test_content_type_move_model_other_database(self):\n+        ContentType.objects.using(\"other\").create(\n+            app_label=\"source_app\", model=\"simplebar\"\n+        )\n+        other_content_types = ContentType.objects.using(\"other\").filter(\n+            model=\"simplebar\"\n+        )\n+        call_command(\n+            \"migrate\",\n+            database=\"other\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(other_content_types.filter(app_label=\"source_app\").exists())\n+        self.assertTrue(other_content_types.filter(app_label=\"target_app\").exists())\n+        call_command(\n+            \"migrate\",\n+            \"source_app\",\n+            \"zero\",\n+            database=\"other\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(other_content_types.filter(app_label=\"source_app\").exists())\n+        self.assertFalse(other_content_types.filter(app_label=\"target_app\").exists())\n+\n+    def test_missing_content_type_move_ignore(self):\n+        ContentType.objects.create(app_label=\"target_app\", model=\"simplebar\")\n+        call_command(\n+            \"migrate\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertFalse(\n+            ContentType.objects.filter(\n+                app_label=\"source_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+        self.assertTrue(\n+            ContentType.objects.filter(\n+                app_label=\"target_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+        call_command(\n+            \"migrate\",\n+            \"source_app\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(\n+            ContentType.objects.filter(\n+                app_label=\"source_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+        self.assertFalse(\n+            ContentType.objects.filter(\n+                app_label=\"target_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+\n+    def test_content_type_move_conflict(self):\n+        ContentType.objects.create(app_label=\"source_app\", model=\"simplebar\")\n+        ContentType.objects.create(app_label=\"target_app\", model=\"simplebar\")\n+        call_command(\n+            \"migrate\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(\n+            ContentType.objects.filter(\n+                app_label=\"source_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+        self.assertTrue(\n+            ContentType.objects.filter(\n+                app_label=\"target_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+        call_command(\n+            \"migrate\",\n+            \"source_app\",\n+            \"zero\",\n+            database=\"default\",\n+            interactive=False,\n+            verbosity=0,\n+        )\n+        self.assertTrue(\n+            ContentType.objects.filter(\n+                app_label=\"source_app\", model=\"simplebar\"\n+            ).exists()\n+        )\n+        self.assertTrue(\n+            ContentType.objects.filter(\n+                app_label=\"target_app\", model=\"simplebar\"\n+            ).exists()\n+        )\ndiff --git a/tests/migrations/test_autodetector.py b/tests/migrations/test_autodetector.py\nindex ac725d317e01..50ba7b3eee2d 100644\n--- a/tests/migrations/test_autodetector.py\n+++ b/tests/migrations/test_autodetector.py\n@@ -2317,6 +2317,251 @@ def test_rename_model_reverse_relation_dependencies(self):\n             changes, \"testapp\", 0, 0, old_name=\"EntityA\", new_name=\"RenamedEntityA\"\n         )\n \n+    def test_move_model_with_no_db_table(self):\n+        before = [self.author_name, self.book]\n+        after = [\n+            ModelState(\n+                \"otherapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"name\", models.CharField(max_length=200)),\n+                ],\n+            ),\n+            ModelState(\n+                \"otherapp\",\n+                \"Book\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"author\", models.ForeignKey(\"otherapp.Author\", models.CASCADE)),\n+                    (\"title\", models.CharField(max_length=200)),\n+                ],\n+            ),\n+        ]\n+        changes = self.get_changes(\n+            before, after, MigrationQuestioner({\"ask_move_model\": True})\n+        )\n+        self.assertNumberMigrations(changes, \"otherapp\", 2)\n+\n+        self.assertMigrationDependencies(\n+            changes, \"otherapp\", 0, [(\"testapp\", \"auto_1\")]\n+        )\n+        self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n+        self.assertMigrationDependencies(\n+            changes, \"otherapp\", 1, [(\"otherapp\", \"auto_1\"), (\"testapp\", \"auto_2\")]\n+        )\n+        self.assertOperationTypes(\n+            changes, \"otherapp\", 1, [\"AlterModelOptions\", \"AlterField\"]\n+        )\n+\n+        self.assertNumberMigrations(changes, \"testapp\", 3)\n+\n+        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n+        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n+        self.assertMigrationDependencies(\n+            changes, \"testapp\", 1, [(\"otherapp\", \"auto_1\"), (\"testapp\", \"auto_1\")]\n+        )\n+        self.assertOperationTypes(changes, \"testapp\", 1, [\"AlterModelOptions\"])\n+        self.assertMigrationDependencies(\n+            changes, \"testapp\", 2, [(\"otherapp\", \"auto_2\"), (\"testapp\", \"auto_2\")]\n+        )\n+        self.assertOperationTypes(changes, \"testapp\", 2, [\"DeleteModel\"])\n+\n+    def test_ask_move_model(self):\n+        before = [self.author_name, self.book]\n+        after = [\n+            ModelState(\n+                \"otherapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"name\", models.CharField(max_length=200)),\n+                ],\n+            ),\n+            ModelState(\n+                \"otherapp\",\n+                \"Book\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"author\", models.ForeignKey(\"otherapp.Author\", models.CASCADE)),\n+                    (\"title\", models.CharField(max_length=200)),\n+                ],\n+            ),\n+        ]\n+        changes = self.get_changes(\n+            before, after, MigrationQuestioner({\"ask_move_model\": False})\n+        )\n+        # if ask_move_model input is false, it will consider\n+        # the changes as deleting 'Author' model from testapp\n+        # and creating it in otherapp.\n+        self.assertNumberMigrations(changes, \"otherapp\", 1)\n+        self.assertNumberMigrations(changes, \"testapp\", 1)\n+        self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\", \"AlterField\"])\n+        self.assertOperationTypes(changes, \"testapp\", 0, [\"DeleteModel\"])\n+\n+    def test_move_model_with_db_table_changed(self):\n+        before = [self.author_with_db_table_options, self.book]\n+        after = [\n+            ModelState(\n+                \"otherapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"name\", models.CharField(max_length=200)),\n+                ],\n+                {\"db_table\": \"changed_db_table\"},\n+            ),\n+            ModelState(\n+                \"otherapp\",\n+                \"Book\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"author\", models.ForeignKey(\"otherapp.Author\", models.CASCADE)),\n+                    (\"title\", models.CharField(max_length=200)),\n+                ],\n+            ),\n+        ]\n+        changes = self.get_changes(\n+            before, after, MigrationQuestioner({\"ask_move_model\": True})\n+        )\n+        self.assertNumberMigrations(changes, \"otherapp\", 2)\n+\n+        self.assertMigrationDependencies(\n+            changes, \"otherapp\", 0, [(\"testapp\", \"auto_1\")]\n+        )\n+        self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n+        self.assertOperationAttributes(\n+            changes,\n+            \"otherapp\",\n+            0,\n+            0,\n+            name=\"author\",\n+            options={\n+                \"db_table\": \"changed_db_table\",\n+                \"indexes\": [],\n+                \"constraints\": [],\n+                \"managed\": False,\n+                \"old_app_label\": \"testapp\",\n+            },\n+        )\n+        self.assertMigrationDependencies(\n+            changes, \"otherapp\", 1, [(\"otherapp\", \"auto_1\"), (\"testapp\", \"auto_2\")]\n+        )\n+        self.assertOperationTypes(\n+            changes, \"otherapp\", 1, [\"AlterModelOptions\", \"AlterField\"]\n+        )\n+\n+        self.assertNumberMigrations(changes, \"testapp\", 3)\n+\n+        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n+        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n+        self.assertMigrationDependencies(\n+            changes, \"testapp\", 1, [(\"otherapp\", \"auto_1\"), (\"testapp\", \"auto_1\")]\n+        )\n+        self.assertOperationTypes(changes, \"testapp\", 1, [\"AlterModelOptions\"])\n+        self.assertMigrationDependencies(\n+            changes, \"testapp\", 2, [(\"otherapp\", \"auto_2\"), (\"testapp\", \"auto_2\")]\n+        )\n+        self.assertOperationTypes(changes, \"testapp\", 2, [\"DeleteModel\"])\n+\n+    def test_move_model_with_explicit_db_table(self):\n+        before = [self.author_name, self.book]\n+        after = [\n+            ModelState(\n+                \"otherapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"name\", models.CharField(max_length=200)),\n+                ],\n+                options={\"db_table\": \"testapp_author\"},\n+            ),\n+            ModelState(\n+                \"otherapp\",\n+                \"Book\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"author\", models.ForeignKey(\"otherapp.Author\", models.CASCADE)),\n+                    (\"title\", models.CharField(max_length=200)),\n+                ],\n+            ),\n+        ]\n+        changes = self.get_changes(\n+            before, after, MigrationQuestioner({\"ask_move_model\": True})\n+        )\n+        self.assertNumberMigrations(changes, \"otherapp\", 2)\n+\n+        self.assertMigrationDependencies(changes, \"otherapp\", 0, [])\n+        self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n+        self.assertMigrationDependencies(\n+            changes, \"otherapp\", 1, [(\"otherapp\", \"auto_1\"), (\"testapp\", \"auto_1\")]\n+        )\n+        self.assertOperationTypes(\n+            changes, \"otherapp\", 1, [\"AlterModelOptions\", \"AlterField\"]\n+        )\n+\n+        self.assertNumberMigrations(changes, \"testapp\", 2)\n+\n+        self.assertMigrationDependencies(\n+            changes, \"testapp\", 0, [(\"otherapp\", \"auto_1\")]\n+        )\n+        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n+        self.assertMigrationDependencies(\n+            changes, \"testapp\", 1, [(\"otherapp\", \"auto_2\"), (\"testapp\", \"auto_1\")]\n+        )\n+        self.assertOperationTypes(changes, \"testapp\", 1, [\"DeleteModel\"])\n+\n+    def test_move_mti_models_operations(self):\n+        before = [self.author_name, self.aardvark_based_on_author]\n+        after = [\n+            ModelState(\n+                \"targetapp\",\n+                \"Author\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                    (\"name\", models.CharField(max_length=200)),\n+                ],\n+            ),\n+            ModelState(\"targetapp\", \"Aardvark\", [], bases=(\"targetapp.Author\",)),\n+        ]\n+        changes = self.get_changes(\n+            before, after, MigrationQuestioner({\"ask_move_model\": True})\n+        )\n+        self.assertNumberMigrations(changes, \"targetapp\", 2)\n+\n+        self.assertMigrationDependencies(\n+            changes, \"targetapp\", 0, [(\"testapp\", \"auto_1\")]\n+        )\n+        self.assertOperationTypes(\n+            changes, \"targetapp\", 0, [\"CreateModel\", \"CreateModel\"]\n+        )\n+        # self.assertOperationAttributes(changes, \"targetapp\", 0, 0, name=\"author\")\n+\n+        self.assertMigrationDependencies(\n+            changes, \"targetapp\", 1, [(\"targetapp\", \"auto_1\"), (\"testapp\", \"auto_2\")]\n+        )\n+        self.assertOperationTypes(\n+            changes, \"targetapp\", 1, [\"AlterModelOptions\", \"AlterModelOptions\"]\n+        )\n+\n+        self.assertNumberMigrations(changes, \"testapp\", 3)\n+\n+        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n+        self.assertOperationTypes(\n+            changes, \"testapp\", 0, [\"AlterModelTable\", \"AlterModelTable\"]\n+        )\n+        self.assertMigrationDependencies(\n+            changes, \"testapp\", 1, [(\"targetapp\", \"auto_1\"), (\"testapp\", \"auto_1\")]\n+        )\n+        self.assertOperationTypes(\n+            changes, \"testapp\", 1, [\"AlterModelOptions\", \"AlterModelOptions\"]\n+        )\n+\n+        self.assertMigrationDependencies(\n+            changes, \"testapp\", 2, [(\"targetapp\", \"auto_2\"), (\"testapp\", \"auto_2\")]\n+        )\n+        self.assertOperationTypes(changes, \"testapp\", 2, [\"DeleteModel\", \"DeleteModel\"])\n+\n     def test_fk_dependency(self):\n         \"\"\"Having a ForeignKey automatically adds a dependency.\"\"\"\n         # Note that testapp (author) has no dependencies,\ndiff --git a/tests/migrations/test_operations.py b/tests/migrations/test_operations.py\nindex e92b1c450658..cc51f5134b6f 100644\n--- a/tests/migrations/test_operations.py\n+++ b/tests/migrations/test_operations.py\n@@ -1428,6 +1428,218 @@ def test_rename_m2m_field_with_2_references(self):\n         self.assertTableNotExists(app_label + \"_person\")\n         self.assertTableExists(app_label + \"_other\")\n \n+    def test_move_model_with_fk(self):\n+        app_label_1 = \"test_mmw_fk_1\"\n+        app_label_2 = \"test_mmw_fk_2\"\n+        project_state = self.apply_operations(\n+            app_label_1,\n+            ProjectState(),\n+            operations=[\n+                migrations.CreateModel(\n+                    \"Rider\",\n+                    fields=[\n+                        (\"id\", models.AutoField(primary_key=True)),\n+                    ],\n+                ),\n+                migrations.CreateModel(\n+                    \"TempRider\",\n+                    fields=[\n+                        (\"id\", models.AutoField(primary_key=True)),\n+                    ],\n+                ),\n+            ],\n+        )\n+        project_state = self.apply_operations(\n+            app_label_2,\n+            project_state,\n+            operations=[\n+                migrations.CreateModel(\n+                    \"Pony\",\n+                    fields=[\n+                        (\"id\", models.AutoField(primary_key=True)),\n+                        (\n+                            \"riders\",\n+                            models.ForeignKey(\n+                                f\"{app_label_1}.TempRider\", on_delete=models.CASCADE\n+                            ),\n+                        ),\n+                    ],\n+                ),\n+            ],\n+        )\n+\n+        self.assertTableExists(f\"{app_label_1}_temprider\")\n+        self.assertTableExists(f\"{app_label_2}_pony\")\n+        temp_rider = project_state.apps.get_model(app_label_1, \"TempRider\")\n+        pony = project_state.apps.get_model(app_label_2, \"Pony\")\n+        pony.objects.create(riders=temp_rider.objects.create())\n+\n+        project_state = self.apply_operations(\n+            app_label_1,\n+            project_state,\n+            operations=[\n+                migrations.AlterModelTable(\n+                    \"TempRider\",\n+                    f\"{app_label_2}_temprider\",\n+                ),\n+            ],\n+        )\n+        project_state = self.apply_operations(\n+            app_label_2,\n+            project_state,\n+            operations=[\n+                migrations.CreateModel(\n+                    \"TempRider\",\n+                    fields=[\n+                        (\"id\", models.AutoField(primary_key=True)),\n+                    ],\n+                    options={\n+                        \"managed\": False,\n+                    },\n+                ),\n+            ],\n+        )\n+        project_state = self.apply_operations(\n+            app_label_1,\n+            project_state,\n+            operations=[\n+                migrations.AlterModelOptions(\"TempRider\", {\"managed\": False}),\n+            ],\n+        )\n+        project_state = self.apply_operations(\n+            app_label_2,\n+            project_state,\n+            operations=[\n+                migrations.AlterModelOptions(\"TempRider\", {}),\n+                migrations.AlterField(\n+                    \"pony\",\n+                    name=\"riders\",\n+                    field=models.ForeignKey(\n+                        on_delete=models.CASCADE, to=f\"{app_label_2}.temprider\"\n+                    ),\n+                ),\n+            ],\n+        )\n+        project_state = self.apply_operations(\n+            app_label_1,\n+            project_state,\n+            operations=[\n+                migrations.DeleteModel(\"TempRider\"),\n+            ],\n+        )\n+        self.assertTableExists(f\"{app_label_2}_temprider\")\n+        self.assertTableExists(f\"{app_label_2}_pony\")\n+        temp_rider = project_state.apps.get_model(app_label_2, \"TempRider\")\n+        pony = project_state.apps.get_model(app_label_2, \"Pony\")\n+        pony.objects.create(riders=temp_rider.objects.create())\n+        self.assertEqual(temp_rider.objects.count(), 2)\n+        self.assertEqual(pony.objects.count(), 2)\n+\n+    def test_move_model_with_inheritance_and_m2m_field(self):\n+        app_label_1 = \"test_mmw_i_m2m_1\"\n+        app_label_2 = \"test_mmw_i_m2m_2\"\n+        project_state = self.set_up_test_model(app_label_1, mti_model=True)\n+        project_state = self.apply_operations(\n+            app_label_2,\n+            project_state,\n+            operations=[\n+                migrations.CreateModel(\n+                    \"Rider\",\n+                    fields=[\n+                        (\"id\", models.AutoField(primary_key=True)),\n+                        (\n+                            \"shetlandpony_fk\",\n+                            models.ForeignKey(\n+                                f\"{app_label_1}.ShetlandPony\", on_delete=models.CASCADE\n+                            ),\n+                        ),\n+                    ],\n+                ),\n+            ],\n+        )\n+\n+        self.assertTableExists(f\"{app_label_1}_shetlandpony\")\n+        self.assertTableExists(f\"{app_label_1}_pony\")\n+        self.assertTableExists(f\"{app_label_2}_rider\")\n+        shetlandpony = project_state.apps.get_model(app_label_1, \"ShetlandPony\")\n+        rider = project_state.apps.get_model(app_label_2, \"Rider\")\n+        rider.objects.create(shetlandpony_fk=shetlandpony.objects.create(weight=2))\n+\n+        project_state = self.apply_operations(\n+            app_label_1,\n+            project_state,\n+            operations=[\n+                migrations.AlterModelTable(\n+                    \"ShetlandPony\",\n+                    f\"{app_label_2}_shetlandpony\",\n+                ),\n+            ],\n+        )\n+        project_state = self.apply_operations(\n+            app_label_2,\n+            project_state,\n+            operations=[\n+                migrations.CreateModel(\n+                    \"ShetlandPony\",\n+                    fields=[\n+                        (\n+                            \"pony_ptr\",\n+                            models.OneToOneField(\n+                                f\"{app_label_1}.Pony\",\n+                                models.CASCADE,\n+                                auto_created=True,\n+                                parent_link=True,\n+                                primary_key=True,\n+                                to_field=\"id\",\n+                                serialize=False,\n+                            ),\n+                        ),\n+                        (\"cuteness\", models.IntegerField(default=1)),\n+                    ],\n+                    bases=[\"%s.Pony\" % app_label_1],\n+                    options={\n+                        \"managed\": False,\n+                    },\n+                ),\n+            ],\n+        )\n+        project_state = self.apply_operations(\n+            app_label_1,\n+            project_state,\n+            operations=[\n+                migrations.AlterModelOptions(\"ShetlandPony\", {\"managed\": False}),\n+            ],\n+        )\n+        project_state = self.apply_operations(\n+            app_label_2,\n+            project_state,\n+            operations=[\n+                migrations.AlterModelOptions(\"ShetlandPony\", {}),\n+                migrations.AlterField(\n+                    \"Rider\",\n+                    name=\"shetlandpony_fk\",\n+                    field=models.ForeignKey(\n+                        on_delete=models.CASCADE, to=f\"{app_label_2}.ShetlandPony\"\n+                    ),\n+                ),\n+            ],\n+        )\n+        project_state = self.apply_operations(\n+            app_label_1,\n+            project_state,\n+            operations=[\n+                migrations.DeleteModel(\"ShetlandPony\"),\n+            ],\n+        )\n+        self.assertTableExists(f\"{app_label_2}_shetlandpony\")\n+        self.assertTableExists(f\"{app_label_1}_pony\")\n+        self.assertTableExists(f\"{app_label_2}_rider\")\n+        shetlandpony = project_state.apps.get_model(app_label_2, \"ShetlandPony\")\n+        rider = project_state.apps.get_model(app_label_2, \"Rider\")\n+        rider.objects.create(shetlandpony_fk=shetlandpony.objects.create(weight=2))\n+        self.assertEqual(shetlandpony.objects.count(), 2)\n+        self.assertEqual(rider.objects.count(), 2)\n+\n     def test_add_field(self):\n         \"\"\"\n         Tests the AddField operation.\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR needs more comprehensive tests in `tests/migrations/test_operations` to ensure data integrity during model moves. Existing tests primarily focus on creation and deletion, but we should validate that references and relationships are intact post-move. The approach of managing models using subclasses instead of state-only operations improves clarity, but ensure this works seamlessly for both SQLite and MySQL. Additionally, consider testing edge cases like model inheritance and generic relationships, particularly concerning ContentType updates after moves."
      }
    ],
    "codediff_length": 31682
  },
  {
    "index": 29,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/manimlib/constants.py b/manimlib/constants.py\nindex 82d2298df8..817467e5ec 100644\n--- a/manimlib/constants.py\n+++ b/manimlib/constants.py\n@@ -275,3 +275,107 @@ class MyText(Text):\n >>> c = Circle()\n >>> manim.play(ShowCreation(c))\n \"\"\"\n+code_languages_list = {\"abap\": \"abap\", \"as\": \"as\", \"as3\": \"as3\", \"ada\": \"ada\", \"antlr\": \"antlr\",\n+                       \"antlr_as\": \"antlr-as\",\n+                       \"antlr_csharp\": \"antlr-csharp\", \"antlr_cpp\": \"antlr-cpp\", \"antlr_java\": \"antlr-java\",\n+                       \"antlr_objc\": \"antlr-objc\", \"antlr_perl\": \"antlr-perl\", \"antlr_python\": \"antlr-python\",\n+                       \"antlr_ruby\": \"antlr-ruby\", \"apacheconf\": \"apacheconf\", \"applescript\": \"applescript\",\n+                       \"aspectj\": \"aspectj\",\n+                       \"aspx_cs\": \"aspx-cs\", \"aspx_vb\": \"aspx-vb\", \"asy\": \"asy\", \"ahk\": \"ahk\", \"autoit\": \"autoit\",\n+                       \"awk\": \"awk\",\n+                       \"basemake\": \"basemake\", \"bash\": \"bash\", \"console\": \"console\", \"bat\": \"bat\",\n+                       \"bbcode\": \"bbcode\",\n+                       \"befunge\": \"befunge\", \"blitzmax\": \"blitzmax\", \"boo\": \"boo\", \"brainfuck\": \"brainfuck\",\n+                       \"bro\": \"bro\",\n+                       \"bugs\": \"bugs\", \"c\": \"c\", \"csharp\": \"csharp\", \"cpp\": \"cpp\", \"c_objdump\": \"c-objdump\",\n+                       \"ca65\": \"ca65\",\n+                       \"cbmbas\": \"cbmbas\", \"ceylon\": \"ceylon\", \"cfengine3\": \"cfengine3\", \"cfs\": \"cfs\",\n+                       \"cheetah\": \"cheetah\",\n+                       \"clojure\": \"clojure\", \"cmake\": \"cmake\", \"cobol\": \"cobol\", \"cobolfree\": \"cobolfree\",\n+                       \"coffee_script\": \"coffee-script\", \"cfm\": \"cfm\", \"common_lisp\": \"common-lisp\", \"coq\": \"coq\",\n+                       \"cpp_objdump\": \"cpp-objdump\", \"croc\": \"croc\", \"css\": \"css\", \"css_django\": \"css+django\",\n+                       \"css_genshitext\": \"css+genshitext\", \"css_lasso\": \"css+lasso\", \"css_mako\": \"css+mako\",\n+                       \"css_myghty\": \"css+myghty\", \"css_php\": \"css+php\", \"css_erb\": \"css+erb\",\n+                       \"css_smarty\": \"css+smarty\",\n+                       \"cuda\": \"cuda\", \"cython\": \"cython\", \"d\": \"d\", \"d_objdump\": \"d-objdump\", \"dpatch\": \"dpatch\",\n+                       \"dart\": \"dart\",\n+                       \"control\": \"control\", \"sourceslist\": \"sourceslist\", \"delphi\": \"delphi\", \"dg\": \"dg\",\n+                       \"diff\": \"diff\",\n+                       \"django\": \"django\", \"dtd\": \"dtd\", \"duel\": \"duel\", \"dylan\": \"dylan\",\n+                       \"dylan_console\": \"dylan-console\",\n+                       \"dylan_lid\": \"dylan-lid\", \"ec\": \"ec\", \"ecl\": \"ecl\", \"elixir\": \"elixir\", \"iex\": \"iex\",\n+                       \"ragel_em\": \"ragel-em\",\n+                       \"erb\": \"erb\", \"erlang\": \"erlang\", \"erl\": \"erl\", \"evoque\": \"evoque\", \"factor\": \"factor\",\n+                       \"fancy\": \"fancy\",\n+                       \"fan\": \"fan\", \"felix\": \"felix\", \"fortran\": \"fortran\", \"Clipper\": \"Clipper\",\n+                       \"fsharp\": \"fsharp\", \"gas\": \"gas\",\n+                       \"genshi\": \"genshi\", \"genshitext\": \"genshitext\", \"pot\": \"pot\", \"Cucumber\": \"Cucumber\",\n+                       \"glsl\": \"glsl\",\n+                       \"gnuplot\": \"gnuplot\", \"go\": \"go\", \"gooddata_cl\": \"gooddata-cl\", \"gosu\": \"gosu\", \"gst\": \"gst\",\n+                       \"groff\": \"groff\",\n+                       \"groovy\": \"groovy\", \"haml\": \"haml\", \"haskell\": \"haskell\", \"hx\": \"hx\", \"html\": \"html\",\n+                       \"html_cheetah\": \"html+cheetah\", \"html_django\": \"html+django\", \"html_evoque\": \"html+evoque\",\n+                       \"html_genshi\": \"html+genshi\", \"html_lasso\": \"html+lasso\", \"html_mako\": \"html+mako\",\n+                       \"html_myghty\": \"html+myghty\", \"html_php\": \"html+php\", \"html_smarty\": \"html+smarty\",\n+                       \"html_velocity\": \"html+velocity\", \"http\": \"http\", \"haxeml\": \"haxeml\", \"hybris\": \"hybris\",\n+                       \"idl\": \"idl\",\n+                       \"ini\": \"ini\", \"io\": \"io\", \"ioke\": \"ioke\", \"irc\": \"irc\", \"jade\": \"jade\", \"jags\": \"jags\",\n+                       \"java\": \"java\",\n+                       \"jsp\": \"jsp\", \"js\": \"js\", \"js_cheetah\": \"js+cheetah\", \"js_django\": \"js+django\",\n+                       \"js_genshitext\": \"js+genshitext\", \"js_lasso\": \"js+lasso\", \"js_mako\": \"js+mako\",\n+                       \"js_myghty\": \"js+myghty\",\n+                       \"js_php\": \"js+php\", \"js_erb\": \"js+erb\", \"js_smarty\": \"js+smarty\", \"json\": \"json\",\n+                       \"julia\": \"julia\",\n+                       \"jlcon\": \"jlcon\", \"kconfig\": \"kconfig\", \"koka\": \"koka\", \"kotlin\": \"kotlin\", \"lasso\": \"lasso\",\n+                       \"lighty\": \"lighty\", \"lhs\": \"lhs\", \"live_script\": \"live-script\", \"llvm\": \"llvm\",\n+                       \"logos\": \"logos\",\n+                       \"logtalk\": \"logtalk\", \"lua\": \"lua\", \"make\": \"make\", \"mako\": \"mako\", \"maql\": \"maql\",\n+                       \"mason\": \"mason\",\n+                       \"matlab\": \"matlab\", \"matlabsession\": \"matlabsession\", \"minid\": \"minid\",\n+                       \"modelica\": \"modelica\",\n+                       \"modula2\": \"modula2\", \"trac_wiki\": \"trac-wiki\", \"monkey\": \"monkey\", \"moocode\": \"moocode\",\n+                       \"moon\": \"moon\",\n+                       \"mscgen\": \"mscgen\", \"mupad\": \"mupad\", \"mxml\": \"mxml\", \"myghty\": \"myghty\", \"mysql\": \"mysql\",\n+                       \"nasm\": \"nasm\",\n+                       \"nemerle\": \"nemerle\", \"newlisp\": \"newlisp\", \"newspeak\": \"newspeak\", \"nginx\": \"nginx\",\n+                       \"nimrod\": \"nimrod\",\n+                       \"nsis\": \"nsis\", \"numpy\": \"numpy\", \"objdump\": \"objdump\", \"objective_c\": \"objective-c\",\n+                       \"objective_c_+\": \"objective-c++\", \"objective_j\": \"objective-j\", \"ocaml\": \"ocaml\",\n+                       \"octave\": \"octave\",\n+                       \"ooc\": \"ooc\", \"opa\": \"opa\", \"openedge\": \"openedge\", \"perl\": \"perl\", \"php\": \"php\",\n+                       \"plpgsql\": \"plpgsql\",\n+                       \"psql\": \"psql\", \"postgresql\": \"postgresql\", \"postscript\": \"postscript\", \"pov\": \"pov\",\n+                       \"powershell\": \"powershell\", \"prolog\": \"prolog\", \"properties\": \"properties\",\n+                       \"protobuf\": \"protobuf\",\n+                       \"puppet\": \"puppet\", \"pypylog\": \"pypylog\", \"python\": \"python\", \"python3\": \"python3\",\n+                       \"py3tb\": \"py3tb\",\n+                       \"pycon\": \"pycon\", \"pytb\": \"pytb\", \"qml\": \"qml\", \"racket\": \"racket\", \"ragel\": \"ragel\",\n+                       \"ragel_c\": \"ragel-c\",\n+                       \"ragel_cpp\": \"ragel-cpp\", \"ragel_d\": \"ragel-d\", \"ragel_java\": \"ragel-java\",\n+                       \"ragel_objc\": \"ragel-objc\",\n+                       \"ragel_ruby\": \"ragel-ruby\", \"raw\": \"raw\", \"rconsole\": \"rconsole\", \"rd\": \"rd\",\n+                       \"rebol\": \"rebol\",\n+                       \"redcode\": \"redcode\", \"registry\": \"registry\", \"rst\": \"rst\", \"rhtml\": \"rhtml\",\n+                       \"RobotFramework\": \"RobotFramework\", \"spec\": \"spec\", \"rb\": \"rb\", \"rbcon\": \"rbcon\",\n+                       \"rust\": \"rust\",\n+                       \"splus\": \"splus\", \"sass\": \"sass\", \"scala\": \"scala\", \"ssp\": \"ssp\", \"scaml\": \"scaml\",\n+                       \"scheme\": \"scheme\",\n+                       \"scilab\": \"scilab\", \"scss\": \"scss\", \"shell_session\": \"shell-session\", \"smali\": \"smali\",\n+                       \"smalltalk\": \"smalltalk\", \"smarty\": \"smarty\", \"snobol\": \"snobol\", \"sp\": \"sp\", \"sql\": \"sql\",\n+                       \"sqlite3\": \"sqlite3\", \"squidconf\": \"squidconf\", \"stan\": \"stan\", \"sml\": \"sml\",\n+                       \"systemverilog\": \"systemverilog\",\n+                       \"tcl\": \"tcl\", \"tcsh\": \"tcsh\", \"tea\": \"tea\", \"tex\": \"tex\", \"text\": \"text\",\n+                       \"treetop\": \"treetop\", \"ts\": \"ts\",\n+                       \"urbiscript\": \"urbiscript\", \"vala\": \"vala\", \"vb.net\": \"vb.net\", \"velocity\": \"velocity\",\n+                       \"verilog\": \"verilog\",\n+                       \"vgl\": \"vgl\", \"vhdl\": \"vhdl\", \"vim\": \"vim\", \"xml\": \"xml\", \"xml_cheetah\": \"xml+cheetah\",\n+                       \"xml_django\": \"xml+django\", \"xml_evoque\": \"xml+evoque\", \"xml_lasso\": \"xml+lasso\",\n+                       \"xml_mako\": \"xml+mako\",\n+                       \"xml_myghty\": \"xml+myghty\", \"xml_php\": \"xml+php\", \"xml_erb\": \"xml+erb\",\n+                       \"xml_smarty\": \"xml+smarty\",\n+                       \"xml_velocity\": \"xml+velocity\", \"xquery\": \"xquery\", \"xslt\": \"xslt\", \"xtend\": \"xtend\",\n+                       \"yaml\": \"yaml\"}\n+\n+code_styles_list = {0: \"autumn\", 1: \"borland\", 2: \"bw\", 3: \"colorful\", 4: \"default\", 5: \"emacs\",\n+                    6: \"friendly\", 7: \"fruity\", 8: \"manni\", 9: \"monokai\", 10: \"murphy\", 11: \"native\",\n+                    12: \"pastie\", 13: \"perldoc\", 14: \"rrt\", 15: \"tango\", 16: \"trac\", 17: \"vim\", 18: \"vs\"}\ndiff --git a/manimlib/mobject/code_mobject.py b/manimlib/mobject/code_mobject.py\nnew file mode 100644\nindex 0000000000..94c6871049\n--- /dev/null\n+++ b/manimlib/mobject/code_mobject.py\n@@ -0,0 +1,313 @@\n+import html\r\n+from manimlib.constants import *\r\n+from manimlib.container.container import Container\r\n+from manimlib.mobject.geometry import Rectangle, Dot, RoundedRectangle\r\n+from manimlib.mobject.shape_matchers import SurroundingRectangle\r\n+from manimlib.mobject.svg.text_mobject import Text\r\n+from manimlib.mobject.types.vectorized_mobject import VGroup\r\n+\r\n+import re\r\n+from pygments import highlight\r\n+from pygments.lexers import get_lexer_by_name\r\n+from pygments.formatters.html import HtmlFormatter\r\n+\r\n+'''\r\n+1) Code is VGroup() with three things\r\n+    1.1) Code[0] is Code.background_mobject\r\n+        which can be a \r\n+            1.1.1) Rectangle() if background == \"rectangle\" \r\n+            1.1.2) VGroup() of Rectangle() and Dot() for three buttons\r\n+    1.2) Code[1] is Code.line_numbers\r\n+        1.2.1) Which is a VGroup() of Text() objects with line numbers as a text, \r\n+        1.2.2) this mean you can use \r\n+                Code.line_numbers[0] or Code[1][0] to access first line number \r\n+    1.3) Code[2] is Code.code\r\n+        1.3.1) Which is a VGroup() of lines with color highlighted, this mean you can use \r\n+            Code.code[1] or Code[2][1] \r\n+                line number 1\r\n+            Code.code[1][0] or Code.code[1][0] \r\n+                first character of line number 1\r\n+            Code.code[1][0:5] or Code.code[1][0:5] \r\n+                first five characters of line number 1\r\n+'''\r\n+\r\n+\r\n+class Code(VGroup):\r\n+    CONFIG = {\r\n+        \"tab_width\": 4,\r\n+        \"line_spacing\": 0.1,\r\n+        \"scale_factor\": 0.5,\r\n+        \"run_time\": 1,\r\n+        \"font\": 'Monospac821 BT',\r\n+        'stroke_width': 0,\r\n+        'margin': 0.3,\r\n+        'indentation_char': \"  \",\r\n+        \"background\": \"rectangle\",  # or window\r\n+        \"corner_radius\": 0.2,\r\n+        'insert_line_no': True,\r\n+        'line_no_from': 1,\r\n+        \"line_no_buff\": 0.4,\r\n+        'style': 'vim',\r\n+        'language': 'cpp',\r\n+        'generate_html_file': False\r\n+    }\r\n+\r\n+    def __init__(self, file_name=None, **kwargs):\r\n+        Container.__init__(self, **kwargs)\r\n+        self.file_name = file_name or self.file_name\r\n+        self.ensure_valid_file()\r\n+        self.style = self.style.lower()\r\n+        self.gen_html_string()\r\n+        strati = self.html_string.find(\"background:\")\r\n+        self.background_color = self.html_string[strati + 12:strati + 19]\r\n+        self.gen_code_json()\r\n+        self.temp_char = Text(\"(\").scale(self.scale_factor)\r\n+\r\n+        self.code = self.gen_colored_lines()\r\n+        if self.insert_line_no:\r\n+            self.line_numbers = self.gen_line_numbers()\r\n+            self.line_numbers.next_to(self.code, direction=LEFT, buff=self.line_no_buff)\r\n+\r\n+        if self.background == \"rectangle\":\r\n+            if self.insert_line_no:\r\n+                forground = VGroup(self.code, self.line_numbers)\r\n+            else:\r\n+                forground = self.code\r\n+            self.background_mobject = SurroundingRectangle(forground, buff=self.margin,\r\n+                                                           color=self.background_color,\r\n+                                                           fill_color=self.background_color,\r\n+                                                           stroke_width=0,\r\n+                                                           fill_opacity=1, )\r\n+            self.background_mobject.round_corners(self.corner_radius)\r\n+        else:\r\n+            if self.insert_line_no:\r\n+                forground = VGroup(self.code, self.line_numbers)\r\n+            else:\r\n+                forground = self.code\r\n+\r\n+            height = forground.get_height() + 0.1 * 3 + 2 * self.margin\r\n+            width = forground.get_width() + 0.1 * 3 + 2 * self.margin\r\n+\r\n+            rrect = RoundedRectangle(corner_radius=self.corner_radius, height=height, width=width,\r\n+                                     stroke_width=0,\r\n+                                     color=self.background_color, fill_opacity=1)\r\n+            red_button = Dot(radius=0.1, stroke_width=0, color='#ff5f56')\r\n+            red_button.shift(LEFT * 0.1 * 3)\r\n+            yellow_button = Dot(radius=0.1, stroke_width=0, color='#ffbd2e')\r\n+            green_button = Dot(radius=0.1, stroke_width=0, color='#27c93f')\r\n+            green_button.shift(RIGHT * 0.1 * 3)\r\n+            buttons = VGroup(red_button, yellow_button, green_button)\r\n+            buttons.shift(\r\n+                UP * (height / 2 - 0.1 * 2 - 0.05) + LEFT * (width / 2 - 0.1 * 5 - self.corner_radius / 2 - 0.05))\r\n+\r\n+            self.background_mobject = VGroup(rrect, buttons)\r\n+            x = (height - forground.get_height()) / 2 - 0.1 * 3\r\n+            self.background_mobject.shift(forground.get_center())\r\n+            self.background_mobject.shift(UP * x)\r\n+\r\n+        if self.insert_line_no:\r\n+            VGroup.__init__(self, self.background_mobject, self.line_numbers, *self.code, **kwargs)\r\n+        else:\r\n+            VGroup.__init__(self, self.background_mobject, Dot(fill_opacity=0, stroke_opacity=0), *self.code, **kwargs)\r\n+\r\n+        self.move_to(np.array([0, 0, 0]))\r\n+\r\n+    def apply_points_function_about_point(self, func, about_point=None, about_edge=None):\r\n+        if about_point is None:\r\n+            if about_edge is None:\r\n+                about_edge = self.get_corner(UP + LEFT)\r\n+            about_point = self.get_critical_point(about_edge)\r\n+        for mob in self.family_members_with_points():\r\n+            mob.points -= about_point\r\n+            mob.points = func(mob.points)\r\n+            mob.points += about_point\r\n+        return self\r\n+\r\n+    def ensure_valid_file(self):\r\n+        if self.file_name is None:\r\n+            raise Exception(\"Must specify file for Code\")\r\n+        possible_paths = [\r\n+            os.path.join(os.path.join(\"assets\", \"codes\"), self.file_name),\r\n+            self.file_name,\r\n+        ]\r\n+        for path in possible_paths:\r\n+            if os.path.exists(path):\r\n+                self.file_path = path\r\n+                return\r\n+        raise IOError(\"No file matching %s in codes directory\" %\r\n+                      self.file_name)\r\n+\r\n+    def gen_line_numbers(self):\r\n+        line_numbers_array = []\r\n+        for line_no in range(0, self.code_json.__len__()):\r\n+            number = str(self.line_no_from + line_no)\r\n+            line_numbers_array.append(number)\r\n+        line_numbers = Text(*[i for i in line_numbers_array], line_spacing=self.line_spacing,\r\n+                            alignment=\"right\", font=self.font, stroke_width=self.stroke_width).scale(self.scale_factor)\r\n+        return line_numbers\r\n+\r\n+    def gen_colored_lines(self):\r\n+        lines_text = []\r\n+        for line_no in range(0, self.code_json.__len__()):\r\n+            line_str = \"\"\r\n+            for word_index in range(self.code_json[line_no].__len__()):\r\n+                line_str = line_str + self.code_json[line_no][word_index][0]\r\n+            lines_text.append(self.tab_spaces[line_no] * \"\\t\" + line_str)\r\n+        code = Text(*[i for i in lines_text], line_spacing=self.line_spacing, tab_width=self.tab_width,\r\n+                    alignment=\"left\", font=self.font, stroke_width=self.stroke_width).scale(self.scale_factor)\r\n+        for line_no in range(code.__len__()):\r\n+            line = code[line_no]\r\n+            line_char_index = self.tab_spaces[line_no]\r\n+            for word_index in range(self.code_json[line_no].__len__()):\r\n+                line[line_char_index:line_char_index + self.code_json[line_no][word_index][0].__len__()].set_color(\r\n+                    self.code_json[line_no][word_index][1])\r\n+                line_char_index += self.code_json[line_no][word_index][0].__len__()\r\n+        return code\r\n+\r\n+    def gen_html_string(self):\r\n+        file = open(self.file_path, \"r\")\r\n+        code_str = file.read()\r\n+        file.close()\r\n+        self.html_string = hilite_me(code_str, self.language, {}, self.style, self.insert_line_no,\r\n+                                     \"border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;\")\r\n+        if self.generate_html_file:\r\n+            os.makedirs(os.path.join(\"assets\", \"codes\", \"generated_html_files\"), exist_ok=True)\r\n+            file = open(os.path.join(\"assets\", \"codes\", \"generated_html_files\", self.file_name + \".html\"), \"w\")\r\n+            file.write(self.html_string)\r\n+            file.close()\r\n+\r\n+    def gen_code_json(self):\r\n+        if self.background_color == \"#111111\" or \\\r\n+                self.background_color == \"#272822\" or \\\r\n+                self.background_color == \"#202020\" or \\\r\n+                self.background_color == \"#000000\":\r\n+            self.default_color = \"#ffffff\"\r\n+        else:\r\n+            self.default_color = \"#000000\"\r\n+        for i in range(3, -1, -1):\r\n+            self.html_string = self.html_string.replace(\"</\" + \" \" * i, \"</\")\r\n+        for i in range(10, -1, -1):\r\n+            self.html_string = self.html_string.replace(\"</span>\" + \" \" * i, \" \" * i + \"</span>\")\r\n+        self.html_string = self.html_string.replace(\"background-color:\", \"background:\")\r\n+\r\n+        if self.insert_line_no:\r\n+            start_point = self.html_string.find(\"</td><td><pre\")\r\n+            start_point = start_point + 9\r\n+        else:\r\n+            start_point = self.html_string.find(\"<pre\")\r\n+        self.html_string = self.html_string[start_point:]\r\n+        # print(self.html_string)\r\n+        lines = self.html_string.split(\"\\n\")\r\n+        lines = lines[0:lines.__len__() - 2]\r\n+        start_point = lines[0].find(\">\")\r\n+        lines[0] = lines[0][start_point + 1:]\r\n+        # print(lines)\r\n+        self.code_json = []\r\n+        self.tab_spaces = []\r\n+        code_json_line_index = -1\r\n+        for line_index in range(0, lines.__len__()):\r\n+            if lines[line_index].__len__() == 0:\r\n+                continue\r\n+            # print(lines[line_index])\r\n+            self.code_json.append([])\r\n+            code_json_line_index = code_json_line_index + 1\r\n+            if lines[line_index].startswith(self.indentation_char):\r\n+                start_point = lines[line_index].find(\"<\")\r\n+                starting_string = lines[line_index][:start_point]\r\n+                indentation_char_count = lines[line_index][:start_point].count(self.indentation_char)\r\n+                if starting_string.__len__() != indentation_char_count * self.indentation_char.__len__():\r\n+                    lines[line_index] = \"\\t\" * indentation_char_count + starting_string[starting_string.rfind(\r\n+                        self.indentation_char) + self.indentation_char.__len__():] + \\\r\n+                                        lines[line_index][start_point:]\r\n+                else:\r\n+                    lines[line_index] = \"\\t\" * indentation_char_count + lines[line_index][start_point:]\r\n+\r\n+            indentation_char_count = 0\r\n+            while lines[line_index][indentation_char_count] == '\\t':\r\n+                indentation_char_count = indentation_char_count + 1\r\n+            self.tab_spaces.append(indentation_char_count)\r\n+            # print(lines[line_index])\r\n+            lines[line_index] = self.correct_non_span(lines[line_index])\r\n+            # print(lines[line_index])\r\n+            words = lines[line_index].split(\"<span\")\r\n+            for word_index in range(1, words.__len__()):\r\n+                color_index = words[word_index].find(\"color:\")\r\n+                if color_index == -1:\r\n+                    color = self.default_color\r\n+                else:\r\n+                    starti = words[word_index][color_index:].find(\"#\")\r\n+                    color = words[word_index][color_index + starti:color_index + starti + 7]\r\n+\r\n+                start_point = words[word_index].find(\">\")\r\n+                end_point = words[word_index].find(\"</span>\")\r\n+                text = words[word_index][start_point + 1:end_point]\r\n+                text = html.unescape(text)\r\n+                if text != \"\":\r\n+                    # print(text, \"'\" + color + \"'\")\r\n+                    self.code_json[code_json_line_index].append([text, color])\r\n+        # print(self.code_json)\r\n+\r\n+    def correct_non_span(self, line_str):\r\n+        words = line_str.split(\"</span>\")\r\n+        line_str = \"\"\r\n+        for i in range(0, words.__len__()):\r\n+            if i != words.__len__() - 1:\r\n+                j = words[i].find(\"<span\")\r\n+            else:\r\n+                j = words[i].__len__()\r\n+            temp = \"\"\r\n+            starti = -1\r\n+            for k in range(0, j):\r\n+                if words[i][k] == \"\\t\" and starti == -1:\r\n+                    continue\r\n+                else:\r\n+                    if starti == -1: starti = k\r\n+                    temp = temp + words[i][k]\r\n+            if temp != \"\":\r\n+                if i != words.__len__() - 1:\r\n+                    temp = '<span style=\"color:' + self.default_color + '\">' + words[i][starti:j] + \"</span>\"\r\n+                else:\r\n+                    temp = '<span style=\"color:' + self.default_color + '\">' + words[i][starti:j]\r\n+                temp = temp + words[i][j:]\r\n+                words[i] = temp\r\n+            if words[i] != \"\":\r\n+                line_str = line_str + words[i] + \"</span>\"\r\n+        return line_str\r\n+\r\n+\r\n+def hilite_me(code, lexer, options, style, linenos, divstyles):\r\n+    lexer = lexer or 'python'\r\n+    style = style or 'colorful'\r\n+    defstyles = 'overflow:auto;width:auto;'\r\n+\r\n+    formatter = HtmlFormatter(style=style,\r\n+                              linenos=False,\r\n+                              noclasses=True,\r\n+                              cssclass='',\r\n+                              cssstyles=defstyles + divstyles,\r\n+                              prestyles='margin: 0')\r\n+    html = highlight(code, get_lexer_by_name(lexer, **options), formatter)\r\n+    if linenos:\r\n+        html = insert_line_numbers(html)\r\n+    html = \"<!-- HTML generated using hilite.me -->\" + html\r\n+    return html\r\n+\r\n+\r\n+def get_default_style():\r\n+    return 'border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;'\r\n+\r\n+\r\n+def insert_line_numbers(html):\r\n+    match = re.search('(<pre[^>]*>)(.*)(</pre>)', html, re.DOTALL)\r\n+    if not match: return html\r\n+\r\n+    pre_open = match.group(1)\r\n+    pre = match.group(2)\r\n+    pre_close = match.group(3)\r\n+\r\n+    html = html.replace(pre_close, '</pre></td></tr></table>')\r\n+    numbers = range(1, pre.count('\\n') + 1)\r\n+    format = '%' + str(len(str(numbers[-1]))) + 'i'\r\n+    lines = '\\n'.join(format % i for i in numbers)\r\n+    html = html.replace(pre_open, '<table><tr><td>' + pre_open + lines + '</pre></td><td>' + pre_open)\r\n+    return html\r\ndiff --git a/manimlib/mobject/svg/text_mobject.py b/manimlib/mobject/svg/text_mobject.py\nindex 456e7e469e..1e9b0b2e93 100644\n--- a/manimlib/mobject/svg/text_mobject.py\n+++ b/manimlib/mobject/svg/text_mobject.py\n@@ -148,68 +193,18 @@ def text2hash(self):\n         settings = self.font + self.slant + self.weight\n         settings += str(self.t2f) + str(self.t2s) + str(self.t2w)\n         settings += str(self.lsh) + str(self.size)\n-        id_str = self.text+settings\n+        id_str = self.text + settings\n         hasher = hashlib.sha256()\n         hasher.update(id_str.encode())\n         return hasher.hexdigest()[:16]\n \n-    def text2settings(self):\n-        settings = []\n-        t2x = [self.t2f, self.t2s, self.t2w]\n-        for i in range(len(t2x)):\n-            fsw = [self.font, self.slant, self.weight]\n-            if t2x[i]:\n-                for word, x in list(t2x[i].items()):\n-                    for start, end in self.find_indexes(word):\n-                        fsw[i] = x\n-                        settings.append(TextSetting(start, end, *fsw))\n-\n-        # Set All text settings(default font slant weight)\n-        fsw = [self.font, self.slant, self.weight]\n-        settings.sort(key=lambda setting: setting.start)\n-        temp_settings = settings.copy()\n-        start = 0\n-        for setting in settings:\n-            if setting.start != start:\n-                temp_settings.append(TextSetting(start, setting.start, *fsw))\n-            start = setting.end\n-        if start != len(self.text):\n-            temp_settings.append(TextSetting(start, len(self.text), *fsw))\n-        settings = sorted(temp_settings, key=lambda setting: setting.start)\n-\n-        if re.search(r'\\n', self.text):\n-            line_num = 0\n-            for start, end in self.find_indexes('\\n'):\n-                for setting in settings:\n-                    if setting.line_num == -1:\n-                        setting.line_num = line_num\n-                    if start < setting.end:\n-                        line_num += 1\n-                        new_setting = copy.copy(setting)\n-                        setting.end = end\n-                        new_setting.start = end\n-                        new_setting.line_num = line_num\n-                        settings.append(new_setting)\n-                        settings.sort(key=lambda setting: setting.start)\n-                        break\n-\n-        for setting in settings:\n-            if setting.line_num == -1:\n-                setting.line_num = 0\n-\n-        return settings\n-\n     def text2svg(self):\n         # anti-aliasing\n         size = self.size * 10\n-        lsh = self.lsh * 10\n-\n-        if self.font == '':\n-            print(NOT_SETTING_FONT_MSG)\n \n         dir_name = consts.TEXT_DIR\n         hash_name = self.text2hash()\n-        file_name = os.path.join(dir_name, hash_name)+'.svg'\n+        file_name = os.path.join(dir_name, hash_name) + '.svg'\n         if os.path.exists(file_name):\n             return file_name\n \n@@ -217,22 +212,92 @@ def text2svg(self):\n         context = cairo.Context(surface)\n         context.set_font_size(size)\n         context.move_to(START_X, START_Y)\n-\n-        settings = self.text2settings()\n-        offset_x = 0\n-        last_line_num = 0\n-        for setting in settings:\n-            font = setting.font\n-            slant = self.str2slant(setting.slant)\n-            weight = self.str2weight(setting.weight)\n-            text = self.text[setting.start:setting.end].replace('\\n', ' ')\n-\n-            context.select_font_face(font, slant, weight)\n-            if setting.line_num != last_line_num:\n-                offset_x = 0\n-                last_line_num = setting.line_num\n-            context.move_to(START_X + offset_x, START_Y + lsh*setting.line_num)\n-            context.show_text(text)\n-            offset_x += context.text_extents(text)[4]\n+        text = self.text\n+        context.select_font_face(self.font, self.str2slant(self.slant), self.str2weight(self.weight))\n+        context.move_to(START_X, START_Y)\n+        context.show_text(text)\n \n         return file_name\n+\n+\n+class SingleStringTextWithFixHeight(SingleStringTextMobject):\n+    def __init__(self, text, **kwargs):\n+        SingleStringTextMobject.__init__(self,text,**kwargs)\n+        max_height = SingleStringTextMobject(\"(gyt{[/QW\", **kwargs).get_height()\n+        rectangle = Rectangle(width=0, height=max_height, fill_opacity=0,\n+                              stroke_opacity=0,\n+                              stroke_width=0)\n+        self.submobjects.append(rectangle)\n+\n+\n+class Text(VGroup):\n+    CONFIG = {\n+        \"line_spacing\": 0.1,\n+        \"alignment\": \"center\"\n+    }\n+\n+    def __init__(self, *text, **config):\n+        Container.__init__(self, **config)\n+        self.lines_list = list(text)\n+        self.lines = []\n+        self.lines.append([])\n+        for line_no in range(self.lines_list.__len__()):\n+            if \"\\n\" in self.lines_list[line_no]:\n+                self.lines_list[line_no:line_no + 1] = self.lines_list[line_no].split(\"\\n\")\n+        for line_no in range(self.lines_list.__len__()):\n+            self.lines[0].append(SingleStringTextWithFixHeight(self.lines_list[line_no], **config))\n+        self.char_height = SingleStringTextWithFixHeight(\"(\", **config).get_height()\n+        self.lines.append([])\n+        self.lines[1].extend([self.alignment for _ in range(self.lines_list.__len__())])\n+        self.lines[0][0].move_to(np.array([0, 0, 0]))\n+        self.align_lines()\n+\n+        self.text = VGroup(*[self.lines[0][i] for i in range(self.lines[0].__len__())])\n+        self.config = config\n+        VGroup.__init__(self, *self.text, **config)\n+        self.move_to(np.array([0, 0, 0]))\n+\n+    def set_all_lines_alignment(self, alignment):\n+        self.lines[1].extend([alignment for _ in range(self.lines_list.__len__())])\n+        for line_no in range(0, self.lines[0].__len__()):\n+            self.change_alignment_for_a_line(alignment, line_no)\n+        self.move_to(np.array([0, 0, 0]))\n+\n+    def align_lines(self):\n+        for line_no in range(0, self.lines[0].__len__()):\n+            if self.lines[1][line_no] == \"center\":\n+                self.lines[0][line_no].move_to(\n+                    np.array([0, 0, 0]) + np.array([0, - line_no * (self.char_height + self.line_spacing), 0]))\n+            elif self.lines[1][line_no] == \"left\":\n+                self.lines[0][line_no].move_to(np.array([0, 0, 0]) +\n+                                               np.array([self.lines[0][line_no].get_width() / 2,\n+                                                         - line_no * (self.char_height + self.line_spacing), 0])\n+                                               )\n+            elif self.lines[1][line_no] == \"right\":\n+                self.lines[0][line_no].move_to(np.array([0, 0, 0]) +\n+                                               np.array([- self.lines[0][line_no].get_width() / 2,\n+                                                         - line_no * (self.char_height + self.line_spacing), 0])\n+                                               )\n+\n+    def set_alignment(self, alignment, line_no):\n+        self.change_alignment_for_a_line(alignment, line_no)\n+        self.move_to(np.array([0, 0, 0]))\n+\n+    def change_alignment_for_a_line(self, alignment, line_no):\n+        self.lines[1][line_no] = alignment\n+        if self.lines[1][line_no] == \"center\":\n+            self.lines[0][line_no].move_to(self.get_top() +\n+                                           np.array([0, -self.char_height / 2, 0]) +\n+                                           np.array([0, - line_no * (self.char_height + self.line_spacing), 0]))\n+        elif self.lines[1][line_no] == \"left\":\n+            self.lines[0][line_no].move_to(self.get_top() +\n+                                           np.array([0, -self.char_height / 2, 0]) +\n+                                           np.array([self.get_width() / 2 - self.lines[0][line_no].get_width() / 2,\n+                                                     - line_no * (self.char_height + self.line_spacing), 0])\n+                                           )\n+        elif self.lines[1][line_no] == \"right\":\n+            self.lines[0][line_no].move_to(self.get_top() +\n+                                           np.array([0, -self.char_height / 2, 0]) +\n+                                           np.array([- self.get_width() / 2 + self.lines[0][line_no].get_width() / 2,\n+                                                     - line_no * (self.char_height + self.line_spacing), 0])\n+                                           )\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR has a few critical areas for improvement. First, add `CodeMobject` to `imports.py` for completeness. Renaming variables to more descriptive names will enhance readability, as current abbreviations can be confusing. Address the issue on line 125 where whitespace before `</pre>` leads to index errors; consider using `strip()` for more reliable matching. While the proposed user file conversion is promising, splitting this into a separate PR is advisable to expedite merging and ensure focused testing. Overall, the core functionality looks solid."
      }
    ],
    "codediff_length": 32242
  },
  {
    "index": 30,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/manimlib/__init__.py b/manimlib/__init__.py\nindex 9b3b3f288e..b440aaa343 100644\n--- a/manimlib/__init__.py\n+++ b/manimlib/__init__.py\n@@ -1,12 +1,73 @@\n-#!/usr/bin/env python\n-import manimlib.config\n-import manimlib.extract_scene\n+from manimlib.constants import *\n \n+from manimlib.animation.animation import *\n+from manimlib.animation.composition import *\n+from manimlib.animation.creation import *\n+from manimlib.animation.fading import *\n+from manimlib.animation.growing import *\n+from manimlib.animation.indication import *\n+from manimlib.animation.movement import *\n+from manimlib.animation.numbers import *\n+from manimlib.animation.rotation import *\n+from manimlib.animation.specialized import *\n+from manimlib.animation.transform import *\n+from manimlib.animation.transform_matching_parts import *\n+from manimlib.animation.update import *\n \n-def main():\n-    args = manimlib.config.parse_cli()\n-    config = manimlib.config.get_configuration(args)\n-    scenes = manimlib.extract_scene.main(config)\n+from manimlib.camera.camera import *\n \n-    for scene in scenes:\n-        scene.run()\n+from manimlib.mobject.coordinate_systems import *\n+from manimlib.mobject.changing import *\n+from manimlib.mobject.frame import *\n+from manimlib.mobject.functions import *\n+from manimlib.mobject.geometry import *\n+from manimlib.mobject.matrix import *\n+from manimlib.mobject.mobject import *\n+from manimlib.mobject.number_line import *\n+from manimlib.mobject.numbers import *\n+from manimlib.mobject.probability import *\n+from manimlib.mobject.shape_matchers import *\n+from manimlib.mobject.interactive import *\n+from manimlib.mobject.svg.brace import *\n+from manimlib.mobject.svg.drawings import *\n+from manimlib.mobject.svg.svg_mobject import *\n+from manimlib.mobject.svg.tex_mobject import *\n+from manimlib.mobject.svg.text_mobject import *\n+from manimlib.mobject.three_dimensions import *\n+from manimlib.mobject.types.image_mobject import *\n+from manimlib.mobject.types.point_cloud_mobject import *\n+from manimlib.mobject.types.surface import *\n+from manimlib.mobject.types.vectorized_mobject import *\n+from manimlib.mobject.types.dot_cloud import *\n+from manimlib.mobject.mobject_update_utils import *\n+from manimlib.mobject.value_tracker import *\n+from manimlib.mobject.vector_field import *\n+\n+from manimlib.once_useful_constructs.arithmetic import *\n+from manimlib.once_useful_constructs.combinatorics import *\n+from manimlib.once_useful_constructs.complex_transformation_scene import *\n+from manimlib.once_useful_constructs.counting import *\n+from manimlib.once_useful_constructs.fractals import *\n+from manimlib.once_useful_constructs.graph_theory import *\n+from manimlib.once_useful_constructs.light import *\n+\n+from manimlib.scene.scene import *\n+from manimlib.scene.sample_space_scene import *\n+from manimlib.scene.three_d_scene import *\n+from manimlib.scene.vector_space_scene import *\n+\n+from manimlib.utils.bezier import *\n+from manimlib.utils.color import *\n+from manimlib.utils.config_ops import *\n+from manimlib.utils.customization import *\n+from manimlib.utils.debug import *\n+from manimlib.utils.directories import *\n+from manimlib.utils.images import *\n+from manimlib.utils.iterables import *\n+from manimlib.utils.file_ops import *\n+from manimlib.utils.paths import *\n+from manimlib.utils.rate_functions import *\n+from manimlib.utils.simple_functions import *\n+from manimlib.utils.sounds import *\n+from manimlib.utils.space_ops import *\n+from manimlib.utils.strings import *\ndiff --git a/manimlib/defaults.yml b/manimlib/defaults.yml\ndeleted file mode 100644\nindex 7b89767c62..0000000000\n--- a/manimlib/defaults.yml\n+++ /dev/null\n@@ -1,56 +0,0 @@\n-directories:\n-  # Set this to true if you want the path to video files\n-  # to match the directory structure of the path to the\n-  # sourcecode generating that video\n-  mirror_module_path: False\n-  # Where should manim output video and image files?\n-  output: \"\"\n-  # If you want to use images, manim will look to these folders to find them\n-  raster_images: \"\"\n-  vector_images: \"\"\n-  # If you want to use sounds, manim will look here to find it.\n-  sounds: \"\"\n-  # Manim often generates tex_files or other kinds of serialized data\n-  # to keep from having to generate the same thing too many times.  By\n-  # default, these will be stored at tempfile.gettempdir(), e.g. this might\n-  # return whatever is at to the TMPDIR environment variable.  If you want to\n-  # specify them elsewhere,\n-  temporary_storage: \"\"\n-tex:\n-  executable: \"latex\"\n-  template_file: \"tex_template.tex\" \n-  intermediate_filetype: \"dvi\"\n-  text_to_replace: \"[tex_expression]\"\n-  # # For ctex, use the following configuration\n-  # executable: \"xelatex -no-pdf\"\n-  # template_file: \"ctex_template.tex\"\n-  # intermediate_filetype: \"xdv\"\n-universal_import_line: \"from manimlib.imports import *\"\n-style:\n-  font: \"Consolas\"\n-  background_color: \"#333333\"\n-# Set the position of preview window, you can use directions, e.g. UL/DR/OL/OO/...\n-# also, you can also specify the position(pixel) of the upper left corner of \n-# the window on the monitor, e.g. \"960,540\"\n-window_position: UR\n-# If break_into_partial_movies is set to True, then many small\n-# files will be written corresponding to each Scene.play and\n-# Scene.wait call, and these files will then be combined\n-# to form the full scene.  Sometimes video-editing is made\n-# easier when working with the broken up scene, which\n-# effectively has cuts at all the places you might want.\n-break_into_partial_movies: False\n-camera_qualities:\n-  low:\n-    resolution: \"854x480\"\n-    frame_rate: 15\n-  medium:\n-    resolution: \"1280x720\"\n-    frame_rate: 30\n-  high:\n-    resolution: \"1920x1080\"\n-    frame_rate: 30\n-  ultra_high:\n-    resolution: \"3840x2160\"\n-    frame_rate: 60\n-  default_quality: \"high\"\ndiff --git a/manimlib/imports.py b/manimlib/imports.py\ndeleted file mode 100644\nindex 2237bfaff2..0000000000\n--- a/manimlib/imports.py\n+++ /dev/null\n@@ -1,106 +0,0 @@\n-\"\"\"\n-I won't pretend like this is best practice, but in creating animations for a video,\n-it can be very nice to simply have all of the Mobjects, Animations, Scenes, etc.\n-of manim available without having to worry about what namespace they come from.\n-\n-Rather than having a large pile of \"from <module> import *\" at the top of every such\n-script, the intent of this file is to make it so that one can just include\n-\"from manimlib.imports import *\".  The effects of adding more modules\n-or refactoring the library on current or older scene scripts should be entirely\n-addressible by changing this file.\n-\n-Note: One should NOT import from this file for main library code, it is meant only\n-as a convenience for scripts creating scenes for videos.\n-\"\"\"\n-\n-\n-from manimlib.constants import *\n-\n-from manimlib.animation.animation import *\n-from manimlib.animation.composition import *\n-from manimlib.animation.creation import *\n-from manimlib.animation.fading import *\n-from manimlib.animation.growing import *\n-from manimlib.animation.indication import *\n-from manimlib.animation.movement import *\n-from manimlib.animation.numbers import *\n-from manimlib.animation.rotation import *\n-from manimlib.animation.specialized import *\n-from manimlib.animation.transform import *\n-from manimlib.animation.transform_matching_parts import *\n-from manimlib.animation.update import *\n-\n-from manimlib.camera.camera import *\n-\n-from manimlib.mobject.coordinate_systems import *\n-from manimlib.mobject.changing import *\n-from manimlib.mobject.frame import *\n-from manimlib.mobject.functions import *\n-from manimlib.mobject.geometry import *\n-from manimlib.mobject.matrix import *\n-from manimlib.mobject.mobject import *\n-from manimlib.mobject.number_line import *\n-from manimlib.mobject.numbers import *\n-from manimlib.mobject.probability import *\n-from manimlib.mobject.shape_matchers import *\n-from manimlib.mobject.interactive import *\n-from manimlib.mobject.svg.brace import *\n-from manimlib.mobject.svg.drawings import *\n-from manimlib.mobject.svg.svg_mobject import *\n-from manimlib.mobject.svg.tex_mobject import *\n-from manimlib.mobject.svg.text_mobject import *\n-from manimlib.mobject.three_dimensions import *\n-from manimlib.mobject.types.image_mobject import *\n-from manimlib.mobject.types.point_cloud_mobject import *\n-from manimlib.mobject.types.surface import *\n-from manimlib.mobject.types.vectorized_mobject import *\n-from manimlib.mobject.types.dot_cloud import *\n-from manimlib.mobject.mobject_update_utils import *\n-from manimlib.mobject.value_tracker import *\n-from manimlib.mobject.vector_field import *\n-\n-from manimlib.once_useful_constructs.arithmetic import *\n-from manimlib.once_useful_constructs.combinatorics import *\n-from manimlib.once_useful_constructs.complex_transformation_scene import *\n-from manimlib.once_useful_constructs.counting import *\n-from manimlib.once_useful_constructs.fractals import *\n-from manimlib.once_useful_constructs.graph_theory import *\n-from manimlib.once_useful_constructs.light import *\n-\n-from manimlib.scene.scene import *\n-from manimlib.scene.sample_space_scene import *\n-from manimlib.scene.three_d_scene import *\n-from manimlib.scene.vector_space_scene import *\n-\n-from manimlib.utils.bezier import *\n-from manimlib.utils.color import *\n-from manimlib.utils.config_ops import *\n-from manimlib.utils.customization import *\n-from manimlib.utils.debug import *\n-from manimlib.utils.directories import *\n-from manimlib.utils.images import *\n-from manimlib.utils.iterables import *\n-from manimlib.utils.file_ops import *\n-from manimlib.utils.paths import *\n-from manimlib.utils.rate_functions import *\n-from manimlib.utils.simple_functions import *\n-from manimlib.utils.sounds import *\n-from manimlib.utils.space_ops import *\n-from manimlib.utils.strings import *\n-\n-# Non manim libraries that are also nice to have without thinking\n-\n-import inspect\n-import itertools as it\n-import numpy as np\n-import operator as op\n-import os\n-import random\n-import re\n-import string\n-import sys\n-import math\n-import sympy\n-\n-from PIL import Image\n-from colour import Color\ndiff --git a/manimlib/utils/init_config.py b/manimlib/utils/init_config.py\nnew file mode 100644\nindex 0000000000..064db879cc\n--- /dev/null\n+++ b/manimlib/utils/init_config.py\n@@ -0,0 +1,84 @@\n+import yaml\n+import os\n+\n+def init_customization():\n+    configuration = {\n+        \"directories\": {\n+            \"mirror_module_path\": False,\n+            \"output\": \"\",\n+            \"raster_images\": \"\",\n+            \"vector_images\": \"\",\n+            \"sounds\": \"\",\n+            \"temporary_storage\": \"\",\n+        },\n+        \"tex\": {\n+            \"executable\": \"\",\n+            \"template_file\": \"\",\n+            \"intermediate_filetype\": \"\",\n+            \"text_to_replace\": \"[tex_expression]\",\n+        },\n+        \"universal_import_line\": \"from manimlib import *\",\n+        \"style\": {\n+            \"font\": \"Consolas\",\n+            \"background_color\": \"\",\n+        },\n+        \"window_position\": \"UR\",\n+        \"break_into_partial_movies\": False,\n+        \"camera_qualities\": {\n+            \"low\": {\n+                \"resolution\": \"854x480\",\n+                \"frame_rate\": 15,\n+            },\n+            \"medium\": {\n+                \"resolution\": \"1280x720\",\n+                \"frame_rate\": 30,\n+            },\n+            \"high\": {\n+                \"resolution\": \"1920x1080\",\n+                \"frame_rate\": 60,\n+            },\n+            \"ultra_high\": {\n+                \"resolution\": \"3840x2160\",\n+                \"frame_rate\": 60,\n+            },\n+            \"default_quality\": \"\",\n+        }\n+    }\n+\n+    scope = input(\" Please select the scope of the configuration [global/local]: \")\n+    if scope == \"global\":\n+        from manimlib.config import get_manim_dir\n+        file_name = os.path.join(get_manim_dir(), \"manimlib\", \"default_config.yml\")\n+    else:\n+        file_name = os.path.join(os.getcwd(), \"custom_config.yml\")\n+\n+    print(\"\\n directories:\")\n+    configuration[\"directories\"][\"output\"] = input(\"  [1/8] Where should manim output video and image files place: \")\n+    configuration[\"directories\"][\"raster_images\"] = input(\"  [2/8] Which folder should manim find raster images (.jpg .png .gif) in (optional): \")\n+    configuration[\"directories\"][\"vector_images\"] = input(\"  [3/8] Which folder should manim find vector images (.svg .xdv) in (optional): \")\n+    configuration[\"directories\"][\"sounds\"] = input(\"  [4/8] Which folder should manim find sound files (.mp3 .wav) in (optional): \")\n+    configuration[\"directories\"][\"temporary_storage\"] = input(\"  [5/8] Which folder should manim storage temporary files: \")\n+\n+    print(\"\\n tex:\")\n+    tex = input(\"  [6/8] Which executable file to use to compile [latex/xelatex]: \")\n+    if tex == \"latex\":\n+        configuration[\"tex\"][\"executable\"] = \"latex\"\n+        configuration[\"tex\"][\"template_file\"] = \"tex_template.tex\"\n+        configuration[\"tex\"][\"intermediate_filetype\"] = \"dvi\"\n+    else:\n+        configuration[\"tex\"][\"executable\"] = \"xelatex -no-pdf\"\n+        configuration[\"tex\"][\"template_file\"] = \"ctex_template.tex\"\n+        configuration[\"tex\"][\"intermediate_filetype\"] = \"xdv\"\n+\n+    print(\"\\n style:\")\n+    configuration[\"style\"][\"background_color\"] = input(\"  [7/8] Which background color do you want (hex code): \")\n+\n+    print(\"\\n camera_qualities:\")\n+    print(\"  Four defined qualities: low: 480p15  medium: 720p30  high: 1080p60  ultra_high: 2160p60\")\n+    configuration[\"camera_qualities\"][\"default_quality\"] = input(\"  [8/8] Which one to choose as the default rendering quality [low/medium/high/ultra_high]: \")\n+\n+    with open(file_name, 'w', encoding=\"utf_8\") as file:\n+        yaml.dump(configuration, file)\n+\n+    print(f\"\\nYou have set up a {scope} configuration file\")\n+    print(f\"You can manually modify it again in: {file_name}\\n\")\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR is on the right path but raises some concerns about user experience and best practices. Using `from manimlib import *` to include `imports.py` could lead to unexpected behavior, especially if it contains non-manim imports. Encourage explicit imports instead. Regarding configuration, default values should be in place for a smoother initial user experience. Be wary of issues arising on different OS setups, such as OpenGL context errors, which might necessitate import order adjustments. Further investigation into these environment-specific problems is advisable."
      }
    ],
    "codediff_length": 13806
  },
  {
    "index": 31,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/docs/async-await.rst b/docs/async-await.rst\nnew file mode 100644\nindex 0000000000..c8981f886a\n--- /dev/null\n+++ b/docs/async-await.rst\n@@ -0,0 +1,81 @@\n+.. _async_await:\n+\n+Using ``async`` and ``await``\n+=============================\n+\n+.. versionadded:: 2.0\n+\n+Routes, error handlers, before request, after request, and teardown\n+functions can all be coroutine functions if Flask is installed with the\n+``async`` extra (``pip install flask[async]``). This allows views to be\n+defined with ``async def`` and use ``await``.\n+\n+.. code-block:: python\n+\n+    @app.route(\"/get-data\")\n+    async def get_data():\n+        data = await async_db_query(...)\n+        return jsonify(data)\n+\n+\n+Performance\n+-----------\n+\n+Async functions require an event loop to run. Flask, as a WSGI\n+application, uses one worker to handle one request/response cycle.\n+When a request comes in to an async view, Flask will start an event loop\n+in a thread, run the view function there, then return the result.\n+\n+Each request still ties up one worker, even for async views. The upside\n+is that you can run async code within a view, for example to make\n+multiple concurrent database queries, HTTP requests to an external API,\n+etc. However, the number of requests your application can handle at one\n+time will remain the same.\n+\n+**Async is not inherently faster than sync code.** Async is beneficial\n+when performing concurrent IO-bound tasks, but will probably not improve\n+CPU-bound tasks. Traditional Flask views will still be appropriate for\n+most use cases, but Flask's async support enables writing and using\n+code that wasn't possible natively before.\n+\n+\n+When to use Quart instead\n+-------------------------\n+\n+Flask's async support is less performant than async-first frameworks due\n+to the way it is implemented. If you have a mainly async codebase it\n+would make sense to consider `Quart`_. Quart is a reimplementation of\n+Flask based on the `ASGI`_ standard instead of WSGI. This allows it to\n+handle many concurrent requests, long running requests, and websockets\n+without requiring individual worker processes or threads.\n+\n+It has also already been possible to run Flask with Gevent or Eventlet\n+to get many of the benefits of async request handling. These libraries\n+patch low-level Python functions to accomplish this, whereas ``async``/\n+``await`` and ASGI use standard, modern Python capabilities. Deciding\n+whether you should use Flask, Quart, or something else is ultimately up\n+to understanding the specific needs of your project.\n+\n+.. _Quart: https://gitlab.com/pgjones/quart\n+.. _ASGI: https://asgi.readthedocs.io/en/latest/\n+\n+\n+Extensions\n+----------\n+\n+Existing Flask extensions only expect views to be synchronous. If they\n+provide decorators to add functionality to views, those will probably\n+not work with async views because they will not await the function or be\n+awaitable. Other functions they provide will not be awaitable either and\n+will probably be blocking if called within an async view.\n+\n+Check the changelog of the extension you want to use to see if they've\n+implemented async support, or make a feature request or PR to them.\n+\n+\n+Other event loops\n+-----------------\n+\n+At the moment Flask only supports :mod:`asyncio`. It's possible to\n+override :meth:`flask.Flask.ensure_sync` to change how async functions\n+are wrapped to use a different library.\ndiff --git a/tests/test_async.py b/tests/test_async.py\nnew file mode 100644\nindex 0000000000..5893ff6954\n--- /dev/null\n+++ b/tests/test_async.py\n@@ -0,0 +1,139 @@\n+import asyncio\n+import sys\n+\n+import pytest\n+\n+from flask import Blueprint\n+from flask import Flask\n+from flask import request\n+from flask.helpers import run_async\n+\n+pytest.importorskip(\"asgiref\")\n+\n+\n+class AppError(Exception):\n+    pass\n+\n+\n+class BlueprintError(Exception):\n+    pass\n+\n+\n+@pytest.fixture(name=\"async_app\")\n+def _async_app():\n+    app = Flask(__name__)\n+\n+    @app.route(\"/\", methods=[\"GET\", \"POST\"])\n+    async def index():\n+        await asyncio.sleep(0)\n+        return request.method\n+\n+    @app.errorhandler(AppError)\n+    async def handle(_):\n+        return \"\", 412\n+\n+    @app.route(\"/error\")\n+    async def error():\n+        raise AppError()\n+\n+    blueprint = Blueprint(\"bp\", __name__)\n+\n+    @blueprint.route(\"/\", methods=[\"GET\", \"POST\"])\n+    async def bp_index():\n+        await asyncio.sleep(0)\n+        return request.method\n+\n+    @blueprint.errorhandler(BlueprintError)\n+    async def bp_handle(_):\n+        return \"\", 412\n+\n+    @blueprint.route(\"/error\")\n+    async def bp_error():\n+        raise BlueprintError()\n+\n+    app.register_blueprint(blueprint, url_prefix=\"/bp\")\n+\n+    return app\n+\n+\n+@pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires Python >= 3.7\")\n+@pytest.mark.parametrize(\"path\", [\"/\", \"/bp/\"])\n+def test_async_route(path, async_app):\n+    test_client = async_app.test_client()\n+    response = test_client.get(path)\n+    assert b\"GET\" in response.get_data()\n+    response = test_client.post(path)\n+    assert b\"POST\" in response.get_data()\n+\n+\n+@pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires Python >= 3.7\")\n+@pytest.mark.parametrize(\"path\", [\"/error\", \"/bp/error\"])\n+def test_async_error_handler(path, async_app):\n+    test_client = async_app.test_client()\n+    response = test_client.get(path)\n+    assert response.status_code == 412\n+\n+\n+@pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires Python >= 3.7\")\n+def test_async_before_after_request():\n+    app_first_called = False\n+    app_before_called = False\n+    app_after_called = False\n+    bp_before_called = False\n+    bp_after_called = False\n+\n+    app = Flask(__name__)\n+\n+    @app.route(\"/\")\n+    def index():\n+        return \"\"\n+\n+    @app.before_first_request\n+    async def before_first():\n+        nonlocal app_first_called\n+        app_first_called = True\n+\n+    @app.before_request\n+    async def before():\n+        nonlocal app_before_called\n+        app_before_called = True\n+\n+    @app.after_request\n+    async def after(response):\n+        nonlocal app_after_called\n+        app_after_called = True\n+        return response\n+\n+    blueprint = Blueprint(\"bp\", __name__)\n+\n+    @blueprint.route(\"/\")\n+    def bp_index():\n+        return \"\"\n+\n+    @blueprint.before_request\n+    async def bp_before():\n+        nonlocal bp_before_called\n+        bp_before_called = True\n+\n+    @blueprint.after_request\n+    async def bp_after(response):\n+        nonlocal bp_after_called\n+        bp_after_called = True\n+        return response\n+\n+    app.register_blueprint(blueprint, url_prefix=\"/bp\")\n+\n+    test_client = app.test_client()\n+    test_client.get(\"/\")\n+    assert app_first_called\n+    assert app_before_called\n+    assert app_after_called\n+    test_client.get(\"/bp/\")\n+    assert bp_before_called\n+    assert bp_after_called\n+\n+\n+@pytest.mark.skipif(sys.version_info >= (3, 7), reason=\"should only raise Python < 3.7\")\n+def test_async_runtime_error():\n+    with pytest.raises(RuntimeError):\n+        run_async(None)\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Phil, while I appreciate your approach, it lacks the scalability benefits of utilizing asynchronous servers, which is a crucial advantage. The current implementation seems to restrict concurrent async view functions to run on a single loop, limiting performance. It might be better to leverage a shared loop for multiple async routes. Additionally, the note on performance should include scaling limitations. Waiting for a future release before merging seems prudent to ensure robustness and avoid unnecessary complexity. There's potential in exploring integrations with asgiref and SQLAlchemy for better async support."
      }
    ],
    "codediff_length": 7032
  },
  {
    "index": 32,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/scripts/flaskext_migrate.py b/scripts/flaskext_migrate.py\nnew file mode 100644\nindex 0000000000..e24ab95cf9\n--- /dev/null\n+++ b/scripts/flaskext_migrate.py\n@@ -0,0 +1,160 @@\n+# Script which modifies source code away from the deprecated \"flask.ext\"\n+# format.\n+#\n+# Run in the terminal by typing: `python flaskext_migrate.py <source_file.py>`\n+#\n+# Author: Keyan Pishdadian 2015\n+\n+from redbaron import RedBaron\n+import sys\n+\n+\n+def read_source(input_file):\n+    \"\"\"Parses the input_file into a RedBaron FST.\"\"\"\n+    with open(input_file, \"r\") as source_code:\n+        red = RedBaron(source_code.read())\n+    return red\n+\n+\n+def write_source(red, input_file):\n+    \"\"\"Overwrites the input_file once the FST has been modified.\"\"\"\n+    with open(input_file, \"w\") as source_code:\n+        source_code.write(red.dumps())\n+\n+\n+def fix_imports(red):\n+    \"\"\"Wrapper which fixes \"from\" style imports and then \"import\" style.\"\"\"\n+    red = fix_standard_imports(red)\n+    red = fix_from_imports(red)\n+    return red\n+\n+\n+def fix_from_imports(red):\n+    \"\"\"\n+    Converts \"from\" style imports to not use \"flask.ext\".\n+\n+    Handles:\n+    Case 1: from flask.ext.foo import bam --> from flask_foo import bam\n+    Case 2: from flask.ext import foo --> import flask_foo as foo\n+    \"\"\"\n+    from_imports = red.find_all(\"FromImport\")\n+    for x, node in enumerate(from_imports):\n+        values = node.value\n+        if (values[0].value == 'flask') and (values[1].value == 'ext'):\n+            # Case 1\n+            if len(node.value) == 3:\n+                package = values[2].value\n+                modules = node.modules()\n+                module_string = _get_modules(modules)\n+                if len(modules) > 1:\n+                    node.replace(\"from flask_%s import %s\"\n+                                 % (package, module_string))\n+                else:\n+                    name = node.names()[0]\n+                    node.replace(\"from flask_%s import %s as %s\"\n+                                 % (package, module_string, name))\n+            # Case 2\n+            else:\n+                module = node.modules()[0]\n+                node.replace(\"import flask_%s as %s\"\n+                             % (module, module))\n+    return red\n+\n+\n+def fix_standard_imports(red):\n+    \"\"\"\n+    Handles import modification in the form:\n+    import flask.ext.foo\" --> import flask_foo\n+    \"\"\"\n+    imports = red.find_all(\"ImportNode\")\n+    for x, node in enumerate(imports):\n+        try:\n+            if (node.value[0].value[0].value == 'flask' and\n+               node.value[0].value[1].value == 'ext'):\n+                package = node.value[0].value[2].value\n+                name = node.names()[0].split('.')[-1]\n+                if name == package:\n+                    node.replace(\"import flask_%s\" % (package))\n+                else:\n+                    node.replace(\"import flask_%s as %s\" % (package, name))\n+        except IndexError:\n+            pass\n+\n+    return red\n+\n+\n+def _get_modules(module):\n+    \"\"\"\n+    Takes a list of modules and converts into a string.\n+\n+    The module list can include parens, this function checks each element in\n+    the list, if there is a paren then it does not add a comma before the next\n+    element. Otherwise a comma and space is added. This is to preserve module\n+    imports which are multi-line and/or occur within parens. While also not\n+    affecting imports which are not enclosed.\n+    \"\"\"\n+    modules_string = [cur + ', ' if cur.isalnum() and next.isalnum()\n+                      else cur\n+                      for (cur, next) in zip(module, module[1:]+[''])]\n+\n+    return ''.join(modules_string)\n+\n+\n+def fix_function_calls(red):\n+    \"\"\"\n+    Modifies function calls in the source to reflect import changes.\n+\n+    Searches the AST for AtomtrailerNodes and replaces them.\n+    \"\"\"\n+    atoms = red.find_all(\"Atomtrailers\")\n+    for x, node in enumerate(atoms):\n+        try:\n+            if (node.value[0].value == 'flask' and\n+               node.value[1].value == 'ext'):\n+                params = _form_function_call(node)\n+                node.replace(\"flask_%s%s\" % (node.value[2], params))\n+        except IndexError:\n+            pass\n+\n+    return red\n+\n+\n+def _form_function_call(node):\n+    \"\"\"\n+    Reconstructs function call strings when making attribute access calls.\n+    \"\"\"\n+    node_vals = node.value\n+    output = \".\"\n+    for x, param in enumerate(node_vals[3::]):\n+        if param.dumps()[0] == \"(\":\n+            output = output[0:-1] + param.dumps()\n+            return output\n+        else:\n+            output += param.dumps() + \".\"\n+\n+\n+def check_user_input():\n+    \"\"\"Exits and gives error message if no argument is passed in the shell.\"\"\"\n+    if len(sys.argv) < 2:\n+        sys.exit(\"No filename was included, please try again.\")\n+\n+\n+def fix_tester(ast):\n+    \"\"\"Wrapper which allows for testing when not running from shell.\"\"\"\n+    ast = fix_imports(ast)\n+    ast = fix_function_calls(ast)\n+    return ast.dumps()\n+\n+\n+def fix():\n+    \"\"\"Wrapper for user argument checking and import fixing.\"\"\"\n+    check_user_input()\n+    input_file = sys.argv[1]\n+    ast = read_source(input_file)\n+    ast = fix_imports(ast)\n+    ast = fix_function_calls(ast)\n+    write_source(ast, input_file)\n+\n+\n+if __name__ == \"__main__\":\n+    fix()\ndiff --git a/scripts/test_import_migration.py b/scripts/test_import_migration.py\nnew file mode 100644\nindex 0000000000..0220e70a03\n--- /dev/null\n+++ b/scripts/test_import_migration.py\n@@ -0,0 +1,71 @@\n+# Tester for the flaskext_migrate.py module located in flask/scripts/\n+#\n+# Author: Keyan Pishdadian\n+import pytest\n+from redbaron import RedBaron\n+import flaskext_migrate as migrate\n+\n+\n+def test_simple_from_import():\n+    red = RedBaron(\"from flask.ext import foo\")\n+    output = migrate.fix_tester(red)\n+    assert output == \"import flask_foo as foo\"\n+\n+\n+def test_from_to_from_import():\n+    red = RedBaron(\"from flask.ext.foo import bar\")\n+    output = migrate.fix_tester(red)\n+    assert output == \"from flask_foo import bar as bar\"\n+\n+\n+def test_multiple_import():\n+    red = RedBaron(\"from flask.ext.foo import bar, foobar, something\")\n+    output = migrate.fix_tester(red)\n+    assert output == \"from flask_foo import bar, foobar, something\"\n+\n+\n+def test_multiline_import():\n+    red = RedBaron(\"from flask.ext.foo import \\\n+                    bar,\\\n+                    foobar,\\\n+                    something\")\n+    output = migrate.fix_tester(red)\n+    assert output == \"from flask_foo import bar, foobar, something\"\n+\n+\n+def test_module_import():\n+    red = RedBaron(\"import flask.ext.foo\")\n+    output = migrate.fix_tester(red)\n+    assert output == \"import flask_foo\"\n+\n+\n+def test_named_module_import():\n+    red = RedBaron(\"import flask.ext.foo as foobar\")\n+    output = migrate.fix_tester(red)\n+    assert output == \"import flask_foo as foobar\"\n+\n+\n+def test__named_from_import():\n+    red = RedBaron(\"from flask.ext.foo import bar as baz\")\n+    output = migrate.fix_tester(red)\n+    assert output == \"from flask_foo import bar as baz\"\n+\n+\n+def test_parens_import():\n+    red = RedBaron(\"from flask.ext.foo import (bar, foo, foobar)\")\n+    output = migrate.fix_tester(red)\n+    assert output == \"from flask_foo import (bar, foo, foobar)\"\n+\n+\n+def test_function_call_migration():\n+    red = RedBaron(\"flask.ext.foo(var)\")\n+    output = migrate.fix_tester(red)\n+    assert output == \"flask_foo(var)\"\n+\n+\n+def test_nested_function_call_migration():\n+    red = RedBaron(\"import flask.ext.foo\\n\\n\"\n+                   \"flask.ext.foo.bar(var)\")\n+    output = migrate.fix_tester(red)\n+    assert output == (\"import flask_foo\\n\\n\"\n+                      \"flask_foo.bar(var)\")\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The script effectively supports single imports but has issues with edge cases that could cause breakage. It's wise to delegate parsing to a robust library like RedBaron, which has proven useful, although it doesnt cover all scenarios. Adding tests was a good move, but ensure that RedBaron is included as a Travis dependency for the build to pass. Additionally, consider moving tests to the same directory as the script for easier integration. Address the remaining edge cases and clarify how to replace function calls without excessive refactoring. Overall, solid attention to detail!"
      }
    ],
    "codediff_length": 7714
  },
  {
    "index": 33,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/keras/engine/training.py b/keras/engine/training.py\nindex eca655d5d4fe..a9c4878ed35b 100644\n--- a/keras/engine/training.py\n+++ b/keras/engine/training.py\n@@ -579,101 +580,6 @@ def _standardize_weights(y, sample_weight=None, class_weight=None,\n             return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n \n \n-class GeneratorEnqueuer(object):\n-    \"\"\"Builds a queue out of a data generator.\n-\n-    Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n-\n-    # Arguments\n-        generator: a generator function which endlessly yields data\n-        pickle_safe: use multiprocessing if True, otherwise threading\n-    \"\"\"\n-\n-    def __init__(self, generator, pickle_safe=False):\n-        self._generator = generator\n-        self._pickle_safe = pickle_safe\n-        self._threads = []\n-        self._stop_event = None\n-        self.queue = None\n-\n-    def start(self, workers=1, max_q_size=10, wait_time=0.05):\n-        \"\"\"Kicks off threads which add data from the generator into the queue.\n-\n-        # Arguments\n-            workers: number of worker threads\n-            max_q_size: queue size (when full, threads could block on put())\n-            wait_time: time to sleep in-between calls to put()\n-        \"\"\"\n-\n-        def data_generator_task():\n-            while not self._stop_event.is_set():\n-                try:\n-                    if self._pickle_safe or self.queue.qsize() < max_q_size:\n-                        generator_output = next(self._generator)\n-                        self.queue.put(generator_output)\n-                    else:\n-                        time.sleep(wait_time)\n-                except Exception:\n-                    self._stop_event.set()\n-                    raise\n-\n-        try:\n-            if self._pickle_safe:\n-                self.queue = multiprocessing.Queue(maxsize=max_q_size)\n-                self._stop_event = multiprocessing.Event()\n-                if hasattr(data_generator_task, 'lock'):\n-                    # We should replace the threading lock of the iterator\n-                    # with a process-safe lock.\n-                    data_generator_task.lock = multiprocessing.Lock()\n-            else:\n-                self.queue = queue.Queue()\n-                self._stop_event = threading.Event()\n-\n-            for _ in range(workers):\n-                if self._pickle_safe:\n-                    # Reset random seed else all children processes\n-                    # share the same seed\n-                    np.random.seed()\n-                    thread = multiprocessing.Process(target=data_generator_task)\n-                    thread.daemon = True\n-                else:\n-                    thread = threading.Thread(target=data_generator_task)\n-                self._threads.append(thread)\n-                thread.start()\n-        except:\n-            self.stop()\n-            raise\n-\n-    def is_running(self):\n-        return self._stop_event is not None and not self._stop_event.is_set()\n-\n-    def stop(self, timeout=None):\n-        \"\"\"Stop running threads and wait for them to exit, if necessary.\n-\n-        Should be called by the same thread which called start().\n-\n-        # Arguments\n-            timeout: maximum time to wait on thread.join()\n-        \"\"\"\n-        if self.is_running():\n-            self._stop_event.set()\n-\n-        for thread in self._threads:\n-            if thread.is_alive():\n-                if self._pickle_safe:\n-                    thread.terminate()\n-                else:\n-                    thread.join(timeout)\n-\n-        if self._pickle_safe:\n-            if self.queue is not None:\n-                self.queue.close()\n-\n-        self._threads = []\n-        self._stop_event = None\n-        self.queue = None\n-\n-\n class Model(Container):\n     \"\"\"The `Model` class adds training & evaluation routines to a `Container`.\n     \"\"\"\n\ndiff --git a/keras/utils/data_utils.py b/keras/utils/data_utils.py\nindex 0928f47b1029..d3caddc10d48 100644\n--- a/keras/utils/data_utils.py\n+++ b/keras/utils/data_utils.py\n@@ -282,3 +295,323 @@ def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n         return True\n     else:\n         return False\n+\n+\n+class Sequence(object):\n+    \"\"\"Base object for fitting to a sequence of data, such as a dataset.\n+    Every `Sequence` must implements the `__getitem__` and the `__len__` methods.\n+\n+    # Examples\n+\n+    ```python\n+    from skimage.io import imread\n+    from skimage.transform import resize\n+    import numpy as np\n+\n+    # Here, `x_set` is list of path to the images\n+    # and `y_set` are the associated classes.\n+\n+    class CIFAR10Sequence(Sequence):\n+        def __init__(self, x_set, y_set, batch_size):\n+            self.X,self.y = x_set,y_set\n+            self.batch_size = batch_size\n+\n+        def __len__(self):\n+            return len(self.X) // self.batch_size\n+\n+        def __getitem__(self,idx):\n+            batch_x = self.X[idx*self.batch_size:(idx+1)*self.batch_size]\n+            batch_y = self.y[idx*self.batch_size:(idx+1)*self.batch_size]\n+\n+            return np.array([\n+                resize(imread(file_name), (200,200))\n+                   for file_name in batch_x]), np.array(batch_y)\n+    ```\n+    \"\"\"\n+\n+    @abstractmethod\n+    def __getitem__(self, index):\n+        \"\"\"Gets batch at position `index`.\n+\n+        # Arguments\n+            index: position of the batch in the Sequence.\n+\n+        # Returns\n+            A batch\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    @abstractmethod\n+    def __len__(self):\n+        \"\"\"Number of batch in the Sequence.\n+\n+        # Returns\n+            The number of batches in the Sequence.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+\n+def get_index(ds, i):\n+    \"\"\"Quick fix for Python2, otherwise, it cannot be pickled.\n+\n+    # Arguments\n+        ds: a Sequence object\n+        i: index\n+\n+    # Returns\n+        The value at index `i`.\n+    \"\"\"\n+    return ds[i]\n+\n+\n+class SequenceEnqueuer(object):\n+    \"\"\"Base class to enqueue inputs.\n+    The task of an Enqueuer is to use parallelism to speed up the preprocessing.\n+    This is done with processes or threads.\n+\n+    # Examples\n+\n+    ```python\n+    enqueuer = SequenceEnqueuer(...)\n+    enqueuer.start()\n+    datas = enqueuer.get()\n+    for data in datas:\n+        # Use the inputs; training, evaluating, predicting.\n+        # ... stop sometime.\n+    enqueuer.close()\n+    ```\n+\n+    The `enqueuer.get()` should be an infinite stream of datas.\n+\n+    \"\"\"\n+\n+    @abstractmethod\n+    def is_running(self):\n+        raise NotImplemented\n+\n+    @abstractmethod\n+    def start(self, workers=1, max_queue_size=10):\n+        \"\"\"Starts the handler's workers.\n+\n+        # Arguments\n+            workers: number of worker threads\n+            max_queue_size: queue size (when full, threads could block on `put()`)\n+        \"\"\"\n+        raise NotImplemented\n+\n+    @abstractmethod\n+    def stop(self, timeout=None):\n+        \"\"\"Stop running threads and wait for them to exit, if necessary.\n+\n+        Should be called by the same thread which called start().\n+\n+        # Arguments\n+            timeout: maximum time to wait on thread.join()\n+        \"\"\"\n+        raise NotImplemented\n+\n+    @abstractmethod\n+    def get(self):\n+        \"\"\"Creates a generator to extract data from the queue. Skip the data if it's None.\n+\n+        # Returns\n+            Generator yielding tuples (inputs, targets)\n+                or (inputs, targets, sample_weights)\n+        \"\"\"\n+        raise NotImplemented\n+\n+\n+class OrderedEnqueuer(SequenceEnqueuer):\n+    \"\"\"Builds a Enqueuer from a Sequence.\n+\n+    Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n+\n+    # Arguments\n+        sequence: A `keras.utils.data_utils.Sequence` object.\n+        use_multiprocessing: use multiprocessing if True, otherwise threading\n+        scheduling: Sequential querying of datas if 'sequential', random otherwise.\n+    \"\"\"\n+\n+    def __init__(self, sequence, use_multiprocessing=False, scheduling='sequential'):\n+        self.sequence = sequence\n+        self.use_multiprocessing = use_multiprocessing\n+        self.scheduling = scheduling\n+        self.workers = 0\n+        self.executor = None\n+        self.queue = None\n+        self.run_thread = None\n+        self.stop_signal = None\n+\n+    def is_running(self):\n+        return self.stop_signal is not None and not self.stop_signal.is_set()\n+\n+    def start(self, workers=1, max_queue_size=10):\n+        \"\"\"Start the handler's workers.\n+\n+        # Arguments\n+            workers: number of worker threads\n+            max_queue_size: queue size (when full, workers could block on put())\n+        \"\"\"\n+        if self.use_multiprocessing:\n+            self.executor = multiprocessing.Pool(workers)\n+        else:\n+            self.executor = ThreadPool(workers)\n+        self.queue = queue.Queue(max_queue_size)\n+        self.stop_signal = threading.Event()\n+        self.run_thread = threading.Thread(target=self._run)\n+        self.run_thread.daemon = True\n+        self.run_thread.start()\n+\n+    def _run(self):\n+        \"\"\"Function to submit request to the executor and queue the `Future` objects.\"\"\"\n+        sequence = list(range(len(self.sequence)))\n+        while True:\n+            if self.scheduling is not 'sequential':\n+                random.shuffle(sequence)\n+            for i in sequence:\n+                if self.stop_signal.is_set():\n+                    return\n+                self.queue.put(\n+                    self.executor.apply_async(get_index, (self.sequence, i)), block=True)\n+\n+    def get(self):\n+        \"\"\"Creates a generator to extract data from the queue. Skip the data if it's None.\n+\n+        # Returns\n+            Generator yielding tuples (inputs, targets)\n+                or (inputs, targets, sample_weights)\n+        \"\"\"\n+        try:\n+            while self.is_running():\n+                inputs = self.queue.get(block=True).get()\n+                if inputs is not None:\n+                    yield inputs\n+        except Exception as e:\n+            self.stop()\n+            raise StopIteration(e)\n+\n+    def stop(self, timeout=None):\n+        \"\"\"Stops running threads and wait for them to exit, if necessary.\n+\n+        Should be called by the same thread which called start().\n+\n+        # Arguments\n+            timeout: maximum time to wait on thread.join()\n+        \"\"\"\n+        self.stop_signal.set()\n+        with self.queue.mutex:\n+            self.queue.queue.clear()\n+            self.queue.unfinished_tasks = 0\n+            self.queue.not_full.notify()\n+        self.executor.close()\n+        self.run_thread.join(timeout)\n+\n+\n+class GeneratorEnqueuer(SequenceEnqueuer):\n+    \"\"\"Builds a queue out of a data generator.\n+\n+    Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n+\n+    # Arguments\n+        generator: a generator function which endlessly yields data\n+        use_multiprocessing: use multiprocessing if True, otherwise threading\n+        wait_time: time to sleep in-between calls to `put()`\n+        random_seed: Initial seed for workers, will be incremented by one for each workers.\n+    \"\"\"\n+\n+    def __init__(self, generator, use_multiprocessing=False, wait_time=0.05, random_seed=None):\n+        self.wait_time = wait_time\n+        self._generator = generator\n+        self._use_multiprocessing = use_multiprocessing\n+        self._threads = []\n+        self._stop_event = None\n+        self.queue = None\n+        self.random_seed = random_seed\n+\n+    def start(self, workers=1, max_queue_size=10):\n+        \"\"\"Kicks off threads which add data from the generator into the queue.\n+\n+        # Arguments\n+            workers: number of worker threads\n+            max_queue_size: queue size (when full, threads could block on put())\n+        \"\"\"\n+\n+        def data_generator_task():\n+            while not self._stop_event.is_set():\n+                try:\n+                    if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n+                        generator_output = next(self._generator)\n+                        self.queue.put(generator_output)\n+                    else:\n+                        time.sleep(self.wait_time)\n+                except Exception:\n+                    self._stop_event.set()\n+                    raise\n+\n+        try:\n+            if self._use_multiprocessing:\n+                self.queue = multiprocessing.Queue(maxsize=max_queue_size)\n+                self._stop_event = multiprocessing.Event()\n+            else:\n+                self.queue = queue.Queue()\n+                self._stop_event = threading.Event()\n+\n+            for _ in range(workers):\n+                if self._use_multiprocessing:\n+                    # Reset random seed else all children processes\n+                    # share the same seed\n+                    np.random.seed(self.random_seed)\n+                    thread = multiprocessing.Process(target=data_generator_task)\n+                    thread.daemon = True\n+                    if self.random_seed is not None:\n+                        self.random_seed += 1\n+                else:\n+                    thread = threading.Thread(target=data_generator_task)\n+                self._threads.append(thread)\n+                thread.start()\n+        except:\n+            self.stop()\n+            raise\n+\n+    def is_running(self):\n+        return self._stop_event is not None and not self._stop_event.is_set()\n+\n+    def stop(self, timeout=None):\n+        \"\"\"Stops running threads and wait for them to exit, if necessary.\n+\n+        Should be called by the same thread which called start().\n+\n+        # Arguments\n+            timeout: maximum time to wait on thread.join()\n+        \"\"\"\n+        if self.is_running():\n+            self._stop_event.set()\n+\n+        for thread in self._threads:\n+            if thread.is_alive():\n+                if self._use_multiprocessing:\n+                    thread.terminate()\n+                else:\n+                    thread.join(timeout)\n+\n+        if self._use_multiprocessing:\n+            if self.queue is not None:\n+                self.queue.close()\n+\n+        self._threads = []\n+        self._stop_event = None\n+        self.queue = None\n+\n+    def get(self):\n+        \"\"\"Creates a generator to extract data from the queue. Skip the data if it's None.\n+\n+        # Returns\n+            A generator\n+        \"\"\"\n+        while self.is_running():\n+            if not self.queue.empty():\n+                inputs = self.queue.get()\n+                if inputs is not None:\n+                    yield inputs\n+            else:\n+                time.sleep(self.wait_time)\ndiff --git a/tests/keras/utils/data_utils_test.py b/tests/keras/utils/data_utils_test.py\nindex f3088c22e1b1..716d7e8af097 100644\n--- a/tests/keras/utils/data_utils_test.py\n+++ b/tests/keras/utils/data_utils_test.py\n@@ -64,5 +77,154 @@ def test_data_utils(in_tmpdir):\n     os.remove('test.txt')\n     os.remove('test.zip')\n \n+\n+\"\"\"Enqueuers Tests\"\"\"\n+\n+\n+class threadsafe_iter:\n+    \"\"\"Takes an iterator/generator and makes it thread-safe by\n+    serializing call to the `next` method of given iterator/generator.\n+    \"\"\"\n+\n+    def __init__(self, it):\n+        self.it = it\n+        self.lock = threading.Lock()\n+\n+    def __iter__(self):\n+        return self\n+\n+    def __next__(self):\n+        return self.next()\n+\n+    def next(self):\n+        with self.lock:\n+            return next(self.it)\n+\n+\n+def threadsafe_generator(f):\n+    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n+    \"\"\"\n+\n+    def g(*a, **kw):\n+        return threadsafe_iter(f(*a, **kw))\n+\n+    return g\n+\n+\n+class TestSequence(Sequence):\n+    def __init__(self, shape):\n+        self.shape = shape\n+\n+    def __getitem__(self, item):\n+        return np.ones(self.shape, dtype=np.uint8) * item\n+\n+    def __len__(self):\n+        return 100\n+\n+\n+class FaultSequence(Sequence):\n+    def __getitem__(self, item):\n+        raise IndexError(item, 'is not present')\n+\n+    def __len__(self):\n+        return 100\n+\n+\n+@threadsafe_generator\n+def create_generator_from_sequence_threads(ds):\n+    for i in cycle(range(len(ds))):\n+        yield ds[i]\n+\n+\n+def create_generator_from_sequence_pcs(ds):\n+    for i in cycle(range(len(ds))):\n+        yield ds[i]\n+\n+\n+def test_generator_enqueuer_threads():\n+    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_threads(\n+        TestSequence([3, 200, 200, 3])), use_multiprocessing=False)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    acc = []\n+    for i in range(100):\n+        acc.append(int(next(gen_output)[0, 0, 0, 0]))\n+\n+    \"\"\"\n+     Not comparing the order since it is not guarantee.\n+     It may get ordered, but not a lot, one thread can take the GIL before he was supposed to.\n+    \"\"\"\n+    assert len(set(acc) - set(range(100))) == 0, \"Output is not the same\"\n+    enqueuer.stop()\n+\n+\n+def test_generator_enqueuer_processes():\n+    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n+        TestSequence([3, 200, 200, 3])), use_multiprocessing=True)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    acc = []\n+    for i in range(100):\n+        acc.append(int(next(gen_output)[0, 0, 0, 0]))\n+    assert acc != list(range(100)), \"Order was keep in GeneratorEnqueuer with processes\"\n+    enqueuer.stop()\n+\n+\n+def test_generator_enqueuer_fail_threads():\n+    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_threads(\n+        FaultSequence()), use_multiprocessing=False)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    with pytest.raises(StopIteration):\n+        next(gen_output)\n+\n+\n+def test_generator_enqueuer_fail_processes():\n+    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n+        FaultSequence()), use_multiprocessing=True)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    with pytest.raises(StopIteration):\n+        next(gen_output)\n+\n+\n+def test_ordered_enqueuer_threads():\n+    enqueuer = OrderedEnqueuer(TestSequence([3, 200, 200, 3]), use_multiprocessing=False)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    acc = []\n+    for i in range(100):\n+        acc.append(next(gen_output)[0, 0, 0, 0])\n+    assert acc == list(range(100)), \"Order was not keep in GeneratorEnqueuer with threads\"\n+    enqueuer.stop()\n+\n+\n+def test_ordered_enqueuer_processes():\n+    enqueuer = OrderedEnqueuer(TestSequence([3, 200, 200, 3]), use_multiprocessing=True)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    acc = []\n+    for i in range(100):\n+        acc.append(next(gen_output)[0, 0, 0, 0])\n+    assert acc == list(range(100)), \"Order was not keep in GeneratorEnqueuer with processes\"\n+    enqueuer.stop()\n+\n+\n+def test_ordered_enqueuer_fail_threads():\n+    enqueuer = OrderedEnqueuer(FaultSequence(), use_multiprocessing=False)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    with pytest.raises(StopIteration):\n+        next(gen_output)\n+\n+\n+def test_ordered_enqueuer_fail_processes():\n+    enqueuer = OrderedEnqueuer(FaultSequence(), use_multiprocessing=True)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    with pytest.raises(StopIteration):\n+        next(gen_output)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR suggests preserving the order in `predict_generator` with multi-processing, which is crucial for certain use cases. However, we should improve the generator-writing experience for users instead of just requiring them to implement their own. This can be achieved through a more structured Dataset API, but must remain backward-compatible. Adding `futures` as an optional dependency would help, but let's avoid unnecessary complexity in the API. The current design should focus on simplicity while addressing pickling issues effectively. Would appreciate thoughts from @PavlosMelissinos on dataset complexities."
      }
    ],
    "codediff_length": 19339
  },
  {
    "index": 34,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/examples/mnist_tfrecord_recordinput.py b/examples/mnist_tfrecord_recordinput.py\nnew file mode 100644\nindex 000000000000..0972763c4ac7\n--- /dev/null\n+++ b/examples/mnist_tfrecord_recordinput.py\n@@ -0,0 +1,227 @@\n+'''Optimized MNIST dataset with TFRecords, the standard TensorFlow data format.\n+\n+TFRecord is a data format supported throughout TensorFlow.\n+For a straightforward usage example see mnist_tfrecord.py.\n+This example demonstrates how to write and read TFRecord data using\n+Input Tensors in a way that is better optimized for high performance\n+on large datasets with Keras.\n+\n+Gets to 99.25% test accuracy after 12 epochs\n+(there is still some margin for parameter tuning).\n+'''\n+import os\n+import copy\n+import time\n+\n+import numpy as np\n+\n+import tensorflow as tf\n+from tensorflow.python.ops import data_flow_ops\n+from keras import backend as K\n+from keras.models import Model\n+from keras import layers\n+from keras.utils.generic_utils import Progbar\n+\n+from keras.datasets import mnist\n+\n+if K.backend() != 'tensorflow':\n+    raise RuntimeError('This example can only run with the '\n+                       'TensorFlow backend for the time being, '\n+                       'because it requires TFRecords, which '\n+                       'are not supported on other platforms.')\n+\n+\n+def images_to_tfrecord(images, labels, filename):\n+    \"\"\"Write images and their labels to a TFRecord file.\n+\n+    # Arguments\n+        images: A numpy array or list of image data with\n+            shape (images, rows, cols, depth).\n+        labels: A numpy array of labels, with one for each image.\n+        filename: Path and name for the output dataset TFRecord file.\n+            An example is `'path/to/mnist_train.tfrecord'`\n+    \"\"\"\n+    def _int64_feature(value):\n+        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n+\n+    def _bytes_feature(value):\n+        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n+\n+    \"\"\" Save data into TFRecord \"\"\"\n+    if not os.path.isfile(filename):\n+        num_examples = images.shape[0]\n+\n+        rows = images.shape[1]\n+        cols = images.shape[2]\n+        depth = images.shape[3]\n+\n+        print('Writing', filename)\n+        writer = tf.python_io.TFRecordWriter(filename)\n+        for index in range(num_examples):\n+            image_raw = images[index].tostring()\n+            example = tf.train.Example(features=tf.train.Features(feature={\n+                'height': _int64_feature(rows),\n+                'width': _int64_feature(cols),\n+                'depth': _int64_feature(depth),\n+                'label': _int64_feature(int(labels[index])),\n+                'image_raw': _bytes_feature(image_raw)}))\n+            writer.write(example.SerializeToString())\n+        writer.close()\n+    else:\n+        print('tfrecord %s already exists' % filename)\n+\n+\n+def read_and_decode_recordinput(tf_glob, one_hot=True,\n+                                classes=None,\n+                                batch_shape=None, parallelism=1):\n+    \"\"\"Get a TF Tensor that supplies shuffled batches of images.\n+\n+    # Arguments\n+        tf_glob: File path for selecting one or more tfrecord files.\n+            Examples are `'path/to/data.tfrecord'` and `'path/to/*.tfrecord'`.\n+        one_hot: Use one hot encoding for labels, also known as categorical.\n+        batch_shape: Specify the desired image batch shape, where the first\n+            entry is the batch size. MNIST might be (128, 28, 28, 1).\n+        parallelism: The number of threads to use for loading new data.\n+            A reasonable value is the number of logical cores on your processor.\n+    \"\"\"\n+    if batch_shape is None:\n+        batch_shape = [1000, 28, 28, 1]\n+    print 'Creating graph for loading %s TFRecords...' % tf_glob\n+    with tf.variable_scope(\"TFRecords\"):\n+        record_input = data_flow_ops.RecordInput(\n+            tf_glob, batch_size=batch_shape[0], parallelism=parallelism)\n+        records_op = record_input.get_yield_op()\n+        records_op = tf.split(records_op, batch_shape[0], 0)\n+        records_op = [tf.reshape(record, []) for record in records_op]\n+        progbar = Progbar(len(records_op))\n+\n+        images = []\n+        labels = []\n+        for i, serialized_example in enumerate(records_op):\n+            progbar.update(i)\n+            with tf.variable_scope(\"parse_images\", reuse=True):\n+                features = tf.parse_single_example(\n+                    serialized_example,\n+                    features={\n+                        'label': tf.FixedLenFeature([], tf.int64),\n+                        'image_raw': tf.FixedLenFeature([], tf.string),\n+                    })\n+                img = tf.decode_raw(features['image_raw'], tf.uint8)\n+                img.set_shape(batch_shape[1] * batch_shape[2])\n+                img = tf.reshape(img, [1] + batch_shape[1:])\n+\n+                img = tf.cast(img, tf.float32) * (1. / 255) - 0.5\n+\n+                label = tf.cast(features['label'], tf.int32)\n+                if one_hot and classes:\n+                    label = tf.one_hot(label, classes)\n+\n+                images.append(img)\n+                labels.append(label)\n+\n+        images = tf.parallel_stack(images, 0)\n+        labels = tf.parallel_stack(labels, 0)\n+        images = tf.cast(images, tf.float32)\n+        images = tf.reshape(images, shape=batch_shape)\n+\n+        return images, labels\n+\n+\n+def save_mnist_as_tfrecord():\n+    \"\"\"Save one tfrecord file for each of the train and test mnist datasets.\n+    \"\"\"\n+    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n+    x_train = x_train[..., np.newaxis]\n+    x_test = x_test[..., np.newaxis]\n+    images_to_tfrecord(images=x_train, labels=y_train, filename='train.mnist.tfrecord')\n+    images_to_tfrecord(images=x_test, labels=y_test, filename='test.mnist.tfrecord')\n+\n+\n+def cnn_layers(x_train_input):\n+    \"\"\"Create the CNN layers for use with either numpy inputs or tensor inputs.\n+    \"\"\"\n+    x = layers.Conv2D(32, (3, 3), activation='relu', padding='valid')(x_train_input)\n+    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n+    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n+    x = layers.Dropout(0.25)(x)\n+    x = layers.Flatten()(x)\n+    x = layers.Dense(128, activation='relu')(x)\n+    x = layers.Dropout(0.5)(x)\n+    x_train_out = layers.Dense(classes,\n+                               activation='softmax',\n+                               name='x_train_out')(x)\n+    return x_train_out\n+\n+\n+sess = K.get_session()\n+\n+save_mnist_as_tfrecord()\n+\n+batch_size = 100\n+batch_shape = [batch_size, 28, 28, 1]\n+epochs = 12\n+steps_per_epoch = 1000\n+classes = 10\n+parallelism = 10\n+\n+x_train_batch, y_train_batch = read_and_decode_recordinput(\n+    'train.mnist.tfrecord',\n+    one_hot=True,\n+    classes=classes,\n+    batch_shape=batch_shape,\n+    parallelism=parallelism)\n+\n+x_batch_shape = x_train_batch.get_shape().as_list()\n+y_batch_shape = y_train_batch.get_shape().as_list()\n+\n+# The input tensors are provided directly into the Model network.\n+# The network is fixed once it is initialized, so it must be\n+# reconstructed every time a new input data source is needed.\n+# This is substantially different from typical\n+# Keras numpy array inputs, and is more like TensorFlow.\n+x_train_in = layers.Input(tensor=x_train_batch, batch_shape=x_batch_shape)\n+x_train_out = cnn_layers(x_train_in)\n+y_train_in = layers.Input(tensor=y_train_batch, batch_shape=y_batch_shape, name='y_labels')\n+train_model = Model(inputs=[x_train_in], outputs=[x_train_out])\n+train_model.compile(optimizer='rmsprop',\n+                    loss='categorical_crossentropy',\n+                    metrics=['accuracy'])\n+\n+# The input data was created with x_train_input,\n+# so only the label data needs to be provided.\n+train_model.fit(y=y_train_in,\n+                batch_size=None,\n+                epochs=epochs,\n+                steps_per_epoch=steps_per_epoch)\n+train_model.save_weights('saved_wt.h5')\n+\n+K.clear_session()\n+\n+# Second Session, test data\n+x_test_batch, y_test_batch = read_and_decode_recordinput(\n+    'test.mnist.tfrecord',\n+    one_hot=True,\n+    classes=classes,\n+    batch_shape=batch_shape,\n+    parallelism=parallelism)\n+\n+x_batch_shape = x_test_batch.get_shape().as_list()\n+y_batch_shape = y_test_batch.get_shape().as_list()\n+\n+# Create a completely new network for new input data.\n+x_test_in = layers.Input(tensor=x_test_batch, batch_shape=x_batch_shape)\n+x_test_out = cnn_layers(x_test_in)\n+y_test_in = layers.Input(tensor=y_test_batch, batch_shape=y_batch_shape, name='y_labels')\n+test_model = Model(inputs=[x_test_in], outputs=[x_test_out])\n+test_model.compile(optimizer='rmsprop',\n+                   loss='categorical_crossentropy',\n+                   metrics=['accuracy'])\n+test_model.load_weights('saved_wt.h5')\n+test_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n+\n+# Take steps for each element of validation data.\n+validation_samples = 10000\n+evaluate_steps = validation_samples / batch_size\n+loss, acc = test_model.evaluate(y=y_test_in, steps=evaluate_steps)\n+print('\\nTest accuracy: {0}'.format(np.mean(acc)))\ndiff --git a/keras/engine/training.py b/keras/engine/training.py\nindex 7b487d34f511..0a31574ba659 100644\n--- a/keras/engine/training.py\n+++ b/keras/engine/training.py\n@@ -923,56 +916,213 @@ def append_metric(layer_num, metric_name, metric_tensor):\n         trainable_weights = self.trainable_weights\n         self._collected_trainable_weights = trainable_weights\n \n-    def _make_train_function(self):\n-        if not hasattr(self, 'train_function'):\n+    def _prepare_sample_weights(self, sample_weight_mode, skip_indices):\n+        \"\"\"Prepares and verifies sample weight member variables during compilation.\n+        \"\"\"\n+        # Prepare sample weights.\n+        if self._tensor_inputs:\n+            if not all(l is None for l in self.target_configuration):\n+                if sample_weight_mode is not None:\n+                    # tensor + sample weights not yet implemented\n+                    raise NotImplementedError\n+                if self.loss_weights is not None:\n+                    # tensor + loss weights not yet implemented\n+                    raise NotImplementedError\n+            self.sample_weight_mode = 'disabled'\n+            return [], self.sample_weight_mode\n+\n+        self.sample_weight_mode = sample_weight_mode\n+        sample_weights = []\n+        sample_weight_modes = []\n+        if isinstance(sample_weight_mode, dict):\n+            for name in sample_weight_mode:\n+                if name not in self.output_names:\n+                    raise ValueError('Unknown entry in '\n+                                     'sample_weight_mode dictionary: \"' +\n+                                     name + '\". '\n+                                     'Only expected the following keys: ' +\n+                                     str(self.output_names))\n+            for i, name in enumerate(self.output_names):\n+                if i in skip_indices:\n+                    weight = None\n+                    sample_weight_modes.append(None)\n+                else:\n+                    if name not in sample_weight_mode:\n+                        raise ValueError('Output \"' + name +\n+                                         '\" missing from sample_weight_modes '\n+                                         'dictionary')\n+                    if sample_weight_mode.get(name) == 'temporal':\n+                        weight = K.placeholder(ndim=2,\n+                                               name=name + '_sample_weights')\n+                        sample_weight_modes.append('temporal')\n+                    else:\n+                        weight = K.placeholder(ndim=1,\n+                                               name=name + '_sample_weights')\n+                        sample_weight_modes.append(None)\n+                sample_weights.append(weight)\n+        elif isinstance(sample_weight_mode, list):\n+            if len(sample_weight_mode) != len(self.outputs):\n+                raise ValueError('When passing a list as sample_weight_mode, '\n+                                 'it should have one entry per model outputs. '\n+                                 'The model has ' + str(len(self.outputs)) +\n+                                 ' outputs, but you passed '\n+                                 'sample_weight_mode=' +\n+                                 str(sample_weight_mode))\n+            for i in range(len(self.output_names)):\n+                if i in skip_indices:\n+                    weight = None\n+                    sample_weight_modes.append(None)\n+                else:\n+                    mode = sample_weight_mode[i]\n+                    name = self.output_names[i]\n+                    if mode == 'temporal':\n+                        weight = K.placeholder(ndim=2,\n+                                               name=name + '_sample_weights')\n+                        sample_weight_modes.append('temporal')\n+                    else:\n+                        weight = K.placeholder(ndim=1,\n+                                               name=name + '_sample_weights')\n+                        sample_weight_modes.append(None)\n+                sample_weights.append(weight)\n+        else:\n+            for i, name in enumerate(self.output_names):\n+                if i in skip_indices:\n+                    sample_weight_modes.append(None)\n+                    sample_weights.append(None)\n+                else:\n+                    if sample_weight_mode == 'temporal':\n+                        sample_weights.append(\n+                            K.placeholder(ndim=2,\n+                                          name=name + '_sample_weights'))\n+                        sample_weight_modes.append('temporal')\n+                    else:\n+                        sample_weights.append(\n+                            K.placeholder(ndim=1,\n+                                          name=name + '_sample_weights'))\n+                        sample_weight_modes.append(None)\n+        self.sample_weight_modes = sample_weight_modes\n+        self._feed_sample_weight_modes = []\n+        for i in range(len(self.outputs)):\n+            if i not in skip_indices:\n+                self._feed_sample_weight_modes.append(self.sample_weight_modes[i])\n+        return sample_weights, sample_weight_mode\n+\n+    def _make_ins(self, x=None, y=None, sample_weights=None, learning_phase_value=None):\n+        \"\"\"Create the list of inputs to the backend in a consistent manner.\n+\n+        # Arguments\n+            x: Numpy array of input data preprocessed to exclude tensors.\n+            y: Numpy array of target data preprocessed to exclude tensors.\n+            sample_weights: optional arrays of the same length as x, containing\n+               weights to apply to the model's loss for each sample.\n+        \"\"\"\n+        def add_if_valid(x):\n+            \"\"\"Ensures concatenation of inputs results in a list.\n+\n+            Inputs are assembled into a list for execution, but if all\n+            elements are `None`, adding them will result in a runtime\n+            error instead of a list of `None` values. Therefore,\n+            add_if_valid converts `None` values into [`None`],\n+            while leaving valid values alone.\n+            \"\"\"\n+            # In a worst case return an empty list\n+            return [None] if x is None else x\n+        ins = add_if_valid(x) + add_if_valid(y) + add_if_valid(sample_weights)\n+        if learning_phase_value and self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+            ins = ins + add_if_valid(learning_phase_value)\n+        return ins\n+\n+    def _make_function(self, function_name):\n+        \"\"\"Create a backend function based on the current model configuration.\n+\n+        Checks the state of all relevant member variables and inputs based on\n+        the backend function type given by the `function_name` parameter,\n+        and assemble the correct variables for this use. Includes additional\n+        verification steps to prevent user error, such as forgetting to call\n+        `compile()`.\n+\n+        # Arguments\n+            function_name: The name of the backend function type that is needed.\n+                Options are `'predict_function'`, `'train_function'`, and\n+                `'test_function'`.\n+        \"\"\"\n+        if function_name is 'predict_function':\n+            if not hasattr(self, 'predict_function'):\n+                self.predict_function = None\n+            if self.predict_function is None:\n+                if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n+                    inputs = self._feed_inputs + [K.learning_phase()]\n+                else:\n+                    inputs = self._feed_inputs\n+                outputs = self.outputs\n+        elif not hasattr(self, function_name):\n             raise RuntimeError('You must compile your model before using it.')\n-        if self.train_function is None:\n-            inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights\n+\n+        function = getattr(self, function_name)\n+        if function is not None:\n+            return function\n+        elif function_name is not 'predict_function':\n+            inputs = [] + self._feed_inputs\n+            if hasattr(self, '_feed_targets'):\n+                inputs = inputs + self._feed_targets\n+            if (hasattr(self, 'sample_weight_mode') and\n+                    self.sample_weight_mode is not 'disabled'):\n+                inputs = inputs + self._feed_sample_weights\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n-                inputs += [K.learning_phase()]\n+                inputs = inputs + [K.learning_phase()]\n \n+            outputs = [self.total_loss] + self.metrics_tensors\n+\n+        if function_name is 'train_function':\n             training_updates = self.optimizer.get_updates(\n                 params=self._collected_trainable_weights,\n                 loss=self.total_loss)\n             updates = self.updates + training_updates\n-            # Gets loss and metrics. Updates weights at each call.\n-            self.train_function = K.function(inputs,\n-                                             [self.total_loss] + self.metrics_tensors,\n-                                             updates=updates,\n-                                             name='train_function',\n-                                             **self._function_kwargs)\n-\n-    def _make_test_function(self):\n-        if not hasattr(self, 'test_function'):\n-            raise RuntimeError('You must compile your model before using it.')\n-        if self.test_function is None:\n-            inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n-                inputs += [K.learning_phase()]\n-            # Return loss and metrics, no gradient updates.\n-            # Does update the network states.\n-            self.test_function = K.function(inputs,\n-                                            [self.total_loss] + self.metrics_tensors,\n-                                            updates=self.state_updates,\n-                                            name='test_function',\n-                                            **self._function_kwargs)\n-\n-    def _make_predict_function(self):\n-        if not hasattr(self, 'predict_function'):\n-            self.predict_function = None\n-        if self.predict_function is None:\n-            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n-                inputs = self._feed_inputs + [K.learning_phase()]\n-            else:\n-                inputs = self._feed_inputs\n-            # Gets network outputs. Does not update weights.\n-            # Does update the network states.\n-            kwargs = getattr(self, '_function_kwargs', {})\n-            self.predict_function = K.function(inputs,\n-                                               self.outputs,\n-                                               updates=self.state_updates,\n-                                               name='predict_function',\n-                                               **kwargs)\n+        else:\n+            updates = self.state_updates\n+\n+        kwargs = getattr(self, '_function_kwargs', {})\n+        # Gets loss and metrics. Updates weights at each call.\n+        return K.function(inputs,\n+                          outputs,\n+                          updates=updates,\n+                          name=function_name,\n+                          **kwargs)\n+\n+    def _check_for_recompile(self, y):\n+        \"\"\"Calls `_compile()` if new Input Tensors were provided by the user.\n+\n+        Also inserts Input Tensors into `self.target_configuration` and removing\n+        them from `y`, so that the tensors are not incorrectly executed as input\n+        feed values.\n+\n+        # Arguments\n+            y: List or array containing heterogeneous input tensors and numpy arrays.\n+\n+        # Returns\n+\n+        `y` with all entries that are input tensors replaced with `None`.\n+        For example, `[numpy_array, tensor, tensor]`\n+        becomes `[numpy_array, None, None]`.\n+        \"\"\"\n+        if K.is_keras_tensor(y, expect_other_types=True):\n+            self.target_configuration = [y]\n+            y = None\n+            self._compile(*self._saved_args, **self._saved_kwargs)\n+        elif y is not None:\n+            recompile = False\n+            self.target_configuration = []\n+            for i, y_i in enumerate(y):\n+                if K.is_keras_tensor(y_i, expect_other_types=True):\n+                    self.target_configuration.append(y_i)\n+                    y[i] = None\n+                    recompile = True\n+                else:\n+                    self.target_configuration.append(None)\n+\n+            if recompile:\n+                self._compile(*self._saved_args, **self._saved_kwargs)\n+        return y\n \n     def _check_num_samples(self, ins, batch_size=None, steps=None, steps_name='steps'):\n         \"\"\"Determine the number of samples provided for training and evaluation.\n\ndiff --git a/tests/keras/engine/test_training.py b/tests/keras/engine/test_training.py\nindex a17187f5a319..9f906cdaabbc 100644\n--- a/tests/keras/engine/test_training.py\n+++ b/tests/keras/engine/test_training.py\n@@ -35,6 +40,57 @@ def on_epoch_end(self):\n         pass\n \n \n+@keras_test\n+def test_standardize_input_data():\n+    a_np = np.random.random((4, 3))\n+    a_shape = a_np.shape\n+    a_name = 'input_a'\n+    p = K.placeholder(a_shape)\n+    a = Input(shape=a_shape, name='a_name')\n+    x = _standardize_input_data(\n+        [a_np], [a_name], [a_shape],\n+        check_batch_axis=False,\n+        exception_prefix='input',\n+        input_tensors=[p])\n+    x = _standardize_input_data(\n+        [None], [a_name], [a_shape],\n+        exception_prefix='input',\n+        input_tensors=[p])\n+\n+\n+def call_model_methods(model, x, y, batch_size=10,\n+                       epochs=1, steps_per_epoch=None):\n+\n+    # train_on_batch\n+    out = model.train_on_batch(x, y)\n+    out = model.train_on_batch(x, y)\n+\n+    # test_on_batch\n+    out = model.test_on_batch(x, y)\n+\n+    # predict_on_batch\n+    out = model.predict_on_batch(x)\n+\n+    # fit\n+    out = model.fit(x, y, epochs=1, batch_size=batch_size,\n+                    steps_per_epoch=steps_per_epoch)\n+    out = model.fit(x, y, epochs=1, batch_size=batch_size,\n+                    steps_per_epoch=steps_per_epoch)\n+\n+    # evaluate\n+    out = model.evaluate(x, y, batch_size=batch_size,\n+                         steps=steps_per_epoch)\n+    out = model.evaluate(x, y, batch_size=batch_size,\n+                         steps=steps_per_epoch)\n+\n+    # predict\n+    out = model.predict(x, batch_size=batch_size,\n+                        steps=steps_per_epoch)\n+    out = model.predict(x, batch_size=batch_size,\n+                        steps=steps_per_epoch)\n+    return out\n+\n+\n @keras_test\n def test_check_array_lengths():\n     _check_array_lengths(None, None, None)\n@@ -598,35 +618,159 @@ def test_model_with_input_feed_tensor():\n     loss = 'mse'\n     model.compile(optimizer, loss, metrics=['mean_squared_error'])\n \n-    # test train_on_batch\n-    out = model.train_on_batch(None,\n-                               output_a_np)\n-    out = model.train_on_batch(None,\n-                               output_a_np)\n-    out = model.test_on_batch(None,\n-                              output_a_np)\n-    out = model.predict_on_batch(None)\n+    # train_on_batch\n     out = model.train_on_batch([],\n                                output_a_np)\n     out = model.train_on_batch({},\n                                output_a_np)\n \n-    # test fit\n-    out = model.fit(None,\n-                    output_a_np, epochs=1, batch_size=10)\n-    out = model.fit(None,\n-                    output_a_np, epochs=1, batch_size=10)\n+    out = call_model_methods(model, None, output_a_np)\n \n-    # test evaluate\n-    out = model.evaluate(None,\n-                         output_a_np, batch_size=10)\n-    out = model.evaluate(None,\n-                         output_a_np, batch_size=10)\n \n-    # test predict\n-    out = model.predict(None, batch_size=10)\n-    out = model.predict(None, batch_size=10)\n-    assert out.shape == (10, 4)\n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n+@keras_test\n+def test_model_with_input_yield_op():\n+    \"\"\"We test building a model with a RecordInput as input.\n+    We should be able to call fit, evaluate, predict,\n+    by only passing them data for the placeholder inputs\n+    in the model.\n+    \"\"\"\n+    import tensorflow as tf\n+\n+    batch_size = 1\n+    input_rows = 20\n+    cols = 20\n+    depth = 1\n+    classes = 2\n+    sess = K.get_session()\n+\n+    # first batch size 1\n+    img_batch_shape = [batch_size, input_rows, cols, depth]\n+    input_a_np, input_a_tf, output_a_np, output_b_np, output_b_tf = create_tensor_data(\n+        img_batch_shape, classes)\n+\n+    coord = tf.train.Coordinator()\n+    threads = tf.train.start_queue_runners(sess, coord)\n+    # tensor input, numpy output\n+    input_only_tfrecord_model(input_a_tf, output_b_np, img_batch_shape, classes)\n+    # tensor input, tensor output\n+    input_label_tfrecord_model(input_a_tf, output_b_tf, img_batch_shape, classes)\n+\n+    coord.request_stop()\n+    coord.join(threads)\n+    K.clear_session()\n+\n+    sess = K.get_session()\n+    # next batch size 3\n+    batch_size = 3\n+    img_batch_shape = [batch_size, input_rows, cols, depth]\n+    input_a_np, input_a_tf, output_a_np, output_b_np, output_b_tf = create_tensor_data(\n+        img_batch_shape, classes)\n+    coord = tf.train.Coordinator()\n+    threads = tf.train.start_queue_runners(sess, coord)\n+    # tensor input, numpy output\n+    input_only_tfrecord_model(input_a_tf, output_b_np, img_batch_shape, classes)\n+    # tensor input, tensor output\n+    input_label_tfrecord_model(input_a_tf, output_b_tf, img_batch_shape, classes)\n+    coord.request_stop()\n+    coord.join(threads)\n+\n+\n+def cnn_layers(x_train_input, classes):\n+    x = Conv2D(32, (1, 1), activation='relu', padding='valid')(x_train_input)\n+    x = Dropout(0.25)(x)\n+    x = Flatten()(x)\n+    x = Dense(128, activation='relu')(x)\n+    x = Dropout(0.5)(x)\n+    x_train_out = Dense(classes,\n+                        activation='softmax',\n+                        name='x_train_out')(x)\n+    return x_train_out\n+\n+\n+def input_only_tfrecord_model(input_a_tf, output_b_np, img_batch_shape, classes):\n+    a = Input(tensor=input_a_tf, batch_shape=img_batch_shape)\n+    b = cnn_layers(a, classes)\n+    model = Model([a], [b])\n+    model.summary()\n+\n+    optimizer = 'rmsprop'\n+    loss = 'mse'\n+    loss_weights = [1., 2.]\n+    with pytest.raises(ValueError) as exc:\n+        model.compile(optimizer, loss, metrics=['mean_squared_error'],\n+                      loss_weights=loss_weights,\n+                      sample_weight_mode=None)\n+\n+    model.compile(optimizer, loss, metrics=['mean_squared_error'],\n+                  sample_weight_mode=None)\n+\n+    call_model_methods(model, None, output_b_np, batch_size=None, steps_per_epoch=2)\n+\n+\n+def input_label_tfrecord_model(input_a_tf, output_b_tf, img_batch_shape, classes):\n+    a = Input(tensor=input_a_tf, batch_shape=img_batch_shape)\n+    b = cnn_layers(a, classes)\n+    y_train_in_out = Input(\n+        tensor=output_b_tf,\n+        batch_shape=[img_batch_shape[0], classes],\n+        name='y_labels')\n+    b = cnn_layers(a, classes)\n+    cce = K.categorical_crossentropy(b, y_train_in_out)\n+    model = Model(inputs=[a], outputs=[b])\n+    model.add_loss(cce)\n+    model.summary()\n+\n+    optimizer = 'rmsprop'\n+    loss = None\n+    loss_weights = [1., 2.]\n+    with pytest.raises(ValueError) as exc:\n+        model.compile(optimizer, loss, metrics=['mean_squared_error'],\n+                      loss_weights=loss_weights)\n+\n+    model.compile(optimizer, loss, metrics=['mean_squared_error'])\n+\n+    call_model_methods(model, None, None,\n+                       batch_size=None, steps_per_epoch=1)\n+\n+    model.compile(optimizer,\n+                  loss='categorical_crossentropy',\n+                  metrics=['mean_squared_error'])\n+\n+    call_model_methods(model, None, y_train_in_out,\n+                       batch_size=None, steps_per_epoch=1)\n+\n+\n+def create_tensor_data(img_batch_shape, classes, batch_count=1):\n+    import tensorflow as tf\n+    [batch_size, input_rows, cols, depth] = img_batch_shape\n+    img_count = batch_size * batch_count\n+    imgs_shape = [img_count, input_rows, cols, depth]\n+    labels_shape = [img_count, classes]\n+    input_a_np = np.multiply(np.random.random(imgs_shape), batch_size)\n+    input_a_np = input_a_np.astype('uint8')\n+    output_a_np = np.multiply(np.random.random([img_count]), classes)\n+    output_a_np = output_a_np.astype('int')\n+    output_b_np = np.random.random(labels_shape)\n+\n+    from tensorflow.contrib.learn.python.learn.datasets import mnist\n+\n+    input_a_tf, output_b_tf = tf.train.shuffle_batch(\n+        tensors=[input_a_np, output_a_np.astype(np.int32)],\n+        batch_size=batch_size,\n+        capacity=batch_size * 2,\n+        min_after_dequeue=1,\n+        enqueue_many=True,\n+        num_threads=1,\n+        name='create_tensor_data_shuffle_batch')\n+\n+    input_a_tf = tf.cast(input_a_tf, tf.float32, name='input_a_tf_cast')\n+    input_a_tf = tf.reshape(input_a_tf, shape=img_batch_shape, name='input_a_tf_reshape')\n+\n+    output_b_tf = tf.cast(output_a_np, tf.int32, name='output_b_tf_cast')\n+    output_b_tf = tf.one_hot(output_b_tf, classes, name='output_b_tf_one_hot')\n+\n+    return input_a_np, input_a_tf, output_a_np, output_b_np, output_b_tf\n \n \n @keras_test\n\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The proposed PR aims to add TFRecords support in the `model.fit()` API, which is beneficial for users needing efficient data handling with TensorFlow operations. However, the implementation suggests modifying `model.fit()` to accept tensors directly, which complicates the API without clear advantages. Using placeholders might be a better option for more generic use cases. Consider focusing on utility improvements and maintaining backward compatibility while exploring alternative approaches, like a dedicated layer for targets, rather than introducing potentially confusing modifications."
      }
    ],
    "codediff_length": 30809
  },
  {
    "index": 35,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/keras/applications/mobilenet.py b/keras/applications/mobilenet.py\nnew file mode 100644\nindex 000000000000..f6631214186f\n--- /dev/null\n+++ b/keras/applications/mobilenet.py\n@@ -0,0 +1,616 @@\n+\"\"\"MobileNet v1 models for Keras.\n+\n+MobileNet is a general architecture and can be used for multiple use cases.\n+Depending on the use case, it can use different input layer size and\n+different width factors. This allows different width models to reduce\n+the number of multiply-adds and thereby reduce inference cost on mobile devices.\n+\n+MobileNets support any input size greater than 32 x 32, with larger image sizes\n+offering better performance. The number of parameters and number of multiply-adds\n+can be modified by using the `alpha` parameter, which increases/decreases the number\n+of filters in each layer. By altering the image size and `alpha` parameter,\n+all 16 models from the paper can be built, with ImageNet weights provided.\n+\n+The paper demonstrates the performance of MobileNets using `alpha` values of\n+1.0 (also called 100 % MobileNet), 0.75, 0.5 and 0.25. For each of these `alpha`\n+values, weights for 4 different input image sizes are provided (224, 192, 160, 128).\n+\n+The following table describes the size and accuracy of the 100% MobileNet on size 224 x 224\n+-----------------------------------------------------------------------------------------\n+Width Multiplier (alpha) | ImageNet Accuracy | Million Multiply-Adds | Million Parameters\n+-----------------------------------------------------------------------------------------\n+|   1.0 MobileNet-224    |      70.6 %       |          529          |        4.2       |\n+|   0.75 MobileNet-224   |      68.4 %       |          325          |        2.6       |\n+|   0.50 MobileNet-224   |      63.7 %       |          149          |        1.3       |\n+|   0.25 MobileNet-224   |      50.6 %       |          41           |        0.5       |\n+-----------------------------------------------------------------------------------------\n+\n+The following table describes the performance of the 100 % MobileNet on various input sizes\n+-----------------------------------------------------------------------------------------\n+        Resolution       | ImageNet Accuracy | Million Multiply-Adds | Million Parameters\n+-----------------------------------------------------------------------------------------\n+|   1.0 MobileNet-224    |      70.6 %       |          529          |        4.2       |\n+|   1.0 MobileNet-192    |      69.1 %       |          529          |        4.2       |\n+|   1.0 MobileNet-160    |      67.2 %       |          529          |        4.2       |\n+|   1.0 MobileNet-128    |      64.4 %       |          529          |        4.2       |\n+-----------------------------------------------------------------------------------------\n+\n+The weights for all 16 models are obtained and translated from Tensorflow checkpoints\n+found in https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md\n+\n+# Reference\n+- [MobileNets: Efficient Convolutional Neural Networks for\n+   Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf))\n+\"\"\"\n+from __future__ import print_function\n+from __future__ import absolute_import\n+from __future__ import division\n+\n+import warnings\n+\n+from ..models import Model\n+from ..layers import Input\n+from ..layers import Activation\n+from ..layers import Dropout\n+from ..layers import Reshape\n+from ..layers import BatchNormalization\n+from ..layers import Convolution2D\n+from ..layers import GlobalAveragePooling2D\n+from ..layers import GlobalMaxPooling2D\n+from ..layers import Conv2D\n+from .. import initializers\n+from .. import regularizers\n+from .. import constraints\n+from ..utils import conv_utils\n+from ..utils.data_utils import get_file\n+from ..engine.topology import get_source_inputs\n+from ..engine import InputSpec\n+from ..applications.imagenet_utils import _obtain_input_shape\n+from ..applications.imagenet_utils import preprocess_input\n+from ..applications.imagenet_utils import decode_predictions\n+from .. import backend as K\n+\n+\n+BASE_WEIGHT_PATH = 'https://github.com/titu1994/MobileNetworks/releases/download/v1.0/'\n+\n+\n+def relu6(x):\n+    return K.relu(x, max_value=6)\n+\n+\n+class DepthwiseConv2D(Conv2D):\n+    \"\"\"Depthwise separable 2D convolution.\n+    Depthwise Separable convolutions consists in performing\n+    just the first step in a depthwise spatial convolution\n+    (which acts on each input channel separately).\n+    The `depth_multiplier` argument controls how many\n+    output channels are generated per input channel in the depthwise step.\n+\n+    # Arguments\n+        kernel_size: An integer or tuple/list of 2 integers, specifying the\n+            width and height of the 2D convolution window.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+        strides: An integer or tuple/list of 2 integers,\n+            specifying the strides of the convolution along the width and height.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+            Specifying any stride value != 1 is incompatible with specifying\n+            any `dilation_rate` value != 1.\n+        padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+        depth_multiplier: The number of depthwise convolution output channels\n+            for each input channel.\n+            The total number of depthwise convolution output\n+            channels will be equal to `filters_in * depth_multiplier`.\n+        data_format: A string,\n+            one of `channels_last` (default) or `channels_first`.\n+            The ordering of the dimensions in the inputs.\n+            `channels_last` corresponds to inputs with shape\n+            `(batch, height, width, channels)` while `channels_first`\n+            corresponds to inputs with shape\n+            `(batch, channels, height, width)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        depthwise_initializer: Initializer for the depthwise kernel matrix\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        depthwise_regularizer: Regularizer function applied to\n+            the depthwise kernel matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        depthwise_constraint: Constraint function applied to\n+            the depthwise kernel matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+\n+    # Input shape\n+        4D tensor with shape:\n+        `[batch, channels, rows, cols]` if data_format='channels_first'\n+        or 4D tensor with shape:\n+        `[batch, rows, cols, channels]` if data_format='channels_last'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `[batch, filters, new_rows, new_cols]` if data_format='channels_first'\n+        or 4D tensor with shape:\n+        `[batch, new_rows, new_cols, filters]` if data_format='channels_last'.\n+        `rows` and `cols` values might have changed due to padding.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 kernel_size,\n+                 strides=(1, 1),\n+                 padding='valid',\n+                 depth_multiplier=1,\n+                 data_format=None,\n+                 activation=None,\n+                 use_bias=True,\n+                 depthwise_initializer='glorot_uniform',\n+                 bias_initializer='zeros',\n+                 depthwise_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 depthwise_constraint=None,\n+                 bias_constraint=None,\n+                 **kwargs):\n+        super(DepthwiseConv2D, self).__init__(\n+            filters=None,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            activation=activation,\n+            use_bias=use_bias,\n+            bias_regularizer=bias_regularizer,\n+            activity_regularizer=activity_regularizer,\n+            bias_constraint=bias_constraint,\n+            **kwargs)\n+        self.depth_multiplier = depth_multiplier\n+        self.depthwise_initializer = initializers.get(depthwise_initializer)\n+        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n+        self.depthwise_constraint = constraints.get(depthwise_constraint)\n+        self.bias_initializer = initializers.get(bias_initializer)\n+\n+    def build(self, input_shape):\n+        if len(input_shape) < 4:\n+            raise ValueError('Inputs to `DepthwiseConv2D` should have rank 4. '\n+                             'Received input shape:', str(input_shape))\n+        if self.data_format == 'channels_first':\n+            channel_axis = 1\n+        else:\n+            channel_axis = 3\n+        if input_shape[channel_axis] is None:\n+            raise ValueError('The channel dimension of the inputs to '\n+                             '`DepthwiseConv2D` '\n+                             'should be defined. Found `None`.')\n+        input_dim = int(input_shape[channel_axis])\n+        depthwise_kernel_shape = (self.kernel_size[0],\n+                                  self.kernel_size[1],\n+                                  input_dim,\n+                                  self.depth_multiplier)\n+\n+        self.depthwise_kernel = self.add_weight(\n+            shape=depthwise_kernel_shape,\n+            initializer=self.depthwise_initializer,\n+            name='depthwise_kernel',\n+            regularizer=self.depthwise_regularizer,\n+            constraint=self.depthwise_constraint)\n+\n+        if self.use_bias:\n+            self.bias = self.add_weight(shape=(input_dim * self.depth_multiplier,),\n+                                        initializer=self.bias_initializer,\n+                                        name='bias',\n+                                        regularizer=self.bias_regularizer,\n+                                        constraint=self.bias_constraint)\n+        else:\n+            self.bias = None\n+        # Set input spec.\n+        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n+        self.built = True\n+\n+    def call(self, inputs, training=None):\n+        outputs = K.depthwise_conv2d(\n+            inputs,\n+            self.depthwise_kernel,\n+            strides=self.strides,\n+            padding=self.padding,\n+            dilation_rate=self.dilation_rate,\n+            data_format=self.data_format)\n+\n+        if self.bias:\n+            outputs = K.bias_add(\n+                outputs,\n+                self.bias,\n+                data_format=self.data_format)\n+\n+        if self.activation is not None:\n+            return self.activation(outputs)\n+\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        if self.data_format == 'channels_first':\n+            rows = input_shape[2]\n+            cols = input_shape[3]\n+            out_filters = input_shape[1] * self.depth_multiplier\n+        elif self.data_format == 'channels_last':\n+            rows = input_shape[1]\n+            cols = input_shape[2]\n+            out_filters = input_shape[3] * self.depth_multiplier\n+\n+        rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n+                                             self.padding,\n+                                             self.strides[0])\n+        cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\n+                                             self.padding,\n+                                             self.strides[1])\n+\n+        if self.data_format == 'channels_first':\n+            return (input_shape[0], out_filters, rows, cols)\n+        elif self.data_format == 'channels_last':\n+            return (input_shape[0], rows, cols, out_filters)\n+\n+    def get_config(self):\n+        config = super(DepthwiseConv2D, self).get_config()\n+        config.pop('filters')\n+        config.pop('kernel_initializer')\n+        config.pop('kernel_regularizer')\n+        config.pop('kernel_constraint')\n+        config['depth_multiplier'] = self.depth_multiplier\n+        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n+        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n+        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n+        return config\n+\n+\n+def MobileNet(input_shape=None,\n+              alpha=1.0,\n+              depth_multiplier=1,\n+              dropout=1e-3,\n+              include_top=True,\n+              weights='imagenet',\n+              input_tensor=None,\n+              pooling=None,\n+              classes=1000):\n+    \"\"\"\n+    Instantiate the MobileNet architecture.\n+    Note that only TensorFlow is supported for now,\n+    therefore it only works with the data format\n+    `image_data_format='channels_last'` in your Keras config\n+    at `~/.keras/keras.json`.\n+\n+    To load a MobileNet model via `load_model`, import the custom\n+    objects `relu6` and `DepthwiseConv2D` and pass them to the\n+    `custom_objects` parameter.\n+    E.g.\n+    model = load_model('mobilenet.h5', custom_objects={\n+                       'relu6': mobilenet.relu6,\n+                       'DepthwiseConv2D': mobilenet.DepthwiseConv2D})\n+\n+    # Arguments\n+        input_shape: optional shape tuple, only to be specified\n+            if `include_top` is False (otherwise the input shape\n+            has to be `(224, 224, 3)` (with `channels_last` data format)\n+            or (3, 224, 224) (with `channels_first` data format).\n+            It should have exactly 3 inputs channels,\n+            and width and height should be no smaller than 32.\n+            E.g. `(200, 200, 3)` would be one valid value.\n+        alpha: controls the width of the network.\n+            - If `alpha` < 1.0, proportionally decreases the number\n+                of filters in each layer.\n+            - If `alpha` > 1.0, proportionally increases the number\n+                of filters in each layer.\n+            - If `alpha` = 1, default number of filters from the paper\n+                 are used at each layer.\n+        depth_multiplier: depth multiplier for depthwise convolution\n+            (also called the resolution multiplier)\n+        dropout: dropout rate\n+        include_top: whether to include the fully-connected\n+            layer at the top of the network.\n+        weights: `None` (random initialization) or\n+            `imagenet` (ImageNet weights)\n+        input_tensor: optional Keras tensor (i.e. output of\n+            `layers.Input()`)\n+            to use as image input for the model.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model\n+                will be the 4D tensor output of the\n+                last convolutional layer.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional layer, and thus\n+                the output of the model will be a\n+                2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n+        classes: optional number of classes to classify images\n+            into, only to be specified if `include_top` is True, and\n+            if no `weights` argument is specified.\n+\n+    # Returns\n+        A Keras model instance.\n+\n+    # Raises\n+        ValueError: in case of invalid argument for `weights`,\n+            or invalid input shape.\n+        RuntimeError: If attempting to run this model with a\n+            backend that does not support separable convolutions.\n+    \"\"\"\n+\n+    if K.backend() != 'tensorflow':\n+        raise RuntimeError('Only Tensorflow backend is currently supported, '\n+                           'as other backends do not support depthwise convolution.')\n+\n+    if weights not in {'imagenet', None}:\n+        raise ValueError('The `weights` argument should be either '\n+                         '`None` (random initialization) or `imagenet` '\n+                         '(pre-training on ImageNet).')\n+\n+    if weights == 'imagenet' and include_top and classes != 1000:\n+        raise ValueError('If using `weights` as ImageNet with `include_top`'\n+                         ' as true, `classes` should be 1000')\n+\n+    if weights == 'imagenet' and depth_multiplier != 1:\n+        if depth_multiplier != 1:\n+            raise ValueError('If imagenet weights are being loaded, '\n+                             'depth multiplier must be 1')\n+\n+        if alpha not in [0.25, 0.50, 0.75, 1.0]:\n+            raise ValueError('If imagenet weights are being loaded, alpha can be one of'\n+                             '`0.25`, `0.50`, `0.75` or `1.0` only.')\n+\n+        rows, cols = (0, 1) if K.image_data_format() == 'channels_last' else (1, 2)\n+        rows = int(input_shape[rows])\n+        cols = int(input_shape[cols])\n+\n+        if rows != cols or rows not in [128, 160, 192, 224]:\n+            raise ValueError('If imagenet weights are being loaded,'\n+                             'image must have a square shape (one of '\n+                             '(128,128), (160,160), (192,192), or (224, 224)).'\n+                             'Image shape provided = (%d, %d)' % (rows, cols))\n+\n+    if K.image_data_format() != 'channels_last':\n+        warnings.warn('The MobileNet family of models is only available '\n+                      'for the input data format \"channels_last\" '\n+                      '(width, height, channels). '\n+                      'However your settings specify the default '\n+                      'data format \"channels_first\" (channels, width, height). '\n+                      'You should set `image_data_format=\"channels_last\"` in your Keras '\n+                      'config located at ~/.keras/keras.json. '\n+                      'The model being returned right now will expect inputs '\n+                      'to follow the \"channels_last\" data format.')\n+        K.set_image_data_format('channels_last')\n+        old_data_format = 'channels_first'\n+    else:\n+        old_data_format = None\n+\n+    # Determine proper input shape. Note, include_top is False by default, as\n+    # input shape can be anything larger than 32x32 and the same number of parameters\n+    # will be used.\n+    input_shape = _obtain_input_shape(input_shape,\n+                                      default_size=224,\n+                                      min_size=32,\n+                                      data_format=K.image_data_format(),\n+                                      include_top=False)\n+\n+    if input_tensor is None:\n+        img_input = Input(shape=input_shape)\n+    else:\n+        if not K.is_keras_tensor(input_tensor):\n+            img_input = Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    x = _conv_block(img_input, 32, alpha, strides=(2, 2))\n+    x = _depthwise_conv_block(x, 64, alpha, depth_multiplier, id=1)\n+\n+    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, strides=(2, 2), id=2)\n+    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, id=3)\n+\n+    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, strides=(2, 2), id=4)\n+    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, id=5)\n+\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, strides=(2, 2), id=6)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=7)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=8)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=9)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=10)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=11)\n+\n+    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, strides=(2, 2), id=12)\n+    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, id=13)\n+\n+    if include_top:\n+        if K.image_data_format() == 'channels_first':\n+            shape = (int(1024 * alpha), 1, 1)\n+        else:\n+            shape = (1, 1, int(1024 * alpha))\n+\n+        x = GlobalAveragePooling2D()(x)\n+        x = Reshape(shape, name='reshape_1')(x)\n+        x = Dropout(dropout, name='dropout')(x)\n+        x = Convolution2D(classes, (1, 1), padding='same', name='conv_preds')(x)\n+        x = Activation('softmax', name='act_softmax')(x)\n+        x = Reshape((classes,), name='reshape_2')(x)\n+    else:\n+        if pooling == 'avg':\n+            x = GlobalAveragePooling2D()(x)\n+        elif pooling == 'max':\n+            x = GlobalMaxPooling2D()(x)\n+\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    rows, cols = (0, 1) if K.image_data_format() == 'channels_last' else (1, 2)\n+    if input_shape[rows] is None:\n+        size = 'None'\n+    else:\n+        size = str(input_shape[rows])\n+\n+    # Create model.\n+    model = Model(inputs, x, name='mobilenet_%0.2f_%s' % (alpha, size))\n+\n+    # load weights\n+    if weights == 'imagenet':\n+        if K.image_data_format() == 'channels_first':\n+            raise AttributeError('Weights for Channels Last format are not available')\n+\n+        if alpha == 1.0:\n+            alpha_text = '1_0'\n+        elif alpha == 0.75:\n+            alpha_text = '7_5'\n+        elif alpha == 0.50:\n+            alpha_text = '5_0'\n+        else:\n+            alpha_text = '2_5'\n+\n+        if include_top:\n+            model_name = 'mobilenet_%s_%d_tf.h5' % (alpha_text, rows)\n+            weigh_path = BASE_WEIGHT_PATH + model_name\n+            weights_path = get_file(model_name,\n+                                    weigh_path,\n+                                    cache_subdir='models')\n+        else:\n+            model_name = 'mobilenet_%s_%d_tf_no_top.h5' % (alpha_text, rows)\n+            weigh_path = BASE_WEIGHT_PATH + model_name\n+            weights_path = get_file(model_name,\n+                                    weigh_path,\n+                                    cache_subdir='models')\n+\n+        model.load_weights(weights_path)\n+\n+    if old_data_format:\n+        K.set_image_data_format(old_data_format)\n+\n+    return model\n+\n+\n+def _conv_block(input, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n+    \"\"\"Adds an initial convolution layer (with batch normalization and relu6).\n+\n+    # Arguments\n+        input: Input tensor of shape `(rows, cols, 3)`\n+            (with `channels_last` data format) or\n+            (3, rows, cols) (with `channels_first` data format).\n+            It should have exactly 3 inputs channels,\n+            and width and height should be no smaller than 32.\n+            E.g. `(224, 224, 3)` would be one valid value.\n+        filters: Integer, the dimensionality of the output space\n+            (i.e. the number output of filters in the convolution).\n+        alpha: controls the width of the network.\n+            - If `alpha` < 1.0, proportionally decreases the number\n+                of filters in each layer.\n+            - If `alpha` > 1.0, proportionally increases the number\n+                of filters in each layer.\n+            - If `alpha` = 1, default number of filters from the paper\n+                 are used at each layer.\n+        kernel: An integer or tuple/list of 2 integers, specifying the\n+            width and height of the 2D convolution window.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+        strides: An integer or tuple/list of 2 integers,\n+            specifying the strides of the convolution along the width and height.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+            Specifying any stride value != 1 is incompatible with specifying\n+            any `dilation_rate` value != 1.\n+\n+    # Input shape\n+        4D tensor with shape:\n+        `(samples, channels, rows, cols)` if data_format='channels_first'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if data_format='channels_last'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n+        or 4D tensor with shape:\n+        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n+        `rows` and `cols` values might have changed due to stride.\n+    \"\"\"\n+    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n+    filters = int(filters * alpha)\n+\n+    x = Convolution2D(filters, kernel, padding='same', use_bias=False, strides=strides,\n+                      name='conv1')(input)\n+    x = BatchNormalization(axis=channel_axis, name='conv1_bn')(x)\n+    x = Activation(relu6, name='conv1_relu')(x)\n+\n+    return x\n+\n+\n+def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n+                          depth_multiplier=1, strides=(1, 1), id=1):\n+    \"\"\"\n+    Adds a depthwise convolution block (depthwise conv, batch normalization, relu6,\n+    pointwise convolution, batch normalization and relu6).\n+\n+    # Arguments\n+        input: Input tensor of shape `(rows, cols, channels)`\n+            (with `channels_last` data format) or\n+            (channels, rows, cols) (with `channels_first` data format).\n+        pointwise_conv_filters: Integer, the dimensionality of the output space\n+            (i.e. the number output of filters in the pointwise convolution).\n+        alpha: controls the width of the network.\n+            - If `alpha` < 1.0, proportionally decreases the number\n+                of filters in each layer.\n+            - If `alpha` > 1.0, proportionally increases the number\n+                of filters in each layer.\n+            - If `alpha` = 1, default number of filters from the paper\n+                 are used at each layer.\n+        depth_multiplier: The number of depthwise convolution output channels\n+            for each input channel.\n+            The total number of depthwise convolution output\n+            channels will be equal to `filters_in * depth_multiplier`.\n+        strides: An integer or tuple/list of 2 integers,\n+            specifying the strides of the convolution along the width and height.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+            Specifying any stride value != 1 is incompatible with specifying\n+            any `dilation_rate` value != 1.\n+        id: Integer, a unique identification designating  the block number.\n+\n+    # Input shape\n+        4D tensor with shape:\n+        `(batch, channels, rows, cols)` if data_format='channels_first'\n+        or 4D tensor with shape:\n+        `(batch, rows, cols, channels)` if data_format='channels_last'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `(batch, filters, new_rows, new_cols)` if data_format='channels_first'\n+        or 4D tensor with shape:\n+        `(batch, new_rows, new_cols, filters)` if data_format='channels_last'.\n+        `rows` and `cols` values might have changed due to stride.\n+    \"\"\"\n+    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n+    pointwise_conv_filters = int(pointwise_conv_filters * alpha)\n+\n+    x = DepthwiseConv2D(kernel_size=(3, 3), padding='same', depth_multiplier=depth_multiplier,\n+                        strides=strides, use_bias=False, name='conv_dw_%d' % id)(input)\n+    x = BatchNormalization(axis=channel_axis, name='conv_dw_%d_bn' % id)(x)\n+    x = Activation(relu6, name='conv_dw_%d_relu' % id)(x)\n+\n+    x = Convolution2D(pointwise_conv_filters, (1, 1), padding='same', use_bias=False, strides=(1, 1),\n+                      name='conv_pw_%d' % id)(x)\n+    x = BatchNormalization(axis=channel_axis, name='conv_pw_%d_bn' % id)(x)\n+    x = Activation(relu6, name='conv_pw_%d_relu' % id)(x)\n+\n+    return x\ndiff --git a/tests/keras/applications/applications_test.py b/tests/keras/applications/applications_test.py\nindex fc432762b258..ab3585c0cf6e 100644\n--- a/tests/keras/applications/applications_test.py\n+++ b/tests/keras/applications/applications_test.py\n@@ -166,5 +169,73 @@ def test_inceptionv3_pooling():\n     assert model.output_shape == (None, 2048)\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason=\"MobileNets are supported only on Tensorflow\")\n+def test_mobilenet():\n+    model = applications.MobileNet(weights=None)\n+    assert model.output_shape == (None, 1000)\n+\n+\n+@keras_test\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason=\"MobileNets are supported only on Tensorflow\")\n+def test_mobilenet_no_top():\n+    model = applications.MobileNet(weights=None, include_top=False)\n+    assert model.output_shape == (None, None, None, 1024)\n+\n+\n+@keras_test\n+@pytest.mark.skipif((K.backend() != 'tensorflow'),\n+                    reason=\"MobileNets are supported only on Tensorflow\")\n+def test_mobilenet_pooling():\n+    model = applications.MobileNet(weights=None, include_top=False, pooling='avg')\n+    assert model.output_shape == (None, 1024)\n+\n+\n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n+@keras_test\n+def test_depthwise_conv_2d():\n+    _convolution_paddings = ['valid', 'same']\n+    num_samples = 2\n+    stack_size = 3\n+    num_row = 7\n+    num_col = 6\n+\n+    with CustomObjectScope({'relu6': applications.mobilenet.relu6,\n+                            'DepthwiseConv2D': applications.mobilenet.DepthwiseConv2D}):\n+        for padding in _convolution_paddings:\n+            for strides in [(1, 1), (2, 2)]:\n+                for multiplier in [1, 2]:\n+                    if padding == 'same' and strides != (1, 1):\n+                        continue\n+\n+                    layer_test(applications.mobilenet.DepthwiseConv2D,\n+                               kwargs={'kernel_size': (3, 3),\n+                                       'padding': padding,\n+                                       'strides': strides,\n+                                       'depth_multiplier': multiplier},\n+                               input_shape=(num_samples, num_row, num_col, stack_size))\n+\n+        layer_test(applications.mobilenet.DepthwiseConv2D,\n+                   kwargs={'kernel_size': 3,\n+                           'padding': padding,\n+                           'data_format': 'channels_first',\n+                           'activation': None,\n+                           'depthwise_regularizer': 'l2',\n+                           'bias_regularizer': 'l2',\n+                           'activity_regularizer': 'l2',\n+                           'depthwise_constraint': 'unit_norm',\n+                           'strides': strides,\n+                           'depth_multiplier': multiplier},\n+                   input_shape=(num_samples, stack_size, num_row, num_col))\n+\n+        # Test invalid use case\n+        with pytest.raises(ValueError):\n+            model = Sequential([applications.mobilenet.DepthwiseConv2D(kernel_size=3,\n+                                                                       padding=padding,\n+                                                                       batch_input_shape=(None, None, 5, None))])\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR has build errors related to `TimeDistributed` in CNTK and Theano, along with a pip install issue for integration tests, but the model tests pass. While youve made efforts to conform to Keras documentation, the private model-building functions deviate slightly. The inclusion of `relu6` in Keras might introduce unnecessary complexity since it could be defined locally. Modifying the tests to correctly handle custom objects would be beneficial. Overall, the code structure appears solid, but ensure thorough comparison with the source paper to validate implementations."
      }
    ],
    "codediff_length": 32381
  },
  {
    "index": 36,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/keras/applications/regnet.py b/keras/applications/regnet.py\nnew file mode 100644\nindex 000000000000..213a58f09d09\n--- /dev/null\n+++ b/keras/applications/regnet.py\n@@ -0,0 +1,1549 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+# pylint: disable=invalid-name\n+# pylint: disable=missing-docstring\n+\"\"\"RegNet models for Keras.\n+\n+References:\n+\n+- [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n+  (CVPR 2020)\n+- [Fast and Accurate Model Scaling](https://arxiv.org/abs/2103.06877)\n+  (CVPR 2021)\n+\"\"\"\n+\n+import tensorflow as tf\n+\n+from keras import backend\n+from keras import layers\n+from keras.applications import imagenet_utils\n+from keras.engine import training\n+from keras.utils import layer_utils\n+from keras.utils import data_utils\n+from tensorflow.python.util.tf_export import keras_export\n+\n+BASE_WEIGHTS_PATH = \"\"\n+\n+WEIGHTS_HASHES = {\n+    \"x002\": (\"\", \"\"),\n+    \"x004\": (\"\", \"\"),\n+    \"x006\": (\"\", \"\"),\n+    \"x008\": (\"\", \"\"),\n+    \"x016\": (\"\", \"\"),\n+    \"x032\": (\"\", \"\"),\n+    \"x040\": (\"\", \"\"),\n+    \"x064\": (\"\", \"\"),\n+    \"x080\": (\"\", \"\"),\n+    \"x120\": (\"\", \"\"),\n+    \"x160\": (\"\", \"\"),\n+    \"x320\": (\"\", \"\"),\n+    \"y002\": (\"\", \"\"),\n+    \"y004\": (\"\", \"\"),\n+    \"y006\": (\"\", \"\"),\n+    \"y008\": (\"\", \"\"),\n+    \"y016\": (\"\", \"\"),\n+    \"y032\": (\"\", \"\"),\n+    \"y040\": (\"\", \"\"),\n+    \"y064\": (\"\", \"\"),\n+    \"y080\": (\"\", \"\"),\n+    \"y120\": (\"\", \"\"),\n+    \"y160\": (\"\", \"\"),\n+    \"y320\": (\"\", \"\")\n+}\n+\n+# The widths and depths are deduced from a quantized linear function. For\n+# more information, please refer to \"Designing Network Design Spaces\" by\n+# Radosavovic et al.\n+\n+# BatchNorm momentum and epsilon values taken from original implementation.\n+\n+MODEL_CONFIGS = {\n+    \"x002\": {\n+        \"depths\": [1, 1, 4, 7],\n+        \"widths\": [24, 56, 152, 368],\n+        \"group_width\": 8,\n+        \"default_size\": 224\n+    },\n+    \"x004\": {\n+        \"depths\": [1, 2, 7, 12],\n+        \"widths\": [32, 64, 160, 384],\n+        \"group_width\": 16,\n+        \"default_size\": 224\n+    },\n+    \"x006\": {\n+        \"depths\": [1, 3, 5, 7],\n+        \"widths\": [48, 96, 240, 528],\n+        \"group_width\": 24,\n+        \"default_size\": 224\n+    },\n+    \"x008\": {\n+        \"depths\": [1, 3, 7, 5],\n+        \"widths\": [64, 128, 288, 672],\n+        \"group_width\": 16,\n+        \"default_size\": 224\n+    },\n+    \"x016\": {\n+        \"depths\": [2, 4, 10, 2],\n+        \"widths\": [72, 168, 408, 912],\n+        \"group_width\": 24,\n+        \"default_size\": 224\n+    },\n+    \"x032\": {\n+        \"depths\": [2, 6, 15, 2],\n+        \"widths\": [96, 192, 432, 1008],\n+        \"group_width\": 48,\n+        \"default_size\": 224\n+    },\n+    \"x040\": {\n+        \"depths\": [2, 5, 14, 2],\n+        \"widths\": [80, 240, 560, 1360],\n+        \"group_width\": 40,\n+        \"default_size\": 224\n+    },\n+    \"x064\": {\n+        \"depths\": [2, 4, 10, 1],\n+        \"widths\": [168, 392, 784, 1624],\n+        \"group_width\": 56,\n+        \"default_size\": 224\n+    },\n+    \"x080\": {\n+        \"depths\": [2, 5, 15, 1],\n+        \"widths\": [80, 240, 720, 1920],\n+        \"group_width\": 120,\n+        \"default_size\": 224\n+    },\n+    \"x120\": {\n+        \"depths\": [2, 5, 11, 1],\n+        \"widths\": [224, 448, 896, 2240],\n+        \"group_width\": 112,\n+        \"default_size\": 224\n+    },\n+    \"x160\": {\n+        \"depths\": [2, 6, 13, 1],\n+        \"widths\": [256, 512, 896, 2048],\n+        \"group_width\": 128,\n+        \"default_size\": 224\n+    },\n+    \"x320\": {\n+        \"depths\": [2, 7, 13, 1],\n+        \"widths\": [336, 672, 1344, 2520],\n+        \"group_width\": 168,\n+        \"default_size\": 224\n+    },\n+    \"y002\": {\n+        \"depths\": [1, 1, 4, 7],\n+        \"widths\": [24, 56, 152, 368],\n+        \"group_width\": 8,\n+        \"default_size\": 224\n+    },\n+    \"y004\": {\n+        \"depths\": [1, 3, 6, 6],\n+        \"widths\": [48, 104, 208, 440],\n+        \"group_width\": 8,\n+        \"default_size\": 224\n+    },\n+    \"y006\": {\n+        \"depths\": [1, 3, 7, 4],\n+        \"widths\": [48, 112, 256, 608],\n+        \"group_width\": 16,\n+        \"default_size\": 224\n+    },\n+    \"y008\": {\n+        \"depths\": [1, 3, 8, 2],\n+        \"widths\": [64, 128, 320, 768],\n+        \"group_width\": 16,\n+        \"default_size\": 224\n+    },\n+    \"y016\": {\n+        \"depths\": [2, 6, 17, 2],\n+        \"widths\": [48, 120, 336, 888],\n+        \"group_width\": 24,\n+        \"default_size\": 224\n+    },\n+    \"y032\": {\n+        \"depths\": [2, 5, 13, 1],\n+        \"widths\": [72, 216, 576, 1512],\n+        \"group_width\": 24,\n+        \"default_size\": 224\n+    },\n+    \"y040\": {\n+        \"depths\": [2, 6, 12, 2],\n+        \"widths\": [128, 192, 512, 1088],\n+        \"group_width\": 64,\n+        \"default_size\": 224\n+    },\n+    \"y064\": {\n+        \"depths\": [2, 7, 14, 2],\n+        \"widths\": [144, 288, 576, 1296],\n+        \"group_width\": 72,\n+        \"default_size\": 224\n+    },\n+    \"y080\": {\n+        \"depths\": [2, 4, 10, 1],\n+        \"widths\": [168, 448, 896, 2016],\n+        \"group_width\": 56,\n+        \"default_size\": 224\n+    },\n+    \"y120\": {\n+        \"depths\": [2, 5, 11, 1],\n+        \"widths\": [224, 448, 896, 2240],\n+        \"group_width\": 112,\n+        \"default_size\": 224\n+    },\n+    \"y160\": {\n+        \"depths\": [2, 4, 11, 1],\n+        \"widths\": [224, 448, 1232, 3024],\n+        \"group_width\": 112,\n+        \"default_size\": 224\n+    },\n+    \"y320\": {\n+        \"depths\": [2, 5, 12, 1],\n+        \"widths\": [232, 696, 1392, 3712],\n+        \"group_width\": 232,\n+        \"default_size\": 224\n+    }\n+}\n+\n+BASE_DOCSTRING = \"\"\"Instantiates Regnet architecture.\n+\n+  References:\n+    - [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n+    (CVPR 2020)\n+    - [Fast and Accurate Model Scaling](https://arxiv.org/abs/2103.06877)\n+    (CVPR 2021)\n+\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+  https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n+\n+  Note: Each Keras Application expects a specific kind of input preprocessing.\n+  For Regnets, preprocessing is included in the model using a `Rescaling` layer.\n+  RegNet models expect their inputs to be float or uint8 tensors of pixels with \n+  values in the [0-255] range.\n+\n+  The naming of models is as follows: `RegNet{block_type}{flops}` where \n+  `block_type` is one of `(Y, Z)` and `flops` signifies hundred million \n+  floating point operations. For example RegNetY64 corresponds to RegNet with \n+  Y block and 6.4 giga flops (64 hundred million flops). \n+\n+  Args:\n+    include_top: Whether to include the fully-connected\n+        layer at the top of the network. Defaults to True.\n+    weights: One of `None` (random initialization),\n+          \"imagenet\" (pre-training on ImageNet),\n+          or the path to the weights file to be loaded. Defaults to \"imagenet\".\n+    input_tensor: Optional Keras tensor\n+        (i.e. output of `layers.Input()`)\n+        to use as image input for the model.\n+    input_shape: Optional shape tuple, only to be specified\n+        if `include_top` is False.\n+        It should have exactly 3 inputs channels.\n+    pooling: Optional pooling mode for feature extraction\n+        when `include_top` is `False`. Defaults to None.\n+        - `None` means that the output of the model will be\n+            the 4D tensor output of the\n+            last convolutional layer.\n+        - `avg` means that global average pooling\n+            will be applied to the output of the\n+            last convolutional layer, and thus\n+            the output of the model will be a 2D tensor.\n+        - `max` means that global max pooling will\n+            be applied.\n+    classes: Optional number of classes to classify images\n+        into, only to be specified if `include_top` is True, and\n+        if no `weights` argument is specified. Defaults to 1000 (number of\n+        ImageNet classes).\n+    classifier_activation: A `str` or callable. The activation function to use\n+        on the \"top\" layer. Ignored unless `include_top=True`. Set\n+        `classifier_activation=None` to return the logits of the \"top\" layer.\n+        Defaults to \"softmax\".\n+        When loading pretrained weights, `classifier_activation` can only\n+        be `None` or `\"softmax\"`.\n+\n+  Returns:\n+    A `keras.Model` instance.\n+\"\"\"\n+\n+\n+def PreStem(name=None):\n+  \"\"\"\n+  Rescales and normalizes inputs to [0,1] and ImageNet mean and std.\n+  \n+  Args:\n+    name: name prefix\n+    \n+  Returns:\n+    Rescaled and normalized tensor\n+  \"\"\"\n+  if name is None:\n+    name = \"prestem\" + str(backend.get_uid(\"prestem\"))\n+\n+  def apply(x):\n+    x = layers.Rescaling(scale=1. / 255., name=name + \"_prestem_rescaling\")(x)\n+\n+    return x\n+\n+  return apply\n+\n+\n+def Stem(name=None):\n+  \"\"\"\n+  Implementation of RegNet stem. (Common to all model variants)\n+  \n+  Args:\n+    name: name prefix   \n+\n+  Returns:\n+    Output tensor of the Stem\n+  \"\"\"\n+  if name is None:\n+    name = \"stem\" + str(backend.get_uid(\"stem\"))\n+\n+  def apply(x):\n+    x = layers.Conv2D(32, (3, 3),\n+                      strides=2,\n+                      use_bias=False,\n+                      padding=\"same\",\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_stem_conv\")(x)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_stem_bn\")(x)\n+    x = layers.ReLU(name=name + \"_stem_relu\")(x)\n+\n+    return x\n+\n+  return apply\n+\n+\n+def SqueezeAndExciteBlock(filters_in, se_filters, name=None):\n+  \"\"\"\n+  Implements the Squeeze and excite block(https://arxiv.org/abs/1709.01507)\n+  \n+  Args:\n+    filters_in: input filters to the block\n+    se_filters: filters to squeeze to\n+    name: name prefix\n+  \n+  Returns:\n+    A function object   \n+  \"\"\"\n+  if name is None:\n+    name = str(backend.get_uid(\"squeeze_and_excite\"))\n+\n+  def apply(inputs):\n+    x = layers.GlobalAveragePooling2D(\n+        name=name + \"_squeeze_and_excite_gap\", keepdims=True)(inputs)\n+    x = layers.Conv2D(se_filters, (1, 1),\n+                      activation=\"relu\",\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_squeeze_and_excite_squeeze\")(x)\n+    x = layers.Conv2D(filters_in, (1, 1),\n+                      activation=\"sigmoid\",\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_squeeze_and_excite_excite\")(x)\n+    x = tf.math.multiply(x, inputs)\n+    return x\n+\n+  return apply\n+\n+\n+def XBlock(filters_in, filters_out, group_width, stride=1, name=None):\n+  \"\"\"\n+  Implementation of X Block. \n+  Reference: [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n+\n+  Args:\n+    filters_in: filters in the input tensor\n+    filters_out: filters in the output tensor\n+    group_width: group width\n+    stride: stride\n+    name: name prefix\n+\n+  Return:\n+    Output tensor of the block \n+  \"\"\"\n+  if name is None:\n+    name = str(backend.get_uid(\"xblock\"))\n+\n+  def apply(inputs):\n+    if filters_in != filters_out and stride == 1:\n+      raise ValueError(\n+          \"\"\"Input filters and output filters are not equal for stride\n+                          1. Please check inputs and try again.\"\"\")\n+\n+    # Declare layers\n+    groups = filters_out // group_width\n+\n+    if stride != 1:\n+      skip = layers.Conv2D(filters_out, (1, 1),\n+                           strides=stride,\n+                           use_bias=False,\n+                           kernel_initializer=\"he_normal\",\n+                           name=name + \"_skip_1x1\")(inputs)\n+      skip = layers.BatchNormalization(momentum=0.9,\n+                                       epsilon=1e-5,\n+                                       name=name + \"_skip_bn\")(skip)\n+    else:\n+      skip = inputs\n+\n+    # Build block\n+    # conv_1x1_1\n+    x = layers.Conv2D(filters_out, (1, 1),\n+                      use_bias=False,\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_conv_1x1_1\")(inputs)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_conv_1x1_1_bn\")(x)\n+    x = layers.ReLU(name=name + \"_conv_1x1_1_relu\")(x)\n+\n+    # conv_3x3\n+    x = layers.Conv2D(filters_out, (3, 3),\n+                      use_bias=False, \n+                      strides=stride,\n+                      groups=groups,\n+                      padding=\"same\",\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_conv_3x3\")(x)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_conv_3x3_bn\")(x)\n+    x = layers.ReLU(name=name + \"_conv_3x3_relu\")(x)\n+\n+    # conv_1x1_2\n+    x = layers.Conv2D(filters_out, (1, 1),\n+                      use_bias=False,\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_conv_1x1_2\")(x)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_conv_1x1_2_bn\")(x)\n+\n+    x = layers.ReLU(name=name + \"_exit_relu\")(x + skip)\n+\n+    return x\n+\n+  return apply\n+\n+\n+def YBlock(filters_in,\n+           filters_out,\n+           group_width,\n+           stride=1,\n+           squeeze_excite_ratio=0.25,\n+           name=None):\n+  \"\"\"\n+  Implementation of Y Block. \n+  Reference: [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n+\n+  Args:\n+    filters_in: filters in the input tensor\n+    filters_out: filters in the output tensor\n+    group_width: group width\n+    stride: stride\n+    squeeze_excite_ratio: expansion ration for Squeeze and Excite block\n+    name: name prefix\n+\n+  Return:\n+    Output tensor of the block \n+  \"\"\"\n+  if name is None:\n+    name = str(backend.get_uid(\"yblock\"))\n+\n+  def apply(inputs):\n+    if filters_in != filters_out and stride == 1:\n+      raise ValueError(\n+          f\"\"\"Input filters({filters_in}) and output filters({filters_out}) are not equal for stride\n+                          {stride}. Input and output filters must be equal for stride={stride}.\"\"\"\n+      )\n+\n+    groups = filters_out // group_width\n+    se_filters = int(filters_in * squeeze_excite_ratio)\n+\n+    if stride != 1:\n+      skip = layers.Conv2D(filters_out, (1, 1),\n+                           strides=stride,\n+                           use_bias=False,\n+                           kernel_initializer=\"he_normal\",\n+                           name=name + \"_skip_1x1\")(inputs)\n+      skip = layers.BatchNormalization(momentum=0.9,\n+                                       epsilon=1e-5,\n+                                       name=name + \"_skip_bn\")(skip)\n+    else:\n+      skip = inputs\n+\n+    # Build block\n+    # conv_1x1_1\n+    x = layers.Conv2D(filters_out, (1, 1),\n+                      use_bias=False,\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_conv_1x1_1\")(inputs)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_conv_1x1_1_bn\")(x)\n+    x = layers.ReLU(name=name + \"_conv_1x1_1_relu\")(x)\n+\n+    # conv_3x3\n+    x = layers.Conv2D(filters_out, (3, 3),\n+                      use_bias=False,\n+                      strides=stride,\n+                      groups=groups,\n+                      padding=\"same\",\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_conv_3x3\")(x)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_conv_3x3_bn\")(x)\n+    x = layers.ReLU(name=name + \"_conv_3x3_relu\")(x)\n+\n+    # Squeeze-Excitation block (https://arxiv.org/abs/1709.01507)\n+    x = SqueezeAndExciteBlock(filters_out, se_filters, name=name)(x)\n+\n+    # conv_1x1_2\n+    x = layers.Conv2D(filters_out, (1, 1),\n+                      use_bias=False,\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_conv_1x1_2\")(x)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_conv_1x1_2_bn\")(x)\n+\n+    x = layers.ReLU(name=name + \"_exit_relu\")(x + skip)\n+\n+    return x\n+\n+  return apply\n+\n+\n+def ZBlock(filters_in,\n+           filters_out,\n+           group_width,\n+           stride=1,\n+           squeeze_excite_ratio=0.25,\n+           bottleneck_ratio=0.25,\n+           name=None):\n+  \"\"\"\n+  Implementation of Z block\n+  Reference: [Fast and Accurate Model Scaling](https://arxiv.org/abs/2103.06877).\n+  \n+  Args:\n+    filters_in: filters in the input tensor\n+    filters_out: filters in the output tensor\n+    group_width: group width\n+    stride: stride\n+    squeeze_excite_ratio: expansion ration for Squeeze and Excite block\n+    bottleneck_ratio: inverted bottleneck ratio \n+    name: name prefix\n+\n+  Return:\n+    Output tensor of the block \n+  \"\"\"\n+  if name is None:\n+    name = str(backend.get_uid(\"zblock\"))\n+\n+  def apply(inputs):\n+    if filters_in != filters_out and stride == 1:\n+      raise ValueError(\n+          f\"\"\"Input filters({filters_in}) and output filters({filters_out}) are not equal for stride\n+                          {stride}. Input and output filters must be equal for stride={stride}.\"\"\"\n+      )\n+\n+    groups = filters_out // group_width\n+    se_filters = int(filters_in * squeeze_excite_ratio)\n+\n+    inv_btlneck_filters = int(filters_out / bottleneck_ratio)\n+\n+    # Build block\n+    # conv_1x1_1\n+    x = layers.Conv2D(inv_btlneck_filters, (1, 1),\n+                      use_bias=False,\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_conv_1x1_1\")(inputs)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_conv_1x1_1_bn\")(x)\n+    x = tf.nn.silu(x)\n+\n+    # conv_3x3\n+    x = layers.Conv2D(inv_btlneck_filters, (3, 3),\n+                      use_bias=False,\n+                      strides=stride,\n+                      groups=groups,\n+                      padding=\"same\",\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_conv_3x3\")(x)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_conv_3x3_bn\")(x)\n+    x = tf.nn.silu(x)\n+\n+    # Squeeze-Excitation block (https://arxiv.org/abs/1709.01507)\n+    x = SqueezeAndExciteBlock(inv_btlneck_filters, se_filters, name=name)\n+\n+    # conv_1x1_2\n+    x = layers.Conv2D(filters_out, (1, 1),\n+                      use_bias=False,\n+                      kernel_initializer=\"he_normal\",\n+                      name=name + \"_conv_1x1_2\")(x)\n+    x = layers.BatchNormalization(momentum=0.9,\n+                                  epsilon=1e-5,\n+                                  name=name + \"_conv_1x1_2_bn\")(x)\n+\n+    if stride != 1:\n+      return x\n+    else:\n+      return x + inputs\n+\n+  return apply\n+\n+\n+def Stage(block_type, depth, group_width, filters_in, filters_out, name=None):\n+  \"\"\"\n+  Implementation of Stage in RegNet.\n+\n+  Args:\n+    block_type: must be one of \"X\", \"Y\", \"Z\"\n+    depth: depth of stage, number of blocks to use\n+    group_width: group width of all blocks in  this stage\n+    filters_in: input filters to this stage\n+    filters_out: output filters from this stage\n+    name: name prefix\n+\n+  Returns:\n+    Output tensor of Stage\n+  \"\"\"\n+  if name is None:\n+    name = str(backend.get_uid(\"stage\"))\n+\n+  def apply(inputs):\n+    x = inputs\n+    if block_type == \"X\":\n+      x = XBlock(filters_in,\n+                 filters_out,\n+                 group_width,\n+                 stride=2,\n+                 name=name + \"_XBlock_0\")(x)\n+      for i in range(depth - 1):\n+        x = XBlock(filters_out,\n+                   filters_out,\n+                   group_width,\n+                   name=name + \"_XBlock_\" + str(i+1))(x)\n+    elif block_type == \"Y\":\n+      x = YBlock(filters_in,\n+                 filters_out,\n+                 group_width,\n+                 stride=2,\n+                 name=name + \"_YBlock_0\")(x)\n+      for i in range(depth - 1):\n+        x = YBlock(filters_out,\n+                   filters_out,\n+                   group_width,\n+                   name=name + \"_YBlock_\" + str(i+1))(x)\n+    elif block_type == \"Z\":\n+      x = ZBlock(filters_in,\n+                 filters_out,\n+                 group_width,\n+                 stride=2,\n+                 name=name + \"_ZBlock_0\")(x)\n+      for i in range(depth - 1):\n+        x = ZBlock(filters_out,\n+                   filters_out,\n+                   group_width,\n+                   name=name + \"_ZBlock_\" + str(i+1))(x)\n+    else:\n+      raise NotImplementedError(f\"\"\"Block type `{block_type}` not recognized. \n+                                block_type must be one of (\"X\", \"Y\", \"Z\"). \"\"\")\n+    return x\n+\n+  return apply\n+\n+\n+def Head(num_classes=1000, name=None):\n+  \"\"\"\n+  Implementation of classification head of RegNet\n+  \n+  Args:\n+  x: Input tensor\n+    num_classes: number of classes for Dense layer\n+  \n+  Returns:\n+    Output logits tensor. \n+  \"\"\"\n+  if name is None:\n+    name = str(backend.get_uid(\"head\"))\n+\n+  def apply(x):\n+    x = layers.GlobalAveragePooling2D(name=name + \"_head_gap\")(x)\n+    x = layers.Dense(num_classes, name=name + \"head_dense\")(x)\n+    return x\n+\n+  return apply\n+\n+\n+def RegNet(depths,\n+           widths,\n+           group_width,\n+           block_type,\n+           default_size,\n+           model_name=\"regnet\",\n+           include_preprocessing=True,\n+           include_top=True,\n+           weights=\"imagenet\",\n+           input_tensor=None,\n+           input_shape=None,\n+           pooling=None,\n+           classes=1000,\n+           classifier_activation=\"softmax\"):\n+  \"\"\" \n+  Instantiates RegNet architecture given specific configuration.\n+\n+  Args:\n+    depths: An iterable containing depths for each individual stages. \n+    widths: An iterable containing output channel width of each individual \n+      stages\n+    group_width: Number of channels to be used in each group. See grouped \n+      convolutions for more information.\n+    block_type: Must be one of {\"x\", \"y\", \"z\"}. For more details see the\n+      papers \"Designing network design spaces\" and \"Fast and Accurate Model \n+      Scaling\"\n+    default_size: Default input image size. \n+    model_name: An optional name for the model.\n+    include_preprocessing: boolean denoting whther to include preprocessing in \n+      the model\n+    include_top: Boolean denoting whether to include classification head to \n+      the model.\n+    weights: one of `None` (random initialization),\n+      \"imagenet\" (pre-training on ImageNet),\n+      or the path to the weights file to be loaded.\n+    input_tensor: optional Keras tensor\n+      (i.e. output of `layers.Input()`)\n+      to use as image input for the model.\n+    input_shape: optional shape tuple, only to be specified\n+      if `include_top` is False.\n+      It should have exactly 3 inputs channels.\n+    pooling: optional pooling mode for feature extraction\n+      when `include_top` is `False`.\n+      - `None` means that the output of the model will be\n+          the 4D tensor output of the\n+          last convolutional layer.\n+      - `avg` means that global average pooling\n+          will be applied to the output of the\n+          last convolutional layer, and thus\n+          the output of the model will be a 2D tensor.\n+      - `max` means that global max pooling will\n+          be applied.\n+    classes: optional number of classes to classify images\n+      into, only to be specified if `include_top` is True, and\n+      if no `weights` argument is specified.\n+    classifier_activation: A `str` or callable. The activation function to use\n+      on the \"top\" layer. Ignored unless `include_top=True`. Set\n+      `classifier_activation=None` to return the logits of the \"top\" layer.        \n+  \n+  Returns:\n+  A `keras.Model` instance.\n+  Raises:\n+      ValueError: in case of invalid argument for `weights`,\n+        or invalid input shape.\n+      ValueError: if `classifier_activation` is not `softmax` or `None` when\n+        using a pretrained top layer.\n+      ValueError: if `include_top` is True but `num_classes` is not 1000.\n+      ValueError: if `block_type` is not one of `{\"X\", \"Y\", \"Z\"}`\n+  \n+  \"\"\"\n+  if not (weights in {\"imagenet\", None} or tf.io.gfile.exists(weights)):\n+    raise ValueError(\"The `weights` argument should be either \"\n+                     \"`None` (random initialization), `imagenet` \"\n+                     \"(pre-training on ImageNet), \"\n+                     \"or the path to the weights file to be loaded.\")\n+\n+  if weights == \"imagenet\" and include_top and classes != 1000:\n+    raise ValueError(\"If using `weights` as `'imagenet'` with `include_top`\"\n+                     \" as true, `classes` should be 1000\")\n+\n+  # Determine proper input shape\n+  input_shape = imagenet_utils.obtain_input_shape(\n+      input_shape,\n+      default_size=default_size,\n+      min_size=32,\n+      data_format=backend.image_data_format(),\n+      require_flatten=include_top,\n+      weights=weights)\n+\n+  if input_tensor is None:\n+    img_input = layers.Input(shape=input_shape)\n+  else:\n+    if not backend.is_keras_tensor(input_tensor):\n+      img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+    else:\n+      img_input = input_tensor\n+\n+  if input_tensor is not None:\n+    inputs = layer_utils.get_source_inputs(input_tensor)\n+  else:\n+    inputs = img_input\n+\n+  x = inputs\n+  if include_preprocessing:\n+    x = PreStem(name=model_name)(x)\n+  x = Stem(name=model_name)(x)\n+\n+  in_channels = 32  # Output from Stem\n+\n+  for num_stage in range(4):\n+    depth = depths[num_stage]\n+    out_channels = widths[num_stage]\n+\n+    x = Stage(block_type,\n+              depth,\n+              group_width,\n+              in_channels,\n+              out_channels,\n+              name=model_name + \"_Stage_\" + str(num_stage))(x)\n+    in_channels = out_channels\n+\n+  if include_top:\n+    x = Head(num_classes=classes)(x)\n+    imagenet_utils.validate_activation(classifier_activation, weights)\n+\n+  else:\n+    if pooling == \"avg\":\n+      x = layers.GlobalAveragePooling2D()(x)\n+    elif pooling == \"max\":\n+      x = layers.GlobalMaxPooling2D()(x)\n+\n+  model = training.Model(inputs=inputs, outputs=x, name=model_name)\n+\n+  # Load weights.\n+  if weights == \"imagenet\":\n+    if include_top:\n+      file_suffix = \".h5\"\n+      file_hash = WEIGHTS_HASHES[model_name[-2:]][0]\n+    else:\n+      file_suffix = \"_notop.h5\"\n+      file_hash = WEIGHTS_HASHES[model_name[-2:]][1]\n+    file_name = model_name + file_suffix\n+    weights_path = data_utils.get_file(file_name,\n+                                       BASE_WEIGHTS_PATH + file_name,\n+                                       cache_subdir=\"models\",\n+                                       file_hash=file_hash)\n+    model.load_weights(weights_path)\n+  elif weights is not None:\n+    model.load_weights(weights)\n+\n+  return model\n+\n+\n+## Instantiating variants ##\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX002\",\n+              \"keras.applications.RegNetX002\")\n+def RegNetX002(model_name=\"regnetx002\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x002\"][\"depths\"],\n+      MODEL_CONFIGS[\"x002\"][\"widths\"],\n+      MODEL_CONFIGS[\"x002\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x002\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX004\",\n+              \"keras.applications.RegNetX004\")\n+def RegNetX004(model_name=\"regnetx004\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x004\"][\"depths\"],\n+      MODEL_CONFIGS[\"x004\"][\"widths\"],\n+      MODEL_CONFIGS[\"x004\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x004\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX006\",\n+              \"keras.applications.RegNetX006\")\n+def RegNetX006(model_name=\"regnetx006\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x006\"][\"depths\"],\n+      MODEL_CONFIGS[\"x006\"][\"widths\"],\n+      MODEL_CONFIGS[\"x006\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x006\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX008\",\n+              \"keras.applications.RegNetX008\")\n+def RegNetX008(model_name=\"regnetx008\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x008\"][\"depths\"],\n+      MODEL_CONFIGS[\"x008\"][\"widths\"],\n+      MODEL_CONFIGS[\"x008\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x008\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX016\",\n+              \"keras.applications.RegNetX016\")\n+def RegNetX016(model_name=\"regnetx016\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x016\"][\"depths\"],\n+      MODEL_CONFIGS[\"x016\"][\"widths\"],\n+      MODEL_CONFIGS[\"x016\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x016\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX032\",\n+              \"keras.applications.RegNetX032\")\n+def RegNetX032(model_name=\"regnetx032\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x032\"][\"depths\"],\n+      MODEL_CONFIGS[\"x032\"][\"widths\"],\n+      MODEL_CONFIGS[\"x032\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x032\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX040\",\n+              \"keras.applications.RegNetX040\")\n+def RegNetX040(model_name=\"regnetx040\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x040\"][\"depths\"],\n+      MODEL_CONFIGS[\"x040\"][\"widths\"],\n+      MODEL_CONFIGS[\"x040\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x040\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX064\",\n+              \"keras.applications.RegNetX064\")\n+def RegNetX064(model_name=\"regnetx064\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x064\"][\"depths\"],\n+      MODEL_CONFIGS[\"x064\"][\"widths\"],\n+      MODEL_CONFIGS[\"x064\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x064\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX080\",\n+              \"keras.applications.RegNetX080\")\n+def RegNetX080(model_name=\"regnetx080\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x080\"][\"depths\"],\n+      MODEL_CONFIGS[\"x080\"][\"widths\"],\n+      MODEL_CONFIGS[\"x080\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x080\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX120\",\n+              \"keras.applications.RegNetX120\")\n+def RegNetX120(model_name=\"regnetx120\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x120\"][\"depths\"],\n+      MODEL_CONFIGS[\"x120\"][\"widths\"],\n+      MODEL_CONFIGS[\"x120\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x120\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX160\",\n+              \"keras.applications.RegNetX160\")\n+def RegNetX160(model_name=\"regnetx160\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x160\"][\"depths\"],\n+      MODEL_CONFIGS[\"x160\"][\"widths\"],\n+      MODEL_CONFIGS[\"x160\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x160\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX320\",\n+              \"keras.applications.RegNetX320\")\n+def RegNetX320(model_name=\"regnetx320\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x320\"][\"depths\"],\n+      MODEL_CONFIGS[\"x320\"][\"widths\"],\n+      MODEL_CONFIGS[\"x320\"][\"group_width\"],\n+      \"X\",\n+      MODEL_CONFIGS[\"x320\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY002\",\n+              \"keras.applications.RegNetY002\")\n+def RegNetY002(model_name=\"regnety002\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y002\"][\"depths\"],\n+      MODEL_CONFIGS[\"y002\"][\"widths\"],\n+      MODEL_CONFIGS[\"y002\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y002\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY004\",\n+              \"keras.applications.RegNetY004\")\n+def RegNetY004(model_name=\"regnety004\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y004\"][\"depths\"],\n+      MODEL_CONFIGS[\"y004\"][\"widths\"],\n+      MODEL_CONFIGS[\"y004\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y004\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY006\",\n+              \"keras.applications.RegNetY006\")\n+def RegNetY006(model_name=\"regnety006\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y006\"][\"depths\"],\n+      MODEL_CONFIGS[\"y006\"][\"widths\"],\n+      MODEL_CONFIGS[\"y006\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y006\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY008\",\n+              \"keras.applications.RegNetY008\")\n+def RegNetY008(model_name=\"regnety008\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y008\"][\"depths\"],\n+      MODEL_CONFIGS[\"y008\"][\"widths\"],\n+      MODEL_CONFIGS[\"y008\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y008\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY016\",\n+              \"keras.applications.RegNetY016\")\n+def RegNetY016(model_name=\"regnety016\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y016\"][\"depths\"],\n+      MODEL_CONFIGS[\"y016\"][\"widths\"],\n+      MODEL_CONFIGS[\"y016\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y016\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY032\",\n+              \"keras.applications.RegNetY032\")\n+def RegNetY032(model_name=\"regnety032\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y032\"][\"depths\"],\n+      MODEL_CONFIGS[\"y032\"][\"widths\"],\n+      MODEL_CONFIGS[\"y032\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y032\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY040\",\n+              \"keras.applications.RegNetY040\")\n+def RegNetY040(model_name=\"regnety040\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y040\"][\"depths\"],\n+      MODEL_CONFIGS[\"y040\"][\"widths\"],\n+      MODEL_CONFIGS[\"y040\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y040\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY064\",\n+              \"keras.applications.RegNetY064\")\n+def RegNetY064(model_name=\"regnety064\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y064\"][\"depths\"],\n+      MODEL_CONFIGS[\"y064\"][\"widths\"],\n+      MODEL_CONFIGS[\"y064\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y064\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY080\",\n+              \"keras.applications.RegNetY080\")\n+def RegNetY080(model_name=\"regnety080\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y080\"][\"depths\"],\n+      MODEL_CONFIGS[\"y080\"][\"widths\"],\n+      MODEL_CONFIGS[\"y080\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y080\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY120\",\n+              \"keras.applications.RegNetY120\")\n+def RegNetY120(model_name=\"regnety120\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y120\"][\"depths\"],\n+      MODEL_CONFIGS[\"y120\"][\"widths\"],\n+      MODEL_CONFIGS[\"y120\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y120\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY160\",\n+              \"keras.applications.RegNetY160\")\n+def RegNetY160(model_name=\"regnety160\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y160\"][\"depths\"],\n+      MODEL_CONFIGS[\"y160\"][\"widths\"],\n+      MODEL_CONFIGS[\"y160\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y160\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY320\",\n+              \"keras.applications.RegNetY320\")\n+def RegNetY320(model_name=\"regnety320\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=None,\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation='softmax'):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y320\"][\"depths\"],\n+      MODEL_CONFIGS[\"y320\"][\"widths\"],\n+      MODEL_CONFIGS[\"y320\"][\"group_width\"],\n+      \"Y\",\n+      MODEL_CONFIGS[\"y320\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation\n+  )\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The current implementation of RegNetX models shows a discrepancy in accuracy compared to the paper, with a 2-3% shortfall across different model variants. While you've made progress, especially with the smallest model reaching closer to expected validation accuracy, consider refining the preprocessing to allow users the option for built-in vs. custom methods. Additionally, the recent build errors regarding proto imports suggest a need for updates in the API generation logic to ensure the new models are correctly integrated. Check the relevant `.bzl` files for the necessary adjustments."
      }
    ],
    "codediff_length": 50667
  },
  {
    "index": 37,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/tests/manual/check_masked_recurrent.py b/tests/manual/check_masked_recurrent.py\nnew file mode 100644\nindex 000000000000..8c849072eca0\n--- /dev/null\n+++ b/tests/manual/check_masked_recurrent.py\n@@ -0,0 +1,130 @@\n+# Dummy test data as input to RNN. This input is 3 timesteps long where the third timestep always matches the\n+# first. Without masking it should be able to learn it, with masking it should fail.\n+\n+import numpy as np\n+from keras.utils.theano_utils import sharedX\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, Merge, Dropout, TimeDistributedDense\n+from keras.layers.embeddings import Embedding\n+from keras.layers.recurrent import SimpleRNN, SimpleDeepRNN, LSTM, GRU\n+import theano\n+\n+theano.config.exception_verbosity='high' \n+\n+# (nb_samples, timesteps, dimensions)\n+X = np.random.random_integers(1, 4, size=(500000,15))\n+\n+print(\"About to compile the first model\")\n+model = Sequential()\n+model.add(Embedding(5, 4, mask_zero=True))\n+model.add(TimeDistributedDense(4,4)) # obviously this is redundant. Just testing.\n+model.add(SimpleRNN(4,4, activation='relu', return_sequences=True))\n+model.add(Dropout(0.5))\n+model.add(SimpleDeepRNN(4,4, depth=2, activation='relu')) \n+model.add(Dropout(0.5))\n+model.add(Dense(4,4, activation='softmax'))\n+model.compile(loss='categorical_crossentropy',\n+        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n+print(\"Compiled model\")\n+\n+W = model.get_weights() # We'll save these so we can reset it later\n+\n+X[:,:10] = 0\n+Xmask0 = X.copy()\n+Xmask0[:,10] = 0\n+\n+Xmask12 = X.copy()\n+Xmask12[:,11] = 0\n+Xmask12[:,12] = 0\n+\n+X0_onehot = np.zeros((X.shape[0], 4))\n+X1_onehot = np.zeros((X.shape[0], 4))\n+for i, row in enumerate(X):\n+    X0_onehot[i, row[10]-1] = 1\n+    X1_onehot[i, row[11]-1] = 1\n+\n+# Uniform score: 4 options = ln(4) nats (2 bits)\n+# we should not do better than this when we mask out the part of the input\n+# that gives us the correct answer\n+uniform_score = np.log(4)\n+batch_size=512\n+\n+# Train it to guess 0th dim\n+model.fit(X, X0_onehot, nb_epoch=1, batch_size=batch_size)\n+score = model.evaluate(X, X0_onehot, batch_size=batch_size)\n+if score > uniform_score*0.9:\n+    raise Exception('Failed to learn to copy timestep 0, score %f' % score)\n+    \n+\n+model.set_weights(W)\n+\n+# Train without showing it the 0th dim to learn 1st dim\n+model.fit(X[:,1:], X1_onehot, nb_epoch=1, batch_size=batch_size)\n+score = model.evaluate(X[:,1:], X1_onehot, batch_size=batch_size)\n+if score > uniform_score*0.9:\n+    raise Exception('Failed to learn to copy timestep 1, score %f' % score)\n+\n+model.set_weights(W)\n+\n+# Train to guess 0th dim when 0th dim has been masked (should fail)\n+model.fit(Xmask0, X0_onehot, nb_epoch=1, batch_size=batch_size)\n+score = model.evaluate(Xmask0, X0_onehot, batch_size=batch_size)\n+if score < uniform_score*0.9:\n+   raise Exception('Somehow learned to copy timestep 0 despite mask, score %f' % score)\n+\n+model.set_weights(W)\n+\n+# Train to guess 1st dim when 0th dim has been masked (should succeed)\n+model.fit(Xmask0, X1_onehot, nb_epoch=1, batch_size=batch_size)\n+score = model.evaluate(Xmask0, X1_onehot, batch_size=batch_size)\n+if score > uniform_score*0.9:\n+    raise Exception('Failed to learn to copy timestep 1 in masked model, score %f' % score)\n+\n+model.set_weights(W)\n+\n+# Finally, make sure the mask is actually blocking input, mask out timesteps 1 and 2, and see if\n+# it can learn timestep 0 (should fail)\n+model.fit(Xmask12, X0_onehot, nb_epoch=1, batch_size=batch_size)\n+\n+score = model.evaluate(Xmask12, X0_onehot, batch_size=batch_size)\n+if score < uniform_score*0.9:\n+    raise Exception('Somehow learned to copy timestep 0 despite masking 1, score %f' % score)\n+\n+# Another testing approach, just initialize models and make sure that prepending zeros doesn't affect\n+# their output\n+print(\"About to compile the second model\")\n+model2 = Sequential()\n+model2.add(Embedding(5, 4, mask_zero=True))\n+model2.add(TimeDistributedDense(4,4))\n+model2.add(Activation('time_distributed_softmax'))\n+model2.add(LSTM(4,4, return_sequences=True))\n+model2.add(Activation('tanh'))\n+model2.add(GRU(4,4, activation='softmax', return_sequences=True))\n+model2.add(SimpleDeepRNN(4,4, depth=2, activation='relu', return_sequences=True)) \n+model2.add(SimpleRNN(4,4, activation='relu', return_sequences=True))\n+model2.compile(loss='categorical_crossentropy',\n+        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n+print(\"Compiled model2\")\n+\n+X2 = np.random.random_integers(1, 4, size=(2,5))\n+y2 = np.random.random((X2.shape[0],X2.shape[1],4))\n+\n+ref = model2.predict(X2)\n+ref_eval = model2.evaluate(X2, y2)\n+mask = np.ones((y2.shape[0], y2.shape[1], 1))\n+\n+for pre_zeros in range(1,10):\n+    padded_X2 = np.concatenate((np.zeros((X2.shape[0], pre_zeros)), X2), axis=1)\n+    padded_mask = np.concatenate((np.zeros((mask.shape[0], pre_zeros, mask.shape[2])), mask), axis=1)\n+    padded_y2 = np.concatenate((np.zeros((y2.shape[0], pre_zeros, y2.shape[2])), y2), axis=1)\n+\n+    pred = model2.predict(padded_X2)\n+    if not np.allclose(ref[:,-1,:], pred[:,-1,:]):\n+        raise Exception(\"Different result after left-padding %d zeros. Ref: %s, Pred: %s\" % (pre_zeros, ref, pred))\n+\n+    pad_eval = model2.evaluate(padded_X2, padded_y2, weights=padded_mask)\n+    if not np.allclose([pad_eval], [ref_eval]):\n+        raise Exception(\"Got dissimilar categorical_crossentropy after left-padding %d zeros. Ref: %f, Pred %f\" %\\\n+                (pref_eval, pred_val))\n+\n+        \n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR raises a few key points. First, the purpose of broadcasting the mask isn't immediately clear; clarifying its role in maintaining tensor shapes would help. Additionally, the increased GPU test duration could require investigation to ensure efficiency. The solution to mask propagation issues through RNNs needs careful consideration. Using a hard-coded mask value appears to be a fallback optionits performance impact may be minimal, but consider potential complications with bias terms in `TimeDistributedDense`. Lastly, thorough testing is crucial, especially before merging, to resolve outstanding issues with masked data handling."
      }
    ],
    "codediff_length": 5533
  },
  {
    "index": 38,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/examples/mnist_tfrecord.py b/examples/mnist_tfrecord.py\nnew file mode 100644\nindex 000000000000..342007fa96ef\n--- /dev/null\n+++ b/examples/mnist_tfrecord.py\n@@ -0,0 +1,158 @@\n+'''MNIST dataset with TFRecords, the standard TensorFlow data format.\n+\n+TFRecord is a data format supported throughout TensorFlow.\n+This example demonstrates how to load TFRecord data using\n+Input Tensors. Input Tensors differ from the normal Keras\n+workflow because instead of fitting to data loaded into a\n+a numpy array, data is supplied via a special tensor that\n+reads data from nodes that are wired directly into model\n+graph with the `Input(tensor=input_tensor)` parameter.\n+\n+There are several advantages to using Input Tensors.\n+First, if a dataset is already in TFRecord format you\n+can load and train on that data directly in Keras.\n+Second, extended backend API capabilities such as TensorFlow\n+data augmentation is easy to integrate directly into your\n+Keras training scripts via input tensors.\n+Third, TensorFlow implements several data APIs for\n+TFRecords, some of which provide significantly faster\n+training performance than numpy arrays can provide because\n+they run via the C++ backend. Please note that this\n+example is tailored for brevity and clarity and not\n+to demonstrate performance or augmentation capabilities.\n+\n+Input Tensors also have important disadvantages. In\n+particular, Input Tensors are fixed at model construction\n+because rewiring networks is not yet supported.\n+For this reason, changing the data input source means\n+model weights must be saved and the model rebuilt\n+from scratch to connect the new input data.\n+validation cannot currently be performed as training\n+progresses, and must be performed after training completes.\n+This example demonstrates how to train with input\n+tensors, save the model weights, and then evaluate the\n+model using the numpy based Keras API.\n+\n+Gets to 99.1% test accuracy after 78 epochs\n+(there is still a lot of margin for parameter tuning).\n+'''\n+import os\n+import copy\n+import time\n+\n+import numpy as np\n+\n+import tensorflow as tf\n+from keras import backend as K\n+from keras.models import Model\n+from keras import layers\n+from keras import objectives\n+from keras.utils import np_utils\n+from keras import objectives\n+\n+from tensorflow.contrib.learn.python.learn.datasets import mnist\n+\n+if K.backend() != 'tensorflow':\n+    raise RuntimeError('This example can only run with the '\n+                       'TensorFlow backend for the time being, '\n+                       'because it requires TFRecords, which '\n+                       'are not supported on other platforms.')\n+\n+\n+def cnn_layers(x_train_input):\n+    x = layers.Conv2D(32, (3, 3),\n+                      activation='relu', padding='valid')(x_train_input)\n+    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n+    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n+    x = layers.Dropout(0.25)(x)\n+    x = layers.Flatten()(x)\n+    x = layers.Dense(128, activation='relu')(x)\n+    x = layers.Dropout(0.5)(x)\n+    x_train_out = layers.Dense(classes,\n+                               activation='softmax',\n+                               name='x_train_out')(x)\n+    return x_train_out\n+\n+sess = K.get_session()\n+\n+batch_size = 128\n+batch_shape = (batch_size, 28, 28, 1)\n+steps_per_epoch = 469\n+epochs = 78\n+classes = 10\n+\n+# The capacity variable controls the maximum queue size\n+# allowed when prefetching data for training.\n+capacity = 10000\n+\n+# min_after_dequeue is the minimum number elements in the queue\n+# after a dequeue, which ensures sufficient mixing of elements.\n+min_after_dequeue = 3000\n+\n+# If `enqueue_many` is `False`, `tensors` is assumed to represent a\n+# single example.  An input tensor with shape `[x, y, z]` will be output\n+# as a tensor with shape `[batch_size, x, y, z]`.\n+#\n+# If `enqueue_many` is `True`, `tensors` is assumed to represent a\n+# batch of examples, where the first dimension is indexed by example,\n+# and all members of `tensors` should have the same size in the\n+# first dimension.  If an input tensor has shape `[*, x, y, z]`, the\n+# output will have shape `[batch_size, x, y, z]`.\n+enqueue_many = True\n+\n+data = mnist.load_mnist()\n+x_train_batch, y_train_batch = tf.train.shuffle_batch(\n+    tensors=[data.train.images, data.train.labels.astype(np.int32)],\n+    batch_size=batch_size,\n+    capacity=capacity,\n+    min_after_dequeue=min_after_dequeue,\n+    enqueue_many=enqueue_many,\n+    num_threads=8)\n+\n+x_train_batch = tf.cast(x_train_batch, tf.float32)\n+x_train_batch = tf.reshape(x_train_batch, shape=batch_shape)\n+\n+y_train_batch = tf.cast(y_train_batch, tf.int32)\n+y_train_batch = tf.one_hot(y_train_batch, classes)\n+\n+x_batch_shape = x_train_batch.get_shape().as_list()\n+y_batch_shape = y_train_batch.get_shape().as_list()\n+\n+x_train_input = layers.Input(tensor=x_train_batch, batch_shape=x_batch_shape)\n+x_train_out = cnn_layers(x_train_input)\n+train_model = Model(inputs=x_train_input, outputs=x_train_out)\n+\n+cce = objectives.categorical_crossentropy(y_train_batch, x_train_out)\n+train_model.add_loss(cce)\n+\n+# Do not pass the loss directly to model.compile()\n+# because it is not yet supported for Input Tensors.\n+train_model.compile(optimizer='rmsprop',\n+                    loss=None,\n+                    metrics=['accuracy'])\n+train_model.summary()\n+\n+coord = tf.train.Coordinator()\n+threads = tf.train.start_queue_runners(sess, coord)\n+train_model.fit(epochs=epochs,\n+                steps_per_epoch=steps_per_epoch)\n+\n+train_model.save_weights('saved_wt.h5')\n+\n+coord.request_stop()\n+coord.join(threads)\n+K.clear_session()\n+\n+# Second Session to test loading trained model without tensors\n+x_test = np.reshape(data.validation.images, (data.validation.images.shape[0], 28, 28, 1))\n+y_test = data.validation.labels\n+x_test_inp = layers.Input(batch_shape=(None,) + (x_test.shape[1:]))\n+test_out = cnn_layers(x_test_inp)\n+test_model = Model(inputs=x_test_inp, outputs=test_out)\n+\n+test_model.load_weights('saved_wt.h5')\n+test_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n+test_model.summary()\n+\n+loss, acc = test_model.evaluate(x_test, np_utils.to_categorical(y_test), classes)\n+print('\\nTest accuracy: {0}'.format(acc))\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR introduces a structured approach for handling data sampling by prioritizing sample recovery and raising relevant errors/warnings based on `steps_per_epoch` and `batch_size` configurations. However, it seems there's confusion regarding the treatment of `batch_size`, which should not be ignored since it's integral to computations. Additionally, validation for input tensors may require further elaboration; keep it clean and concise to maintain clarity. The coordination with PR #6928 appears crucial for achieving consistency across the API."
      }
    ],
    "codediff_length": 6253
  },
  {
    "index": 39,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/keras/backend/theano_backend.py b/keras/backend/theano_backend.py\nindex 5ea9a2f16ab5..94cd2a387481 100644\n--- a/keras/backend/theano_backend.py\n+++ b/keras/backend/theano_backend.py\n@@ -647,6 +704,67 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n     return conv_out\n \n \n+def conv3d(x, kernel, strides=(1, 1, 1), border_mode='valid', dim_ordering='th',\n+           volume_shape=None, filter_shape=None):\n+    '''\n+    Run on cuDNN if available.\n+    border_mode: string, \"same\" or \"valid\".\n+    '''\n+    if dim_ordering not in {'th', 'tf'}:\n+        raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n+\n+    if border_mode not in {'same', 'valid'}:\n+        raise Exception('Invalid border mode: ' + str(border_mode))\n+\n+    if dim_ordering == 'tf':\n+        # TF uses the last dimension as channel dimension,\n+        # instead of the 2nd one.\n+        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)\n+        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3, input_depth)\n+        # TH kernel shape: (out_depth, input_depth, kernel_dim1, kernel_dim2, kernel_dim3)\n+        # TF kernel shape: (kernel_dim1, kernel_dim2, kernel_dim3, input_depth, out_depth)\n+        x = x.dimshuffle((0, 4, 1, 2, 3))\n+        kernel = kernel.dimshuffle((4, 3, 0, 1, 2))\n+        if volume_shape:\n+            volume_shape = (volume_shape[0], volume_shape[4],\n+                            volume_shape[1], volume_shape[2], volume_shape[3])\n+        if filter_shape:\n+            filter_shape = (filter_shape[4], filter_shape[3],\n+                            filter_shape[0], filter_shape[1], filter_shape[2])\n+\n+    if border_mode == 'same':\n+        assert(strides == (1, 1, 1))\n+        pad_dim1 = (kernel.shape[2] - 1)\n+        pad_dim2 = (kernel.shape[3] - 1)\n+        pad_dim3 = (kernel.shape[4] - 1)\n+        output_shape = (x.shape[0], x.shape[1],\n+                        x.shape[2] + pad_dim1,\n+                        x.shape[3] + pad_dim2,\n+                        x.shape[4] + pad_dim3)\n+        output = T.zeros(output_shape)\n+        indices = (slice(None), slice(None),\n+                   slice(pad_dim1 // 2, x.shape[2] + pad_dim1 // 2),\n+                   slice(pad_dim2 // 2, x.shape[3] + pad_dim2 // 2),\n+                   slice(pad_dim3 // 2, x.shape[4] + pad_dim3 // 2))\n+        x = T.set_subtensor(output[indices], x)\n+        border_mode = 'valid'\n+\n+    border_mode_3d = (border_mode, border_mode, border_mode)\n+    conv_out = conv3d2d.conv3d(signals=x.dimshuffle(0, 2, 1, 3, 4),\n+                               filters=kernel.dimshuffle(0, 2, 1, 3, 4),\n+                               border_mode=border_mode_3d)\n+    conv_out = conv_out.dimshuffle(0, 2, 1, 3, 4)\n+\n+    # support strides by manually slicing the output\n+    if strides != (1, 1, 1):\n+        conv_out = conv_out[:, :, ::strides[0], ::strides[1], ::strides[2]]\n+\n+    if dim_ordering == 'tf':\n+        conv_out = conv_out.dimshuffle((0, 2, 3, 4, 1))\n+\n+    return conv_out\n+\n+\n def pool2d(x, pool_size, strides=(1, 1), border_mode='valid',\n            dim_ordering='th', pool_mode='max'):\n     if border_mode == 'same':\n@@ -682,6 +800,64 @@ def pool2d(x, pool_size, strides=(1, 1), border_mode='valid',\n     return pool_out\n \n \n+def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n+           dim_ordering='th', pool_mode='max'):\n+    if border_mode == 'same':\n+        # TODO: add implementation for border_mode=\"same\"\n+        raise Exception('border_mode=\"same\" not supported with Theano.')\n+    elif border_mode == 'valid':\n+        ignore_border = True\n+        padding = (0, 0)\n+    else:\n+        raise Exception('Invalid border mode: ' + str(border_mode))\n+\n+    if dim_ordering not in {'th', 'tf'}:\n+        raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n+\n+    if dim_ordering == 'tf':\n+        x = x.dimshuffle((0, 4, 1, 2, 3))\n+\n+    if pool_mode == 'max':\n+        # pooling over conv_dim2, conv_dim1 (last two channels)\n+        output = downsample.max_pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n+                                        ds=(pool_size[1], pool_size[0]),\n+                                        st=(strides[1], strides[0]),\n+                                        ignore_border=ignore_border,\n+                                        padding=padding,\n+                                        mode='max')\n+\n+        # pooling over conv_dim3\n+        pool_out = downsample.max_pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n+                                          ds=(1, pool_size[2]),\n+                                          st=(1, strides[2]),\n+                                          ignore_border=ignore_border,\n+                                          padding=padding,\n+                                          mode='max')\n+\n+    elif pool_mode == 'avg':\n+        # pooling over conv_dim2, conv_dim1 (last two channels)\n+        output = downsample.max_pool_2d(input=x.dimshuffle(0, 1, 4, 3, 2),\n+                                        ds=(pool_size[1], pool_size[0]),\n+                                        st=(strides[1], strides[0]),\n+                                        ignore_border=ignore_border,\n+                                        padding=padding,\n+                                        mode='average_exc_pad')\n+\n+        # pooling over conv_dim3\n+        pool_out = downsample.max_pool_2d(input=output.dimshuffle(0, 1, 4, 3, 2),\n+                                          ds=(1, pool_size[2]),\n+                                          st=(1, strides[2]),\n+                                          ignore_border=ignore_border,\n+                                          padding=padding,\n+                                          mode='average_exc_pad')\n+    else:\n+        raise Exception('Invalid pooling mode: ' + str(pool_mode))\n+\n+    if dim_ordering == 'tf':\n+        pool_out = pool_out.dimshuffle((0, 2, 3, 4, 1))\n+    return pool_out\n+\n+\n # RANDOMNESS\n \n \ndiff --git a/keras/layers/convolutional.py b/keras/layers/convolutional.py\nindex 2ab14a7080ed..08ad068def49 100644\n--- a/keras/layers/convolutional.py\n+++ b/keras/layers/convolutional.py\n@@ -343,6 +343,196 @@ def get_config(self):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class Convolution3D(Layer):\n+    '''Convolution operator for filtering windows of three-dimensional inputs.\n+    When using this layer as the first layer in a model,\n+    provide the keyword argument `input_shape`\n+    (tuple of integers, does not include the sample axis),\n+    e.g. `input_shape=(3, 10, 128, 128)` for 10 frames of 128x128 RGB pictures.\n+\n+    Note: this layer will only work with Theano for the time being.\n+\n+    # Input shape\n+        5D tensor with shape:\n+        `(samples, channels, conv_dim1, conv_dim2, conv_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, conv_dim1, conv_dim2, conv_dim3, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        5D tensor with shape:\n+        `(samples, nb_filter, new_conv_dim1, new_conv_dim2, new_conv_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, nb_filter)` if dim_ordering='tf'.\n+        `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have changed due to padding.\n+\n+    # Arguments\n+        nb_filter: Number of convolution filters to use.\n+        kernel_dim1: Length of the first dimension in the covolution kernel.\n+        kernel_dim2: Length of the second dimension in the convolution kernel.\n+        kernel_dim3: Length of the third dimension in the convolution kernel.\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)), or alternatively,\n+            Theano function to use for weights initialization.\n+            This parameter is only relevant if you don't pass\n+            a `weights` argument.\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of numpy arrays to set as initial weights.\n+        border_mode: 'valid' or 'same'.\n+        subsample: tuple of length 3. Factor by which to subsample output.\n+            Also called strides elsewhere.\n+            Note: 'subsample' is implemented by slicing the output of conv3d with strides=(1,1,1).\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 4.\n+    '''\n+    input_ndim = 5\n+\n+    def __init__(self, nb_filter, kernel_dim1, kernel_dim2, kernel_dim3,\n+                 init='glorot_uniform', activation='linear', weights=None,\n+                 border_mode='valid', subsample=(1, 1, 1), dim_ordering='th',\n+                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_constraint=None, b_constraint=None, **kwargs):\n+        if K._BACKEND != 'theano':\n+            raise Exception(self.__class__.__name__ +\n+                            ' is currently only working with Theano backend.')\n+        if border_mode not in {'valid', 'same'}:\n+            raise Exception('Invalid border mode for Convolution3D:', border_mode)\n+        self.nb_filter = nb_filter\n+        self.kernel_dim1 = kernel_dim1\n+        self.kernel_dim2 = kernel_dim2\n+        self.kernel_dim3 = kernel_dim3\n+        self.init = initializations.get(init)\n+        self.activation = activations.get(activation)\n+        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n+        self.border_mode = border_mode\n+        self.subsample = tuple(subsample)\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n+\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+        self.constraints = [self.W_constraint, self.b_constraint]\n+\n+        self.initial_weights = weights\n+        self.input = K.placeholder(ndim=5)\n+        super(Convolution3D, self).__init__(**kwargs)\n+\n+    def build(self):\n+\n+        if self.dim_ordering == 'th':\n+            stack_size = self.input_shape[1]\n+            self.W_shape = (self.nb_filter, stack_size,\n+                            self.kernel_dim1, self.kernel_dim2, self.kernel_dim3)\n+        elif self.dim_ordering == 'tf':\n+            stack_size = self.input_shape[4]\n+            self.W_shape = (self.kernel_dim1, self.kernel_dim2, self.kernel_dim3,\n+                            stack_size, self.nb_filter)\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        self.W = self.init(self.W_shape)\n+        self.b = K.zeros((self.nb_filter,))\n+        self.trainable_weights = [self.W, self.b]\n+        self.regularizers = []\n+\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n+\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        if self.dim_ordering == 'th':\n+            conv_dim1 = input_shape[2]\n+            conv_dim2 = input_shape[3]\n+            conv_dim3 = input_shape[4]\n+        elif self.dim_ordering == 'tf':\n+            conv_dim1 = input_shape[1]\n+            conv_dim2 = input_shape[2]\n+            conv_dim3 = input_shape[3]\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        conv_dim1 = conv_output_length(conv_dim1, self.kernel_dim1,\n+                                       self.border_mode, self.subsample[0])\n+        conv_dim2 = conv_output_length(conv_dim2, self.kernel_dim2,\n+                                       self.border_mode, self.subsample[1])\n+        conv_dim3 = conv_output_length(conv_dim3, self.kernel_dim3,\n+                                       self.border_mode, self.subsample[2])\n+\n+        if self.dim_ordering == 'th':\n+            return (input_shape[0], self.nb_filter, conv_dim1, conv_dim2, conv_dim3)\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0], conv_dim1, conv_dim2, conv_dim3, self.nb_filter)\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        conv_out = K.conv3d(X, self.W, strides=self.subsample,\n+                            border_mode=self.border_mode,\n+                            dim_ordering=self.dim_ordering,\n+                            volume_shape=self.input_shape,\n+                            filter_shape=self.W_shape)\n+\n+        if self.dim_ordering == 'th':\n+            output = conv_out + K.reshape(self.b, (1, self.nb_filter, 1, 1, 1))\n+        elif self.dim_ordering == 'tf':\n+            output = conv_out + K.reshape(self.b, (1, 1, 1, 1, self.nb_filter))\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+        output = self.activation(output)\n+        return output\n+\n+    def get_config(self):\n+        config = {\"name\": self.__class__.__name__,\n+                  \"nb_filter\": self.nb_filter,\n+                  \"kernel_dim1\": self.kernel_dim1,\n+                  \"kernel_dim2\": self.kernel_dim2,\n+                  \"kernel_dim3\": self.kernel_dim3,\n+                  \"dim_ordering\": self.dim_ordering,\n+                  \"init\": self.init.__name__,\n+                  \"activation\": self.activation.__name__,\n+                  \"border_mode\": self.border_mode,\n+                  \"subsample\": self.subsample,\n+                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n+                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n+        base_config = super(Convolution3D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class _Pooling1D(Layer):\n     '''Abstract class for different pooling 1D layers.\n     '''\n@@ -584,6 +778,158 @@ def _pooling_function(self, inputs, pool_size, strides,\n         return output\n \n \n+class _Pooling3D(Layer):\n+    '''Abstract class for different pooling 3D layers.\n+    '''\n+    input_ndim = 5\n+\n+    def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid',\n+                 dim_ordering='th', **kwargs):\n+        super(_Pooling3D, self).__init__(**kwargs)\n+        self.input = K.placeholder(ndim=5)\n+        self.pool_size = tuple(pool_size)\n+        if strides is None:\n+            strides = self.pool_size\n+        self.strides = tuple(strides)\n+        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n+        self.border_mode = border_mode\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n+\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        if self.dim_ordering == 'th':\n+            len_dim1 = input_shape[2]\n+            len_dim2 = input_shape[3]\n+            len_dim3 = input_shape[4]\n+        elif self.dim_ordering == 'tf':\n+            len_dim1 = input_shape[1]\n+            len_dim2 = input_shape[2]\n+            len_dim3 = input_shape[3]\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        len_dim1 = conv_output_length(len_dim1, self.pool_size[0],\n+                                      self.border_mode, self.strides[0])\n+        len_dim2 = conv_output_length(len_dim2, self.pool_size[1],\n+                                      self.border_mode, self.strides[1])\n+        len_dim3 = conv_output_length(len_dim3, self.pool_size[2],\n+                                      self.border_mode, self.strides[2])\n+\n+        if self.dim_ordering == 'th':\n+            return (input_shape[0], input_shape[1], len_dim1, len_dim2, len_dim3)\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0], len_dim1, len_dim2, len_dim3, input_shape[4])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+    def _pooling_function(self, inputs, pool_size, strides,\n+                          border_mode, dim_ordering):\n+        raise NotImplementedError\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        output = self._pooling_function(inputs=X, pool_size=self.pool_size,\n+                                        strides=self.strides,\n+                                        border_mode=self.border_mode,\n+                                        dim_ordering=self.dim_ordering)\n+        return output\n+\n+    def get_config(self):\n+        config = {'name': self.__class__.__name__,\n+                  'pool_size': self.pool_size,\n+                  'border_mode': self.border_mode,\n+                  'strides': self.strides,\n+                  'dim_ordering': self.dim_ordering}\n+        base_config = super(_Pooling3D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class MaxPooling3D(_Pooling3D):\n+    '''Max pooling operation for 3D data (spatial or spatio-temporal).\n+\n+    Note: this layer will only work with Theano for the time being.\n+\n+    # Input shape\n+        5D tensor with shape:\n+        `(samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, len_pool_dim1, len_pool_dim2, len_pool_dim3, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        5D tensor with shape:\n+        `(nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)` if dim_ordering='tf'.\n+\n+    # Arguments\n+        pool_size: tuple of 3 integers,\n+            factors by which to downscale (dim1, dim2, dim3).\n+            (2, 2, 2) will halve the size of the 3D input in each dimension.\n+        strides: tuple of 3 integers, or None. Strides values.\n+        border_mode: 'valid' or 'same'.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 4.\n+    '''\n+\n+    def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid',\n+                 dim_ordering='th', **kwargs):\n+        if K._BACKEND != 'theano':\n+            raise Exception(self.__class__.__name__ +\n+                            ' is currently only working with Theano backend.')\n+        super(MaxPooling3D, self).__init__(pool_size, strides, border_mode,\n+                                           dim_ordering, **kwargs)\n+\n+    def _pooling_function(self, inputs, pool_size, strides,\n+                          border_mode, dim_ordering):\n+        output = K.pool3d(inputs, pool_size, strides,\n+                          border_mode, dim_ordering, pool_mode='max')\n+        return output\n+\n+\n+class AveragePooling3D(_Pooling3D):\n+    '''Average pooling operation for 3D data (spatial or spatio-temporal).\n+\n+    Note: this layer will only work with Theano for the time being.\n+\n+    # Input shape\n+        5D tensor with shape:\n+        `(samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, len_pool_dim1, len_pool_dim2, len_pool_dim3, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        5D tensor with shape:\n+        `(nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)` if dim_ordering='tf'.\n+\n+    # Arguments\n+        pool_size: tuple of 3 integers,\n+            factors by which to downscale (dim1, dim2, dim3).\n+            (2, 2, 2) will halve the size of the 3D input in each dimension.\n+        strides: tuple of 3 integers, or None. Strides values.\n+        border_mode: 'valid' or 'same'.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 4.\n+    '''\n+\n+    def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid',\n+                 dim_ordering='th', **kwargs):\n+        if K._BACKEND != 'theano':\n+            raise Exception(self.__class__.__name__ +\n+                            ' is currently only working with Theano backend.')\n+        super(AveragePooling3D, self).__init__(pool_size, strides, border_mode,\n+                                               dim_ordering, **kwargs)\n+\n+    def _pooling_function(self, inputs, pool_size, strides,\n+                          border_mode, dim_ordering):\n+        output = K.pool3d(inputs, pool_size, strides,\n+                          border_mode, dim_ordering, pool_mode='avg')\n+        return output\n+\n+\n class UpSampling1D(Layer):\n     '''Repeat each temporal step `length` times along the time axis.\n \n@@ -679,6 +1025,72 @@ def get_config(self):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class UpSampling3D(Layer):\n+    '''Repeat the first, second and third dimension of the data\n+    by size[0], size[1] and size[2] respectively.\n+\n+    Note: this layer will only work with Theano for the time being.\n+\n+    # Input shape\n+        5D tensor with shape:\n+        `(samples, channels, dim1, dim2, dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, dim1, dim2, dim3, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        5D tensor with shape:\n+        `(samples, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)` if dim_ordering='tf'.\n+\n+    # Arguments\n+        size: tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.\n+        dim_ordering: 'th' or 'tf'.\n+            In 'th' mode, the channels dimension (the depth)\n+            is at index 1, in 'tf' mode is it at index 4.\n+    '''\n+    input_ndim = 5\n+\n+    def __init__(self, size=(2, 2, 2), dim_ordering='th', **kwargs):\n+        if K._BACKEND != 'theano':\n+            raise Exception(self.__class__.__name__ +\n+                            ' is currently only working with Theano backend.')\n+        super(UpSampling3D, self).__init__(**kwargs)\n+        self.input = K.placeholder(ndim=5)\n+        self.size = tuple(size)\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n+\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        if self.dim_ordering == 'th':\n+            return (input_shape[0],\n+                    input_shape[1],\n+                    self.size[0] * input_shape[2],\n+                    self.size[1] * input_shape[3],\n+                    self.size[2] * input_shape[4])\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0],\n+                    self.size[0] * input_shape[1],\n+                    self.size[1] * input_shape[2],\n+                    self.size[2] * input_shape[3],\n+                    input_shape[4])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        return K.resize_volumes(X, self.size[0], self.size[1], self.size[2],\n+                                self.dim_ordering)\n+\n+    def get_config(self):\n+        config = {'name': self.__class__.__name__,\n+                  'size': self.size}\n+        base_config = super(UpSampling3D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class ZeroPadding1D(Layer):\n     '''Zero-padding layer for 1D input (e.g. temporal sequence).\n \n@@ -769,3 +1181,63 @@ def get_config(self):\n                   'padding': self.padding}\n         base_config = super(ZeroPadding2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class ZeroPadding3D(Layer):\n+    '''Zero-padding layer for 3D data (spatial or spatio-temporal).\n+\n+    Note: this layer will only work with Theano for the time being.\n+\n+    # Input shape\n+        5D tensor with shape:\n+        (samples, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)\n+\n+    # Output shape\n+        5D tensor with shape:\n+        (samples, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)\n+\n+    # Arguments\n+        padding: tuple of int (length 3)\n+            How many zeros to add at the beginning and end of\n+            the 3 padding dimensions (axis 3, 4 and 5).\n+    '''\n+    input_ndim = 5\n+\n+    def __init__(self, padding=(1, 1, 1), dim_ordering='th', **kwargs):\n+        if K._BACKEND != 'theano':\n+            raise Exception(self.__class__.__name__ +\n+                            ' is currently only working with Theano backend.')\n+        super(ZeroPadding3D, self).__init__(**kwargs)\n+        self.padding = tuple(padding)\n+        self.input = K.placeholder(ndim=5)\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n+\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        if self.dim_ordering == 'th':\n+            return (input_shape[0],\n+                    input_shape[1],\n+                    input_shape[2] + 2 * self.padding[0],\n+                    input_shape[3] + 2 * self.padding[1],\n+                    input_shape[4] + 2 * self.padding[2])\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0],\n+                    input_shape[1] + 2 * self.padding[0],\n+                    input_shape[2] + 2 * self.padding[1],\n+                    input_shape[3] + 2 * self.padding[2],\n+                    input_shape[4])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        return K.spatial_3d_padding(X, padding=self.padding,\n+                                    dim_ordering=self.dim_ordering)\n+\n+    def get_config(self):\n+        config = {'name': self.__class__.__name__,\n+                  'padding': self.padding}\n+        base_config = super(ZeroPadding3D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\ndiff --git a/tests/keras/layers/test_convolutional.py b/tests/keras/layers/test_convolutional.py\nindex fc67f6901337..cc32cc4935eb 100644\n--- a/tests/keras/layers/test_convolutional.py\n+++ b/tests/keras/layers/test_convolutional.py\n@@ -204,6 +204,92 @@ def test_averagepooling_2d():\n         layer.get_config()\n \n \n+@pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n+def test_convolution_3d():\n+    nb_samples = 8\n+    nb_filter = 9\n+    stack_size = 7\n+    len_conv_dim1 = 2\n+    len_conv_dim2 = 10\n+    len_conv_dim3 = 6\n+\n+    input_len_dim1 = 10\n+    input_len_dim2 = 11\n+    input_len_dim3 = 12\n+\n+    weights_in = [np.ones((nb_filter, stack_size, len_conv_dim1, len_conv_dim2, len_conv_dim3)),\n+                  np.ones(nb_filter)]\n+\n+    input = np.ones((nb_samples, stack_size, input_len_dim1,\n+                     input_len_dim2, input_len_dim3))\n+    for weight in [None, weights_in]:\n+        for border_mode in ['same', 'valid']:\n+            for subsample in [(1, 1, 1), (2, 2, 2)]:\n+                if border_mode == 'same' and subsample != (1, 1, 1):\n+                    continue\n+                for W_regularizer in [None, 'l2']:\n+                    for b_regularizer in [None, 'l2']:\n+                        for act_regularizer in [None, 'l2']:\n+                            layer = convolutional.Convolution3D(\n+                                nb_filter, len_conv_dim1, len_conv_dim2, len_conv_dim3,\n+                                weights=weight,\n+                                border_mode=border_mode,\n+                                W_regularizer=W_regularizer,\n+                                b_regularizer=b_regularizer,\n+                                activity_regularizer=act_regularizer,\n+                                subsample=subsample,\n+                                input_shape=(stack_size, None, None, None))\n+\n+                            layer.input = K.variable(input)\n+                            for train in [True, False]:\n+                                out = K.eval(layer.get_output(train))\n+                                if border_mode == 'same' and subsample == (1, 1, 1):\n+                                    assert out.shape[2:] == input.shape[2:]\n+                            layer.get_config()\n+\n+\n+@pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n+def test_maxpooling_3d():\n+    nb_samples = 9\n+    stack_size = 7\n+    input_len_dim1 = 10\n+    input_len_dim2 = 11\n+    input_len_dim3 = 12\n+    pool_size = (3, 3, 3)\n+\n+    input = np.ones((nb_samples, stack_size, input_len_dim1,\n+                     input_len_dim2, input_len_dim3))\n+    for strides in [(1, 1, 1), (2, 2, 2)]:\n+        layer = convolutional.MaxPooling3D(strides=strides,\n+                                           border_mode='valid',\n+                                           pool_size=pool_size)\n+        layer.input = K.variable(input)\n+        for train in [True, False]:\n+            K.eval(layer.get_output(train))\n+        layer.get_config()\n+\n+\n+@pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n+def test_averagepooling_3d():\n+    nb_samples = 9\n+    stack_size = 7\n+    input_len_dim1 = 10\n+    input_len_dim2 = 11\n+    input_len_dim3 = 12\n+    pool_size = (3, 3, 3)\n+\n+    input = np.ones((nb_samples, stack_size, input_len_dim1,\n+                     input_len_dim2, input_len_dim3))\n+    for strides in [(1, 1, 1), (2, 2, 2)]:\n+        layer = convolutional.AveragePooling3D(strides=strides,\n+                                               border_mode='valid',\n+                                               pool_size=pool_size)\n+        layer.input = K.variable(input)\n+        for train in [True, False]:\n+            K.eval(layer.get_output(train))\n+        layer.get_config()\n+\n+\n def test_zero_padding_2d():\n     nb_samples = 9\n     stack_size = 7\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The loss discrepancy between GPU and CPU runs is concerning and may stem from precision differences in `conv3d2d.conv3d`. Ensure that random seed management is consistent across both platforms. The suggested terminology changes are thoughtful, but replacing \"time\" with terms like \"first_spatial_dim\" could be misleading in the context of temporal signals. Consider simplifying to \"spatial_dim1\", \"spatial_dim2\", and \"spatial_dim3\" for clarity. Additionally, layers need proper exception handling and serialization via `get_config()` to ensure robust behavior. Address these issues before proceeding."
      }
    ],
    "codediff_length": 32331
  },
  {
    "index": 40,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/keras/applications/mobilenetv2.py b/keras/applications/mobilenetv2.py\nnew file mode 100644\nindex 000000000000..3add1538ac1a\n--- /dev/null\n+++ b/keras/applications/mobilenetv2.py\n@@ -0,0 +1,526 @@\n+\"\"\"MobileNet v2 models for Keras.\n+\n+MobileNetV2 is a general architecture and can be used for multiple use cases.\n+Depending on the use case, it can use different input layer size and\n+different width factors. This allows different width models to reduce\n+the number of multiply-adds and thereby\n+reduce inference cost on mobile devices.\n+\n+MobileNetV2 is very similar to the original MobileNet,\n+except that it uses inverted residual blocks with\n+bottlenecking features. It has a drastically lower\n+parameter count than the original MobileNet.\n+MobileNets support any input size greater\n+than 32 x 32, with larger image sizes\n+offering better performance.\n+\n+The number of parameters and number of multiply-adds\n+can be modified by using the `alpha` parameter,\n+which increases/decreases the number of filters in each layer.\n+By altering the image size and `alpha` parameter,\n+all 22 models from the paper can be built, with ImageNet weights provided.\n+\n+The paper demonstrates the performance of MobileNets using `alpha` values of\n+1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4\n+\n+For each of these `alpha` values, weights for 5 different input image sizes\n+are provided (224, 192, 160, 128, and 96).\n+\n+\n+The following table describes the performance of\n+MobileNet on various input sizes:\n+------------------------------------------------------------------------\n+MACs stands for Multiply Adds\n+\n+ Classification Checkpoint| MACs (M)   | Parameters (M)| Top 1 Accuracy| Top 5 Accuracy\n+--------------------------|------------|---------------|---------|----|-------------\n+| [mobilenet_v2_1.4_224]  | 582 | 6.06 |          75.0 | 92.5 |\n+| [mobilenet_v2_1.3_224]  | 509 | 5.34 |          74.4 | 92.1 |\n+| [mobilenet_v2_1.0_224]  | 300 | 3.47 |          71.8 | 91.0 |\n+| [mobilenet_v2_1.0_192]  | 221 | 3.47 |          70.7 | 90.1 |\n+| [mobilenet_v2_1.0_160]  | 154 | 3.47 |          68.8 | 89.0 |\n+| [mobilenet_v2_1.0_128]  | 99  | 3.47 |          65.3 | 86.9 |\n+| [mobilenet_v2_1.0_96]   | 56  | 3.47 |          60.3 | 83.2 |\n+| [mobilenet_v2_0.75_224] | 209 | 2.61 |          69.8 | 89.6 |\n+| [mobilenet_v2_0.75_192] | 153 | 2.61 |          68.7 | 88.9 |\n+| [mobilenet_v2_0.75_160] | 107 | 2.61 |          66.4 | 87.3 |\n+| [mobilenet_v2_0.75_128] | 69  | 2.61 |          63.2 | 85.3 |\n+| [mobilenet_v2_0.75_96]  | 39  | 2.61 |          58.8 | 81.6 |\n+| [mobilenet_v2_0.5_224]  | 97  | 1.95 |          65.4 | 86.4 |\n+| [mobilenet_v2_0.5_192]  | 71  | 1.95 |          63.9 | 85.4 |\n+| [mobilenet_v2_0.5_160]  | 50  | 1.95 |          61.0 | 83.2 |\n+| [mobilenet_v2_0.5_128]  | 32  | 1.95 |          57.7 | 80.8 |\n+| [mobilenet_v2_0.5_96]   | 18  | 1.95 |          51.2 | 75.8 |\n+| [mobilenet_v2_0.35_224] | 59  | 1.66 |          60.3 | 82.9 |\n+| [mobilenet_v2_0.35_192] | 43  | 1.66 |          58.2 | 81.2 |\n+| [mobilenet_v2_0.35_160] | 30  | 1.66 |          55.7 | 79.1 |\n+| [mobilenet_v2_0.35_128] | 20  | 1.66 |          50.8 | 75.0 |\n+| [mobilenet_v2_0.35_96]  | 11  | 1.66 |          45.5 | 70.4 |\n+\n+The weights for all 16 models are obtained and translated from the Tensorflow checkpoints\n+from TensorFlow checkpoints found at\n+https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md\n+\n+# Reference\n+This file contains building code for MobileNetV2, based on\n+[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)\n+\n+Tests comparing this model to the existing Tensorflow model can be\n+found at [mobilenet_v2_keras](https://github.com/JonathanCMitchell/mobilenet_v2_keras)\n+\"\"\"\n+from __future__ import print_function\n+from __future__ import absolute_import\n+from __future__ import division\n+\n+import os\n+import warnings\n+import h5py\n+import numpy as np\n+\n+from ..models import Model\n+from ..layers import Input\n+from ..layers import Activation\n+from ..layers import Dropout\n+from ..layers import Reshape\n+from ..layers import BatchNormalization\n+from ..layers import Conv2D\n+from ..layers import DepthwiseConv2D\n+from ..layers import GlobalAveragePooling2D\n+from ..layers import Add\n+from ..layers import Flatten\n+from ..layers import Dense\n+from .. import initializers\n+from .. import regularizers\n+from .. import constraints\n+from ..utils import conv_utils\n+from ..utils.data_utils import get_file\n+from ..engine import get_source_inputs\n+from ..engine.base_layer import InputSpec\n+from . import imagenet_utils\n+from .imagenet_utils import _obtain_input_shape\n+from .imagenet_utils import decode_predictions\n+from .. import backend as K\n+\n+# TODO Change path to v1.1\n+BASE_WEIGHT_PATH = 'https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/'\n+\n+\n+def relu6(x):\n+    return K.relu(x, max_value=6)\n+\n+\n+def preprocess_input(x):\n+    \"\"\"Preprocesses a numpy array encoding a batch of images.\n+\n+    This function applies the \"Inception\" preprocessing which converts\n+    the RGB values from [0, 255] to [-1, 1]. Note that this preprocessing\n+    function is different from `imagenet_utils.preprocess_input()`.\n+\n+    # Arguments\n+        x: a 4D numpy array consists of RGB values within [0, 255].\n+\n+    # Returns\n+        Preprocessed array.\n+    \"\"\"\n+    x /= 128.\n+    x -= 1.\n+    return x.astype(np.float32)\n+\n+\n+# This function is taken from the original tf repo.\n+# It ensures that all layers have a channel number that is divisible by 8\n+# It can be seen here:\n+# https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n+\n+\n+def _make_divisible(v, divisor, min_value=None):\n+    if min_value is None:\n+        min_value = divisor\n+    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n+    # Make sure that round down does not go down by more than 10%.\n+    if new_v < 0.9 * v:\n+        new_v += divisor\n+    return new_v\n+\n+\n+def MobileNetV2(input_shape=None,\n+                alpha=1.0,\n+                depth_multiplier=1,\n+                dropout=1e-3,\n+                include_top=True,\n+                weights='imagenet',\n+                input_tensor=None,\n+                classes=1000):\n+    \"\"\"Instantiates the MobileNetV2 architecture.\n+\n+    To load a MobileNetV2 model via `load_model`, import the custom\n+    objects `relu6` and pass them to the `custom_objects` parameter.\n+    E.g.\n+    model = load_model('mobilenet.h5', custom_objects={\n+                       'relu6': mobilenet.relu6})\n+\n+    # Arguments\n+        input_shape: optional shape tuple, to be specified if you would\n+            like to use a model with an input img resolution that is not\n+            (224, 224, 3).\n+            It should have exactly 3 inputs channels (224, 224, 3).\n+            You can also omit this option if you would like\n+            to infer input_shape from an input_tensor.\n+            If you choose to include both input_tensor and input_shape then\n+            input_shape will be used if they match, if the shapes\n+            do not match then we will throw an error.\n+            E.g. `(160, 160, 3)` would be one valid value.\n+        alpha: controls the width of the network. This is known as the\n+        width multiplier in the MobileNetV2 paper.\n+            - If `alpha` < 1.0, proportionally decreases the number\n+                of filters in each layer.\n+            - If `alpha` > 1.0, proportionally increases the number\n+                of filters in each layer.\n+            - If `alpha` = 1, default number of filters from the paper\n+                 are used at each layer.\n+        depth_multiplier: depth multiplier for depthwise convolution\n+            (also called the resolution multiplier)\n+        dropout: dropout rate, dropout is currently not in use\n+        include_top: whether to include the fully-connected\n+            layer at the top of the network.\n+        weights: one of `None` (random initialization),\n+              'imagenet' (pre-training on ImageNet),\n+              or the path to the weights file to be loaded.\n+        input_tensor: optional Keras tensor (i.e. output of\n+            `layers.Input()`)\n+            to use as image input for the model.\n+        classes: optional number of classes to classify images\n+            into, only to be specified if `include_top` is True, and\n+            if no `weights` argument is specified.\n+\n+    # Returns\n+        A Keras model instance.\n+\n+    # Raises\n+        ValueError: in case of invalid argument for `weights`,\n+            or invalid input shape or invalid depth_multiplier, alpha,\n+            rows when weights='imagenet'\n+    \"\"\"\n+\n+    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n+        raise ValueError('The `weights` argument should be either '\n+                         '`None` (random initialization), `imagenet` '\n+                         '(pre-training on ImageNet), '\n+                         'or the path to the weights file to be loaded.')\n+\n+    if weights == 'imagenet' and include_top and classes != 1000:\n+        raise ValueError('If using `weights` as ImageNet with `include_top` '\n+                         'as true, `classes` should be 1000')\n+\n+    # Determine proper input shape and default size.\n+    # If both input_shape and input_tensor are used, they should match\n+    if input_shape is not None and input_tensor is not None:\n+        try:\n+            is_input_t_tensor = K.is_keras_tensor(input_tensor)\n+        except ValueError:\n+            try:\n+                is_input_t_tensor = K.is_keras_tensor(\n+                    get_source_inputs(input_tensor))\n+            except ValueError:\n+                raise ValueError('input_tensor: ', input_tensor,\n+                                 'is not type input_tensor')\n+        if is_input_t_tensor:\n+            if K.image_data_format == 'channels_first':\n+                if input_tensor._keras_shape[1] != input_shape[1]:\n+                    raise ValueError('input_shape: ', input_shape,\n+                                     'and input_tensor: ', input_tensor,\n+                                     'do not meet the same shape requirements')\n+            else:\n+                if input_tensor._keras_shape[2] != input_shape[1]:\n+                    raise ValueError('input_shape: ', input_shape,\n+                                     'and input_tensor: ', input_tensor,\n+                                     'do not meet the same shape requirements')\n+        else:\n+            raise ValueError('input_tensor specified: ', input_tensor,\n+                             'is not a keras tensor')\n+\n+    # If input_shape is None, infer shape from input_tensor\n+    if input_shape is None and input_tensor is not None:\n+\n+        try:\n+            K.is_keras_tensor(input_tensor)\n+        except ValueError:\n+            raise ValueError('input_tensor: ', input_tensor,\n+                             'is type: ', type(input_tensor),\n+                             'which is not a valid type')\n+\n+        if input_shape is None and not K.is_keras_tensor(input_tensor):\n+            default_size = 224\n+        elif input_shape is None and K.is_keras_tensor(input_tensor):\n+            if K.image_data_format() == 'channels_first':\n+                rows = input_tensor._keras_shape[2]\n+                cols = input_tensor._keras_shape[3]\n+            else:\n+                rows = input_tensor._keras_shape[1]\n+                cols = input_tensor._keras_shape[2]\n+\n+            if rows == cols and rows in [96, 128, 160, 192, 224]:\n+                default_size = rows\n+            else:\n+                default_size = 224\n+\n+    # If input_shape is None and no input_tensor\n+    elif input_shape is None:\n+        default_size = 224\n+\n+    # If input_shape is not None, assume default size\n+    else:\n+        if K.image_data_format() == 'channels_first':\n+            rows = input_shape[1]\n+            cols = input_shape[2]\n+        else:\n+            rows = input_shape[0]\n+            cols = input_shape[1]\n+\n+        if rows == cols and rows in [96, 128, 160, 192, 224]:\n+            default_size = rows\n+        else:\n+            default_size = 224\n+\n+    input_shape = _obtain_input_shape(input_shape,\n+                                      default_size=default_size,\n+                                      min_size=32,\n+                                      data_format=K.image_data_format(),\n+                                      require_flatten=include_top,\n+                                      weights=weights)\n+\n+    if K.image_data_format() == 'channels_last':\n+        row_axis, col_axis = (0, 1)\n+    else:\n+        row_axis, col_axis = (1, 2)\n+    rows = input_shape[row_axis]\n+    cols = input_shape[col_axis]\n+\n+    if weights == 'imagenet':\n+        if depth_multiplier != 1:\n+            raise ValueError('If imagenet weights are being loaded, '\n+                             'depth multiplier must be 1')\n+\n+        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:\n+            raise ValueError('If imagenet weights are being loaded, '\n+                             'alpha can be one of'\n+                             '`0.25`, `0.50`, `0.75` or `1.0` only.')\n+\n+        if rows != cols or rows not in [96, 128, 160, 192, 224]:\n+            if rows is None:\n+                rows = 224\n+                warnings.warn('MobileNet shape is undefined.'\n+                              ' Weights for input shape'\n+                              '(224, 224) will be loaded.')\n+            else:\n+                raise ValueError('If imagenet weights are being loaded, '\n+                                 'input must have a static square shape'\n+                                 '(one of (96, 96), (128, 128), (160, 160),'\n+                                 '(192, 192), or (224, 224)).'\n+                                 'Input shape provided = %s' % (input_shape,))\n+\n+    if K.image_data_format() != 'channels_last':\n+        warnings.warn('The MobileNet family of models is only available '\n+                      'for the input data format \"channels_last\" '\n+                      '(width, height, channels). '\n+                      'However your settings specify the default '\n+                      'data format \"channels_first\" (channels, width, height).'\n+                      ' You should set `image_data_format=\"channels_last\"` '\n+                      'in your Keras config located at ~/.keras/keras.json. '\n+                      'The model being returned right now will expect inputs '\n+                      'to follow the \"channels_last\" data format.')\n+        K.set_image_data_format('channels_last')\n+        old_data_format = 'channels_first'\n+    else:\n+        old_data_format = None\n+\n+    if input_tensor is None:\n+        img_input = Input(shape=input_shape)\n+    else:\n+        if not K.is_keras_tensor(input_tensor):\n+            img_input = Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    first_block_filters = _make_divisible(32 * alpha, 8)\n+    x = Conv2D(first_block_filters,\n+               kernel_size=3,\n+               strides=(2, 2), padding='same',\n+               use_bias=False, name='Conv1')(img_input)\n+    x = BatchNormalization(epsilon=1e-3, momentum=0.999, name='bn_Conv1')(x)\n+    x = Activation(relu6, name='Conv1_relu')(x)\n+\n+    x = _first_inverted_res_block(x,\n+                                  filters=16,\n+                                  alpha=alpha,\n+                                  stride=1,\n+                                  expansion=1,\n+                                  block_id=0)\n+\n+    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n+                            expansion=6, block_id=1)\n+    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n+                            expansion=6, block_id=2)\n+\n+    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n+                            expansion=6, block_id=3)\n+    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n+                            expansion=6, block_id=4)\n+    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n+                            expansion=6, block_id=5)\n+\n+    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n+                            expansion=6, block_id=6)\n+    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n+                            expansion=6, block_id=7)\n+    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n+                            expansion=6, block_id=8)\n+    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n+                            expansion=6, block_id=9)\n+\n+    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n+                            expansion=6, block_id=10)\n+    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n+                            expansion=6, block_id=11)\n+    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n+                            expansion=6, block_id=12)\n+\n+    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n+                            expansion=6, block_id=13)\n+    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n+                            expansion=6, block_id=14)\n+    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n+                            expansion=6, block_id=15)\n+\n+    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n+                            expansion=6, block_id=16)\n+\n+    # no alpha applied to last conv as stated in the paper:\n+    # if the width multiplier is greater than 1 we\n+    # increase the number of output channels\n+    if alpha > 1.0:\n+        last_block_filters = _make_divisible(1280 * alpha, 8)\n+    else:\n+        last_block_filters = 1280\n+\n+    x = Conv2D(last_block_filters,\n+               kernel_size=1,\n+               use_bias=False,\n+               name='Conv_1')(x)\n+    x = BatchNormalization(epsilon=1e-3, momentum=0.999, name='Conv_1_bn')(x)\n+    x = Activation(relu6, name='out_relu')(x)\n+\n+    if include_top:\n+        x = GlobalAveragePooling2D()(x)\n+        x = Dense(classes, activation='softmax',\n+                  use_bias=True, name='Logits')(x)\n+\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    # Create model.\n+    model = Model(inputs, x, name='mobilenetv2_%0.2f_%s' % (alpha, rows))\n+\n+    # load weights\n+    if weights == 'imagenet':\n+        if K.image_data_format() == 'channels_first':\n+            raise ValueError('Weights for \"channels_first\" format '\n+                             'are not available.')\n+\n+        if include_top:\n+            model_name = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + \\\n+                str(alpha) + '_' + str(rows) + '.h5'\n+            weigh_path = BASE_WEIGHT_PATH + model_name\n+            weights_path = get_file(model_name, weigh_path,\n+                                    cache_subdir='models')\n+        else:\n+            model_name = 'mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' + \\\n+                str(alpha) + '_' + str(rows) + '_no_top' + '.h5'\n+            weigh_path = BASE_WEIGHT_PATH + model_name\n+            weights_path = get_file(model_name, weigh_path,\n+                                    cache_subdir='models')\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    if old_data_format:\n+        K.set_image_data_format(old_data_format)\n+    return model\n+\n+\n+def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n+    in_channels = inputs._keras_shape[-1]\n+    prefix = 'features.' + str(block_id) + '.conv.'\n+    pointwise_conv_filters = int(filters * alpha)\n+    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n+    # Expand\n+\n+    x = Conv2D(expansion * in_channels, kernel_size=1, padding='same',\n+               use_bias=False, activation=None,\n+               name='mobl%d_conv_expand' % block_id)(inputs)\n+    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n+                           name='bn%d_conv_bn_expand' %\n+                           block_id)(x)\n+    x = Activation(relu6, name='conv_%d_relu' % block_id)(x)\n+\n+    # Depthwise\n+    x = DepthwiseConv2D(kernel_size=3, strides=stride, activation=None,\n+                        use_bias=False, padding='same',\n+                        name='mobl%d_conv_depthwise' % block_id)(x)\n+    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n+                           name='bn%d_conv_depthwise' % block_id)(x)\n+\n+    x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)\n+\n+    # Project\n+    x = Conv2D(pointwise_filters,\n+               kernel_size=1, padding='same', use_bias=False, activation=None,\n+               name='mobl%d_conv_project' % block_id)(x)\n+    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n+                           name='bn%d_conv_bn_project' % block_id)(x)\n+\n+    if in_channels == pointwise_filters and stride == 1:\n+        return Add(name='res_connect_' + str(block_id))([inputs, x])\n+\n+    return x\n+\n+\n+def _first_inverted_res_block(inputs,\n+                              expansion, stride,\n+                              alpha, filters, block_id):\n+    in_channels = inputs._keras_shape[-1]\n+    prefix = 'features.' + str(block_id) + '.conv.'\n+    pointwise_conv_filters = int(filters * alpha)\n+    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n+\n+    # Depthwise\n+    x = DepthwiseConv2D(kernel_size=3,\n+                        strides=stride, activation=None,\n+                        use_bias=False, padding='same',\n+                        name='mobl%d_conv_depthwise' %\n+                        block_id)(inputs)\n+    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n+                           name='bn%d_conv_depthwise' %\n+                           block_id)(x)\n+    x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)\n+\n+    # Project\n+    x = Conv2D(pointwise_filters,\n+               kernel_size=1,\n+               padding='same',\n+               use_bias=False,\n+               activation=None,\n+               name='mobl%d_conv_project' %\n+               block_id)(x)\n+    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n+                           name='bn%d_conv_project' %\n+                           block_id)(x)\n+\n+    if in_channels == pointwise_filters and stride == 1:\n+        return Add(name='res_connect_' + str(block_id))([inputs, x])\n+\n+    return x\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR demonstrates significant progress, but there are still issues to address. You've resolved PEP8 violations and adjusted class counts from 1001 to 1000, which is good. However, pay attention to the failed tests: they remain unrelated to your changes. Ensure you're fully testing changes, especially around model loading. Additionally, clarify class discrepancies due to background classes in TensorFlow and provide better guidance for future unit test integration. Overall, you're on the right track, but focus on consolidating test results and improving documentation for ease of use."
      }
    ],
    "codediff_length": 22869
  },
  {
    "index": 41,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/keras/layers/containers.py b/keras/layers/containers.py\nindex 39619ef25f12..9970c90657e6 100644\n--- a/keras/layers/containers.py\n+++ b/keras/layers/containers.py\n@@ -277,6 +277,64 @@ def add_node(self, layer, name, input=None, inputs=[],\n         if create_output:\n             self.add_output(name, input=name)\n \n+    def add_shared_node(self, layer, name, inputs=[], merge_mode=None, concat_axis=-1, dot_axes=-1, outputs=[], create_output=False):\n+\n+        if name in self.namespace:\n+            raise Exception('Duplicate node identifier: ' + name)\n+        for o in outputs:\n+            if o in self.namespace:\n+                raise Exception('Duplicate node identifier: ' + o)\n+        if merge_mode:\n+            if merge_mode not in {'sum', 'ave', 'mul', 'dot', 'cos', 'concat', 'join'}:\n+                raise Eception(\"Invalid merge mode\")\n+        layers = []\n+        for i in range(len(inputs)):\n+            input = inputs[i]\n+            if input in self.nodes:\n+                n = self.nodes[input]\n+                if hasattr(n, 'get_output_at'):#is it a siamese layer?\n+                    if n.merge_mode is None:\n+                        for j in range(len(n.inputs)):\n+                            sh = SiameseHead(j)\n+                            sh.previous = n\n+                            layers.append(sh)\n+                    else:\n+                        layers.append(n)\n+            elif input in self.inputs:\n+                n = self.inputs[input]\n+                layers.append(n)\n+            else:\n+                raise Exception('Unknown identifier: ' + n)\n+        s = Siamese(layer, layers, merge_mode, concat_axis=concat_axis, dot_axes=dot_axes)\n+        s.set_name(name)\n+        self.namespace.add(name)\n+        self.nodes[name] = s\n+        self.node_config.append({'name': name,\n+                                'inputs': inputs,\n+                                'merge_mode': merge_mode,\n+                                'concat_axis': concat_axis,\n+                                'dot_axes': dot_axes,\n+                                'create_output': create_output if merge_mode else False})\n+        if not merge_mode:\n+            for i in range(len(outputs)):\n+                sh = SiameseHead(i)\n+                sh.previous = s\n+                sh_name = outputs[i]\n+                sh.set_name(sh_name)\n+                self.namespace.add(sh_name)\n+                self.nodes[sh_name] = sh\n+                self.node_config.append({'name': sh_name,\n+                                        'inputs': [s],\n+                                        'create_output': create_output})\n+                if create_output:\n+                    self.add_output(sh_name, input=sh_name)\n+    \n+        if create_output and merge_mode:\n+            if merge_mode == 'join':\n+                raise Exception(\"Output can not be of type OrderedDict\")\n+            self.add_output(name, input=name)\n+\n+\n     def add_output(self, name, input=None, inputs=[],\n                    merge_mode='concat', concat_axis=-1, dot_axes=-1):\n         if name in self.output_order:\ndiff --git a/keras/layers/core.py b/keras/layers/core.py\nindex 7663ef421d84..13b417fa38e5 100644\n--- a/keras/layers/core.py\n+++ b/keras/layers/core.py\n@@ -940,3 +966,406 @@ def get_config(self):\n                   \"input_dim\": self.input_dim}\n         base_config = super(MaxoutDense, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class Lambda(Layer):\n+\n+    \"\"\"Lambda layer for evaluating arbitrary function\n+\n+    Input shape\n+    -----------\n+    output_shape of previous layer\n+\n+    Output shape\n+    ------------\n+    Specified by output_shape argument\n+\n+    Arguments\n+    ---------\n+    function - The function to be evaluated. Takes one argument : output of previous layer\n+    output_shape - Expected output shape from function. Could be a tuple or a function of the shape of the input\n+    \"\"\"\n+\n+    def __init__(self, function, output_shape=None, **kwargs):\n+        super(Lambda, self).__init__(**kwargs)\n+        py3 = sys.version_info[0] == 3\n+        if py3:\n+            self.function = marshal.dumps(function.__code__)\n+        else:\n+            self.function = marshal.dumps(function.func_code)\n+        if output_shape is None:\n+            self._output_shape = None\n+        elif type(output_shape) in {tuple, list} :\n+            self._output_shape = tuple(output_shape)\n+        else:\n+            if py3:\n+                self._output_shape = marshal.dumps(output_shape.__code__)\n+            else:\n+                self._output_shape = marshal.dumps(output_shape.func_code)\n+\n+    @property\n+    def output_shape(self):\n+        if self._output_shape is None:\n+            return self.input_shape\n+        elif type(self._output_shape) == tuple:\n+            return (self.input_shape[0], ) + self._output_shape\n+        else:\n+            output_shape_func = marshal.loads(self._output_shape)\n+            output_shape_func = types.FunctionType(output_shape_func, globals())\n+            shape = output_shape_func(self.previous.output_shape)\n+            if type(shape) not in {list, tuple}:\n+                raise Exception(\"output_shape function must return a tuple\")\n+            return tuple(shape)\n+\n+    def get_output(self, train=False):\n+        func = marshal.loads(self.function)\n+        func = types.FunctionType(func, globals())\n+        if hasattr(self, 'previous'):\n+            return func(self.previous.get_output(train))\n+        else:\n+            return func(self.input)\n+\n+\n+class MaskedLambda(MaskedLayer, Lambda):\n+    pass\n+\n+\n+class LambdaMerge(Lambda):\n+    \"\"\"LambdaMerge layer for evaluating arbitrary function over multiple inputs\n+\n+    Input shape\n+    -----------\n+    None\n+\n+    Output shape\n+    ------------\n+    Specified by output_shape argument\n+\n+    Arguments\n+    ---------\n+    layers - Input layers. Similar to layers argument of Merge\n+    function - The function to be evaluated. Takes one argument : list of outputs from input layers\n+    output_shape - Expected output shape from function. Could be a tuple or a function of list of input shapes\n+    \"\"\"\n+    def __init__(self, layers, function, output_shape=None):\n+        if len(layers) < 2:\n+            raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n+        self.layers = layers\n+        self.params = []\n+        self.regularizers = []\n+        self.constraints = []\n+        self.updates = []\n+        for l in self.layers:\n+            params, regs, consts, updates = l.get_params()\n+            self.regularizers += regs\n+            self.updates += updates\n+            # params and constraints have the same size\n+            for p, c in zip(params, consts):\n+                if p not in self.params:\n+                    self.params.append(p)\n+                    self.constraints.append(c)\n+        py3 = sys.version_info[0] == 3\n+        if py3:\n+            self.function = marshal.dumps(function.__code__)\n+        else:\n+            self.function = marshal.dumps(function.func_code)\n+        if output_shape is None:\n+            self._output_shape = None\n+        elif type(output_shape) in {tuple, list}:\n+            self._output_shape = tuple(output_shape)\n+        else:\n+            if py3:\n+                self._output_shape = marshal.dumps(output_shape.__code__)\n+            else:\n+                self._output_shape = marshal.dumps(output_shape.func_code)\n+\n+    @property\n+    def output_shape(self):\n+        input_shapes = [layer.output_shape for layer in self.layers]\n+        if self._output_shape is None:\n+            return input_shapes[0]\n+        elif type(self._output_shape) == tuple:\n+            return (input_shapes[0][0], ) + self._output_shape\n+        else:\n+            output_shape_func = marshal.loads(self._output_shape)\n+            output_shape_func = types.FunctionType(output_shape_func, globals())\n+            shape = output_shape_func(input_shapes)\n+            if type(shape) not in {list, tuple}:\n+                raise Exception(\"output_shape function must return a tuple\")\n+            return tuple(shape)\n+\n+    def get_params(self):\n+        return self.params, self.regularizers, self.constraints, self.updates\n+\n+    def get_output(self, train=False):\n+        func = marshal.loads(self.function)\n+        func = types.FunctionType(func, globals())\n+        inputs = [layer.get_output(train) for layer in self.layers]\n+        return func(inputs)\n+\n+    def get_input(self, train=False):\n+        res = []\n+        for i in range(len(self.layers)):\n+            o = self.layers[i].get_input(train)\n+            if not type(o) == list:\n+                o = [o]\n+            for output in o:\n+                if output not in res:\n+                    res.append(output)\n+        return res\n+\n+    @property\n+    def input(self):\n+        return self.get_input()\n+\n+    def supports_masked_input(self):\n+        return False\n+\n+    def get_output_mask(self, train=None):\n+        return None\n+\n+    def get_weights(self):\n+        weights = []\n+        for l in self.layers:\n+            weights += l.get_weights()\n+        return weights\n+\n+    def set_weights(self, weights):\n+        for i in range(len(self.layers)):\n+            nb_param = len(self.layers[i].params)\n+            self.layers[i].set_weights(weights[:nb_param])\n+            weights = weights[nb_param:]\n+\n+    def get_config(self):\n+        config = {\"name\": self.__class__.__name__,\n+                  \"layers\": [l.get_config() for l in self.layers],\n+                  \"function\": self.function,\n+                  \"output_shape\": self._output_shape\n+                  }\n+        base_config = super(LambdaMerge, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+class Siamese(Layer):\n+    def __init__(self, layer, inputs, merge_mode=None, concat_axis=1, dot_axes=-1):\n+\n+        if merge_mode not in ['sum', 'mul', 'concat', 'ave', 'join', 'cos', 'dot', None]:\n+            raise Exception(\"Invalid merge mode: \" + str(mode))\n+\n+        if merge_mode in {'cos', 'dot'}:\n+            if len(inputs) > 2:\n+                raise Exception(mode + \" merge takes exactly 2 layers\")\n+            shape1 = inputs[0].output_shape\n+            shape2 = inputs[1].output_shape\n+            n1 = len(shape1)\n+            n2 = len(shape2)\n+            if mode == 'dot':\n+                if type(dot_axes) == int:\n+                    if dot_axes < 0:\n+                        dot_axes = [range(dot_axes % n1, n1), range(dot_axes % n2, n2)]\n+                    else:\n+                        dot_axes = [range(n1 - dot_axes, n2), range(1, dot_axes + 1)]\n+                for i in range(len(dot_axes[0])):\n+                    if shape1[dot_axes[0][i]] != shape2[dot_axes[1][i]]:\n+                        raise Exception(\" Dot incompatible layers can not be merged using dot mode\")\n+\n+        self.layer = layer\n+        self.inputs = inputs\n+        self.params = []\n+        self.merge_mode = merge_mode\n+        self.concat_axis = concat_axis\n+        self.dot_axes = dot_axes\n+        layer.set_previous(inputs[0])\n+        self.regularizers = []\n+        self.constraints = []\n+        self.updates = []\n+        layers = [layer] + inputs\n+        for l in layers:\n+            params, regs, consts, updates = l.get_params()\n+            self.regularizers += regs\n+            self.updates += updates\n+            # params and constraints have the same size\n+            for p, c in zip(params, consts):\n+                if p not in self.params:\n+                    self.params.append(p)\n+                    self.constraints.append(c)\n+\n+    @property\n+    def output_shape(self):\n+        if merge_mode is None:\n+            return self.layer.output_shape\n+        input_shapes = [self.layer.output_shape]*len(self.inputs)\n+        if self.merge_mode in ['sum', 'mul', 'ave']:\n+            return input_shapes[0]\n+        elif self.merge_mode == 'concat':\n+            output_shape = list(input_shapes[0])\n+            for shape in input_shapes[1:]:\n+                output_shape[self.concat_axis] += shape[self.concat_axis]\n+            return tuple(output_shape)\n+        elif self.merge_mode == 'join':\n+            return None\n+        elif self.merge_mode == 'dot':\n+            shape1 = list(input_shapes[0])\n+            shape2 = list(input_shapes[1])\n+            for i in self.dot_axes[0]:\n+                shape1.pop(i)\n+            for i in self.dot_axes[1]:\n+                shape2.pop(i)\n+            shape = shape1 + shape2[1:]\n+            if len(shape) == 1:\n+                shape.append(1)\n+            return tuple(shape)\n+        elif self.merge_mode == 'cos':\n+            return tuple(input_shapes[0][0], 1)\n+\n+    def get_params(self):\n+        return self.params, self.regularizers, self.constraints, self.updates\n+\n+    def get_output_at(self, head, train=False):\n+        self.layer.set_previous(self.inputs[head])\n+        return self.layer.get_output(train)\n+\n+    def get_output_join(self, train=False):\n+        o = OrderedDict()\n+        for i in range(len(inputs)):\n+            X = self.get_output_at(i, train)\n+            if X.name is None:\n+                raise ValueError(\"merge_mode='join' only works with named inputs\")\n+            o[X.name] = X\n+        return o\n+\n+    def get_output_sum(self, train=False):\n+        s = self.get_output_at(0, train)\n+        for i in range(1, len(self.inputs)):\n+            s += self.get_output_at(i, train)\n+        return s\n+\n+    def get_output_ave(self, train=False):\n+        n = len(self.inputs)\n+        s = self.get_output_at(0, train)\n+        for i in range(1, n):\n+            s += self.get_output_at(i, train)\n+        s /= n\n+        return s\n+\n+    def get_output_concat(self, train=False):\n+        inputs = [self.get_output_at(i, train) for i in range(len(self.inputs))]\n+        return T.concatenate(inputs, axis=self.concat_axis)\n+\n+    def get_output_mul(self, train=False):\n+        s = self.get_output_at(0, train)\n+        for i in range(1, len(self.inputs)):\n+            s *= self.get_output_at(i, train)\n+        return s\n+\n+    def get_output_dot(self, train=False):\n+        l1 = self.get_output_at(0, train)\n+        l2 = self.get_output_at(1, train)\n+        output = T.batched_tensordot(l1, l2, self.dot_axes)\n+        output = output.dimshuffle((0, 'x'))\n+        return output\n+\n+    def get_output_cos(self, train=False):\n+        l1 = self.get_output_at(0, train)\n+        l2 = self.get_output_at(1, train)\n+        output, _ = theano.scan(lambda v1, v2: T.dot(v1, v2)/T.sqrt(T.dot(v1, v1) * T.dot(v2, v2)), sequences=[l1, l2], outputs_info=None)\n+        output = output.dimshuffle((0, 'x'))\n+        return output\n+\n+    def get_output(self, train=False):\n+        mode = self.merge_mode\n+        if mode == 'join':\n+            return self.get_output_join(train)\n+        elif mode == 'concat':\n+            return self.get_output_concat(train)\n+        elif mode == 'sum':\n+            return self.get_output_sum(train)\n+        elif mode == 'ave':\n+            return self.get_output_ave(train)\n+        elif mode == 'mul':\n+            return self.get_output_mul(train)\n+        elif mode == 'dot':\n+            return self.get_output_dot(train)\n+        elif mode == 'cos':\n+            return self.get_output_dot(train)\n+\n+    def get_input(self, train=False):\n+        res = []\n+        for i in range(len(self.inputs)):\n+            o = self.inputs[i].get_input(train)\n+            if not type(o) == list:\n+                o = [o]\n+            for output in o:\n+                if output not in res:\n+                    res.append(output)\n+        return res\n+\n+    @property\n+    def input(self):\n+        return self.get_input()\n+\n+    def supports_masked_input(self):\n+        return False\n+\n+    def get_output_mask(self, train=None):\n+        return None\n+\n+    def get_weights(self):\n+        weights = layer.get_weights()\n+        for m in self.inputs:\n+            weights += m.get_weights()\n+        return weights\n+\n+    def set_weights(self, weights):\n+        nb_param = len(self.layer.params)\n+        self.layer.set_weights(weights[:nb_param])\n+        weights = weights[nb_param:]\n+        for i in range(len(self.inputs)):\n+            nb_param = len(self.inputs[i].params)\n+            self.inputs[i].set_weights(weights[:nb_param])\n+            weights = weights[nb_param:]\n+\n+    def get_config(self):\n+\n+        config = {\"name\": self.__class__.__name__,\n+                  \"layer\": self.layer.get_config,\n+                  \"inputs\": [m.get_config() for m in self.inputs],\n+                  \"merge_mode\": self.merge_mode,\n+                  \"concat_axis\": self.concat_axis,\n+                  \"dot_axes\": self.dot_axes\n+                  }\n+        base_config = super(Siamese, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+class SiameseHead(Layer):\n+    def __init__(self, head):\n+        self.head = head\n+        self.params = []\n+    def get_output(self, train=False):\n+        return self.get_input(train)\n+\n+    @property\n+    def input_shape(self):\n+        return self.previous.layer.output_shape\n+\n+    def get_input(self, train=False):\n+        return self.previous.get_output_at(self.head, train)\n+\n+    def get_config(self):\n+\n+        config = {\"name\": self.__class__.__name__,\n+                  \"head\": self.head\n+                  }\n+\n+        base_config = super(SiameseHead, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+    def set_previous(self, layer):\n+        self.previous = layer\n+\n+def add_shared_layer(layer,inputs):\n+    input_layers = [l.layers[-1] for l in inputs]\n+    s = Siamese(layer, input_layers)\n+    for i in range(len(inputs)):\n+        sh = SiameseHead(i)\n+        inputs[i].add (s)\n+        inputs[i].add(sh)\n+\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR suggests an API for building Siamese networks with shared layers but raises concerns about complexity and usability. The current implementation relies on implicit layer sharing, which can lead to performance issues. Instead, a more explicit branch structure may enhance clarity. The proposed \"simply by index\" method is appealing, but it may obscure layer sharing under the hood. Additionally, integrating specific merging modes and avoiding weight duplication could increase flexibility. Evaluating these changes against real-world scenarios might prove beneficial for refining the approach."
      }
    ],
    "codediff_length": 17999
  },
  {
    "index": 42,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/docs/sources/layers/core.md b/docs/sources/layers/core.md\nindex 75b22a3cff4d..ccb5b2c8d38d 100644\n--- a/docs/sources/layers/core.md\n+++ b/docs/sources/layers/core.md\n@@ -374,3 +374,82 @@ Given an input of dimensions `(nb_samples, timesteps, input_dim)`, return the in\n - __Input shape__: 3D tensor with shape: `(nb_samples, timesteps, features)`.\n \n - __Output shape__: 3D tensor with shape: `(nb_samples, timesteps, features)`.\n+\n+## Lambda\n+```python\n+keras.layers.core.Lambda(function, output_shape=None)\n+```\n+\n+Used for evaluating an arbitrary function on the output of the previous layer. \n+\n+- __Input shape__: Output shape of the previous layer.\n+\n+- __Output shape__: Specified by the `output_shape` argument.\n+\n+- __Arguments__:\n+\n+    - __function__: The funtion to be evaluated. Takes one argument: output of the previous layer.\n+    - __output_shape__: Shape of the tensor returned by `function`. Should be a tuple or a function of input shape(excluding samples dimension).\n+\n+- __Example__:\n+\n+```python\n+#custom softmax function\n+def sharp_softmax(X,beta=1.5):\n+    return theano.tensor.nnet.softmax(X*beta)\n+\n+def output_shape(input_shape):\n+    return input_shape #shape is unchanged\n+\n+SharpSoftmax = Lambda(sharp_softmax, output_shape)\n+\n+model = Sequential()\n+model.add(Dense(input_dim=10,output_dim=10))\n+model.add(SharpSoftmax)\n+model.add(Dense(1))\n+model.add(Activation('sigmoid'))\n+```\n+\n+\n+## LambdaMerge\n+```python\n+keras.layers.core.LambdaMerge(layers, function, output_shape=None)\n+```\n+\n+Merge the output of a list of layers (or containers) into a single tensor, using an arbitrary function.\n+\n+- __Arguments__:\n+    - __layers__: List of layers or [containers](/layers/containers/).\n+    - __function__: The function to be evaluated. Takes one argument: list of outputs from input layers. \n+    - __output_shape__: Shape of the tensor returned by `function`. Should be a tuple or a function of list of input shapes.\n+- __Example__:\n+\n+```python\n+#root mean square function\n+def rms(X):\n+    s = X[0]**2\n+    for i in range(1, len(X)):\n+        s += X[i]**2\n+    s /= len(X)\n+    s = theano.tensor.sqrt(s)\n+    return s\n+\n+def output_shape(input_shapes):\n+    return input_shapes[0]\n+\n+left = Sequential()\n+left.add(Dense(input_dim = 10,output_dim = 10))\n+left.add(Activation('sigmoid'))\n+\n+right = Sequential()\n+right.add(Dense(input_dim = 10,output_dim = 10))\n+right.add(Activation('sigmoid'))\n+\n+model = Sequential()\n+model.add(LambdaMerge([left,right], rms, output_shape))\n+\n+model.add(Dense(1))\n+model.add(Activation('sigmoid'))\n+```\n+\n+---\ndiff --git a/keras/layers/core.py b/keras/layers/core.py\nindex 81a7701cccbe..d20964003170 100644\n--- a/keras/layers/core.py\n+++ b/keras/layers/core.py\n@@ -899,3 +903,181 @@ def get_config(self):\n                   \"input_dim\": self.input_dim}\n         base_config = super(MaxoutDense, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class Lambda(Layer):\n+\n+    \"\"\"Lambda layer for evaluating arbitrary function\n+\n+    Input shape\n+    -----------\n+    output_shape of previous layer\n+\n+    Output shape\n+    ------------\n+    Specified by output_shape argument\n+\n+    Arguments\n+    ---------\n+    function - The function to be evaluated. Takes one argument : output of previous layer\n+    output_shape - Expected output shape from function. Could be a tuple or a function of the shape of the input\n+    \"\"\"\n+\n+    def __init__(self, function, output_shape=None):\n+        super(Lambda, self).__init__()\n+        py3 = sys.version_info[0] == 3\n+        if py3:\n+            self.function = marshal.dumps(function.__code__)\n+        else:\n+            self.function = marshal.dumps(function.func_code)\n+        if output_shape is None:\n+            self._output_shape = None\n+        elif type(output_shape) in {tuple, list} :\n+            self._output_shape = tuple(output_shape)\n+        else:\n+            if py3:\n+                self._output_shape = marshal.dumps(output_shape.__code__)\n+            else:\n+                self._output_shape = marshal.dumps(output_shape.func_code)\n+\n+    @property\n+    def output_shape(self):\n+        if self._ouput_shape is None:\n+            return self.input_shape\n+        elif type(self._output_shape) == tuple:\n+            return (self.input_shape[0], ) + self._output_shape\n+        else:\n+            output_shape_func = marshal.loads(self._output_shape)\n+            output_shape_func = types.FunctionType(output_shape_func, globals())\n+            shape = output_shape_func(self.previous.output_shape[1:])\n+            if type(shape) not in {list, tuple}:\n+                raise Exception(\"output_shape function must return a tuple\")\n+            return (self.input_shape[0], ) + tuple(shape)\n+\n+    def get_output(self, train=False):\n+        func = marshal.loads(self.function)\n+        func = types.FunctionType(func, globals())\n+        if hasattr(self, 'previous'):\n+            return func(self.previous.get_output(train))\n+        else:\n+            return func(self.input)\n+\n+\n+class MaskedLambda(MaskedLayer, Lambda):\n+    pass\n+\n+\n+class LambdaMerge(Lambda):\n+    \"\"\"LambdaMerge layer for evaluating arbitrary function over multiple inputs\n+\n+    Input shape\n+    -----------\n+    None\n+\n+    Output shape\n+    ------------\n+    Specified by output_shape argument\n+\n+    Arguments\n+    ---------\n+    layers - Input layers. Similar to layers argument of Merge\n+    function - The function to be evaluated. Takes one argument : list of outputs from input layers\n+    output_shape - Expected output shape from function. Could be a tuple or a function of list of input shapes\n+    \"\"\"\n+    def __init__(self, layers, function, output_shape=None):\n+        if len(layers) < 2:\n+            raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n+        self.layers = layers\n+        self.params = []\n+        self.regularizers = []\n+        self.constraints = []\n+        self.updates = []\n+        for l in self.layers:\n+            params, regs, consts, updates = l.get_params()\n+            self.regularizers += regs\n+            self.updates += updates\n+            # params and constraints have the same size\n+            for p, c in zip(params, consts):\n+                if p not in self.params:\n+                    self.params.append(p)\n+                    self.constraints.append(c)\n+        py3 = sys.version_info[0] == 3\n+        if py3:\n+            self.function = marshal.dumps(function.__code__)\n+        else:\n+            self.function = marshal.dumps(function.func_code)\n+        if output_shape is None:\n+            self._output_shape = None\n+        elif type(output_shape) in {tuple, list}:\n+            self._output_shape = tuple(output_shape)\n+        else:\n+            if py3:\n+                self._output_shape = marshal.dumps(output_shape.__code__)\n+            else:\n+                self._output_shape = marshal.dumps(output_shape.func_code)\n+\n+    @property\n+    def output_shape(self):\n+        input_shapes = [layer.output_shape for layer in self.layers]\n+        if self._output_shape is None:\n+            return input_shapes[0]\n+        elif type(self._output_shape) == tuple:\n+            return (input_shapes[0][0], ) + self._output_shape\n+        else:\n+            output_shape_func = marshal.loads(self._output_shape)\n+            output_shape_func = types.FunctionType(output_shape_func, globals())\n+            shape = output_shape_func([shape[1:] for shape in input_shapes])\n+            if type(shape) not in {list, tuple}:\n+                raise Exception(\"output_shape function must return a tuple\")\n+            return (input_shapes[0][0], ) + tuple(shape)\n+\n+    def get_params(self):\n+        return self.params, self.regularizers, self.constraints, self.updates\n+\n+    def get_output(self, train=False):\n+        func = marshal.loads(self.function)\n+        func = types.FunctionType(func, globals())\n+        inputs = [layer.get_output(train) for layer in self.layers]\n+        return func(inputs)\n+\n+    def get_input(self, train=False):\n+        res = []\n+        for i in range(len(self.layers)):\n+            o = self.layers[i].get_input(train)\n+            if not type(o) == list:\n+                o = [o]\n+            for output in o:\n+                if output not in res:\n+                    res.append(output)\n+        return res\n+\n+    @property\n+    def input(self):\n+        return self.get_input()\n+\n+    def supports_masked_input(self):\n+        return False\n+\n+    def get_output_mask(self, train=None):\n+        return None\n+\n+    def get_weights(self):\n+        weights = []\n+        for l in self.layers:\n+            weights += l.get_weights()\n+        return weights\n+\n+    def set_weights(self, weights):\n+        for i in range(len(self.layers)):\n+            nb_param = len(self.layers[i].params)\n+            self.layers[i].set_weights(weights[:nb_param])\n+            weights = weights[nb_param:]\n+\n+    def get_config(self):\n+        config = {\"name\": self.__class__.__name__,\n+                  \"layers\": [l.get_config() for l in self.layers],\n+                  \"function\": self.function,\n+                  \"output_shape\": self._output_shape\n+                  }\n+        base_config = super(LambdaMerge, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\ndiff --git a/tests/auto/test_sequential_model.py b/tests/auto/test_sequential_model.py\nindex 39d01b4f491d..d14eb7271f6a 100644\n--- a/tests/auto/test_sequential_model.py\n+++ b/tests/auto/test_sequential_model.py\n@@ -275,7 +278,83 @@ def test_merge_overlap(self):\n         nloss = model.evaluate(X_train, y_train, verbose=0)\n         print(nloss)\n         assert(loss == nloss)\n+        \n+    def test_lambda(self):\n+        print('Test lambda: sum')\n+\n+        def func(X):\n+            s = X[0]\n+            for i in range(1,len(X)):\n+                s += X[i]\n+            return s\n+            \n+        def activation(X):\n+          return theano.tensor.nnet.softmax(X)\n+          \n+        def output_shape(input_shapes):\n+            return input_shapes[0]\n \n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+        \n+        right = Sequential()\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        right.add(Activation('relu'))\n+        \n+        model = Sequential()\n+\n+        model.add(LambdaMerge([left, right], function=func, output_shape=output_shape))\n+\n+        model.add(Dense(nb_class))\n+        model.add(Lambda(activation))\n+\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+        loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.7:\n+            raise Exception('Score too low, learning issue.')\n+        preds = model.predict([X_test, X_test], verbose=0)\n+        classes = model.predict_classes([X_test, X_test], verbose=0)\n+        probas = model.predict_proba([X_test, X_test], verbose=0)\n+        print(model.get_config(verbose=1))\n+\n+        print('test weight saving')\n+        model.save_weights('temp.h5', overwrite=True)\n+        left = Sequential()\n+        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        left.add(Activation('relu'))\n+        right = Sequential()\n+        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+        right.add(Activation('relu'))\n+        model = Sequential()\n+        model.add(LambdaMerge([left, right], function=func, output_shape=output_shape))\n+        model.add(Dense(nb_class))\n+        model.add(Lambda(activation))\n+        model.load_weights('temp.h5')\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+        \n+        print ('test serializing')\n+        del func, activation # Make sure that the model has the function code, not just the function name.\n+        sys.setrecursionlimit(50000)\n+        model_str = pickle.dumps(model)\n+        model = pickle.loads(model_str)\n+        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+        \n     def test_count_params(self):\n         print('test count params')\n         input_dim = 20\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR introduces a `Lambda` and `LambdaMerge` layer with serialization support. However, consider renaming `Lambda` to `MaskedLambda` for clarity and improving its inheritance from `Layer`. Documentation should be added to clarify argument formats, as the current usage isn't intuitive. I support enforcing type checks to avoid polymorphism issues. The return of `OrderedDict` from `Merge` can enhance flexibility and API consistency but may complicate single-output scenarios. Overall, good direction, just some refinements needed!"
      }
    ],
    "codediff_length": 13286
  },
  {
    "index": 43,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/tests/keras/layers/recurrent_test.py b/tests/keras/layers/recurrent_test.py\nindex 2dce96ee67fa..19d318a060a3 100644\n--- a/tests/keras/layers/recurrent_test.py\n+++ b/tests/keras/layers/recurrent_test.py\n@@ -568,5 +568,174 @@ def test_batch_size_equal_one(layer_class):\n     model.train_on_batch(x, y)\n \n \n+def test_rnn_cell_with_constants_layer():\n+\n+    class RNNCellWithConstants(keras.layers.Layer):\n+\n+        def __init__(self, units, **kwargs):\n+            self.units = units\n+            self.state_size = units\n+            super(RNNCellWithConstants, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            if not isinstance(input_shape, list):\n+                raise TypeError('expects constants shape')\n+            [input_shape, constant_shape] = input_shape\n+            # will (and should) raise if more than one constant passed\n+\n+            self.input_kernel = self.add_weight(\n+                shape=(input_shape[-1], self.units),\n+                initializer='uniform',\n+                name='kernel')\n+            self.recurrent_kernel = self.add_weight(\n+                shape=(self.units, self.units),\n+                initializer='uniform',\n+                name='recurrent_kernel')\n+            self.constant_kernel = self.add_weight(\n+                shape=(constant_shape[-1], self.units),\n+                initializer='uniform',\n+                name='constant_kernel')\n+            self.built = True\n+\n+        def call(self, inputs, states, constants):\n+            [prev_output] = states\n+            [constant] = constants\n+            h_input = keras.backend.dot(inputs, self.input_kernel)\n+            h_state = keras.backend.dot(prev_output, self.recurrent_kernel)\n+            h_const = keras.backend.dot(constant, self.constant_kernel)\n+            output = h_input + h_state + h_const\n+            return output, [output]\n+\n+        def get_config(self):\n+            config = {'units': self.units}\n+            base_config = super(RNNCellWithConstants, self).get_config()\n+            return dict(list(base_config.items()) + list(config.items()))\n+\n+    # Test basic case.\n+    x = keras.Input((None, 5))\n+    c = keras.Input((3,))\n+    cell = RNNCellWithConstants(32)\n+    layer = recurrent.RNN(cell)\n+    y = layer(x, constants=c)\n+    model = keras.models.Model([x, c], y)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(\n+        [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n+        np.zeros((6, 32))\n+    )\n+\n+    # Test basic case serialization.\n+    x_np = np.random.random((6, 5, 5))\n+    c_np = np.random.random((6, 3))\n+    y_np = model.predict([x_np, c_np])\n+    weights = model.get_weights()\n+    config = layer.get_config()\n+    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n+    with keras.utils.CustomObjectScope(custom_objects):\n+        layer = recurrent.RNN.from_config(config.copy())\n+    y = layer(x, constants=c)\n+    model = keras.models.Model([x, c], y)\n+    model.set_weights(weights)\n+    y_np_2 = model.predict([x_np, c_np])\n+    assert_allclose(y_np, y_np_2, atol=1e-4)\n+\n+    # test flat list inputs\n+    with keras.utils.CustomObjectScope(custom_objects):\n+        layer = recurrent.RNN.from_config(config.copy())\n+    y = layer([x, c])\n+    model = keras.models.Model([x, c], y)\n+    model.set_weights(weights)\n+    y_np_3 = model.predict([x_np, c_np])\n+    assert_allclose(y_np, y_np_3, atol=1e-4)\n+\n+\n+def test_rnn_cell_with_constants_layer_passing_initial_state():\n+\n+    class RNNCellWithConstants(keras.layers.Layer):\n+\n+        def __init__(self, units, **kwargs):\n+            self.units = units\n+            self.state_size = units\n+            super(RNNCellWithConstants, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            if not isinstance(input_shape, list):\n+                raise TypeError('expects constants shape')\n+            [input_shape, constant_shape] = input_shape\n+            # will (and should) raise if more than one constant passed\n+\n+            self.input_kernel = self.add_weight(\n+                shape=(input_shape[-1], self.units),\n+                initializer='uniform',\n+                name='kernel')\n+            self.recurrent_kernel = self.add_weight(\n+                shape=(self.units, self.units),\n+                initializer='uniform',\n+                name='recurrent_kernel')\n+            self.constant_kernel = self.add_weight(\n+                shape=(constant_shape[-1], self.units),\n+                initializer='uniform',\n+                name='constant_kernel')\n+            self.built = True\n+\n+        def call(self, inputs, states, constants):\n+            [prev_output] = states\n+            [constant] = constants\n+            h_input = keras.backend.dot(inputs, self.input_kernel)\n+            h_state = keras.backend.dot(prev_output, self.recurrent_kernel)\n+            h_const = keras.backend.dot(constant, self.constant_kernel)\n+            output = h_input + h_state + h_const\n+            return output, [output]\n+\n+        def get_config(self):\n+            config = {'units': self.units}\n+            base_config = super(RNNCellWithConstants, self).get_config()\n+            return dict(list(base_config.items()) + list(config.items()))\n+\n+    # Test basic case.\n+    x = keras.Input((None, 5))\n+    c = keras.Input((3,))\n+    s = keras.Input((32,))\n+    cell = RNNCellWithConstants(32)\n+    layer = recurrent.RNN(cell)\n+    y = layer(x, initial_state=s, constants=c)\n+    model = keras.models.Model([x, s, c], y)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(\n+        [np.zeros((6, 5, 5)), np.zeros((6, 32)), np.zeros((6, 3))],\n+        np.zeros((6, 32))\n+    )\n+\n+    # Test basic case serialization.\n+    x_np = np.random.random((6, 5, 5))\n+    s_np = np.random.random((6, 32))\n+    c_np = np.random.random((6, 3))\n+    y_np = model.predict([x_np, s_np, c_np])\n+    weights = model.get_weights()\n+    config = layer.get_config()\n+    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n+    with keras.utils.CustomObjectScope(custom_objects):\n+        layer = recurrent.RNN.from_config(config.copy())\n+    y = layer(x, initial_state=s, constants=c)\n+    model = keras.models.Model([x, s, c], y)\n+    model.set_weights(weights)\n+    y_np_2 = model.predict([x_np, s_np, c_np])\n+    assert_allclose(y_np, y_np_2, atol=1e-4)\n+\n+    # verify that state is used\n+    y_np_2_different_s = model.predict([x_np, s_np + 10., c_np])\n+    with pytest.raises(AssertionError):\n+        assert_allclose(y_np, y_np_2_different_s, atol=1e-4)\n+\n+    # test flat list inputs\n+    with keras.utils.CustomObjectScope(custom_objects):\n+        layer = recurrent.RNN.from_config(config.copy())\n+    y = layer([x, s, c])\n+    model = keras.models.Model([x, s, c], y)\n+    model.set_weights(weights)\n+    y_np_3 = model.predict([x_np, s_np, c_np])\n+    assert_allclose(y_np, y_np_3, atol=1e-4)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR proposes integrating constants as inputs to the RNN layer, emphasizing that these should be handled similarly to initial states, allowing consistent behavior across both training and inference. Current suggestions for passing these constants via the `__call__` method conflict with Keras principles, as all inputs should be explicitly passed. Consideration for a dedicated `AttentionRNN` class was mentioned, but maintaining backward compatibility with `RNN` is preferred. Overall, the feedback emphasizes clarity, performance, and coherent API design without compromising existing functionalities."
      }
    ],
    "codediff_length": 7019
  },
  {
    "index": 44,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/keras/src/quantizers/quantizers.py b/keras/src/quantizers/quantizers.py\nindex 3e4aac181e12..2f7db0c9787b 100644\n--- a/keras/src/quantizers/quantizers.py\n+++ b/keras/src/quantizers/quantizers.py\n@@ -127,6 +128,142 @@ def get_config(self):\n         }\n \n \n+def adjust_and_nudge(min_range, max_range, num_bits, narrow_range):\n+    \"\"\"Adjusts and nudges the quantization range for better accuracy.\"\"\"\n+\n+    quant_max = ops.cast(ops.subtract(ops.power(2, num_bits), 1.0), \"float32\")\n+\n+    quant_min = ops.cast(0.0 if not narrow_range else 1.0, \"float32\")\n+\n+    # Calculate the scale and ensure it's positive\n+    scale = ops.divide(\n+        ops.subtract(max_range, min_range), ops.subtract(quant_max, quant_min)\n+    )\n+\n+    inv_scale = ops.reciprocal(scale)\n+\n+    # Calculate the zero point from the min range\n+    zero_point_from_min = quant_min - ops.divide(min_range, scale)\n+\n+    # Ensure zero point is within valid range [0, quant_max]\n+    zero_point = ops.clip(zero_point_from_min, quant_min, quant_max)\n+\n+    # Nudge zero point if it's very close to an integer\n+    nudged_zero_point = ops.round(zero_point)\n+\n+    # Calculate nudged limits\n+    nudged_min = ops.multiply(ops.subtract(quant_min, nudged_zero_point), scale)\n+    nudged_max = ops.multiply(ops.subtract(quant_max, nudged_zero_point), scale)\n+\n+    return nudged_min, nudged_max, scale, inv_scale\n+\n+\n+@keras_export(\"keras.quantizers.fake_quant_with_min_max_vars_per_channel\")\n+def fake_quant_with_min_max_vars(\n+    inputs,\n+    min_vals,\n+    max_vals,\n+    num_bits,\n+    narrow_range=False,\n+    axis=None,\n+):\n+    \"\"\"\n+    Perform per-tensor or per-channel fake quantization.\n+\n+    `[min_vals, max_vals]` define the clamping range for the `inputs`.\n+\n+    The `inputs` are quantized into the quantization range:\n+    - `[0, 2^num_bits - 1]` when `narrow_range=False`\n+    - `[1, 2^num_bits - 1]` when `narrow_range=True`\n+\n+    After quantization, the values are dequantized and output as floats within\n+    the `[min_vals, max_vals]` interval.\n+\n+    This operation supports gradient computation, allowing `min_vals` and\n+    `max_vals` to be trained.\n+\n+    Args:\n+        inputs: Input tensor of float dtype.\n+        min_vals: A global minimum scalar or a per-channel minimum tensor.\n+        max_vals: A global maximum scalar or a per-channel maximum tensor.\n+        num_bits: Quantization bit width (e.g., `8` for int8).\n+        narrow_range: Whether to use narrow quantization range.\n+        axis: Axis along which to perform per-channel quantization. If `None`,\n+              per-tensor quantization is performed. Defaults to `None`.\n+\n+\n+    Returns:\n+        Fake-quantized tensor\n+    \"\"\"\n+    inputs = ops.convert_to_tensor(inputs)\n+    min_vals = ops.convert_to_tensor(min_vals)\n+    max_vals = ops.convert_to_tensor(max_vals)\n+\n+    if axis is not None:\n+        axis = canonicalize_axis(axis, inputs.ndim)\n+\n+    @ops.custom_gradient\n+    def _fake_quant_with_min_max_vars_per_channel(x, min_val, max_val):\n+        # Calculate quantization parameters for all channels at once\n+        nudged_min, nudged_max, scale, inv_scale = adjust_and_nudge(\n+            min_val, max_val, num_bits, narrow_range\n+        )\n+\n+        quant_zero = ops.floor(\n+            ops.add(ops.multiply(-nudged_min, inv_scale), 0.5)\n+        )\n+        x_clamped = ops.clip(x, nudged_min, nudged_max)\n+        x_clamped_shifted = ops.subtract(x_clamped, nudged_min)\n+        result = ops.multiply(\n+            ops.floor(\n+                ops.add(\n+                    ops.subtract(\n+                        ops.multiply(x_clamped_shifted, inv_scale), quant_zero\n+                    ),\n+                    0.5,\n+                )\n+            ),\n+            scale,\n+        )\n+\n+        # Create gradient mask for all channels\n+        masks = ops.cast(\n+            (x >= nudged_min) & (x <= nudged_max),\n+            dtype=\"float32\",\n+        )\n+\n+        def grad(*args, upstream=None):\n+            if upstream is None:\n+                (upstream,) = args\n+\n+            # Gradient for x\n+            dx = ops.multiply(upstream, masks)\n+            axes = [i for i in range(len(dx.shape)) if i != axis]\n+            # Gradient for min_val\n+            # When x is clipped to min, the gradient flows to min_val\n+            min_mask = ops.cast(x <= nudged_min, dtype=\"float32\")\n+            grad_min = ops.multiply(upstream, min_mask)\n+            if axis is not None:\n+                grad_min = ops.sum(grad_min, axis=axes)\n+            else:\n+                grad_min = ops.sum(grad_min)\n+\n+            # Gradient for max_val\n+            # When x is clipped to max, the gradient flows to max_val\n+            max_mask = ops.cast(x >= nudged_max, dtype=\"float32\")\n+            grad_max = ops.multiply(upstream, max_mask)\n+            if axis is not None:\n+                grad_max = ops.sum(grad_max, axis=axes)\n+            else:\n+                grad_max = ops.sum(grad_max)\n+\n+            return dx, grad_min, grad_max\n+\n+        return result, grad\n+\n+    return _fake_quant_with_min_max_vars_per_channel(inputs, min_vals, max_vals)\n+\n+\n \"\"\"Float8-related methods\"\"\"\n \n \ndiff --git a/keras/src/quantizers/quantizers_test.py b/keras/src/quantizers/quantizers_test.py\nindex 2d62240080ed..d71f8fe2a1f8 100644\n--- a/keras/src/quantizers/quantizers_test.py\n+++ b/keras/src/quantizers/quantizers_test.py\n@@ -100,3 +103,381 @@ def test_quantize_and_dequantize(self):\n         )\n         # A loose assertion due to an expected quantization error\n         self.assertAllClose(qdq_values, values, atol=5e-1)\n+\n+    @parameterized.named_parameters(\n+        [\n+            {\n+                \"testcase_name\": \"wide_8bits_input_mins_0.0_input_maxs_255.0\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [0.0],\n+                \"input_maxs\": [255.0],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [255.0],\n+                \"expected_steps\": [1.0],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"wide_8bits_input_mins_0.5_input_maxs_128.0\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [0.5],\n+                \"input_maxs\": [128.0],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [127.5],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"wide_8bits_input_mins_-128.0_input_maxs_-0.5\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [-128.0],\n+                \"input_maxs\": [-0.5],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [-127.5],\n+                \"expected_nudged_input_maxs\": [0.0],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"wide_8bits_input_mins_-0.1_input_maxs_127.4\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [-0.1],\n+                \"input_maxs\": [127.4],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [127.5],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"narrow_8bits_input_mins_0.0_input_maxs_254.0\",\n+                \"narrow_range\": True,\n+                \"input_mins\": [0.0],\n+                \"input_maxs\": [254.0],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [254.0],\n+                \"expected_steps\": [1.0],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"narrow_8bits_input_mins_0.1_input_maxs_127.1\",\n+                \"narrow_range\": True,\n+                \"input_mins\": [0.1],\n+                \"input_maxs\": [127.1],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [127.0],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": (\n+                    \"narrow_8bits_input_mins_-127.1_input_maxs_-0.1\"\n+                ),\n+                \"narrow_range\": True,\n+                \"input_mins\": [-127.1],\n+                \"input_maxs\": [-0.1],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [-127.0],\n+                \"expected_nudged_input_maxs\": [0.0],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": (\n+                    \"narrow_8bits_input_mins_-0.1_input_maxs_126.9\"\n+                ),\n+                \"narrow_range\": True,\n+                \"input_mins\": [-0.1],\n+                \"input_maxs\": [126.9],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [127.0],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"wide_7bits_input_mins_0.0_input_maxs_127.0\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [0.0],\n+                \"input_maxs\": [127.0],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [127.0],\n+                \"expected_steps\": [1.0],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"wide_7bits_input_mins_0.5_input_maxs_64.0\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [0.5],\n+                \"input_maxs\": [64.0],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [63.5],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"wide_7bits_input_mins_-64.0_input_maxs_-0.5\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [-64.0],\n+                \"input_maxs\": [-0.5],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [-63.5],\n+                \"expected_nudged_input_maxs\": [0.0],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"wide_7bits_input_mins_-0.1_input_maxs_63.4\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [-0.1],\n+                \"input_maxs\": [63.4],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [63.5],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"narrow_7bits_input_mins_0.0_input_maxs_126.0\",\n+                \"narrow_range\": True,\n+                \"input_mins\": [0.0],\n+                \"input_maxs\": [126.0],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [126.0],\n+                \"expected_steps\": [1.0],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"narrow_7bits_input_mins_0.1_input_maxs_63.1\",\n+                \"narrow_range\": True,\n+                \"input_mins\": [0.1],\n+                \"input_maxs\": [63.1],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [63.0],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": (\n+                    \"narrow_7bits_input_mins_-63.1_input_maxs_-0.1\"\n+                ),\n+                \"narrow_range\": True,\n+                \"input_mins\": [-63.1],\n+                \"input_maxs\": [-0.1],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [-63.0],\n+                \"expected_nudged_input_maxs\": [0.0],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"narrow_7bits_input_mins_-0.1_input_maxs_62.9\",\n+                \"narrow_range\": True,\n+                \"input_mins\": [-0.1],\n+                \"input_maxs\": [62.9],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [0.0],\n+                \"expected_nudged_input_maxs\": [63.0],\n+                \"expected_steps\": [0.5],\n+                \"axis\": None,\n+            },\n+            {\n+                \"testcase_name\": \"wide_8bits_multi_channel\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [0.0, 0.5, -128.0, -0.1],\n+                \"input_maxs\": [255.0, 128.0, -0.5, 127.4],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [0.0, 0.0, -127.5, 0.0],\n+                \"expected_nudged_input_maxs\": [255.0, 127.5, 0.0, 127.5],\n+                \"expected_steps\": [1.0, 0.5, 0.5, 0.5],\n+                \"axis\": 1,\n+            },\n+            {\n+                \"testcase_name\": \"narrow_8bits_multi_channel\",\n+                \"narrow_range\": True,\n+                \"input_mins\": [0.0, 0.1, -127.1, -0.1],\n+                \"input_maxs\": [254.0, 127.1, -0.1, 126.9],\n+                \"num_bits\": 8,\n+                \"expected_nudged_input_mins\": [0.0, 0.0, -127.0, 0.0],\n+                \"expected_nudged_input_maxs\": [254.0, 127.0, 0.0, 127.0],\n+                \"expected_steps\": [1.0, 0.5, 0.5, 0.5],\n+                \"axis\": 1,\n+            },\n+            {\n+                \"testcase_name\": \"wide_7bits_multi_channel\",\n+                \"narrow_range\": False,\n+                \"input_mins\": [0.0, 0.5, -64.0, -0.1],\n+                \"input_maxs\": [127.0, 64.0, -0.5, 63.4],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [0.0, 0.0, -63.5, 0.0],\n+                \"expected_nudged_input_maxs\": [127.0, 63.5, 0.0, 63.5],\n+                \"expected_steps\": [1.0, 0.5, 0.5, 0.5],\n+                \"axis\": 1,\n+            },\n+            {\n+                \"testcase_name\": \"narrow_7bits_multi_channel\",\n+                \"narrow_range\": True,\n+                \"input_mins\": [0.0, 0.1, -63.1, -0.1],\n+                \"input_maxs\": [126.0, 63.1, -0.1, 62.9],\n+                \"num_bits\": 7,\n+                \"expected_nudged_input_mins\": [0.0, 0.0, -63.0, 0.0],\n+                \"expected_nudged_input_maxs\": [126.0, 63.0, 0.0, 63.0],\n+                \"expected_steps\": [1.0, 0.5, 0.5, 0.5],\n+                \"axis\": 1,\n+            },\n+        ]\n+    )\n+    def test_op(\n+        self,\n+        input_mins,\n+        input_maxs,\n+        num_bits,\n+        narrow_range,\n+        axis,\n+        expected_nudged_input_mins,\n+        expected_nudged_input_maxs,\n+        expected_steps,\n+    ):\n+        num_channels = len(input_mins)\n+        inputs_list = []\n+        expected_list = []\n+        initial_gradients_list = []\n+        expected_backprops_wrt_input_list = []\n+        for i in range(num_channels):\n+            expected_nudged_input_min = expected_nudged_input_mins[i]\n+            expected_nudged_input_max = expected_nudged_input_maxs[i]\n+            expected_step = expected_steps[i]\n+\n+            inputs_list.append(\n+                [\n+                    expected_nudged_input_min - expected_step,\n+                    expected_nudged_input_min - 0.01,\n+                    expected_nudged_input_min,\n+                    expected_nudged_input_min + 0.01,\n+                    expected_nudged_input_min + expected_step - 0.01,\n+                    expected_nudged_input_min + expected_step,\n+                    expected_nudged_input_min + expected_step + 0.01,\n+                    expected_nudged_input_max - 0.01,\n+                    expected_nudged_input_max,\n+                    expected_nudged_input_max + 0.01,\n+                    expected_nudged_input_max + expected_step,\n+                ]\n+            )\n+            expected_list.append(\n+                [\n+                    expected_nudged_input_min,\n+                    expected_nudged_input_min,\n+                    expected_nudged_input_min,\n+                    expected_nudged_input_min,\n+                    expected_nudged_input_min + expected_step,\n+                    expected_nudged_input_min + expected_step,\n+                    expected_nudged_input_min + expected_step,\n+                    expected_nudged_input_max,\n+                    expected_nudged_input_max,\n+                    expected_nudged_input_max,\n+                    expected_nudged_input_max,\n+                ]\n+            )\n+            initial_gradients_list.append(\n+                list(range(1, len(inputs_list[-1]) + 1))\n+            )\n+            expected_backprops_wrt_input_list.append(\n+                [0.0, 0.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 0.0, 0.0]\n+            )\n+        inputs = ops.transpose(ops.array(inputs_list, dtype=\"float32\"))\n+        expected = ops.transpose(ops.array(expected_list, dtype=\"float32\"))\n+        expected_backprops_wrt_input = ops.transpose(\n+            ops.array(expected_backprops_wrt_input_list, dtype=\"float32\")\n+        )\n+        input_min = ops.array(input_mins, dtype=\"float32\")\n+        input_max = ops.array(input_maxs, dtype=\"float32\")\n+        initial_gradients = ops.transpose(\n+            ops.array(initial_gradients_list, dtype=\"float32\")\n+        )\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            @tf.function(jit_compile=True)\n+            def test_op(\n+                inputs, input_mins, input_maxs, num_bits, narrow_range, axis\n+            ):\n+                with tf.GradientTape() as tape:\n+                    tape.watch(inputs)\n+                    result = quantizers.fake_quant_with_min_max_vars(\n+                        inputs,\n+                        input_mins,\n+                        input_maxs,\n+                        num_bits,\n+                        narrow_range,\n+                        axis,\n+                    )\n+                return initial_gradients * tape.gradient(result, inputs)\n+\n+            gradients = test_op(\n+                inputs, input_mins, input_maxs, num_bits, narrow_range, axis\n+            )\n+            # test gradients\n+            self.assertAllClose(gradients, expected_backprops_wrt_input)\n+\n+        if backend.backend() == \"torch\":\n+            import torch\n+\n+            def test_op(inputs, input_mins, input_maxs, num_bits, narrow_range):\n+                # Create tensor and enable gradient tracking\n+                inputs = torch.tensor(\n+                    inputs, dtype=torch.float32, requires_grad=True\n+                )\n+\n+                # Apply the quantization operation\n+                result = quantizers.fake_quant_with_min_max_vars(\n+                    inputs, input_mins, input_maxs, num_bits, narrow_range\n+                )\n+\n+                # Compute gradients\n+                result.backward(torch.ones_like(result))\n+\n+                return initial_gradients * inputs.grad\n+\n+            gradients = test_op(\n+                inputs, input_min, input_max, num_bits, narrow_range\n+            )\n+            # test gradients\n+            self.assertAllClose(gradients, expected_backprops_wrt_input)\n+\n+        if backend.backend() == \"jax\":\n+            import jax\n+\n+            def test_op(inputs, input_mins, input_maxs, num_bits, narrow_range):\n+                # Define the function to compute gradients for\n+                def quantize_fn(x):\n+                    return quantizers.fake_quant_with_min_max_vars(\n+                        x, input_mins, input_maxs, num_bits, narrow_range\n+                    )\n+\n+                _, f_vjp = jax.vjp(quantize_fn, inputs)\n+                # NOTE:python 3.10 input_gradients = f_vjp.args[0].args[0][0] !\n+                input_gradients = f_vjp.args[0].args[0][1]\n+\n+                return ops.multiply(initial_gradients, input_gradients)\n+\n+            gradients = test_op(\n+                inputs, input_min, input_max, num_bits, narrow_range\n+            )\n+            # test gradients\n+            self.assertAllClose(gradients, expected_backprops_wrt_input)\n+        outputs = quantizers.fake_quant_with_min_max_vars(\n+            inputs,\n+            input_min,\n+            input_max,\n+            num_bits=num_bits,\n+            narrow_range=narrow_range,\n+            axis=axis,\n+        )\n+        self.assertAllClose(outputs, expected)\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "You've done a solid job refactoring the functions into one to reduce duplication, but consider whether we still need the separate functions for different parameter types. I appreciate the addition of tests and aligning with `parameterized.named_parameters()`, but I'm concerned about the JAX GPU test failures. Itd be helpful to further investigate the precision issue you mentioned. Also, ensure the unnecessary functions are truly removed to keep the codebase clean. Once CI passes, we should be ready for merge."
      }
    ],
    "codediff_length": 20897
  },
  {
    "index": 45,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/openhands/core/loop.py b/openhands/core/loop.py\nnew file mode 100644\nindex 000000000000..2a2808dd0980\n--- /dev/null\n+++ b/openhands/core/loop.py\n@@ -0,0 +1,50 @@\n+import asyncio\n+\n+from openhands.controller import AgentController\n+from openhands.core.logger import openhands_logger as logger\n+from openhands.core.schema import AgentState\n+from openhands.runtime.base import Runtime\n+\n+\n+async def run_agent_until_done(\n+    controller: AgentController,\n+    runtime: Runtime,\n+    end_states: list[AgentState],\n+):\n+    \"\"\"\n+    run_agent_until_done takes a controller and a runtime, and will run\n+    the agent until it reaches a terminal state.\n+    Note that runtime must be connected before being passed in here.\n+    \"\"\"\n+    controller.agent_task = asyncio.create_task(controller.start_step_loop())\n+\n+    def status_callback(msg_type, msg_id, msg):\n+        if msg_type == 'error':\n+            logger.error(msg)\n+            if controller:\n+                controller.state.last_error = msg\n+                asyncio.create_task(controller.set_agent_state_to(AgentState.ERROR))\n+        else:\n+            logger.info(msg)\n+\n+    if hasattr(runtime, 'status_callback') and runtime.status_callback:\n+        raise ValueError(\n+            'Runtime status_callback was set, but run_agent_until_done will override it'\n+        )\n+    if hasattr(controller, 'status_callback') and controller.status_callback:\n+        raise ValueError(\n+            'Controller status_callback was set, but run_agent_until_done will override it'\n+        )\n+\n+    runtime.status_callback = status_callback\n+    controller.status_callback = status_callback\n+\n+    while controller.state.agent_state not in end_states:\n+        await asyncio.sleep(1)\n+\n+    if not controller.agent_task.done():\n+        controller.agent_task.cancel()\n+        try:\n+            await controller.agent_task\n+        except asyncio.CancelledError:\n+            pass\ndiff --git a/tests/runtime/test_stress_remote_runtime.py b/tests/runtime/test_stress_remote_runtime.py\nnew file mode 100644\nindex 000000000000..4d96ee132a2d\n--- /dev/null\n+++ b/tests/runtime/test_stress_remote_runtime.py\n@@ -0,0 +1,231 @@\n+\"\"\"Bash-related tests for the EventStreamRuntime, which connects to the ActionExecutor running in the sandbox.\"\"\"\n+\n+import asyncio\n+import os\n+import tempfile\n+from unittest.mock import MagicMock\n+\n+import pandas as pd\n+import pytest\n+from conftest import TEST_IN_CI\n+\n+from evaluation.utils.shared import (\n+    EvalException,\n+    EvalMetadata,\n+    EvalOutput,\n+    assert_and_raise,\n+    codeact_user_response,\n+    make_metadata,\n+    prepare_dataset,\n+    reset_logger_for_multiprocessing,\n+    run_evaluation,\n+)\n+from openhands.agenthub import Agent\n+from openhands.controller.state.state import State\n+from openhands.core.config import (\n+    AgentConfig,\n+    AppConfig,\n+    LLMConfig,\n+    SandboxConfig,\n+)\n+from openhands.core.logger import openhands_logger as logger\n+from openhands.core.main import create_runtime, run_controller\n+from openhands.events.action import CmdRunAction, MessageAction\n+from openhands.events.observation import CmdOutputObservation\n+from openhands.events.serialization.event import event_to_dict\n+from openhands.llm import LLM\n+from openhands.runtime.base import Runtime\n+from openhands.utils.async_utils import call_async_from_sync\n+\n+AGENT_CLS_TO_FAKE_USER_RESPONSE_FN = {\n+    'CodeActAgent': codeact_user_response,\n+}\n+\n+\n+def get_config(\n+    metadata: EvalMetadata,\n+) -> AppConfig:\n+    assert (\n+        os.environ.get('SANDBOX_REMOTE_RUNTIME_API_URL') is not None\n+    ), 'SANDBOX_REMOTE_RUNTIME_API_URL must be set.'\n+    assert (\n+        os.environ.get('ALLHANDS_API_KEY') is not None\n+    ), 'ALLHANDS_API_KEY must be set.'\n+    config = AppConfig(\n+        default_agent=metadata.agent_class,\n+        run_as_openhands=False,\n+        max_iterations=metadata.max_iterations,\n+        runtime='remote',\n+        sandbox=SandboxConfig(\n+            base_container_image='python:3.11-bookworm',\n+            enable_auto_lint=True,\n+            use_host_network=False,\n+            # large enough timeout, since some testcases take very long to run\n+            timeout=300,\n+            api_key=os.environ.get('ALLHANDS_API_KEY', None),\n+            remote_runtime_api_url=os.environ.get('SANDBOX_REMOTE_RUNTIME_API_URL'),\n+            keep_remote_runtime_alive=False,\n+        ),\n+        # do not mount workspace\n+        workspace_base=None,\n+        workspace_mount_path=None,\n+    )\n+    agent_config = AgentConfig(\n+        codeact_enable_jupyter=False,\n+        codeact_enable_browsing=False,\n+        codeact_enable_llm_editor=False,\n+    )\n+    config.set_agent_config(agent_config)\n+    return config\n+\n+\n+def initialize_runtime(\n+    runtime: Runtime,\n+):\n+    \"\"\"Initialize the runtime for the agent.\n+\n+    This function is called before the runtime is used to run the agent.\n+    \"\"\"\n+    logger.info('-' * 30)\n+    logger.info('BEGIN Runtime Initialization Fn')\n+    logger.info('-' * 30)\n+    obs: CmdOutputObservation\n+\n+    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n+    action.timeout = 600\n+    logger.info(action, extra={'msg_type': 'ACTION'})\n+    obs = runtime.run_action(action)\n+    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n+    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n+\n+    action = CmdRunAction(command='mkdir -p /dummy_dir')\n+    action.timeout = 600\n+    logger.info(action, extra={'msg_type': 'ACTION'})\n+    obs = runtime.run_action(action)\n+    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n+    assert_and_raise(\n+        obs.exit_code == 0,\n+        f'Failed to create /dummy_dir: {str(obs)}',\n+    )\n+\n+    with tempfile.TemporaryDirectory() as temp_dir:\n+        # Construct the full path for the desired file name within the temporary directory\n+        temp_file_path = os.path.join(temp_dir, 'dummy_file')\n+        # Write to the file with the desired name within the temporary directory\n+        with open(temp_file_path, 'w') as f:\n+            f.write('dummy content')\n+\n+        # Copy the file to the desired location\n+        runtime.copy_to(temp_file_path, '/dummy_dir/')\n+\n+    logger.info('-' * 30)\n+    logger.info('END Runtime Initialization Fn')\n+    logger.info('-' * 30)\n+\n+\n+def process_instance(\n+    instance: pd.Series,\n+    metadata: EvalMetadata,\n+    reset_logger: bool = True,\n+) -> EvalOutput:\n+    config = get_config(metadata)\n+\n+    # Setup the logger properly, so you can run multi-processing to parallelize the evaluation\n+    if reset_logger:\n+        log_dir = os.path.join(metadata.eval_output_dir, 'infer_logs')\n+        reset_logger_for_multiprocessing(logger, instance.instance_id, log_dir)\n+    else:\n+        logger.info(f'Starting evaluation for instance {instance.instance_id}.')\n+\n+    runtime = create_runtime(config)\n+    call_async_from_sync(runtime.connect)\n+\n+    try:\n+        initialize_runtime(runtime)\n+\n+        instruction = 'dummy instruction'\n+        agent = Agent.get_cls(metadata.agent_class)(\n+            llm=LLM(config=metadata.llm_config),\n+            config=config.get_agent_config(metadata.agent_class),\n+        )\n+\n+        def next_command(*args, **kwargs):\n+            return CmdRunAction(command='ls -lah')\n+\n+        agent.step = MagicMock(side_effect=next_command)\n+\n+        # Here's how you can run the agent (similar to the `main` function) and get the final task state\n+        state: State | None = asyncio.run(\n+            run_controller(\n+                config=config,\n+                initial_user_action=MessageAction(content=instruction),\n+                runtime=runtime,\n+                fake_user_response_fn=AGENT_CLS_TO_FAKE_USER_RESPONSE_FN[\n+                    metadata.agent_class\n+                ],\n+                agent=agent,\n+            )\n+        )\n+\n+        # if fatal error, throw EvalError to trigger re-run\n+        if (\n+            state.last_error\n+            and 'fatal error during agent execution' in state.last_error\n+            and 'stuck in a loop' not in state.last_error\n+        ):\n+            raise EvalException('Fatal error detected: ' + state.last_error)\n+\n+    finally:\n+        runtime.close()\n+\n+    test_result = {}\n+    if state is None:\n+        raise ValueError('State should not be None.')\n+    histories = [event_to_dict(event) for event in state.history.get_events()]\n+    metrics = state.metrics.get() if state.metrics else None\n+\n+    # Save the output\n+    output = EvalOutput(\n+        instance_id=instance.instance_id,\n+        instruction=instruction,\n+        instance=instance.to_dict(),  # SWE Bench specific\n+        test_result=test_result,\n+        metadata=metadata,\n+        history=histories,\n+        metrics=metrics,\n+        error=state.last_error if state and state.last_error else None,\n+    )\n+    return output\n+\n+\n+@pytest.mark.skipif(\n+    TEST_IN_CI,\n+    reason='This test should only be run locally, not in CI.',\n+)\n+def test_stress_remote_runtime(n_eval_workers: int = 64):\n+    \"\"\"Mimic evaluation setting to test remote runtime in a multi-processing setting.\"\"\"\n+\n+    llm_config = LLMConfig()\n+    metadata = make_metadata(\n+        llm_config,\n+        'dummy_dataset_descrption',\n+        'CodeActAgent',\n+        max_iterations=10,\n+        eval_note='dummy_eval_note',\n+        eval_output_dir='./dummy_eval_output_dir',\n+        details={},\n+    )\n+\n+    # generate 300 random dummy instances\n+    dummy_instance = pd.DataFrame(\n+        {\n+            'instance_id': [f'dummy_instance_{i}' for i in range(300)],\n+        }\n+    )\n+\n+    output_file = os.path.join(metadata.eval_output_dir, 'output.jsonl')\n+    instances = prepare_dataset(\n+        dummy_instance, output_file, eval_n_limit=len(dummy_instance)\n+    )\n+\n+    run_evaluation(instances, metadata, output_file, n_eval_workers, process_instance)\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The minor refactor in the agent controller enhances maintainability and clarity, especially with the error handling improvements by replacing `FatalErrorObservation` with `ErrorObservation`. However, consistency in return types for the `run` method and error naming could be improved for better readability. Additionally, ensure test names accurately reflect their functionality. Overall, while the refactor is a strong step forward, these suggestions will bolster the code quality further."
      }
    ],
    "codediff_length": 9947
  },
  {
    "index": 46,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/openhands/events/action/agent.py b/openhands/events/action/agent.py\nindex 958ad7d7d757..e7ba21662897 100644\n--- a/openhands/events/action/agent.py\n+++ b/openhands/events/action/agent.py\n@@ -111,3 +111,87 @@ def __str__(self) -> str:\n         ret = '**RecallAction**\\n'\n         ret += f'QUERY: {self.query[:50]}'\n         return ret\n+\n+\n+@dataclass\n+class CondensationAction(Action):\n+    \"\"\"This action indicates a condensation of the conversation history is happening.\n+\n+    There are two ways to specify the events to be forgotten:\n+    1. By providing a list of event IDs.\n+    2. By providing the start and end IDs of a range of events.\n+\n+    In the second case, we assume that event IDs are monotonically increasing, and that _all_ events between the start and end IDs are to be forgotten.\n+\n+    Raises:\n+        ValueError: If the optional fields are not instantiated in a valid configuration.\n+    \"\"\"\n+\n+    action: str = ActionType.CONDENSATION\n+\n+    forgotten_event_ids: list[int] | None = None\n+    \"\"\"The IDs of the events that are being forgotten (removed from the `View` given to the LLM).\"\"\"\n+\n+    forgotten_events_start_id: int | None = None\n+    \"\"\"The ID of the first event to be forgotten in a range of events.\"\"\"\n+\n+    forgotten_events_end_id: int | None = None\n+    \"\"\"The ID of the last event to be forgotten in a range of events.\"\"\"\n+\n+    summary: str | None = None\n+    \"\"\"An optional summary of the events being forgotten.\"\"\"\n+\n+    summary_offset: int | None = None\n+    \"\"\"An optional offset to the start of the resulting view indicating where the summary should be inserted.\"\"\"\n+\n+    def _validate_field_polymorphism(self) -> bool:\n+        \"\"\"Check if the optional fields are instantiated in a valid configuration.\"\"\"\n+        # For the forgotton events, there are only two valid configurations:\n+        # 1. We're forgetting events based on the list of provided IDs, or\n+        using_event_ids = self.forgotten_event_ids is not None\n+        # 2. We're forgetting events based on the range of IDs.\n+        using_event_range = (\n+            self.forgotten_events_start_id is not None\n+            and self.forgotten_events_end_id is not None\n+        )\n+\n+        # Either way, we can only have one of the two valid configurations.\n+        forgotten_event_configuration = using_event_ids ^ using_event_range\n+\n+        # We also need to check that if the summary is provided, so is the\n+        # offset (and vice versa).\n+        summary_configuration = (\n+            self.summary is None and self.summary_offset is None\n+        ) or (self.summary is not None and self.summary_offset is not None)\n+\n+        return forgotten_event_configuration and summary_configuration\n+\n+    def __post_init__(self):\n+        if not self._validate_field_polymorphism():\n+            raise ValueError('Invalid configuration of the optional fields.')\n+\n+    @property\n+    def forgotten(self) -> list[int]:\n+        \"\"\"The list of event IDs that should be forgotten.\"\"\"\n+        # Start by making sure the fields are instantiated in a valid\n+        # configuration. We check this whenever the event is initialized, but we\n+        # can't make the dataclass immutable so we need to check it again here\n+        # to make sure the configuration is still valid.\n+        if not self._validate_field_polymorphism():\n+            raise ValueError('Invalid configuration of the optional fields.')\n+\n+        if self.forgotten_event_ids is not None:\n+            return self.forgotten_event_ids\n+\n+        # If we've gotten this far, the start/end IDs are not None.\n+        assert self.forgotten_events_start_id is not None\n+        assert self.forgotten_events_end_id is not None\n+        return list(\n+            range(self.forgotten_events_start_id, self.forgotten_events_end_id + 1)\n+        )\n+\n+    @property\n+    def message(self) -> str:\n+        if self.summary:\n+            return f'Summary: {self.summary}'\n+        return f'Condenser is dropping the events: {self.forgotten}.'\ndiff --git a/openhands/memory/condenser/condenser.py b/openhands/memory/condenser/condenser.py\nindex 411ed39386f6..e2b8213d7627 100644\n--- a/openhands/memory/condenser/condenser.py\n+++ b/openhands/memory/condenser/condenser.py\n@@ -32,6 +34,75 @@ def get_condensation_metadata(state: State) -> list[dict[str, Any]]:\n \"\"\"Registry of condenser configurations to their corresponding condenser classes.\"\"\"\n \n \n+class View(BaseModel):\n+    \"\"\"Linearly ordered view of events.\n+\n+    Produced by a condenser to indicate the included events are ready to process as LLM input.\n+    \"\"\"\n+\n+    events: list[Event]\n+\n+    def __len__(self) -> int:\n+        return len(self.events)\n+\n+    def __iter__(self):\n+        return iter(self.events)\n+\n+    # To preserve list-like indexing, we ideally support slicing and position-based indexing.\n+    # The only challenge with that is switching the return type based on the input type -- we\n+    # can mark the different signatures for MyPy with `@overload` decorators.\n+\n+    @overload\n+    def __getitem__(self, key: slice) -> list[Event]: ...\n+\n+    @overload\n+    def __getitem__(self, key: int) -> Event: ...\n+\n+    def __getitem__(self, key: int | slice) -> Event | list[Event]:\n+        if isinstance(key, slice):\n+            start, stop, step = key.indices(len(self))\n+            return [self[i] for i in range(start, stop, step)]\n+        elif isinstance(key, int):\n+            return self.events[key]\n+        else:\n+            raise ValueError(f'Invalid key type: {type(key)}')\n+\n+    @staticmethod\n+    def from_events(events: list[Event]) -> View:\n+        \"\"\"Create a view from a list of events, respecting the semantics of any condensation events.\"\"\"\n+        forgotten_event_ids: set[int] = set()\n+        for event in events:\n+            if isinstance(event, CondensationAction):\n+                forgotten_event_ids.update(event.forgotten)\n+\n+        kept_events = [event for event in events if event.id not in forgotten_event_ids]\n+\n+        # If we have a summary, insert it at the specified offset.\n+        summary: str | None = None\n+        summary_offset: int | None = None\n+\n+        # The relevant summary is always in the last condensation event (i.e., the most recent one).\n+        for event in reversed(events):\n+            if isinstance(event, CondensationAction):\n+                if event.summary is not None and event.summary_offset is not None:\n+                    summary = event.summary\n+                    summary_offset = event.summary_offset\n+                    break\n+\n+        if summary is not None and summary_offset is not None:\n+            kept_events.insert(\n+                summary_offset, AgentCondensationObservation(content=summary)\n+            )\n+\n+        return View(events=kept_events)\n+\n+\n+class Condensation(BaseModel):\n+    \"\"\"Produced by a condenser to indicate the history has been condensed.\"\"\"\n+\n+    action: CondensationAction\n+\n+\n class Condenser(ABC):\n     \"\"\"Abstract condenser interface.\n \n\ndiff --git a/tests/unit/test_condenser.py b/tests/unit/test_condenser.py\nindex 17b569fdec70..bf484c7b367c 100644\n--- a/tests/unit/test_condenser.py\n+++ b/tests/unit/test_condenser.py\n@@ -98,6 +98,73 @@ def mock_state() -> State:\n     return mock_state\n \n \n+class RollingCondenserTestHarness:\n+    \"\"\"Test harness for rolling condensers.\n+\n+    Simulates the behavior of a simple agent loop (appropriately handling the distinction between `View` and `Condensation` results) and provides utilities for testing the results.\n+    \"\"\"\n+\n+    def __init__(self, condenser: RollingCondenser):\n+        self.condenser = condenser\n+        self.callbacks: list[Callable[[list[Event]], None]] = []\n+\n+    def add_callback(self, callback: Callable[[list[Event]], None]):\n+        \"\"\"Add a callback to the test harness.\n+\n+        This callback will be called on the history after each event is added, but before the condenser is applied. You can use this to export information about the event that was just added, or to set LLM responses based on the state.\n+        \"\"\"\n+        self.callbacks.append(callback)\n+\n+    def views(self, events: Iterable[Event]) -> Iterable[View]:\n+        \"\"\"Generate a sequence of views similating the condenser's behavior over the given event stream.\n+\n+        This generator assumes we're starting from an empty history.\n+        \"\"\"\n+        mock_state = MagicMock()\n+        mock_state.extra_data = {}\n+        mock_state.history = []\n+\n+        for event in events:\n+            mock_state.history.append(event)\n+            for callback in self.callbacks:\n+                callback(mock_state.history)\n+\n+            match self.condenser.condensed_history(mock_state):\n+                case View() as view:\n+                    yield view\n+\n+                case Condensation(event=condensation_event):\n+                    mock_state.history.append(condensation_event)\n+\n+    def expected_size(self, index: int, max_size: int) -> int:\n+        \"\"\"Calculate the expected size of the view at the given index.\n+\n+        Assumes the condenser triggers condensation when the view is _longer_ than the max size, and that the target size is half the max size.\n+        \"\"\"\n+        # Until we hit the max size, the views should grow monotonically.\n+        if index < max_size:\n+            return index + 1\n+\n+        # Once we hit the max size, the next view should be reduced to the target size.\n+        target_size = max_size // 2\n+\n+        # So when the index is the same as the max size, we should have target size + 1 events in the view.\n+        # And the maximum value we will ever see is the max size (approximately 2 * target size).\n+        # Put together, we get the following formula:\n+        return ((index - max_size) % target_size) + target_size + 1\n+\n+    def expected_condensations(self, index: int, max_size: int) -> int:\n+        \"\"\"Calculate the expected number of condensation events at the given index.\n+\n+        Assumes the condenser triggers condensation when the view is _longer_ than the max size, and that the target size is half the max size.\n+        \"\"\"\n+        if index < max_size:\n+            return 0\n+\n+        target_size = max_size // 2\n+        return ((index - max_size) // target_size) + 1\n+\n+\n def test_noop_condenser_from_config():\n     \"\"\"Test that the NoOpCondenser objects can be made from config.\"\"\"\n     config = NoOpCondenserConfig()\n@@ -297,135 +364,49 @@ def test_llm_amortized_summarization_condenser_invalid_config():\n     pytest.raises(ValueError, LLMSummarizingCondenser, llm=MagicMock(), keep_first=-1)\n \n \n-def test_llm_summarizing_condenser_grows_to_max_size(mock_llm, mock_state):\n-    \"\"\"Test that LLMSummarizingCondenser correctly maintains an event context up to max size.\"\"\"\n-    max_size = 15\n+def test_llm_summarizing_condenser_gives_expected_view_size(mock_llm):\n+    \"\"\"Test that LLMSummarizingCondenser maintains the correct view size.\"\"\"\n+    max_size = 10\n     condenser = LLMSummarizingCondenser(max_size=max_size, llm=mock_llm)\n \n-    for i in range(max_size):\n-        event = create_test_event(f'Event {i}')\n-        mock_state.history.append(event)\n-        results = condenser.condensed_history(mock_state)\n-        assert len(results) == i + 1\n-\n-\n-def test_llm_summarizing_condenser_forgets_and_summarizes(mock_llm, mock_state):\n-    \"\"\"Test that the LLMSummarizingCondenser forgets events and maintains a summary.\"\"\"\n-    max_size = 4\n-    keep_first = 1\n-    condenser = LLMSummarizingCondenser(\n-        max_size=max_size, keep_first=keep_first, llm=mock_llm\n-    )\n-\n-    # Add initial event\n-    first_event = create_test_event('Event 0')\n-    mock_state.history.append(first_event)\n+    events = [create_test_event(f'Event {i}', id=i) for i in range(max_size * 10)]\n \n     # Set up mock LLM response\n     mock_llm.set_mock_response_content('Summary of forgotten events')\n \n-    # Add enough events to trigger forgetting\n-    for i in range(max_size + 3):  # +3 to ensure we're well past max_size\n-        event = create_test_event(f'Event {i+1}')\n-        mock_state.history.append(event)\n-\n-    # Get the condensed history\n-    results = condenser.condensed_history(mock_state)\n-\n-    # We should have exactly 3 events:\n-    # 1. First event (keep_first = 1)\n-    # 2. Summary event\n-    # 3. Most recent event\n-    assert len(results) == 3, f'Expected 3 events, got {len(results)}: {results}'\n-    assert (\n-        results[0] == first_event\n-    ), f'First event should be {first_event}, got {results[0]}'\n-    assert isinstance(\n-        results[1], AgentCondensationObservation\n-    ), f'Second event should be a summary, got {results[1]}'\n-    assert (\n-        results[1].content == 'Summary of forgotten events'\n-    ), f\"Summary content should be 'Summary of forgotten events', got {results[1].content}\"\n-    assert results[2] == event, f'Last event should be {event}, got {results[2]}'\n-\n-\n-def test_llm_summarizing_condenser_llm_call(mock_llm, mock_state):\n-    \"\"\"Test that the LLM is called correctly when forgetting events.\"\"\"\n-    max_size = 4\n-    keep_first = 1\n-    condenser = LLMSummarizingCondenser(\n-        max_size=max_size, keep_first=keep_first, llm=mock_llm\n-    )\n+    harness = RollingCondenserTestHarness(condenser)\n \n-    # Add initial event\n-    first_event = create_test_event('Event 0')\n-    mock_state.history.append(first_event)\n+    for i, view in enumerate(harness.views(events)):\n+        assert len(view) == harness.expected_size(i, max_size)\n \n-    # Set up mock LLM response\n-    mock_llm.set_mock_response_content('Summary of forgotten events')\n-    mock_llm.metrics.get.return_value = {'test_metric': 1.0}\n-\n-    # Add enough events to trigger forgetting\n-    for i in range(max_size):\n-        event = create_test_event(f'Event {i+1}')\n-        mock_state.history.append(event)\n-        condenser.condensed_history(mock_state)\n-\n-    # Verify LLM was called with correct prompt\n-    mock_llm.completion.assert_called_once()\n-    call_args = mock_llm.completion.call_args[1]\n-    assert 'messages' in call_args\n-    assert len(call_args['messages']) == 1\n-\n-    # Verify metrics were added to state\n-    assert 'condenser_meta' in mock_state.extra_data\n-    assert len(mock_state.extra_data['condenser_meta']) == 1\n-    assert mock_state.extra_data['condenser_meta'][0]['metrics'] == {'test_metric': 1.0}\n-\n-\n-def test_llm_summarizing_condenser_resets_when_given_truncated_history(\n-    mock_llm, mock_state\n-):\n-    \"\"\"Test that the condenser, when it sees a shorter history than it has in the past (due to truncation), will reset its tracking.\"\"\"\n-    max_size = 4\n-    keep_first = 1\n+\n+def test_llm_summarizing_condenser_keeps_first_and_summary_events(mock_llm, mock_state):\n+    \"\"\"Test that the LLM summarizing condenser appropriately maintains the event prefix and any summary events.\"\"\"\n+    max_size = 10\n+    keep_first = 3\n     condenser = LLMSummarizingCondenser(\n         max_size=max_size, keep_first=keep_first, llm=mock_llm\n     )\n \n-    # Add initial event\n-    first_event = create_test_event('Event 0')\n-    mock_state.history.append(first_event)\n-\n-    # Set up mock LLM response\n     mock_llm.set_mock_response_content('Summary of forgotten events')\n \n-    # Add enough events to trigger forgetting\n-    for i in range(max_size + 3):  # +3 to ensure we're well past max_size\n-        event = create_test_event(f'Event {i+1}')\n-        mock_state.history.append(event)\n+    events = [create_test_event(f'Event {i}', id=i) for i in range(max_size * 10)]\n+    harness = RollingCondenserTestHarness(condenser)\n \n-    # Get the condensed history\n-    results = condenser.condensed_history(mock_state)\n+    for i, view in enumerate(harness.views(events)):\n+        assert len(view) == harness.expected_size(i, max_size)\n \n-    # We should have exactly 3 events:\n-    # 1. First event (keep_first = 1)\n-    # 2. Summary event\n-    # 3. Most recent event\n-    assert len(results) == 3, f'Expected 3 events, got {len(results)}: {results}'\n+        # Ensure that the we've called out the summarizing LLM once per condensation\n+        assert mock_llm.completion.call_count == harness.expected_condensations(\n+            i, max_size\n+        )\n \n-    # Now, call condensation on a small history that contains only two events.\n-    alternate_history = [\n-        create_test_event('Alt. Event 0'),\n-        create_test_event('Alt. Event 1'),\n-    ]\n-    mock_state.history = alternate_history\n+        # Ensure that the prefix is appropiately maintained\n+        assert view[:keep_first] == events[: min(keep_first, i + 1)]\n \n-    # When we do this, the condenser should start tracking the alternative history\n-    # as the de-facto history. That means we lose the summarization event and any\n-    # other events that were in the previous history.\n-    results = condenser.condensed_history(mock_state)\n-    assert results == alternate_history\n+        # If we've condensed, ensure that the summary event is present\n+        if i > max_size:\n+            assert isinstance(view[keep_first], AgentCondensationObservation)\n \n \n def test_amortized_forgetting_condenser_from_config():\n@@ -547,128 +505,46 @@ def test_llm_attention_condenser_invalid_config():\n     pytest.raises(ValueError, LLMAttentionCondenser.from_config, config)\n \n \n-def test_rolling_condenser_handles_truncation(mock_state: State):\n-    \"\"\"Test that RollingCondenser correctly handles history truncation.\"\"\"\n-\n-    class TestRollingCondenser(RollingCondenser):\n-        \"\"\"Test implementation of RollingCondenser that just returns all events.\"\"\"\n-\n-        def condense(self, events: list[Event]) -> list[Event]:\n-            return events\n-\n-    condenser = TestRollingCondenser()\n-\n-    # Initial history with 3 events\n-    events = [\n-        create_test_event('Event 1', id=1),\n-        create_test_event('Event 2', id=2),\n-        create_test_event('Event 3', id=3),\n-    ]\n-    mock_state.history = events\n-\n-    # First condensation - should return all events\n-    results = condenser.condensed_history(mock_state)\n-    assert len(results) == 3\n-    assert [e._id for e in results] == [1, 2, 3]\n-\n-    # Simulate truncation - history is now shorter, and the condensation should\n-    # just include the truncated history\n-    mock_state.history = mock_state.history[-1:]\n-\n-    results = condenser.condensed_history(mock_state)\n-    assert len(results) == 1\n-    assert results[0]._id == 3\n-\n-    # Adding more events and condensing should \"rebase\" us from the truncated history\n-    mock_state.history += [\n-        create_test_event('Event 4', id=4),\n-        create_test_event('Event 5', id=5),\n-    ]\n-\n-    results = condenser.condensed_history(mock_state)\n-    assert len(results) == 3\n-    assert [e._id for e in results] == [3, 4, 5]\n-\n-\n-def test_llm_attention_condenser_keeps_first_events(mock_llm, mock_state):\n-    \"\"\"Test that the LLMAttentionCondenser keeps the right number of initial events when forgetting.\"\"\"\n-    max_size = 4\n-    condenser = LLMAttentionCondenser(max_size=max_size, keep_first=1, llm=mock_llm)\n-\n-    first_event = create_test_event('Event 0', id=0)\n-    mock_state.history.append(first_event)\n-\n-    for i in range(max_size * 10):\n-        event = create_test_event(f'Event {i+1}', id=i + 1)\n-        mock_state.history.append(event)\n-\n-        mock_llm.set_mock_response_content(\n-            ImportantEventSelection(\n-                ids=[event.id for event in mock_state.history]\n-            ).model_dump_json()\n-        )\n-        results = condenser.condensed_history(mock_state)\n-\n-        # The first event is always the first event.\n-        assert results[0] == first_event\n-\n-\n-def test_llm_attention_condenser_grows_to_max_size(mock_llm, mock_state):\n-    \"\"\"Test that LLMAttentionCondenser correctly maintains an event context up to max size.\"\"\"\n-    max_size = 15\n-    condenser = LLMAttentionCondenser(max_size=max_size, llm=mock_llm)\n-\n-    for i in range(max_size):\n-        event = create_test_event(f'Event {i}')\n-        mock_state.history.append(event)\n-        mock_llm.set_mock_response_content(\n-            ImportantEventSelection(ids=[event.id for event in mock_state.history])\n-        )\n-        results = condenser.condensed_history(mock_state)\n-        assert len(results) == i + 1\n-\n-\n-def test_llm_attention_condenser_forgets_when_larger_than_max_size(\n-    mock_llm, mock_state\n-):\n-    \"\"\"Test that the LLMAttentionCondenser forgets events when the context grows too large.\"\"\"\n-    max_size = 2\n+def test_llm_attention_condenser_gives_expected_view_size(mock_llm):\n+    \"\"\"Test that the LLMAttentionCondenser gives views of the expected size.\"\"\"\n+    max_size = 10\n     condenser = LLMAttentionCondenser(max_size=max_size, keep_first=0, llm=mock_llm)\n \n-    for i in range(max_size * 10):\n-        event = create_test_event(f'Event {i}', id=i)\n-        mock_state.history.append(event)\n+    events = [create_test_event(f'Event {i}', id=i) for i in range(max_size * 10)]\n \n+    def set_response_content(history: list[Event]):\n         mock_llm.set_mock_response_content(\n             ImportantEventSelection(\n-                ids=[event.id for event in mock_state.history]\n+                ids=[event.id for event in history]\n             ).model_dump_json()\n         )\n \n-        results = condenser.condensed_history(mock_state)\n+    harness = RollingCondenserTestHarness(condenser)\n+    harness.add_callback(set_response_content)\n \n-        # The number of results should bounce back and forth between 1, 2, 1, 2, ...\n-        assert len(results) == (i % 2) + 1\n+    for i, view in enumerate(harness.views(events)):\n+        assert len(view) == harness.expected_size(i, max_size)\n \n \n-def test_llm_attention_condenser_handles_events_outside_history(mock_llm, mock_state):\n+def test_llm_attention_condenser_handles_events_outside_history(mock_llm):\n     \"\"\"Test that the LLMAttentionCondenser handles event IDs that aren't from the event history.\"\"\"\n     max_size = 2\n     condenser = LLMAttentionCondenser(max_size=max_size, keep_first=0, llm=mock_llm)\n \n-    for i in range(max_size * 10):\n-        event = create_test_event(f'Event {i}', id=i)\n-        mock_state.history.append(event)\n+    events = [create_test_event(f'Event {i}', id=i) for i in range(max_size * 10)]\n \n+    def set_response_content(history: list[Event]):\n         mock_llm.set_mock_response_content(\n             ImportantEventSelection(\n-                ids=[event.id for event in mock_state.history] + [-1, -2, -3, -4]\n+                ids=[event.id for event in history] + [-1, -2, -3, -4]\n             ).model_dump_json()\n         )\n-        results = condenser.condensed_history(mock_state)\n \n-        # The number of results should bounce back and forth between 1, 2, 1, 2, ...\n-        assert len(results) == (i % 2) + 1\n+    harness = RollingCondenserTestHarness(condenser)\n+    harness.add_callback(set_response_content)\n+\n+    for i, view in enumerate(harness.views(events)):\n+        assert len(view) == harness.expected_size(i, max_size)\n \n \n def test_llm_attention_condenser_handles_too_many_events(mock_llm, mock_state):\n\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR effectively tackles the condensation issues faced in the previous implementation. Your refactoring minimizes complexity while retaining functionality, particularly with the introduction of `CondensationAction`. However, ensure that debugging details are clear and that you're maintaining type safety with the union of event features. Consider whether saving `considered_event_ids` in the `State` rather than the event stream could streamline data management and compatibility. Lastly, refining error handling within condensers will significantly improve user experience for future sessions. Let's finalize and merge this while planning follow-ups for ongoing adjustments."
      }
    ],
    "codediff_length": 23158
  },
  {
    "index": 47,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/agenthub/coact_agent/executor/action_parser.py b/agenthub/coact_agent/executor/action_parser.py\nnew file mode 100644\nindex 000000000000..b15eb3bc6e29\n--- /dev/null\n+++ b/agenthub/coact_agent/executor/action_parser.py\n@@ -0,0 +1,71 @@\n+import re\n+\n+from agenthub.codeact_agent.action_parser import (\n+    CodeActActionParserAgentDelegate,\n+    CodeActActionParserCmdRun,\n+    CodeActActionParserFinish,\n+    CodeActActionParserIPythonRunCell,\n+    CodeActActionParserMessage,\n+    CodeActResponseParser,\n+)\n+from openhands.controller.action_parser import ActionParser\n+from openhands.events.action import (\n+    Action,\n+    AgentFinishAction,\n+)\n+\n+\n+class ExecutorResponseParser(CodeActResponseParser):\n+    \"\"\"Parser action:\n+    - CmdRunAction(command) - bash command to run\n+    - IPythonRunCellAction(code) - IPython code to run\n+    - AgentDelegateAction(agent, inputs) - delegate action for (sub)task\n+    - MessageAction(content) - Message action to run (e.g. ask for clarification)\n+    - AgentFinishAction() - end the interaction\n+    \"\"\"\n+\n+    def __init__(self):\n+        # Need pay attention to the item order in self.action_parsers\n+        super().__init__()\n+        self.action_parsers = [\n+            CodeActActionParserFinish(),\n+            CodeActActionParserCmdRun(),\n+            CodeActActionParserIPythonRunCell(),\n+            CodeActActionParserAgentDelegate(),\n+            CoActActionParserRequest(),\n+        ]\n+        self.default_parser = CodeActActionParserMessage()\n+\n+    def parse_response(self, response) -> str:\n+        action = response.choices[0].message.content\n+        if action is None:\n+            return ''\n+        for action_suffix in ['bash', 'ipython', 'browse', 'request']:\n+            if (\n+                f'<execute_{action_suffix}>' in action\n+                and f'</execute_{action_suffix}>' not in action\n+            ):\n+                action += f'</execute_{action_suffix}>'\n+        return action\n+\n+\n+class CoActActionParserRequest(ActionParser):\n+    def __init__(self):\n+        self.request = None\n+\n+    def check_condition(self, action_str: str) -> bool:\n+        self.request = re.search(\n+            r'<execute_request>(.*)</execute_request>', action_str, re.DOTALL\n+        )\n+        return self.request is not None\n+\n+    def parse(self, action_str: str) -> Action:\n+        assert (\n+            self.request is not None\n+        ), 'self.request should not be None when parse is called'\n+\n+        replan_request = self.request.group(1).strip()\n+        return AgentFinishAction(\n+            thought=replan_request,\n+            outputs={'content': replan_request},\n+        )\ndiff --git a/agenthub/coact_agent/executor/system_prompt.j2 b/agenthub/coact_agent/executor/system_prompt.j2\nnew file mode 100644\nindex 000000000000..ccc5e99f21ee\n--- /dev/null\n+++ b/agenthub/coact_agent/executor/system_prompt.j2\n@@ -0,0 +1,62 @@\n+{% set MINIMAL_SYSTEM_PREFIX %}\n+You are an autonomous intelligent programming agent playing the role of a subordinate employee responsible for local planning and execution of specific tasks in a multi-tier task execution structure, tasked with software development. You will be given coding-based tasks. The global agent has set a global plan for the tasks, divided into multiple phases. These phase plans will be given to you as a whole. Your responsibility is to perform them and return the results to the global agent. When you face some issues that require a new global plan, you can request a new global plan from the global planner agent.\n+\n+Here's the information you'll have:\n+* The user message and the broken-down phase list: These are the tasks you're trying to complete now. Because your plan may be part of a larger plan, you must only focus on the phases given to you.\n+* The current codebase: This is what you need to navigate through and make the changes to complete the tasks given by the global agent.\n+\n+The agent can use a Python environment with <execute_ipython>, e.g.:\n+<execute_ipython>\n+print(\"Hello World!\")\n+</execute_ipython>\n+\n+The agent can execute bash commands wrapped with <execute_bash>, e.g. <execute_bash> ls </execute_bash>.\n+If a bash command returns exit code `-1`, this means the process is not yet finished.\n+The assistant must then send a second <execute_bash>. The second <execute_bash> can be empty\n+(which will retrieve any additional logs), or it can contain text to be sent to STDIN of the running process,\n+or it can contain the text `ctrl+c` to interrupt the process.\n+\n+For commands that may run indefinitely, the output should be redirected to a file and the command run\n+in the background, e.g. <execute_bash> python3 app.py > server.log 2>&1 & </execute_bash>\n+If a command execution result says \"Command timed out. Sending SIGINT to the process\", the assistant should retry running the command in the background.\n+\n+As a local executor agent, there are some additional actions that you can use to communicate back to the global planner agent\n+- `<execute_request>`: You have encountered an exception in the execution process. You suspect problems with the global planner's plan and trigger a request for replanning. Explain why you decide to request a new global plan using this action.\n+\n+{% endset %}\n+{% set BROWSING_PREFIX %}\n+The agent can browse the Internet with <execute_browse> and </execute_browse>.\n+For example, <execute_browse> Tell me the usa's president using google search </execute_browse>.\n+Or <execute_browse> Tell me what is in http://example.com </execute_browse>.\n+{% endset %}\n+{% set PIP_INSTALL_PREFIX %}\n+The agent can install Python packages using the %pip magic command in an IPython environment by using the following syntax: <execute_ipython> %pip install [package needed] </execute_ipython> and should always import packages and define variables before starting to use them.\n+{% endset %}\n+{% set SYSTEM_PREFIX = MINIMAL_SYSTEM_PREFIX + BROWSING_PREFIX + PIP_INSTALL_PREFIX %}\n+{% set COMMAND_DOCS %}\n+Apart from the standard Python library, the agent can also use the following functions (already imported) in <execute_ipython> environment:\n+{{ agent_skills_docs }}\n+IMPORTANT:\n+- `open_file` only returns the first 100 lines of the file by default! The agent MUST use `scroll_down` repeatedly to read the full file BEFORE making edits!\n+- The agent shall adhere to THE `edit_file_by_replace`, `append_file` and `insert_content_at_line` FUNCTIONS REQUIRING PROPER INDENTATION. If the agent would like to add the line '        print(x)', it must fully write the line out, with all leading spaces before the code!\n+- Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n+- Any code issued should be less than 50 lines to avoid context being cut off!\n+- After EVERY `create_file` the method `append_file` shall be used to write the FIRST content!\n+- For `edit_file_by_replace` NEVER provide empty parameters!\n+- For `edit_file_by_replace` the file must be read fully before any replacements!\n+{% endset %}\n+{% set SYSTEM_SUFFIX %}\n+Responses should be concise.\n+The agent should attempt fewer things at a time instead of putting too many commands OR too much code in one \"execute\" block.\n+Include ONLY ONE <execute_ipython>, <execute_bash>, or <execute_browse> per response, unless the agent is finished with the task or needs more input or action from the user in order to proceed.\n+If the agent is finished with the task you MUST include <finish></finish> in your response.\n+IMPORTANT: Execute code using <execute_ipython>, <execute_bash>, or <execute_browse> whenever possible.\n+The agent should utilize full file paths and the `pwd` command to prevent path-related errors.\n+The agent must avoid apologies and thanks in its responses.\n+Remeber to execute ALL the phases of the global plan and only return the summary of the whole process to the global agent. Strictly follow it and do NOT do anything beyond the scope of the global plan.\n+\n+{% endset %}\n+{# Combine all parts without newlines between them #}\n+{{ SYSTEM_PREFIX -}}\n+{{- COMMAND_DOCS -}}\n+{{- SYSTEM_SUFFIX }}\ndiff --git a/agenthub/coact_agent/executor/user_prompt.j2 b/agenthub/coact_agent/executor/user_prompt.j2\nnew file mode 100644\nindex 000000000000..020d015a4228\n--- /dev/null\n+++ b/agenthub/coact_agent/executor/user_prompt.j2\n@@ -0,0 +1,290 @@\n+{% set DEFAULT_EXAMPLE %}\n+--- START OF EXAMPLE 1 ---\n+\n+USER: The user message is: \"Create a list of numbers from 1 to 10, and display them in a web page at port 5000. There's an existing Python file `app.py` that you can use.\"\n+Execute the following plan to fulfill it:\n+# Phases\n+## Phase 1\n+- description: Edit the existing Python file `app.py` to insert the code for starting a web server, at the end of the file.\n+- reason: The existing Python file is missing the code to start the web server. Normally, the server is started using the `app.run(port=5000)` command. This phase will add this necessary code in a __main__ block to start the server. You can use the `insert_content_at_line` function or other appropriate ways to add the code.\n+- expected_state: The Python file `app.py` is updated with the code to start the web server.\n+<file_path> /workspace/app.py </file_path>\n+<expected_content>\n+8|\n+9|if __name__ == '__main__':\n+10|    app.run(port=5000)\n+</expected_content>\n+## Phase 2\n+- description: Run the Python file `app.py` to display the list of numbers on a web page at port 5000.\n+- reason: After updating the Python file, we need to run it to display the list of numbers on a web page. The command a shell command can be: `python3 app.py`.\n+- expected_state: The list of numbers is displayed on a web page at port 5000.\n+\n+ASSISTANT:\n+Let's start with the first phase: editing the Python file `app.py` to insert the code for starting a web server at line 8. I will open the file at that line to check its current content:\n+<execute_ipython>\n+open_file('/workspace/app.py', 8)\n+</execute_ipython>\n+\n+USER:\n+OBSERVATION:\n+[File: /workspace/app.py (8 lines total)]\n+(this is the beginning of the file)\n+1|from flask import Flask\n+2|app = Flask(__name__)\n+3|\n+4|@app.route('/')\n+5|def index():\n+6|    numbers = list(range(1, 11))\n+7|    return str(numbers)\n+8|\n+(this is the end of the file)\n+\n+ASSISTANT:\n+As expected, the file is missing the code to start the web server. Now I will write the task from the plan and save it to the file `app.py`:\n+<execute_ipython>\n+EDITED_CODE=\"\"\"\n+if __name__ == '__main__':\n+    app.run(port=5000)\"\"\"\n+\n+insert_content_at_line(\n+  'app.py',\n+  8,\n+  EDITED_CODE,\n+)\n+</execute_ipython>\n+\n+USER:\n+OBSERVATION:\n+(this is the beginning of the file)\n+1|from flask import Flask\n+2|app = Flask(__name__)\n+3|\n+4|@app.route('/')\n+5|def index():\n+6|    numbers = list(range(1, 11))\n+7|    return str(numbers)\n+8|\n+9|if __name__ == '__main__':\n+10|    app.run(port=5000)\n+(this is the end of the file)\n+[File updated (edited at line 9). Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.]\n+\n+ASSISTANT:\n+I have updated the Python file `app.py` to properly start the web server and display a list of numbers from 1 to 10 when you run it. Let's proceed to phase 2 to run the Python file:\n+<execute_bash>\n+python3 app.py > server.log 2>&1 &\n+</execute_bash>\n+\n+USER:\n+OBSERVATION:\n+[1] 121[1]+  Exit 1                  python3 app.py > server.log 2>&1\n+\n+ASSISTANT: Looks like the server is running with PID 121 then crashed. Let me check the server log:\n+<execute_bash>\n+cat server.log\n+</execute_bash>\n+\n+USER:\n+OBSERVATION:\n+Traceback (most recent call last):\n+  File \"/workspace/app.py\", line 2, in <module>\n+    from flask import Flask\n+ModuleNotFoundError: No module named 'flask'\n+\n+ASSISTANT:\n+It seems that Flask is not installed. Let me install Flask for you:\n+<execute_bash>\n+pip install flask\n+</execute_bash>\n+\n+USER:\n+OBSERVATION:\n+Defaulting to user installation because normal site-packages is not writeable\n+Collecting flask\n+  Using cached flask-3.0.3-py3-none-any.whl (101 kB)\n+Collecting blinker>=1.6.2\n+  Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n+Collecting Werkzeug>=3.0.0\n+  Using cached werkzeug-3.0.2-py3-none-any.whl (226 kB)\n+Collecting click>=8.1.3\n+  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n+Collecting itsdangerous>=2.1.2\n+  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n+Requirement already satisfied: Jinja2>=3.1.2 in /home/openhands/.local/lib/python3.10/site-packages (from flask) (3.1.3)\n+Requirement already satisfied: MarkupSafe>=2.0 in /home/openhands/.local/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n+Installing collected packages: Werkzeug, itsdangerous, click, blinker, flask\n+Successfully installed Werkzeug-3.0.2 blinker-1.7.0 click-8.1.7 flask-3.0.3 itsdangerous-2.2.0\n+\n+ASSISTANT:\n+Now that Flask is installed, let me run the Python file again:\n+<execute_bash>\n+python3 app.py > server.log 2>&1 &\n+</execute_bash>\n+\n+USER:\n+OBSERVATION:\n+[1] 124\n+\n+ASSISTANT:\n+Let me check the server log again:\n+<execute_bash>\n+cat server.log\n+</execute_bash>\n+\n+USER:\n+OBSERVATION:\n+* Serving Flask app 'app'\n+ * Debug mode: off\n+WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n+ * Running on http://127.0.0.1:5000\n+Press CTRL+C to quit\n+\n+ASSISTANT:\n+The server is running on port 5000 with PID 124. You can access the list of numbers by visiting http://127.0.0.1:5000. The tasks are now complete.\n+Summary:\n+- Edited the existing Python file `app.py` to insert the code for starting a web server.\n+- Ran the Python file to display the list of numbers on a web page at port 5000.\n+<finish></finish>\n+\n+--- END OF EXAMPLE 1 ---\n+\n+--- START OF EXAMPLE 2 ---\n+\n+USER: The user message is: \"Now browse the newly started server's homepage and show me the content.\"\n+Execute the following plan to fulfill it:\n+# Phases\n+## Phase 1\n+- description: Browse the server's homepage at http://127.0.0.1:5000\n+- reason: To verify that the server is running and displaying the list of numbers correctly, we need to browse the server's homepage. We can use the `execute_browse` function to access the URL.\n+- expected_state: The content of the server's homepage is retrieved.\n+\n+ASSISTANT:\n+Sure! Let me start the first phase by browsing the server's homepage at http://127.0.0.1:5000:\n+<execute_browse>\n+Get the content on \"http://127.0.0.1:5000\"\n+</execute_browse>\n+\n+USER:\n+OBSERVATION:\n+[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n+\n+ASSISTANT:\n+The content of the server's homepage is \"[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\". The tasks are now complete.\n+Summary:\n+- Browsed the server's homepage at http://127.0.0.1:5000 and retrieved the content \"[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\".\n+<finish></finish>\n+\n+--- END OF EXAMPLE 2 ---\n+\n+--- START OF EXAMPLE 3 ---\n+\n+USER: The user message is: \"Now kill the server, make it display the numbers in a table format.\"\n+Execute the following plan to fulfill it:\n+# Phases\n+## Phase 1\n+- description: Stop the server running on port 5000.\n+- reason: The server needs to be stopped before modifying the Python code.\n+- expected_state: The server is stopped.\n+## Phase 2\n+- description: Modify the Python code to display the numbers in a table format.\n+- reason: The current code returns the numbers as a string, which is incorrect. We need to modify it, starting at line 6, to display the numbers in a table format by using HTML tags for table rows and cells.\n+- expected_state: The Python code is updated to display the numbers in a table format.\n+<file_path> /workspace/app.py </file_path>\n+<expected_content>\n+6|    numbers = list(range(1, 11))\n+7|    return \"<table>\" + \"\".join([f\"<tr><td>{num}</td></tr>\" for num in numbers]) + \"</table>\"\n+</expected_content>\n+## Phase 3\n+- description: Run the Python file again to display the numbers in a table format on the web page at port 5000.\n+- reason: After modifying the Python code, we need to run the file again to display the numbers in the expected table format.\n+- expected_state: The numbers are displayed in a table format on the web page.\n+\n+ASSISTANT:\n+Sure! Let me execute the first phase to stop the server first:\n+<execute_bash>\n+kill 124\n+</execute_bash>\n+\n+USER:\n+OBSERVATION:\n+[1]+  Terminated              python3 app.py > server.log 2>&1\n+\n+ASSISTANT:\n+The server has been stopped. Let me move to the second phase to open the Python file at line 6 and modify it.\n+<execute_ipython>\n+open_file('app.py', 6)\n+</execute_ipython>\n+\n+USER:\n+[File: /workspace/app.py (10 lines total)]\n+(this is the beginning of the file)\n+1|from flask import Flask\n+2|app = Flask(__name__)\n+3|\n+4|@app.route('/')\n+5|def index():\n+6|    numbers = list(range(1, 11))\n+7|    return str(numbers)\n+8|\n+9|if __name__ == '__main__':\n+10|    app.run(port=5000)\n+(this is the end of the file)\n+\n+ASSISTANT:\n+I should edit the file to display the numbers in a table format. I should include correct indentation. Let me update the file:\n+<execute_ipython>\n+edit_file_by_replace(\n+  'app.py',\n+  to_replace=\"    return str(numbers)\",\n+  new_content=\"    return '<table>' + ''.join([f'<tr><td>{i}</td></tr>' for i in numbers]) + '</table>'\",\n+)\n+</execute_ipython>\n+\n+USER:\n+Observation:\n+[File: /workspace/app.py (10 lines total after edit)]\n+(this is the beginning of the file)\n+1|from flask import Flask\n+2|app = Flask(__name__)\n+3|\n+4|@app.route('/')\n+5|def index():\n+6|    numbers = list(range(1, 11))\n+7|    return '<table>' + ''.join([f'<tr><td>{i}</td></tr>' for i in numbers]) + '</table>'\n+8|\n+9|if __name__ == '__main__':\n+10|    app.run(port=5000)\n+(this is the end of the file)\n+[File updated (edited at line 7). Please review the changes and make sure they are correct (correct indentation, no duplicate lines, etc). Edit the file again if necessary.]\n+\n+ASSISTANT:\n+Now, let's move the third phase: to run the updated file:\n+<execute_bash>\n+python3 app.py > server.log 2>&1 &\n+</execute_bash>\n+\n+USER:\n+Observation:\n+[1] 126\n+\n+ASSISTANT:\n+The server is running on port 5000 with PID 126. You can access the list of numbers in a table format by visiting http://127.0.0.1:5000. The tasks are now complete.\n+Summary:\n+- Stopped the server running on port 5000.\n+- Modified the Python code to display the numbers in a table format.\n+- Ran the Python file again to display the numbers in a table format on the web page at port 5000.\n+<finish></finish>\n+\n+--- END OF EXAMPLE 3 ---\n+{% endset %}\n+Here is an example of how you can interact with the environment for task solving:\n+{{ DEFAULT_EXAMPLE }}\n+{% if micro_agent %}\n+--- BEGIN OF GUIDELINE ---\n+The following information may assist you in completing your task:\n+\n+{{ micro_agent }}\n+--- END OF GUIDELINE ---\n+{% endif %}\n+\n+NOW, LET'S START!\ndiff --git a/agenthub/coact_agent/planner/action_parser.py b/agenthub/coact_agent/planner/action_parser.py\nnew file mode 100644\nindex 000000000000..955e02a17442\n--- /dev/null\n+++ b/agenthub/coact_agent/planner/action_parser.py\n@@ -0,0 +1,100 @@\n+import re\n+\n+from agenthub.codeact_agent.action_parser import (\n+    CodeActActionParserAgentDelegate,\n+    CodeActActionParserCmdRun,\n+    CodeActActionParserFinish,\n+    CodeActActionParserIPythonRunCell,\n+    CodeActActionParserMessage,\n+    CodeActResponseParser,\n+)\n+from openhands.controller.action_parser import ActionParser\n+from openhands.events.action import (\n+    Action,\n+    AgentDelegateAction,\n+)\n+\n+\n+class PlannerResponseParser(CodeActResponseParser):\n+    \"\"\"Parser action:\n+    - CmdRunAction(command) - bash command to run\n+    - IPythonRunCellAction(code) - IPython code to run\n+    - AgentDelegateAction(agent, inputs) - delegate action for (sub)task\n+    - MessageAction(content) - Message action to run (e.g. ask for clarification)\n+    - AgentFinishAction() - end the interaction\n+    \"\"\"\n+\n+    def __init__(self, initial_task_str=None):\n+        # Need pay attention to the item order in self.action_parsers\n+        super().__init__()\n+        self.action_parsers = [\n+            CodeActActionParserFinish(),\n+            CodeActActionParserCmdRun(),\n+            CodeActActionParserIPythonRunCell(),\n+            CodeActActionParserAgentDelegate(),\n+            CoActActionParserGlobalPlan(initial_task_str=initial_task_str),\n+        ]\n+        self.default_parser = CodeActActionParserMessage()\n+\n+    def parse_response(self, response) -> str:\n+        action = response.choices[0].message.content\n+        if action is None:\n+            return ''\n+        for action_suffix in [\n+            'bash',\n+            'ipython',\n+            'browse',\n+            'global_plan',\n+            'decide',\n+            'revise',\n+            'overrule',\n+            'collation',\n+        ]:\n+            if (\n+                f'<execute_{action_suffix}>' in action\n+                and f'</execute_{action_suffix}>' not in action\n+            ):\n+                action += f'</execute_{action_suffix}>'\n+        return action\n+\n+\n+class CoActActionParserGlobalPlan(ActionParser):\n+    \"\"\"Parser action:\n+    - AgentDelegateAction(agent, inputs) - delegate action for (sub)task\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        initial_task_str: list | None = None,\n+    ):\n+        self.global_plan: re.Match | None = None\n+        self.initial_task_str = initial_task_str or ['']\n+\n+    def check_condition(self, action_str: str) -> bool:\n+        self.global_plan = re.search(\n+            r'<execute_global_plan>(.*)</execute_global_plan>', action_str, re.DOTALL\n+        )\n+        return self.global_plan is not None\n+\n+    def parse(self, action_str: str) -> Action:\n+        assert (\n+            self.global_plan is not None\n+        ), 'self.global_plan should not be None when parse is called'\n+        thought = action_str.replace(self.global_plan.group(0), '').strip()\n+        global_plan_actions = self.global_plan.group(1).strip()\n+\n+        # Some extra processing when doing swe-bench eval: extract text up to and including '--- END ISSUE ---'\n+        issue_text_pattern = re.compile(r'(.*--- END ISSUE ---)', re.DOTALL)\n+        issue_text_match = issue_text_pattern.match(self.initial_task_str[0])\n+\n+        if issue_text_match:\n+            self.initial_task_str[0] = issue_text_match.group(1)\n+\n+        return AgentDelegateAction(\n+            agent='CoActExecutorAgent',\n+            thought=thought,\n+            inputs={\n+                'task': f'The user message is: {self.initial_task_str[0]}.\\nExecute the following plan to fulfill it:\\n{global_plan_actions}'\n+            },\n+            action_suffix='global_plan',\n+        )\ndiff --git a/agenthub/coact_agent/planner/planner_agent.py b/agenthub/coact_agent/planner/planner_agent.py\nnew file mode 100644\nindex 000000000000..cbacb8faf777\n--- /dev/null\n+++ b/agenthub/coact_agent/planner/planner_agent.py\n@@ -0,0 +1,51 @@\n+import os\n+\n+from agenthub.coact_agent.planner.action_parser import PlannerResponseParser\n+from agenthub.codeact_agent.codeact_agent import CodeActAgent\n+from openhands.core.config import AgentConfig\n+from openhands.llm.llm import LLM\n+from openhands.runtime.plugins.agent_skills.agentskills import (\n+    DOCUMENTATION_DICT as AGENTSKILLS_DOCS_DICT,\n+)\n+from openhands.utils.prompt import PromptManager\n+\n+\n+class GlobalPlannerAgent(CodeActAgent):\n+    VERSION = '1.0'\n+\n+    def __init__(self, llm: LLM, config: AgentConfig) -> None:\n+        super().__init__(llm, config)\n+\n+        self.action_parser = PlannerResponseParser(\n+            initial_task_str=self.initial_task_str\n+        )\n+\n+        # Planner agent can do everything except file-editing operations\n+        planner_agentskills_exclude = [\n+            'create_file',\n+            'edit_file_by_replace',\n+            'insert_content_at_line',\n+            'append_file',\n+        ]\n+        planner_agentskills = [\n+            v\n+            for k, v in AGENTSKILLS_DOCS_DICT.items()\n+            if k not in planner_agentskills_exclude\n+        ]\n+        executor_editing_agentskills = [\n+            v\n+            for k, v in AGENTSKILLS_DOCS_DICT.items()\n+            if k in planner_agentskills_exclude\n+        ]\n+        self.prompt_manager = PromptManager(\n+            prompt_dir=os.path.join(os.path.dirname(__file__)),\n+            agent_skills_docs=''.join(planner_agentskills),\n+            system_extra_vars={\n+                'executor_editing_agent_skills_docs': ''.join(\n+                    executor_editing_agentskills\n+                )\n+            },\n+            micro_agent=self.micro_agent,\n+        )\n+\n+        self.stop_sequences.append('</execute_global_plan>')\ndiff --git a/agenthub/coact_agent/planner/system_prompt.j2 b/agenthub/coact_agent/planner/system_prompt.j2\nnew file mode 100644\nindex 000000000000..2dc430ba9fc6\n--- /dev/null\n+++ b/agenthub/coact_agent/planner/system_prompt.j2\n@@ -0,0 +1,95 @@\n+{% set MINIMAL_SYSTEM_PREFIX %}\n+You are an autonomous intelligent agent playing the role of a strategic leader in a multi-tier task execution structure, tasked with performing programming tasks. You will be given coding-related tasks. Your responsibility is to provide high-level, strategic plans with phases that can be executed by the local agent and thorough explanations for each phase for local agent's understanding.\n+\n+Here's the information you'll have:\n+* The user's objective: This is the task you're trying to complete.\n+* The user's codebase: This is what you need to navigate and use to define the plan.\n+\n+The structure of the global plan should be as follows:\n+<execute_global_plan>\n+# Phases\n+## Phase 1\n+- reason: <<Assistant's thorough thoughts on why this phase is necessary. The planner can add some tips/codes to instruct the executor finish the task easier.>>\n+- description: <<Describe what needs to be done in this phase.>>\n+- expected_state: <<Describe the expected state after this phase is completed. If the task involves code editing, provide the expectation of the code after the edit.>>\n+<file_path> <<Optinal - Provide the file path here. In one phase only 1 file is edited.>> </file_path>\n+<expected_content>\n+<<Optional - Provide the expected content here WITH LINE NUMBERS and a vertical bar before the actual code e.g., 1|, 11|,. Only use partial content to avoid repetition.>>\n+</expected_content>\n+## Phase 2\n+- reason: ...\n+- description: ...\n+- expected_state: ...\n+<file_path> ... </file_path>\n+<expected_content>\n+...|...\n+</expected_content>\n+## Phase ...\n+</execute_global_plan>\n+\n+You can use a Python environment with <execute_ipython>, e.g.:\n+<execute_ipython>\n+print(\"Hello World!\")\n+</execute_ipython>\n+\n+The agent can execute bash commands wrapped with <execute_bash>, e.g. <execute_bash> ls </execute_bash>.\n+If a bash command returns exit code `-1`, this means the process is not yet finished.\n+The assistant must then send a second <execute_bash>. The second <execute_bash> can be empty\n+(which will retrieve any additional logs), or it can contain text to be sent to STDIN of the running process,\n+or it can contain the text `ctrl+c` to interrupt the process.\n+\n+For commands that may run indefinitely, the output should be redirected to a file and the command run\n+in the background, e.g. <execute_bash> python3 app.py > server.log 2>&1 & </execute_bash>\n+If a command execution result says \"Command timed out. Sending SIGINT to the process\", the assistant should retry running the command in the background.\n+\n+As a planner agent, there are some additional actions that you can use to communicate and delegate tasks to the local agent\n+- `<execute_global_plan>`: Construct a multi-stage global plan, providing separate subtask descriptions and expected states for each phase.\n+{% endset %}\n+{% set PIP_INSTALL_PREFIX %}\n+The agent can install Python packages using the %pip magic command in an IPython environment by using the following syntax: <execute_ipython> %pip install [package needed] </execute_ipython> and should always import packages and define variables before starting to use them.\n+{% endset %}\n+{% set SYSTEM_PREFIX = MINIMAL_SYSTEM_PREFIX + PIP_INSTALL_PREFIX %}\n+{% set COMMAND_DOCS %}\n+Apart from the standard Python library, the agent can also use the following functions (already imported) in <execute_ipython> environment:\n+{{ planner_agent_skills_docs }}\n+IMPORTANT:\n+- Prefer to use these functions to interact with the codebase if possible, instead of using bash commands like `cat`, which are less efficient when the files are very large.\n+- `open_file` only returns the first 100 lines of the file by default! The agent MUST use `scroll_down` repeatedly to read the full file BEFORE making edits!\n+- NEVER use MORE THAN 5 consecutive `scroll_down` commands. When you use `open_file` and see that the file is too large, try to use search functions above to locate the relevant code, which can save you a lot of time.\n+- Don't be too tempted to construct your own functions, instead prefer to use above provided functions to interact with the codebase.\n+\n+IMPORTANT FOR CREATING FUNCTION_CALLS FOR LOCAL AGENT:\n+- The call shall adhere to THE `edit_file_by_replace`, `append_file` and `insert_content_at_line` FUNCTIONS REQUIRING PROPER INDENTATION. If the agent would like to add the line '        print(x)', it must fully write the line out, with all leading spaces before the code!\n+- Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\n+- Any code issued should be less than 50 lines to avoid context being cut off!\n+- After EVERY `create_file` the method `append_file` shall be used to write the FIRST content!\n+- For `edit_file_by_replace` NEVER provide empty parameters!\n+- For `edit_file_by_replace` the file must be read fully before any replacements!\n+{% endset %}\n+{% set EXECUTOR_SKILLS %}\n+Importantly, apart from the above functions, the local executor agent can use the following additional ones (already imported) in <execute_ipython> environment:\n+{{ executor_editing_agent_skills_docs }}\n+Create plans with phases that use those functions and always include all necessary parameters for editing-related functions so that the local agent doesn't need to waste resources to find them again.\n+{% endset %}\n+{% set SYSTEM_SUFFIX %}\n+{% endset %}\n+{% set SYSTEM_SUFFIX %}\n+Responses should be concise.\n+The agent should attempt fewer things at a time instead of putting too many commands OR too much code in one \"execute\" block.\n+Include ONLY ONE <execute_ipython>, or <execute_bash> per response, unless the agent is finished with the task or needs more input or action from the user in order to proceed.\n+If the agent is finished with the task you MUST include <finish></finish> in your response.\n+IMPORTANT: Execute code using <execute_ipython>, or <execute_bash> whenever possible.\n+The agent should utilize full file paths and the `pwd` command to prevent path-related errors.\n+The agent must avoid apologies and thanks in its responses.\n+The planner agent can only use <execute_browse> to browse the Internet if it's required to come up with the plan. To delegate the plan, please only use <execute_global_plan>.\n+The planner agent can navigate the codebase to locate the necessary information to construct the plan.\n+After the local agent successfully completes the plan without any issues, you can collate the result and if it is good, you can exit.\n+Remember to create an as-detailed-as-possible plan for the local agent to follow, but with minimal number of phases.\n+Do NOT repeat what you have done into the plan, only provide the minimal required steps remaining for the local agent to follow to resolve the task. For example, if you have examined the codebase with a lot of steps and found the relevant code, just provide them with the plan to edit it.\n+The planner agent must NEVER perform the plan by itself and must delegate the plan to the local agent using <execute_global_plan>. Do not go beyond this scope.\n+\n+{% endset %}\n+{# Combine all parts without newlines between them #}\n+{{ SYSTEM_PREFIX -}}\n+{{- COMMAND_DOCS -}}\n+{{- SYSTEM_SUFFIX }}\ndiff --git a/agenthub/coact_agent/planner/user_prompt.j2 b/agenthub/coact_agent/planner/user_prompt.j2\nnew file mode 100644\nindex 000000000000..5cf7b3bbd40d\n--- /dev/null\n+++ b/agenthub/coact_agent/planner/user_prompt.j2\n@@ -0,0 +1,181 @@\n+{% set DEFAULT_EXAMPLE %}\n+--- START OF EXAMPLE 1 ---\n+\n+USER: I want to go to the user 'Every-Hands-AI' github profile using browser and check how many repos they have, then write the number to the new 'repo_count.txt' file. Use a plan that has the least number of steps.\n+\n+ASSISTANT:\n+I have enough information, let's create a plan to achieve the task:\n+<execute_global_plan>\n+# Phases\n+## Phase 1\n+- reason: As the first step, we need to identify the number of repositories in the user's GitHub profile.\n+- description: Browse the GitHub profile of the user 'Every-Hands-AI' to find the number of repositories.\n+- expected_state: The number of repositories is identified.\n+## Phase 2\n+- reason: The number of repositories should be stored in a file as requested by the user.\n+- description: Create a new file `repo_count.txt` and write the number of repositories to it.\n+- expected_state: The file `repo_count.txt` is created and contains the number of repositories.\n+</execute_global_plan>\n+\n+USER:\n+OBSERVATION:\n+LocalExecutorAgent finishes task with content: The GitHub profile 'Every-Hands-AI' does not exist or is not accessible. Please provide a new global plan with a valid GitHub username or an alternative task.\n+\n+ASSISTANT:\n+It seems the GitHub profile 'Every-Hands-AI' does not exist or is not accessible. Please provide a valid GitHub username or an alternative task.\n+\n+USER:\n+Sorry, the correct github handle is \"All-Hands-AI\".\n+\n+ASSISTANT:\n+Let's create a new plan to achieve the task:\n+<execute_global_plan>\n+# Phases\n+## Phase 1\n+- reason: As the first step, we need to identify the number of repositories in the user's GitHub profile.\n+- description: Browse the GitHub profile of the user 'All-Hands-AI' to find the number of repositories.\n+- expected_state: The number of repositories is identified.\n+## Phase 2\n+- reason: The number of repositories should be stored in a file as requested by the user.\n+- description: Create a new file `repo_count.txt` and write the number of repositories to it.\n+- expected_state: The file `repo_count.txt` is created and contains the number of repositories.\n+</execute_global_plan>\n+\n+USER:\n+OBSERVATION:\n+LocalExecutorAgent finishes task with content: The file repo_count.txt has been successfully updated with the number of repositories. The tasks are now complete.\n+Summary:\n+- Browsed the GitHub profile of the user 'All-Hands-AI' and found that the number of repositories is 2.\n+- Appended the number of repositories to the file repo_count.txt.\n+\n+ASSISTANT:\n+The tasks have been successfully completed. The file repo_count.txt has been created and updated with the number of repositories (2) for the user 'All-Hands-AI'.\n+If you have any further requests, feel free to ask. Otherwise, I will consider the task complete.\n+<finish></finish>\n+\n+--- END OF EXAMPLE 1 ---\n+\n+--- START OF EXAMPLE 2 ---\n+\n+USER: Create a list of numbers from 1 to 10, and display them in a web page at port 5000. There's an existing Python file `app.py` that you can use.\n+\n+ASSISTANT: Let's check the existing Python file `app.py`:\n+<execute_ipython>\n+open_file('app.py')\n+</execute_ipython>\n+\n+USER:\n+OBSERVATION:\n+[File: /workspace/app.py (8 lines total)]\n+(this is the beginning of the file)\n+1|from flask import Flask\n+2|app = Flask(__name__)\n+3|\n+4|@app.route('/')\n+5|def index():\n+6|    numbers = list(range(1, 11))\n+7|    return str(numbers)\n+8|\n+(this is the end of the file)\n+\n+ASSISTANT:\n+Great! Seems like the existing Python file `app.py` is ready to display a list of numbers from 1 to 10, but it's missing the code to start the web server. Let me create a plan to achieve the task:\n+<execute_global_plan>\n+# Phases\n+## Phase 1\n+- reason: The existing Python file is missing the code to start the web server. Normally, the server is started using the `app.run(port=5000)` command. This phase will add this necessary code in a __main__ block to start the server. You can use the `insert_content_at_line` function or other appropriate ways to add the code.\n+- description: Edit the existing Python file `app.py` to insert the code for starting a web server, at the end of the file.\n+- expected_state: The Python file `app.py` is updated with the code to start the web server.\n+<file_path> /workspace/app.py </file_path>\n+<expected_content>\n+8|\n+9|if __name__ == '__main__':\n+10|    app.run(port=5000)\n+</expected_content>\n+## Phase 2\n+- reason: After updating the Python file, we need to run it to display the list of numbers on a web page. The command a shell command can be: `python3 app.py`.\n+- description: Run the Python file `app.py` to display the list of numbers on a web page at port 5000.\n+- expected_state: The list of numbers is displayed on a web page at port 5000.\n+</execute_global_plan>\n+\n+USER:\n+OBSERVATION:\n+LocalExecutorAgent finishes task with content: The server is running on port 5000 with PID 124. You can access the list of numbers by visiting http://127.0.0.1:5000. The tasks are now complete.\n+Summary:\n+- Edited the existing Python file `app.py` to insert the code for starting a web server.\n+- Ran the Python file to display the list of numbers on a web page at port 5000.\n+\n+ASSISTANT:\n+The server is running on port 5000 with PID 124. You can access the list of numbers by visiting http://127.0.0.1:5000. If you have any further questions, feel free to ask!\n+\n+USER: Now browse the newly started server's homepage and show me the content.\n+\n+ASSISTANT:\n+Sure, let me create a plan to achieve the task:\n+<execute_global_plan>\n+# Phases\n+## Phase 1\n+- reason: To verify that the server is running and displaying the list of numbers correctly, we need to browse the server's homepage. We can use the `execute_browse` function to access the URL.\n+- description: Browse the server's homepage at http://127.0.0.1:5000\n+- expected_state: The content of the server's homepage is retrieved.\n+</execute_global_plan>\n+\n+USER:\n+OBSERVATION:\n+LocalExecutorAgent finishes task with content: The content of the server's homepage is \"[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\". The tasks are now complete.\n+Summary:\n+- Browsed the server's homepage at http://127.0.0.1:5000 and retrieved the content \"[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\".\n+\n+ASSISTANT:\n+The content of the server's homepage is \"[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\". If you have any further questions, feel free to ask!\n+\n+USER: Now kill the server, make it display the numbers in a table format.\n+\n+ASSISTANT:\n+Sure! Let me create a plan to achieve the task:\n+<execute_global_plan>\n+# Phases\n+## Phase 1\n+- reason: The server needs to be stopped before modifying the Python code.\n+- description: Stop the server running on port 5000.\n+- expected_state: The server is stopped.\n+## Phase 2\n+- reason: The current code returns the numbers as a string, which is incorrect. We need to modify it, starting at line 6, to display the numbers in a table format by using HTML tags for table rows and cells.\n+- description: Modify the Python code to display the numbers in a table format.\n+- expected_state: The Python code is updated to display the numbers in a table format.\n+<file_path> /workspace/app.py </file_path>\n+<expected_content>\n+6|    numbers = list(range(1, 11))\n+7|    return \"<table>\" + \"\".join([f\"<tr><td>{num}</td></tr>\" for num in numbers]) + \"</table>\"\n+</expected_content>\n+## Phase 3\n+- reason: After modifying the Python code, we need to run the file again to display the numbers in the expected table format.\n+- description: Run the Python file again to display the numbers in a table format on the web page at port 5000.\n+- expected_state: The numbers are displayed in a table format on the web page.\n+</execute_global_plan>\n+\n+USER:\n+OBSERVATION:\n+LocalExecutorAgent finishes task with content: The server is running on port 5000 with PID 126. You can access the list of numbers in a table format by visiting http://127.0.0.1:5000. The tasks are now complete.\n+Summary:\n+- Stopped the server running on port 5000.\n+- Modified the Python code to display the numbers in a table format.\n+- Ran the Python file again to display the numbers in a table format on the web page at port 5000.\n+\n+ASSISTANT:\n+The server is running on port 5000 with PID 126. You can access the list of numbers in a table format by visiting http://127.0.0.1:5000. Let me know if you have any further requests!\n+<finish></finish>\n+\n+--- END OF EXAMPLE 2 ---\n+{% endset %}\n+Here is an example of how you can interact with the environment for task solving:\n+{{ DEFAULT_EXAMPLE }}\n+{% if micro_agent %}\n+--- BEGIN OF GUIDELINE ---\n+The following information may assist you in completing your task:\n+\n+{{ micro_agent }}\n+--- END OF GUIDELINE ---\n+{% endif %}\n+\n+NOW, LET'S START!\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR presents initial testing results for CoAct and CodeAct v1.8 across 13 common instances. CoAct resolved 8/13, while CodeAct v1.8 managed 9/13, which is decent for a first draft. There are significant issues, notably with timeout and multiprocessing when utilizing more than one worker. The agent's trajectory evaluation lacks clarity, and the serialization of actions differs from CodeAct's usual format, potentially causing confusion. Before proceeding, further evaluation on a broader set is recommended, as the current results may not be robust enough for acceptance."
      }
    ],
    "codediff_length": 41074
  },
  {
    "index": 48,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/openhands/core/config/mcp_config.py b/openhands/core/config/mcp_config.py\nnew file mode 100644\nindex 000000000000..1a80f033223a\n--- /dev/null\n+++ b/openhands/core/config/mcp_config.py\n@@ -0,0 +1,68 @@\n+from typing import List\n+from urllib.parse import urlparse\n+\n+from pydantic import BaseModel, Field, ValidationError\n+\n+\n+class MCPSSEConfig(BaseModel):\n+    \"\"\"Configuration for MCP SSE (Server-Sent Events) settings.\n+\n+    Attributes:\n+        mcp_servers: List of MCP server URLs.\n+    \"\"\"\n+\n+    mcp_servers: List[str] = Field(default_factory=list)\n+\n+    model_config = {'extra': 'forbid'}\n+\n+    def validate_servers(self) -> None:\n+        \"\"\"Validate that server URLs are valid and unique.\"\"\"\n+        # Check for duplicate server URLs\n+        if len(set(self.mcp_servers)) != len(self.mcp_servers):\n+            raise ValueError('Duplicate MCP server URLs are not allowed')\n+\n+        # Validate URLs\n+        for url in self.mcp_servers:\n+            try:\n+                result = urlparse(url)\n+                if not all([result.scheme, result.netloc]):\n+                    raise ValueError(f'Invalid URL format: {url}')\n+            except Exception as e:\n+                raise ValueError(f'Invalid URL {url}: {str(e)}')\n+\n+\n+class MCPConfig(BaseModel):\n+    \"\"\"Configuration for MCP (Message Control Protocol) settings.\n+\n+    Attributes:\n+        sse: SSE-specific configuration.\n+    \"\"\"\n+\n+    sse: MCPSSEConfig = Field(default_factory=MCPSSEConfig)\n+\n+    model_config = {'extra': 'forbid'}\n+\n+    @classmethod\n+    def from_toml_section(cls, data: dict) -> dict[str, 'MCPConfig']:\n+        \"\"\"\n+        Create a mapping of MCPConfig instances from a toml dictionary representing the [mcp] section.\n+\n+        The configuration is built from all keys in data.\n+\n+        Returns:\n+            dict[str, MCPConfig]: A mapping where the key \"mcp\" corresponds to the [mcp] configuration\n+        \"\"\"\n+        # Initialize the result mapping\n+        mcp_mapping: dict[str, MCPConfig] = {}\n+\n+        try:\n+            # Create SSE config if present\n+            sse_config = MCPSSEConfig.model_validate(data)\n+            sse_config.validate_servers()\n+\n+            # Create the main MCP config\n+            mcp_mapping['mcp'] = cls(sse=sse_config)\n+        except ValidationError as e:\n+            raise ValueError(f'Invalid MCP configuration: {e}')\n+\n+        return mcp_mapping\ndiff --git a/openhands/mcp/client.py b/openhands/mcp/client.py\nnew file mode 100644\nindex 000000000000..1a50aacc63fb\n--- /dev/null\n+++ b/openhands/mcp/client.py\n@@ -0,0 +1,98 @@\n+from contextlib import AsyncExitStack\n+from typing import Dict, List, Optional\n+\n+from mcp import ClientSession\n+from mcp.client.sse import sse_client\n+from pydantic import BaseModel, Field\n+\n+from openhands.core.logger import openhands_logger as logger\n+from openhands.mcp.tool import BaseTool, MCPClientTool\n+\n+\n+class MCPClient(BaseModel):\n+    \"\"\"\n+    A collection of tools that connects to an MCP server and manages available tools through the Model Context Protocol.\n+    \"\"\"\n+\n+    session: Optional[ClientSession] = None\n+    exit_stack: AsyncExitStack = AsyncExitStack()\n+    description: str = 'MCP client tools for server interaction'\n+    tools: List[BaseTool] = Field(default_factory=list)\n+    tool_map: Dict[str, BaseTool] = Field(default_factory=dict)\n+\n+    class Config:\n+        arbitrary_types_allowed = True\n+\n+    async def connect_sse(self, server_url: str, timeout: float = 30.0) -> None:\n+        \"\"\"Connect to an MCP server using SSE transport.\n+\n+        Args:\n+            server_url: The URL of the SSE server to connect to.\n+            timeout: Connection timeout in seconds. Default is 30 seconds.\n+        \"\"\"\n+        if not server_url:\n+            raise ValueError('Server URL is required.')\n+        if self.session:\n+            await self.disconnect()\n+\n+        try:\n+            streams_context = sse_client(\n+                url=server_url,\n+            )\n+            streams = await self.exit_stack.enter_async_context(streams_context)\n+            self.session = await self.exit_stack.enter_async_context(\n+                ClientSession(*streams)\n+            )\n+\n+            await self._initialize_and_list_tools()\n+        except Exception as e:\n+            logger.error(f'Error connecting to {server_url}: {str(e)}')\n+            raise\n+\n+    async def _initialize_and_list_tools(self) -> None:\n+        \"\"\"Initialize session and populate tool map.\"\"\"\n+        if not self.session:\n+            raise RuntimeError('Session not initialized.')\n+\n+        await self.session.initialize()\n+        response = await self.session.list_tools()\n+\n+        # Clear existing tools\n+        self.tools = []\n+\n+        # Create proper tool objects for each server tool\n+        for tool in response.tools:\n+            server_tool = MCPClientTool(\n+                name=tool.name,\n+                description=tool.description,\n+                inputSchema=tool.inputSchema,\n+                session=self.session,\n+            )\n+            self.tool_map[tool.name] = server_tool\n+            self.tools.append(server_tool)\n+\n+        logger.info(\n+            f'Connected to server with tools: {[tool.name for tool in response.tools]}'\n+        )\n+\n+    async def call_tool(self, tool_name: str, args: Dict):\n+        \"\"\"Call a tool on the MCP server.\"\"\"\n+        if tool_name not in self.tool_map:\n+            raise ValueError(f'Tool {tool_name} not found.')\n+        return await self.tool_map[tool_name].execute(**args)\n+\n+    async def disconnect(self) -> None:\n+        \"\"\"Disconnect from the MCP server and clean up resources.\"\"\"\n+        if self.session:\n+            try:\n+                # Close the session first\n+                if hasattr(self.session, 'close'):\n+                    await self.session.close()\n+                # Then close the exit stack\n+                await self.exit_stack.aclose()\n+            except Exception as e:\n+                logger.error(f'Error during disconnect: {str(e)}')\n+            finally:\n+                self.session = None\n+                self.tools = []\n+                logger.info('Disconnected from MCP server')\ndiff --git a/openhands/mcp/tool.py b/openhands/mcp/tool.py\nnew file mode 100644\nindex 000000000000..71d73f046aa4\n--- /dev/null\n+++ b/openhands/mcp/tool.py\n@@ -0,0 +1,54 @@\n+from abc import ABC, abstractmethod\n+from typing import Dict, Optional\n+\n+from mcp import ClientSession\n+from mcp.types import CallToolResult, TextContent, Tool\n+\n+\n+class BaseTool(ABC, Tool):\n+    @classmethod\n+    def postfix(cls) -> str:\n+        return '_mcp_tool_call'\n+\n+    class Config:\n+        arbitrary_types_allowed = True\n+\n+    @abstractmethod\n+    async def execute(self, **kwargs) -> CallToolResult:\n+        \"\"\"Execute the tool with given parameters.\"\"\"\n+\n+    def to_param(self) -> Dict:\n+        \"\"\"Convert tool to function call format.\"\"\"\n+        return {\n+            'type': 'function',\n+            'function': {\n+                'name': self.name + self.postfix(),\n+                'description': self.description,\n+                'parameters': self.inputSchema,\n+            },\n+        }\n+\n+\n+class MCPClientTool(BaseTool):\n+    \"\"\"Represents a tool proxy that can be called on the MCP server from the client side.\"\"\"\n+\n+    session: Optional[ClientSession] = None\n+\n+    async def execute(self, **kwargs) -> CallToolResult:\n+        \"\"\"Execute the tool by making a remote call to the MCP server.\"\"\"\n+        if not self.session:\n+            return CallToolResult(\n+                content=[TextContent(text='Not connected to MCP server', type='text')],\n+                isError=True,\n+            )\n+\n+        try:\n+            result = await self.session.call_tool(self.name, kwargs)\n+            return result\n+        except Exception as e:\n+            return CallToolResult(\n+                content=[\n+                    TextContent(text=f'Error executing tool: {str(e)}', type='text')\n+                ],\n+                isError=True,\n+            )\ndiff --git a/openhands/mcp/utils.py b/openhands/mcp/utils.py\nnew file mode 100644\nindex 000000000000..c8894f1ca9a2\n--- /dev/null\n+++ b/openhands/mcp/utils.py\n@@ -0,0 +1,135 @@\n+import json\n+\n+from openhands.core.config.mcp_config import MCPConfig\n+from openhands.core.logger import openhands_logger as logger\n+from openhands.events.action.mcp import McpAction\n+from openhands.events.observation.mcp import MCPObservation\n+from openhands.events.observation.observation import Observation\n+from openhands.mcp.client import MCPClient\n+\n+\n+def convert_mcp_clients_to_tools(mcp_clients: list[MCPClient] | None) -> list[dict]:\n+    \"\"\"\n+    Converts a list of MCPClient instances to ChatCompletionToolParam format\n+    that can be used by CodeActAgent.\n+\n+    Args:\n+        mcp_clients: List of MCPClient instances or None\n+\n+    Returns:\n+        List of dicts of tools ready to be used by CodeActAgent\n+    \"\"\"\n+    if mcp_clients is None:\n+        logger.warning('mcp_clients is None, returning empty list')\n+        return []\n+\n+    all_mcp_tools = []\n+    try:\n+        for client in mcp_clients:\n+            # Each MCPClient has an mcp_clients property that is a ToolCollection\n+            # The ToolCollection has a to_params method that converts tools to ChatCompletionToolParam format\n+            for tool in client.tools:\n+                mcp_tools = tool.to_param()\n+                all_mcp_tools.append(mcp_tools)\n+    except Exception as e:\n+        logger.error(f'Error in convert_mcp_clients_to_tools: {e}')\n+        return []\n+    return all_mcp_tools\n+\n+\n+async def create_mcp_clients(\n+    sse_mcp_server: list[str],\n+) -> list[MCPClient]:\n+    mcp_clients: list[MCPClient] = []\n+    # Initialize SSE connections\n+    if sse_mcp_server:\n+        for server_url in sse_mcp_server:\n+            logger.info(\n+                f'Initializing MCP agent for {server_url} with SSE connection...'\n+            )\n+\n+            client = MCPClient()\n+            try:\n+                await client.connect_sse(server_url)\n+                # Only add the client to the list after a successful connection\n+                mcp_clients.append(client)\n+                logger.info(f'Connected to MCP server {server_url} via SSE')\n+            except Exception as e:\n+                logger.error(f'Failed to connect to {server_url}: {str(e)}')\n+                try:\n+                    await client.disconnect()\n+                except Exception as disconnect_error:\n+                    logger.error(\n+                        f'Error during disconnect after failed connection: {str(disconnect_error)}'\n+                    )\n+\n+    return mcp_clients\n+\n+\n+async def fetch_mcp_tools_from_config(mcp_config: MCPConfig) -> list[dict]:\n+    \"\"\"\n+    Retrieves the list of MCP tools from the MCP clients.\n+\n+    Returns:\n+        A list of tool dictionaries. Returns an empty list if no connections could be established.\n+    \"\"\"\n+    mcp_clients = []\n+    mcp_tools = []\n+    try:\n+        logger.debug(f'Creating MCP clients with config: {mcp_config}')\n+        mcp_clients = await create_mcp_clients(\n+            mcp_config.sse.mcp_servers,\n+        )\n+\n+        if not mcp_clients:\n+            logger.warning('No MCP clients were successfully connected')\n+            return []\n+\n+        mcp_tools = convert_mcp_clients_to_tools(mcp_clients)\n+\n+        # Always disconnect clients to clean up resources\n+        for mcp_client in mcp_clients:\n+            try:\n+                await mcp_client.disconnect()\n+            except Exception as disconnect_error:\n+                logger.error(f'Error disconnecting MCP client: {str(disconnect_error)}')\n+    except Exception as e:\n+        logger.error(f'Error fetching MCP tools: {str(e)}')\n+        return []\n+\n+    logger.debug(f'MCP tools: {mcp_tools}')\n+    return mcp_tools\n+\n+\n+async def call_tool_mcp(mcp_clients: list[MCPClient], action: McpAction) -> Observation:\n+    \"\"\"\n+    Call a tool on an MCP server and return the observation.\n+\n+    Args:\n+        action: The MCP action to execute\n+        sse_mcp_servers: List of SSE MCP server URLs\n+\n+    Returns:\n+        The observation from the MCP server\n+    \"\"\"\n+    if not mcp_clients:\n+        raise ValueError('No MCP clients found')\n+\n+    logger.debug(f'MCP action received: {action}')\n+    # Find the MCP agent that has the matching tool name\n+    matching_client = None\n+    logger.debug(f'MCP clients: {mcp_clients}')\n+    logger.debug(f'MCP action name: {action.name}')\n+    for client in mcp_clients:\n+        logger.debug(f'MCP client tools: {client.tools}')\n+        if action.name in [tool.name for tool in client.tools]:\n+            matching_client = client\n+            break\n+    if matching_client is None:\n+        raise ValueError(f'No matching MCP agent found for tool name: {action.name}')\n+    logger.debug(f'Matching client: {matching_client}')\n+    args_dict = json.loads(action.arguments) if action.arguments else {}\n+    response = await matching_client.call_tool(action.name, args_dict)\n+    logger.debug(f'MCP response: {response}')\n+\n+    return MCPObservation(content=f'MCP result:{response.model_dump(mode=\"json\")}')\ndiff --git a/tests/unit/test_mcp_config.py b/tests/unit/test_mcp_config.py\nnew file mode 100644\nindex 000000000000..c91574025ebd\n--- /dev/null\n+++ b/tests/unit/test_mcp_config.py\n@@ -0,0 +1,63 @@\n+import pytest\n+\n+from openhands.core.config.mcp_config import MCPConfig, MCPSSEConfig\n+\n+\n+def test_valid_sse_config():\n+    \"\"\"Test a valid SSE configuration.\"\"\"\n+    config = MCPSSEConfig(mcp_servers=['http://server1:8080', 'http://server2:8080'])\n+    config.validate_servers()  # Should not raise any exception\n+\n+\n+def test_empty_sse_config():\n+    \"\"\"Test SSE configuration with empty servers list.\"\"\"\n+    config = MCPSSEConfig(mcp_servers=[])\n+    config.validate_servers()\n+\n+\n+def test_invalid_sse_url():\n+    \"\"\"Test SSE configuration with invalid URL format.\"\"\"\n+    config = MCPSSEConfig(mcp_servers=['not_a_url'])\n+    with pytest.raises(ValueError) as exc_info:\n+        config.validate_servers()\n+    assert 'Invalid URL' in str(exc_info.value)\n+\n+\n+def test_duplicate_sse_urls():\n+    \"\"\"Test SSE configuration with duplicate server URLs.\"\"\"\n+    config = MCPSSEConfig(mcp_servers=['http://server1:8080', 'http://server1:8080'])\n+    with pytest.raises(ValueError) as exc_info:\n+        config.validate_servers()\n+    assert 'Duplicate MCP server URLs are not allowed' in str(exc_info.value)\n+\n+\n+def test_from_toml_section_valid():\n+    \"\"\"Test creating config from valid TOML section.\"\"\"\n+    data = {\n+        'mcp_servers': ['http://server1:8080'],\n+    }\n+    result = MCPConfig.from_toml_section(data)\n+    assert 'mcp' in result\n+    assert result['mcp'].sse.mcp_servers == ['http://server1:8080']\n+\n+\n+def test_from_toml_section_invalid_sse():\n+    \"\"\"Test creating config from TOML section with invalid SSE URL.\"\"\"\n+    data = {\n+        'mcp_servers': ['not_a_url'],\n+    }\n+    with pytest.raises(ValueError) as exc_info:\n+        MCPConfig.from_toml_section(data)\n+    assert 'Invalid URL' in str(exc_info.value)\n+\n+\n+def test_complex_urls():\n+    \"\"\"Test SSE configuration with complex URLs.\"\"\"\n+    config = MCPSSEConfig(\n+        mcp_servers=[\n+            'https://user:pass@server1:8080/path?query=1',\n+            'wss://server2:8443/ws',\n+            'http://subdomain.example.com:9090',\n+        ]\n+    )\n+    config.validate_servers()  # Should not raise any exception\ndiff --git a/tests/unit/test_mcp_timeout.py b/tests/unit/test_mcp_timeout.py\nnew file mode 100644\nindex 000000000000..d50f44fb59c0\n--- /dev/null\n+++ b/tests/unit/test_mcp_timeout.py\n@@ -0,0 +1,83 @@\n+import asyncio\n+from unittest import mock\n+\n+import pytest\n+\n+from openhands.core.config.mcp_config import MCPConfig, MCPSSEConfig\n+from openhands.mcp import MCPClient, create_mcp_clients, fetch_mcp_tools_from_config\n+\n+\n+@pytest.mark.asyncio\n+async def test_sse_connection_timeout():\n+    \"\"\"Test that SSE connection timeout is handled gracefully.\"\"\"\n+    # Create a mock MCPClient\n+    mock_client = mock.MagicMock(spec=MCPClient)\n+\n+    # Configure the mock to raise a TimeoutError when connect_sse is called\n+    async def mock_connect_sse(*args, **kwargs):\n+        await asyncio.sleep(0.1)  # Simulate some delay\n+        raise asyncio.TimeoutError('Connection timed out')\n+\n+    mock_client.connect_sse.side_effect = mock_connect_sse\n+    mock_client.disconnect = mock.AsyncMock()\n+\n+    # Mock the MCPClient constructor to return our mock\n+    with mock.patch('openhands.mcp.utils.MCPClient', return_value=mock_client):\n+        # Create a list of server URLs to test\n+        sse_servers = ['http://server1:8080', 'http://server2:8080']\n+\n+        # Call create_mcp_clients with the server URLs\n+        clients = await create_mcp_clients(sse_mcp_server=sse_servers)\n+\n+        # Verify that no clients were successfully connected\n+        assert len(clients) == 0\n+\n+        # Verify that connect_sse was called for each server\n+        assert mock_client.connect_sse.call_count == 2\n+\n+        # Verify that disconnect was called for each failed connection\n+        assert mock_client.disconnect.call_count == 2\n+\n+\n+@pytest.mark.asyncio\n+async def test_fetch_mcp_tools_with_timeout():\n+    \"\"\"Test that fetch_mcp_tools_from_config handles timeouts gracefully.\"\"\"\n+    # Create a mock MCPConfig\n+    mock_config = mock.MagicMock(spec=MCPConfig)\n+    mock_config.sse = mock.MagicMock(spec=MCPSSEConfig)\n+\n+    # Configure the mock config\n+    mock_config.sse.mcp_servers = ['http://server1:8080']\n+\n+    # Mock create_mcp_clients to return an empty list (simulating all connections failing)\n+    with mock.patch('openhands.mcp.utils.create_mcp_clients', return_value=[]):\n+        # Call fetch_mcp_tools_from_config\n+        tools = await fetch_mcp_tools_from_config(mock_config)\n+\n+        # Verify that an empty list of tools is returned\n+        assert tools == []\n+\n+\n+@pytest.mark.asyncio\n+async def test_mixed_connection_results():\n+    \"\"\"Test that fetch_mcp_tools_from_config returns tools even when some connections fail.\"\"\"\n+    # Create a mock MCPConfig\n+    mock_config = mock.MagicMock(spec=MCPConfig)\n+    mock_config.sse = mock.MagicMock(spec=MCPSSEConfig)\n+\n+    # Configure the mock config\n+    mock_config.sse.mcp_servers = ['http://server1:8080', 'http://server2:8080']\n+\n+    # Create a successful client\n+    successful_client = mock.MagicMock(spec=MCPClient)\n+    successful_client.tools = [mock.MagicMock()]\n+\n+    # Mock create_mcp_clients to return our successful client\n+    with mock.patch(\n+        'openhands.mcp.utils.create_mcp_clients', return_value=[successful_client]\n+    ):\n+        # Call fetch_mcp_tools_from_config\n+        tools = await fetch_mcp_tools_from_config(mock_config)\n+\n+        # Verify that tools were returned\n+        assert len(tools) > 0\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR makes solid progress towards configuring MCP servers, but I'd recommend addressing a few concerns before merging. The proposed customization method could complicate user experience compared to leveraging existing micro-agent configurations, as seen in #7620. Evaluate the benefits of this new approach critically. Also, consider how integration with popular MCP servers like Stdio can be streamlined. The suggestion to leverage frameworks like MetaMCP is promising, but ensure it aligns with our user base's needs. Let's keep simplicity in focus while enhancing functionality."
      }
    ],
    "codediff_length": 18945
  },
  {
    "index": 49,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/openhands/runtime/impl/local/local_runtime.py b/openhands/runtime/impl/local/local_runtime.py\nnew file mode 100644\nindex 000000000000..547f2981d515\n--- /dev/null\n+++ b/openhands/runtime/impl/local/local_runtime.py\n@@ -0,0 +1,342 @@\n+\"\"\"\n+This runtime runs the action_execution_server directly on the local machine without Docker.\n+\"\"\"\n+\n+import os\n+import shutil\n+import subprocess\n+import tempfile\n+import threading\n+from typing import Callable, Optional\n+\n+import requests\n+import tenacity\n+\n+import openhands\n+from openhands.core.config import AppConfig\n+from openhands.core.exceptions import AgentRuntimeDisconnectedError\n+from openhands.core.logger import openhands_logger as logger\n+from openhands.events import EventStream\n+from openhands.events.action import (\n+    Action,\n+)\n+from openhands.events.observation import (\n+    Observation,\n+)\n+from openhands.events.serialization import event_to_dict, observation_from_dict\n+from openhands.runtime.impl.action_execution.action_execution_client import (\n+    ActionExecutionClient,\n+)\n+from openhands.runtime.impl.docker.docker_runtime import (\n+    APP_PORT_RANGE_1,\n+    APP_PORT_RANGE_2,\n+    EXECUTION_SERVER_PORT_RANGE,\n+    VSCODE_PORT_RANGE,\n+)\n+from openhands.runtime.plugins import PluginRequirement\n+from openhands.runtime.utils import find_available_tcp_port\n+from openhands.runtime.utils.command import get_action_execution_server_startup_command\n+from openhands.utils.async_utils import call_sync_from_async\n+from openhands.utils.tenacity_stop import stop_if_should_exit\n+\n+\n+def check_dependencies(code_repo_path: str, poetry_venvs_path: str):\n+    ERROR_MESSAGE = 'Please follow the instructions in https://github.com/All-Hands-AI/OpenHands/blob/main/Development.md to install OpenHands.'\n+    if not os.path.exists(code_repo_path):\n+        raise ValueError(\n+            f'Code repo path {code_repo_path} does not exist. ' + ERROR_MESSAGE\n+        )\n+    if not os.path.exists(poetry_venvs_path):\n+        raise ValueError(\n+            f'Poetry venvs path {poetry_venvs_path} does not exist. ' + ERROR_MESSAGE\n+        )\n+    # Check jupyter is installed\n+    logger.debug('Checking dependencies: Jupyter')\n+    output = subprocess.check_output(\n+        'poetry run jupyter --version',\n+        shell=True,\n+        text=True,\n+        cwd=code_repo_path,\n+    )\n+    logger.debug(f'Jupyter output: {output}')\n+    if 'jupyter' not in output.lower():\n+        raise ValueError('Jupyter is not properly installed. ' + ERROR_MESSAGE)\n+\n+    # Check libtmux is installed\n+    logger.debug('Checking dependencies: libtmux')\n+    import libtmux\n+\n+    server = libtmux.Server()\n+    session = server.new_session(session_name='test-session')\n+    pane = session.attached_pane\n+    pane.send_keys('echo \"test\"')\n+    pane_output = '\\n'.join(pane.cmd('capture-pane', '-p').stdout)\n+    session.kill_session()\n+    if 'test' not in pane_output:\n+        raise ValueError('libtmux is not properly installed. ' + ERROR_MESSAGE)\n+\n+    # Check browser works\n+    logger.debug('Checking dependencies: browser')\n+    from openhands.runtime.browser.browser_env import BrowserEnv\n+\n+    browser = BrowserEnv()\n+    browser.close()\n+\n+\n+class LocalRuntime(ActionExecutionClient):\n+    \"\"\"This runtime will run the action_execution_server directly on the local machine.\n+    When receiving an event, it will send the event to the server via HTTP.\n+\n+    Args:\n+        config (AppConfig): The application configuration.\n+        event_stream (EventStream): The event stream to subscribe to.\n+        sid (str, optional): The session ID. Defaults to 'default'.\n+        plugins (list[PluginRequirement] | None, optional): List of plugin requirements. Defaults to None.\n+        env_vars (dict[str, str] | None, optional): Environment variables to set. Defaults to None.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: AppConfig,\n+        event_stream: EventStream,\n+        sid: str = 'default',\n+        plugins: list[PluginRequirement] | None = None,\n+        env_vars: dict[str, str] | None = None,\n+        status_callback: Callable | None = None,\n+        attach_to_existing: bool = False,\n+        headless_mode: bool = True,\n+    ):\n+        self.config = config\n+        self._user_id = os.getuid()\n+        self._username = os.getenv('USER')\n+\n+        if self.config.workspace_base is not None:\n+            logger.warning(\n+                f'Workspace base path is set to {self.config.workspace_base}. '\n+                'It will be used as the path for the agent to run in. '\n+                'Be careful, the agent can EDIT files in this directory!'\n+            )\n+            self.config.workspace_mount_path_in_sandbox = self.config.workspace_base\n+            self._temp_workspace = None\n+        else:\n+            # A temporary directory is created for the agent to run in\n+            # This is used for the local runtime only\n+            self._temp_workspace = tempfile.mkdtemp(\n+                prefix=f'openhands_workspace_{sid}',\n+            )\n+            self.config.workspace_mount_path_in_sandbox = self._temp_workspace\n+\n+        logger.warning(\n+            'Initializing LocalRuntime. WARNING: NO SANDBOX IS USED. '\n+            'This is an experimental feature, please report issues to https://github.com/All-Hands-AI/OpenHands/issues. '\n+            '`run_as_openhands` will be ignored since the current user will be used to launch the server. '\n+            'We highly recommend using a sandbox (eg. DockerRuntime) unless you '\n+            'are running in a controlled environment.\\n'\n+            f'Temp workspace: {self._temp_workspace}. '\n+            f'User ID: {self._user_id}. '\n+            f'Username: {self._username}.'\n+        )\n+\n+        if self.config.workspace_base is not None:\n+            logger.warning(\n+                f'Workspace base path is set to {self.config.workspace_base}. It will be used as the path for the agent to run in.'\n+            )\n+            self.config.workspace_mount_path_in_sandbox = self.config.workspace_base\n+        else:\n+            logger.warning(\n+                'Workspace base path is NOT set. Agent will run in a temporary directory.'\n+            )\n+            self._temp_workspace = tempfile.mkdtemp()\n+            self.config.workspace_mount_path_in_sandbox = self._temp_workspace\n+\n+        self._host_port = -1\n+        self._vscode_port = -1\n+        self._app_ports: list[int] = []\n+\n+        self.api_url = f'{self.config.sandbox.local_runtime_url}:{self._host_port}'\n+        self.status_callback = status_callback\n+        self.server_process: Optional[subprocess.Popen[str]] = None\n+        self.action_semaphore = threading.Semaphore(1)  # Ensure one action at a time\n+\n+        # Update env vars\n+        if self.config.sandbox.runtime_startup_env_vars:\n+            os.environ.update(self.config.sandbox.runtime_startup_env_vars)\n+\n+        # Initialize the action_execution_server\n+        super().__init__(\n+            config,\n+            event_stream,\n+            sid,\n+            plugins,\n+            env_vars,\n+            status_callback,\n+            attach_to_existing,\n+            headless_mode,\n+        )\n+\n+    def _get_action_execution_server_host(self):\n+        return self.api_url\n+\n+    async def connect(self):\n+        \"\"\"Start the action_execution_server on the local machine.\"\"\"\n+        self.send_status_message('STATUS$STARTING_RUNTIME')\n+\n+        self._host_port = self._find_available_port(EXECUTION_SERVER_PORT_RANGE)\n+        self._vscode_port = self._find_available_port(VSCODE_PORT_RANGE)\n+        self._app_ports = [\n+            self._find_available_port(APP_PORT_RANGE_1),\n+            self._find_available_port(APP_PORT_RANGE_2),\n+        ]\n+        self.api_url = f'{self.config.sandbox.local_runtime_url}:{self._host_port}'\n+\n+        # Start the server process\n+        cmd = get_action_execution_server_startup_command(\n+            server_port=self._host_port,\n+            plugins=self.plugins,\n+            app_config=self.config,\n+            python_prefix=[],\n+            override_user_id=self._user_id,\n+            override_username=self._username,\n+        )\n+\n+        self.log('debug', f'Starting server with command: {cmd}')\n+        env = os.environ.copy()\n+        # Get the code repo path\n+        code_repo_path = os.path.dirname(os.path.dirname(openhands.__file__))\n+        env['PYTHONPATH'] = f'{code_repo_path}:$PYTHONPATH'\n+        env['OPENHANDS_REPO_PATH'] = code_repo_path\n+        env['LOCAL_RUNTIME_MODE'] = '1'\n+        # run poetry show -v | head -n 1 | awk '{print $2}'\n+        poetry_venvs_path = (\n+            subprocess.check_output(\n+                ['poetry', 'show', '-v'],\n+                env=env,\n+                cwd=code_repo_path,\n+                text=True,\n+                shell=False,\n+            )\n+            .splitlines()[0]\n+            .split(':')[1]\n+            .strip()\n+        )\n+        env['POETRY_VIRTUALENVS_PATH'] = poetry_venvs_path\n+        logger.debug(f'POETRY_VIRTUALENVS_PATH: {poetry_venvs_path}')\n+\n+        check_dependencies(code_repo_path, poetry_venvs_path)\n+        self.server_process = subprocess.Popen(\n+            cmd,\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.STDOUT,\n+            universal_newlines=True,\n+            bufsize=1,\n+            env=env,\n+        )\n+\n+        # Start a thread to read and log server output\n+        def log_output():\n+            while (\n+                self.server_process\n+                and self.server_process.poll()\n+                and self.server_process.stdout\n+            ):\n+                line = self.server_process.stdout.readline()\n+                if not line:\n+                    break\n+                self.log('debug', f'Server: {line.strip()}')\n+\n+        self._log_thread = threading.Thread(target=log_output, daemon=True)\n+        self._log_thread.start()\n+\n+        self.log('info', f'Waiting for server to become ready at {self.api_url}...')\n+        self.send_status_message('STATUS$WAITING_FOR_CLIENT')\n+\n+        await call_sync_from_async(self._wait_until_alive)\n+\n+        if not self.attach_to_existing:\n+            await call_sync_from_async(self.setup_initial_env)\n+\n+        self.log(\n+            'debug',\n+            f'Server initialized with plugins: {[plugin.name for plugin in self.plugins]}',\n+        )\n+        if not self.attach_to_existing:\n+            self.send_status_message(' ')\n+        self._runtime_initialized = True\n+\n+    def _find_available_port(self, port_range, max_attempts=5):\n+        port = port_range[1]\n+        for _ in range(max_attempts):\n+            port = find_available_tcp_port(port_range[0], port_range[1])\n+            return port\n+        return port\n+\n+    @tenacity.retry(\n+        wait=tenacity.wait_exponential(min=1, max=10),\n+        stop=tenacity.stop_after_attempt(10) | stop_if_should_exit(),\n+        before_sleep=lambda retry_state: logger.debug(\n+            f'Waiting for server to be ready... (attempt {retry_state.attempt_number})'\n+        ),\n+    )\n+    def _wait_until_alive(self):\n+        \"\"\"Wait until the server is ready to accept requests.\"\"\"\n+        if self.server_process and self.server_process.poll() is not None:\n+            raise RuntimeError('Server process died')\n+\n+        try:\n+            response = self.session.get(f'{self.api_url}/alive')\n+            response.raise_for_status()\n+            return True\n+        except Exception as e:\n+            self.log('debug', f'Server not ready yet: {e}')\n+            raise\n+\n+    async def execute_action(self, action: Action) -> Observation:\n+        \"\"\"Execute an action by sending it to the server.\"\"\"\n+        if not self._runtime_initialized:\n+            raise AgentRuntimeDisconnectedError('Runtime not initialized')\n+\n+        if self.server_process is None or self.server_process.poll() is not None:\n+            raise AgentRuntimeDisconnectedError('Server process died')\n+\n+        with self.action_semaphore:\n+            try:\n+                response = await call_sync_from_async(\n+                    lambda: self.session.post(\n+                        f'{self.api_url}/execute_action',\n+                        json={'action': event_to_dict(action)},\n+                    )\n+                )\n+                return observation_from_dict(response.json())\n+            except requests.exceptions.ConnectionError:\n+                raise AgentRuntimeDisconnectedError('Server connection lost')\n+\n+    def close(self):\n+        \"\"\"Stop the server process.\"\"\"\n+        if self.server_process:\n+            self.server_process.terminate()\n+            try:\n+                self.server_process.wait(timeout=5)\n+            except subprocess.TimeoutExpired:\n+                self.server_process.kill()\n+            self.server_process = None\n+            self._log_thread.join()\n+\n+        if self._temp_workspace:\n+            shutil.rmtree(self._temp_workspace)\n+\n+        super().close()\n+\n+    @property\n+    def vscode_url(self) -> str | None:\n+        token = super().get_vscode_token()\n+        if not token:\n+            return None\n+        vscode_url = f'http://localhost:{self._vscode_port}/?tkn={token}&folder={self.config.workspace_mount_path_in_sandbox}'\n+        return vscode_url\n+\n+    @property\n+    def web_hosts(self):\n+        hosts: dict[str, int] = {}\n+        for port in self._app_ports:\n+            hosts[f'http://localhost:{port}'] = port\n+        return hosts\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "It looks like you need to adjust your local runtime configuration to get the tests working. Ensure the environment is set up correctly first, especially with overridden `SandboxConfig` settings for user and workspace. Your errors indicate a connection issue with the service not starting properly. The current user setup might require careful debugging to function without root. Additionally, reassess the use of `WORKSPACE_BASE`; it doesn't appear to influence file paths as expected. Focus on getting the server ready before running tests to resolve connection refusals."
      }
    ],
    "codediff_length": 13539
  },
  {
    "index": 50,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/opendevin/runtime/plugins/agent_skills/aider/LICENSE.txt b/opendevin/runtime/plugins/agent_skills/aider/LICENSE.txt\nnew file mode 100644\nindex 000000000000..d64569567334\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/aider/LICENSE.txt\n@@ -0,0 +1,202 @@\n+\n+                                 Apache License\n+                           Version 2.0, January 2004\n+                        http://www.apache.org/licenses/\n+\n+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n+\n+   1. Definitions.\n+\n+      \"License\" shall mean the terms and conditions for use, reproduction,\n+      and distribution as defined by Sections 1 through 9 of this document.\n+\n+      \"Licensor\" shall mean the copyright owner or entity authorized by\n+      the copyright owner that is granting the License.\n+\n+      \"Legal Entity\" shall mean the union of the acting entity and all\n+      other entities that control, are controlled by, or are under common\n+      control with that entity. For the purposes of this definition,\n+      \"control\" means (i) the power, direct or indirect, to cause the\n+      direction or management of such entity, whether by contract or\n+      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n+      outstanding shares, or (iii) beneficial ownership of such entity.\n+\n+      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n+      exercising permissions granted by this License.\n+\n+      \"Source\" form shall mean the preferred form for making modifications,\n+      including but not limited to software source code, documentation\n+      source, and configuration files.\n+\n+      \"Object\" form shall mean any form resulting from mechanical\n+      transformation or translation of a Source form, including but\n+      not limited to compiled object code, generated documentation,\n+      and conversions to other media types.\n+\n+      \"Work\" shall mean the work of authorship, whether in Source or\n+      Object form, made available under the License, as indicated by a\n+      copyright notice that is included in or attached to the work\n+      (an example is provided in the Appendix below).\n+\n+      \"Derivative Works\" shall mean any work, whether in Source or Object\n+      form, that is based on (or derived from) the Work and for which the\n+      editorial revisions, annotations, elaborations, or other modifications\n+      represent, as a whole, an original work of authorship. For the purposes\n+      of this License, Derivative Works shall not include works that remain\n+      separable from, or merely link (or bind by name) to the interfaces of,\n+      the Work and Derivative Works thereof.\n+\n+      \"Contribution\" shall mean any work of authorship, including\n+      the original version of the Work and any modifications or additions\n+      to that Work or Derivative Works thereof, that is intentionally\n+      submitted to Licensor for inclusion in the Work by the copyright owner\n+      or by an individual or Legal Entity authorized to submit on behalf of\n+      the copyright owner. For the purposes of this definition, \"submitted\"\n+      means any form of electronic, verbal, or written communication sent\n+      to the Licensor or its representatives, including but not limited to\n+      communication on electronic mailing lists, source code control systems,\n+      and issue tracking systems that are managed by, or on behalf of, the\n+      Licensor for the purpose of discussing and improving the Work, but\n+      excluding communication that is conspicuously marked or otherwise\n+      designated in writing by the copyright owner as \"Not a Contribution.\"\n+\n+      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n+      on behalf of whom a Contribution has been received by Licensor and\n+      subsequently incorporated within the Work.\n+\n+   2. Grant of Copyright License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      copyright license to reproduce, prepare Derivative Works of,\n+      publicly display, publicly perform, sublicense, and distribute the\n+      Work and such Derivative Works in Source or Object form.\n+\n+   3. Grant of Patent License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      (except as stated in this section) patent license to make, have made,\n+      use, offer to sell, sell, import, and otherwise transfer the Work,\n+      where such license applies only to those patent claims licensable\n+      by such Contributor that are necessarily infringed by their\n+      Contribution(s) alone or by combination of their Contribution(s)\n+      with the Work to which such Contribution(s) was submitted. If You\n+      institute patent litigation against any entity (including a\n+      cross-claim or counterclaim in a lawsuit) alleging that the Work\n+      or a Contribution incorporated within the Work constitutes direct\n+      or contributory patent infringement, then any patent licenses\n+      granted to You under this License for that Work shall terminate\n+      as of the date such litigation is filed.\n+\n+   4. Redistribution. You may reproduce and distribute copies of the\n+      Work or Derivative Works thereof in any medium, with or without\n+      modifications, and in Source or Object form, provided that You\n+      meet the following conditions:\n+\n+      (a) You must give any other recipients of the Work or\n+          Derivative Works a copy of this License; and\n+\n+      (b) You must cause any modified files to carry prominent notices\n+          stating that You changed the files; and\n+\n+      (c) You must retain, in the Source form of any Derivative Works\n+          that You distribute, all copyright, patent, trademark, and\n+          attribution notices from the Source form of the Work,\n+          excluding those notices that do not pertain to any part of\n+          the Derivative Works; and\n+\n+      (d) If the Work includes a \"NOTICE\" text file as part of its\n+          distribution, then any Derivative Works that You distribute must\n+          include a readable copy of the attribution notices contained\n+          within such NOTICE file, excluding those notices that do not\n+          pertain to any part of the Derivative Works, in at least one\n+          of the following places: within a NOTICE text file distributed\n+          as part of the Derivative Works; within the Source form or\n+          documentation, if provided along with the Derivative Works; or,\n+          within a display generated by the Derivative Works, if and\n+          wherever such third-party notices normally appear. The contents\n+          of the NOTICE file are for informational purposes only and\n+          do not modify the License. You may add Your own attribution\n+          notices within Derivative Works that You distribute, alongside\n+          or as an addendum to the NOTICE text from the Work, provided\n+          that such additional attribution notices cannot be construed\n+          as modifying the License.\n+\n+      You may add Your own copyright statement to Your modifications and\n+      may provide additional or different license terms and conditions\n+      for use, reproduction, or distribution of Your modifications, or\n+      for any such Derivative Works as a whole, provided Your use,\n+      reproduction, and distribution of the Work otherwise complies with\n+      the conditions stated in this License.\n+\n+   5. Submission of Contributions. Unless You explicitly state otherwise,\n+      any Contribution intentionally submitted for inclusion in the Work\n+      by You to the Licensor shall be under the terms and conditions of\n+      this License, without any additional terms or conditions.\n+      Notwithstanding the above, nothing herein shall supersede or modify\n+      the terms of any separate license agreement you may have executed\n+      with Licensor regarding such Contributions.\n+\n+   6. Trademarks. This License does not grant permission to use the trade\n+      names, trademarks, service marks, or product names of the Licensor,\n+      except as required for reasonable and customary use in describing the\n+      origin of the Work and reproducing the content of the NOTICE file.\n+\n+   7. Disclaimer of Warranty. Unless required by applicable law or\n+      agreed to in writing, Licensor provides the Work (and each\n+      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+      implied, including, without limitation, any warranties or conditions\n+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n+      PARTICULAR PURPOSE. You are solely responsible for determining the\n+      appropriateness of using or redistributing the Work and assume any\n+      risks associated with Your exercise of permissions under this License.\n+\n+   8. Limitation of Liability. In no event and under no legal theory,\n+      whether in tort (including negligence), contract, or otherwise,\n+      unless required by applicable law (such as deliberate and grossly\n+      negligent acts) or agreed to in writing, shall any Contributor be\n+      liable to You for damages, including any direct, indirect, special,\n+      incidental, or consequential damages of any character arising as a\n+      result of this License or out of the use or inability to use the\n+      Work (including but not limited to damages for loss of goodwill,\n+      work stoppage, computer failure or malfunction, or any and all\n+      other commercial damages or losses), even if such Contributor\n+      has been advised of the possibility of such damages.\n+\n+   9. Accepting Warranty or Additional Liability. While redistributing\n+      the Work or Derivative Works thereof, You may choose to offer,\n+      and charge a fee for, acceptance of support, warranty, indemnity,\n+      or other liability obligations and/or rights consistent with this\n+      License. However, in accepting such obligations, You may act only\n+      on Your own behalf and on Your sole responsibility, not on behalf\n+      of any other Contributor, and only if You agree to indemnify,\n+      defend, and hold each Contributor harmless for any liability\n+      incurred by, or claims asserted against, such Contributor by reason\n+      of your accepting any such warranty or additional liability.\n+\n+   END OF TERMS AND CONDITIONS\n+\n+   APPENDIX: How to apply the Apache License to your work.\n+\n+      To apply the Apache License to your work, attach the following\n+      boilerplate notice, with the fields enclosed by brackets \"[]\"\n+      replaced with your own identifying information. (Don't include\n+      the brackets!)  The text should be enclosed in the appropriate\n+      comment syntax for the file format. We also recommend that a\n+      file or class name and description of purpose be included on the\n+      same \"printed page\" as the copyright notice for easier\n+      identification within third-party archives.\n+\n+   Copyright [yyyy] [name of copyright owner]\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\ndiff --git a/opendevin/runtime/plugins/agent_skills/aider/linter.py b/opendevin/runtime/plugins/agent_skills/aider/linter.py\nnew file mode 100644\nindex 000000000000..419b5e3c5ace\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/aider/linter.py\n@@ -0,0 +1,223 @@\n+import os\n+import subprocess\n+import sys\n+import traceback\n+import warnings\n+from dataclasses import dataclass\n+from pathlib import Path\n+\n+from grep_ast import TreeContext, filename_to_lang\n+from tree_sitter_languages import get_parser  # noqa: E402\n+\n+# tree_sitter is throwing a FutureWarning\n+warnings.simplefilter('ignore', category=FutureWarning)\n+\n+\n+@dataclass\n+class LintResult:\n+    text: str\n+    lines: list\n+\n+\n+class Linter:\n+    def __init__(self, encoding='utf-8', root=None):\n+        self.encoding = encoding\n+        self.root = root\n+\n+        self.languages = dict(\n+            python=self.py_lint,\n+        )\n+        self.all_lint_cmd = None\n+\n+    def set_linter(self, lang, cmd):\n+        if lang:\n+            self.languages[lang] = cmd\n+            return\n+\n+        self.all_lint_cmd = cmd\n+\n+    def get_rel_fname(self, fname):\n+        if self.root:\n+            return os.path.relpath(fname, self.root)\n+        else:\n+            return fname\n+\n+    def run_cmd(self, cmd, rel_fname, code):\n+        cmd += ' ' + rel_fname\n+        cmd = cmd.split()\n+\n+        process = subprocess.Popen(\n+            cmd, cwd=self.root, stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n+        )\n+        stdout, _ = process.communicate()\n+        errors = stdout.decode().strip()\n+        self.returncode = process.returncode\n+        if self.returncode == 0:\n+            return  # zero exit status\n+\n+        cmd = ' '.join(cmd)\n+        res = ''\n+        res += errors\n+        line_num = extract_error_line_from(res)\n+        return LintResult(text=res, lines=[line_num])\n+\n+    def get_abs_fname(self, fname):\n+        if os.path.isabs(fname):\n+            return fname\n+        elif os.path.isfile(fname):\n+            rel_fname = self.get_rel_fname(fname)\n+            return os.path.abspath(rel_fname)\n+        else:  # if a temp file\n+            return self.get_rel_fname(fname)\n+\n+    def lint(self, fname, cmd=None) -> LintResult | None:\n+        code = Path(fname).read_text(self.encoding)\n+        absolute_fname = self.get_abs_fname(fname)\n+        if cmd:\n+            cmd = cmd.strip()\n+        if not cmd:\n+            lang = filename_to_lang(fname)\n+            if not lang:\n+                return None\n+            if self.all_lint_cmd:\n+                cmd = self.all_lint_cmd\n+            else:\n+                cmd = self.languages.get(lang)\n+        if callable(cmd):\n+            linkres = cmd(fname, absolute_fname, code)\n+        elif cmd:\n+            linkres = self.run_cmd(cmd, absolute_fname, code)\n+        else:\n+            linkres = basic_lint(absolute_fname, code)\n+        return linkres\n+\n+    def flake_lint(self, rel_fname, code):\n+        fatal = 'F821,F822,F831,E112,E113,E999,E902'\n+        flake8 = f'flake8 --select={fatal} --isolated'\n+\n+        try:\n+            flake_res = self.run_cmd(flake8, rel_fname, code)\n+        except FileNotFoundError:\n+            flake_res = None\n+        return flake_res\n+\n+    def py_lint(self, fname, rel_fname, code):\n+        error = self.flake_lint(rel_fname, code)\n+        if not error:\n+            error = lint_python_compile(fname, code)\n+        if not error:\n+            error = basic_lint(rel_fname, code)\n+        return error\n+\n+\n+def lint_python_compile(fname, code):\n+    try:\n+        compile(code, fname, 'exec')  # USE TRACEBACK BELOW HERE\n+        return\n+    except IndentationError as err:\n+        end_lineno = getattr(err, 'end_lineno', err.lineno)\n+        if isinstance(end_lineno, int):\n+            line_numbers = list(range(end_lineno - 1, end_lineno))\n+        else:\n+            line_numbers = []\n+\n+        tb_lines = traceback.format_exception(type(err), err, err.__traceback__)\n+        last_file_i = 0\n+\n+        target = '# USE TRACEBACK'\n+        target += ' BELOW HERE'\n+        for i in range(len(tb_lines)):\n+            if target in tb_lines[i]:\n+                last_file_i = i\n+                break\n+        tb_lines = tb_lines[:1] + tb_lines[last_file_i + 1 :]\n+\n+    res = ''.join(tb_lines)\n+    return LintResult(text=res, lines=line_numbers)\n+\n+\n+def basic_lint(fname, code):\n+    \"\"\"\n+    Use tree-sitter to look for syntax errors, display them with tree context.\n+    \"\"\"\n+\n+    lang = filename_to_lang(fname)\n+    if not lang:\n+        return\n+\n+    parser = get_parser(lang)\n+    tree = parser.parse(bytes(code, 'utf-8'))\n+\n+    errors = traverse_tree(tree.root_node)\n+    if not errors:\n+        return\n+    return LintResult(text=f'{fname}:{errors[0]}', lines=errors)\n+\n+\n+def extract_error_line_from(lint_error):\n+    # moved from opendevin.agentskills#_lint_file\n+    for line in lint_error.splitlines(True):\n+        if line.strip():\n+            # The format of the error message is: <filename>:<line>:<column>: <error code> <error message>\n+            parts = line.split(':')\n+            if len(parts) >= 2:\n+                try:\n+                    first_error_line = int(parts[1])\n+                    break\n+                except ValueError:\n+                    continue\n+    return first_error_line\n+\n+\n+def tree_context(fname, code, line_nums):\n+    context = TreeContext(\n+        fname,\n+        code,\n+        color=False,\n+        line_number=True,\n+        child_context=False,\n+        last_line=False,\n+        margin=0,\n+        mark_lois=True,\n+        loi_pad=3,\n+        # header_max=30,\n+        show_top_of_file_parent_scope=False,\n+    )\n+    line_nums = set(line_nums)\n+    context.add_lines_of_interest(line_nums)\n+    context.add_context()\n+    output = context.format()\n+\n+    return output\n+\n+\n+# Traverse the tree to find errors\n+def traverse_tree(node):\n+    errors = []\n+    if node.type == 'ERROR' or node.is_missing:\n+        line_no = node.start_point[0] + 1\n+        errors.append(line_no)\n+\n+    for child in node.children:\n+        errors += traverse_tree(child)\n+\n+    return errors\n+\n+\n+def main():\n+    \"\"\"\n+    Main function to parse files provided as command line arguments.\n+    \"\"\"\n+    if len(sys.argv) < 2:\n+        print('Usage: python linter.py <file1> <file2> ...')\n+        sys.exit(1)\n+\n+    linter = Linter(root=os.getcwd())\n+    for file_path in sys.argv[1:]:\n+        errors = linter.lint(file_path)\n+        if errors:\n+            print(errors)\n+\n+\n+if __name__ == '__main__':\n+    main()\ndiff --git a/poetry.lock b/poetry.lock\nindex 8ac425f2ea84..6673c9710c65 100644\n--- a/poetry.lock\n+++ b/poetry.lock\n@@ -8033,6 +8059,122 @@ torchhub = [\"filelock\", \"huggingface-hub (>=0.23.2,<1.0)\", \"importlib-metadata\",\n video = [\"av (==9.2.0)\", \"decord (==0.6.0)\"]\n vision = [\"Pillow (>=10.0.1,<=15.0)\"]\n \n+[[package]]\n+name = \"tree-sitter\"\n+version = \"0.21.3\"\n+description = \"Python bindings for the Tree-Sitter parsing library\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"tree-sitter-0.21.3.tar.gz\", hash = \"sha256:b5de3028921522365aa864d95b3c41926e0ba6a85ee5bd000e10dc49b0766988\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:351f302b6615230c9dac9829f0ba20a94362cd658206ca9a7b2d58d73373dfb0\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:766e79ae1e61271e7fdfecf35b6401ad9b47fc07a0965ad78e7f97fddfdf47a6\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2c4d3d4d4b44857e87de55302af7f2d051c912c466ef20e8f18158e64df3542a\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:84eedb06615461b9e2847be7c47b9c5f2195d7d66d31b33c0a227eff4e0a0199\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:9d33ea425df8c3d6436926fe2991429d59c335431bf4e3c71e77c17eb508be5a\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:fae1ee0ff6d85e2fd5cd8ceb9fe4af4012220ee1e4cbe813305a316caf7a6f63\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-win_amd64.whl\", hash = \"sha256:bb41be86a987391f9970571aebe005ccd10222f39c25efd15826583c761a37e5\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:54b22c3c2aab3e3639a4b255d9df8455da2921d050c4829b6a5663b057f10db5\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:ab6e88c1e2d5e84ff0f9e5cd83f21b8e5074ad292a2cf19df3ba31d94fbcecd4\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:fc3fd34ed4cd5db445bc448361b5da46a2a781c648328dc5879d768f16a46771\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:fabc7182f6083269ce3cfcad202fe01516aa80df64573b390af6cd853e8444a1\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:4f874c3f7d2a2faf5c91982dc7d88ff2a8f183a21fe475c29bee3009773b0558\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:ee61ee3b7a4eedf9d8f1635c68ba4a6fa8c46929601fc48a907c6cfef0cfbcb2\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-win_amd64.whl\", hash = \"sha256:0b7256c723642de1c05fbb776b27742204a2382e337af22f4d9e279d77df7aa2\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:669b3e5a52cb1e37d60c7b16cc2221c76520445bb4f12dd17fd7220217f5abf3\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:2aa2a5099a9f667730ff26d57533cc893d766667f4d8a9877e76a9e74f48f0d3\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6a3e06ae2a517cf6f1abb682974f76fa760298e6d5a3ecf2cf140c70f898adf0\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:af992dfe08b4fefcfcdb40548d0d26d5d2e0a0f2d833487372f3728cd0772b48\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:c7cbab1dd9765138505c4a55e2aa857575bac4f1f8a8b0457744a4fefa1288e6\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:e1e66aeb457d1529370fcb0997ae5584c6879e0e662f1b11b2f295ea57e22f54\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-win_amd64.whl\", hash = \"sha256:013c750252dc3bd0e069d82e9658de35ed50eecf31c6586d0de7f942546824c5\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:4986a8cb4acebd168474ec2e5db440e59c7888819b3449a43ce8b17ed0331b07\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:6e217fee2e7be7dbce4496caa3d1c466977d7e81277b677f954d3c90e3272ec2\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f32a88afff4f2bc0f20632b0a2aa35fa9ae7d518f083409eca253518e0950929\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f3652ac9e47cdddf213c5d5d6854194469097e62f7181c0a9aa8435449a163a9\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:60b4df3298ff467bc01e2c0f6c2fb43aca088038202304bf8e41edd9fa348f45\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:00e4d0c99dff595398ef5e88a1b1ddd53adb13233fb677c1fd8e497fb2361629\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-win_amd64.whl\", hash = \"sha256:50c91353a26946e4dd6779837ecaf8aa123aafa2d3209f261ab5280daf0962f5\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:b17b8648b296ccc21a88d72ca054b809ee82d4b14483e419474e7216240ea278\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:f2f057fd01d3a95cbce6794c6e9f6db3d376cb3bb14e5b0528d77f0ec21d6478\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:839759de30230ffd60687edbb119b31521d5ac016749358e5285816798bb804a\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:5df40aa29cb7e323898194246df7a03b9676955a0ac1f6bce06bc4903a70b5f7\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:1d9be27dde007b569fa78ff9af5fe40d2532c998add9997a9729e348bb78fa59\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:c4ac87735e6f98fe085244c7c020f0177d13d4c117db72ba041faa980d25d69d\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-win_amd64.whl\", hash = \"sha256:fbbd137f7d9a5309fb4cb82e2c3250ba101b0dd08a8abdce815661e6cf2cbc19\"},\n+]\n+\n+[[package]]\n+name = \"tree-sitter-languages\"\n+version = \"1.10.2\"\n+description = \"Binary Python wheels for all tree sitter languages.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:5580348f0b20233b1d5431fa178ccd3d07423ca4a3275df02a44608fd72344b9\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:103c7466644486b1e9e03850df46fc6aa12f13ca636c74f173270276220ac80b\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d13db84511c6f1a7dc40383b66deafa74dabd8b877e3d65ab253f3719eccafd6\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:57adfa32be7e465b54aa72f915f6c78a2b66b227df4f656b5d4fbd1ca7a92b3f\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1c6385e033e460ceb8f33f3f940335f422ef2b763700a04f0089391a68b56153\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:dfa3f38cc5381c5aba01dd7494f59b8a9050e82ff6e06e1233e3a0cbae297e3c\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:9f195155acf47f8bc5de7cee46ecd07b2f5697f007ba89435b51ef4c0b953ea5\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:2de330e2ac6d7426ca025a3ec0f10d5640c3682c1d0c7702e812dcfb44b58120\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-win32.whl\", hash = \"sha256:c9731cf745f135d9770eeba9bb4e2ff4dabc107b5ae9b8211e919f6b9100ea6d\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:6dd75851c41d0c3c4987a9b7692d90fa8848706c23115669d8224ffd6571e357\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:7eb7d7542b2091c875fe52719209631fca36f8c10fa66970d2c576ae6a1b8289\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:6b41bcb00974b1c8a1800c7f1bb476a1d15a0463e760ee24872f2d53b08ee424\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6f370cd7845c6c81df05680d5bd96db8a99d32b56f4728c5d05978911130a853\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a1dc195c88ef4c72607e112a809a69190e096a2e5ebc6201548b3e05fdd169ad\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9ae34ac314a7170be24998a0f994c1ac80761d8d4bd126af27ee53a023d3b849\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:01b5742d5f5bd675489486b582bd482215880b26dde042c067f8265a6e925d9c\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:ab1cbc46244d34fd16f21edaa20231b2a57f09f092a06ee3d469f3117e6eb954\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:0b1149e7467a4e92b8a70e6005fe762f880f493cf811fc003554b29f04f5e7c8\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-win32.whl\", hash = \"sha256:049276343962f4696390ee555acc2c1a65873270c66a6cbe5cb0bca83bcdf3c6\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:7f3fdd468a577f04db3b63454d939e26e360229b53c80361920aa1ebf2cd7491\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:c0f4c8b2734c45859edc7fcaaeaab97a074114111b5ba51ab4ec7ed52104763c\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:eecd3c1244ac3425b7a82ba9125b4ddb45d953bbe61de114c0334fd89b7fe782\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:15db3c8510bc39a80147ee7421bf4782c15c09581c1dc2237ea89cefbd95b846\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:92c6487a6feea683154d3e06e6db68c30e0ae749a7ce4ce90b9e4e46b78c85c7\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6d2f1cd1d1bdd65332f9c2b67d49dcf148cf1ded752851d159ac3e5ee4f4d260\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:976c8039165b8e12f17a01ddee9f4e23ec6e352b165ad29b44d2bf04e2fbe77e\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:dafbbdf16bf668a580902e1620f4baa1913e79438abcce721a50647564c687b9\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:1aeabd3d60d6d276b73cd8f3739d595b1299d123cc079a317f1a5b3c5461e2ca\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-win32.whl\", hash = \"sha256:fab8ee641914098e8933b87ea3d657bea4dd00723c1ee7038b847b12eeeef4f5\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:5e606430d736367e5787fa5a7a0c5a1ec9b85eded0b3596bbc0d83532a40810b\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:838d5b48a7ed7a17658721952c77fda4570d2a069f933502653b17e15a9c39c9\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:987b3c71b1d278c2889e018ee77b8ee05c384e2e3334dec798f8b611c4ab2d1e\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:faa00abcb2c819027df58472da055d22fa7dfcb77c77413d8500c32ebe24d38b\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0e102fbbf02322d9201a86a814e79a9734ac80679fdb9682144479044f401a73\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:8f0b87cf1a7b03174ba18dfd81582be82bfed26803aebfe222bd20e444aba003\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:c0f1b9af9cb67f0b942b020da9fdd000aad5e92f2383ae0ba7a330b318d31912\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:5a4076c921f7a4d31e643843de7dfe040b65b63a238a5aa8d31d93aabe6572aa\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-win32.whl\", hash = \"sha256:fa6391a3a5d83d32db80815161237b67d70576f090ce5f38339206e917a6f8bd\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-win_amd64.whl\", hash = \"sha256:55649d3f254585a064121513627cf9788c1cfdadbc5f097f33d5ba750685a4c0\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:6f85d1edaa2d22d80d4ea5b6d12b95cf3644017b6c227d0d42854439e02e8893\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:d78feed4a764ef3141cb54bf00fe94d514d8b6e26e09423e23b4c616fcb7938c\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:da1aca27531f9dd5308637d76643372856f0f65d0d28677d1bcf4211e8ed1ad0\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:1031ea440dafb72237437d754eff8940153a3b051e3d18932ac25e75ce060a15\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:99d3249beaef2c9fe558ecc9a97853c260433a849dcc68266d9770d196c2e102\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:59a4450f262a55148fb7e68681522f0c2a2f6b7d89666312a2b32708d8f416e1\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:ce74eab0e430370d5e15a96b6c6205f93405c177a8b2e71e1526643b2fb9bab1\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:9b4dd2b6b3d24c85dffe33d6c343448869eaf4f41c19ddba662eb5d65d8808f4\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-win32.whl\", hash = \"sha256:92d734fb968fe3927a7596d9f0459f81a8fa7b07e16569476b28e27d0d753348\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:46a13f7d38f2eeb75f7cf127d1201346093748c270d686131f0cbc50e42870a1\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:f8c6a936ae99fdd8857e91f86c11c2f5e507ff30631d141d98132bb7ab2c8638\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:c283a61423f49cdfa7b5a5dfbb39221e3bd126fca33479cd80749d4d7a6b7349\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:76e60be6bdcff923386a54a5edcb6ff33fc38ab0118636a762024fa2bc98de55\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c00069f9575bd831eabcce2cdfab158dde1ed151e7e5614c2d985ff7d78a7de1\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:475ff53203d8a43ccb19bb322fa2fb200d764001cc037793f1fadd714bb343da\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:26fe7c9c412e4141dea87ea4b3592fd12e385465b5bdab106b0d5125754d4f60\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:8fed27319957458340f24fe14daad467cd45021da034eef583519f83113a8c5e\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:3657a491a7f96cc75a3568ddd062d25f3be82b6a942c68801a7b226ff7130181\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-win32.whl\", hash = \"sha256:33f7d584d01a7a3c893072f34cfc64ec031f3cfe57eebc32da2f8ac046e101a7\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:1b944af3ee729fa70fc8ae82224a9ff597cdb63addea084e0ea2fa2b0ec39bb7\"},\n+]\n+\n+[package.dependencies]\n+tree-sitter = \"*\"\n+\n [[package]]\n name = \"triton\"\n version = \"2.2.0\"\n\ndiff --git a/tests/unit/test_aider_linter.py b/tests/unit/test_aider_linter.py\nnew file mode 100644\nindex 000000000000..f13d6d24b921\n--- /dev/null\n+++ b/tests/unit/test_aider_linter.py\n@@ -0,0 +1,197 @@\n+import os\n+\n+import pytest\n+\n+from opendevin.runtime.plugins.agent_skills.aider import Linter, LintResult\n+\n+\n+@pytest.fixture\n+def temp_file(tmp_path):\n+    # Fixture to create a temporary file\n+    temp_name = os.path.join(tmp_path, 'lint-test.py')\n+    with open(temp_name, 'w', encoding='utf-8') as tmp_file:\n+        tmp_file.write(\"\"\"def foo():\n+    print(\"Hello, World!\")\n+foo()\n+\"\"\")\n+    tmp_file.close()\n+    yield temp_name\n+    os.remove(temp_name)\n+\n+\n+@pytest.fixture\n+def temp_ruby_file_errors(tmp_path):\n+    # Fixture to create a temporary file\n+    temp_name = os.path.join(tmp_path, 'lint-test.rb')\n+    with open(temp_name, 'w', encoding='utf-8') as tmp_file:\n+        tmp_file.write(\"\"\"def foo():\n+    print(\"Hello, World!\")\n+foo()\n+\"\"\")\n+    tmp_file.close()\n+    yield temp_name\n+    os.remove(temp_name)\n+\n+\n+@pytest.fixture\n+def temp_ruby_file_errors_parentheses(tmp_path):\n+    # Fixture to create a temporary file\n+    temp_name = os.path.join(tmp_path, 'lint-test.rb')\n+    with open(temp_name, 'w', encoding='utf-8') as tmp_file:\n+        tmp_file.write(\"\"\"def print_hello_world()\\n    puts 'Hello World'\\n\"\"\")\n+    tmp_file.close()\n+    yield temp_name\n+    os.remove(temp_name)\n+\n+\n+@pytest.fixture\n+def temp_ruby_file_correct(tmp_path):\n+    # Fixture to create a temporary file\n+    temp_name = os.path.join(tmp_path, 'lint-test.rb')\n+    with open(temp_name, 'w', encoding='utf-8') as tmp_file:\n+        tmp_file.write(\"\"\"def foo\n+  puts \"Hello, World!\"\n+end\n+foo\n+\"\"\")\n+    tmp_file.close()\n+    yield temp_name\n+    os.remove(temp_name)\n+\n+\n+@pytest.fixture\n+def linter(tmp_path):\n+    return Linter(root=tmp_path)\n+\n+\n+def test_get_rel_fname(linter, temp_file, tmp_path):\n+    # Test get_rel_fname method\n+    rel_fname = linter.get_rel_fname(temp_file)\n+\n+    assert rel_fname == os.path.relpath(temp_file, tmp_path)\n+\n+\n+def test_run_cmd(linter, temp_file):\n+    # Test run_cmd method with a simple command\n+    result = linter.run_cmd('echo', temp_file, '')\n+\n+    assert result is None  # echo command should return zero exit status\n+\n+\n+def test_set_linter(linter):\n+    # Test set_linter method\n+    def custom_linter(fname, rel_fname, code):\n+        return LintResult(text='Custom Linter', lines=[1])\n+\n+    linter.set_linter('custom', custom_linter)\n+\n+    assert 'custom' in linter.languages\n+    assert linter.languages['custom'] == custom_linter\n+\n+\n+def test_py_lint(linter, temp_file):\n+    # Test py_lint method\n+    result = linter.py_lint(\n+        temp_file, linter.get_rel_fname(temp_file), \"print('Hello, World!')\\n\"\n+    )\n+\n+    assert result is None  # No lint errors expected for this simple code\n+\n+\n+def test_py_lint_fail(linter, temp_file):\n+    # Test py_lint method\n+    result = linter.py_lint(\n+        temp_file, linter.get_rel_fname(temp_file), \"print('Hello, World!')\\n\"\n+    )\n+\n+    assert result is None\n+\n+\n+def test_basic_lint(temp_file):\n+    from opendevin.runtime.plugins.agent_skills.aider.linter import basic_lint\n+\n+    poorly_formatted_code = \"\"\"\n+        def foo()\n+            print(\"Hello, World!\")\n+        print(\"Wrong indent\")\n+        foo(\n+        \"\"\"\n+    result = basic_lint(temp_file, poorly_formatted_code)\n+\n+    assert isinstance(result, LintResult)\n+    assert result.text == f'{temp_file}:2'\n+    assert 2 in result.lines\n+\n+\n+def test_basic_lint_fail_returns_text_and_lines(temp_file):\n+    from opendevin.runtime.plugins.agent_skills.aider.linter import basic_lint\n+\n+    poorly_formatted_code = \"\"\"\n+        def foo()\n+            print(\"Hello, World!\")\n+        print(\"Wrong indent\")\n+        foo(\n+        \"\"\"\n+\n+    result = basic_lint(temp_file, poorly_formatted_code)\n+\n+    assert isinstance(result, LintResult)\n+    assert result.text == f'{temp_file}:2'\n+    assert 2 in result.lines\n+\n+\n+def test_lint_python_compile(temp_file):\n+    from opendevin.runtime.plugins.agent_skills.aider.linter import lint_python_compile\n+\n+    result = lint_python_compile(temp_file, \"print('Hello, World!')\\n\")\n+\n+    assert result is None\n+\n+\n+def test_lint_python_compile_fail_returns_text_and_lines(temp_file):\n+    from opendevin.runtime.plugins.agent_skills.aider.linter import lint_python_compile\n+\n+    poorly_formatted_code = \"\"\"\n+        def foo()\n+            print(\"Hello, World!\")\n+        print(\"Wrong indent\")\n+        foo(\n+        \"\"\"\n+    result = lint_python_compile(temp_file, poorly_formatted_code)\n+\n+    assert temp_file in result.text\n+    assert 1 in result.lines\n+\n+\n+def test_lint(linter, temp_file):\n+    result = linter.lint(temp_file)\n+    assert result is None\n+\n+\n+def test_lint_fail(linter, temp_file):\n+    # Test lint method\n+    with open(temp_file, 'w', encoding='utf-8') as lint_file:\n+        lint_file.write(\"\"\"\n+def foo()\n+    print(\"Hello, World!\")\n+  print(\"Wrong indent\")\n+foo(\n+\"\"\")\n+    errors = linter.lint(temp_file)\n+\n+    assert errors is not None\n+\n+\n+def test_lint_pass_ruby(linter, temp_ruby_file_correct):\n+    result = linter.lint(temp_ruby_file_correct)\n+    assert result is None\n+\n+\n+def test_lint_fail_ruby(linter, temp_ruby_file_errors):\n+    errors = linter.lint(temp_ruby_file_errors)\n+    assert errors is not None\n+\n+\n+def test_lint_fail_ruby_no_parentheses(linter, temp_ruby_file_errors_parentheses):\n+    errors = linter.lint(temp_ruby_file_errors_parentheses)\n+    assert errors is not None\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Great to see the progress on the linter integration! I noticed some failing tests, particularly related to the LLM API key; however, this seems to be an existing issue on the main branch. Ensure the failing unit tests are resolved, as they could impact stability. Regarding the sandbox errors, it appears unrelated to your PR but should be addressed to streamline debugging. The Jupyter kernel issue you encountered may stem from configuration discrepancies. Let's prioritize the sandbox tests to facilitate smoother evaluations on future PRs."
      }
    ],
    "codediff_length": 40310
  },
  {
    "index": 51,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/openhands/runtime/impl/cli/cli_runtime.py b/openhands/runtime/impl/cli/cli_runtime.py\nnew file mode 100644\nindex 000000000000..3f6ed4e36ede\n--- /dev/null\n+++ b/openhands/runtime/impl/cli/cli_runtime.py\n@@ -0,0 +1,782 @@\n+\"\"\"\n+This runtime runs commands locally using subprocess and performs file operations using Python's standard library.\n+It does not implement browser functionality.\n+\"\"\"\n+\n+import asyncio\n+import os\n+import select\n+import shutil\n+import signal\n+import subprocess\n+import tempfile\n+import time\n+import zipfile\n+from pathlib import Path\n+from typing import Any, Callable\n+\n+from binaryornot.check import is_binary\n+from openhands_aci.editor.editor import OHEditor\n+from openhands_aci.editor.exceptions import ToolError\n+from openhands_aci.editor.results import ToolResult\n+from openhands_aci.utils.diff import get_diff\n+from pydantic import SecretStr\n+\n+from openhands.core.config import AppConfig\n+from openhands.core.config.mcp_config import MCPConfig, MCPStdioServerConfig\n+from openhands.core.exceptions import LLMMalformedActionError\n+from openhands.core.logger import openhands_logger as logger\n+from openhands.events import EventStream\n+from openhands.events.action import (\n+    BrowseInteractiveAction,\n+    BrowseURLAction,\n+    CmdRunAction,\n+    FileEditAction,\n+    FileReadAction,\n+    FileWriteAction,\n+    IPythonRunCellAction,\n+)\n+from openhands.events.action.mcp import MCPAction\n+from openhands.events.event import FileEditSource, FileReadSource\n+from openhands.events.observation import (\n+    CmdOutputObservation,\n+    ErrorObservation,\n+    FileEditObservation,\n+    FileReadObservation,\n+    FileWriteObservation,\n+    Observation,\n+)\n+from openhands.integrations.provider import PROVIDER_TOKEN_TYPE\n+from openhands.runtime.base import Runtime\n+from openhands.runtime.plugins import PluginRequirement\n+\n+\n+class CLIRuntime(Runtime):\n+    \"\"\"\n+    A runtime implementation that runs commands locally using subprocess and performs\n+    file operations using Python's standard library. It does not implement browser functionality.\n+\n+    Args:\n+        config (AppConfig): The application configuration.\n+        event_stream (EventStream): The event stream to subscribe to.\n+        sid (str, optional): The session ID. Defaults to 'default'.\n+        plugins (list[PluginRequirement] | None, optional): List of plugin requirements. Defaults to None.\n+        env_vars (dict[str, str] | None, optional): Environment variables to set. Defaults to None.\n+        status_callback (Callable | None, optional): Callback for status updates. Defaults to None.\n+        attach_to_existing (bool, optional): Whether to attach to an existing session. Defaults to False.\n+        headless_mode (bool, optional): Whether to run in headless mode. Defaults to False.\n+        user_id (str | None, optional): User ID for authentication. Defaults to None.\n+        git_provider_tokens (PROVIDER_TOKEN_TYPE | None, optional): Git provider tokens. Defaults to None.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: AppConfig,\n+        event_stream: EventStream,\n+        sid: str = 'default',\n+        plugins: list[PluginRequirement] | None = None,\n+        env_vars: dict[str, str] | None = None,\n+        status_callback: Callable[[str, str, str], None] | None = None,\n+        attach_to_existing: bool = False,\n+        headless_mode: bool = False,\n+        user_id: str | None = None,\n+        git_provider_tokens: PROVIDER_TOKEN_TYPE | None = None,\n+    ):\n+        super().__init__(\n+            config,\n+            event_stream,\n+            sid,\n+            plugins,\n+            env_vars,\n+            status_callback,\n+            attach_to_existing,\n+            headless_mode,\n+            user_id,\n+            git_provider_tokens,\n+        )\n+\n+        # Set up workspace\n+        if self.config.workspace_base is not None:\n+            logger.warning(\n+                f'Workspace base path is set to {self.config.workspace_base}. '\n+                'It will be used as the path for the agent to run in. '\n+                'Be careful, the agent can EDIT files in this directory!'\n+            )\n+            self._workspace_path = self.config.workspace_base\n+        else:\n+            # Create a temporary directory for the workspace\n+            self._workspace_path = tempfile.mkdtemp(\n+                prefix=f'openhands_workspace_{sid}_'\n+            )\n+            logger.info(f'Created temporary workspace at {self._workspace_path}')\n+\n+        # Initialize runtime state\n+        self._runtime_initialized = False\n+        self.file_editor = OHEditor(workspace_root=self._workspace_path)\n+        self._shell_stream_callback: Callable[[str], None] | None = None\n+\n+        logger.warning(\n+            'Initializing CLIRuntime. WARNING: NO SANDBOX IS USED. '\n+            'This runtime executes commands directly on the local system. '\n+            'Use with caution in untrusted environments.'\n+        )\n+\n+    async def connect(self) -> None:\n+        \"\"\"Initialize the runtime connection.\"\"\"\n+        self.send_status_message('STATUS$STARTING_RUNTIME')\n+\n+        # Ensure workspace directory exists\n+        os.makedirs(self._workspace_path, exist_ok=True)\n+\n+        # Change to the workspace directory\n+        os.chdir(self._workspace_path)\n+\n+        if not self.attach_to_existing:\n+            await asyncio.to_thread(self.setup_initial_env)\n+\n+        self._runtime_initialized = True\n+        self.send_status_message('STATUS$CONTAINER_STARTED')\n+        logger.info(f'CLIRuntime initialized with workspace at {self._workspace_path}')\n+\n+    def add_env_vars(self, env_vars: dict[str, Any]) -> None:\n+        \"\"\"\n+        Adds environment variables to the current runtime environment.\n+        For CLIRuntime, this means updating os.environ for the current process,\n+        so that subsequent commands inherit these variables.\n+        This overrides the BaseRuntime behavior which tries to run shell commands\n+        before it's initialized and modify .bashrc, which is not ideal for local CLI.\n+        \"\"\"\n+        if not env_vars:\n+            return\n+\n+        # We log only keys to avoid leaking sensitive values like tokens into logs.\n+        logger.info(\n+            f'[CLIRuntime] Setting environment variables for this session: {list(env_vars.keys())}'\n+        )\n+\n+        for key, value in env_vars.items():\n+            if isinstance(value, SecretStr):\n+                os.environ[key] = value.get_secret_value()\n+                logger.warning(f'[CLIRuntime] Set os.environ[\"{key}\"] (from SecretStr)')\n+            else:\n+                os.environ[key] = value\n+                logger.debug(f'[CLIRuntime] Set os.environ[\"{key}\"]')\n+\n+        # We don't use self.run() here because this method is called\n+        # during initialization before self._runtime_initialized is True.\n+\n+    def _safe_terminate_process(self, process_obj, signal_to_send=signal.SIGTERM):\n+        \"\"\"\n+        Safely attempts to terminate/kill a process group or a single process.\n+\n+        Args:\n+            process_obj: the subprocess.Popen object started with start_new_session=True\n+            signal_to_send: the signal to send to the process group or process.\n+        \"\"\"\n+        pid = getattr(process_obj, 'pid', None)\n+        if pid is None:\n+            return\n+\n+        group_desc = (\n+            'kill process group'\n+            if signal_to_send == signal.SIGKILL\n+            else 'terminate process group'\n+        )\n+        process_desc = (\n+            'kill process' if signal_to_send == signal.SIGKILL else 'terminate process'\n+        )\n+\n+        try:\n+            # Try to terminate/kill the entire process group\n+            logger.debug(f'[_safe_terminate_process] Original PID to act on: {pid}')\n+            pgid_to_kill = os.getpgid(\n+                pid\n+            )  # This might raise ProcessLookupError if pid is already gone\n+            logger.debug(\n+                f'[_safe_terminate_process] Attempting to {group_desc} for PID {pid} (PGID: {pgid_to_kill}) with {signal_to_send}.'\n+            )\n+            os.killpg(pgid_to_kill, signal_to_send)\n+            logger.debug(\n+                f'[_safe_terminate_process] Successfully sent signal {signal_to_send} to PGID {pgid_to_kill} (original PID: {pid}).'\n+            )\n+        except ProcessLookupError as e_pgid:\n+            logger.warning(\n+                f'[_safe_terminate_process] ProcessLookupError getting PGID for PID {pid} (it might have already exited): {e_pgid}. Falling back to direct kill/terminate.'\n+            )\n+            try:\n+                if signal_to_send == signal.SIGKILL:\n+                    process_obj.kill()\n+                else:\n+                    process_obj.terminate()\n+                logger.debug(\n+                    f'[_safe_terminate_process] Fallback: Terminated {process_desc} (PID: {pid}).'\n+                )\n+            except Exception as e_fallback:\n+                logger.error(\n+                    f'[_safe_terminate_process] Fallback: Error during {process_desc} (PID: {pid}): {e_fallback}'\n+                )\n+        except (AttributeError, OSError) as e_os:\n+            logger.error(\n+                f'[_safe_terminate_process] OSError/AttributeError during {group_desc} for PID {pid}: {e_os}. Falling back.'\n+            )\n+            # Fallback: try to terminate/kill the main process directly.\n+            try:\n+                if signal_to_send == signal.SIGKILL:\n+                    process_obj.kill()\n+                else:\n+                    process_obj.terminate()\n+                logger.debug(\n+                    f'[_safe_terminate_process] Fallback: Terminated {process_desc} (PID: {pid}).'\n+                )\n+            except Exception as e_fallback:\n+                logger.error(\n+                    f'[_safe_terminate_process] Fallback: Error during {process_desc} (PID: {pid}): {e_fallback}'\n+                )\n+        except (KeyboardInterrupt, SystemExit):\n+            raise\n+        except Exception as e:\n+            logger.error(f'Error: {e}')\n+\n+    def _execute_shell_command(\n+        self, command: str, timeout: float\n+    ) -> CmdOutputObservation:\n+        \"\"\"\n+        Execute a shell command and stream its output to a callback function.\n+        Args:\n+            command: The shell command to execute\n+            timeout: Timeout in seconds for the command\n+        Returns:\n+            CmdOutputObservation containing the complete output and exit code\n+        \"\"\"\n+        output_lines = []\n+        timed_out = False\n+        start_time = time.monotonic()\n+\n+        # Use shell=True to run complex bash commands\n+        process = subprocess.Popen(\n+            ['bash', '-c', command],\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.STDOUT,\n+            text=True,\n+            bufsize=1,  # Explicitly line-buffered for text mode\n+            universal_newlines=True,\n+            start_new_session=True,\n+        )\n+        logger.debug(\n+            f'[_execute_shell_command] PID of bash -c: {process.pid} for command: \"{command}\"'\n+        )\n+\n+        exit_code = None\n+\n+        try:\n+            if process.stdout:\n+                while process.poll() is None:\n+                    if (\n+                        timeout is not None\n+                        and (time.monotonic() - start_time) > timeout\n+                    ):\n+                        logger.debug(\n+                            f'Command \"{command}\" timed out after {timeout:.1f} seconds. Terminating.'\n+                        )\n+                        # Attempt to terminate the process group (SIGTERM)\n+                        self._safe_terminate_process(\n+                            process, signal_to_send=signal.SIGTERM\n+                        )\n+                        timed_out = True\n+                        break\n+\n+                    ready_to_read, _, _ = select.select([process.stdout], [], [], 0.1)\n+\n+                    if ready_to_read:\n+                        line = process.stdout.readline()\n+                        if line:\n+                            logger.debug(f'LINE: {line}')\n+                            output_lines.append(line)\n+                            if self._shell_stream_callback:\n+                                self._shell_stream_callback(line)\n+\n+            # Attempt to read any remaining data from stdout\n+            if process.stdout and not process.stdout.closed:\n+                try:\n+                    while line:\n+                        line = process.stdout.readline()\n+                        if line:\n+                            logger.debug(f'LINE: {line}')\n+                            output_lines.append(line)\n+                            if self._shell_stream_callback:\n+                                self._shell_stream_callback(line)\n+                except Exception as e:\n+                    logger.warning(\n+                        f'Error reading directly from stdout after loop for \"{command}\": {e}'\n+                    )\n+\n+            exit_code = process.returncode\n+\n+            # If timeout occurred, ensure exit_code reflects this for the observation.\n+            if timed_out:\n+                exit_code = -1\n+\n+        except Exception as e:\n+            logger.error(\n+                f'Outer exception in _execute_shell_command for \"{command}\": {e}'\n+            )\n+            if process and process.poll() is None:\n+                self._safe_terminate_process(process, signal_to_send=signal.SIGKILL)\n+            return CmdOutputObservation(\n+                command=command,\n+                content=''.join(output_lines) + f'\\nError during execution: {e}',\n+                exit_code=-1,\n+            )\n+\n+        complete_output = ''.join(output_lines)\n+        logger.debug(\n+            f'[_execute_shell_command] Complete output for \"{command}\" (len: {len(complete_output)}): {complete_output!r}'\n+        )\n+        obs_metadata = {'working_dir': self._workspace_path}\n+        if timed_out:\n+            obs_metadata['suffix'] = (\n+                f'[The command timed out after {timeout:.1f} seconds.]'\n+            )\n+            # exit_code = -1 # This is already set if timed_out is True\n+\n+        return CmdOutputObservation(\n+            command=command,\n+            content=complete_output,\n+            exit_code=exit_code,\n+            metadata=obs_metadata,\n+        )\n+\n+    def run(self, action: CmdRunAction) -> Observation:\n+        \"\"\"Run a command using subprocess.\"\"\"\n+        if not self._runtime_initialized:\n+            return ErrorObservation(\n+                f'Runtime not initialized for command: {action.command}'\n+            )\n+\n+        if action.is_input:\n+            logger.warning(\n+                f\"CLIRuntime received an action with `is_input=True` (command: '{action.command}'). \"\n+                'CLIRuntime currently does not support sending input or signals to active processes. '\n+                'This action will be ignored and an error observation will be returned.'\n+            )\n+            return ErrorObservation(\n+                content=f\"CLIRuntime does not support interactive input from the agent (e.g., 'C-c'). The command '{action.command}' was not sent to any process.\",\n+                error_id='AGENT_ERROR$BAD_ACTION',\n+            )\n+\n+        try:\n+            effective_timeout = (\n+                action.timeout\n+                if action.timeout is not None\n+                else self.config.sandbox.timeout\n+            )\n+\n+            logger.debug(\n+                f'Running command in CLIRuntime: \"{action.command}\" with effective timeout: {effective_timeout}s'\n+            )\n+            return self._execute_shell_command(\n+                action.command, timeout=effective_timeout\n+            )\n+        except Exception as e:\n+            logger.error(\n+                f'Error in CLIRuntime.run for command \"{action.command}\": {str(e)}'\n+            )\n+            return ErrorObservation(\n+                f'Error running command \"{action.command}\": {str(e)}'\n+            )\n+\n+    def run_ipython(self, action: IPythonRunCellAction) -> Observation:\n+        \"\"\"Run a Python code cell.\n+        This functionality is not implemented in CLIRuntime.\n+        Users should also disable the Jupyter plugin in AgentConfig.\n+        \"\"\"\n+        # This functionality is not implemented in CLIRuntime.\n+        # If you need to run IPython/Jupyter cells, please consider using a different runtime\n+        # or ensure the Jupyter plugin is disabled in your AgentConfig to avoid\n+        # attempting to use this disabled feature.\n+        logger.warning(\n+            \"run_ipython is called on CLIRuntime, but it's not implemented. \"\n+            'Please disable the Jupyter plugin in AgentConfig.'\n+        )\n+        return ErrorObservation(\n+            'Executing IPython cells is not implemented in CLIRuntime. '\n+        )\n+\n+    def _sanitize_filename(self, filename: str) -> str:\n+        # if path is absolute, ensure it starts with _workspace_path\n+        if filename == '/workspace':\n+            actual_filename = self._workspace_path\n+        elif filename.startswith('/workspace/'):\n+            # Map /workspace/ to the actual workspace path\n+            # Note: /workspace is widely used, so we map it to allow using it with CLIRuntime\n+            actual_filename = os.path.join(\n+                self._workspace_path, filename[len('/workspace/') :]\n+            )\n+        elif filename.startswith('/'):\n+            if not filename.startswith(self._workspace_path):\n+                raise LLMMalformedActionError(\n+                    f'Invalid path: {filename}. You can only work with files in {self._workspace_path}.'\n+                )\n+            actual_filename = filename\n+        else:\n+            actual_filename = os.path.join(self._workspace_path, filename.lstrip('/'))\n+\n+        # Resolve the path to handle any '..' or '.' components\n+        resolved_path = os.path.realpath(actual_filename)\n+\n+        # Check if the resolved path is still within the workspace\n+        if not resolved_path.startswith(self._workspace_path):\n+            raise LLMMalformedActionError(\n+                f'Invalid path traversal: {filename}. Path resolves outside the workspace. Resolved: {resolved_path}, Workspace: {self._workspace_path}'\n+            )\n+\n+        return resolved_path\n+\n+    def read(self, action: FileReadAction) -> Observation:\n+        \"\"\"Read a file using Python's standard library or OHEditor.\"\"\"\n+        if not self._runtime_initialized:\n+            return ErrorObservation('Runtime not initialized')\n+\n+        file_path = self._sanitize_filename(action.path)\n+\n+        # Cannot read binary files\n+        if os.path.exists(file_path) and is_binary(file_path):\n+            return ErrorObservation('ERROR_BINARY_FILE')\n+\n+        # Use OHEditor for OH_ACI implementation source\n+        if action.impl_source == FileReadSource.OH_ACI:\n+            result_str, _ = self._execute_file_editor(\n+                command='view',\n+                path=file_path,\n+                view_range=action.view_range,\n+            )\n+\n+            return FileReadObservation(\n+                content=result_str,\n+                path=action.path,\n+                impl_source=FileReadSource.OH_ACI,\n+            )\n+\n+        try:\n+            # Check if the file exists\n+            if not os.path.exists(file_path):\n+                return ErrorObservation(f'File not found: {action.path}')\n+\n+            # Check if it's a directory\n+            if os.path.isdir(file_path):\n+                return ErrorObservation(f'Cannot read directory: {action.path}')\n+\n+            # Read the file\n+            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n+                content = f.read()\n+\n+            return FileReadObservation(content=content, path=action.path)\n+        except Exception as e:\n+            logger.error(f'Error reading file: {str(e)}')\n+            return ErrorObservation(f'Error reading file {action.path}: {str(e)}')\n+\n+    def write(self, action: FileWriteAction) -> Observation:\n+        \"\"\"Write to a file using Python's standard library.\"\"\"\n+        if not self._runtime_initialized:\n+            return ErrorObservation('Runtime not initialized')\n+\n+        file_path = self._sanitize_filename(action.path)\n+\n+        try:\n+            # Create parent directories if they don't exist\n+            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n+\n+            # Write to the file\n+            with open(file_path, 'w', encoding='utf-8') as f:\n+                f.write(action.content)\n+\n+            return FileWriteObservation(content='', path=action.path)\n+        except Exception as e:\n+            logger.error(f'Error writing to file: {str(e)}')\n+            return ErrorObservation(f'Error writing to file {action.path}: {str(e)}')\n+\n+    def browse(self, action: BrowseURLAction) -> Observation:\n+        \"\"\"Not implemented for CLI runtime.\"\"\"\n+        return ErrorObservation(\n+            'Browser functionality is not implemented in CLIRuntime'\n+        )\n+\n+    def browse_interactive(self, action: BrowseInteractiveAction) -> Observation:\n+        \"\"\"Not implemented for CLI runtime.\"\"\"\n+        return ErrorObservation(\n+            'Browser functionality is not implemented in CLIRuntime'\n+        )\n+\n+    def _execute_file_editor(\n+        self,\n+        command: str,\n+        path: str,\n+        file_text: str | None = None,\n+        view_range: list[int] | None = None,\n+        old_str: str | None = None,\n+        new_str: str | None = None,\n+        insert_line: int | None = None,\n+        enable_linting: bool = False,\n+    ) -> tuple[str, tuple[str | None, str | None]]:\n+        \"\"\"Execute file editor command and handle exceptions.\n+\n+        Args:\n+            command: Editor command to execute\n+            path: File path\n+            file_text: Optional file text content\n+            view_range: Optional view range tuple (start, end)\n+            old_str: Optional string to replace\n+            new_str: Optional replacement string\n+            insert_line: Optional line number for insertion\n+            enable_linting: Whether to enable linting\n+\n+        Returns:\n+            tuple: A tuple containing the output string and a tuple of old and new file content\n+        \"\"\"\n+        result: ToolResult | None = None\n+        try:\n+            result = self.file_editor(\n+                command=command,\n+                path=path,\n+                file_text=file_text,\n+                view_range=view_range,\n+                old_str=old_str,\n+                new_str=new_str,\n+                insert_line=insert_line,\n+                enable_linting=enable_linting,\n+            )\n+        except ToolError as e:\n+            result = ToolResult(error=e.message)\n+\n+        if result.error:\n+            return f'ERROR:\\n{result.error}', (None, None)\n+\n+        if not result.output:\n+            logger.warning(f'No output from file_editor for {path}')\n+            return '', (None, None)\n+\n+        return result.output, (result.old_content, result.new_content)\n+\n+    def edit(self, action: FileEditAction) -> Observation:\n+        \"\"\"Edit a file using the OHEditor.\"\"\"\n+        if not self._runtime_initialized:\n+            return ErrorObservation('Runtime not initialized')\n+\n+        # Ensure the path is within the workspace\n+        file_path = self._sanitize_filename(action.path)\n+\n+        # Check if it's a binary file\n+        if os.path.exists(file_path) and is_binary(file_path):\n+            return ErrorObservation('ERROR_BINARY_FILE')\n+\n+        assert action.impl_source == FileEditSource.OH_ACI\n+\n+        result_str, (old_content, new_content) = self._execute_file_editor(\n+            command=action.command,\n+            path=file_path,\n+            file_text=action.file_text,\n+            old_str=action.old_str,\n+            new_str=action.new_str,\n+            insert_line=action.insert_line,\n+            enable_linting=False,\n+        )\n+\n+        return FileEditObservation(\n+            content=result_str,\n+            path=action.path,\n+            old_content=action.old_str,\n+            new_content=action.new_str,\n+            impl_source=FileEditSource.OH_ACI,\n+            diff=get_diff(\n+                old_contents=old_content or '',\n+                new_contents=new_content or '',\n+                filepath=action.path,\n+            ),\n+        )\n+\n+    async def call_tool_mcp(self, action: MCPAction) -> Observation:\n+        \"\"\"Not implemented for CLI runtime.\"\"\"\n+        return ErrorObservation('MCP functionality is not implemented in CLIRuntime')\n+\n+    @property\n+    def workspace_root(self) -> Path:\n+        \"\"\"Return the workspace root path.\"\"\"\n+        return Path(os.path.abspath(self._workspace_path))\n+\n+    def copy_to(self, host_src: str, sandbox_dest: str, recursive: bool = False):\n+        \"\"\"Copy a file or directory from the host to the sandbox.\"\"\"\n+        if not self._runtime_initialized:\n+            raise RuntimeError('Runtime not initialized')\n+        if not os.path.exists(host_src):  # Source must exist on host\n+            raise FileNotFoundError(f\"Source path '{host_src}' does not exist.\")\n+\n+        dest = self._sanitize_filename(sandbox_dest)\n+\n+        try:\n+            # Case 1: Source is a directory and recursive copy.\n+            if os.path.isdir(host_src) and recursive:\n+                # Target is dest / basename(host_src)\n+                final_target_dir = os.path.join(dest, os.path.basename(host_src))\n+\n+                # If source and final target are the same, skip.\n+                if os.path.realpath(host_src) == os.path.realpath(final_target_dir):\n+                    logger.debug(\n+                        'Skipping recursive copy: source and target are identical.'\n+                    )\n+                    pass\n+                else:\n+                    # Ensure parent of final_target_dir exists.\n+                    os.makedirs(dest, exist_ok=True)\n+                    shutil.copytree(host_src, final_target_dir, dirs_exist_ok=True)\n+                    # Why: Copies dir host_src into dest. Merges if target exists.\n+\n+            # Case 2: Source is a file.\n+            elif os.path.isfile(host_src):\n+                final_target_file_path: str\n+                # Scenario A: sandbox_dest is clearly a directory.\n+                if os.path.isdir(dest) or (sandbox_dest.endswith(('/', os.sep))):\n+                    target_dir = dest\n+                    os.makedirs(target_dir, exist_ok=True)\n+                    final_target_file_path = os.path.join(\n+                        target_dir, os.path.basename(host_src)\n+                    )\n+                    # Why: Copies file into specified directory.\n+\n+                # Scenario B: sandbox_dest is likely a new directory (e.g., 'new_dir').\n+                elif not os.path.exists(dest) and '.' not in os.path.basename(dest):\n+                    target_dir = dest\n+                    os.makedirs(target_dir, exist_ok=True)\n+                    final_target_file_path = os.path.join(\n+                        target_dir, os.path.basename(host_src)\n+                    )\n+                    # Why: Creates 'new_dir' and copies file into it.\n+\n+                # Scenario C: sandbox_dest is a full file path.\n+                else:\n+                    final_target_file_path = dest\n+                    os.makedirs(os.path.dirname(final_target_file_path), exist_ok=True)\n+                    # Why: Copies file to a specific path, possibly renaming.\n+\n+                shutil.copy2(host_src, final_target_file_path)\n+\n+            else:  # Source is not a valid file or directory.\n+                raise FileNotFoundError(\n+                    f\"Source path '{host_src}' is not a valid file or directory.\"\n+                )\n+\n+        except FileNotFoundError as e:\n+            logger.error(f'File not found during copy: {str(e)}')\n+            raise\n+        except shutil.SameFileError as e:\n+            # We can be lenient here, just ignore this error.\n+            logger.debug(\n+                f'Skipping copy as source and destination are the same: {str(e)}'\n+            )\n+            pass\n+        except Exception as e:\n+            logger.error(f'Unexpected error copying file: {str(e)}')\n+            raise RuntimeError(f'Unexpected error copying file: {str(e)}')\n+\n+    def list_files(self, path: str | None = None) -> list[str]:\n+        \"\"\"List files in the sandbox.\"\"\"\n+        if not self._runtime_initialized:\n+            raise RuntimeError('Runtime not initialized')\n+\n+        if path is None:\n+            dir_path = self._workspace_path\n+        else:\n+            dir_path = self._sanitize_filename(path)\n+\n+        try:\n+            if not os.path.exists(dir_path):\n+                return []\n+\n+            if not os.path.isdir(dir_path):\n+                return [dir_path]\n+\n+            # List files in the directory\n+            return [os.path.join(dir_path, f) for f in os.listdir(dir_path)]\n+        except Exception as e:\n+            logger.error(f'Error listing files: {str(e)}')\n+            return []\n+\n+    def copy_from(self, path: str) -> Path:\n+        \"\"\"Zip all files in the sandbox and return a path in the local filesystem.\"\"\"\n+        if not self._runtime_initialized:\n+            raise RuntimeError('Runtime not initialized')\n+\n+        source_path = self._sanitize_filename(path)\n+\n+        if not os.path.exists(source_path):\n+            raise FileNotFoundError(f'Path not found: {path}')\n+\n+        # Create a temporary zip file\n+        temp_zip = tempfile.NamedTemporaryFile(suffix='.zip', delete=False)\n+        temp_zip.close()\n+\n+        try:\n+            with zipfile.ZipFile(temp_zip.name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n+                if os.path.isdir(source_path):\n+                    # Add all files in the directory\n+                    for root, _, files in os.walk(source_path):\n+                        for file in files:\n+                            file_path = os.path.join(root, file)\n+                            arcname = os.path.relpath(file_path, source_path)\n+                            zipf.write(file_path, arcname)\n+                else:\n+                    # Add a single file\n+                    zipf.write(source_path, os.path.basename(source_path))\n+\n+            return Path(temp_zip.name)\n+        except Exception as e:\n+            logger.error(f'Error creating zip file: {str(e)}')\n+            raise RuntimeError(f'Error creating zip file: {str(e)}')\n+\n+    def close(self) -> None:\n+        self._runtime_initialized = False\n+        super().close()\n+\n+    @classmethod\n+    async def delete(cls, conversation_id: str) -> None:\n+        \"\"\"Delete any resources associated with a conversation.\"\"\"\n+        # Look for temporary directories that might be associated with this conversation\n+        temp_dir = tempfile.gettempdir()\n+        prefix = f'openhands_workspace_{conversation_id}_'\n+\n+        for item in os.listdir(temp_dir):\n+            if item.startswith(prefix):\n+                try:\n+                    path = os.path.join(temp_dir, item)\n+                    if os.path.isdir(path):\n+                        shutil.rmtree(path)\n+                        logger.info(f'Deleted workspace directory: {path}')\n+                except Exception as e:\n+                    logger.error(f'Error deleting workspace directory: {str(e)}')\n+\n+    @property\n+    def additional_agent_instructions(self) -> str:\n+        return '\\n\\n'.join(\n+            [\n+                f'Your working directory is {self._workspace_path}. You can only read and write files in this directory.',\n+                \"You are working directly on the user's machine. In most cases, the working environment is already set up.\",\n+            ]\n+        )\n+\n+    def get_mcp_config(\n+        self, extra_stdio_servers: list[MCPStdioServerConfig] | None = None\n+    ) -> MCPConfig:\n+        # TODO: Load MCP config from a local file\n+        return MCPConfig()\n+\n+    def subscribe_to_shell_stream(\n+        self, callback: Callable[[str], None] | None = None\n+    ) -> bool:\n+        \"\"\"\n+        Subscribe to shell command output stream.\n+\n+        Args:\n+            callback: A function that will be called with each line of output from shell commands.\n+                     If None, any existing subscription will be removed.\n+        \"\"\"\n+        self._shell_stream_callback = callback\n+        return True\ndiff --git a/tests/unit/core/config/test_config_utils.py b/tests/unit/core/config/test_config_utils.py\nnew file mode 100644\nindex 000000000000..a18b26b817e7\n--- /dev/null\n+++ b/tests/unit/core/config/test_config_utils.py\n@@ -0,0 +1,135 @@\n+import pytest\n+\n+from openhands.core.config.agent_config import AgentConfig\n+from openhands.core.config.app_config import AppConfig\n+from openhands.core.config.utils import finalize_config, load_from_env, load_from_toml\n+\n+# Define a dummy agent name often used in tests or as a default\n+DEFAULT_AGENT_NAME = 'CodeActAgent'\n+\n+def test_finalize_config_cli_disables_jupyter_and_browsing_when_true():\n+    \"\"\"\n+    Test that finalize_config sets enable_jupyter and enable_browsing to False\n+    when runtime is 'cli' and they were initially True.\n+    \"\"\"\n+    app_config = AppConfig()\n+    app_config.runtime = 'cli'\n+    \n+    agent_config = AgentConfig(enable_jupyter=True, enable_browsing=True)\n+    app_config.agents[DEFAULT_AGENT_NAME] = agent_config\n+    \n+    finalize_config(app_config)\n+    \n+    assert not app_config.agents[DEFAULT_AGENT_NAME].enable_jupyter, \\\n+        \"enable_jupyter should be False when runtime is 'cli'\"\n+    assert not app_config.agents[DEFAULT_AGENT_NAME].enable_browsing, \\\n+        \"enable_browsing should be False when runtime is 'cli'\"\n+\n+def test_finalize_config_cli_keeps_jupyter_and_browsing_false_when_false():\n+    \"\"\"\n+    Test that finalize_config keeps enable_jupyter and enable_browsing as False\n+    when runtime is 'cli' and they were initially False.\n+    \"\"\"\n+    app_config = AppConfig()\n+    app_config.runtime = 'cli'\n+    \n+    agent_config = AgentConfig(enable_jupyter=False, enable_browsing=False)\n+    app_config.agents[DEFAULT_AGENT_NAME] = agent_config\n+    \n+    finalize_config(app_config)\n+    \n+    assert not app_config.agents[DEFAULT_AGENT_NAME].enable_jupyter, \\\n+        \"enable_jupyter should remain False when runtime is 'cli' and initially False\"\n+    assert not app_config.agents[DEFAULT_AGENT_NAME].enable_browsing, \\\n+        \"enable_browsing should remain False when runtime is 'cli' and initially False\"\n+\n+def test_finalize_config_other_runtime_keeps_jupyter_and_browsing_true_by_default():\n+    \"\"\"\n+    Test that finalize_config keeps enable_jupyter and enable_browsing as True (default)\n+    when runtime is not 'cli'.\n+    \"\"\"\n+    app_config = AppConfig()\n+    app_config.runtime = 'docker'  # A non-cli runtime\n+    \n+    # AgentConfig defaults enable_jupyter and enable_browsing to True\n+    agent_config = AgentConfig()\n+    app_config.agents[DEFAULT_AGENT_NAME] = agent_config\n+    \n+    finalize_config(app_config)\n+    \n+    assert app_config.agents[DEFAULT_AGENT_NAME].enable_jupyter, \\\n+        \"enable_jupyter should remain True by default for non-cli runtimes\"\n+    assert app_config.agents[DEFAULT_AGENT_NAME].enable_browsing, \\\n+        \"enable_browsing should remain True by default for non-cli runtimes\"\n+\n+def test_finalize_config_other_runtime_keeps_jupyter_and_browsing_false_if_set():\n+    \"\"\"\n+    Test that finalize_config keeps enable_jupyter and enable_browsing as False\n+    when runtime is not 'cli' but they were explicitly set to False.\n+    \"\"\"\n+    app_config = AppConfig()\n+    app_config.runtime = 'docker'  # A non-cli runtime\n+    \n+    agent_config = AgentConfig(enable_jupyter=False, enable_browsing=False)\n+    app_config.agents[DEFAULT_AGENT_NAME] = agent_config\n+    \n+    finalize_config(app_config)\n+    \n+    assert not app_config.agents[DEFAULT_AGENT_NAME].enable_jupyter, \\\n+        \"enable_jupyter should remain False for non-cli runtimes if explicitly set to False\"\n+    assert not app_config.agents[DEFAULT_AGENT_NAME].enable_browsing, \\\n+        \"enable_browsing should remain False for non-cli runtimes if explicitly set to False\"\n+\n+def test_finalize_config_no_agents_defined():\n+    \"\"\"\n+    Test that finalize_config runs without error if no agents are defined in the config,\n+    even when runtime is 'cli'.\n+    \"\"\"\n+    app_config = AppConfig()\n+    app_config.runtime = 'cli'\n+    # No agents are added to app_config.agents\n+    \n+    try:\n+        finalize_config(app_config)\n+    except Exception as e:\n+        pytest.fail(f\"finalize_config raised an exception with no agents defined: {e}\")\n+\n+def test_finalize_config_multiple_agents_cli_runtime():\n+    \"\"\"\n+    Test that finalize_config correctly disables jupyter and browsing for multiple agents\n+    when runtime is 'cli'.\n+    \"\"\"\n+    app_config = AppConfig()\n+    app_config.runtime = 'cli'\n+    \n+    agent_config1 = AgentConfig(enable_jupyter=True, enable_browsing=True)\n+    agent_config2 = AgentConfig(enable_jupyter=True, enable_browsing=True)\n+    app_config.agents['Agent1'] = agent_config1\n+    app_config.agents['Agent2'] = agent_config2\n+    \n+    finalize_config(app_config)\n+    \n+    assert not app_config.agents['Agent1'].enable_jupyter, \"Jupyter should be disabled for Agent1\"\n+    assert not app_config.agents['Agent1'].enable_browsing, \"Browsing should be disabled for Agent1\"\n+    assert not app_config.agents['Agent2'].enable_jupyter, \"Jupyter should be disabled for Agent2\"\n+    assert not app_config.agents['Agent2'].enable_browsing, \"Browsing should be disabled for Agent2\"\n+\n+def test_finalize_config_multiple_agents_other_runtime():\n+    \"\"\"\n+    Test that finalize_config correctly keeps jupyter and browsing enabled (or as set)\n+    for multiple agents when runtime is not 'cli'.\n+    \"\"\"\n+    app_config = AppConfig()\n+    app_config.runtime = 'docker'\n+    \n+    agent_config1 = AgentConfig(enable_jupyter=True, enable_browsing=True) # Defaults\n+    agent_config2 = AgentConfig(enable_jupyter=False, enable_browsing=False) # Explicitly false\n+    app_config.agents['Agent1'] = agent_config1\n+    app_config.agents['Agent2'] = agent_config2\n+    \n+    finalize_config(app_config)\n+    \n+    assert app_config.agents['Agent1'].enable_jupyter, \"Jupyter should be True for Agent1\"\n+    assert app_config.agents['Agent1'].enable_browsing, \"Browsing should be True for Agent1\"\n+    assert not app_config.agents['Agent2'].enable_jupyter, \"Jupyter should be False for Agent2\"\n+    assert not app_config.agents['Agent2'].enable_browsing, \"Browsing should be False for Agent2\"\n\\ No newline at end of file\ndiff --git a/tests/unit/test_cli_workspace.py b/tests/unit/test_cli_workspace.py\nnew file mode 100644\nindex 000000000000..6360fbd2961d\n--- /dev/null\n+++ b/tests/unit/test_cli_workspace.py\n@@ -0,0 +1,82 @@\n+\"\"\"Test CLIRuntime class.\"\"\"\n+\n+import os\n+import tempfile\n+\n+import pytest\n+\n+from openhands.core.config import AppConfig\n+from openhands.core.exceptions import LLMMalformedActionError\n+from openhands.events import EventStream\n+from openhands.runtime.impl.cli.cli_runtime import CLIRuntime\n+from openhands.storage import get_file_store\n+\n+\n+@pytest.fixture\n+def temp_dir():\n+    \"\"\"Create a temporary directory for testing.\"\"\"\n+    with tempfile.TemporaryDirectory() as temp_dir:\n+        yield temp_dir\n+\n+\n+@pytest.fixture\n+def cli_runtime(temp_dir):\n+    \"\"\"Create a CLIRuntime instance for testing.\"\"\"\n+    file_store = get_file_store('local', temp_dir)\n+    event_stream = EventStream('test', file_store)\n+    config = AppConfig()\n+    config.workspace_base = temp_dir\n+    runtime = CLIRuntime(config, event_stream)\n+    runtime._runtime_initialized = True  # Skip initialization\n+    return runtime\n+\n+\n+def test_sanitize_filename_valid_path(cli_runtime):\n+    \"\"\"Test _sanitize_filename with a valid path.\"\"\"\n+    test_path = os.path.join(cli_runtime._workspace_path, 'test.txt')\n+    sanitized_path = cli_runtime._sanitize_filename(test_path)\n+    assert sanitized_path == os.path.realpath(test_path)\n+\n+\n+def test_sanitize_filename_relative_path(cli_runtime):\n+    \"\"\"Test _sanitize_filename with a relative path.\"\"\"\n+    test_path = 'test.txt'\n+    expected_path = os.path.join(cli_runtime._workspace_path, test_path)\n+    sanitized_path = cli_runtime._sanitize_filename(test_path)\n+    assert sanitized_path == os.path.realpath(expected_path)\n+\n+\n+def test_sanitize_filename_outside_workspace(cli_runtime):\n+    \"\"\"Test _sanitize_filename with a path outside the workspace.\"\"\"\n+    test_path = '/tmp/test.txt'  # Path outside workspace\n+    with pytest.raises(LLMMalformedActionError) as exc_info:\n+        cli_runtime._sanitize_filename(test_path)\n+    assert 'Invalid path:' in str(exc_info.value)\n+    assert 'You can only work with files in' in str(exc_info.value)\n+\n+\n+def test_sanitize_filename_path_traversal(cli_runtime):\n+    \"\"\"Test _sanitize_filename with path traversal attempt.\"\"\"\n+    test_path = os.path.join(cli_runtime._workspace_path, '..', 'test.txt')\n+    with pytest.raises(LLMMalformedActionError) as exc_info:\n+        cli_runtime._sanitize_filename(test_path)\n+    assert 'Invalid path traversal:' in str(exc_info.value)\n+    assert 'Path resolves outside the workspace' in str(exc_info.value)\n+\n+\n+def test_sanitize_filename_absolute_path_with_dots(cli_runtime):\n+    \"\"\"Test _sanitize_filename with absolute path containing dots.\"\"\"\n+    test_path = os.path.join(cli_runtime._workspace_path, 'subdir', '..', 'test.txt')\n+    # Create the parent directory\n+    os.makedirs(os.path.join(cli_runtime._workspace_path, 'subdir'), exist_ok=True)\n+    sanitized_path = cli_runtime._sanitize_filename(test_path)\n+    assert sanitized_path == os.path.join(cli_runtime._workspace_path, 'test.txt')\n+\n+\n+def test_sanitize_filename_nested_path(cli_runtime):\n+    \"\"\"Test _sanitize_filename with a nested path.\"\"\"\n+    nested_dir = os.path.join(cli_runtime._workspace_path, 'dir1', 'dir2')\n+    os.makedirs(nested_dir, exist_ok=True)\n+    test_path = os.path.join(nested_dir, 'test.txt')\n+    sanitized_path = cli_runtime._sanitize_filename(test_path)\n+    assert sanitized_path == os.path.realpath(test_path)\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR shows solid progress with runtime implementations added in `__init__.py` and security validations in `cli_runtime.py`. However, three unit tests are currently failing, which indicates that some edge cases might not be handled correctly. Specifically, ensure that `LLMMalformedActionError` is correctly raised for invalid paths, as mentioned in `agent_controller.py`. Additionally, don't forget to integrate the new `CliRuntime` into the runtime test workflow and resolve any merge conflicts before completion. Overall, the groundwork is strong, but attention to these points is crucial."
      }
    ],
    "codediff_length": 42500
  },
  {
    "index": 52,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/opendevin/runtime/plugins/agent_skills/repomap/LICENSE.txt b/opendevin/runtime/plugins/agent_skills/repomap/LICENSE.txt\nnew file mode 100644\nindex 000000000000..d64569567334\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/repomap/LICENSE.txt\n@@ -0,0 +1,202 @@\n+\n+                                 Apache License\n+                           Version 2.0, January 2004\n+                        http://www.apache.org/licenses/\n+\n+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n+\n+   1. Definitions.\n+\n+      \"License\" shall mean the terms and conditions for use, reproduction,\n+      and distribution as defined by Sections 1 through 9 of this document.\n+\n+      \"Licensor\" shall mean the copyright owner or entity authorized by\n+      the copyright owner that is granting the License.\n+\n+      \"Legal Entity\" shall mean the union of the acting entity and all\n+      other entities that control, are controlled by, or are under common\n+      control with that entity. For the purposes of this definition,\n+      \"control\" means (i) the power, direct or indirect, to cause the\n+      direction or management of such entity, whether by contract or\n+      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n+      outstanding shares, or (iii) beneficial ownership of such entity.\n+\n+      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n+      exercising permissions granted by this License.\n+\n+      \"Source\" form shall mean the preferred form for making modifications,\n+      including but not limited to software source code, documentation\n+      source, and configuration files.\n+\n+      \"Object\" form shall mean any form resulting from mechanical\n+      transformation or translation of a Source form, including but\n+      not limited to compiled object code, generated documentation,\n+      and conversions to other media types.\n+\n+      \"Work\" shall mean the work of authorship, whether in Source or\n+      Object form, made available under the License, as indicated by a\n+      copyright notice that is included in or attached to the work\n+      (an example is provided in the Appendix below).\n+\n+      \"Derivative Works\" shall mean any work, whether in Source or Object\n+      form, that is based on (or derived from) the Work and for which the\n+      editorial revisions, annotations, elaborations, or other modifications\n+      represent, as a whole, an original work of authorship. For the purposes\n+      of this License, Derivative Works shall not include works that remain\n+      separable from, or merely link (or bind by name) to the interfaces of,\n+      the Work and Derivative Works thereof.\n+\n+      \"Contribution\" shall mean any work of authorship, including\n+      the original version of the Work and any modifications or additions\n+      to that Work or Derivative Works thereof, that is intentionally\n+      submitted to Licensor for inclusion in the Work by the copyright owner\n+      or by an individual or Legal Entity authorized to submit on behalf of\n+      the copyright owner. For the purposes of this definition, \"submitted\"\n+      means any form of electronic, verbal, or written communication sent\n+      to the Licensor or its representatives, including but not limited to\n+      communication on electronic mailing lists, source code control systems,\n+      and issue tracking systems that are managed by, or on behalf of, the\n+      Licensor for the purpose of discussing and improving the Work, but\n+      excluding communication that is conspicuously marked or otherwise\n+      designated in writing by the copyright owner as \"Not a Contribution.\"\n+\n+      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n+      on behalf of whom a Contribution has been received by Licensor and\n+      subsequently incorporated within the Work.\n+\n+   2. Grant of Copyright License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      copyright license to reproduce, prepare Derivative Works of,\n+      publicly display, publicly perform, sublicense, and distribute the\n+      Work and such Derivative Works in Source or Object form.\n+\n+   3. Grant of Patent License. Subject to the terms and conditions of\n+      this License, each Contributor hereby grants to You a perpetual,\n+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n+      (except as stated in this section) patent license to make, have made,\n+      use, offer to sell, sell, import, and otherwise transfer the Work,\n+      where such license applies only to those patent claims licensable\n+      by such Contributor that are necessarily infringed by their\n+      Contribution(s) alone or by combination of their Contribution(s)\n+      with the Work to which such Contribution(s) was submitted. If You\n+      institute patent litigation against any entity (including a\n+      cross-claim or counterclaim in a lawsuit) alleging that the Work\n+      or a Contribution incorporated within the Work constitutes direct\n+      or contributory patent infringement, then any patent licenses\n+      granted to You under this License for that Work shall terminate\n+      as of the date such litigation is filed.\n+\n+   4. Redistribution. You may reproduce and distribute copies of the\n+      Work or Derivative Works thereof in any medium, with or without\n+      modifications, and in Source or Object form, provided that You\n+      meet the following conditions:\n+\n+      (a) You must give any other recipients of the Work or\n+          Derivative Works a copy of this License; and\n+\n+      (b) You must cause any modified files to carry prominent notices\n+          stating that You changed the files; and\n+\n+      (c) You must retain, in the Source form of any Derivative Works\n+          that You distribute, all copyright, patent, trademark, and\n+          attribution notices from the Source form of the Work,\n+          excluding those notices that do not pertain to any part of\n+          the Derivative Works; and\n+\n+      (d) If the Work includes a \"NOTICE\" text file as part of its\n+          distribution, then any Derivative Works that You distribute must\n+          include a readable copy of the attribution notices contained\n+          within such NOTICE file, excluding those notices that do not\n+          pertain to any part of the Derivative Works, in at least one\n+          of the following places: within a NOTICE text file distributed\n+          as part of the Derivative Works; within the Source form or\n+          documentation, if provided along with the Derivative Works; or,\n+          within a display generated by the Derivative Works, if and\n+          wherever such third-party notices normally appear. The contents\n+          of the NOTICE file are for informational purposes only and\n+          do not modify the License. You may add Your own attribution\n+          notices within Derivative Works that You distribute, alongside\n+          or as an addendum to the NOTICE text from the Work, provided\n+          that such additional attribution notices cannot be construed\n+          as modifying the License.\n+\n+      You may add Your own copyright statement to Your modifications and\n+      may provide additional or different license terms and conditions\n+      for use, reproduction, or distribution of Your modifications, or\n+      for any such Derivative Works as a whole, provided Your use,\n+      reproduction, and distribution of the Work otherwise complies with\n+      the conditions stated in this License.\n+\n+   5. Submission of Contributions. Unless You explicitly state otherwise,\n+      any Contribution intentionally submitted for inclusion in the Work\n+      by You to the Licensor shall be under the terms and conditions of\n+      this License, without any additional terms or conditions.\n+      Notwithstanding the above, nothing herein shall supersede or modify\n+      the terms of any separate license agreement you may have executed\n+      with Licensor regarding such Contributions.\n+\n+   6. Trademarks. This License does not grant permission to use the trade\n+      names, trademarks, service marks, or product names of the Licensor,\n+      except as required for reasonable and customary use in describing the\n+      origin of the Work and reproducing the content of the NOTICE file.\n+\n+   7. Disclaimer of Warranty. Unless required by applicable law or\n+      agreed to in writing, Licensor provides the Work (and each\n+      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+      implied, including, without limitation, any warranties or conditions\n+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n+      PARTICULAR PURPOSE. You are solely responsible for determining the\n+      appropriateness of using or redistributing the Work and assume any\n+      risks associated with Your exercise of permissions under this License.\n+\n+   8. Limitation of Liability. In no event and under no legal theory,\n+      whether in tort (including negligence), contract, or otherwise,\n+      unless required by applicable law (such as deliberate and grossly\n+      negligent acts) or agreed to in writing, shall any Contributor be\n+      liable to You for damages, including any direct, indirect, special,\n+      incidental, or consequential damages of any character arising as a\n+      result of this License or out of the use or inability to use the\n+      Work (including but not limited to damages for loss of goodwill,\n+      work stoppage, computer failure or malfunction, or any and all\n+      other commercial damages or losses), even if such Contributor\n+      has been advised of the possibility of such damages.\n+\n+   9. Accepting Warranty or Additional Liability. While redistributing\n+      the Work or Derivative Works thereof, You may choose to offer,\n+      and charge a fee for, acceptance of support, warranty, indemnity,\n+      or other liability obligations and/or rights consistent with this\n+      License. However, in accepting such obligations, You may act only\n+      on Your own behalf and on Your sole responsibility, not on behalf\n+      of any other Contributor, and only if You agree to indemnify,\n+      defend, and hold each Contributor harmless for any liability\n+      incurred by, or claims asserted against, such Contributor by reason\n+      of your accepting any such warranty or additional liability.\n+\n+   END OF TERMS AND CONDITIONS\n+\n+   APPENDIX: How to apply the Apache License to your work.\n+\n+      To apply the Apache License to your work, attach the following\n+      boilerplate notice, with the fields enclosed by brackets \"[]\"\n+      replaced with your own identifying information. (Don't include\n+      the brackets!)  The text should be enclosed in the appropriate\n+      comment syntax for the file format. We also recommend that a\n+      file or class name and description of purpose be included on the\n+      same \"printed page\" as the copyright notice for easier\n+      identification within third-party archives.\n+\n+   Copyright [yyyy] [name of copyright owner]\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\ndiff --git a/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-elixir-tags.scm b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-elixir-tags.scm\nnew file mode 100644\nindex 000000000000..9eb39d95d85b\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-elixir-tags.scm\n@@ -0,0 +1,54 @@\n+; Definitions\n+\n+; * modules and protocols\n+(call\n+  target: (identifier) @ignore\n+  (arguments (alias) @name.definition.module)\n+  (#match? @ignore \"^(defmodule|defprotocol)$\")) @definition.module\n+\n+; * functions/macros\n+(call\n+  target: (identifier) @ignore\n+  (arguments\n+    [\n+      ; zero-arity functions with no parentheses\n+      (identifier) @name.definition.function\n+      ; regular function clause\n+      (call target: (identifier) @name.definition.function)\n+      ; function clause with a guard clause\n+      (binary_operator\n+        left: (call target: (identifier) @name.definition.function)\n+        operator: \"when\")\n+    ])\n+  (#match? @ignore \"^(def|defp|defdelegate|defguard|defguardp|defmacro|defmacrop|defn|defnp)$\")) @definition.function\n+\n+; References\n+\n+; ignore calls to kernel/special-forms keywords\n+(call\n+  target: (identifier) @ignore\n+  (#match? @ignore \"^(def|defp|defdelegate|defguard|defguardp|defmacro|defmacrop|defn|defnp|defmodule|defprotocol|defimpl|defstruct|defexception|defoverridable|alias|case|cond|else|for|if|import|quote|raise|receive|require|reraise|super|throw|try|unless|unquote|unquote_splicing|use|with)$\"))\n+\n+; ignore module attributes\n+(unary_operator\n+  operator: \"@\"\n+  operand: (call\n+    target: (identifier) @ignore))\n+\n+; * function call\n+(call\n+  target: [\n+   ; local\n+   (identifier) @name.reference.call\n+   ; remote\n+   (dot\n+     right: (identifier) @name.reference.call)\n+  ]) @reference.call\n+\n+; * pipe into function call\n+(binary_operator\n+  operator: \"|>\"\n+  right: (identifier) @name.reference.call) @reference.call\n+\n+; * modules\n+(alias) @name.reference.module @reference.module\ndiff --git a/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-javascript-tags.scm b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-javascript-tags.scm\nnew file mode 100644\nindex 000000000000..3bc55c5c58cd\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-javascript-tags.scm\n@@ -0,0 +1,88 @@\n+(\n+  (comment)* @doc\n+  .\n+  (method_definition\n+    name: (property_identifier) @name.definition.method) @definition.method\n+  (#not-eq? @name.definition.method \"constructor\")\n+  (#strip! @doc \"^[\\\\s\\\\*/]+|^[\\\\s\\\\*/]$\")\n+  (#select-adjacent! @doc @definition.method)\n+)\n+\n+(\n+  (comment)* @doc\n+  .\n+  [\n+    (class\n+      name: (_) @name.definition.class)\n+    (class_declaration\n+      name: (_) @name.definition.class)\n+  ] @definition.class\n+  (#strip! @doc \"^[\\\\s\\\\*/]+|^[\\\\s\\\\*/]$\")\n+  (#select-adjacent! @doc @definition.class)\n+)\n+\n+(\n+  (comment)* @doc\n+  .\n+  [\n+    (function\n+      name: (identifier) @name.definition.function)\n+    (function_declaration\n+      name: (identifier) @name.definition.function)\n+    (generator_function\n+      name: (identifier) @name.definition.function)\n+    (generator_function_declaration\n+      name: (identifier) @name.definition.function)\n+  ] @definition.function\n+  (#strip! @doc \"^[\\\\s\\\\*/]+|^[\\\\s\\\\*/]$\")\n+  (#select-adjacent! @doc @definition.function)\n+)\n+\n+(\n+  (comment)* @doc\n+  .\n+  (lexical_declaration\n+    (variable_declarator\n+      name: (identifier) @name.definition.function\n+      value: [(arrow_function) (function)]) @definition.function)\n+  (#strip! @doc \"^[\\\\s\\\\*/]+|^[\\\\s\\\\*/]$\")\n+  (#select-adjacent! @doc @definition.function)\n+)\n+\n+(\n+  (comment)* @doc\n+  .\n+  (variable_declaration\n+    (variable_declarator\n+      name: (identifier) @name.definition.function\n+      value: [(arrow_function) (function)]) @definition.function)\n+  (#strip! @doc \"^[\\\\s\\\\*/]+|^[\\\\s\\\\*/]$\")\n+  (#select-adjacent! @doc @definition.function)\n+)\n+\n+(assignment_expression\n+  left: [\n+    (identifier) @name.definition.function\n+    (member_expression\n+      property: (property_identifier) @name.definition.function)\n+  ]\n+  right: [(arrow_function) (function)]\n+) @definition.function\n+\n+(pair\n+  key: (property_identifier) @name.definition.function\n+  value: [(arrow_function) (function)]) @definition.function\n+\n+(\n+  (call_expression\n+    function: (identifier) @name.reference.call) @reference.call\n+  (#not-match? @name.reference.call \"^(require)$\")\n+)\n+\n+(call_expression\n+  function: (member_expression\n+    property: (property_identifier) @name.reference.call)\n+  arguments: (_) @reference.call)\n+\n+(new_expression\n+  constructor: (_) @name.reference.class) @reference.class\ndiff --git a/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-ocaml-tags.scm b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-ocaml-tags.scm\nnew file mode 100644\nindex 000000000000..1799f8acf2d2\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-ocaml-tags.scm\n@@ -0,0 +1,116 @@\n+; Modules\n+;--------\n+\n+(\n+  (comment)? @doc .\n+  (module_definition (module_binding (module_name) @name.definition.module) @definition.module)\n+  (#strip! @doc \"^\\\\(\\\\*\\\\*?\\\\s*|\\\\s\\\\*\\\\)$\")\n+)\n+\n+(module_path (module_name) @name.reference.module) @reference.module\n+\n+; Modules types\n+;--------------\n+\n+(\n+  (comment)? @doc .\n+  (module_type_definition (module_type_name) @name.definition.interface) @definition.interface\n+  (#strip! @doc \"^\\\\(\\\\*\\\\*?\\\\s*|\\\\s\\\\*\\\\)$\")\n+)\n+\n+(module_type_path (module_type_name) @name.reference.implementation) @reference.implementation\n+\n+; Functions\n+;----------\n+\n+(\n+  (comment)? @doc .\n+  (value_definition\n+    [\n+      (let_binding\n+        pattern: (value_name) @name.definition.function\n+        (parameter))\n+      (let_binding\n+        pattern: (value_name) @name.definition.function\n+        body: [(fun_expression) (function_expression)])\n+    ] @definition.function\n+  )\n+  (#strip! @doc \"^\\\\(\\\\*\\\\*?\\\\s*|\\\\s\\\\*\\\\)$\")\n+)\n+\n+(\n+  (comment)? @doc .\n+  (external (value_name) @name.definition.function) @definition.function\n+  (#strip! @doc \"^\\\\(\\\\*\\\\*?\\\\s*|\\\\s\\\\*\\\\)$\")\n+)\n+\n+(application_expression\n+  function: (value_path (value_name) @name.reference.call)) @reference.call\n+\n+(infix_expression\n+  left: (value_path (value_name) @name.reference.call)\n+  (infix_operator) @reference.call\n+  (#eq? @reference.call \"@@\"))\n+\n+(infix_expression\n+  (infix_operator) @reference.call\n+  right: (value_path (value_name) @name.reference.call)\n+  (#eq? @reference.call \"|>\"))\n+\n+; Operator\n+;---------\n+\n+(\n+  (comment)? @doc .\n+  (value_definition\n+    (let_binding\n+      pattern: (parenthesized_operator [\n+        (prefix_operator)\n+        (infix_operator)\n+        (hash_operator)\n+        (indexing_operator)\n+        (let_operator)\n+        (and_operator)\n+        (match_operator)\n+      ] @name.definition.function)) @definition.function)\n+  (#strip! @doc \"^\\\\(\\\\*\\\\*?\\\\s*|\\\\s\\\\*\\\\)$\")\n+)\n+\n+[\n+  (prefix_operator)\n+  (sign_operator)\n+  (infix_operator)\n+  (hash_operator)\n+  (indexing_operator)\n+  (let_operator)\n+  (and_operator)\n+  (match_operator)\n+] @name.reference.call @reference.call\n+\n+; Classes\n+;--------\n+\n+(\n+  (comment)? @doc .\n+  [\n+    (class_definition (class_binding (class_name) @name.definition.class) @definition.class)\n+    (class_type_definition (class_type_binding (class_type_name) @name.definition.class) @definition.class)\n+  ]\n+  (#strip! @doc \"^\\\\(\\\\*\\\\*?\\\\s*|\\\\s\\\\*\\\\)$\")\n+)\n+\n+[\n+  (class_path (class_name) @name.reference.class)\n+  (class_type_path (class_type_name) @name.reference.class)\n+] @reference.class\n+\n+; Methods\n+;--------\n+\n+(\n+  (comment)? @doc .\n+  (method_definition (method_name) @name.definition.method) @definition.method\n+  (#strip! @doc \"^\\\\(\\\\*\\\\*?\\\\s*|\\\\s\\\\*\\\\)$\")\n+)\n+\n+(method_invocation (method_name) @name.reference.call) @reference.call\ndiff --git a/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-ruby-tags.scm b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-ruby-tags.scm\nnew file mode 100644\nindex 000000000000..79e71d2d646e\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-ruby-tags.scm\n@@ -0,0 +1,64 @@\n+; Method definitions\n+\n+(\n+  (comment)* @doc\n+  .\n+  [\n+    (method\n+      name: (_) @name.definition.method) @definition.method\n+    (singleton_method\n+      name: (_) @name.definition.method) @definition.method\n+  ]\n+  (#strip! @doc \"^#\\\\s*\")\n+  (#select-adjacent! @doc @definition.method)\n+)\n+\n+(alias\n+  name: (_) @name.definition.method) @definition.method\n+\n+(setter\n+  (identifier) @ignore)\n+\n+; Class definitions\n+\n+(\n+  (comment)* @doc\n+  .\n+  [\n+    (class\n+      name: [\n+        (constant) @name.definition.class\n+        (scope_resolution\n+          name: (_) @name.definition.class)\n+      ]) @definition.class\n+    (singleton_class\n+      value: [\n+        (constant) @name.definition.class\n+        (scope_resolution\n+          name: (_) @name.definition.class)\n+      ]) @definition.class\n+  ]\n+  (#strip! @doc \"^#\\\\s*\")\n+  (#select-adjacent! @doc @definition.class)\n+)\n+\n+; Module definitions\n+\n+(\n+  (module\n+    name: [\n+      (constant) @name.definition.module\n+      (scope_resolution\n+        name: (_) @name.definition.module)\n+    ]) @definition.module\n+)\n+\n+; Calls\n+\n+(call method: (identifier) @name.reference.call) @reference.call\n+\n+(\n+  [(identifier) (constant)] @name.reference.call @reference.call\n+  (#is-not? local)\n+  (#not-match? @name.reference.call \"^(lambda|load|require|require_relative|__FILE__|__LINE__)$\")\n+)\ndiff --git a/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-rust-tags.scm b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-rust-tags.scm\nnew file mode 100644\nindex 000000000000..dadfa7acb6f0\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/repomap/queries/tree-sitter-rust-tags.scm\n@@ -0,0 +1,60 @@\n+; ADT definitions\n+\n+(struct_item\n+    name: (type_identifier) @name.definition.class) @definition.class\n+\n+(enum_item\n+    name: (type_identifier) @name.definition.class) @definition.class\n+\n+(union_item\n+    name: (type_identifier) @name.definition.class) @definition.class\n+\n+; type aliases\n+\n+(type_item\n+    name: (type_identifier) @name.definition.class) @definition.class\n+\n+; method definitions\n+\n+(declaration_list\n+    (function_item\n+        name: (identifier) @name.definition.method)) @definition.method\n+\n+; function definitions\n+\n+(function_item\n+    name: (identifier) @name.definition.function) @definition.function\n+\n+; trait definitions\n+(trait_item\n+    name: (type_identifier) @name.definition.interface) @definition.interface\n+\n+; module definitions\n+(mod_item\n+    name: (identifier) @name.definition.module) @definition.module\n+\n+; macro definitions\n+\n+(macro_definition\n+    name: (identifier) @name.definition.macro) @definition.macro\n+\n+; references\n+\n+(call_expression\n+    function: (identifier) @name.reference.call) @reference.call\n+\n+(call_expression\n+    function: (field_expression\n+        field: (field_identifier) @name.reference.call)) @reference.call\n+\n+(macro_invocation\n+    macro: (identifier) @name.reference.call) @reference.call\n+\n+; implementations\n+\n+(impl_item\n+    trait: (type_identifier) @name.reference.implementation) @reference.implementation\n+\n+(impl_item\n+    type: (type_identifier) @name.reference.implementation\n+    !trait) @reference.implementation\ndiff --git a/opendevin/runtime/plugins/agent_skills/repomap/repomap.py b/opendevin/runtime/plugins/agent_skills/repomap/repomap.py\nnew file mode 100644\nindex 000000000000..38a596bea28f\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/repomap/repomap.py\n@@ -0,0 +1,841 @@\n+\"\"\"\n+Modifications made on June, 2024.\n+Description of changes: removed redundant methods, attributes, and adapted some classes to be OpenDevin-specific.\n+\"\"\"\n+\n+import base64\n+import os\n+import re\n+import warnings\n+from collections import Counter, defaultdict, namedtuple\n+from importlib import resources\n+from pathlib import Path, PurePosixPath\n+from typing import Any, Set\n+\n+import networkx as nx\n+import pathspec\n+from diskcache import Cache\n+from grep_ast import TreeContext, filename_to_lang\n+from pygments.lexers import guess_lexer_for_filename\n+from pygments.token import Token\n+from pygments.util import ClassNotFound\n+from tqdm import tqdm\n+from tree_sitter_languages import get_language, get_parser\n+\n+if __package__ is None or __package__ == '':\n+    from utils import get_token_count_from_text, is_image_file\n+else:\n+    from .utils import get_token_count_from_text, is_image_file\n+\n+# tree_sitter is throwing a FutureWarning\n+warnings.simplefilter('ignore', category=FutureWarning)\n+\n+Tag = namedtuple('Tag', ('rel_fname', 'fname', 'line', 'name', 'kind'))\n+\n+\n+class RepoMap:\n+    \"\"\"\n+    The RepoMap class represents a mapping of a repository's files and their associated tags.\n+    \"\"\"\n+\n+    CACHE_VERSION = 3\n+    TAGS_CACHE_DIR = f'.aider.tags.cache.v{CACHE_VERSION}'\n+\n+    cache_missing = False\n+    aider_ignore_ts = 0\n+\n+    warned_files: Set[str] = set()\n+\n+    def __init__(\n+        self,\n+        model_name,\n+        map_tokens=1024,\n+        root=None,\n+        repo_content_prefix=None,\n+        max_context_window=None,\n+        aider_ignore_file='.aiderignore',\n+    ):\n+        \"\"\"\n+        Initialize the Repomap object.\n+\n+        Args:\n+            llm (LLM): The LLM object used for token counting.\n+            io (InputOutput): The InputOutput object used for file I/O operations.\n+            map_tokens (int, optional): The maximum number of tokens to map. Defaults to 1024.\n+            root (str, optional): The root directory path. If not provided, the current working directory is used. Defaults to None.\n+            repo_content_prefix (str, optional): The prefix to add to the repository content paths. Defaults to None.\n+            verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\n+            max_context_window (int, optional): The maximum context window size. Defaults to None.\n+        \"\"\"\n+        if not root:\n+            root = os.getcwd()\n+        self.root = root\n+\n+        self.load_tags_cache()\n+        self.model_name = model_name\n+        self.max_map_tokens = map_tokens\n+        self.max_context_window = max_context_window\n+\n+        self.token_count = get_token_count_from_text\n+        self.repo_content_prefix = repo_content_prefix\n+        self.abs_fnames: Any = set()\n+        if aider_ignore_file:\n+            self.aider_ignore_file = Path(aider_ignore_file)\n+\n+    def get_repo_map(\n+        self, chat_files, other_files, mentioned_fnames=None, mentioned_idents=None\n+    ):\n+        \"\"\"\n+        Generate a repository map based on the provided files and parameters.\n+\n+        Args:\n+            chat_files (list): List of files in the chat.\n+            other_files (list): List of other files in the repository.\n+            mentioned_fnames (set, optional): Set of mentioned filenames. Defaults to None.\n+            mentioned_idents (set, optional): Set of mentioned identifiers. Defaults to None.\n+\n+        Returns:\n+            str: The generated repository map as a string.\n+        \"\"\"\n+\n+        if self.max_map_tokens <= 0:\n+            return\n+        if not other_files:\n+            return\n+        if not mentioned_fnames:\n+            mentioned_fnames = set()\n+        if not mentioned_idents:\n+            mentioned_idents = set()\n+\n+        max_map_tokens = self.max_map_tokens\n+\n+        # With no files in the chat, give a bigger view of the entire repo\n+        MUL = 16\n+        padding = 4096\n+        if max_map_tokens and self.max_context_window:\n+            target = min(max_map_tokens * MUL, self.max_context_window - padding)\n+        else:\n+            target = 0\n+        if not chat_files and self.max_context_window and target > 0:\n+            max_map_tokens = target\n+\n+        try:\n+            files_listing = self.get_ranked_tags_map(\n+                chat_files,\n+                other_files,\n+                max_map_tokens,\n+                mentioned_fnames,\n+                mentioned_idents,\n+            )\n+        except RecursionError:\n+            self.log_error('Disabling repo map, git repo too large?')\n+            self.max_map_tokens = 0\n+            return\n+\n+        if not files_listing:\n+            return\n+\n+        # num_tokens = self.token_count(self.model_name, files_listing)\n+        # logger.info(f'Repo-map: {num_tokens/1024:.1f} k-tokens')\n+\n+        if chat_files:\n+            other = 'other '\n+        else:\n+            other = ''\n+\n+        if self.repo_content_prefix:\n+            repo_content = self.repo_content_prefix.format(other=other)\n+        else:\n+            repo_content = ''\n+\n+        repo_content += files_listing\n+\n+        return repo_content\n+\n+    def get_rel_fname(self, fname):\n+        \"\"\"\n+        Returns the relative path of a file with respect to the root directory.\n+\n+        Args:\n+            fname (str): The absolute path of the file.\n+\n+        Returns:\n+            str: The relative path of the file.\n+        \"\"\"\n+        return os.path.relpath(fname, self.root)\n+\n+    def load_tags_cache(self):\n+        \"\"\"\n+        Loads the tags cache from the specified path.\n+\n+        If the cache directory does not exist, sets the `cache_missing` flag to True.\n+        \"\"\"\n+        path = Path(self.root) / self.TAGS_CACHE_DIR\n+        if not path.exists():\n+            self.cache_missing = True\n+        self.TAGS_CACHE = Cache(path)\n+\n+    def save_tags_cache(self):\n+        pass\n+\n+    def get_mtime(self, fname):\n+        \"\"\"\n+        Get the modification time of a file.\n+\n+        Args:\n+            fname (str): The path to the file.\n+\n+        Returns:\n+            float: The modification time of the file in seconds since the epoch.\n+\n+        Raises:\n+            FileNotFoundError: If the file is not found.\n+        \"\"\"\n+        try:\n+            return os.path.getmtime(fname)\n+        except FileNotFoundError:\n+            self.log_error(f'File not found error: {fname}')\n+\n+    def get_tags(self, fname, rel_fname):\n+        \"\"\"\n+        Retrieves the tags associated with a file.\n+\n+        Args:\n+            fname (str): The absolute path of the file.\n+            rel_fname (str): The relative path of the file.\n+\n+        Returns:\n+            list: A list of tags associated with the file.\n+        \"\"\"\n+        # Check if the file is in the cache and if the modification time has not changed\n+        file_mtime = self.get_mtime(fname)\n+        if file_mtime is None:\n+            return []\n+\n+        cache_key = fname\n+        if (\n+            cache_key in self.TAGS_CACHE\n+            and self.TAGS_CACHE[cache_key]['mtime'] == file_mtime\n+        ):\n+            return self.TAGS_CACHE[cache_key]['data']\n+\n+        # miss!\n+\n+        data = list(self.get_tags_raw(fname, rel_fname))\n+\n+        # Update the cache\n+        self.TAGS_CACHE[cache_key] = {'mtime': file_mtime, 'data': data}\n+        self.save_tags_cache()\n+        return data\n+\n+    def get_tags_raw(self, fname, rel_fname):\n+        \"\"\"\n+        Retrieves tags from the given file.\n+\n+        Args:\n+            fname (str): The absolute path of the file.\n+            rel_fname (str): The relative path of the file.\n+\n+        Yields:\n+            Tag: A Tag object representing a tag found in the file.\n+\n+        Returns:\n+            None: If the language of the file is not supported or if the file is empty.\n+        \"\"\"\n+        lang = filename_to_lang(fname)\n+        if not lang:\n+            return\n+\n+        language = get_language(lang)\n+        parser = get_parser(lang)\n+\n+        # Load the tags queries\n+        try:\n+            scm_fname = (\n+                resources.files(__package__)\n+                .joinpath('queries')\n+                .joinpath(f'tree-sitter-{lang}-tags.scm')\n+            )\n+        except KeyError:\n+            return\n+        query_scm = str(scm_fname)\n+        if not Path(query_scm).exists():\n+            return\n+        query_scm = scm_fname.read_text()\n+\n+        code = self.read_text(fname)\n+        if not code:\n+            return\n+        tree = parser.parse(bytes(code, 'utf-8'))\n+\n+        # Run the tags queries\n+        query = language.query(query_scm)\n+        captures = query.captures(tree.root_node)\n+\n+        captures = list(captures)\n+\n+        saw = set()\n+        for node, tag in captures:\n+            if tag.startswith('name.definition.'):\n+                kind = 'def'\n+            elif tag.startswith('name.reference.'):\n+                kind = 'ref'\n+            else:\n+                continue\n+\n+            saw.add(kind)\n+\n+            result = Tag(\n+                rel_fname=rel_fname,\n+                fname=fname,\n+                name=node.text.decode('utf-8'),\n+                kind=kind,\n+                line=node.start_point[0],\n+            )\n+\n+            yield result\n+\n+        if 'ref' in saw:\n+            return\n+        if 'def' not in saw:\n+            return\n+\n+        # We saw defs, without any refs\n+        # Some tags files only provide defs (cpp, for example)\n+        # Use pygments to backfill refs\n+\n+        try:\n+            lexer = guess_lexer_for_filename(fname, code)\n+        except ClassNotFound:\n+            return\n+\n+        tokens = list(lexer.get_tokens(code))\n+        tokens = [token[1] for token in tokens if token[0] in Token.Name]\n+\n+        for token in tokens:\n+            yield Tag(\n+                rel_fname=rel_fname,\n+                fname=fname,\n+                name=token,\n+                kind='ref',\n+                line=-1,\n+            )\n+\n+    def get_ranked_tags(\n+        self, chat_fnames, other_fnames, mentioned_fnames, mentioned_idents\n+    ):\n+        \"\"\"\n+        Returns a list of ranked tags based on the provided file names and mentioned identifiers.\n+\n+        Args:\n+            chat_fnames (list): List of file names from the chat.\n+            other_fnames (list): List of other file names.\n+            mentioned_fnames (list): List of mentioned file names.\n+            mentioned_idents (list): List of mentioned identifiers.\n+\n+        Returns:\n+            list: A list of ranked tags.\n+        \"\"\"\n+        defines = defaultdict(set)\n+        references = defaultdict(list)\n+        definitions = defaultdict(set)\n+\n+        personalization = dict()\n+\n+        fnames_set = set(chat_fnames).union(set(other_fnames))\n+        chat_rel_fnames = set()\n+\n+        fnames = sorted(fnames_set)\n+\n+        # Default personalization for unspecified files is 1/num_nodes\n+        # https://networkx.org/documentation/stable/_modules/networkx/algorithms/link_analysis/pagerank_alg.html#pagerank\n+        personalize = 10 / len(fnames)\n+\n+        if self.cache_missing:\n+            fnames = tqdm(fnames)\n+        self.cache_missing = False\n+\n+        for fname in fnames:\n+            if not Path(fname).is_file():\n+                if fname not in self.warned_files:\n+                    if Path(fname).exists():\n+                        self.log_error(\n+                            f\"Repo-map can't include {fname}, it is not a normal file\"\n+                        )\n+                    else:\n+                        self.log_error(\n+                            f\"Repo-map can't include {fname}, it no longer exists\"\n+                        )\n+\n+                self.warned_files.add(fname)\n+                continue\n+\n+            # dump(fname)\n+            rel_fname = self.get_rel_fname(fname)\n+\n+            if fname in chat_fnames:\n+                personalization[rel_fname] = personalize\n+                chat_rel_fnames.add(rel_fname)\n+\n+            if fname in mentioned_fnames:\n+                personalization[rel_fname] = personalize\n+\n+            tags = list(self.get_tags(fname, rel_fname))\n+\n+            for tag in tags:\n+                if tag.kind == 'def':\n+                    defines[tag.name].add(rel_fname)\n+                    key = (rel_fname, tag.name)\n+                    definitions[key].add(tag)\n+\n+                if tag.kind == 'ref':\n+                    references[tag.name].append(rel_fname)\n+\n+        ##\n+        # dump(defines)\n+        # dump(references)\n+        # dump(personalization)\n+\n+        if not references:\n+            references = defaultdict(list)\n+            for k, v in defines.items():\n+                references[k] = list(v)\n+\n+        idents = set(defines.keys()).intersection(set(references.keys()))\n+\n+        G = nx.MultiDiGraph()\n+\n+        for ident in idents:\n+            definers = defines[ident]\n+            if ident in mentioned_idents:\n+                mul = 10\n+            else:\n+                mul = 1\n+            for referencer, num_refs in Counter(references[ident]).items():\n+                for definer in definers:\n+                    # if referencer == definer:\n+                    #    continue\n+                    G.add_edge(referencer, definer, weight=mul * num_refs, ident=ident)\n+\n+        if not references:\n+            pass\n+\n+        if personalization:\n+            pers_args = dict(personalization=personalization, dangling=personalization)\n+        else:\n+            pers_args = dict()\n+\n+        try:\n+            ranked = nx.pagerank(G, weight='weight', **pers_args)\n+        except ZeroDivisionError:\n+            return []\n+\n+        # distribute the rank from each source node, across all of its out edges\n+        ranked_definitions: Any = defaultdict(float)\n+        for src in G.nodes:\n+            src_rank = ranked[src]\n+            total_weight = sum(\n+                data['weight'] for _src, _dst, data in G.out_edges(src, data=True)\n+            )\n+            # dump(src, src_rank, total_weight)\n+            for _src, dst, data in G.out_edges(src, data=True):\n+                data['rank'] = src_rank * data['weight'] / total_weight\n+                ident = data['ident']\n+                ranked_definitions[(dst, ident)] += data['rank']\n+\n+        ranked_tags = []\n+        ranked_definitions = sorted(\n+            ranked_definitions.items(), reverse=True, key=lambda x: x[1]\n+        )\n+\n+        # dump(ranked_definitions)\n+\n+        for (fname, ident), rank in ranked_definitions:\n+            # print(f\"{rank:.03f} {fname} {ident}\")\n+            if fname in chat_rel_fnames:\n+                continue\n+            ranked_tags += list(definitions.get((fname, ident), []))\n+\n+        rel_other_fnames_without_tags = set(\n+            self.get_rel_fname(fname) for fname in other_fnames\n+        )\n+\n+        fnames_already_included = set(rt[0] for rt in ranked_tags)\n+\n+        top_rank = sorted(\n+            [(rank, node) for (node, rank) in ranked.items()], reverse=True\n+        )\n+        for rank, fname in top_rank:\n+            if fname in rel_other_fnames_without_tags:\n+                rel_other_fnames_without_tags.remove(fname)\n+            if fname not in fnames_already_included:\n+                ranked_tags.append((fname,))\n+\n+        for fname in rel_other_fnames_without_tags:\n+            ranked_tags.append((fname,))\n+\n+        return ranked_tags\n+\n+    def get_ranked_tags_map(\n+        self,\n+        chat_fnames,\n+        other_fnames=None,\n+        max_map_tokens=None,\n+        mentioned_fnames=None,\n+        mentioned_idents=None,\n+    ):\n+        \"\"\"\n+        Returns the best tree representation of ranked tags based on the given parameters.\n+\n+        Args:\n+            chat_fnames (list): A list of chat filenames.\n+            other_fnames (list, optional): A list of other filenames. Defaults to None.\n+            max_map_tokens (int, optional): The maximum number of tokens allowed in the map. Defaults to None.\n+            mentioned_fnames (set, optional): A set of mentioned filenames. Defaults to None.\n+            mentioned_idents (set, optional): A set of mentioned identifiers. Defaults to None.\n+\n+        Returns:\n+            The best tree representation of ranked tags.\n+        \"\"\"\n+        if not other_fnames:\n+            other_fnames = list()\n+        if not max_map_tokens:\n+            max_map_tokens = self.max_map_tokens\n+        if not mentioned_fnames:\n+            mentioned_fnames = set()\n+        if not mentioned_idents:\n+            mentioned_idents = set()\n+\n+        ranked_tags = self.get_ranked_tags(\n+            chat_fnames, other_fnames, mentioned_fnames, mentioned_idents\n+        )\n+\n+        num_tags = len(ranked_tags)\n+        lower_bound = 0\n+        upper_bound = num_tags\n+        best_tree = None\n+        best_tree_tokens = 0\n+\n+        chat_rel_fnames = {self.get_rel_fname(fname) for fname in chat_fnames}\n+\n+        # Guess a small starting number to help with giant repos\n+        middle = min(max_map_tokens // 25, num_tags)\n+\n+        self.tree_cache = dict()\n+\n+        while lower_bound <= upper_bound:\n+            tree = self.to_tree(ranked_tags[:middle], chat_rel_fnames)\n+            num_tokens = self.token_count(self.model_name, tree)\n+\n+            if num_tokens < max_map_tokens and num_tokens > best_tree_tokens:\n+                best_tree = tree\n+                best_tree_tokens = num_tokens\n+\n+            if num_tokens < max_map_tokens:\n+                lower_bound = middle + 1\n+            else:\n+                upper_bound = middle - 1\n+\n+            middle = (lower_bound + upper_bound) // 2\n+\n+        return best_tree\n+\n+    tree_cache: Any = dict()\n+\n+    def render_tree(self, abs_fname, rel_fname, lois):\n+        \"\"\"\n+        Renders the tree for a given file and lines of interest (lois).\n+\n+        Args:\n+            abs_fname (str): The absolute file path.\n+            rel_fname (str): The relative file path.\n+            lois (list): A list of lines of interest.\n+\n+        Returns:\n+            str: The rendered tree.\n+        \"\"\"\n+        key = (rel_fname, tuple(sorted(lois)))\n+\n+        if key in self.tree_cache:\n+            return self.tree_cache[key]\n+\n+        code = self.read_text(abs_fname) or ''\n+        if not code.endswith('\\n'):\n+            code += '\\n'\n+\n+        context = TreeContext(\n+            rel_fname,\n+            code,\n+            color=False,\n+            line_number=False,\n+            child_context=False,\n+            last_line=False,\n+            margin=0,\n+            mark_lois=False,\n+            loi_pad=0,\n+            # header_max=30,\n+            show_top_of_file_parent_scope=False,\n+        )\n+\n+        context.add_lines_of_interest(lois)\n+        context.add_context()\n+        res = context.format()\n+        self.tree_cache[key] = res\n+        return res\n+\n+    def to_tree(self, tags, chat_rel_fnames):\n+        \"\"\"\n+        Converts a list of tags into a tree-like structure.\n+\n+        Args:\n+            tags (list): A list of tags.\n+            chat_rel_fnames (list): A list of chat relative filenames.\n+\n+        Returns:\n+            str: The tree-like structure as a string.\n+        \"\"\"\n+        if not tags:\n+            return ''\n+\n+        tags = [tag for tag in tags if tag[0] not in chat_rel_fnames]\n+        tags = sorted(tags)\n+\n+        cur_fname: Any = None\n+        cur_abs_fname = None\n+        lois: Any = None\n+        output = ''\n+\n+        # add a bogus tag at the end so we trip the this_fname != cur_fname...\n+        dummy_tag = (None,)\n+        for tag in tags + [dummy_tag]:\n+            this_rel_fname = tag[0]\n+\n+            # ... here ... to output the final real entry in the list\n+            if this_rel_fname != cur_fname:\n+                if lois is not None:\n+                    output += cur_fname + ':\\n'\n+                    output += self.render_tree(cur_abs_fname, cur_fname, lois)\n+                    lois = None\n+                elif cur_fname:\n+                    output += '\\n' + cur_fname + '\\n'\n+\n+                if type(tag) is Tag:\n+                    lois = []\n+                    cur_abs_fname = tag.fname\n+                cur_fname = this_rel_fname\n+\n+            if lois is not None:\n+                lois.append(tag.line)\n+\n+        # truncate long lines, in case we get minified js or something else crazy\n+        output = '\\n'.join([line[:100] for line in output.splitlines()]) + '\\n'\n+\n+        return output\n+\n+    def get_history_aware_repo_map(self, messages: list) -> str:\n+        cur_msg_text = self.get_cur_message_text(messages)\n+        mentioned_fnames = self.get_file_mentions(cur_msg_text)\n+        mentioned_idents = self.get_ident_mentions(cur_msg_text)\n+\n+        other_files = set(self.get_all_abs_files()) - set(self.abs_fnames)\n+        repo_content = self.get_repo_map(\n+            self.abs_fnames,\n+            other_files,\n+            mentioned_fnames=mentioned_fnames,\n+            mentioned_idents=mentioned_idents,\n+        )\n+\n+        # fall back to global repo map if files in chat are disjoint from rest of repo\n+        if not repo_content:\n+            repo_content = self.get_repo_map(\n+                set(),\n+                set(self.get_all_abs_files()),\n+                mentioned_fnames=mentioned_fnames,\n+                mentioned_idents=mentioned_idents,\n+            )\n+\n+        # fall back to completely unhinted repo\n+        if not repo_content:\n+            repo_content = self.get_repo_map(\n+                set(),\n+                set(self.get_all_abs_files()),\n+            )\n+\n+        return repo_content\n+\n+    def get_cur_message_text(self, messages):\n+        text = ''\n+\n+        for msg in messages:\n+            text += msg['content'] + '\\n'\n+        return text\n+\n+    def get_ident_mentions(self, text):\n+        # Split the string on any character that is not alphanumeric\n+        # \\W+ matches one or more non-word characters (equivalent to [^a-zA-Z0-9_]+)\n+        words = set(re.split(r'\\W+', text))\n+        return words\n+\n+    def get_file_mentions(self, content):\n+        words = set(word for word in content.split())\n+\n+        # drop sentence punctuation from the end\n+        words = set(word.rstrip(',.!;:') for word in words)\n+\n+        # strip away all kinds of quotes\n+        quotes = ''.join(['\"', \"'\", '`'])\n+        words = set(word.strip(quotes) for word in words)\n+\n+        addable_rel_fnames = self.get_addable_relative_files()\n+\n+        mentioned_rel_fnames = set()\n+        fname_to_rel_fnames: Any = {}\n+        for rel_fname in addable_rel_fnames:\n+            if rel_fname in words:\n+                mentioned_rel_fnames.add(str(rel_fname))\n+\n+            fname = os.path.basename(rel_fname)\n+\n+            # Don't add basenames that could be plain words like \"run\" or \"make\"\n+            if '/' in fname or '.' in fname or '_' in fname or '-' in fname:\n+                if fname not in fname_to_rel_fnames:\n+                    fname_to_rel_fnames[fname] = []\n+                fname_to_rel_fnames[fname].append(rel_fname)\n+\n+        for fname, rel_fnames in fname_to_rel_fnames.items():\n+            if len(rel_fnames) == 1 and fname in words:\n+                mentioned_rel_fnames.add(rel_fnames[0])\n+\n+        return mentioned_rel_fnames\n+\n+    def get_all_relative_files(self):\n+        import git\n+\n+        # Construct a git repo object and get all the relative files tracked by git and staged files\n+        try:\n+            repo = git.Repo(self.root)\n+\n+            if repo.bare:\n+                raise Exception('The repository is bare.')\n+\n+            # Get a list of all tracked files\n+            tracked_files: Any = [\n+                item.path for item in repo.tree().traverse() if item.type == 'blob'\n+            ]\n+        except git.InvalidGitRepositoryError:\n+            # logger.error(\n+            #     'The directory is not a git repository. RepoMap will not be enabled.'\n+            # )\n+            return []\n+        except git.NoSuchPathError:\n+            # logger.error('The directory does not exist. RepoMap will not be enabled.')\n+            return []\n+        except Exception:\n+            # logger.error(\n+            #     f'An error occurred when getting tracked files in git repo: {e}'\n+            # )\n+            return []\n+\n+        # Add staged files\n+        index = repo.index\n+        staged_files = [path for path, _ in index.entries.keys()]\n+\n+        tracked_files.extend(staged_files)\n+\n+        # Normalize the paths\n+        tracked_files = set(self.normalize_path(path) for path in tracked_files)\n+        tracked_files = [\n+            fname for fname in tracked_files if not self.ignored_file(fname)\n+        ]\n+\n+        files = [\n+            fname\n+            for fname in tracked_files\n+            if Path(self.abs_root_path(fname)).is_file()\n+        ]\n+        return sorted(set(files))\n+\n+    def get_all_abs_files(self):\n+        files = self.get_all_relative_files()\n+        files = [self.abs_root_path(path) for path in files]\n+        return files\n+\n+    def abs_root_path(self, path):\n+        res = Path(self.root) / path\n+        return self.safe_abs_path(res)\n+\n+    def get_addable_relative_files(self):\n+        return set(self.get_all_relative_files()) - set(\n+            self.get_inchat_relative_files()\n+        )\n+\n+    def safe_abs_path(self, res):\n+        \"Gives an abs path, which safely returns a full (not 8.3) windows path\"\n+        res = Path(res).resolve()\n+        return str(res)\n+\n+    def get_inchat_relative_files(self):\n+        files = [self.get_rel_fname(fname) for fname in self.abs_fnames]\n+        return sorted(set(files))\n+\n+    def read_image(self, filename):\n+        try:\n+            with open(str(filename), 'rb') as image_file:\n+                encoded_string = base64.b64encode(image_file.read())\n+                return encoded_string.decode('utf-8')\n+        except FileNotFoundError:\n+            self.log_error(f'{filename}: file not found error')\n+            return\n+        except IsADirectoryError:\n+            self.log_error(f'{filename}: is a directory')\n+            return\n+        except Exception as e:\n+            self.log_error(f'{filename}: {e}')\n+            return\n+\n+    def read_text(self, filename):\n+        if is_image_file(filename):\n+            return self.read_image(filename)\n+\n+        try:\n+            with open(str(filename), 'r', encoding='utf-8') as f:\n+                return f.read()\n+        except FileNotFoundError:\n+            self.log_error(\n+                f'Error when constructing repomap: {filename}: file not found error'\n+            )\n+            return\n+        except IsADirectoryError:\n+            self.log_error(f'{filename}: is a directory')\n+            return\n+        except UnicodeError as e:\n+            self.log_error(f'{filename}: {e}')\n+            self.log_error('Use --encoding to set the unicode encoding.')\n+            return\n+\n+    def log_error(self, message):\n+        # logger.error(f'Error when constructing repomap: {message}')\n+        pass\n+\n+    def ignored_file(self, fname):\n+        if not self.aider_ignore_file or not self.aider_ignore_file.is_file():\n+            return\n+\n+        try:\n+            fname = self.normalize_path(fname)\n+        except ValueError:\n+            return\n+\n+        mtime = self.aider_ignore_file.stat().st_mtime\n+        if mtime != self.aider_ignore_ts:\n+            self.aider_ignore_ts = int(mtime)\n+            lines = self.aider_ignore_file.read_text().splitlines()\n+            self.aider_ignore_spec = pathspec.PathSpec.from_lines(\n+                pathspec.patterns.GitWildMatchPattern,\n+                lines,\n+            )\n+\n+        return self.aider_ignore_spec.match_file(fname)\n+\n+    def normalize_path(self, path):\n+        return str(Path(PurePosixPath((Path(self.root) / path).relative_to(self.root))))\ndiff --git a/opendevin/runtime/plugins/agent_skills/repomap/utils.py b/opendevin/runtime/plugins/agent_skills/repomap/utils.py\nnew file mode 100644\nindex 000000000000..a42d8d26b75d\n--- /dev/null\n+++ b/opendevin/runtime/plugins/agent_skills/repomap/utils.py\n@@ -0,0 +1,68 @@\n+\"\"\"\n+Modifications made on June, 2024.\n+Description of changes: removed redundant methods, attributes, and adapted some classes to be OpenDevin-specific.\n+\"\"\"\n+\n+import warnings\n+\n+with warnings.catch_warnings():\n+    warnings.simplefilter('ignore')\n+    import litellm\n+import tempfile\n+\n+IMAGE_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp'}\n+\n+\n+def is_image_file(file_name):\n+    \"\"\"\n+    Check if a file is an image file based on its extension.\n+\n+    Args:\n+        file_name (str): The name of the file.\n+    \"\"\"\n+    file_name = str(file_name)  # Convert file_name to string\n+    return any(file_name.endswith(ext) for ext in IMAGE_EXTENSIONS)\n+\n+\n+def get_token_count_from_text(model_name, text):\n+    \"\"\"\n+    Get the token count from the given text using the specified model.\n+\n+    Args:\n+        model_name (str): The name of the model to use for token counting.\n+        text (str): The input text.\n+    \"\"\"\n+    return litellm.token_counter(model=model_name, text=text)\n+\n+\n+def get_model_max_input_tokens(model_name: str):\n+    model_info = None\n+    try:\n+        if not model_name.startswith('openrouter'):\n+            model_info = litellm.get_model_info(model_name.split(':')[0])\n+        else:\n+            model_info = litellm.get_model_info(model_name)\n+    # noinspection PyBroadException\n+    except Exception:\n+        return 4096\n+\n+    if model_info is not None and 'max_input_tokens' in model_info:\n+        max_input_tokens = model_info['max_input_tokens']\n+    else:\n+        # Max input tokens for gpt3.5, so this is a safe fallback for any potentially viable model\n+        max_input_tokens = 4096\n+    return max_input_tokens\n+\n+\n+class IgnorantTemporaryDirectory:\n+    def __init__(self):\n+        self.temp_dir = tempfile.TemporaryDirectory()\n+\n+    def __enter__(self):\n+        return self.temp_dir.__enter__()\n+\n+    def __exit__(self, exc_type, exc_val, exc_tb):\n+        try:\n+            self.temp_dir.__exit__(exc_type, exc_val, exc_tb)\n+        except (OSError, PermissionError):\n+            pass  # Ignore errors (Windows)\ndiff --git a/poetry.lock b/poetry.lock\nindex b30a478b94af..b251aab36a77 100644\n--- a/poetry.lock\n+++ b/poetry.lock\n@@ -6779,6 +6814,122 @@ torchhub = [\"filelock\", \"huggingface-hub (>=0.23.0,<1.0)\", \"importlib-metadata\",\n video = [\"av (==9.2.0)\", \"decord (==0.6.0)\"]\n vision = [\"Pillow (>=10.0.1,<=15.0)\"]\n \n+[[package]]\n+name = \"tree-sitter\"\n+version = \"0.21.3\"\n+description = \"Python bindings for the Tree-Sitter parsing library\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"tree-sitter-0.21.3.tar.gz\", hash = \"sha256:b5de3028921522365aa864d95b3c41926e0ba6a85ee5bd000e10dc49b0766988\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:351f302b6615230c9dac9829f0ba20a94362cd658206ca9a7b2d58d73373dfb0\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:766e79ae1e61271e7fdfecf35b6401ad9b47fc07a0965ad78e7f97fddfdf47a6\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2c4d3d4d4b44857e87de55302af7f2d051c912c466ef20e8f18158e64df3542a\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:84eedb06615461b9e2847be7c47b9c5f2195d7d66d31b33c0a227eff4e0a0199\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:9d33ea425df8c3d6436926fe2991429d59c335431bf4e3c71e77c17eb508be5a\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:fae1ee0ff6d85e2fd5cd8ceb9fe4af4012220ee1e4cbe813305a316caf7a6f63\"},\n+    {file = \"tree_sitter-0.21.3-cp310-cp310-win_amd64.whl\", hash = \"sha256:bb41be86a987391f9970571aebe005ccd10222f39c25efd15826583c761a37e5\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:54b22c3c2aab3e3639a4b255d9df8455da2921d050c4829b6a5663b057f10db5\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:ab6e88c1e2d5e84ff0f9e5cd83f21b8e5074ad292a2cf19df3ba31d94fbcecd4\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:fc3fd34ed4cd5db445bc448361b5da46a2a781c648328dc5879d768f16a46771\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:fabc7182f6083269ce3cfcad202fe01516aa80df64573b390af6cd853e8444a1\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:4f874c3f7d2a2faf5c91982dc7d88ff2a8f183a21fe475c29bee3009773b0558\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:ee61ee3b7a4eedf9d8f1635c68ba4a6fa8c46929601fc48a907c6cfef0cfbcb2\"},\n+    {file = \"tree_sitter-0.21.3-cp311-cp311-win_amd64.whl\", hash = \"sha256:0b7256c723642de1c05fbb776b27742204a2382e337af22f4d9e279d77df7aa2\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:669b3e5a52cb1e37d60c7b16cc2221c76520445bb4f12dd17fd7220217f5abf3\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:2aa2a5099a9f667730ff26d57533cc893d766667f4d8a9877e76a9e74f48f0d3\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6a3e06ae2a517cf6f1abb682974f76fa760298e6d5a3ecf2cf140c70f898adf0\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:af992dfe08b4fefcfcdb40548d0d26d5d2e0a0f2d833487372f3728cd0772b48\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:c7cbab1dd9765138505c4a55e2aa857575bac4f1f8a8b0457744a4fefa1288e6\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:e1e66aeb457d1529370fcb0997ae5584c6879e0e662f1b11b2f295ea57e22f54\"},\n+    {file = \"tree_sitter-0.21.3-cp312-cp312-win_amd64.whl\", hash = \"sha256:013c750252dc3bd0e069d82e9658de35ed50eecf31c6586d0de7f942546824c5\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:4986a8cb4acebd168474ec2e5db440e59c7888819b3449a43ce8b17ed0331b07\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:6e217fee2e7be7dbce4496caa3d1c466977d7e81277b677f954d3c90e3272ec2\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f32a88afff4f2bc0f20632b0a2aa35fa9ae7d518f083409eca253518e0950929\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:f3652ac9e47cdddf213c5d5d6854194469097e62f7181c0a9aa8435449a163a9\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:60b4df3298ff467bc01e2c0f6c2fb43aca088038202304bf8e41edd9fa348f45\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:00e4d0c99dff595398ef5e88a1b1ddd53adb13233fb677c1fd8e497fb2361629\"},\n+    {file = \"tree_sitter-0.21.3-cp38-cp38-win_amd64.whl\", hash = \"sha256:50c91353a26946e4dd6779837ecaf8aa123aafa2d3209f261ab5280daf0962f5\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:b17b8648b296ccc21a88d72ca054b809ee82d4b14483e419474e7216240ea278\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:f2f057fd01d3a95cbce6794c6e9f6db3d376cb3bb14e5b0528d77f0ec21d6478\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:839759de30230ffd60687edbb119b31521d5ac016749358e5285816798bb804a\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:5df40aa29cb7e323898194246df7a03b9676955a0ac1f6bce06bc4903a70b5f7\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:1d9be27dde007b569fa78ff9af5fe40d2532c998add9997a9729e348bb78fa59\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:c4ac87735e6f98fe085244c7c020f0177d13d4c117db72ba041faa980d25d69d\"},\n+    {file = \"tree_sitter-0.21.3-cp39-cp39-win_amd64.whl\", hash = \"sha256:fbbd137f7d9a5309fb4cb82e2c3250ba101b0dd08a8abdce815661e6cf2cbc19\"},\n+]\n+\n+[[package]]\n+name = \"tree-sitter-languages\"\n+version = \"1.10.2\"\n+description = \"Binary Python wheels for all tree sitter languages.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:5580348f0b20233b1d5431fa178ccd3d07423ca4a3275df02a44608fd72344b9\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:103c7466644486b1e9e03850df46fc6aa12f13ca636c74f173270276220ac80b\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d13db84511c6f1a7dc40383b66deafa74dabd8b877e3d65ab253f3719eccafd6\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:57adfa32be7e465b54aa72f915f6c78a2b66b227df4f656b5d4fbd1ca7a92b3f\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1c6385e033e460ceb8f33f3f940335f422ef2b763700a04f0089391a68b56153\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:dfa3f38cc5381c5aba01dd7494f59b8a9050e82ff6e06e1233e3a0cbae297e3c\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:9f195155acf47f8bc5de7cee46ecd07b2f5697f007ba89435b51ef4c0b953ea5\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:2de330e2ac6d7426ca025a3ec0f10d5640c3682c1d0c7702e812dcfb44b58120\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-win32.whl\", hash = \"sha256:c9731cf745f135d9770eeba9bb4e2ff4dabc107b5ae9b8211e919f6b9100ea6d\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp310-cp310-win_amd64.whl\", hash = \"sha256:6dd75851c41d0c3c4987a9b7692d90fa8848706c23115669d8224ffd6571e357\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:7eb7d7542b2091c875fe52719209631fca36f8c10fa66970d2c576ae6a1b8289\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:6b41bcb00974b1c8a1800c7f1bb476a1d15a0463e760ee24872f2d53b08ee424\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6f370cd7845c6c81df05680d5bd96db8a99d32b56f4728c5d05978911130a853\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:a1dc195c88ef4c72607e112a809a69190e096a2e5ebc6201548b3e05fdd169ad\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9ae34ac314a7170be24998a0f994c1ac80761d8d4bd126af27ee53a023d3b849\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:01b5742d5f5bd675489486b582bd482215880b26dde042c067f8265a6e925d9c\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:ab1cbc46244d34fd16f21edaa20231b2a57f09f092a06ee3d469f3117e6eb954\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:0b1149e7467a4e92b8a70e6005fe762f880f493cf811fc003554b29f04f5e7c8\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-win32.whl\", hash = \"sha256:049276343962f4696390ee555acc2c1a65873270c66a6cbe5cb0bca83bcdf3c6\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp311-cp311-win_amd64.whl\", hash = \"sha256:7f3fdd468a577f04db3b63454d939e26e360229b53c80361920aa1ebf2cd7491\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:c0f4c8b2734c45859edc7fcaaeaab97a074114111b5ba51ab4ec7ed52104763c\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:eecd3c1244ac3425b7a82ba9125b4ddb45d953bbe61de114c0334fd89b7fe782\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:15db3c8510bc39a80147ee7421bf4782c15c09581c1dc2237ea89cefbd95b846\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:92c6487a6feea683154d3e06e6db68c30e0ae749a7ce4ce90b9e4e46b78c85c7\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6d2f1cd1d1bdd65332f9c2b67d49dcf148cf1ded752851d159ac3e5ee4f4d260\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:976c8039165b8e12f17a01ddee9f4e23ec6e352b165ad29b44d2bf04e2fbe77e\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:dafbbdf16bf668a580902e1620f4baa1913e79438abcce721a50647564c687b9\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:1aeabd3d60d6d276b73cd8f3739d595b1299d123cc079a317f1a5b3c5461e2ca\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-win32.whl\", hash = \"sha256:fab8ee641914098e8933b87ea3d657bea4dd00723c1ee7038b847b12eeeef4f5\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp312-cp312-win_amd64.whl\", hash = \"sha256:5e606430d736367e5787fa5a7a0c5a1ec9b85eded0b3596bbc0d83532a40810b\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:838d5b48a7ed7a17658721952c77fda4570d2a069f933502653b17e15a9c39c9\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:987b3c71b1d278c2889e018ee77b8ee05c384e2e3334dec798f8b611c4ab2d1e\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:faa00abcb2c819027df58472da055d22fa7dfcb77c77413d8500c32ebe24d38b\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:0e102fbbf02322d9201a86a814e79a9734ac80679fdb9682144479044f401a73\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:8f0b87cf1a7b03174ba18dfd81582be82bfed26803aebfe222bd20e444aba003\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:c0f1b9af9cb67f0b942b020da9fdd000aad5e92f2383ae0ba7a330b318d31912\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:5a4076c921f7a4d31e643843de7dfe040b65b63a238a5aa8d31d93aabe6572aa\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-win32.whl\", hash = \"sha256:fa6391a3a5d83d32db80815161237b67d70576f090ce5f38339206e917a6f8bd\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp37-cp37m-win_amd64.whl\", hash = \"sha256:55649d3f254585a064121513627cf9788c1cfdadbc5f097f33d5ba750685a4c0\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:6f85d1edaa2d22d80d4ea5b6d12b95cf3644017b6c227d0d42854439e02e8893\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:d78feed4a764ef3141cb54bf00fe94d514d8b6e26e09423e23b4c616fcb7938c\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:da1aca27531f9dd5308637d76643372856f0f65d0d28677d1bcf4211e8ed1ad0\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:1031ea440dafb72237437d754eff8940153a3b051e3d18932ac25e75ce060a15\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:99d3249beaef2c9fe558ecc9a97853c260433a849dcc68266d9770d196c2e102\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:59a4450f262a55148fb7e68681522f0c2a2f6b7d89666312a2b32708d8f416e1\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:ce74eab0e430370d5e15a96b6c6205f93405c177a8b2e71e1526643b2fb9bab1\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:9b4dd2b6b3d24c85dffe33d6c343448869eaf4f41c19ddba662eb5d65d8808f4\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-win32.whl\", hash = \"sha256:92d734fb968fe3927a7596d9f0459f81a8fa7b07e16569476b28e27d0d753348\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp38-cp38-win_amd64.whl\", hash = \"sha256:46a13f7d38f2eeb75f7cf127d1201346093748c270d686131f0cbc50e42870a1\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:f8c6a936ae99fdd8857e91f86c11c2f5e507ff30631d141d98132bb7ab2c8638\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:c283a61423f49cdfa7b5a5dfbb39221e3bd126fca33479cd80749d4d7a6b7349\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:76e60be6bdcff923386a54a5edcb6ff33fc38ab0118636a762024fa2bc98de55\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:c00069f9575bd831eabcce2cdfab158dde1ed151e7e5614c2d985ff7d78a7de1\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:475ff53203d8a43ccb19bb322fa2fb200d764001cc037793f1fadd714bb343da\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:26fe7c9c412e4141dea87ea4b3592fd12e385465b5bdab106b0d5125754d4f60\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:8fed27319957458340f24fe14daad467cd45021da034eef583519f83113a8c5e\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:3657a491a7f96cc75a3568ddd062d25f3be82b6a942c68801a7b226ff7130181\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-win32.whl\", hash = \"sha256:33f7d584d01a7a3c893072f34cfc64ec031f3cfe57eebc32da2f8ac046e101a7\"},\n+    {file = \"tree_sitter_languages-1.10.2-cp39-cp39-win_amd64.whl\", hash = \"sha256:1b944af3ee729fa70fc8ae82224a9ff597cdb63addea084e0ea2fa2b0ec39bb7\"},\n+]\n+\n+[package.dependencies]\n+tree-sitter = \"*\"\n+\n [[package]]\n name = \"triton\"\n version = \"2.2.0\"\n\ndiff --git a/tests/unit/test_repomap.py b/tests/unit/test_repomap.py\nnew file mode 100644\nindex 000000000000..3e040f5c5688\n--- /dev/null\n+++ b/tests/unit/test_repomap.py\n@@ -0,0 +1,198 @@\n+\"\"\"\n+Modifications made on June, 2024.\n+Description of changes: removed redundant methods, attributes, and adapted some classes to be OpenDevin-specific.\n+\"\"\"\n+\n+import os\n+import unittest\n+\n+from opendevin.runtime.plugins.agent_skills.repomap.repomap import RepoMap\n+from opendevin.runtime.plugins.agent_skills.repomap.utils import (\n+    IgnorantTemporaryDirectory,\n+)\n+\n+\n+class TestRepoMap(unittest.TestCase):\n+    def setUp(self):\n+        self.GPT35_MODEL_NAME = 'gpt-3.5-turbo'\n+\n+    def test_get_repo_map(self):\n+        # Create a temporary directory with sample files for testing\n+        test_files = [\n+            'test_file1.py',\n+            'test_file2.py',\n+            'test_file3.md',\n+            'test_file4.json',\n+        ]\n+\n+        with IgnorantTemporaryDirectory() as temp_dir:\n+            for file in test_files:\n+                with open(os.path.join(temp_dir, file), 'w') as f:\n+                    f.write('')\n+\n+            repo_map = RepoMap(model_name=self.GPT35_MODEL_NAME, root=temp_dir)\n+            other_files = [os.path.join(temp_dir, file) for file in test_files]\n+            result = repo_map.get_repo_map([], other_files)\n+\n+            # Check if the result contains the expected tags map\n+            self.assertIn('test_file1.py', result)\n+            self.assertIn('test_file2.py', result)\n+            self.assertIn('test_file3.md', result)\n+            self.assertIn('test_file4.json', result)\n+\n+    def test_get_repo_map_with_identifiers(self):\n+        # Create a temporary directory with a sample Python file containing identifiers\n+        test_file1 = 'test_file_with_identifiers.py'\n+        file_content1 = \"\"\"\\\n+class MyClass:\n+    def my_method(self, arg1, arg2):\n+        return arg1 + arg2\n+\n+def my_function(arg1, arg2):\n+    return arg1 * arg2\n+\"\"\"\n+\n+        test_file2 = 'test_file_import.py'\n+        file_content2 = \"\"\"\\\n+from test_file_with_identifiers import MyClass\n+\n+obj = MyClass()\n+print(obj.my_method(1, 2))\n+print(my_function(3, 4))\n+\"\"\"\n+\n+        test_file3 = 'test_file_pass.py'\n+        file_content3 = 'pass'\n+\n+        with IgnorantTemporaryDirectory() as temp_dir:\n+            with open(os.path.join(temp_dir, test_file1), 'w') as f:\n+                f.write(file_content1)\n+\n+            with open(os.path.join(temp_dir, test_file2), 'w') as f:\n+                f.write(file_content2)\n+\n+            with open(os.path.join(temp_dir, test_file3), 'w') as f:\n+                f.write(file_content3)\n+\n+            repo_map = RepoMap(model_name=self.GPT35_MODEL_NAME, root=temp_dir)\n+            other_files = [\n+                os.path.join(temp_dir, test_file1),\n+                os.path.join(temp_dir, test_file2),\n+                os.path.join(temp_dir, test_file3),\n+            ]\n+            result = repo_map.get_repo_map([], other_files)\n+\n+            # Check if the result contains the expected tags map with identifiers\n+            self.assertIn('test_file_with_identifiers.py', result)\n+            self.assertIn('MyClass', result)\n+            self.assertIn('my_method', result)\n+            self.assertIn('my_function', result)\n+            self.assertIn('test_file_pass.py', result)\n+\n+    def test_get_repo_map_all_files(self):\n+        test_files = [\n+            'test_file0.py',\n+            'test_file1.txt',\n+            'test_file2.md',\n+            'test_file3.json',\n+            'test_file4.html',\n+            'test_file5.css',\n+            'test_file6.js',\n+        ]\n+\n+        with IgnorantTemporaryDirectory() as temp_dir:\n+            for file in test_files:\n+                with open(os.path.join(temp_dir, file), 'w') as f:\n+                    f.write('')\n+\n+            repo_map = RepoMap(model_name=self.GPT35_MODEL_NAME, root=temp_dir)\n+\n+            other_files = [os.path.join(temp_dir, file) for file in test_files]\n+            result = repo_map.get_repo_map([], other_files)\n+\n+            # Check if the result contains each specific file in the expected tags map without ctags\n+            for file in test_files:\n+                self.assertIn(file, result)\n+\n+    def test_get_repo_map_excludes_added_files(self):\n+        # Create a temporary directory with sample files for testing\n+        test_files = [\n+            'test_file1.py',\n+            'test_file2.py',\n+            'test_file3.md',\n+            'test_file4.json',\n+        ]\n+\n+        with IgnorantTemporaryDirectory() as temp_dir:\n+            for file in test_files:\n+                with open(os.path.join(temp_dir, file), 'w') as f:\n+                    f.write('def foo(): pass\\n')\n+\n+            repo_map = RepoMap(model_name=self.GPT35_MODEL_NAME, root=temp_dir)\n+            test_files = [os.path.join(temp_dir, file) for file in test_files]\n+            result = repo_map.get_repo_map(test_files[:2], test_files[2:])\n+\n+            # Check if the result contains the expected tags map\n+            self.assertNotIn('test_file1.py', result)\n+            self.assertNotIn('test_file2.py', result)\n+            self.assertIn('test_file3.md', result)\n+            self.assertIn('test_file4.json', result)\n+\n+\n+class TestRepoMapTypescript(unittest.TestCase):\n+    def setUp(self):\n+        self.GPT35_MODEL_NAME = 'gpt-3.5-turbo'\n+\n+    def test_get_repo_map_typescript(self):\n+        # Create a temporary directory with a sample TypeScript file\n+        test_file_ts = 'test_file.ts'\n+        file_content_ts = \"\"\"\\\n+interface IMyInterface {\n+    someMethod(): void;\n+}\n+\n+type ExampleType = {\n+    key: string;\n+    value: number;\n+};\n+\n+enum Status {\n+    New,\n+    InProgress,\n+    Completed,\n+}\n+\n+export class MyClass {\n+    constructor(public value: number) {}\n+\n+    add(input: number): number {\n+        return this.value + input;\n+        return this.value + input;\n+    }\n+}\n+\n+export function myFunction(input: number): number {\n+    return input * 2;\n+}\n+\"\"\"\n+\n+        with IgnorantTemporaryDirectory() as temp_dir:\n+            with open(os.path.join(temp_dir, test_file_ts), 'w') as f:\n+                f.write(file_content_ts)\n+\n+            repo_map = RepoMap(model_name=self.GPT35_MODEL_NAME, root=temp_dir)\n+            other_files = [os.path.join(temp_dir, test_file_ts)]\n+            result = repo_map.get_repo_map([], other_files)\n+\n+            # Check if the result contains the expected tags map with TypeScript identifiers\n+            self.assertIn('test_file.ts', result)\n+            self.assertIn('IMyInterface', result)\n+            self.assertIn('ExampleType', result)\n+            self.assertIn('Status', result)\n+            self.assertIn('MyClass', result)\n+            self.assertIn('add', result)\n+            self.assertIn('myFunction', result)\n+\n+\n+if __name__ == '__main__':\n+    unittest.main()\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR raises important considerations about the `RepoMap` implementation. First, we might want to restrict its use to GitHub repos initially, as they are most likely where it's beneficial. Configuring `RepoMap` construction could address performance concerns with large workspaces, like OpenDevin. Additionally, the file permission issues when accessing Docker mounts highlight a need for a more robust solution, potentially integrating `RepoMap` into `agentskills` to avoid mounting complications. Finally, integrating comprehensive tests is crucial for maintaining stability as we move forward."
      }
    ],
    "codediff_length": 77215
  },
  {
    "index": 53,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/openhands/llm/fn_call_converter.py b/openhands/llm/fn_call_converter.py\nindex 04ce6eb6f43b..14166814f37e 100644\n--- a/openhands/llm/fn_call_converter.py\n+++ b/openhands/llm/fn_call_converter.py\n@@ -205,28 +235,100 @@ def index() -> str:\n     10      app.run(port=5000)\n Review the changes and make sure they are as expected. Edit the file again if necessary.\n \n+\"\"\",\n+    },\n+    'browser': {\n+        'view_page': \"\"\"\n ASSISTANT:\n-Running the updated file:\n-<function=execute_bash>\n-<parameter=command>\n-python3 app.py > server.log 2>&1 &\n+Let me check how the page looks in the browser:\n+<function=browser>\n+<parameter=code>\n+goto('http://127.0.0.1:5000')\n+noop(1000)  # Wait for page to load\n </parameter>\n </function>\n \n-USER: EXECUTION RESULT of [execute_bash]:\n-[1] 126\n-\n+USER: EXECUTION RESULT of [browser]:\n+[Browser shows the numbers in a table format]\n+\"\"\"\n+    },\n+    'finish': {\n+        'task_completed': \"\"\"\n ASSISTANT:\n The server is running on port 5000 with PID 126. You can access the list of numbers in a table format by visiting http://127.0.0.1:5000. Let me know if you have any further requests!\n <function=finish>\n+<parameter=message>The task has been completed. The web server is running and displaying numbers 1-10 in a table format at http://127.0.0.1:5000.</parameter>\n+<parameter=task_completed>true</parameter>\n </function>\n+\"\"\"\n+    },\n+}\n+\n+\n+def get_example_for_tools(tools: list[dict]) -> str:\n+    \"\"\"Generate an in-context learning example based on available tools.\"\"\"\n+    available_tools = set()\n+    for tool in tools:\n+        if tool['type'] == 'function':\n+            name = tool['function']['name']\n+            if name == EXECUTE_BASH_TOOL_NAME:\n+                available_tools.add('execute_bash')\n+            elif name == STR_REPLACE_EDITOR_TOOL_NAME:\n+                available_tools.add('str_replace_editor')\n+            elif name == BROWSER_TOOL_NAME:\n+                available_tools.add('browser')\n+            elif name == FINISH_TOOL_NAME:\n+                available_tools.add('finish')\n+\n+    if not available_tools:\n+        return ''\n+\n+    example = \"\"\"Here's a running example of how to perform a task with the provided tools.\n+\n+--------------------- START OF EXAMPLE ---------------------\n+\n+USER: Create a list of numbers from 1 to 10, and display them in a web page at port 5000.\n+\n+\"\"\"\n+\n+    # Build example based on available tools\n+    if 'execute_bash' in available_tools:\n+        example += TOOL_EXAMPLES['execute_bash']['check_dir']\n+\n+    if 'str_replace_editor' in available_tools:\n+        example += TOOL_EXAMPLES['str_replace_editor']['create_file']\n \n+    if 'execute_bash' in available_tools:\n+        example += TOOL_EXAMPLES['execute_bash']['run_server']\n+\n+    if 'browser' in available_tools:\n+        example += TOOL_EXAMPLES['browser']['view_page']\n+\n+    if 'execute_bash' in available_tools:\n+        example += TOOL_EXAMPLES['execute_bash']['kill_server']\n+\n+    if 'str_replace_editor' in available_tools:\n+        example += TOOL_EXAMPLES['str_replace_editor']['edit_file']\n+\n+    if 'execute_bash' in available_tools:\n+        example += TOOL_EXAMPLES['execute_bash']['run_server_again']\n+\n+    if 'finish' in available_tools:\n+        example += TOOL_EXAMPLES['finish']['task_completed']\n+\n+    example += \"\"\"\n --------------------- END OF EXAMPLE ---------------------\n \n Do NOT assume the environment is the same as in the example above.\n \n --------------------- NEW TASK DESCRIPTION ---------------------\n-\"\"\").lstrip()\n+\"\"\"\n+    example = example.lstrip()\n+\n+    return example\n+\n+\n+IN_CONTEXT_LEARNING_EXAMPLE_PREFIX = get_example_for_tools\n \n IN_CONTEXT_LEARNING_EXAMPLE_SUFFIX = \"\"\"\n --------------------- END OF NEW TASK DESCRIPTION ---------------------\n\ndiff --git a/tests/unit/test_llm_fncall_converter.py b/tests/unit/test_llm_fncall_converter.py\nindex 29a8e5475485..29cfd80840b7 100644\n--- a/tests/unit/test_llm_fncall_converter.py\n+++ b/tests/unit/test_llm_fncall_converter.py\n@@ -140,6 +142,264 @@ def test_convert_tools_to_description():\n     )\n \n \n+def test_get_example_for_tools_no_tools():\n+    \"\"\"Test that get_example_for_tools returns empty string when no tools are available.\"\"\"\n+    tools = []\n+    example = get_example_for_tools(tools)\n+    assert example == ''\n+\n+\n+def test_get_example_for_tools_single_tool():\n+    \"\"\"Test that get_example_for_tools generates correct example with a single tool.\"\"\"\n+    tools = [\n+        {\n+            'type': 'function',\n+            'function': {\n+                'name': 'execute_bash',\n+                'description': 'Execute a bash command in the terminal.',\n+                'parameters': {\n+                    'type': 'object',\n+                    'properties': {\n+                        'command': {\n+                            'type': 'string',\n+                            'description': 'The bash command to execute.',\n+                        }\n+                    },\n+                    'required': ['command'],\n+                },\n+            },\n+        }\n+    ]\n+    example = get_example_for_tools(tools)\n+    assert example.startswith(\n+        \"Here's a running example of how to perform a task with the provided tools.\"\n+    )\n+    assert (\n+        'USER: Create a list of numbers from 1 to 10, and display them in a web page at port 5000.'\n+        in example\n+    )\n+    assert TOOL_EXAMPLES['execute_bash']['check_dir'] in example\n+    assert TOOL_EXAMPLES['execute_bash']['run_server'] in example\n+    assert TOOL_EXAMPLES['execute_bash']['kill_server'] in example\n+    assert TOOL_EXAMPLES['str_replace_editor']['create_file'] not in example\n+    assert TOOL_EXAMPLES['browser']['view_page'] not in example\n+    assert TOOL_EXAMPLES['finish']['task_completed'] not in example\n+\n+\n+def test_get_example_for_tools_single_tool_is_finish():\n+    \"\"\"Test get_example_for_tools with only the finish tool.\"\"\"\n+    tools = [\n+        {\n+            'type': 'function',\n+            'function': {\n+                'name': 'finish',\n+                'description': 'Finish the interaction when the task is complete.',\n+            },\n+        }\n+    ]\n+    example = get_example_for_tools(tools)\n+    assert example.startswith(\n+        \"Here's a running example of how to perform a task with the provided tools.\"\n+    )\n+    assert (\n+        'USER: Create a list of numbers from 1 to 10, and display them in a web page at port 5000.'\n+        in example\n+    )\n+    assert TOOL_EXAMPLES['finish']['task_completed'] in example\n+    assert TOOL_EXAMPLES['execute_bash']['check_dir'] not in example\n+    assert TOOL_EXAMPLES['str_replace_editor']['create_file'] not in example\n+    assert TOOL_EXAMPLES['browser']['view_page'] not in example\n+\n+\n+def test_get_example_for_tools_multiple_tools():\n+    \"\"\"Test that get_example_for_tools generates correct example with multiple tools.\"\"\"\n+    tools = [\n+        {\n+            'type': 'function',\n+            'function': {\n+                'name': 'execute_bash',\n+                'description': 'Execute a bash command in the terminal.',\n+                'parameters': {\n+                    'type': 'object',\n+                    'properties': {\n+                        'command': {\n+                            'type': 'string',\n+                            'description': 'The bash command to execute.',\n+                        }\n+                    },\n+                    'required': ['command'],\n+                },\n+            },\n+        },\n+        {\n+            'type': 'function',\n+            'function': {\n+                'name': 'str_replace_editor',\n+                'description': 'Custom editing tool for viewing, creating and editing files.',\n+                'parameters': {\n+                    'type': 'object',\n+                    'properties': {\n+                        'command': {\n+                            'type': 'string',\n+                            'description': 'The commands to run.',\n+                            'enum': [\n+                                'view',\n+                                'create',\n+                                'str_replace',\n+                                'insert',\n+                                'undo_edit',\n+                            ],\n+                        },\n+                        'path': {\n+                            'type': 'string',\n+                            'description': 'Absolute path to file or directory.',\n+                        },\n+                    },\n+                    'required': ['command', 'path'],\n+                },\n+            },\n+        },\n+    ]\n+    example = get_example_for_tools(tools)\n+    assert example.startswith(\n+        \"Here's a running example of how to perform a task with the provided tools.\"\n+    )\n+    assert (\n+        'USER: Create a list of numbers from 1 to 10, and display them in a web page at port 5000.'\n+        in example\n+    )\n+    assert TOOL_EXAMPLES['execute_bash']['check_dir'] in example\n+    assert TOOL_EXAMPLES['execute_bash']['run_server'] in example\n+    assert TOOL_EXAMPLES['execute_bash']['kill_server'] in example\n+    assert TOOL_EXAMPLES['str_replace_editor']['create_file'] in example\n+    assert TOOL_EXAMPLES['str_replace_editor']['edit_file'] in example\n+    assert TOOL_EXAMPLES['browser']['view_page'] not in example\n+    assert TOOL_EXAMPLES['finish']['task_completed'] not in example\n+\n+\n+def test_get_example_for_tools_multiple_tools_with_finish():\n+    \"\"\"Test get_example_for_tools with multiple tools including finish.\"\"\"\n+    # Uses execute_bash and finish tools\n+    tools = [\n+        {\n+            'type': 'function',\n+            'function': {\n+                'name': 'execute_bash',\n+                'description': 'Execute a bash command in the terminal.',\n+                'parameters': {  # Params added for completeness, not strictly needed by get_example_for_tools\n+                    'type': 'object',\n+                    'properties': {\n+                        'command': {\n+                            'type': 'string',\n+                            'description': 'The bash command to execute.',\n+                        }\n+                    },\n+                    'required': ['command'],\n+                },\n+            },\n+        },\n+        {\n+            'type': 'function',\n+            'function': {\n+                'name': 'str_replace_editor',\n+                'description': 'Custom editing tool for viewing, creating and editing files.',\n+                'parameters': {\n+                    'type': 'object',\n+                    'properties': {\n+                        'command': {\n+                            'type': 'string',\n+                            'description': 'The commands to run.',\n+                            'enum': [\n+                                'view',\n+                                'create',\n+                                'str_replace',\n+                                'insert',\n+                                'undo_edit',\n+                            ],\n+                        },\n+                        'path': {\n+                            'type': 'string',\n+                            'description': 'Absolute path to file or directory.',\n+                        },\n+                    },\n+                    'required': ['command', 'path'],\n+                },\n+            },\n+        },\n+        {\n+            'type': 'function',\n+            'function': {\n+                'name': 'browser',\n+                'description': 'Interact with the browser.',\n+                'parameters': {\n+                    'type': 'object',\n+                    'properties': {\n+                        'code': {\n+                            'type': 'string',\n+                            'description': 'The Python code that interacts with the browser.',\n+                        }\n+                    },\n+                    'required': ['code'],\n+                },\n+            },\n+        },\n+        {\n+            'type': 'function',\n+            'function': {\n+                'name': 'finish',\n+                'description': 'Finish the interaction.',\n+            },\n+        },\n+    ]\n+    example = get_example_for_tools(tools)\n+    assert example.startswith(\n+        \"Here's a running example of how to perform a task with the provided tools.\"\n+    )\n+    assert (\n+        'USER: Create a list of numbers from 1 to 10, and display them in a web page at port 5000.'\n+        in example\n+    )\n+\n+    # Check for execute_bash parts (order matters for get_example_for_tools)\n+    assert TOOL_EXAMPLES['execute_bash']['check_dir'].strip() in example\n+    assert TOOL_EXAMPLES['execute_bash']['run_server'].strip() in example\n+    assert TOOL_EXAMPLES['execute_bash']['kill_server'].strip() in example\n+    assert TOOL_EXAMPLES['execute_bash']['run_server_again'].strip() in example\n+\n+    # Check for str_replace_editor parts\n+    assert TOOL_EXAMPLES['str_replace_editor']['create_file'] in example\n+    assert TOOL_EXAMPLES['str_replace_editor']['edit_file'] in example\n+\n+    # Check for browser part\n+    assert TOOL_EXAMPLES['browser']['view_page'] in example\n+\n+    # Check for finish part\n+    assert TOOL_EXAMPLES['finish']['task_completed'] in example\n+\n+\n+def test_get_example_for_tools_all_tools():\n+    \"\"\"Test that get_example_for_tools generates correct example with all tools.\"\"\"\n+    tools = FNCALL_TOOLS  # FNCALL_TOOLS already includes 'finish'\n+    example = get_example_for_tools(tools)\n+    assert example.startswith(\n+        \"Here's a running example of how to perform a task with the provided tools.\"\n+    )\n+    assert (\n+        'USER: Create a list of numbers from 1 to 10, and display them in a web page at port 5000.'\n+        in example\n+    )\n+    assert TOOL_EXAMPLES['execute_bash']['check_dir'] in example\n+    assert TOOL_EXAMPLES['execute_bash']['run_server'] in example\n+    assert TOOL_EXAMPLES['execute_bash']['kill_server'] in example\n+    assert TOOL_EXAMPLES['str_replace_editor']['create_file'] in example\n+    assert TOOL_EXAMPLES['str_replace_editor']['edit_file'] in example\n+    assert TOOL_EXAMPLES['finish']['task_completed'] in example\n+\n+    # These are not in global FNCALL_TOOLS\n+    # assert TOOL_EXAMPLES['web_read']['read_docs'] not in example # web_read is removed\n+    assert TOOL_EXAMPLES['browser']['view_page'] not in example\n+\n+\n FNCALL_MESSAGES = [\n     {\n         'content': [\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR addresses the compatibility of tool usage in the function call converter by implementing a dynamic example generation system. The key changes include the introduction of a `TOOL_EXAMPLES` dictionary and a `get_example_for_tools` function, which ensure only available tools are referenced. However, unit tests need to be updated for `fn_call_converter.py` to verify these changes. Careful attention is needed to retain all original example context during refactoring. Please ensure tests in `test_llm_fncall_converter.py` thoroughly cover the new logic."
      }
    ],
    "codediff_length": 14462
  },
  {
    "index": 54,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/agenthub/monologue_agent/utils/prompts.py b/agenthub/monologue_agent/utils/prompts.py\nindex d1b37a062dd5..6cb9ed9d4fd8 100644\n--- a/agenthub/monologue_agent/utils/prompts.py\n+++ b/agenthub/monologue_agent/utils/prompts.py\n@@ -134,37 +134,115 @@ def get_request_action_prompt(\n             elif latest_thought['action'] == 'error':\n                 hint = 'Looks like that last command failed. Maybe you need to fix it, or try something else.'\n \n-    bg_commands_message = ''\n-    if len(background_commands_obs) > 0:\n-        bg_commands_message = 'The following commands are running in the background:'\n-        for command_obs in background_commands_obs:\n-            bg_commands_message += (\n-                f'\\n`{command_obs.command_id}`: {command_obs.command}'\n-            )\n-        bg_commands_message += '\\nYou can end any process by sending a `kill` action with the numerical `command_id` above.'\n+    # FIXME are background commands already in recent events?\n+    bg_commands_message = format_background_commands(background_commands_obs)\n \n     user = 'opendevin' if config.run_as_devin else 'root'\n \n     return ACTION_PROMPT % {\n         'task': task,\n-        'monologue': json.dumps(thoughts, indent=2),\n+        'monologue': json.dumps(default_events + recent_events, indent=2),\n         'background_commands': bg_commands_message,\n         'hint': hint,\n         'user': user,\n         'timeout': config.sandbox_timeout,\n-        'WORKSPACE_MOUNT_PATH_IN_SANDBOX': config.workspace_mount_path_in_sandbox,\n+        # unused 'workspace_mount_path_in_sandbox': config.workspace_mount_path_in_sandbox,\n+    }\n+\n+\n+def get_action_prompt_template(\n+    task: str,\n+    default_events: list[dict],\n+    background_commands_obs: list[CmdOutputObservation],\n+) -> str:\n+    \"\"\"\n+    Gets the action prompt template with default_events pre-filled and a placeholder for recent_events and hint.\n+\n+    Parameters:\n+    - task: The current task the agent is trying to accomplish\n+    - default_events: The default events to include in the prompt\n+    - background_commands_obs: list of all observed background commands running\n+\n+    Returns:\n+    - str: Formatted prompt template string with default_events pre-filled and placeholders for recent_events and hint\n+    \"\"\"\n+    bg_commands_message = format_background_commands(background_commands_obs)\n+    user = 'opendevin' if config.run_as_devin else 'root'\n+\n+    return ACTION_PROMPT % {\n+        'task': task,\n+        'monologue': json.dumps(default_events, indent=2) + '%(recent_events)s',\n+        'background_commands': bg_commands_message,\n+        'hint': '%(hint)s',\n+        'user': user,\n+        'timeout': config.sandbox_timeout,\n+    }\n+\n+\n+def get_action_prompt_for_summarization(\n+    prompt_template: str,\n+    recent_events: list[dict],\n+) -> str:\n+    \"\"\"\n+    Gets the action prompt formatted with recent_events and a generated hint.\n+\n+    Parameters:\n+    - prompt_template: The prompt template with placeholders for recent_events and hint\n+    - recent_events: The recent events to include in the prompt\n+\n+    Returns:\n+    - str: Formatted prompt string with recent_events and a generated hint included\n+    \"\"\"\n+    hint = ''\n+    if recent_events is not None and len(recent_events) > 0:\n+        latest_thought = recent_events[-1]\n+        if 'action' in latest_thought:\n+            if latest_thought['action'] == 'message':\n+                if latest_thought['args']['content'].startswith('OK so my task is'):\n+                    hint = \"You're just getting started! What should you do first?\"\n+                else:\n+                    hint = \"You've been thinking a lot lately. Maybe it's time to take action?\"\n+            elif latest_thought['action'] == 'error':\n+                hint = 'Looks like that last command failed. Maybe you need to fix it, or try something else.'\n+\n+    return prompt_template % {\n+        'recent_events': json.dumps(recent_events, indent=2),\n+        'hint': hint,\n     }\n \n \n+def format_background_commands(\n+    background_commands_obs: list[CmdOutputObservation] | None,\n+) -> str:\n+    \"\"\"\n+    Formats the background commands for sending in the prompt\n+\n+    Parameters:\n+    - background_commands_obs: list of all background commands running\n+\n+    Returns:\n+    - Formatted string with all background commands\n+    \"\"\"\n+    if background_commands_obs is None or len(background_commands_obs) == 0:\n+        return ''\n+\n+    bg_commands_message = 'The following commands are running in the background:'\n+    for obs in background_commands_obs:\n+        bg_commands_message += f'\\n`{obs.command_id}`: {obs.command}'\n+    bg_commands_message += '\\nYou can end any process by sending a `kill` action with the numerical `command_id` above.'\n+\n+    return bg_commands_message\n+\n+\n def parse_action_response(orig_response: str) -> Action:\n     \"\"\"\n     Parses a string to find an action within it\n \n     Parameters:\n-    - response (str): The string to be parsed\n+    - orig_response: The string to be parsed\n \n     Returns:\n-    - Action: The action that was found in the response string\n+    - The action that was found in the response string\n     \"\"\"\n     # attempt to load the JSON dict from the response\n     action_dict = json.loads(orig_response)\n\ndiff --git a/opendevin/memory/condenser.py b/opendevin/memory/condenser.py\nindex b8b1842dc7e5..1fdd4b36bfe7 100644\n--- a/opendevin/memory/condenser.py\n+++ b/opendevin/memory/condenser.py\n@@ -1,26 +1,160 @@\n+from typing import Callable\n+\n from opendevin.core.logger import opendevin_logger as logger\n from opendevin.llm.llm import LLM\n \n+from . import get_summarize_prompt, parse_summary_response\n+\n+MAX_TOKEN_COUNT_PADDING = (\n+    512  # estimation of tokens to add to the prompt for the max token count\n+)\n+\n \n class MemoryCondenser:\n-    def condense(self, summarize_prompt: str, llm: LLM):\n+    \"\"\"\n+    Condenses the prompt with a call to the LLM.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        action_prompt: Callable[..., str],\n+        summarize_prompt: Callable[[list[dict]], str] = get_summarize_prompt,\n+    ):\n         \"\"\"\n-        Attempts to condense the monologue by using the llm\n+        Initialize the MemoryCondenser with the action and summarize prompts.\n+\n+        action_prompt is a callable that returns the prompt that is about to be sent to the LLM.\n+        The prompt callable will be called with recent events as arguments.\n+        summarize_prompt is a callable that returns a specific prompt that tells the LLM to summarize the recent events.\n+        The prompt callable will be called with events as argument.\n+        If not provided, the default summarize prompt will be used.\n \n         Parameters:\n-        - llm (LLM): llm to be used for summarization\n+        - action_prompt: The function to generate an action prompt. The function should accept recent events as arguments.\n+        - summarize_prompt: The function to generate a summarize prompt. The function should accept srecent events as arguments.\n+        \"\"\"\n+        self.action_prompt = action_prompt\n+        self.summarize_prompt = summarize_prompt\n \n-        Raises:\n-        - Exception: the same exception as it got from the llm or processing the response\n+    def condense(\n+        self,\n+        llm: LLM,\n+        recent_events: list[dict],\n+    ) -> tuple[list[dict], bool]:\n         \"\"\"\n+        Attempts to condense the recent events of the monologue by using the llm. Returns the condensed recent events if successful, or False if not.\n \n+        It includes default events in the prompt for context, but does not alter them.\n+        Condenses the events using a summary prompt.\n+\n+        Parameters:\n+        - llm: LLM to be used for summarization.\n+        - recent_events: List of recent events that may be condensed.\n+\n+        Returns:\n+        - The condensed recent events if successful, or False if condensation failed.\n+        \"\"\"\n         try:\n-            messages = [{'content': summarize_prompt, 'role': 'user'}]\n-            resp = llm.completion(messages=messages)\n-            summary_response = resp['choices'][0]['message']['content']\n-            return summary_response\n+            logger.debug('Condensing recent events')\n+\n+            # attempt to condense the recent events\n+            new_recent_events = self._attempt_condense(llm, recent_events)\n+\n+            if not new_recent_events or len(new_recent_events) == 0:\n+                logger.debug('Condensation failed: new_recent_events is empty')\n+                return [], False\n+\n+            # re-generate the action prompt with the condensed events\n+            new_action_prompt = self.action_prompt(new_recent_events)\n+\n+            # hope for the best\n+            return new_recent_events, True\n+\n         except Exception as e:\n-            logger.error('Error condensing thoughts: %s', str(e), exc_info=False)\n+            logger.error('Condensation failed: %s', str(e), exc_info=False)\n+            return [], False\n+\n+    def _attempt_condense(\n+        self,\n+        llm: LLM,\n+        recent_events: list[dict],\n+    ) -> list[dict] | None:\n+        \"\"\"\n+        Attempts to condense the recent events by splitting them in half and summarizing the first half.\n+\n+        Parameters:\n+        - llm: The llm to use for summarization.\n+        - default_events: The list of default events to include in the prompt.\n+        - recent_events: The list of recent events to include in the prompt.\n+\n+        Returns:\n+        - The condensed recent events if successful, None otherwise.\n+        \"\"\"\n \n-            # TODO If the llm fails with ContextWindowExceededError, we can try to condense the monologue chunk by chunk\n-            raise\n+        # Split events\n+        midpoint = len(recent_events) // 2\n+        first_half = recent_events[:midpoint].copy()\n+        second_half = recent_events[midpoint:].copy()\n+\n+        # attempt to condense the first half of the recent events\n+        summarize_prompt = self.summarize_prompt(first_half)\n+\n+        # send the summarize prompt to the LLM\n+        messages = [{'content': summarize_prompt, 'role': 'user'}]\n+        response = llm.completion(messages=messages)\n+        response_content = response['choices'][0]['message']['content']\n+\n+        # the new list of recent events will be source events or summarize actions\n+        condensed_events = parse_summary_response(response_content)\n+\n+        # new recent events list\n+        if (\n+            not condensed_events\n+            or not isinstance(condensed_events, list)\n+            or len(condensed_events) == 0\n+        ):\n+            return None\n+\n+        condensed_events.extend(second_half)\n+        return condensed_events\n+\n+    def _needs_condense(self, **kwargs):\n+        \"\"\"\n+        Checks if the prompt needs to be condensed based on the token count against the limits of the llm passed in the call.\n+\n+        Parameters:\n+        - llm: The llm to use for checking the token count.\n+        - action_prompt: The prompt to check for token count. If not provided, it will attempt to generate it using the available arguments.\n+        - default_events: The list of default events to include in the prompt.\n+        - recent_events: The list of recent events to include in the prompt.\n+\n+        Returns:\n+        - True if the prompt needs to be condensed, False otherwise.\n+        \"\"\"\n+        llm = kwargs.get('llm')\n+        action_prompt = kwargs.get('action_prompt')\n+\n+        if not llm:\n+            logger.warning(\"Missing argument 'llm', cannot check token count.\")\n+            return False\n+\n+        if not action_prompt:\n+            # Attempt to generate the action_prompt using the available arguments\n+            recent_events = kwargs.get('recent_events', [])\n+\n+            action_prompt = self.action_prompt(recent_events)\n+\n+        token_count = llm.get_token_count([{'content': action_prompt, 'role': 'user'}])\n+        return token_count >= self.get_token_limit(llm)\n+\n+    def get_token_limit(self, llm: LLM) -> int:\n+        \"\"\"\n+        Returns the token limit to use for the llm passed in the call.\n+\n+        Parameters:\n+        - llm: The llm to get the token limit from.\n+\n+        Returns:\n+        - The token limit of the llm.\n+        \"\"\"\n+        return llm.max_input_tokens - MAX_TOKEN_COUNT_PADDING\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Revert all current test artifact changes before regenerating to ensure a clean slate. The script functions well if tests pass without invoking the real LLM. Your approach to condensing events is promising, but make sure to address edge cases, like handling a last observation significantly larger than others. Its essential to maintain clarity when summarizing, especially with potential ContextWindowExceeded errors. Iterating on these strategies will be necessary as we refine the implementation further."
      }
    ],
    "codediff_length": 12382
  },
  {
    "index": 55,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/interpreter/code_interpreters/container_utils/auto_remove.py b/interpreter/code_interpreters/container_utils/auto_remove.py\nnew file mode 100644\nindex 0000000000..a717151574\n--- /dev/null\n+++ b/interpreter/code_interpreters/container_utils/auto_remove.py\n@@ -0,0 +1,68 @@\n+import threading\n+import time\n+from functools import wraps\n+\n+def access_aware(cls):\n+    class AccessAwareWrapper:\n+        def __init__(self, wrapped, auto_remove_timeout, close_callback=None):\n+            self._wrapped = wrapped \n+            self._last_accessed = time.time()\n+            self._auto_remove = auto_remove_timeout is not None\n+            self._timeout = auto_remove_timeout\n+            self.close_callback = close_callback  # Store the callback\n+            if self._auto_remove:\n+                self._monitor_thread = threading.Thread(target=self._monitor_object, daemon=True)\n+                self._monitor_thread.start()\n+\n+        def _monitor_object(self):\n+            while True:\n+                time.sleep(1)  # Check every second\n+                if self._auto_remove and self.check_timeout():\n+                    # If a close_callback is defined, call it\n+                    if self.close_callback:\n+                        try:\n+                            self.close_callback()  # Call the callback\n+                        except Exception as e:\n+                            # Log or handle the exception as required\n+                            return f\"An error occurred during callback: {e}\"\n+                    \n+                    try:\n+                        self._wrapped.stop()\n+                    except Exception:\n+                        continue # why care? we are removing it anyway\n+\n+                    # If the wrapped object has a __del__ method, call it\n+                    if self._wrapped and hasattr(self._wrapped, '__del__'):\n+                        try:\n+                            self._wrapped.__del__()\n+                        except Exception as e:\n+                            # Log or handle the exception as required\n+                            return f\"An error occurred during deletion: {e}\"\n+\n+                    # Remove the strong reference to the wrapped object. this makes it go bye bye.\n+                    self._wrapped = None\n+                    break\n+\n+        def touch(self):\n+            self._last_accessed = time.time()\n+\n+        def check_timeout(self):\n+            return time.time() - self._last_accessed > self._timeout\n+\n+        def __getattr__(self, attr):\n+            if self._wrapped is None:\n+                raise ValueError(\"Object has been removed due to inactivity.\")\n+            self.touch()  # Update last accessed time\n+            return getattr(self._wrapped, attr)  # Use the actual object here\n+\n+        def __del__(self):\n+            if self._auto_remove:\n+                self._monitor_thread.join()  # Ensure the monitoring thread is cleaned up\n+\n+    @wraps(cls)\n+    def wrapper(*args, **kwargs):\n+        auto_remove_timeout = kwargs.pop('auto_remove_timeout', None)  # Extract the auto_remove_timeout argument\n+        close_callback = kwargs.pop('close_callback', None)  # Extract the close_callback argument\n+        obj = cls(*args, **kwargs)  # Create an instance of the original class\n+        return AccessAwareWrapper(obj, auto_remove_timeout, close_callback)  # Wrap it\n+    return wrapper\ndiff --git a/interpreter/code_interpreters/container_utils/build_image.py b/interpreter/code_interpreters/container_utils/build_image.py\nnew file mode 100644\nindex 0000000000..12d95286d6\n--- /dev/null\n+++ b/interpreter/code_interpreters/container_utils/build_image.py\n@@ -0,0 +1,108 @@\n+import os\n+import json\n+import hashlib\n+import subprocess\n+from docker import DockerClient\n+from docker.errors import DockerException\n+from rich import print as Print\n+\n+def get_files_hash(*file_paths):\n+    \"\"\"Return the SHA256 hash of multiple files.\"\"\"\n+    hasher = hashlib.sha256()\n+    for file_path in file_paths:\n+        with open(file_path, \"rb\") as f:\n+            while chunk := f.read(4096):\n+                hasher.update(chunk)\n+    return hasher.hexdigest()\n+\n+\n+def build_docker_images(\n+    dockerfile_dir = os.path.join(os.path.abspath(os.path.dirname(os.path.dirname(__file__))), \"dockerfiles\")\n+,\n+):\n+    \"\"\"\n+    Builds a Docker image for the Open Interpreter runtime container if needed.\n+\n+    Args:\n+        dockerfile_dir (str): The directory containing the Dockerfile and requirements.txt files.\n+\n+    Returns:\n+        None\n+    \"\"\"\n+    try:\n+        client = DockerClient.from_env()\n+    except DockerException:\n+        Print(\"[bold red]ERROR[/bold red]: Could not connect to Docker daemon. Is Docker Engine installed and running?\")\n+        Print(\n+            \"\\nFor information on Docker installation, visit: https://docs.docker.com/engine/install/ and follow the instructions for your system.\"\n+        )\n+        return\n+\n+    image_name = \"openinterpreter-runtime-container\"\n+    hash_file_path = os.path.join(dockerfile_dir, \"hash.json\")\n+\n+    dockerfile_name = \"Dockerfile\"\n+    requirements_name = \"requirements.txt\"\n+    dockerfile_path = os.path.join(dockerfile_dir, dockerfile_name)\n+    requirements_path = os.path.join(dockerfile_dir, requirements_name)\n+\n+    if not os.path.exists(dockerfile_path) or not os.path.exists(requirements_path):\n+        Print(\"ERROR: Dockerfile or requirements.txt not found. Did you delete or rename them?\")\n+        raise RuntimeError(\n+            \"No container Dockerfiles or requirements.txt found. Make sure they are in the dockerfiles/ subdir of the module.\"\n+        )\n+\n+    current_hash = get_files_hash(dockerfile_path, requirements_path)\n+\n+    stored_hashes = {}\n+    if os.path.exists(hash_file_path):\n+        with open(hash_file_path, \"rb\") as f:\n+            stored_hashes = json.load(f)\n+\n+    original_hash = stored_hashes.get(\"original_hash\")\n+    previous_hash = stored_hashes.get(\"last_hash\")\n+\n+    if current_hash == original_hash:\n+        images = client.images.list(name=image_name, all=True)\n+        if not images:\n+            Print(\"Downloading default image from Docker Hub, please wait...\")\n+            \n+            subprocess.run([\"docker\", \"pull\", \"unaidedelf/openinterpreter-runtime-container:latest\"])\n+            subprocess.run([\"docker\", \"tag\", \"unaidedelf/openinterpreter-runtime-container:latest\", image_name ],\n+                       check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n+    elif current_hash != previous_hash:\n+        Print(\"Dockerfile or requirements.txt has changed. Building container...\")\n+\n+        try:\n+            # Run the subprocess without capturing stdout and stderr\n+            # This will allow Docker's output to be printed to the console in real-time\n+            subprocess.run(\n+                [\n+                    \"docker\",\n+                    \"build\",\n+                    \"-t\",\n+                    f\"{image_name}:latest\",\n+                    dockerfile_dir,\n+                ],\n+                check=True,  # This will raise a CalledProcessError if the command returns a non-zero exit code\n+                text=True,\n+            )\n+\n+            # Update the stored current hash\n+            stored_hashes[\"last_hash\"] = current_hash\n+            with open(hash_file_path, \"w\") as f:\n+                json.dump(stored_hashes, f)\n+\n+        except subprocess.CalledProcessError:\n+            # Suppress Docker's error messages and display your own error message\n+            Print(\"Docker Build Error: Building Docker image failed. Please review the error message above and resolve the issue.\")\n+\n+        except FileNotFoundError:\n+            Print(\"ERROR: The 'docker' command was not found on your system.\")\n+            Print(\n+                \"Please ensure Docker Engine is installed and the 'docker' command is available in your PATH.\"\n+            )\n+            Print(\n+                \"For information on Docker installation, visit: https://docs.docker.com/engine/install/\"\n+            )\n+            Print(\"If Docker is installed, try starting a new terminal session.\")\ndiff --git a/interpreter/code_interpreters/container_utils/container_utils.py b/interpreter/code_interpreters/container_utils/container_utils.py\nnew file mode 100644\nindex 0000000000..da9c23a9f5\n--- /dev/null\n+++ b/interpreter/code_interpreters/container_utils/container_utils.py\n@@ -0,0 +1,255 @@\n+\"\"\"wrapper classes of the Docker python sdk which allows interaction like its a subprocess object.\"\"\"\n+import os\n+import re\n+import select\n+import struct\n+import threading\n+import time\n+\n+\n+# Third-party imports       \n+import appdirs\n+import docker\n+from docker.utils import kwargs_from_env\n+from docker.tls import TLSConfig\n+from rich import print as Print\n+\n+# Modules\n+from .auto_remove import access_aware\n+\n+class DockerStreamWrapper:\n+    def __init__(self, exec_id, sock):\n+        self.exec_id = exec_id\n+        self._sock = sock\n+        self._stdout_r, self._stdout_w = os.pipe()\n+        self._stderr_r, self._stderr_w = os.pipe()\n+        self.stdout = self.Stream(self, self._stdout_r)\n+        self.stderr = self.Stream(self, self._stderr_r)\n+\n+        ## stdin pipe and fd. dosent need a pipe, but its easier and thread safe and less mem intensive than a queue.Queue()\n+        self._stdin_r, self._stdin_w = os.pipe()  # Pipe for stdin\n+        self.stdin = os.fdopen(self._stdin_w, 'w')\n+        self._stdin_buffer = b\"\"  # Buffer for stdin data. more complex = better fr\n+\n+        ## start recieving thread to watch socket, and send data from stdin pipe.\n+        self._stop_event = threading.Event()\n+        self._thread = threading.Thread(target=self._listen, daemon=True)\n+        self._thread.start()\n+    \n+    class Stream:\n+        def __init__(self, parent, read_fd):\n+            self.parent = parent\n+            self._read_fd = read_fd\n+            self._buffer = \"\"\n+\n+        def readline(self):\n+            while '\\n' not in self._buffer:\n+                ready_to_read, _, _ = select.select([self._read_fd], [], [], None)\n+                if not ready_to_read:\n+                    return ''\n+                chunk = os.read(self._read_fd, 1024).decode('utf-8')\n+                self._buffer += chunk\n+\n+            newline_pos = self._buffer.find('\\n')\n+            line = self._buffer[:newline_pos]\n+            self._buffer = self._buffer[newline_pos + 1:]\n+            return line\n+\n+    def _listen(self):\n+        while not self._stop_event.is_set():\n+            ready_to_read, _, _ = select.select([self._sock, self._stdin_r], [], [], None)\n+            \n+            for s in ready_to_read:\n+                if s == self._sock:\n+                    raw_data = self._sock.recv(2048)\n+                    stdout, stderr = self.demux_docker_stream(raw_data)\n+                    os.write(self._stdout_w, stdout.encode())\n+                    os.write(self._stderr_w, stderr.encode())\n+                elif s == self._stdin_r:\n+                    # Read from the read end of the stdin pipe and add to the buffer\n+                    data_to_write = os.read(self._stdin_r, 2048).decode('utf-8')\n+                    \n+                    # Remove escape characters for quotes but leave other backslashes untouched\n+                    data_to_write =  re.sub(r'\\\\([\\'\"])', r'\\1', data_to_write)\n+\n+                    data_to_write = data_to_write.replace('\\\\n', '\\n')\n+\n+                    self._stdin_buffer += data_to_write.encode()\n+\n+                    # Check for newline and send line by line\n+                    while b'\\n' in self._stdin_buffer:\n+                        newline_pos = self._stdin_buffer.find(b'\\n')\n+                        line = self._stdin_buffer[:newline_pos + 1]  # Include the newline\n+                        self._stdin_buffer = self._stdin_buffer[newline_pos + 1:]\n+\n+\n+                        # Send the line to the Docker container\n+                        self._sock.sendall(line)\n+\n+\n+    def demux_docker_stream(self, data):\n+        stdout = \"\"\n+        stderr = \"\"\n+        offset = 0\n+        while offset + 8 <= len(data):\n+            header = data[offset:offset + 8]\n+            stream_type, length = struct.unpack('>BxxxL', header)\n+            offset += 8\n+            chunk = data[offset:offset + length].decode('utf-8')\n+            offset += length\n+            if stream_type == 1:\n+                stdout += chunk\n+            elif stream_type == 2:\n+                stderr += chunk\n+\n+        return stdout, stderr\n+\n+    def flush(self):\n+        pass\n+\n+    def terminate(self):\n+        self._stop_event.set()\n+        self._thread.join()\n+        os.close(self._stdout_r)\n+        os.close(self._stdout_w)\n+        os.close(self._stderr_r)\n+        os.close(self._stderr_w)\n+\n+\n+# The `@access_aware` decorator enables automatic container cleanup based on activity monitoring.\n+# It functions under the following conditions:\n+# 1. The container is subject to removal when it remains unaccessed beyond the duration specified by `auto_remove_timeout`.\n+# 2. This feature necessitates a non-None argument; absence of a valid argument renders this functionality inactive.\n+# 3. During interactive sessions, the auto-removal feature is disabled to prevent unintended interruptions.\n+# 4. The \"INTERPRETER_CONTAINER_TIMEOUT\" environment variable allows customization of the timeout period. \n+#    It accepts an integer value representing the desired timeout in seconds.\n+# 5. In the event of an unexpected program termination, the container is still ensured to be removed,\n+#    courtesy of the integration with the `atexit` module, safeguarding system resources from being unnecessarily occupied.\n+@access_aware \n+class DockerProcWrapper:\n+    def __init__(self, command, session_id, auto_remove_timeout=None, close_callback=None, mount=False): ## Mounting isnt implemented in main code, but i did it here prior so we just hide it behind a flag for now.\n+        \n+        # Docker stuff\n+        client_kwargs = kwargs_from_env()\n+        if client_kwargs.get('tls'):\n+            client_kwargs['tls'] = TLSConfig(**client_kwargs['tls'])\n+        self.client = docker.APIClient(**client_kwargs)\n+        self.image_name = \"openinterpreter-runtime-container:latest\"\n+        self.exec_id = None\n+        self.exec_socket = None\n+\n+        # close callback\n+        self.close_callback = close_callback\n+\n+        # session info\n+        self.session_id = session_id\n+        self.session_path = os.path.join(appdirs.user_data_dir(\"Open Interpreter\"), \"sessions\", session_id)\n+        self.mount = mount\n+\n+\n+        # Initialize container\n+        self.init_container()\n+\n+        self.init_exec_instance()\n+        \n+\n+        self.wrapper = DockerStreamWrapper(self.exec_id, self.exec_socket)\n+        self.stdout = self.wrapper.stdout\n+        self.stderr = self.wrapper.stderr\n+        self.stdin = self.wrapper.stdin\n+\n+        self.stdin.write(command + \"\\n\")\n+\n+    def init_container(self):\n+        self.container = None\n+        try:\n+            containers = self.client.containers(\n+                filters={\"label\": f\"session_id={self.session_id}\"}, all=True)\n+            if containers:\n+                self.container = containers[0]\n+                container_id = self.container.get('Id')\n+                container_info = self.client.inspect_container(container_id)\n+                if container_info.get('State', {}).get('Running') is False:\n+                    self.client.start(container=container_id)\n+                    self.wait_for_container_start(container_id)\n+            else:\n+                if self.mount:\n+\n+                    os.makedirs(self.session_path, exist_ok=True)\n+\n+                    host_config = self.client.create_host_config(\n+                        binds={self.session_path: {'bind': '/mnt/data', 'mode': 'rw'}}\n+                    )\n+                else:\n+                    host_config = None\n+                \n+                self.container = self.client.create_container(\n+                    image=self.image_name,\n+                    detach=True,\n+                    labels={'session_id': os.path.basename(self.session_path)},\n+                    host_config=host_config,\n+                    user=\"docker\",\n+                    stdin_open=True,\n+                    tty=False\n+                )\n+\n+                self.client.start(container=self.container.get('Id'))\n+                self.wait_for_container_start(self.container.get('Id'))\n+\n+\n+        except Exception as e:\n+            print(f\"An error occurred: {e}\")\n+\n+    def init_exec_instance(self):\n+        if self.container:\n+            container_info = self.client.inspect_container(self.container.get('Id'))\n+\n+            if container_info.get(\"State\").get('Running') is False: # Not sure of the cause of this, but this works for now.\n+                self.client.start(self.container.get(\"Id\"))\n+\n+            self.exec_id = self.client.exec_create(\n+                self.container.get(\"Id\"),\n+                cmd=\"/bin/bash\",\n+                stdin=True,\n+                stdout=True,\n+                stderr=True,\n+                workdir=\"/mnt/data\",\n+                user=\"docker\",\n+                tty=False\n+\n+            )['Id']\n+            # when socket=True, this returns a socketIO socket, so we just kinda hijack the underlying socket\n+            # since docker sets up the socketio wierd and tries to make it hard to mess with and write to.\n+            # We make the socket \"Cooperative\"\n+            self.exec_socket = self.client.exec_start(\n+                self.exec_id, socket=True, tty=False, demux=False)._sock  # type: ignore\n+\n+\n+    def wait_for_container_start(self, container_id, timeout=30):\n+        start_time = time.time()\n+        while True:\n+            container_info = self.client.inspect_container(container_id)\n+            if container_info.get('State', {}).get('Running') is True:\n+                return True\n+            elif time.time() - start_time > timeout:\n+                raise TimeoutError(\n+                    \"Container did not start within the specified timeout.\")\n+            time.sleep(1)\n+    \n+    def terminate(self):\n+        self.wrapper.terminate()\n+        self.client.stop(self.container.get(\"Id\"))\n+        self.client.remove_container(self.container.get(\"Id\"))\n+\n+    def stop(self):\n+        self.wrapper.terminate()\n+        self.client.stop(self.container.get(\"Id\"), 30)\n+\n+\n+    def __del__(self):\n+        self.terminate()\n+\n+\n+\n+\n+\ndiff --git a/interpreter/code_interpreters/container_utils/download_file.py b/interpreter/code_interpreters/container_utils/download_file.py\nnew file mode 100644\nindex 0000000000..dc3047e5d7\n--- /dev/null\n+++ b/interpreter/code_interpreters/container_utils/download_file.py\n@@ -0,0 +1,52 @@\n+import docker\n+import tarfile\n+import os\n+import tempfile\n+import appdirs\n+from tqdm import tqdm\n+\n+\n+def download_file_from_container(container_id, file_path_in_container, local_dir):\n+    # Check if the specified local directory exists\n+    if not os.path.isdir(local_dir):\n+        # If not, use a \"Downloads\" folder in the user's data directory as the default\n+        local_dir = os.path.join(appdirs.user_data_dir(), \"Open Interpreter\", \"downloads\")\n+        print(f\"file is being downloaded to {local_dir}\")\n+        # Create the Downloads directory if it doesn't exist\n+        os.makedirs(local_dir, exist_ok=True)\n+    \n+    # Create a Docker client\n+    client = docker.from_env()\n+\n+    # Get the container\n+    container = client.containers.get(container_id)\n+\n+    # Use get_archive to get a file from the container\n+    stream, stat = container.get_archive(os.path.join(\"/mnt/data/\", file_path_in_container))\n+    \n+    # Get the file name from the stat info\n+    file_name = os.path.basename(stat['name'])\n+    # Get the size of the file from the stat object for the progress bar\n+    total_size = stat['size']\n+    # Initialize the progress bar\n+    pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\")\n+\n+    # Update the progress bar within the loop where chunks are being written\n+    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n+        for chunk in stream:\n+            temp_file.write(chunk)\n+            pbar.update(len(chunk))\n+            temp_file.flush()\n+    pbar.close()\n+    \n+    # Open the temporary tar file for reading\n+    with tarfile.open(temp_file.name, 'r') as tar:\n+        # Extract the file to the local directory\n+        tar.extractall(path=local_dir)\n+    \n+    # Delete the temporary tar file\n+    os.remove(temp_file.name)\n+\n+    # Return the path to the extracted file\n+    return os.path.join(local_dir, file_name)\n+\ndiff --git a/interpreter/code_interpreters/container_utils/upload_file.py b/interpreter/code_interpreters/container_utils/upload_file.py\nnew file mode 100644\nindex 0000000000..ecf79fd1de\n--- /dev/null\n+++ b/interpreter/code_interpreters/container_utils/upload_file.py\n@@ -0,0 +1,62 @@\n+\"\"\"Short function to upload a file to a docker container via the docker module. yes its hacky, but its easy and I didnt want to over complicate.\"\"\"\n+import io\n+import tarfile\n+import os\n+import docker\n+from tqdm import tqdm\n+\n+def copy_file_to_container(container_id, local_path, path_in_container, pbar=True):\n+    # Validate input\n+    if not os.path.exists(local_path):\n+        raise ValueError(f\"The specified local path {local_path} does not exist.\")\n+    \n+    # Create a Docker client\n+    client = docker.APIClient()\n+\n+    # Get the container\n+    container = client.containers()[0]\n+\n+    container_id = container.get(\"Id\")\n+\n+    # Get the directory path and name in the container\n+    dir_path_in_container = os.path.dirname(path_in_container)\n+    name = os.path.basename(path_in_container)\n+\n+    # Calculate the total size of the content to be uploaded\n+    total_size = 0\n+    for dirpath, dirnames, filenames in os.walk(local_path):\n+        for filename in filenames:\n+            filepath = os.path.join(dirpath, filename)\n+            total_size += os.path.getsize(filepath)\n+\n+    # Create a tarball in memory\n+    file_data = io.BytesIO()\n+    with tarfile.open(fileobj=file_data, mode='w') as tar:\n+        # Check if the local path is a directory or a file\n+        if os.path.isdir(local_path):\n+            # Add the entire directory to the tar archive with the specified name\n+            tar.add(local_path, arcname=name)\n+        else:\n+            # Add the local file to the tar archive with the specified file name\n+            tar.add(local_path, arcname=name)\n+\n+    # Seek to the beginning of the in-memory tarball\n+    file_data.seek(0)\n+\n+    # Create a tqdm progress bar\n+    with tqdm(total=total_size, unit='B', unit_scale=True, desc='Uploading') as pbar:\n+        # Define a generator to read the file data in chunks and update the progress bar\n+        def file_data_with_progress():\n+            chunk_size = 1024  # Define an appropriate chunk size\n+            while True:\n+                chunk = file_data.read(chunk_size)\n+                if not chunk:\n+                    break\n+                pbar.update(len(chunk))\n+                yield chunk\n+            file_data.close()\n+\n+        # Use put_archive to copy the file or directory into the container\n+        client.put_archive(container=container_id, path=dir_path_in_container, data=file_data_with_progress())\n+    \n+    return path_in_container\ndiff --git a/interpreter/code_interpreters/dockerfiles/Dockerfile b/interpreter/code_interpreters/dockerfiles/Dockerfile\nnew file mode 100644\nindex 0000000000..761207e7c0\n--- /dev/null\n+++ b/interpreter/code_interpreters/dockerfiles/Dockerfile\n@@ -0,0 +1,78 @@\n+# Base image\n+FROM debian:bullseye\n+\n+# Set environment variables to disable prompts\n+ENV DEBIAN_FRONTEND=noninteractive\n+\n+# Create user with sudo only in the container for security.\n+RUN useradd docker && echo \"docker:docker\" | chpasswd && mkdir -p /home/docker && chown docker:docker /home/docker\n+RUN mkdir -p /mnt/data && chown -R docker:docker /mnt/data\n+\n+RUN apt-get update && apt-get install -y sudo\n+\n+RUN usermod -aG sudo docker\n+\n+RUN echo \"docker ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\n+# add write perms for user 'docker' in the correct dirs ( where it cant mess things up much. it does have passwordless sudo, so it can override this if it figures out it has sudo.)\n+RUN chown -R docker:docker /mnt/data\n+\n+# Update package list and install essential utilities and runtimes\n+RUN sudo apt-get update && apt-get install -y \\\n+    jq \\\n+    build-essential \\\n+    python3 \\\n+    python3-pip \\\n+    python3-dev \\\n+    curl \\\n+    libssl-dev \\\n+    libexpat1 \\\n+    r-base \\\n+    r-base-dev\n+\n+# install nodejs stuff\n+RUN sudo apt-get update\n+RUN sudo apt-get install -y ca-certificates curl gnupg\n+RUN sudo mkdir -p /etc/apt/keyrings\n+RUN curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n+RUN NODE_MAJOR=20\n+RUN echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_18.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n+RUN sudo apt-get update\n+RUN sudo apt-get install nodejs -y\n+\n+\n+# Copy unified requirements.txt into the image\n+COPY ./requirements.txt /tmp/\n+\n+# Install Python packages\n+RUN awk '/\\[python\\]/, /^\\s*$/' /tmp/requirements.txt | grep -v '^\\[python\\]' > /tmp/python_requirements.txt && \\\n+    pip3 install --no-cache-dir -r /tmp/python_requirements.txt --no-input\n+\n+# Create an initial empty package.json\n+RUN echo '{}' > /tmp/package.json\n+\n+RUN awk '/\\[node\\]/, /^\\s*$/' /tmp/requirements.txt | grep -v '^\\[node\\]' > /tmp/node_requirements.txt && \\\n+    while read -r package; do \\\n+        name=$(echo $package | grep -Eo \"^[a-zA-Z0-9_-]+\") && \\\n+        version=$(echo $package | grep -Eo \"[>=<]+[0-9.]+\"); \\\n+        if [ -z \"$version\" ]; then version=\"*\"; fi && \\\n+        jq \".dependencies.\\\"$name\\\" = \\\"$version\\\"\" /tmp/package.json > /tmp/package_temp.json && mv /tmp/package_temp.json /tmp/package.json; \\\n+    done < /tmp/node_requirements.txt\n+\n+\n+\n+# Install NodeJS packages\n+RUN npm install --save --yes --no-package-lock --prefix /tmp\n+\n+# Install R packages\n+RUN awk '/\\[r\\]/,/^\\s*$/' /tmp/requirements.txt | grep -v '^\\[r\\]' > /tmp/r_requirements.txt && \\\n+    while read -r package; do \\\n+        R -e \"install.packages('$package', repos='http://cran.rstudio.com/', dependencies=TRUE, INSTALL_opts = c('--no-tests'))\"; \\\n+    done < /tmp/r_requirements.txt\n+\n+# Install NodeJS packages\n+\n+# Remove temporary files\n+RUN rm /tmp/package.json /tmp/node_requirements.txt /tmp/python_requirements.txt /tmp/r_requirements.txt\n+\n+# workdir set. pray it dosent try to go exploring.\n+WORKDIR /mnt/data\ndiff --git a/interpreter/code_interpreters/dockerfiles/docker_manager.py b/interpreter/code_interpreters/dockerfiles/docker_manager.py\nnew file mode 100644\nindex 0000000000..6446fd106b\n--- /dev/null\n+++ b/interpreter/code_interpreters/dockerfiles/docker_manager.py\n@@ -0,0 +1,59 @@\n+import os\n+\n+class DockerManager:\n+    \n+    here = os.path.abspath(__file__)\n+    requirements_file = os.path.normpath(os.path.join(here, \"..\", \"requirements.txt\"))\n+    docker_file = os.path.normpath(os.path.join(here,\"..\", \"Dockerfile\"))\n+\n+    def add_dependency(language, dependency):\n+        lines = []\n+        language_section_found = False\n+        dependency_name = dependency.split('==')[0]\n+\n+        with open(DockerManager.requirements_file, 'r') as f:\n+            lines = f.readlines()\n+\n+        for i, line in enumerate(lines):\n+            if line.strip() == f'[{language}]':\n+                language_section_found = True\n+            elif language_section_found:\n+                if line.strip() == '' or line.strip().startswith('['):\n+                    break\n+                existing_dependency_name = line.strip().split('==')[0]\n+                if existing_dependency_name == dependency_name:\n+                    print(f\"Dependency {dependency} already exists under [{language}].\")\n+                    return\n+\n+        if not language_section_found:\n+            print(f\"Error: Language section [{language}] not found. Please add it first.\")\n+            return\n+\n+        lines.insert(i, f\"{dependency}\\n\")\n+\n+        with open(DockerManager.requirements_file, 'w') as f:\n+            f.writelines(lines)\n+\n+    def remove_dependency(language, dependency_name):\n+        lines = []\n+        language_section_found = False\n+\n+        with open(DockerManager.requirements_file, 'r') as f:\n+            lines = f.readlines()\n+\n+        for i, line in enumerate(lines):\n+            if line.strip() == f'[{language}]':\n+                language_section_found = True\n+            elif language_section_found:\n+                if line.strip() == '' or line.strip().startswith('['):\n+                    break\n+                existing_dependency_name = line.strip().split('==')[0]\n+                if existing_dependency_name == dependency_name:\n+                    del lines[i]\n+                    break\n+        else:\n+            raise ValueError(f\"Error: Language section [{language}] or dependency {dependency_name} not found.\")\n+\n+        with open(DockerManager.requirements_file, 'w') as f:\n+            f.writelines(lines)\n+\ndiff --git a/interpreter/terminal_interface/magic_commands.py b/interpreter/terminal_interface/magic_commands.py\nindex fddfbc9335..5973659f7a 100644\n--- a/interpreter/terminal_interface/magic_commands.py\n+++ b/interpreter/terminal_interface/magic_commands.py\n@@ -74,33 +90,153 @@ def handle_debug(self, arguments=None):\n     else:\n         display_markdown_message(\"> Unknown argument to debug command.\")\n \n+\n def handle_reset(self, arguments):\n     self.reset()\n     display_markdown_message(\"> Reset Done\")\n \n+\n def default_handle(self, arguments):\n     display_markdown_message(\"> Unknown command\")\n     handle_help(self,arguments)\n \n def handle_save_message(self, json_path):\n     if json_path == \"\":\n-      json_path = \"messages.json\"\n+        json_path = \"messages.json\"\n     if not json_path.endswith(\".json\"):\n-      json_path += \".json\"\n+        json_path += \".json\"\n     with open(json_path, 'w') as f:\n-      json.dump(self.messages, f, indent=2)\n+        json.dump(self.messages, f, indent=2)\n+\n+    display_markdown_message(\n+        f\"> messages json export to {os.path.abspath(json_path)}\")\n \n-    display_markdown_message(f\"> messages json export to {os.path.abspath(json_path)}\")\n \n def handle_load_message(self, json_path):\n     if json_path == \"\":\n-      json_path = \"messages.json\"\n+        json_path = \"messages.json\"\n     if not json_path.endswith(\".json\"):\n-      json_path += \".json\"\n+        json_path += \".json\"\n     with open(json_path, 'r') as f:\n-      self.messages = json.load(f)\n+        self.messages = json.load(f)\n+\n+    display_markdown_message(\n+        f\"> messages json loaded from {os.path.abspath(json_path)}\")\n+\n+def handle_container_upload(self,type=None, *args):\n+    def is_gui_available():\n+        try:\n+            from PyQt5.QtWidgets import QApplication\n+            app = QApplication([])\n+            del app\n+            return True\n+        except Exception as e:\n+            print(f\"An error occurred: {str(e)}\")\n+            return False\n+        \n+    args = list(args)\n+    if self.use_containers:\n+        try:\n+            client = docker.APIClient()\n+        except:\n+            error_message = (\n+                \"We were not able to connect to the Docker Container daemon. \"\n+                \"Please ensure Docker is installed and running. If you have not run any code yet, \"\n+                \"you will need to in order to start a container.\"\n+            )\n+            display_markdown_message(f\"{error_message}\")\n+            return\n+        if len(args) == 0:\n+            if is_gui_available():\n+                try:\n+                    from .components.file_dialog import FileDialog\n+\n+                    fd = FileDialog()\n+                    if type is not None:\n+                        path = fd.get_path(type=type)\n+                    else:\n+                        path = fd.get_path(type=None)\n+                    if path is not None: # if none, they exited\n+                        \n+                        args.append(path)\n+                    else: # We shall now exit on them out of spite\n+                        return\n+                except ImportError as e:\n+                    Print(f\"Internal import error {e}\")\n+                    return \n+            else:\n+                Print(f\"No GUI available for your system.\\n please provide a filepath manually. use the command %upload <filetype (file or folder)> <path>\")\n+                return\n+                 \n+        for filepath in args:\n+            if os.path.exists(filepath):\n+                session_id = self.session_id\n+                if session_id is None:\n+                    Print(\"[BOLD] [RED] No session found. Please run any code to start one. [/RED] [/BOLD]\")\n+                    return\n+                containers = client.containers(filters={\"label\": f\"session_id={session_id}\"})\n+                if containers:\n+                    container_id = containers[0]['Id']\n+                    # /mnt/data is default workdir for container\n+                    copy_file_to_container(\n+                        container_id=container_id, local_path=filepath, path_in_container=f\"/mnt/data/{os.path.basename(filepath)}\"\n+                    )\n+                    success_message = f\"[{filepath}](#) successfully uploaded to container in dir `/mnt/data`.\"\n+                    display_markdown_message(success_message)\n+                else:\n+                    no_container_message = (\n+                        \"No container found to upload to. Please run any code to start one. \"\n+                        \"This will be fixed in a later update.\"\n+                    )\n+                    display_markdown_message(f\"**'{no_container_message}'**\")\n+            else:\n+                file_not_found_message = f\"File `{filepath}` does not exist.\"\n+                display_markdown_message(file_not_found_message)\n+    else:\n+        ignore_command_message = \"File uploads are only used when using containerized code execution. Ignoring command.\"\n+        display_markdown_message(f\"**{ignore_command_message}**\")\n+        \n+def handle_container_download(self, *args):\n+    if self.use_containers:\n+        try:\n+            client = docker.APIClient()\n+        except Exception as e:\n+            print(\"[BOLD][RED]Unable to connect to the Docker Container daemon. Please ensure Docker is installed and running. ignoring command[/RED][/BOLD]\")\n+            return\n+        \n+        session_id = self.session_id\n+        if session_id is None:\n+            print(\"No session found. Please run any code to start one.\")\n+            return\n+\n+        containers = client.containers(filters={\"label\": f\"session_id={session_id}\"})\n+        if not containers:\n+            print(\"No container found to download from. Please run any code to start one.\")\n+            return\n+        \n+        container_id = containers[0]['Id']\n+\n+        # Define the local directory where the files will be downloaded.\n+        # Using 'Open Interpreter' as the appname and no author.\n+        local_dir = appdirs.user_data_dir(appname=\"Open Interpreter\")\n+\n+        for file_path_in_container in args:\n+\n+            if not file_path_in_container.startswith(\"/mnt/data\"):\n+                file_path_in_container = os.path.join(\"/mnt/data\", file_path_in_container)\n+\n+            # Construct the local file path\n+            local_file_path = os.path.join(local_dir, os.path.basename(file_path_in_container))\n+            \n+            # Attempt to download the file and handle exceptions\n+            try:\n+                download_file_from_container(container_id, file_path_in_container, local_file_path)\n+                print(f\"File downloaded to {local_file_path}\")\n+            except docker.errors.NotFound:\n+                print(f\"File {file_path_in_container} not found in the container.\")\n+    else:\n+        print(\"File downloads are only used when using containerized code execution. Ignoring command.\")\n \n-    display_markdown_message(f\"> messages json loaded from {os.path.abspath(json_path)}\")\n \n def handle_count_tokens(self, prompt):\n     messages = [{\"role\": \"system\", \"message\": self.system_message}] + self.messages\n\ndiff --git a/poetry.lock b/poetry.lock\nindex b7b908b9f8..d4d50a06d3 100644\n--- a/poetry.lock\n+++ b/poetry.lock\n@@ -1633,6 +1784,67 @@ files = [\n     {file = \"PyPika-0.48.9.tar.gz\", hash = \"sha256:838836a61747e7c8380cd1b7ff638694b7a7335345d0f559b04b2cd832ad5378\"},\n ]\n \n+[[package]]\n+name = \"pyqt5\"\n+version = \"5.15.10\"\n+description = \"Python bindings for the Qt cross platform application toolkit\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"PyQt5-5.15.10-cp37-abi3-macosx_10_13_x86_64.whl\", hash = \"sha256:93288d62ebd47b1933d80c27f5d43c7c435307b84d480af689cef2474e87e4c8\"},\n+    {file = \"PyQt5-5.15.10-cp37-abi3-manylinux_2_17_x86_64.whl\", hash = \"sha256:b89478d16d4118664ff58ed609e0a804d002703c9420118de7e4e70fa1cb5486\"},\n+    {file = \"PyQt5-5.15.10-cp37-abi3-win32.whl\", hash = \"sha256:ff99b4f91aa8eb60510d5889faad07116d3340041916e46c07d519f7cad344e1\"},\n+    {file = \"PyQt5-5.15.10-cp37-abi3-win_amd64.whl\", hash = \"sha256:501355f327e9a2c38db0428e1a236d25ebcb99304cd6e668c05d1188d514adec\"},\n+    {file = \"PyQt5-5.15.10.tar.gz\", hash = \"sha256:d46b7804b1b10a4ff91753f8113e5b5580d2b4462f3226288e2d84497334898a\"},\n+]\n+\n+[package.dependencies]\n+PyQt5-Qt5 = \">=5.15.2\"\n+PyQt5-sip = \">=12.13,<13\"\n+\n+[[package]]\n+name = \"pyqt5-qt5\"\n+version = \"5.15.2\"\n+description = \"The subset of a Qt installation needed by PyQt5.\"\n+optional = false\n+python-versions = \"*\"\n+files = [\n+    {file = \"PyQt5_Qt5-5.15.2-py3-none-macosx_10_13_intel.whl\", hash = \"sha256:76980cd3d7ae87e3c7a33bfebfaee84448fd650bad6840471d6cae199b56e154\"},\n+    {file = \"PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl\", hash = \"sha256:1988f364ec8caf87a6ee5d5a3a5210d57539988bf8e84714c7d60972692e2f4a\"},\n+    {file = \"PyQt5_Qt5-5.15.2-py3-none-win32.whl\", hash = \"sha256:9cc7a768b1921f4b982ebc00a318ccb38578e44e45316c7a4a850e953e1dd327\"},\n+    {file = \"PyQt5_Qt5-5.15.2-py3-none-win_amd64.whl\", hash = \"sha256:750b78e4dba6bdf1607febedc08738e318ea09e9b10aea9ff0d73073f11f6962\"},\n+]\n+\n+[[package]]\n+name = \"pyqt5-sip\"\n+version = \"12.13.0\"\n+description = \"The sip module support for PyQt5\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"PyQt5_sip-12.13.0-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:a7e3623b2c743753625c4650ec7696362a37fb36433b61824cf257f6d3d43cca\"},\n+    {file = \"PyQt5_sip-12.13.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:6e4ac714252370ca037c7d609da92388057165edd4f94e63354f6d65c3ed9d53\"},\n+    {file = \"PyQt5_sip-12.13.0-cp310-cp310-win32.whl\", hash = \"sha256:d5032da3fff62da055104926ffe76fd6044c1221f8ad35bb60804bcb422fe866\"},\n+    {file = \"PyQt5_sip-12.13.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:9a8cdd6cb66adcbe5c941723ed1544eba05cf19b6c961851b58ccdae1c894afb\"},\n+    {file = \"PyQt5_sip-12.13.0-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:0f85fb633a522f04e48008de49dce1ff1d947011b48885b8428838973fbca412\"},\n+    {file = \"PyQt5_sip-12.13.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:ec60162e034c42fb99859206d62b83b74f987d58937b3a82bdc07b5c3d190dec\"},\n+    {file = \"PyQt5_sip-12.13.0-cp311-cp311-win32.whl\", hash = \"sha256:205cd449d08a2b024a468fb6100cd7ed03e946b4f49706f508944006f955ae1a\"},\n+    {file = \"PyQt5_sip-12.13.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:1c8371682f77852256f1f2d38c41e2e684029f43330f0635870895ab01c02f6c\"},\n+    {file = \"PyQt5_sip-12.13.0-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:7fe3375b508c5bc657d73b9896bba8a768791f1f426c68053311b046bcebdddf\"},\n+    {file = \"PyQt5_sip-12.13.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:773731b1b5ab1a7cf5621249f2379c95e3d2905e9bd96ff3611b119586daa876\"},\n+    {file = \"PyQt5_sip-12.13.0-cp312-cp312-win32.whl\", hash = \"sha256:fb4a5271fa3f6bc2feb303269a837a95a6d8dd16be553aa40e530de7fb81bfdf\"},\n+    {file = \"PyQt5_sip-12.13.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:3a4498f3b1b15f43f5d12963accdce0fd652b0bcaae6baf8008663365827444c\"},\n+    {file = \"PyQt5_sip-12.13.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:9b984c2620a7a7eaf049221b09ae50a345317add2624c706c7d2e9e6632a9587\"},\n+    {file = \"PyQt5_sip-12.13.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:3188a06956aef86f604fb0d14421a110fad70d2a9e943dbacbfc3303f651dade\"},\n+    {file = \"PyQt5_sip-12.13.0-cp38-cp38-win32.whl\", hash = \"sha256:108a15f603e1886988c4b0d9d41cb74c9f9815bf05cefc843d559e8c298a10ce\"},\n+    {file = \"PyQt5_sip-12.13.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:db228cd737f5cbfc66a3c3e50042140cb80b30b52edc5756dbbaa2346ec73137\"},\n+    {file = \"PyQt5_sip-12.13.0-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:5338773bbaedaa4f16a73c142fb23cc18c327be6c338813af70260b756c7bc92\"},\n+    {file = \"PyQt5_sip-12.13.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:29fa9cc964517c9fc3f94f072b9a2aeef4e7a2eda1879cb835d9e06971161cdf\"},\n+    {file = \"PyQt5_sip-12.13.0-cp39-cp39-win32.whl\", hash = \"sha256:96414c93f3d33963887cf562d50d88b955121fbfd73f937c8eca46643e77bf61\"},\n+    {file = \"PyQt5_sip-12.13.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:bbc7cd498bf19e0862097be1ad2243e824dea56726f00c11cff1b547c2d31d01\"},\n+    {file = \"PyQt5_sip-12.13.0.tar.gz\", hash = \"sha256:7f321daf84b9c9dbca61b80e1ef37bdaffc0e93312edae2cd7da25b953971d91\"},\n+]\n+\n [[package]]\n name = \"pyreadline3\"\n version = \"3.4.1\"\n@@ -1915,6 +2198,214 @@ pygments = \">=2.13.0,<3.0.0\"\n [package.extras]\n jupyter = [\"ipywidgets (>=7.5.1,<9)\"]\n \n+[[package]]\n+name = \"rpds-py\"\n+version = \"0.10.6\"\n+description = \"Python bindings to Rust's persistent data structures (rpds)\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"rpds_py-0.10.6-cp310-cp310-macosx_10_7_x86_64.whl\", hash = \"sha256:6bdc11f9623870d75692cc33c59804b5a18d7b8a4b79ef0b00b773a27397d1f6\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:26857f0f44f0e791f4a266595a7a09d21f6b589580ee0585f330aaccccb836e3\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d7f5e15c953ace2e8dde9824bdab4bec50adb91a5663df08d7d994240ae6fa31\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:61fa268da6e2e1cd350739bb61011121fa550aa2545762e3dc02ea177ee4de35\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:c48f3fbc3e92c7dd6681a258d22f23adc2eb183c8cb1557d2fcc5a024e80b094\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c0503c5b681566e8b722fe8c4c47cce5c7a51f6935d5c7012c4aefe952a35eed\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:734c41f9f57cc28658d98270d3436dba65bed0cfc730d115b290e970150c540d\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:a5d7ed104d158c0042a6a73799cf0eb576dfd5fc1ace9c47996e52320c37cb7c\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-musllinux_1_2_aarch64.whl\", hash = \"sha256:e3df0bc35e746cce42579826b89579d13fd27c3d5319a6afca9893a9b784ff1b\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-musllinux_1_2_i686.whl\", hash = \"sha256:73e0a78a9b843b8c2128028864901f55190401ba38aae685350cf69b98d9f7c9\"},\n+    {file = \"rpds_py-0.10.6-cp310-cp310-musllinux_1_2_x86_64.whl\", hash = \"sha256:5ed505ec6305abd2c2c9586a7b04fbd4baf42d4d684a9c12ec6110deefe2a063\"},\n+    {file = \"rpds_py-0.10.6-cp310-none-win32.whl\", hash = \"sha256:d97dd44683802000277bbf142fd9f6b271746b4846d0acaf0cefa6b2eaf2a7ad\"},\n+    {file = \"rpds_py-0.10.6-cp310-none-win_amd64.whl\", hash = \"sha256:b455492cab07107bfe8711e20cd920cc96003e0da3c1f91297235b1603d2aca7\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-macosx_10_7_x86_64.whl\", hash = \"sha256:e8cdd52744f680346ff8c1ecdad5f4d11117e1724d4f4e1874f3a67598821069\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:66414dafe4326bca200e165c2e789976cab2587ec71beb80f59f4796b786a238\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:cc435d059f926fdc5b05822b1be4ff2a3a040f3ae0a7bbbe672babb468944722\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:8e7f2219cb72474571974d29a191714d822e58be1eb171f229732bc6fdedf0ac\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:3953c6926a63f8ea5514644b7afb42659b505ece4183fdaaa8f61d978754349e\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:2bb2e4826be25e72013916eecd3d30f66fd076110de09f0e750163b416500721\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:7bf347b495b197992efc81a7408e9a83b931b2f056728529956a4d0858608b80\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:102eac53bb0bf0f9a275b438e6cf6904904908562a1463a6fc3323cf47d7a532\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-musllinux_1_2_aarch64.whl\", hash = \"sha256:40f93086eef235623aa14dbddef1b9fb4b22b99454cb39a8d2e04c994fb9868c\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-musllinux_1_2_i686.whl\", hash = \"sha256:e22260a4741a0e7a206e175232867b48a16e0401ef5bce3c67ca5b9705879066\"},\n+    {file = \"rpds_py-0.10.6-cp311-cp311-musllinux_1_2_x86_64.whl\", hash = \"sha256:f4e56860a5af16a0fcfa070a0a20c42fbb2012eed1eb5ceeddcc7f8079214281\"},\n+    {file = \"rpds_py-0.10.6-cp311-none-win32.whl\", hash = \"sha256:0774a46b38e70fdde0c6ded8d6d73115a7c39d7839a164cc833f170bbf539116\"},\n+    {file = \"rpds_py-0.10.6-cp311-none-win_amd64.whl\", hash = \"sha256:4a5ee600477b918ab345209eddafde9f91c0acd931f3776369585a1c55b04c57\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-macosx_10_7_x86_64.whl\", hash = \"sha256:5ee97c683eaface61d38ec9a489e353d36444cdebb128a27fe486a291647aff6\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:0713631d6e2d6c316c2f7b9320a34f44abb644fc487b77161d1724d883662e31\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:b5a53f5998b4bbff1cb2e967e66ab2addc67326a274567697379dd1e326bded7\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:6a555ae3d2e61118a9d3e549737bb4a56ff0cec88a22bd1dfcad5b4e04759175\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:945eb4b6bb8144909b203a88a35e0a03d22b57aefb06c9b26c6e16d72e5eb0f0\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:52c215eb46307c25f9fd2771cac8135d14b11a92ae48d17968eda5aa9aaf5071\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:c1b3cd23d905589cb205710b3988fc8f46d4a198cf12862887b09d7aaa6bf9b9\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:64ccc28683666672d7c166ed465c09cee36e306c156e787acef3c0c62f90da5a\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-musllinux_1_2_aarch64.whl\", hash = \"sha256:516a611a2de12fbea70c78271e558f725c660ce38e0006f75139ba337d56b1f6\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-musllinux_1_2_i686.whl\", hash = \"sha256:9ff93d3aedef11f9c4540cf347f8bb135dd9323a2fc705633d83210d464c579d\"},\n+    {file = \"rpds_py-0.10.6-cp312-cp312-musllinux_1_2_x86_64.whl\", hash = \"sha256:d858532212f0650be12b6042ff4378dc2efbb7792a286bee4489eaa7ba010586\"},\n+    {file = \"rpds_py-0.10.6-cp312-none-win32.whl\", hash = \"sha256:3c4eff26eddac49d52697a98ea01b0246e44ca82ab09354e94aae8823e8bda02\"},\n+    {file = \"rpds_py-0.10.6-cp312-none-win_amd64.whl\", hash = \"sha256:150eec465dbc9cbca943c8e557a21afdcf9bab8aaabf386c44b794c2f94143d2\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-macosx_10_7_x86_64.whl\", hash = \"sha256:cf693eb4a08eccc1a1b636e4392322582db2a47470d52e824b25eca7a3977b53\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:4134aa2342f9b2ab6c33d5c172e40f9ef802c61bb9ca30d21782f6e035ed0043\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:e782379c2028a3611285a795b89b99a52722946d19fc06f002f8b53e3ea26ea9\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:2f6da6d842195fddc1cd34c3da8a40f6e99e4a113918faa5e60bf132f917c247\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:b4a9fe992887ac68256c930a2011255bae0bf5ec837475bc6f7edd7c8dfa254e\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b788276a3c114e9f51e257f2a6f544c32c02dab4aa7a5816b96444e3f9ffc336\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:caa1afc70a02645809c744eefb7d6ee8fef7e2fad170ffdeacca267fd2674f13\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:bddd4f91eede9ca5275e70479ed3656e76c8cdaaa1b354e544cbcf94c6fc8ac4\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-musllinux_1_2_aarch64.whl\", hash = \"sha256:775049dfa63fb58293990fc59473e659fcafd953bba1d00fc5f0631a8fd61977\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-musllinux_1_2_i686.whl\", hash = \"sha256:c6c45a2d2b68c51fe3d9352733fe048291e483376c94f7723458cfd7b473136b\"},\n+    {file = \"rpds_py-0.10.6-cp38-cp38-musllinux_1_2_x86_64.whl\", hash = \"sha256:0699ab6b8c98df998c3eacf51a3b25864ca93dab157abe358af46dc95ecd9801\"},\n+    {file = \"rpds_py-0.10.6-cp38-none-win32.whl\", hash = \"sha256:ebdab79f42c5961682654b851f3f0fc68e6cc7cd8727c2ac4ffff955154123c1\"},\n+    {file = \"rpds_py-0.10.6-cp38-none-win_amd64.whl\", hash = \"sha256:24656dc36f866c33856baa3ab309da0b6a60f37d25d14be916bd3e79d9f3afcf\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-macosx_10_7_x86_64.whl\", hash = \"sha256:0898173249141ee99ffcd45e3829abe7bcee47d941af7434ccbf97717df020e5\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:9e9184fa6c52a74a5521e3e87badbf9692549c0fcced47443585876fcc47e469\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5752b761902cd15073a527b51de76bbae63d938dc7c5c4ad1e7d8df10e765138\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:99a57006b4ec39dbfb3ed67e5b27192792ffb0553206a107e4aadb39c5004cd5\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:09586f51a215d17efdb3a5f090d7cbf1633b7f3708f60a044757a5d48a83b393\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:e225a6a14ecf44499aadea165299092ab0cba918bb9ccd9304eab1138844490b\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:b2039f8d545f20c4e52713eea51a275e62153ee96c8035a32b2abb772b6fc9e5\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:34ad87a831940521d462ac11f1774edf867c34172010f5390b2f06b85dcc6014\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-musllinux_1_2_aarch64.whl\", hash = \"sha256:dcdc88b6b01015da066da3fb76545e8bb9a6880a5ebf89e0f0b2e3ca557b3ab7\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-musllinux_1_2_i686.whl\", hash = \"sha256:25860ed5c4e7f5e10c496ea78af46ae8d8468e0be745bd233bab9ca99bfd2647\"},\n+    {file = \"rpds_py-0.10.6-cp39-cp39-musllinux_1_2_x86_64.whl\", hash = \"sha256:7854a207ef77319ec457c1eb79c361b48807d252d94348305db4f4b62f40f7f3\"},\n+    {file = \"rpds_py-0.10.6-cp39-none-win32.whl\", hash = \"sha256:e6fcc026a3f27c1282c7ed24b7fcac82cdd70a0e84cc848c0841a3ab1e3dea2d\"},\n+    {file = \"rpds_py-0.10.6-cp39-none-win_amd64.whl\", hash = \"sha256:e98c4c07ee4c4b3acf787e91b27688409d918212dfd34c872201273fdd5a0e18\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-macosx_10_7_x86_64.whl\", hash = \"sha256:68fe9199184c18d997d2e4293b34327c0009a78599ce703e15cd9a0f47349bba\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:3339eca941568ed52d9ad0f1b8eb9fe0958fa245381747cecf2e9a78a5539c42\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a360cfd0881d36c6dc271992ce1eda65dba5e9368575663de993eeb4523d895f\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:031f76fc87644a234883b51145e43985aa2d0c19b063e91d44379cd2786144f8\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:1f36a9d751f86455dc5278517e8b65580eeee37d61606183897f122c9e51cef3\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:052a832078943d2b2627aea0d19381f607fe331cc0eb5df01991268253af8417\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:023574366002bf1bd751ebaf3e580aef4a468b3d3c216d2f3f7e16fdabd885ed\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:defa2c0c68734f4a82028c26bcc85e6b92cced99866af118cd6a89b734ad8e0d\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-musllinux_1_2_aarch64.whl\", hash = \"sha256:879fb24304ead6b62dbe5034e7b644b71def53c70e19363f3c3be2705c17a3b4\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-musllinux_1_2_i686.whl\", hash = \"sha256:53c43e10d398e365da2d4cc0bcaf0854b79b4c50ee9689652cdc72948e86f487\"},\n+    {file = \"rpds_py-0.10.6-pp310-pypy310_pp73-musllinux_1_2_x86_64.whl\", hash = \"sha256:3777cc9dea0e6c464e4b24760664bd8831738cc582c1d8aacf1c3f546bef3f65\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-macosx_10_7_x86_64.whl\", hash = \"sha256:40578a6469e5d1df71b006936ce95804edb5df47b520c69cf5af264d462f2cbb\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:cf71343646756a072b85f228d35b1d7407da1669a3de3cf47f8bbafe0c8183a4\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:10f32b53f424fc75ff7b713b2edb286fdbfc94bf16317890260a81c2c00385dc\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:81de24a1c51cfb32e1fbf018ab0bdbc79c04c035986526f76c33e3f9e0f3356c\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ac17044876e64a8ea20ab132080ddc73b895b4abe9976e263b0e30ee5be7b9c2\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5e8a78bd4879bff82daef48c14d5d4057f6856149094848c3ed0ecaf49f5aec2\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:78ca33811e1d95cac8c2e49cb86c0fb71f4d8409d8cbea0cb495b6dbddb30a55\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:c63c3ef43f0b3fb00571cff6c3967cc261c0ebd14a0a134a12e83bdb8f49f21f\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-musllinux_1_2_aarch64.whl\", hash = \"sha256:7fde6d0e00b2fd0dbbb40c0eeec463ef147819f23725eda58105ba9ca48744f4\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-musllinux_1_2_i686.whl\", hash = \"sha256:79edd779cfc46b2e15b0830eecd8b4b93f1a96649bcb502453df471a54ce7977\"},\n+    {file = \"rpds_py-0.10.6-pp38-pypy38_pp73-musllinux_1_2_x86_64.whl\", hash = \"sha256:9164ec8010327ab9af931d7ccd12ab8d8b5dc2f4c6a16cbdd9d087861eaaefa1\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-macosx_10_7_x86_64.whl\", hash = \"sha256:d29ddefeab1791e3c751e0189d5f4b3dbc0bbe033b06e9c333dca1f99e1d523e\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-macosx_11_0_arm64.whl\", hash = \"sha256:30adb75ecd7c2a52f5e76af50644b3e0b5ba036321c390b8e7ec1bb2a16dd43c\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:dd609fafdcdde6e67a139898196698af37438b035b25ad63704fd9097d9a3482\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-manylinux_2_17_armv7l.manylinux2014_armv7l.whl\", hash = \"sha256:6eef672de005736a6efd565577101277db6057f65640a813de6c2707dc69f396\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:6cf4393c7b41abbf07c88eb83e8af5013606b1cdb7f6bc96b1b3536b53a574b8\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:ad857f42831e5b8d41a32437f88d86ead6c191455a3499c4b6d15e007936d4cf\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:1d7360573f1e046cb3b0dceeb8864025aa78d98be4bb69f067ec1c40a9e2d9df\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl\", hash = \"sha256:d08f63561c8a695afec4975fae445245386d645e3e446e6f260e81663bfd2e38\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-musllinux_1_2_aarch64.whl\", hash = \"sha256:f0f17f2ce0f3529177a5fff5525204fad7b43dd437d017dd0317f2746773443d\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-musllinux_1_2_i686.whl\", hash = \"sha256:442626328600bde1d09dc3bb00434f5374948838ce75c41a52152615689f9403\"},\n+    {file = \"rpds_py-0.10.6-pp39-pypy39_pp73-musllinux_1_2_x86_64.whl\", hash = \"sha256:e9616f5bd2595f7f4a04b67039d890348ab826e943a9bfdbe4938d0eba606971\"},\n+    {file = \"rpds_py-0.10.6.tar.gz\", hash = \"sha256:4ce5a708d65a8dbf3748d2474b580d606b1b9f91b5c6ab2a316e0b0cf7a4ba50\"},\n+]\n+\n+[[package]]\n+name = \"ruamel-yaml\"\n+version = \"0.17.40\"\n+description = \"ruamel.yaml is a YAML parser/emitter that supports roundtrip preservation of comments, seq/map flow style, and map key order\"\n+optional = false\n+python-versions = \">=3\"\n+files = [\n+    {file = \"ruamel.yaml-0.17.40-py3-none-any.whl\", hash = \"sha256:b16b6c3816dff0a93dca12acf5e70afd089fa5acb80604afd1ffa8b465b7722c\"},\n+    {file = \"ruamel.yaml-0.17.40.tar.gz\", hash = \"sha256:6024b986f06765d482b5b07e086cc4b4cd05dd22ddcbc758fa23d54873cf313d\"},\n+]\n+\n+[package.dependencies]\n+\"ruamel.yaml.clib\" = {version = \">=0.2.7\", markers = \"platform_python_implementation == \\\"CPython\\\" and python_version < \\\"3.13\\\"\"}\n+\n+[package.extras]\n+docs = [\"mercurial (>5.7)\", \"ryd\"]\n+jinja2 = [\"ruamel.yaml.jinja2 (>=0.2)\"]\n+\n+[[package]]\n+name = \"ruamel-yaml-clib\"\n+version = \"0.2.8\"\n+description = \"C version of reader, parser and emitter for ruamel.yaml derived from libyaml\"\n+optional = false\n+python-versions = \">=3.6\"\n+files = [\n+    {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:b42169467c42b692c19cf539c38d4602069d8c1505e97b86387fcf7afb766e1d\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-macosx_13_0_arm64.whl\", hash = \"sha256:07238db9cbdf8fc1e9de2489a4f68474e70dffcb32232db7c08fa61ca0c7c462\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux2014_aarch64.whl\", hash = \"sha256:d92f81886165cb14d7b067ef37e142256f1c6a90a65cd156b063a43da1708cfd\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl\", hash = \"sha256:fff3573c2db359f091e1589c3d7c5fc2f86f5bdb6f24252c2d8e539d4e45f412\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-win32.whl\", hash = \"sha256:c69212f63169ec1cfc9bb44723bf2917cbbd8f6191a00ef3410f5a7fe300722d\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp310-cp310-win_amd64.whl\", hash = \"sha256:cabddb8d8ead485e255fe80429f833172b4cadf99274db39abc080e068cbcc31\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_10_9_universal2.whl\", hash = \"sha256:bef08cd86169d9eafb3ccb0a39edb11d8e25f3dae2b28f5c52fd997521133069\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_13_0_arm64.whl\", hash = \"sha256:b16420e621d26fdfa949a8b4b47ade8810c56002f5389970db4ddda51dbff248\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-manylinux2014_aarch64.whl\", hash = \"sha256:b5edda50e5e9e15e54a6a8a0070302b00c518a9d32accc2346ad6c984aacd279\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl\", hash = \"sha256:25c515e350e5b739842fc3228d662413ef28f295791af5e5110b543cf0b57d9b\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-win32.whl\", hash = \"sha256:53a300ed9cea38cf5a2a9b069058137c2ca1ce658a874b79baceb8f892f915a7\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl\", hash = \"sha256:c2a72e9109ea74e511e29032f3b670835f8a59bbdc9ce692c5b4ed91ccf1eedb\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-macosx_10_9_universal2.whl\", hash = \"sha256:ebc06178e8821efc9692ea7544aa5644217358490145629914d8020042c24aa1\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-macosx_13_0_arm64.whl\", hash = \"sha256:edaef1c1200c4b4cb914583150dcaa3bc30e592e907c01117c08b13a07255ec2\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-manylinux2014_aarch64.whl\", hash = \"sha256:7048c338b6c86627afb27faecf418768acb6331fc24cfa56c93e8c9780f815fa\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\", hash = \"sha256:d176b57452ab5b7028ac47e7b3cf644bcfdc8cacfecf7e71759f7f51a59e5c92\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:a5aa27bad2bb83670b71683aae140a1f52b0857a2deff56ad3f6c13a017a26ed\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:c58ecd827313af6864893e7af0a3bb85fd529f862b6adbefe14643947cfe2942\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-macosx_12_0_arm64.whl\", hash = \"sha256:f481f16baec5290e45aebdc2a5168ebc6d35189ae6fea7a58787613a25f6e875\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-manylinux2014_aarch64.whl\", hash = \"sha256:3fcc54cb0c8b811ff66082de1680b4b14cf8a81dce0d4fbf665c2265a81e07a1\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:7f67a1ee819dc4562d444bbafb135832b0b909f81cc90f7aa00260968c9ca1b3\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-win32.whl\", hash = \"sha256:75e1ed13e1f9de23c5607fe6bd1aeaae21e523b32d83bb33918245361e9cc51b\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp37-cp37m-win_amd64.whl\", hash = \"sha256:3f215c5daf6a9d7bbed4a0a4f760f3113b10e82ff4c5c44bec20a68c8014f675\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:1b617618914cb00bf5c34d4357c37aa15183fa229b24767259657746c9077615\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-macosx_12_0_arm64.whl\", hash = \"sha256:a6a9ffd280b71ad062eae53ac1659ad86a17f59a0fdc7699fd9be40525153337\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-manylinux2014_aarch64.whl\", hash = \"sha256:665f58bfd29b167039f714c6998178d27ccd83984084c286110ef26b230f259f\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:700e4ebb569e59e16a976857c8798aee258dceac7c7d6b50cab63e080058df91\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-win32.whl\", hash = \"sha256:955eae71ac26c1ab35924203fda6220f84dce57d6d7884f189743e2abe3a9fbe\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp38-cp38-win_amd64.whl\", hash = \"sha256:56f4252222c067b4ce51ae12cbac231bce32aee1d33fbfc9d17e5b8d6966c312\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:03d1162b6d1df1caa3a4bd27aa51ce17c9afc2046c31b0ad60a0a96ec22f8001\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-macosx_12_0_arm64.whl\", hash = \"sha256:bba64af9fa9cebe325a62fa398760f5c7206b215201b0ec825005f1b18b9bccf\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-manylinux2014_aarch64.whl\", hash = \"sha256:9eb5dee2772b0f704ca2e45b1713e4e5198c18f515b52743576d196348f374d3\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl\", hash = \"sha256:da09ad1c359a728e112d60116f626cc9f29730ff3e0e7db72b9a2dbc2e4beed5\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-win32.whl\", hash = \"sha256:84b554931e932c46f94ab306913ad7e11bba988104c5cff26d90d03f68258cd5\"},\n+    {file = \"ruamel.yaml.clib-0.2.8-cp39-cp39-win_amd64.whl\", hash = \"sha256:25ac8c08322002b06fa1d49d1646181f0b2c72f5cbc15a85e80b4c30a544bb15\"},\n+    {file = \"ruamel.yaml.clib-0.2.8.tar.gz\", hash = \"sha256:beb2e0404003de9a4cab9753a8805a8fe9320ee6673136ed7f04255fe60bb512\"},\n+]\n+\n+[[package]]\n+name = \"semgrep\"\n+version = \"1.45.0\"\n+description = \"Lightweight static analysis for many languages. Find bug variants with patterns that look like source code.\"\n+optional = false\n+python-versions = \">=3.7\"\n+files = [\n+    {file = \"semgrep-1.45.0-cp37.cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-any.whl\", hash = \"sha256:b466501971f9491ab089d01e29dec6fab404b5f99e1279c888d4a8e6aac3b443\"},\n+    {file = \"semgrep-1.45.0-cp37.cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-macosx_10_14_x86_64.whl\", hash = \"sha256:4f2bc7482746d3383d909d46a0d878184580eac1b2cafe65c4192f2d226d3df5\"},\n+    {file = \"semgrep-1.45.0-cp37.cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-macosx_11_0_arm64.whl\", hash = \"sha256:7bac4ac8c613ba9851cce28537117636f7356489324cb55503a6f90be51d6e91\"},\n+    {file = \"semgrep-1.45.0-cp37.cp38.cp39.cp310.cp311.py37.py38.py39.py310.py311-none-musllinux_1_0_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:519aa0752d206b2442895be6ba279d4826783f1db0b994f87a5855bdce310078\"},\n+    {file = \"semgrep-1.45.0.tar.gz\", hash = \"sha256:f2efad4236a0cf8b397e8f367b49d77a5ea0ec92de518f158247160041dbd980\"},\n+]\n+\n+[package.dependencies]\n+attrs = \">=21.3\"\n+boltons = \">=21.0,<22.0\"\n+click = \">=8.1,<9.0\"\n+click-option-group = \">=0.5,<1.0\"\n+colorama = \">=0.4.0,<0.5.0\"\n+defusedxml = \">=0.7.1,<0.8.0\"\n+glom = \">=22.1,<23.0\"\n+jsonschema = \">=4.6,<5.0\"\n+packaging = \">=21.0\"\n+peewee = \">=3.14,<4.0\"\n+python-lsp-jsonrpc = \">=1.0.0,<1.1.0\"\n+requests = \">=2.22,<3.0\"\n+rich = \">=12.6.0\"\n+\"ruamel.yaml\" = \">=0.16.0,<0.18\"\n+tomli = \">=2.0.1,<2.1.0\"\n+typing-extensions = \">=4.2,<5.0\"\n+urllib3 = \">=1.26,<2.0\"\n+wcmatch = \">=8.3,<9.0\"\n+\n+[package.extras]\n+experiments = [\"jsonnet (>=0.18,<1.0)\"]\n+\n [[package]]\n name = \"setuptools\"\n version = \"68.2.2\"\n@@ -2246,22 +2737,91 @@ files = [\n     {file = \"typing_extensions-4.8.0.tar.gz\", hash = \"sha256:df8e4339e9cb77357558cbdbceca33c303714cf861d1eef15e1070055ae8b7ef\"},\n ]\n \n+[[package]]\n+name = \"ujson\"\n+version = \"5.8.0\"\n+description = \"Ultra fast JSON encoder and decoder for Python\"\n+optional = false\n+python-versions = \">=3.8\"\n+files = [\n+    {file = \"ujson-5.8.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:f4511560d75b15ecb367eef561554959b9d49b6ec3b8d5634212f9fed74a6df1\"},\n+    {file = \"ujson-5.8.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:9399eaa5d1931a0ead49dce3ffacbea63f3177978588b956036bfe53cdf6af75\"},\n+    {file = \"ujson-5.8.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c4e7bb7eba0e1963f8b768f9c458ecb193e5bf6977090182e2b4f4408f35ac76\"},\n+    {file = \"ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:40931d7c08c4ce99adc4b409ddb1bbb01635a950e81239c2382cfe24251b127a\"},\n+    {file = \"ujson-5.8.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:d53039d39de65360e924b511c7ca1a67b0975c34c015dd468fca492b11caa8f7\"},\n+    {file = \"ujson-5.8.0-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:bdf04c6af3852161be9613e458a1fb67327910391de8ffedb8332e60800147a2\"},\n+    {file = \"ujson-5.8.0-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:a70f776bda2e5072a086c02792c7863ba5833d565189e09fabbd04c8b4c3abba\"},\n+    {file = \"ujson-5.8.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:f26629ac531d712f93192c233a74888bc8b8212558bd7d04c349125f10199fcf\"},\n+    {file = \"ujson-5.8.0-cp310-cp310-win32.whl\", hash = \"sha256:7ecc33b107ae88405aebdb8d82c13d6944be2331ebb04399134c03171509371a\"},\n+    {file = \"ujson-5.8.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:3b27a8da7a080add559a3b73ec9ebd52e82cc4419f7c6fb7266e62439a055ed0\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-macosx_10_9_x86_64.whl\", hash = \"sha256:193349a998cd821483a25f5df30b44e8f495423840ee11b3b28df092ddfd0f7f\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-macosx_11_0_arm64.whl\", hash = \"sha256:4ddeabbc78b2aed531f167d1e70387b151900bc856d61e9325fcdfefb2a51ad8\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:5ce24909a9c25062e60653073dd6d5e6ec9d6ad7ed6e0069450d5b673c854405\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:27a2a3c7620ebe43641e926a1062bc04e92dbe90d3501687957d71b4bdddaec4\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:2b852bdf920fe9f84e2a2c210cc45f1b64f763b4f7d01468b33f7791698e455e\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-musllinux_1_1_aarch64.whl\", hash = \"sha256:20768961a6a706170497129960762ded9c89fb1c10db2989c56956b162e2a8a3\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-musllinux_1_1_i686.whl\", hash = \"sha256:e0147d41e9fb5cd174207c4a2895c5e24813204499fd0839951d4c8784a23bf5\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-musllinux_1_1_x86_64.whl\", hash = \"sha256:e3673053b036fd161ae7a5a33358ccae6793ee89fd499000204676baafd7b3aa\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-win32.whl\", hash = \"sha256:a89cf3cd8bf33a37600431b7024a7ccf499db25f9f0b332947fbc79043aad879\"},\n+    {file = \"ujson-5.8.0-cp311-cp311-win_amd64.whl\", hash = \"sha256:3659deec9ab9eb19e8646932bfe6fe22730757c4addbe9d7d5544e879dc1b721\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-macosx_10_9_x86_64.whl\", hash = \"sha256:102bf31c56f59538cccdfec45649780ae00657e86247c07edac434cb14d5388c\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-macosx_11_0_arm64.whl\", hash = \"sha256:299a312c3e85edee1178cb6453645217ba23b4e3186412677fa48e9a7f986de6\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:f2e385a7679b9088d7bc43a64811a7713cc7c33d032d020f757c54e7d41931ae\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:ad24ec130855d4430a682c7a60ca0bc158f8253ec81feed4073801f6b6cb681b\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:16fde596d5e45bdf0d7de615346a102510ac8c405098e5595625015b0d4b5296\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-musllinux_1_1_aarch64.whl\", hash = \"sha256:6d230d870d1ce03df915e694dcfa3f4e8714369cce2346686dbe0bc8e3f135e7\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-musllinux_1_1_i686.whl\", hash = \"sha256:9571de0c53db5cbc265945e08f093f093af2c5a11e14772c72d8e37fceeedd08\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-musllinux_1_1_x86_64.whl\", hash = \"sha256:7cba16b26efe774c096a5e822e4f27097b7c81ed6fb5264a2b3f5fd8784bab30\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-win32.whl\", hash = \"sha256:48c7d373ff22366eecfa36a52b9b55b0ee5bd44c2b50e16084aa88b9de038916\"},\n+    {file = \"ujson-5.8.0-cp312-cp312-win_amd64.whl\", hash = \"sha256:5ac97b1e182d81cf395ded620528c59f4177eee024b4b39a50cdd7b720fdeec6\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:2a64cc32bb4a436e5813b83f5aab0889927e5ea1788bf99b930fad853c5625cb\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:e54578fa8838ddc722539a752adfce9372474114f8c127bb316db5392d942f8b\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:9721cd112b5e4687cb4ade12a7b8af8b048d4991227ae8066d9c4b3a6642a582\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:9d9707e5aacf63fb919f6237d6490c4e0244c7f8d3dc2a0f84d7dec5db7cb54c\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:0be81bae295f65a6896b0c9030b55a106fb2dec69ef877253a87bc7c9c5308f7\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:ae7f4725c344bf437e9b881019c558416fe84ad9c6b67426416c131ad577df67\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:9ab282d67ef3097105552bf151438b551cc4bedb3f24d80fada830f2e132aeb9\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:94c7bd9880fa33fcf7f6d7f4cc032e2371adee3c5dba2922b918987141d1bf07\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-win32.whl\", hash = \"sha256:bf5737dbcfe0fa0ac8fa599eceafae86b376492c8f1e4b84e3adf765f03fb564\"},\n+    {file = \"ujson-5.8.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:11da6bed916f9bfacf13f4fc6a9594abd62b2bb115acfb17a77b0f03bee4cfd5\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:69b3104a2603bab510497ceabc186ba40fef38ec731c0ccaa662e01ff94a985c\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:9249fdefeb021e00b46025e77feed89cd91ffe9b3a49415239103fc1d5d9c29a\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2873d196725a8193f56dde527b322c4bc79ed97cd60f1d087826ac3290cf9207\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:6a4dafa9010c366589f55afb0fd67084acd8added1a51251008f9ff2c3e44042\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:7a42baa647a50fa8bed53d4e242be61023bd37b93577f27f90ffe521ac9dc7a3\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:f3554eaadffe416c6f543af442066afa6549edbc34fe6a7719818c3e72ebfe95\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:fb87decf38cc82bcdea1d7511e73629e651bdec3a43ab40985167ab8449b769c\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:407d60eb942c318482bbfb1e66be093308bb11617d41c613e33b4ce5be789adc\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-win32.whl\", hash = \"sha256:0fe1b7edaf560ca6ab023f81cbeaf9946a240876a993b8c5a21a1c539171d903\"},\n+    {file = \"ujson-5.8.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:3f9b63530a5392eb687baff3989d0fb5f45194ae5b1ca8276282fb647f8dcdb3\"},\n+    {file = \"ujson-5.8.0-pp38-pypy38_pp73-macosx_10_9_x86_64.whl\", hash = \"sha256:efeddf950fb15a832376c0c01d8d7713479fbeceaed1eaecb2665aa62c305aec\"},\n+    {file = \"ujson-5.8.0-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:7d8283ac5d03e65f488530c43d6610134309085b71db4f675e9cf5dff96a8282\"},\n+    {file = \"ujson-5.8.0-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:eb0142f6f10f57598655340a3b2c70ed4646cbe674191da195eb0985a9813b83\"},\n+    {file = \"ujson-5.8.0-pp38-pypy38_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:07d459aca895eb17eb463b00441986b021b9312c6c8cc1d06880925c7f51009c\"},\n+    {file = \"ujson-5.8.0-pp38-pypy38_pp73-win_amd64.whl\", hash = \"sha256:d524a8c15cfc863705991d70bbec998456a42c405c291d0f84a74ad7f35c5109\"},\n+    {file = \"ujson-5.8.0-pp39-pypy39_pp73-macosx_10_9_x86_64.whl\", hash = \"sha256:d6f84a7a175c75beecde53a624881ff618e9433045a69fcfb5e154b73cdaa377\"},\n+    {file = \"ujson-5.8.0-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:b748797131ac7b29826d1524db1cc366d2722ab7afacc2ce1287cdafccddbf1f\"},\n+    {file = \"ujson-5.8.0-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:2e72ba76313d48a1a3a42e7dc9d1db32ea93fac782ad8dde6f8b13e35c229130\"},\n+    {file = \"ujson-5.8.0-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl\", hash = \"sha256:f504117a39cb98abba4153bf0b46b4954cc5d62f6351a14660201500ba31fe7f\"},\n+    {file = \"ujson-5.8.0-pp39-pypy39_pp73-win_amd64.whl\", hash = \"sha256:a8c91b6f4bf23f274af9002b128d133b735141e867109487d17e344d38b87d94\"},\n+    {file = \"ujson-5.8.0.tar.gz\", hash = \"sha256:78e318def4ade898a461b3d92a79f9441e7e0e4d2ad5419abed4336d702c7425\"},\n+]\n+\n [[package]]\n name = \"urllib3\"\n-version = \"2.0.7\"\n+version = \"1.26.18\"\n description = \"HTTP library with thread-safe connection pooling, file post, and more.\"\n optional = false\n-python-versions = \">=3.7\"\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, !=3.5.*\"\n files = [\n-    {file = \"urllib3-2.0.7-py3-none-any.whl\", hash = \"sha256:fdb6d215c776278489906c2f8916e6e7d4f5a9b602ccbcfdf7f016fc8da0596e\"},\n-    {file = \"urllib3-2.0.7.tar.gz\", hash = \"sha256:c97dfde1f7bd43a71c8d2a58e369e9b2bf692d1334ea9f9cae55add7d0dd0f84\"},\n+    {file = \"urllib3-1.26.18-py2.py3-none-any.whl\", hash = \"sha256:34b97092d7e0a3a8cf7cd10e386f401b3737364026c45e622aa02903dffe0f07\"},\n+    {file = \"urllib3-1.26.18.tar.gz\", hash = \"sha256:f8ecc1bba5667413457c529ab955bf8c67b45db799d159066261719e328580a0\"},\n ]\n \n [package.extras]\n-brotli = [\"brotli (>=1.0.9)\", \"brotlicffi (>=0.8.0)\"]\n-secure = [\"certifi\", \"cryptography (>=1.9)\", \"idna (>=2.0.0)\", \"pyopenssl (>=17.1.0)\", \"urllib3-secure-extra\"]\n-socks = [\"pysocks (>=1.5.6,!=1.5.7,<2.0)\"]\n-zstd = [\"zstandard (>=0.18.0)\"]\n+brotli = [\"brotli (==1.0.9)\", \"brotli (>=1.0.9)\", \"brotlicffi (>=0.8.0)\", \"brotlipy (>=0.6.0)\"]\n+secure = [\"certifi\", \"cryptography (>=1.3.4)\", \"idna (>=2.0.0)\", \"ipaddress\", \"pyOpenSSL (>=0.14)\", \"urllib3-secure-extra\"]\n+socks = [\"PySocks (>=1.5.6,!=1.5.7,<2.0)\"]\n \n [[package]]\n name = \"uvicorn\"\n\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Great effort on this PR! The functionality to use containers is impressive and can enhance security for running LLMs. However, there were several issues with local runs, particularly around directory permissions and container management. Your approach to bind mounts needs reassessment; consider simplifying the setup to avoid persistent errors. The inclusion of unrelated files also clutters the scope. Let's focus on merging this MVP version to facilitate easier future improvements while tidying up these issues. Keep pushing forward!"
      }
    ],
    "codediff_length": 77857
  },
  {
    "index": 56,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/source_documents/state_of_the_union.txt b/source_documents/state_of_the_union.txt\ndeleted file mode 100644\nindex d50175de40..0000000000\n--- a/source_documents/state_of_the_union.txt\n+++ /dev/null\n@@ -1,723 +0,0 @@\n-Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n-\n-Last year COVID-19 kept us apart. This year we are finally together again. \n-\n-Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n-\n-With a duty to one another to the American people to the Constitution. \n-\n-And with an unwavering resolve that freedom will always triumph over tyranny. \n-\n-Six days ago, Russias Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n-\n-He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n-\n-He met the Ukrainian people. \n-\n-From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \n-\n-Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. \n-\n-In this struggle as President Zelenskyy said in his speech to the European Parliament Light will win over darkness. The Ukrainian Ambassador to the United States is here tonight. \n-\n-Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \n-\n-Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \n-\n-Throughout our history weve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \n-\n-They keep moving.   \n-\n-And the costs and the threats to America and the world keep rising.   \n-\n-Thats why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \n-\n-The United States is a member along with 29 other nations. \n-\n-It matters. American diplomacy matters. American resolve matters. \n-\n-Putins latest attack on Ukraine was premeditated and unprovoked. \n-\n-He rejected repeated efforts at diplomacy. \n-\n-He thought the West and NATO wouldnt respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \n-\n-We prepared extensively and carefully. \n-\n-We spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \n-\n-I spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \n-\n-We countered Russias lies with truth.   \n-\n-And now that he has acted the free world is holding him accountable. \n-\n-Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \n-\n-We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \n-\n-Together with our allies we are right now enforcing powerful economic sanctions. \n-\n-We are cutting off Russias largest banks from the international financial system.  \n-\n-Preventing Russias central bank from defending the Russian Ruble making Putins $630 Billion war fund worthless.   \n-\n-We are choking off Russias access to technology that will sap its economic strength and weaken its military for years to come.  \n-\n-Tonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \n-\n-The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  \n-\n-We are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. \n-\n-And tonight I am announcing that we will join our allies in closing off American air space to all Russian flights  further isolating Russia  and adding an additional squeeze on their economy. The Ruble has lost 30% of its value. \n-\n-The Russian stock market has lost 40% of its value and trading remains suspended. Russias economy is reeling and Putin alone is to blame. \n-\n-Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \n-\n-We are giving more than $1 Billion in direct assistance to Ukraine. \n-\n-And we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.  \n-\n-Let me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.  \n-\n-Our forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies  in the event that Putin decides to keep moving west.  \n-\n-For that purpose weve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. \n-\n-As I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.  \n-\n-And we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.  \n-\n-Putin has unleashed violence and chaos.  But while he may make gains on the battlefield  he will pay a continuing high price over the long run. \n-\n-And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n-\n-To all Americans, I will be honest with you, as Ive always promised. A Russian dictator, invading a foreign country, has costs around the world. \n-\n-And Im taking robust action to make sure the pain of our sanctions  is targeted at Russias economy. And I will use every tool at our disposal to protect American businesses and consumers. \n-\n-Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \n-\n-America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \n-\n-These steps will help blunt gas prices here at home. And I know the news about whats happening can seem alarming. \n-\n-But I want you to know that we are going to be okay. \n-\n-When the history of this era is written Putins war on Ukraine will have left Russia weaker and the rest of the world stronger. \n-\n-While it shouldnt have taken something so terrible for people around the world to see whats at stake now everyone sees it clearly. \n-\n-We see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.  \n-\n-In the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security. \n-\n-This is a real test. Its going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people. \n-\n-To our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you. \n-\n-Putin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people. \n-\n-He will never extinguish their love of freedom. He will never weaken the resolve of the free world. \n-\n-We meet tonight in an America that has lived through two of the hardest years this nation has ever faced. \n-\n-The pandemic has been punishing. \n-\n-And so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \n-\n-I understand. \n-\n-I remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \n-\n-Thats why one of the first things I did as President was fight to pass the American Rescue Plan.  \n-\n-Because people were hurting. We needed to act, and we did. \n-\n-Few pieces of legislation have done more in a critical moment in our history to lift us out of crisis. \n-\n-It fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \n-\n-Helped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \n-\n-And as my Dad used to say, it gave people a little breathing room. \n-\n-And unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working peopleand left no one behind. \n-\n-And it worked. It created jobs. Lots of jobs. \n-\n-In factour economy created over 6.5 Million new jobs just last year, more jobs created in one year  \n-than ever before in the history of America. \n-\n-Our economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasnt worked for the working people of this nation for too long.  \n-\n-For the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else. \n-\n-But that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \n-\n-Vice President Harris and I ran for office with a new economic vision for America. \n-\n-Invest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up  \n-and the middle out, not from the top down.  \n-\n-Because we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \n-\n-America used to have the best roads, bridges, and airports on Earth. \n-\n-Now our infrastructure is ranked 13th in the world. \n-\n-We wont be able to compete for the jobs of the 21st Century if we dont fix that. \n-\n-Thats why it was so important to pass the Bipartisan Infrastructure Lawthe most sweeping investment to rebuild America in history. \n-\n-This was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen. \n-\n-Were done talking about infrastructure weeks. \n-\n-Were going to have an infrastructure decade. \n-\n-It is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the worldparticularly with China.  \n-\n-As Ive told Xi Jinping, it is never a good bet to bet against the American people. \n-\n-Well create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America. \n-\n-And well do it all to withstand the devastating effects of the climate crisis and promote environmental justice. \n-\n-Well build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipesso every childand every Americanhas clean water to drink at home and at school, provide affordable high-speed internet for every Americanurban, suburban, rural, and tribal communities. \n-\n-4,000 projects have already been announced. \n-\n-And tonight, Im announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair. \n-\n-When we use taxpayer dollars to rebuild America  we are going to Buy American: buy American products to support American jobs. \n-\n-The federal government spends about $600 Billion a year to keep the country safe and secure. \n-\n-Theres been a law on the books for almost a century \n-to make sure taxpayers dollars support American jobs and businesses. \n-\n-Every Administration says theyll do it, but we are actually doing it. \n-\n-We will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America. \n-\n-But to compete for the best jobs of the future, we also need to level the playing field with China and other competitors. \n-\n-Thats why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing. \n-\n-Let me give you one example of why its so important to pass it. \n-\n-If you travel 20 miles east of Columbus, Ohio, youll find 1,000 empty acres of land. \n-\n-It wont look like much, but if you stop and look closely, youll see a Field of dreams, the ground on which Americas future will be built. \n-\n-This is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor mega site. \n-\n-Up to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. \n-\n-Some of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives. \n-\n-Smartphones. The Internet. Technology we have yet to invent. \n-\n-But thats just the beginning. \n-\n-Intels CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from  \n-$20 billion to $100 billion. \n-\n-That would be one of the biggest investments in manufacturing in American history. \n-\n-And all theyre waiting for is for you to pass this bill. \n-\n-So lets not wait any longer. Send it to my desk. Ill sign it.  \n-\n-And we will really take off. \n-\n-And Intel is not alone. \n-\n-Theres something happening in America. \n-\n-Just look around and youll see an amazing story. \n-\n-The rebirth of the pride that comes from stamping products Made In America. The revitalization of American manufacturing.   \n-\n-Companies are choosing to build new factories here, when just a few years ago, they would have built them overseas. \n-\n-Thats what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. \n-\n-GM is making the largest investment in its history$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. \n-\n-All told, we created 369,000 new manufacturing jobs in America just last year. \n-\n-Powered by people Ive met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, whos here with us tonight. \n-\n-As Ohio Senator Sherrod Brown says, Its time to bury the label Rust Belt. \n-\n-Its time. \n-\n-But with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  \n-\n-Inflation is robbing them of the gains they might otherwise feel. \n-\n-I get it. Thats why my top priority is getting prices under control. \n-\n-Look, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories. \n-\n-The pandemic also disrupted global supply chains. \n-\n-When factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up. \n-\n-Look at cars. \n-\n-Last year, there werent enough semiconductors to make all the cars that people wanted to buy. \n-\n-And guess what, prices of automobiles went up. \n-\n-Sowe have a choice. \n-\n-One way to fight inflation is to drive down wages and make Americans poorer.  \n-\n-I have a better plan to fight inflation. \n-\n-Lower your costs, not your wages. \n-\n-Make more cars and semiconductors in America. \n-\n-More infrastructure and innovation in America. \n-\n-More goods moving faster and cheaper in America. \n-\n-More jobs where you can earn a good living in America. \n-\n-And instead of relying on foreign supply chains, lets make it in America. \n-\n-Economists call it increasing the productive capacity of our economy. \n-\n-I call it building a better America. \n-\n-My plan to fight inflation will lower your costs and lower the deficit. \n-\n-17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And heres the plan: \n-\n-First  cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.  \n-\n-He and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  \n-\n-But drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshuas mom. \n-\n-Imagine what its like to look at your child who needs insulin and have no idea how youre going to pay for it.  \n-\n-What it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. \n-\n-Joshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  \n-\n-For Joshua, and for the 200,000 other young people with Type 1 diabetes, lets cap the cost of insulin at $35 a month so everyone can afford it.  \n-\n-Drug companies will still do very well. And while were at it let Medicare negotiate lower prices for prescription drugs, like the VA already does. \n-\n-Look, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Lets close the coverage gap and make those savings permanent. \n-\n-Second  cut energy costs for families an average of $500 a year by combatting climate change.  \n-\n-Lets provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double Americas clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because youll never have to pay at the gas pump again. \n-\n-Third  cut the cost of child care. Many families pay up to $14,000 a year for child care per child.  \n-\n-Middle-class and working families shouldnt have to pay more than 7% of their income for care of young children.  \n-\n-My plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldnt afford child care, to be able to get back to work. \n-\n-My plan doesnt stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.  \n-\n-All of these will lower costs. \n-\n-And under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.  \n-\n-The one thing all Americans agree on is that the tax system is not fair. We have to fix it.  \n-\n-Im not looking to punish anyone. But lets make sure corporations and the wealthiest Americans start paying their fair share. \n-\n-Just last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.  \n-\n-Thats simply not fair. Thats why Ive proposed a 15% minimum tax rate for corporations. \n-\n-We got more than 130 countries to agree on a global minimum tax rate so companies cant get out of paying their taxes at home by shipping jobs and factories overseas. \n-\n-Thats why Ive proposed closing loopholes so the very wealthy dont pay a lower tax rate than a teacher or a firefighter.  \n-\n-So thats my plan. It will grow the economy and lower costs for families. \n-\n-So what are we waiting for? Lets get this done. And while youre at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  \n-\n-My plan will not only lower costs to give families a fair shot, it will lower the deficit. \n-\n-The previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted. \n-\n-But in my administration, the watchdogs have been welcomed back. \n-\n-Were going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \n-\n-And tonight, Im announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \n-\n-By the end of this year, the deficit will be down to less than half what it was before I took office.  \n-\n-The only president ever to cut the deficit by more than one trillion dollars in a single year. \n-\n-Lowering your costs also means demanding more competition. \n-\n-Im a capitalist, but capitalism without competition isnt capitalism. \n-\n-Its exploitationand it drives up prices. \n-\n-When corporations dont have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. \n-\n-We see it happening with ocean carriers moving goods in and out of America. \n-\n-During the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits. \n-\n-Tonight, Im announcing a crackdown on these companies overcharging American businesses and consumers. \n-\n-And as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \n-\n-That ends on my watch. \n-\n-Medicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \n-\n-Well also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \n-\n-Lets pass the Paycheck Fairness Act and paid leave.  \n-\n-Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \n-\n-Lets increase Pell Grants and increase our historic support of HBCUs, and invest in what Jillour First Lady who teaches full-timecalls Americas best-kept secret: community colleges. \n-\n-And lets pass the PRO Act when a majority of workers want to form a unionthey shouldnt be stopped.  \n-\n-When we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we havent done in a long time: build a better America. \n-\n-For more than two years, COVID-19 has impacted every decision in our lives and the life of the nation. \n-\n-And I know youre tired, frustrated, and exhausted. \n-\n-But I also know this. \n-\n-Because of the progress weve made, because of your resilience and the tools we have, tonight I can say  \n-we are moving forward safely, back to more normal routines.  \n-\n-Weve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \n-\n-Just a few days ago, the Centers for Disease Control and Preventionthe CDCissued new mask guidelines. \n-\n-Under these new guidelines, most Americans in most of the country can now be mask free.   \n-\n-And based on the projections, more of the country will reach that point across the next couple of weeks. \n-\n-Thanks to the progress we have made this past year, COVID-19 need no longer control our lives.  \n-\n-I know some are talking about living with COVID-19. Tonight  I say that we will never just accept living with COVID-19. \n-\n-We will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard. \n-\n-Here are four common sense steps as we move forward safely.  \n-\n-First, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If youre vaccinated and boosted you have the highest degree of protection. \n-\n-We will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children. \n-\n-The scientists are working hard to get that done and well be ready with plenty of vaccines when they do. \n-\n-Were also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.  \n-\n-Weve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.  \n-\n-And were launching the Test to Treat initiative so people can get tested at a pharmacy, and if theyre positive, receive antiviral pills on the spot at no cost.  \n-\n-If youre immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. \n-\n-Were leaving no one behind or ignoring anyones needs as we move forward. \n-\n-And on testing, we have made hundreds of millions of tests available for you to order for free.   \n-\n-Even if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week. \n-\n-Second  we must prepare for new variants. Over the past year, weve gotten much better at detecting new variants. \n-\n-If necessary, well be able to deploy new vaccines within 100 days instead of many more months or years.  \n-\n-And, if Congress provides the funds we need, well have new stockpiles of tests, masks, and pills ready if needed. \n-\n-I cannot promise a new variant wont come. But I can promise you well do everything within our power to be ready if it does.  \n-\n-Third  we can end the shutdown of schools and businesses. We have the tools we need. \n-\n-Its time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   \n-\n-Were doing that here in the federal government. The vast majority of federal workers will once again work in person. \n-\n-Our schools are open. Lets keep it that way. Our kids need to be in school. \n-\n-And with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely. \n-\n-We achieved this because we provided free vaccines, treatments, tests, and masks. \n-\n-Of course, continuing this costs money. \n-\n-I will soon send Congress a request. \n-\n-The vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.   \n-\n-Fourth, we will continue vaccinating the world.     \n-\n-Weve sent 475 Million vaccine doses to 112 countries, more than any other nation. \n-\n-And we wont stop. \n-\n-We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \n-\n-Lets use this moment to reset. Lets stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \n-\n-Lets stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \n-\n-We cant change how divided weve been. But we can change how we move forwardon COVID-19 and other issues we must face together. \n-\n-I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n-\n-They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n-\n-Officer Mora was 27 years old. \n-\n-Officer Rivera was 22. \n-\n-Both Dominican Americans whod grown up on the same streets they later chose to patrol as police officers. \n-\n-I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n-\n-Ive worked on these issues a long time. \n-\n-I know what works: Investing in crime preventionand community police officers wholl walk the beat, wholl know the neighborhood, and who can restore trust and safety. \n-\n-So lets not abandon our streets. Or choose between safety and equal justice. \n-\n-Lets come together to protect our communities, restore trust, and hold law enforcement accountable. \n-\n-Thats why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \n-\n-Thats why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruptiontrusted messengers breaking the cycle of violence and trauma and giving young people hope.  \n-\n-We should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities. \n-\n-I ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.  \n-\n-And I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at homethey have no serial numbers and cant be traced. \n-\n-And I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon? \n-\n-Ban assault weapons and high-capacity magazines. \n-\n-Repeal the liability shield that makes gun manufacturers the only industry in America that cant be sued. \n-\n-These laws dont infringe on the Second Amendment. They save lives. \n-\n-The most fundamental right in America is the right to vote  and to have it counted. And its under assault. \n-\n-In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \n-\n-We cannot let this happen. \n-\n-Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youre at it, pass the Disclose Act so Americans can know who is funding our elections. \n-\n-Tonight, Id like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n-\n-One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n-\n-And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nations top legal minds, who will continue Justice Breyers legacy of excellence. \n-\n-A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since shes been nominated, shes received a broad range of supportfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n-\n-And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n-\n-We can do both. At our border, weve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n-\n-Weve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n-\n-Were putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n-\n-Were securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. \n-\n-We can do all this while keeping lit the torch of liberty that has led generations of immigrants to this landmy forefathers and so many of yours. \n-\n-Provide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers. \n-\n-Revise our laws so businesses have the workers they need and families dont wait decades to reunite. \n-\n-Its not only the right thing to doits the economically smart thing to do. \n-\n-Thats why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce. \n-\n-Lets get it done once and for all. \n-\n-Advancing liberty and justice also requires protecting the rights of women. \n-\n-The constitutional right affirmed in Roe v. Wadestanding precedent for half a centuryis under attack as never before. \n-\n-If we want to go forwardnot backwardwe must protect access to health care. Preserve a womans right to choose. And lets continue to advance maternal health care in America. \n-\n-And for our LGBTQ+ Americans, lets finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \n-\n-As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \n-\n-While it often appears that we never agree, that isnt true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \n-\n-And soon, well strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \n-\n-So tonight Im offering a Unity Agenda for the Nation. Four big things we can do together.  \n-\n-First, beat the opioid epidemic. \n-\n-There is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.  \n-\n-Get rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers. \n-\n-If youre suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery. \n-\n-Second, lets take on mental health. Especially among our children, whose lives and education have been turned upside down.  \n-\n-The American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \n-\n-I urge every parent to make sure your school does just that. And we can all play a partsign up to be a tutor or a mentor. \n-\n-Children were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media. \n-\n-As Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment theyre conducting on our children for profit. \n-\n-Its time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \n-\n-And lets get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \n-\n-Third, support our veterans. \n-\n-Veterans are the best of us. \n-\n-Ive always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \n-\n-My administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n-\n-Our troops in Iraq and Afghanistan faced many dangers. \n-\n-One was stationed at bases and breathing in toxic smoke from burn pits that incinerated wastes of warmedical and hazard material, jet fuel, and more. \n-\n-When they came home, many of the worlds fittest and best trained warriors were never the same. \n-\n-Headaches. Numbness. Dizziness. \n-\n-A cancer that would put them in a flag-draped coffin. \n-\n-I know. \n-\n-One of those soldiers was my son Major Beau Biden. \n-\n-We dont know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \n-\n-But Im committed to finding out everything we can. \n-\n-Committed to military families like Danielle Robinson from Ohio. \n-\n-The widow of Sergeant First Class Heath Robinson.  \n-\n-He was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \n-\n-Stationed near Baghdad, just yards from burn pits the size of football fields. \n-\n-Heaths widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter. \n-\n-But cancer from prolonged exposure to burn pits ravaged Heaths lungs and body. \n-\n-Danielle says Heath was a fighter to the very end. \n-\n-He didnt know how to stop fighting, and neither did she. \n-\n-Through her pain she found purpose to demand we do better. \n-\n-Tonight, Daniellewe are. \n-\n-The VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. \n-\n-And tonight, Im announcing were expanding eligibility to veterans suffering from nine respiratory cancers. \n-\n-Im also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve. \n-\n-And fourth, lets end cancer as we know it. \n-\n-This is personal to me and Jill, to Kamala, and to so many of you. \n-\n-Cancer is the #2 cause of death in Americasecond only to heart disease. \n-\n-Last month, I announced our plan to supercharge  \n-the Cancer Moonshot that President Obama asked me to lead six years ago. \n-\n-Our goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  \n-\n-More support for patients and families. \n-\n-To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \n-\n-Its based on DARPAthe Defense Department project that led to the Internet, GPS, and so much more.  \n-\n-ARPA-H will have a singular purposeto drive breakthroughs in cancer, Alzheimers, diabetes, and more. \n-\n-A unity agenda for the nation. \n-\n-We can do this. \n-\n-My fellow Americanstonight , we have gathered in a sacred spacethe citadel of our democracy. \n-\n-In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \n-\n-We have fought for freedom, expanded liberty, defeated totalitarianism and terror. \n-\n-And built the strongest, freest, and most prosperous nation the world has ever known. \n-\n-Now is the hour. \n-\n-Our moment of responsibility. \n-\n-Our test of resolve and conscience, of history itself. \n-\n-It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n-\n-Well I know this nation.  \n-\n-We will meet the test. \n-\n-To protect freedom and liberty, to expand fairness and opportunity. \n-\n-We will save democracy. \n-\n-As hard as these times have been, I am more optimistic about America today than I have been my whole life. \n-\n-Because I see the future that is within our grasp. \n-\n-Because I know there is simply nothing beyond our capacity. \n-\n-We are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \n-\n-The only nation that can be defined by a single word: possibilities. \n-\n-So on this night, in our 245th year as a nation, I have come to report on the State of the Union. \n-\n-And my report is this: the State of the Union is strongbecause you, the American people, are strong. \n-\n-We are stronger today than we were a year ago. \n-\n-And we will be stronger a year from now than we are today. \n-\n-Now is our moment to meet and overcome the challenges of our time. \n-\n-And we will, as one people. \n-\n-One America. \n-\n-The United States of America. \n-\n-May God bless you all. May God protect our troops.\n\\ No newline at end of file\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The addition of `n_gpu_layers` and `use_mlock` can indeed enhance performance by utilizing GPU resources more effectively. However, your installation issues indicate potential compatibility or configuration problems, especially with the CUDA setup. The error regarding native architecture is common when mismatched versions or unsupported options are used. Your approach to environment activation is crucialfollow the bash script guidelines strictly for consistency. Consider isolating changes in separate pull requests for easier management and evaluation. Lastly, assess your CPU as it may become the limiting factor in performance."
      }
    ],
    "codediff_length": 39518
  },
  {
    "index": 57,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/gpu_installer_cuda.bat b/gpu_installer_cuda.bat\nnew file mode 100644\nindex 0000000000..12869909bc\n--- /dev/null\n+++ b/gpu_installer_cuda.bat\n@@ -0,0 +1,103 @@\n+@echo off\n+\n+setlocal\n+pip --version >nul 2>&1\n+if %errorlevel% EQU 0 (\n+    set PIP_COMMAND=pip\n+) else (\n+    pip3 --version >nul 2>&1\n+    if %errorlevel% EQU 0 (\n+        set PIP_COMMAND=pip3\n+    ) else (\n+        echo Error: pip or pip3 is required.\n+        exit /b 1\n+    )\n+)\n+\n+%PIP_COMMAND% --version | findstr /C:\"python 3\" >nul 2>&1 && (\n+    echo pip is for Python 3.x.\n+) || (\n+    echo pip is not for Python 3.x.\n+    echo trying pip3...\n+    pip3 --version | findstr /C:\"python 3\" >nul 2>&1 && (\n+        set PIP_COMMAND=pip3\n+        echo pip3 is for Python 3.x.\n+    ) || (\n+        echo pip3 is not for Python 3.x.\n+        echo Error: pip or pip3 is required.\n+        exit /b 1\n+    )\n+)\n+\n+python --version >nul 2>&1 && (\n+    python -c \"import sys; assert sys.version_info >= (3, 10), 'Error: Python version should be larger than 3.10.'\" >nul 2>&1 && (\n+        set PYTHON_COMMAND=python\n+    ) || (\n+        py --version >nul 2>&1 && (\n+            py -c \"import sys; assert sys.version_info >= (3, 10), 'Error: Python version should be larger than 3.10.'\" >nul 2>&1 && (\n+                set PYTHON_COMMAND=py\n+            ) || (\n+                echo Error: Python version should be larger than 3.10.\n+                exit /b 1\n+            )\n+        )\n+    )\n+) || (\n+     echo Error: python or py is required.\n+    exit /b 1\n+)\n+\n+\n+\n+set /p use_venv=\"Do you want to use a virtual environment (venv) for the installation? (y/n): \"\n+if /i \"%use_venv%\"==\"y\" (\n+    %PYTHON_COMMAND% -c \"import venv\" >nul 2>&1\n+    if %errorlevel% NEQ 0 (\n+        echo Error: 'venv' module is not installed. Please install it using 'pip install virtualenv'.\n+        exit /b 1\n+    )\n+    echo Creating a virtual environment...\n+    %PYTHON_COMMAND% -m venv venv\n+    if exist \"venv\\Scripts\\activate.bat\" (\n+        echo Activating the virtual environment...\n+        call venv\\Scripts\\activate.bat\n+    ) else (\n+        echo Error: Virtual environment 'venv' not found.\n+        exit /b 1\n+    )\n+)\n+\n+where nvcc >nul 2>&1\n+if %errorlevel% equ 0 (\n+    nvcc --version | findstr /C:\"release 11.8\" >nul 2>&1\n+    if %errorlevel% equ 0 (\n+        echo CUDA 11.8 is installed.\n+    ) else (\n+        echo Error: CUDA 11.8 is not installed. \n+        exit /b 1\n+    )\n+) else (\n+    echo Error: nvcc is not found. Please install CUDA 11.8.\n+    exit /b 1\n+)\n+\n+echo Installing requirements...\n+%PIP_COMMAND% install -r requirements.txt -q\n+\n+%PYTHON_COMMAND% -c \"import torch; assert torch.version.cuda.startswith('11.8'), 'Error: CUDA version should be 11.8.'\" >nul 2>&1 && (\n+    echo PyTorch version is compatible with CUDA 11.8.\n+) || (\n+    echo Error: PyTorch version should be compatible with CUDA 11.8.\n+    echo Installing PyTorch for CUDA 11.8...\n+    %PIP_COMMAND% install torch torchvision --force-reinstall --upgrade --no-cache-dir --index-url https://download.pytorch.org/whl/cu118\n+)\n+\n+echo Building and installing llama-cpp-python package...\n+set CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\n+set FORCE_CMAKE=1\n+%PIP_COMMAND% uninstall llama-cpp-python -y\n+%PIP_COMMAND% install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n+\n+echo Installation completed. You can now run ingest.py and privateGPT.py.\n+\n+endlocal\ndiff --git a/gpu_installer_cuda.sh b/gpu_installer_cuda.sh\nnew file mode 100644\nindex 0000000000..fe1f318d1d\n--- /dev/null\n+++ b/gpu_installer_cuda.sh\n@@ -0,0 +1,110 @@\n+#!/usr/bin/bash\n+\n+set -e\n+\n+pip --version > /dev/null 2>&1\n+if [ $? -eq 0 ]; then\n+    PIP_COMMAND=\"pip\"\n+else\n+    pip3 --version > /dev/null 2>&1\n+    if [ $? -eq 0 ]; then\n+        PIP_COMMAND=\"pip3\"\n+    else\n+        echo \"Error: pip or pip3 is required.\"\n+        exit 1\n+    fi\n+fi\n+\n+PIP_OUTPUT=\"$($PIP_COMMAND --version 2>&1)\"\n+if [[ $PIP_OUTPUT == *\"python 3\"* ]]; then\n+    echo \"pip is for Python 3.x.\"\n+else\n+    echo \"pip is not for Python 3.x.\"\n+    echo \"Trying pip3...\"\n+    PIP_OUTPUT=\"$($PIP_COMMAND --version 2>&1)\"\n+    if [[ $PIP_OUTPUT == *\"python 3\"* ]]; then\n+        PIP_COMMAND=\"pip3\"\n+        echo \"pip3 is for Python 3.x.\"\n+    else\n+        echo \"pip3 is not for Python 3.x.\"\n+        echo \"Error: pip or pip3 is required.\"\n+        exit 1\n+    fi\n+fi\n+\n+\n+python --version > /dev/null 2>&1\n+if [ $? -eq 0 ]; then\n+    python -c \"import sys; assert sys.version_info >= (3, 10), 'Error: Python version should be larger than 3.10.'\" > /dev/null 2>&1\n+    if [ $? -eq 0 ]; then\n+        PYTHON_COMMAND=\"python\"\n+    else\n+        py --version > /dev/null 2>&1\n+        if [ $? -eq 0 ]; then\n+            py -c \"import sys; assert sys.version_info >= (3, 10), 'Error: Python version should be larger than 3.10.'\" > /dev/null 2>&1\n+            if [ $? -eq 0 ]; then\n+                PYTHON_COMMAND=\"py\"\n+            else\n+                echo \"Error: Python version should be larger than 3.10.\"\n+                exit 1\n+            fi\n+        fi\n+    fi\n+else\n+    echo \"Error: python or py is required.\"\n+    exit 1\n+fi\n+\n+read -p \"Do you want to use a virtual environment (venv) for the installation? (y/n): \" use_venv\n+if [[ \"$use_venv\" == [Yy]* ]]; then\n+    # Check if virtualenv module is installed\n+    PYTHON_COMMAND -c \"import venv\" > /dev/null 2>&1\n+    if [ $? -ne 0 ]; then\n+        echo \"Error: 'venv' module is not installed. Please install it using 'pip install virtualenv'.\"\n+        exit 1\n+    fi\n+\n+    echo \"Creating a virtual environment...\"\n+    PYTHON_COMMAND -m venv venv\n+\n+    if [ -f \"venv/bin/activate\" ]; then\n+        echo \"Activating the virtual environment...\"\n+        source venv/bin/activate\n+    else\n+        echo \"Error: Virtual environment 'venv' not found.\"\n+        exit 1\n+    fi\n+fi\n+nvcc --version > /dev/null 2>&1\n+if [ $? -eq 0 ]; then\n+    nvcc --version | grep \"release 11.8\" > /dev/null 2>&1\n+    if [ $? -eq 0 ]; then\n+        echo \"CUDA 11.8 is installed.\"\n+    else\n+        echo \"Error: CUDA 11.8 is not installed.\"\n+        exit 1\n+    fi\n+else\n+    echo \"Error: nvcc is not found. Please install CUDA 11.8.\"\n+    exit 1\n+fi\n+\n+echo \"Installing requirements...\"\n+$PIP_COMMAND install -r requirements.txt -q\n+\n+$PYTHON_COMMAND -c \"import torch; assert torch.version.cuda.startswith('11.8'), 'Error: CUDA version should be 11.8.'\" > /dev/null 2>&1\n+if [ $? -eq 0 ]; then\n+    echo \"PyTorch version is compatible with CUDA 11.8.\"\n+else\n+    echo \"Error: PyTorch version should be compatible with CUDA 11.8.\"\n+    echo \"Installing PyTorch for CUDA 11.8...\"\n+    $PIP_COMMAND install torch torchvision --force-reinstall --upgrade --no-cache-dir --index-url https://download.pytorch.org/whl/cu118\n+fi\n+\n+echo \"Building and installing llama-cpp-python package...\"\n+CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\n+FORCE_CMAKE=1\n+$PIP_COMMAND uninstall llama-cpp-python -y\n+$PIP_COMMAND install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n+\n+echo \"Installation completed. You can now run ingest.py and privateGPT.py.\"\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Your README.md requires a command adjustment for the installation process; ensure users run `cd llama-cpp-python` before executing setup. Several users faced issues likely due to CMake versions; highlight the need for an updated version in docs. The suggestion to utilize pycuda for GPU memory checks is valid, as shell scraping can introduce vulnerabilities. Consider implementing better error feedback, especially regarding GPU usage, since some users found it unclear. Lastly, clarify model specifications and add guidance for installing PyTorch with CUDA for GPU support."
      }
    ],
    "codediff_length": 7073
  },
  {
    "index": 58,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/models/tf.py b/models/tf.py\nnew file mode 100644\nindex 000000000000..40e7d20a9d84\n--- /dev/null\n+++ b/models/tf.py\n@@ -0,0 +1,558 @@\n+# YOLOv5  by Ultralytics, GPL-3.0 license\n+\"\"\"\n+TensorFlow/Keras and TFLite versions of YOLOv5\n+Authored by https://github.com/zldrobit in PR https://github.com/ultralytics/yolov5/pull/1127\n+\n+Usage:\n+    $ python models/tf.py --weights yolov5s.pt --cfg yolov5s.yaml\n+\n+Export int8 TFLite models:\n+    $ python models/tf.py --weights yolov5s.pt --cfg models/yolov5s.yaml --tfl-int8 \\\n+        --source path/to/images/ --ncalib 100\n+\n+Detection:\n+    $ python detect.py --weights yolov5s.pb          --img 320\n+    $ python detect.py --weights yolov5s_saved_model --img 320\n+    $ python detect.py --weights yolov5s-fp16.tflite --img 320\n+    $ python detect.py --weights yolov5s-int8.tflite --img 320 --tfl-int8\n+\n+For TensorFlow.js:\n+    $ python models/tf.py --weights yolov5s.pt --cfg models/yolov5s.yaml --img 320 --tf-nms --agnostic-nms\n+    $ pip install tensorflowjs\n+    $ tensorflowjs_converter \\\n+          --input_format=tf_frozen_model \\\n+          --output_node_names='Identity,Identity_1,Identity_2,Identity_3' \\\n+          yolov5s.pb \\\n+          web_model\n+    $ # Edit web_model/model.json to sort Identity* in ascending order\n+    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example\n+    $ npm install\n+    $ ln -s ../../yolov5/web_model public/web_model\n+    $ npm start\n+\"\"\"\n+\n+import argparse\n+import logging\n+import os\n+import sys\n+import traceback\n+from copy import deepcopy\n+from pathlib import Path\n+\n+sys.path.append('./')  # to run '$ python *.py' files in subdirectories\n+\n+import numpy as np\n+import tensorflow as tf\n+import torch\n+import torch.nn as nn\n+import yaml\n+from tensorflow import keras\n+from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n+\n+from models.common import Conv, Bottleneck, SPP, DWConv, Focus, BottleneckCSP, Concat, autopad, C3\n+from models.experimental import MixConv2d, CrossConv, attempt_load\n+from models.yolo import Detect\n+from utils.datasets import LoadImages\n+from utils.general import make_divisible, check_file, check_dataset\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class tf_BN(keras.layers.Layer):\n+    # TensorFlow BatchNormalization wrapper\n+    def __init__(self, w=None):\n+        super(tf_BN, self).__init__()\n+        self.bn = keras.layers.BatchNormalization(\n+            beta_initializer=keras.initializers.Constant(w.bias.numpy()),\n+            gamma_initializer=keras.initializers.Constant(w.weight.numpy()),\n+            moving_mean_initializer=keras.initializers.Constant(w.running_mean.numpy()),\n+            moving_variance_initializer=keras.initializers.Constant(w.running_var.numpy()),\n+            epsilon=w.eps)\n+\n+    def call(self, inputs):\n+        return self.bn(inputs)\n+\n+\n+class tf_Pad(keras.layers.Layer):\n+    def __init__(self, pad):\n+        super(tf_Pad, self).__init__()\n+        self.pad = tf.constant([[0, 0], [pad, pad], [pad, pad], [0, 0]])\n+\n+    def call(self, inputs):\n+        return tf.pad(inputs, self.pad, mode='constant', constant_values=0)\n+\n+\n+class tf_Conv(keras.layers.Layer):\n+    # Standard convolution\n+    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True, w=None):\n+        # ch_in, ch_out, weights, kernel, stride, padding, groups\n+        super(tf_Conv, self).__init__()\n+        assert g == 1, \"TF v2.2 Conv2D does not support 'groups' argument\"\n+        assert isinstance(k, int), \"Convolution with multiple kernels are not allowed.\"\n+        # TensorFlow convolution padding is inconsistent with PyTorch (e.g. k=3 s=2 'SAME' padding)\n+        # see https://stackoverflow.com/questions/52975843/comparing-conv2d-with-padding-between-tensorflow-and-pytorch\n+\n+        conv = keras.layers.Conv2D(\n+            c2, k, s, 'SAME' if s == 1 else 'VALID', use_bias=False,\n+            kernel_initializer=keras.initializers.Constant(w.conv.weight.permute(2, 3, 1, 0).numpy()))\n+        self.conv = conv if s == 1 else keras.Sequential([tf_Pad(autopad(k, p)), conv])\n+        self.bn = tf_BN(w.bn) if hasattr(w, 'bn') else tf.identity\n+\n+        # YOLOv5 activations\n+        if isinstance(w.act, nn.LeakyReLU):\n+            self.act = (lambda x: keras.activations.relu(x, alpha=0.1)) if act else tf.identity\n+        elif isinstance(w.act, nn.Hardswish):\n+            self.act = (lambda x: x * tf.nn.relu6(x + 3) * 0.166666667) if act else tf.identity\n+        elif isinstance(w.act, nn.SiLU):\n+            self.act = (lambda x: keras.activations.swish(x)) if act else tf.identity\n+\n+    def call(self, inputs):\n+        return self.act(self.bn(self.conv(inputs)))\n+\n+\n+class tf_Focus(keras.layers.Layer):\n+    # Focus wh information into c-space\n+    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True, w=None):\n+        # ch_in, ch_out, kernel, stride, padding, groups\n+        super(tf_Focus, self).__init__()\n+        self.conv = tf_Conv(c1 * 4, c2, k, s, p, g, act, w.conv)\n+\n+    def call(self, inputs):  # x(b,w,h,c) -> y(b,w/2,h/2,4c)\n+        # inputs = inputs / 255.  # normalize 0-255 to 0-1\n+        return self.conv(tf.concat([inputs[:, ::2, ::2, :],\n+                                    inputs[:, 1::2, ::2, :],\n+                                    inputs[:, ::2, 1::2, :],\n+                                    inputs[:, 1::2, 1::2, :]], 3))\n+\n+\n+class tf_Bottleneck(keras.layers.Layer):\n+    # Standard bottleneck\n+    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5, w=None):  # ch_in, ch_out, shortcut, groups, expansion\n+        super(tf_Bottleneck, self).__init__()\n+        c_ = int(c2 * e)  # hidden channels\n+        self.cv1 = tf_Conv(c1, c_, 1, 1, w=w.cv1)\n+        self.cv2 = tf_Conv(c_, c2, 3, 1, g=g, w=w.cv2)\n+        self.add = shortcut and c1 == c2\n+\n+    def call(self, inputs):\n+        return inputs + self.cv2(self.cv1(inputs)) if self.add else self.cv2(self.cv1(inputs))\n+\n+\n+class tf_Conv2d(keras.layers.Layer):\n+    # Substitution for PyTorch nn.Conv2D\n+    def __init__(self, c1, c2, k, s=1, g=1, bias=True, w=None):\n+        super(tf_Conv2d, self).__init__()\n+        assert g == 1, \"TF v2.2 Conv2D does not support 'groups' argument\"\n+        self.conv = keras.layers.Conv2D(\n+            c2, k, s, 'VALID', use_bias=bias,\n+            kernel_initializer=keras.initializers.Constant(w.weight.permute(2, 3, 1, 0).numpy()),\n+            bias_initializer=keras.initializers.Constant(w.bias.numpy()) if bias else None, )\n+\n+    def call(self, inputs):\n+        return self.conv(inputs)\n+\n+\n+class tf_BottleneckCSP(keras.layers.Layer):\n+    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n+    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, w=None):\n+        # ch_in, ch_out, number, shortcut, groups, expansion\n+        super(tf_BottleneckCSP, self).__init__()\n+        c_ = int(c2 * e)  # hidden channels\n+        self.cv1 = tf_Conv(c1, c_, 1, 1, w=w.cv1)\n+        self.cv2 = tf_Conv2d(c1, c_, 1, 1, bias=False, w=w.cv2)\n+        self.cv3 = tf_Conv2d(c_, c_, 1, 1, bias=False, w=w.cv3)\n+        self.cv4 = tf_Conv(2 * c_, c2, 1, 1, w=w.cv4)\n+        self.bn = tf_BN(w.bn)\n+        self.act = lambda x: keras.activations.relu(x, alpha=0.1)\n+        self.m = keras.Sequential([tf_Bottleneck(c_, c_, shortcut, g, e=1.0, w=w.m[j]) for j in range(n)])\n+\n+    def call(self, inputs):\n+        y1 = self.cv3(self.m(self.cv1(inputs)))\n+        y2 = self.cv2(inputs)\n+        return self.cv4(self.act(self.bn(tf.concat((y1, y2), axis=3))))\n+\n+\n+class tf_C3(keras.layers.Layer):\n+    # CSP Bottleneck with 3 convolutions\n+    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, w=None):\n+        # ch_in, ch_out, number, shortcut, groups, expansion\n+        super(tf_C3, self).__init__()\n+        c_ = int(c2 * e)  # hidden channels\n+        self.cv1 = tf_Conv(c1, c_, 1, 1, w=w.cv1)\n+        self.cv2 = tf_Conv(c1, c_, 1, 1, w=w.cv2)\n+        self.cv3 = tf_Conv(2 * c_, c2, 1, 1, w=w.cv3)\n+        self.m = keras.Sequential([tf_Bottleneck(c_, c_, shortcut, g, e=1.0, w=w.m[j]) for j in range(n)])\n+\n+    def call(self, inputs):\n+        return self.cv3(tf.concat((self.m(self.cv1(inputs)), self.cv2(inputs)), axis=3))\n+\n+\n+class tf_SPP(keras.layers.Layer):\n+    # Spatial pyramid pooling layer used in YOLOv3-SPP\n+    def __init__(self, c1, c2, k=(5, 9, 13), w=None):\n+        super(tf_SPP, self).__init__()\n+        c_ = c1 // 2  # hidden channels\n+        self.cv1 = tf_Conv(c1, c_, 1, 1, w=w.cv1)\n+        self.cv2 = tf_Conv(c_ * (len(k) + 1), c2, 1, 1, w=w.cv2)\n+        self.m = [keras.layers.MaxPool2D(pool_size=x, strides=1, padding='SAME') for x in k]\n+\n+    def call(self, inputs):\n+        x = self.cv1(inputs)\n+        return self.cv2(tf.concat([x] + [m(x) for m in self.m], 3))\n+\n+\n+class tf_Detect(keras.layers.Layer):\n+    def __init__(self, nc=80, anchors=(), ch=(), w=None):  # detection layer\n+        super(tf_Detect, self).__init__()\n+        self.stride = tf.convert_to_tensor(w.stride.numpy(), dtype=tf.float32)\n+        self.nc = nc  # number of classes\n+        self.no = nc + 5  # number of outputs per anchor\n+        self.nl = len(anchors)  # number of detection layers\n+        self.na = len(anchors[0]) // 2  # number of anchors\n+        self.grid = [tf.zeros(1)] * self.nl  # init grid\n+        self.anchors = tf.convert_to_tensor(w.anchors.numpy(), dtype=tf.float32)\n+        self.anchor_grid = tf.reshape(tf.convert_to_tensor(w.anchor_grid.numpy(), dtype=tf.float32),\n+                                      [self.nl, 1, -1, 1, 2])\n+        self.m = [tf_Conv2d(x, self.no * self.na, 1, w=w.m[i]) for i, x in enumerate(ch)]\n+        self.export = False  # onnx export\n+        self.training = True  # set to False after building model\n+        for i in range(self.nl):\n+            ny, nx = opt.img_size[0] // self.stride[i], opt.img_size[1] // self.stride[i]\n+            self.grid[i] = self._make_grid(nx, ny)\n+\n+    def call(self, inputs):\n+        # x = x.copy()  # for profiling\n+        z = []  # inference output\n+        self.training |= self.export\n+        x = []\n+        for i in range(self.nl):\n+            x.append(self.m[i](inputs[i]))\n+            # x(bs,20,20,255) to x(bs,3,20,20,85)\n+            ny, nx = opt.img_size[0] // self.stride[i], opt.img_size[1] // self.stride[i]\n+            x[i] = tf.transpose(tf.reshape(x[i], [-1, ny * nx, self.na, self.no]), [0, 2, 1, 3])\n+\n+            if not self.training:  # inference\n+                y = tf.sigmoid(x[i])\n+                xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\n+                wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]\n+                # Normalize xywh to 0-1 to reduce calibration error\n+                xy /= tf.constant([[opt.img_size[1], opt.img_size[0]]], dtype=tf.float32)\n+                wh /= tf.constant([[opt.img_size[1], opt.img_size[0]]], dtype=tf.float32)\n+                y = tf.concat([xy, wh, y[..., 4:]], -1)\n+                z.append(tf.reshape(y, [-1, 3 * ny * nx, self.no]))\n+\n+        return x if self.training else (tf.concat(z, 1), x)\n+\n+    @staticmethod\n+    def _make_grid(nx=20, ny=20):\n+        # yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n+        # return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\n+        xv, yv = tf.meshgrid(tf.range(nx), tf.range(ny))\n+        return tf.cast(tf.reshape(tf.stack([xv, yv], 2), [1, 1, ny * nx, 2]), dtype=tf.float32)\n+\n+\n+class tf_Upsample(keras.layers.Layer):\n+    def __init__(self, size, scale_factor, mode, w=None):\n+        super(tf_Upsample, self).__init__()\n+        assert scale_factor == 2, \"scale_factor must be 2\"\n+        # self.upsample = keras.layers.UpSampling2D(size=scale_factor, interpolation=mode)\n+        if opt.tf_raw_resize:\n+            # with default arguments: align_corners=False, half_pixel_centers=False\n+            self.upsample = lambda x: tf.raw_ops.ResizeNearestNeighbor(images=x,\n+                                                                       size=(x.shape[1] * 2, x.shape[2] * 2))\n+        else:\n+            self.upsample = lambda x: tf.image.resize(x, (x.shape[1] * 2, x.shape[2] * 2), method=mode)\n+\n+    def call(self, inputs):\n+        return self.upsample(inputs)\n+\n+\n+class tf_Concat(keras.layers.Layer):\n+    def __init__(self, dimension=1, w=None):\n+        super(tf_Concat, self).__init__()\n+        assert dimension == 1, \"convert only NCHW to NHWC concat\"\n+        self.d = 3\n+\n+    def call(self, inputs):\n+        return tf.concat(inputs, self.d)\n+\n+\n+def parse_model(d, ch, model):  # model_dict, input_channels(3)\n+    logger.info('\\n%3s%18s%3s%10s  %-40s%-30s' % ('', 'from', 'n', 'params', 'module', 'arguments'))\n+    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n+    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n+    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n+\n+    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n+    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n+        m_str = m\n+        m = eval(m) if isinstance(m, str) else m  # eval strings\n+        for j, a in enumerate(args):\n+            try:\n+                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n+            except:\n+                pass\n+\n+        n = max(round(n * gd), 1) if n > 1 else n  # depth gain\n+        if m in [nn.Conv2d, Conv, Bottleneck, SPP, DWConv, MixConv2d, Focus, CrossConv, BottleneckCSP, C3]:\n+            c1, c2 = ch[f], args[0]\n+            c2 = make_divisible(c2 * gw, 8) if c2 != no else c2\n+\n+            args = [c1, c2, *args[1:]]\n+            if m in [BottleneckCSP, C3]:\n+                args.insert(2, n)\n+                n = 1\n+        elif m is nn.BatchNorm2d:\n+            args = [ch[f]]\n+        elif m is Concat:\n+            c2 = sum([ch[-1 if x == -1 else x + 1] for x in f])\n+        elif m is Detect:\n+            args.append([ch[x + 1] for x in f])\n+            if isinstance(args[1], int):  # number of anchors\n+                args[1] = [list(range(args[1] * 2))] * len(f)\n+        else:\n+            c2 = ch[f]\n+\n+        tf_m = eval('tf_' + m_str.replace('nn.', ''))\n+        m_ = keras.Sequential([tf_m(*args, w=model.model[i][j]) for j in range(n)]) if n > 1 \\\n+            else tf_m(*args, w=model.model[i])  # module\n+\n+        torch_m_ = nn.Sequential(*[m(*args) for _ in range(n)]) if n > 1 else m(*args)  # module\n+        t = str(m)[8:-2].replace('__main__.', '')  # module type\n+        np = sum([x.numel() for x in torch_m_.parameters()])  # number params\n+        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\n+        logger.info('%3s%18s%3s%10.0f  %-40s%-30s' % (i, f, n, np, t, args))  # print\n+        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n+        layers.append(m_)\n+        ch.append(c2)\n+    return keras.Sequential(layers), sorted(save)\n+\n+\n+class tf_Model():\n+    def __init__(self, cfg='yolov5s.yaml', ch=3, nc=None, model=None):  # model, input channels, number of classes\n+        super(tf_Model, self).__init__()\n+        if isinstance(cfg, dict):\n+            self.yaml = cfg  # model dict\n+        else:  # is *.yaml\n+            import yaml  # for torch hub\n+            self.yaml_file = Path(cfg).name\n+            with open(cfg) as f:\n+                self.yaml = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n+\n+        # Define model\n+        if nc and nc != self.yaml['nc']:\n+            print('Overriding %s nc=%g with nc=%g' % (cfg, self.yaml['nc'], nc))\n+            self.yaml['nc'] = nc  # override yaml value\n+        self.model, self.savelist = parse_model(deepcopy(self.yaml), ch=[ch], model=model)  # model, savelist, ch_out\n+\n+    def predict(self, inputs, profile=False):\n+        y = []  # outputs\n+        x = inputs\n+        for i, m in enumerate(self.model.layers):\n+            if m.f != -1:  # if not from previous layer\n+                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n+\n+            x = m(x)  # run\n+            y.append(x if m.i in self.savelist else None)  # save output\n+\n+        # Add TensorFlow NMS\n+        if opt.tf_nms:\n+            boxes = xywh2xyxy(x[0][..., :4])\n+            probs = x[0][:, :, 4:5]\n+            classes = x[0][:, :, 5:]\n+            scores = probs * classes\n+            if opt.agnostic_nms:\n+                nms = agnostic_nms_layer()((boxes, classes, scores))\n+                return nms, x[1]\n+            else:\n+                boxes = tf.expand_dims(boxes, 2)\n+                nms = tf.image.combined_non_max_suppression(\n+                    boxes, scores, opt.topk_per_class, opt.topk_all, opt.iou_thres, opt.score_thres, clip_boxes=False)\n+                return nms, x[1]\n+\n+        return x[0]  # output only first tensor [1,6300,85] = [xywh, conf, class0, class1, ...]\n+        # x = x[0][0]  # [x(1,6300,85), ...] to x(6300,85)\n+        # xywh = x[..., :4]  # x(6300,4) boxes\n+        # conf = x[..., 4:5]  # x(6300,1) confidences\n+        # cls = tf.reshape(tf.cast(tf.argmax(x[..., 5:], axis=1), tf.float32), (-1, 1))  # x(6300,1)  classes\n+        # return tf.concat([conf, cls, xywh], 1)\n+\n+\n+class agnostic_nms_layer(keras.layers.Layer):\n+    # wrap map_fn to avoid TypeSpec related error https://stackoverflow.com/a/65809989/3036450\n+    def call(self, input):\n+        return tf.map_fn(agnostic_nms, input,\n+                         fn_output_signature=(tf.float32, tf.float32, tf.float32, tf.int32),\n+                         name='agnostic_nms')\n+\n+\n+def agnostic_nms(x):\n+    boxes, classes, scores = x\n+    class_inds = tf.cast(tf.argmax(classes, axis=-1), tf.float32)\n+    scores_inp = tf.reduce_max(scores, -1)\n+    selected_inds = tf.image.non_max_suppression(\n+        boxes, scores_inp, max_output_size=opt.topk_all, iou_threshold=opt.iou_thres, score_threshold=opt.score_thres)\n+    selected_boxes = tf.gather(boxes, selected_inds)\n+    padded_boxes = tf.pad(selected_boxes,\n+                          paddings=[[0, opt.topk_all - tf.shape(selected_boxes)[0]], [0, 0]],\n+                          mode=\"CONSTANT\", constant_values=0.0)\n+    selected_scores = tf.gather(scores_inp, selected_inds)\n+    padded_scores = tf.pad(selected_scores,\n+                           paddings=[[0, opt.topk_all - tf.shape(selected_boxes)[0]]],\n+                           mode=\"CONSTANT\", constant_values=-1.0)\n+    selected_classes = tf.gather(class_inds, selected_inds)\n+    padded_classes = tf.pad(selected_classes,\n+                            paddings=[[0, opt.topk_all - tf.shape(selected_boxes)[0]]],\n+                            mode=\"CONSTANT\", constant_values=-1.0)\n+    valid_detections = tf.shape(selected_inds)[0]\n+    return padded_boxes, padded_scores, padded_classes, valid_detections\n+\n+\n+def xywh2xyxy(xywh):\n+    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n+    x, y, w, h = tf.split(xywh, num_or_size_splits=4, axis=-1)\n+    return tf.concat([x - w / 2, y - h / 2, x + w / 2, y + h / 2], axis=-1)\n+\n+\n+def representative_dataset_gen():\n+    # Representative dataset for use with converter.representative_dataset\n+    n = 0\n+    for path, img, im0s, vid_cap in dataset:\n+        # Get sample input data as a numpy array in a method of your choosing.\n+        n += 1\n+        input = np.transpose(img, [1, 2, 0])\n+        input = np.expand_dims(input, axis=0).astype(np.float32)\n+        input /= 255.0\n+        yield [input]\n+        if n >= opt.ncalib:\n+            break\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument('--cfg', type=str, default='yolov5s.yaml', help='cfg path')\n+    parser.add_argument('--weights', type=str, default='yolov5s.pt', help='weights path')\n+    parser.add_argument('--img-size', nargs='+', type=int, default=[320, 320], help='image size')  # height, width\n+    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n+    parser.add_argument('--dynamic-batch-size', action='store_true', help='dynamic batch size')\n+    parser.add_argument('--source', type=str, default='../data/coco128.yaml', help='dir of images or data.yaml file')\n+    parser.add_argument('--ncalib', type=int, default=100, help='number of calibration images')\n+    parser.add_argument('--tfl-int8', action='store_true', dest='tfl_int8', help='export TFLite int8 model')\n+    parser.add_argument('--tf-nms', action='store_true', dest='tf_nms', help='TF NMS (without TFLite export)')\n+    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n+    parser.add_argument('--tf-raw-resize', action='store_true', dest='tf_raw_resize',\n+                        help='use tf.raw_ops.ResizeNearestNeighbor for resize')\n+    parser.add_argument('--topk-per-class', type=int, default=100, help='topk per class to keep in NMS')\n+    parser.add_argument('--topk-all', type=int, default=100, help='topk for all classes to keep in NMS')\n+    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')\n+    parser.add_argument('--score-thres', type=float, default=0.4, help='score threshold for NMS')\n+    opt = parser.parse_args()\n+    opt.cfg = check_file(opt.cfg)  # check file\n+    opt.img_size *= 2 if len(opt.img_size) == 1 else 1  # expand\n+    print(opt)\n+\n+    # Input\n+    img = torch.zeros((opt.batch_size, 3, *opt.img_size))  # image size(1,3,320,192) iDetection\n+\n+    # Load PyTorch model\n+    model = attempt_load(opt.weights, map_location=torch.device('cpu'), inplace=True, fuse=False)\n+    model.model[-1].export = False  # set Detect() layer export=True\n+    y = model(img)  # dry run\n+    nc = y[0].shape[-1] - 5\n+\n+    # TensorFlow saved_model export\n+    try:\n+        print('\\nStarting TensorFlow saved_model export with TensorFlow %s...' % tf.__version__)\n+        tf_model = tf_Model(opt.cfg, model=model, nc=nc)\n+        img = tf.zeros((opt.batch_size, *opt.img_size, 3))  # NHWC Input for TensorFlow\n+\n+        m = tf_model.model.layers[-1]\n+        assert isinstance(m, tf_Detect), \"the last layer must be Detect\"\n+        m.training = False\n+        y = tf_model.predict(img)\n+\n+        inputs = keras.Input(shape=(*opt.img_size, 3), batch_size=None if opt.dynamic_batch_size else opt.batch_size)\n+        keras_model = keras.Model(inputs=inputs, outputs=tf_model.predict(inputs))\n+        keras_model.summary()\n+        path = opt.weights.replace('.pt', '_saved_model')  # filename\n+        keras_model.save(path, save_format='tf')\n+        print('TensorFlow saved_model export success, saved as %s' % path)\n+    except Exception as e:\n+        print('TensorFlow saved_model export failure: %s' % e)\n+        traceback.print_exc(file=sys.stdout)\n+\n+    # TensorFlow GraphDef export\n+    try:\n+        print('\\nStarting TensorFlow GraphDef export with TensorFlow %s...' % tf.__version__)\n+\n+        # https://github.com/leimao/Frozen_Graph_TensorFlow\n+        full_model = tf.function(lambda x: keras_model(x))\n+        full_model = full_model.get_concrete_function(\n+            tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n+\n+        frozen_func = convert_variables_to_constants_v2(full_model)\n+        frozen_func.graph.as_graph_def()\n+        f = opt.weights.replace('.pt', '.pb')  # filename\n+        tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n+                          logdir=os.path.dirname(f),\n+                          name=os.path.basename(f),\n+                          as_text=False)\n+\n+        print('TensorFlow GraphDef export success, saved as %s' % f)\n+    except Exception as e:\n+        print('TensorFlow GraphDef export failure: %s' % e)\n+        traceback.print_exc(file=sys.stdout)\n+\n+    # TFLite model export\n+    if not opt.tf_nms:\n+        try:\n+            print('\\nStarting TFLite export with TensorFlow %s...' % tf.__version__)\n+\n+            # fp32 TFLite model export ---------------------------------------------------------------------------------\n+            # converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n+            # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n+            # converter.allow_custom_ops = False\n+            # converter.experimental_new_converter = True\n+            # tflite_model = converter.convert()\n+            # f = opt.weights.replace('.pt', '.tflite')  # filename\n+            # open(f, \"wb\").write(tflite_model)\n+\n+            # fp16 TFLite model export ---------------------------------------------------------------------------------\n+            converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n+            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n+            # converter.representative_dataset = representative_dataset_gen\n+            # converter.target_spec.supported_types = [tf.float16]\n+            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n+            converter.allow_custom_ops = False\n+            converter.experimental_new_converter = True\n+            tflite_model = converter.convert()\n+            f = opt.weights.replace('.pt', '-fp16.tflite')  # filename\n+            open(f, \"wb\").write(tflite_model)\n+            print('\\nTFLite export success, saved as %s' % f)\n+\n+            # int8 TFLite model export ---------------------------------------------------------------------------------\n+            if opt.tfl_int8:\n+                # Representative Dataset\n+                if opt.source.endswith('.yaml'):\n+                    with open(check_file(opt.source)) as f:\n+                        data = yaml.load(f, Loader=yaml.FullLoader)  # data dict\n+                        check_dataset(data)  # check\n+                    opt.source = data['train']\n+                dataset = LoadImages(opt.source, img_size=opt.img_size, auto=False)\n+                converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n+                converter.optimizations = [tf.lite.Optimize.DEFAULT]\n+                converter.representative_dataset = representative_dataset_gen\n+                converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n+                converter.inference_input_type = tf.uint8  # or tf.int8\n+                converter.inference_output_type = tf.uint8  # or tf.int8\n+                converter.allow_custom_ops = False\n+                converter.experimental_new_converter = True\n+                converter.experimental_new_quantizer = False\n+                tflite_model = converter.convert()\n+                f = opt.weights.replace('.pt', '-int8.tflite')  # filename\n+                open(f, \"wb\").write(tflite_model)\n+                print('\\nTFLite (int8) export success, saved as %s' % f)\n+\n+        except Exception as e:\n+            print('\\nTFLite export failure: %s' % e)\n+            traceback.print_exc(file=sys.stdout)\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Thanks for the clarifications regarding the input size and handling in TensorFlow. It's good to see that you correctly set `auto=False` to maintain consistent preprocessing. However, the model loading error in the Android emulator indicates a potential mismatch in input expectations. Check that the input shapes align with your model's architecture, especially since the error points to a concatenation operation expecting two inputs instead of one. Also verify youre using the correct model configuration when exporting to TFLite."
      }
    ],
    "codediff_length": 27167
  },
  {
    "index": 59,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/utils/loggers/mlflow/mlflow_utils.py b/utils/loggers/mlflow/mlflow_utils.py\nnew file mode 100644\nindex 000000000000..aa96b4b78d26\n--- /dev/null\n+++ b/utils/loggers/mlflow/mlflow_utils.py\n@@ -0,0 +1,134 @@\n+\"\"\"Utilities and tools for tracking runs with Mlflow.\"\"\"\n+\n+import sys\n+from argparse import Namespace\n+from pathlib import Path\n+from typing import Any, Dict\n+\n+FILE = Path(__file__).resolve()\n+ROOT = FILE.parents[3]\n+if str(ROOT) not in sys.path:\n+    sys.path.append(str(ROOT))\n+\n+from utils.general import LOGGER, colorstr\n+\n+try:\n+    import mlflow\n+    assert hasattr(mlflow, \"__version__\")\n+except (ImportError, AssertionError):\n+    mlflow = None\n+\n+from collections.abc import MutableMapping\n+\n+\n+class MlflowLogger:\n+    \"\"\"Log training run, artifacts, parameters, and metrics to Mlflow.\n+\n+    This logger expects that Mlflow is setup by the user.\n+    \"\"\"\n+\n+    def __init__(self, opt: Namespace) -> None:\n+        \"\"\"Initializes the MlflowLogger\n+\n+        Args:\n+            opt (Namespace): Commandline arguments for this run\n+        \"\"\"\n+        prefix = colorstr(\"Mlflow: \")\n+        try:\n+            self.mlflow, self.mlflow_active_run = mlflow, None if not mlflow else mlflow.start_run()\n+            if self.mlflow_active_run is not None:\n+                self.run_id = self.mlflow_active_run.info.run_id\n+                LOGGER.info(f\"{prefix}Using run_id({self.run_id})\")\n+                self.setup(opt)\n+        except Exception as err:\n+            LOGGER.error(f\"{prefix}Failing init - {repr(err)}\")\n+            LOGGER.warning(f\"{prefix}Continuining without Mlflow\")\n+            self.mlflow = None\n+            self.mlflow_active_run = None\n+\n+    def setup(self, opt: Namespace) -> None:\n+        if opt.weights is not None and str(opt.weights).strip() != \"\":\n+            model_name = Path(opt.weights).stem\n+        else:\n+            model_name = \"yolov5\"\n+        self.model_name = model_name\n+        self.weights = Path(opt.weights)\n+        self.client = mlflow.tracking.MlflowClient()\n+        self.log_params(vars(opt))\n+        self.log_metrics(vars(opt), is_param=True)\n+\n+    @staticmethod\n+    def _flatten_params(params_dict, parent_key=\"\", sep=\"/\"):\n+        \"\"\"Static method to flatten configs for logging to mlflow\"\"\"\n+        items = []\n+        for key, value in params_dict.items():\n+            new_key = parent_key + sep + key if parent_key else key\n+            if isinstance(value, MutableMapping):\n+                items.extend(MlflowLogger._flatten_params(value, new_key, sep).items())\n+            else:\n+                items.append((new_key, value))\n+        return dict(items)\n+\n+    def log_artifacts(self, artifact: Path, relpath: str = None) -> None:\n+        \"\"\"Member function to log artifacts (either directory or single item).\n+\n+        Args:\n+            artifact (Path): File or folder to be logged\n+            relpath (str): Name (or path) relative to experiment for logging artifact in mlflow\n+        \"\"\"\n+        if not isinstance(artifact, Path):\n+            artifact = Path(artifact)\n+        if artifact.is_dir():\n+            self.mlflow.log_artifacts(f\"{artifact.resolve()}/\", artifact_path=str(artifact.stem))\n+        else:\n+            self.mlflow.log_artifact(str(artifact.resolve()), artifact_path=relpath)\n+\n+    def log_model(self, model_path: Path, model_name: str = None) -> None:\n+        \"\"\"Member function to log model as an Mlflow model.\n+\n+        Args:\n+            model_path: Path to the model .pt being logged\n+            model_name: Name (or path) relative to experiment for logging model in mlflow\n+        \"\"\"\n+        self.mlflow.pyfunc.log_model(artifact_path=self.model_name if model_name is None else model_name,\n+                                     code_path=[str(ROOT.resolve())],\n+                                     artifacts={\"model_path\": str(model_path.resolve())},\n+                                     python_model=self.mlflow.pyfunc.PythonModel())\n+\n+    def log_params(self, params: Dict[str, Any]) -> None:\n+        \"\"\"Member funtion to log parameters.\n+        Mlflow doesn't have mutable parameters and so this function is used\n+        only to log initial training parameters.\n+\n+        Args:\n+            params (Dict[str, Any]): Parameters as dict\n+        \"\"\"\n+        try:\n+            flattened_params = MlflowLogger._flatten_params(params_dict=params)\n+            run = self.client.get_run(run_id=self.run_id)\n+            logged_params = run.data.params\n+            [\n+                self.mlflow.log_param(key=k, value=v) for k, v in flattened_params.items()\n+                if k not in logged_params and v is not None and str(v).strip() != \"\"]\n+        except Exception as err:\n+            LOGGER.warning(f\"Mlflow: failed to log all params because - {err}\")\n+\n+    def log_metrics(self, metrics: Dict[str, float], epoch: int = None, is_param: bool = False) -> None:\n+        \"\"\"Member function to log metrics.\n+        Mlflow requires metrics to be floats.\n+\n+        Args:\n+            metrics (Dict[str, float]): Dictionary with metric names and values\n+            epoch (int, optional): Training epoch. Defaults to None.\n+            is_param (bool, optional): Set it to True to log keys with a prefix \"params/\". Defaults to False.\n+        \"\"\"\n+        prefix = \"param/\" if is_param else \"\"\n+        metrics_dict = {\n+            f\"{prefix}{k.replace(':','-')}\": float(v)\n+            for k, v in metrics.items() if (isinstance(v, float) or isinstance(v, int))}\n+        self.mlflow.log_metrics(metrics=metrics_dict, step=epoch)\n+\n+    def finish_run(self) -> None:\n+        \"\"\"Member function to end mlflow run.\n+        \"\"\"\n+        self.mlflow.end_run()\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The integration of MLflow in your code is progressing well, but you need to ensure that the `on_pretrain_routine_end` method in `utils/loggers/__init__.py` properly logs relevant artifacts. Its good to see you addressed the empty weights issue in training; ensure this fix is tested thoroughly. Also, adding `mlflow.set_tracking_uri` in `train.py` will make your logging to the database more transparent. Keep an eye on the version discrepancies; MLflow 1.27 can introduce unexpected behaviors."
      }
    ],
    "codediff_length": 5721
  },
  {
    "index": 60,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/utils/wandb_logging/wandb_utils.py b/utils/wandb_logging/wandb_utils.py\nindex 264cd4840e3c..c9a32f5b6026 100644\n--- a/utils/wandb_logging/wandb_utils.py\n+++ b/utils/wandb_logging/wandb_utils.py\n@@ -22,87 +27,183 @@ def remove_prefix(from_string, prefix):\n     return from_string[len(prefix):]\n \n \n+def check_wandb_config_file(data_config_file):\n+    wandb_config = '_wandb.'.join(data_config_file.rsplit('.', 1))  # updated data.yaml path\n+    if Path(wandb_config).is_file():\n+        return wandb_config\n+    return data_config_file\n+\n+\n+def resume_and_get_id(opt):\n+    # It's more elegant to stick to 1 wandb.init call, but as useful config data is overwritten in the WandbLogger's wandb.init call\n+    if isinstance(opt.resume, str):\n+        if opt.resume.startswith(WANDB_ARTIFACT_PREFIX):\n+            run_path = Path(remove_prefix(opt.resume, WANDB_ARTIFACT_PREFIX))\n+            run_id = run_path.stem\n+            project = run_path.parent.stem\n+            model_artifact_name = WANDB_ARTIFACT_PREFIX + 'run_' + run_id + '_model'\n+            assert wandb, 'install wandb to resume wandb runs'\n+            # Resume wandb-artifact:// runs here| workaround for not overwriting wandb.config\n+            run = wandb.init(id=run_id, project=project, resume='allow')\n+            opt.resume = model_artifact_name\n+            return run\n+    return None\n+\n+\n class WandbLogger():\n     def __init__(self, opt, name, run_id, data_dict, job_type='Training'):\n-        self.wandb = wandb\n-        self.wandb_run = wandb.init(config=opt, resume=\"allow\",\n-                                    project='YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem,\n-                                    name=name,\n-                                    job_type=job_type,\n-                                    id=run_id) if self.wandb else None\n-\n-        if job_type == 'Training':\n-            self.setup_training(opt, data_dict)\n-            if opt.bbox_interval == -1:\n-                opt.bbox_interval = (opt.epochs // 10) if opt.epochs > 10 else opt.epochs\n-            if opt.save_period == -1:\n-                opt.save_period = (opt.epochs // 10) if opt.epochs > 10 else opt.epochs\n+        # Pre-training routine --\n+        self.job_type = job_type\n+        self.wandb, self.wandb_run, self.data_dict = wandb, None if not wandb else wandb.run, data_dict\n+        if self.wandb:\n+            self.wandb_run = wandb.init(config=opt,\n+                                        resume=\"allow\",\n+                                        project='YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem,\n+                                        name=name,\n+                                        job_type=job_type,\n+                                        id=run_id) if not wandb.run else wandb.run\n+            if self.job_type == 'Training':\n+                if not opt.resume:\n+                    wandb_data_dict = self.check_and_upload_dataset(opt) if opt.upload_dataset else data_dict\n+                    # Info useful for resuming from artifacts\n+                    self.wandb_run.config.opt = vars(opt)\n+                    self.wandb_run.config.data_dict = wandb_data_dict\n+                self.data_dict = self.setup_training(opt, data_dict)\n+            if self.job_type == 'Dataset Creation':\n+                self.data_dict = self.check_and_upload_dataset(opt)\n+\n+    def check_and_upload_dataset(self, opt):\n+        assert wandb, 'Install wandb to upload dataset'\n+        check_dataset(self.data_dict)\n+        config_path = self.log_dataset_artifact(opt.data,\n+                                                   opt.single_cls,\n+                                                   'YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem)\n+        print(\"Created dataset config file \", config_path)\n+        with open(config_path) as f:\n+            wandb_data_dict = yaml.load(f, Loader=yaml.SafeLoader)\n+        return wandb_data_dict\n \n     def setup_training(self, opt, data_dict):\n-        self.log_dict = {}\n-        self.train_artifact_path, self.trainset_artifact = \\\n-            self.download_dataset_artifact(data_dict['train'], opt.artifact_alias)\n-        self.test_artifact_path, self.testset_artifact = \\\n-            self.download_dataset_artifact(data_dict['val'], opt.artifact_alias)\n-        self.result_artifact, self.result_table, self.weights = None, None, None\n-        if self.train_artifact_path is not None:\n-            train_path = Path(self.train_artifact_path) / 'data/images/'\n-            data_dict['train'] = str(train_path)\n-        if self.test_artifact_path is not None:\n-            test_path = Path(self.test_artifact_path) / 'data/images/'\n-            data_dict['val'] = str(test_path)\n+        self.log_dict, self.current_epoch, self.log_imgs = {}, 0, 16  # Logging Constants\n+        self.bbox_interval = opt.bbox_interval\n+        if isinstance(opt.resume, str):\n+            modeldir, _ = self.download_model_artifact(opt)\n+            if modeldir:\n+                self.weights = Path(modeldir) / \"last.pt\"\n+                config = self.wandb_run.config\n+                opt.weights, opt.save_period, opt.batch_size, opt.bbox_interval, opt.epochs, opt.hyp = str(\n+                    self.weights), config.save_period, config.total_batch_size, config.bbox_interval, config.epochs, \\\n+                                                                                                       config.opt['hyp']\n+            data_dict = dict(self.wandb_run.config.data_dict)  # eliminates the need for config file to resume\n+        if 'val_artifact' not in self.__dict__:  # If --upload_dataset is set, use the existing artifact, don't download\n+            self.train_artifact_path, self.train_artifact = self.download_dataset_artifact(data_dict.get('train'),\n+                                                                                           opt.artifact_alias)\n+            self.val_artifact_path, self.val_artifact = self.download_dataset_artifact(data_dict.get('val'),\n+                                                                                       opt.artifact_alias)\n+            self.result_artifact, self.result_table, self.val_table, self.weights = None, None, None, None\n+            if self.train_artifact_path is not None:\n+                train_path = Path(self.train_artifact_path) / 'data/images/'\n+                data_dict['train'] = str(train_path)\n+            if self.val_artifact_path is not None:\n+                val_path = Path(self.val_artifact_path) / 'data/images/'\n+                data_dict['val'] = str(val_path)\n+                self.val_table = self.val_artifact.get(\"val\")\n+                self.map_val_table_path()\n+        if self.val_artifact is not None:\n             self.result_artifact = wandb.Artifact(\"run_\" + wandb.run.id + \"_progress\", \"evaluation\")\n             self.result_table = wandb.Table([\"epoch\", \"id\", \"prediction\", \"avg_confidence\"])\n-        if opt.resume_from_artifact:\n-            modeldir, _ = self.download_model_artifact(opt.resume_from_artifact)\n-            if modeldir:\n-                self.weights = Path(modeldir) / \"best.pt\"\n-                opt.weights = self.weights\n+        if opt.bbox_interval == -1:\n+            self.bbox_interval = opt.bbox_interval = (opt.epochs // 10) if opt.epochs > 10 else 1\n+        return data_dict\n \n     def download_dataset_artifact(self, path, alias):\n         if path.startswith(WANDB_ARTIFACT_PREFIX):\n             dataset_artifact = wandb.use_artifact(remove_prefix(path, WANDB_ARTIFACT_PREFIX) + \":\" + alias)\n             assert dataset_artifact is not None, \"'Error: W&B dataset artifact doesn\\'t exist'\"\n             datadir = dataset_artifact.download()\n-            labels_zip = Path(datadir) / \"data/labels.zip\"\n-            shutil.unpack_archive(labels_zip, Path(datadir) / 'data/labels', 'zip')\n-            print(\"Downloaded dataset to : \", datadir)\n             return datadir, dataset_artifact\n         return None, None\n \n-    def download_model_artifact(self, name):\n-        model_artifact = wandb.use_artifact(name + \":latest\")\n-        assert model_artifact is not None, 'Error: W&B model artifact doesn\\'t exist'\n-        modeldir = model_artifact.download()\n-        print(\"Downloaded model to : \", modeldir)\n-        return modeldir, model_artifact\n+    def download_model_artifact(self, opt):\n+        if opt.resume.startswith(WANDB_ARTIFACT_PREFIX):\n+            model_artifact = wandb.use_artifact(remove_prefix(opt.resume, WANDB_ARTIFACT_PREFIX) + \":latest\")\n+            assert model_artifact is not None, 'Error: W&B model artifact doesn\\'t exist'\n+            modeldir = model_artifact.download()\n+            epochs_trained = model_artifact.metadata.get('epochs_trained')\n+            total_epochs = model_artifact.metadata.get('total_epochs')\n+            assert epochs_trained < total_epochs, 'training to %g epochs is finished, nothing to resume.' % (\n+                total_epochs)\n+            return modeldir, model_artifact\n+        return None, None\n \n-    def log_model(self, path, opt, epoch):\n-        datetime_suffix = datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n+    def log_model(self, path, opt, epoch, fitness_score, best_model=False):\n         model_artifact = wandb.Artifact('run_' + wandb.run.id + '_model', type='model', metadata={\n             'original_url': str(path),\n-            'epoch': epoch + 1,\n+            'epochs_trained': epoch + 1,\n             'save period': opt.save_period,\n             'project': opt.project,\n-            'datetime': datetime_suffix\n+            'total_epochs': opt.epochs,\n+            'fitness_score': fitness_score\n         })\n         model_artifact.add_file(str(path / 'last.pt'), name='last.pt')\n-        model_artifact.add_file(str(path / 'best.pt'), name='best.pt')\n-        wandb.log_artifact(model_artifact)\n+        wandb.log_artifact(model_artifact,\n+                           aliases=['latest', 'epoch ' + str(self.current_epoch), 'best' if best_model else ''])\n         print(\"Saving model artifact on epoch \", epoch + 1)\n \n-    def log_dataset_artifact(self, dataset, class_to_id, name='dataset'):\n+    def log_dataset_artifact(self, data_file, single_cls, project, overwrite_config=False):\n+        with open(data_file) as f:\n+            data = yaml.load(f, Loader=yaml.SafeLoader)  # data dict\n+        nc, names = (1, ['item']) if single_cls else (int(data['nc']), data['names'])\n+        names = {k: v for k, v in enumerate(names)}  # to index dictionary\n+        self.train_artifact = self.create_dataset_table(LoadImagesAndLabels(\n+            data['train']), names, name='train') if data.get('train') else None\n+        self.val_artifact = self.create_dataset_table(LoadImagesAndLabels(\n+            data['val']), names, name='val') if data.get('val') else None\n+        if data.get('train'):\n+            data['train'] = WANDB_ARTIFACT_PREFIX + str(Path(project) / 'train')\n+        if data.get('val'):\n+            data['val'] = WANDB_ARTIFACT_PREFIX + str(Path(project) / 'val')\n+        path = data_file if overwrite_config else '_wandb.'.join(data_file.rsplit('.', 1))  # updated data.yaml path\n+        data.pop('download', None)\n+        with open(path, 'w') as f:\n+            yaml.dump(data, f)\n+\n+        if self.job_type == 'Training':  # builds correct artifact pipeline graph\n+            self.wandb_run.use_artifact(self.val_artifact)\n+            self.wandb_run.use_artifact(self.train_artifact)\n+            self.val_artifact.wait()\n+            self.val_table = self.val_artifact.get('val')\n+            self.map_val_table_path()\n+        else:\n+            self.wandb_run.log_artifact(self.train_artifact)\n+            self.wandb_run.log_artifact(self.val_artifact)\n+        return path\n+\n+    def map_val_table_path(self):\n+        self.val_table_map = {}\n+        print(\"Mapping dataset\")\n+        for i, data in enumerate(tqdm(self.val_table.data)):\n+            self.val_table_map[data[3]] = data[0]\n+\n+    def create_dataset_table(self, dataset, class_to_id, name='dataset'):\n+        # TODO: Explore multiprocessing to slpit this loop parallely| This is essential for speeding up the the logging\n         artifact = wandb.Artifact(name=name, type=\"dataset\")\n-        image_path = dataset.path\n-        artifact.add_dir(image_path, name='data/images')\n-        table = wandb.Table(columns=[\"id\", \"train_image\", \"Classes\"])\n+        for img_file in tqdm([dataset.path]) if Path(dataset.path).is_dir() else tqdm(dataset.img_files):\n+            if Path(img_file).is_dir():\n+                artifact.add_dir(img_file, name='data/images')\n+                labels_path = 'labels'.join(dataset.path.rsplit('images', 1))\n+                artifact.add_dir(labels_path, name='data/labels')\n+            else:\n+                artifact.add_file(img_file, name='data/images/' + Path(img_file).name)\n+                label_file = Path(img2label_paths([img_file])[0])\n+                artifact.add_file(str(label_file),\n+                                  name='data/labels/' + label_file.name) if label_file.exists() else None\n+        table = wandb.Table(columns=[\"id\", \"train_image\", \"Classes\", \"name\"])\n         class_set = wandb.Classes([{'id': id, 'name': name} for id, name in class_to_id.items()])\n-        for si, (img, labels, paths, shapes) in enumerate(dataset):\n+        for si, (img, labels, paths, shapes) in enumerate(tqdm(dataset)):\n             height, width = shapes[0]\n-            labels[:, 2:] = (xywh2xyxy(labels[:, 2:].view(-1, 4)))\n-            labels[:, 2:] *= torch.Tensor([width, height, width, height])\n-            box_data = []\n-            img_classes = {}\n+            labels[:, 2:] = (xywh2xyxy(labels[:, 2:].view(-1, 4))) * torch.Tensor([width, height, width, height])\n+            box_data, img_classes = [], {}\n             for cls, *xyxy in labels[:, 1:].tolist():\n                 cls = int(cls)\n                 box_data.append({\"position\": {\"minX\": xyxy[0], \"minY\": xyxy[1], \"maxX\": xyxy[2], \"maxY\": xyxy[3]},\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The proposed changes in this PR are focused on streamlining the dataset and resume functionality. Instead of hardcoding dataset paths, using a check for the existence of specified files is recommended. Adding an `--upload-dataset` argument will simplify dataset logging, while removing `--log-imgs` makes the code cleaner. The suggested resume command improves readability, but ensure it handles cases with modified data or architecture gracefully. Consider logging model checkpoints by default, as this offers more flexibility for users, particularly in fault-prone environments. Test thoroughly to confirm robustness."
      }
    ],
    "codediff_length": 14096
  },
  {
    "index": 61,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/.github/workflows/ci-testing-setup-install.yml b/.github/workflows/ci-testing-setup-install.yml\nnew file mode 100644\nindex 000000000000..d9654ac708a4\n--- /dev/null\n+++ b/.github/workflows/ci-testing-setup-install.yml\n@@ -0,0 +1,80 @@\n+name: CI CPU testing\n+\n+on:  # https://help.github.com/en/actions/reference/events-that-trigger-workflows\n+  push:\n+    branches: [ master, develop ]\n+  pull_request:\n+    # The branches below must be a subset of the branches above\n+    branches: [ master, develop ]\n+\n+jobs:\n+  cpu-tests:\n+\n+    runs-on: ${{ matrix.os }}\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        os: [ubuntu-latest, macos-latest, windows-latest]\n+        python-version: [3.6, 3.7, 3.8, 3.9]  # temporarily to check compatibility with different versions of python\n+        model: ['yolov5s']  # models to test\n+\n+    # Timeout: https://stackoverflow.com/a/59076067/4521646\n+    timeout-minutes: 50\n+    steps:\n+      - uses: actions/checkout@v2\n+      - name: Set up Python ${{ matrix.python-version }}\n+        uses: actions/setup-python@v2\n+        with:\n+          python-version: ${{ matrix.python-version }}\n+\n+      # Note: This uses an internal pip API and may not always work\n+      # https://github.com/actions/cache/blob/master/examples.md#multiple-oss-in-a-workflow\n+      - name: Get pip cache\n+        id: pip-cache\n+        run: |\n+          python -c \"from pip._internal.locations import USER_CACHE_DIR; print('::set-output name=dir::' + USER_CACHE_DIR)\"\n+\n+      - name: Cache pip\n+        uses: actions/cache@v1\n+        with:\n+          path: ${{ steps.pip-cache.outputs.dir }}\n+          key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('requirements.txt') }}\n+          restore-keys: |\n+            ${{ runner.os }}-${{ matrix.python-version }}-pip-\n+\n+      - name: Install dependencies\n+        run: |\n+          python -m pip install --upgrade pip\n+          pip install -e .\n+          python --version\n+          pip --version\n+          pip list\n+        shell: bash\n+\n+      - name: Download data\n+        run: |\n+          # curl -L -o tmp.zip https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip\n+          # unzip -q tmp.zip -d ../\n+          # rm tmp.zip\n+\n+      - name: Tests workflow\n+        run: |\n+          di=cpu # inference devices  # define device\n+\n+          # train\n+          python -m yolov5.train --img 128 --batch 16 --weights weights/${{ matrix.model }}.pt --cfg models/${{ matrix.model }}.yaml --epochs 1 --device $di\n+          # detect\n+          python -m yolov5.detect --weights weights/${{ matrix.model }}.pt --device $di\n+          python -m yolov5.detect --weights runs/train/exp/weights/last.pt --device $di\n+          # test\n+          python -m yolov5.test --img 128 --batch 16 --weights weights/${{ matrix.model }}.pt --device $di\n+          python -m yolov5.test --img 128 --batch 16 --weights runs/train/exp/weights/last.pt --device $di\n+\n+          # hub\n+          python hubconf.py\n+          # inspect\n+          python -m yolov5.models.yolo --cfg models/${{ matrix.model }}.yaml\n+          # export\n+          pip install -e .[export]\n+          python -m yolov5.models.export --img 128 --batch 1 --weights weights/${{ matrix.model }}.pt\n+        shell: bash\ndiff --git a/setup.py b/setup.py\nnew file mode 100644\nindex 000000000000..243a1fa507a4\n--- /dev/null\n+++ b/setup.py\n@@ -0,0 +1,81 @@\n+import pathlib\n+\n+import setuptools\n+\n+root_directory = pathlib.Path(__file__).parent\n+long_description = (root_directory / \"README.md\").read_text(encoding=\"utf-8\")\n+\n+setuptools.setup(\n+    name=\"ultralytics-yolov5\",\n+    version=\"0.0.2.dev\",\n+    description=\"Ultralytics YOLOv5  Python package, https://ultralytics.com\",\n+    python_requires=\">=3.6.2\",\n+    long_description=long_description,\n+    long_description_content_type=\"text/markdown\",\n+    url=\"https://github.com/ultralytics/yolov5\",\n+    author=\"Ultralytics\",\n+    author_email='glenn.jocher@ultralytics.com',\n+    license=\"GPLv3\",\n+    packages=[\"yolov5\"],\n+    include_package_data=True,\n+    install_requires=[\n+        \"matplotlib>=3.2.2\",\n+        \"numpy>=1.18.5\",\n+        \"opencv-python>=4.1.2\",\n+        \"Pillow\",\n+        \"PyYAML>=5.3.1\",\n+        \"scipy>=1.4.1\",\n+        \"torch>=1.7.0\",\n+        \"torchvision>=0.8.1\",\n+        \"tqdm>=4.41.0\",\n+        \"tensorboard>=2.4.1\",\n+        \"seaborn>=0.11.0\",\n+        \"pandas\"\n+    ],\n+    extras_require={\n+        \"tests\": [\n+            \"pytest\",\n+        ],\n+        \"export\": [\n+            \"coremltools>=4.1\",\n+            \"onnx>=1.9.0\",\n+            \"scikit-learn>=0.24\"\n+        ],\n+        \"extras\": [\n+            \"Cython\",\n+            \"pycocotools>=2.0\",\n+            \"thop\"\n+        ]\n+    },\n+    classifiers=[\n+        \"Intended Audience :: Developers\",\n+        \"Intended Audience :: Science/Research\",\n+        \"License :: OSI Approved :: GNU General Public License v3 (GPLv3)\",\n+        \"Programming Language :: Python :: 3\",\n+        \"Programming Language :: Python :: 3.6\",\n+        \"Programming Language :: Python :: 3.7\",\n+        \"Programming Language :: Python :: 3.8\",\n+        \"Programming Language :: Python :: 3.9\",\n+        \"Topic :: Software Development\",\n+        \"Topic :: Scientific/Engineering\",\n+        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n+        \"Topic :: Scientific/Engineering :: Image Recognition\",\n+        \"Operating System :: Microsoft :: Windows\",\n+        \"Operating System :: POSIX\",\n+        \"Operating System :: Unix\",\n+        \"Operating System :: MacOS\"\n+    ],\n+    project_urls={\n+        'Bug Reports': 'https://github.com/ultralytics/yolov5/issues',\n+        'Funding': 'https://ultralytics.com',\n+        'Source': 'https://github.com/ultralytics/yolov5/',\n+    },\n+    entry_points={\n+        'console_scripts': [\n+            'yolov5_train=yolov5.train:main',\n+            'yolov5_test=yolov5.test:main',\n+            'yolov5_detect=yolov5.detect:main',\n+            'yolov5_export=yolov5.models.export:main'\n+        ],\n+    }\n+)\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR introduces significant changes but highlights issues with model deserialization due to directory restructuring. You're correctly addressing `yolov5_in_syspath`, yet using `sys.path.append` can create interference, which should be avoided. Instead, leveraging `entry_points` in `setup.py` for command execution is a cleaner solution. Additionally, its crucial to remove `requirements.txt` to prevent user confusion. Updating the README with clear instructions will aid usability. Cleaning up the approach ensures compatibility and minimizes potential breaks for end-users."
      }
    ],
    "codediff_length": 6075
  },
  {
    "index": 62,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/utils/wandb_logging/wandb_utils.py b/utils/wandb_logging/wandb_utils.py\nnew file mode 100644\nindex 000000000000..264cd4840e3c\n--- /dev/null\n+++ b/utils/wandb_logging/wandb_utils.py\n@@ -0,0 +1,145 @@\n+import json\n+import shutil\n+import sys\n+from datetime import datetime\n+from pathlib import Path\n+\n+import torch\n+\n+sys.path.append(str(Path(__file__).parent.parent.parent))  # add utils/ to path\n+from utils.general import colorstr, xywh2xyxy\n+\n+try:\n+    import wandb\n+except ImportError:\n+    wandb = None\n+    print(f\"{colorstr('wandb: ')}Install Weights & Biases for YOLOv5 logging with 'pip install wandb' (recommended)\")\n+\n+WANDB_ARTIFACT_PREFIX = 'wandb-artifact://'\n+\n+\n+def remove_prefix(from_string, prefix):\n+    return from_string[len(prefix):]\n+\n+\n+class WandbLogger():\n+    def __init__(self, opt, name, run_id, data_dict, job_type='Training'):\n+        self.wandb = wandb\n+        self.wandb_run = wandb.init(config=opt, resume=\"allow\",\n+                                    project='YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem,\n+                                    name=name,\n+                                    job_type=job_type,\n+                                    id=run_id) if self.wandb else None\n+\n+        if job_type == 'Training':\n+            self.setup_training(opt, data_dict)\n+            if opt.bbox_interval == -1:\n+                opt.bbox_interval = (opt.epochs // 10) if opt.epochs > 10 else opt.epochs\n+            if opt.save_period == -1:\n+                opt.save_period = (opt.epochs // 10) if opt.epochs > 10 else opt.epochs\n+\n+    def setup_training(self, opt, data_dict):\n+        self.log_dict = {}\n+        self.train_artifact_path, self.trainset_artifact = \\\n+            self.download_dataset_artifact(data_dict['train'], opt.artifact_alias)\n+        self.test_artifact_path, self.testset_artifact = \\\n+            self.download_dataset_artifact(data_dict['val'], opt.artifact_alias)\n+        self.result_artifact, self.result_table, self.weights = None, None, None\n+        if self.train_artifact_path is not None:\n+            train_path = Path(self.train_artifact_path) / 'data/images/'\n+            data_dict['train'] = str(train_path)\n+        if self.test_artifact_path is not None:\n+            test_path = Path(self.test_artifact_path) / 'data/images/'\n+            data_dict['val'] = str(test_path)\n+            self.result_artifact = wandb.Artifact(\"run_\" + wandb.run.id + \"_progress\", \"evaluation\")\n+            self.result_table = wandb.Table([\"epoch\", \"id\", \"prediction\", \"avg_confidence\"])\n+        if opt.resume_from_artifact:\n+            modeldir, _ = self.download_model_artifact(opt.resume_from_artifact)\n+            if modeldir:\n+                self.weights = Path(modeldir) / \"best.pt\"\n+                opt.weights = self.weights\n+\n+    def download_dataset_artifact(self, path, alias):\n+        if path.startswith(WANDB_ARTIFACT_PREFIX):\n+            dataset_artifact = wandb.use_artifact(remove_prefix(path, WANDB_ARTIFACT_PREFIX) + \":\" + alias)\n+            assert dataset_artifact is not None, \"'Error: W&B dataset artifact doesn\\'t exist'\"\n+            datadir = dataset_artifact.download()\n+            labels_zip = Path(datadir) / \"data/labels.zip\"\n+            shutil.unpack_archive(labels_zip, Path(datadir) / 'data/labels', 'zip')\n+            print(\"Downloaded dataset to : \", datadir)\n+            return datadir, dataset_artifact\n+        return None, None\n+\n+    def download_model_artifact(self, name):\n+        model_artifact = wandb.use_artifact(name + \":latest\")\n+        assert model_artifact is not None, 'Error: W&B model artifact doesn\\'t exist'\n+        modeldir = model_artifact.download()\n+        print(\"Downloaded model to : \", modeldir)\n+        return modeldir, model_artifact\n+\n+    def log_model(self, path, opt, epoch):\n+        datetime_suffix = datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n+        model_artifact = wandb.Artifact('run_' + wandb.run.id + '_model', type='model', metadata={\n+            'original_url': str(path),\n+            'epoch': epoch + 1,\n+            'save period': opt.save_period,\n+            'project': opt.project,\n+            'datetime': datetime_suffix\n+        })\n+        model_artifact.add_file(str(path / 'last.pt'), name='last.pt')\n+        model_artifact.add_file(str(path / 'best.pt'), name='best.pt')\n+        wandb.log_artifact(model_artifact)\n+        print(\"Saving model artifact on epoch \", epoch + 1)\n+\n+    def log_dataset_artifact(self, dataset, class_to_id, name='dataset'):\n+        artifact = wandb.Artifact(name=name, type=\"dataset\")\n+        image_path = dataset.path\n+        artifact.add_dir(image_path, name='data/images')\n+        table = wandb.Table(columns=[\"id\", \"train_image\", \"Classes\"])\n+        class_set = wandb.Classes([{'id': id, 'name': name} for id, name in class_to_id.items()])\n+        for si, (img, labels, paths, shapes) in enumerate(dataset):\n+            height, width = shapes[0]\n+            labels[:, 2:] = (xywh2xyxy(labels[:, 2:].view(-1, 4)))\n+            labels[:, 2:] *= torch.Tensor([width, height, width, height])\n+            box_data = []\n+            img_classes = {}\n+            for cls, *xyxy in labels[:, 1:].tolist():\n+                cls = int(cls)\n+                box_data.append({\"position\": {\"minX\": xyxy[0], \"minY\": xyxy[1], \"maxX\": xyxy[2], \"maxY\": xyxy[3]},\n+                                 \"class_id\": cls,\n+                                 \"box_caption\": \"%s\" % (class_to_id[cls]),\n+                                 \"scores\": {\"acc\": 1},\n+                                 \"domain\": \"pixel\"})\n+                img_classes[cls] = class_to_id[cls]\n+            boxes = {\"ground_truth\": {\"box_data\": box_data, \"class_labels\": class_to_id}}  # inference-space\n+            table.add_data(si, wandb.Image(paths, classes=class_set, boxes=boxes), json.dumps(img_classes))\n+        artifact.add(table, name)\n+        labels_path = 'labels'.join(image_path.rsplit('images', 1))\n+        zip_path = Path(labels_path).parent / (name + '_labels.zip')\n+        if not zip_path.is_file():  # make_archive won't check if file exists\n+            shutil.make_archive(zip_path.with_suffix(''), 'zip', labels_path)\n+        artifact.add_file(str(zip_path), name='data/labels.zip')\n+        wandb.log_artifact(artifact)\n+        print(\"Saving data to W&B...\")\n+\n+    def log(self, log_dict):\n+        if self.wandb_run:\n+            for key, value in log_dict.items():\n+                self.log_dict[key] = value\n+\n+    def end_epoch(self):\n+        if self.wandb_run and self.log_dict:\n+            wandb.log(self.log_dict)\n+        self.log_dict = {}\n+\n+    def finish_run(self):\n+        if self.wandb_run:\n+            if self.result_artifact:\n+                print(\"Add Training Progress Artifact\")\n+                self.result_artifact.add(self.result_table, 'result')\n+                train_results = wandb.JoinedTable(self.testset_artifact.get(\"val\"), self.result_table, \"id\")\n+                self.result_artifact.add(train_results, 'joined_result')\n+                wandb.log_artifact(self.result_artifact)\n+            if self.log_dict:\n+                wandb.log(self.log_dict)\n+            wandb.run.finish()\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR showed a failure in a test that seems unrelated to the current changes. It may be beneficial to implement automated CI reruns to handle intermittent failures. Additionally, the top-level directory should remain clean, and moving new functionality to `utils/wandb_utils.py` or creating a `utils/wandb` directory would help with organization. Numerous code improvements were also suggested, including using `Path()` for file paths and eliminating redundancy, which youve mostly addressed. Clarification is needed regarding compatibility with the `--nosave` option in `train.py`."
      }
    ],
    "codediff_length": 7249
  },
  {
    "index": 63,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/models/common.py b/models/common.py\nindex 75cc4e97bbc7..521551f273d0 100644\n--- a/models/common.py\n+++ b/models/common.py\n@@ -881,3 +881,165 @@ def forward(self, x):\n         if isinstance(x, list):\n             x = torch.cat(x, 1)\n         return self.linear(self.drop(self.pool(self.conv(x)).flatten(1)))\n+\n+\n+# contributed by @aash1999\n+class ChannelAttention(nn.Module):\n+\n+    def __init__(self, in_planes, ratio=16):\n+        \"\"\"\n+        Initialize the Channel Attention module.\n+\n+        Args:\n+            in_planes (int): Number of input channels.\n+            ratio (int): Reduction ratio for the hidden channels in the channel attention block.\n+        \"\"\"\n+        super().__init__()\n+        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n+        self.max_pool = nn.AdaptiveMaxPool2d(1)\n+        self.f1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n+        self.relu = nn.ReLU()\n+        self.f2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n+        self.sigmoid = nn.Sigmoid()\n+\n+    def forward(self, x):\n+        \"\"\"\n+        Forward pass of the Channel Attention module.\n+\n+        Args:\n+            x (torch.Tensor): Input tensor.\n+\n+        Returns:\n+            out (torch.Tensor): Output tensor after applying channel attention.\n+        \"\"\"\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore')\n+            avg_out = self.f2(self.relu(self.f1(self.avg_pool(x))))\n+            max_out = self.f2(self.relu(self.f1(self.max_pool(x))))\n+            out = self.sigmoid(avg_out + max_out)\n+            return out\n+\n+\n+# contributed by @aash1999\n+class SpatialAttention(nn.Module):\n+\n+    def __init__(self, kernel_size=7):\n+        \"\"\"\n+        Initialize the Spatial Attention module.\n+\n+        Args:\n+            kernel_size (int): Size of the convolutional kernel for spatial attention.\n+        \"\"\"\n+        super().__init__()\n+        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n+        padding = 3 if kernel_size == 7 else 1\n+        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n+        self.sigmoid = nn.Sigmoid()\n+\n+    def forward(self, x):\n+        \"\"\"\n+        Forward pass of the Spatial Attention module.\n+\n+        Args:\n+            x (torch.Tensor): Input tensor.\n+\n+        Returns:\n+            out (torch.Tensor): Output tensor after applying spatial attention.\n+        \"\"\"\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore')\n+            avg_out = torch.mean(x, dim=1, keepdim=True)\n+            max_out, _ = torch.max(x, dim=1, keepdim=True)\n+            x = torch.cat([avg_out, max_out], dim=1)\n+            x = self.conv(x)\n+            return self.sigmoid(x)\n+\n+\n+# contributed by @aash1999\n+class CBAM(nn.Module):\n+    # ch_in, ch_out, shortcut, groups, expansion, ratio, kernel_size\n+    def __init__(self, c1, c2, kernel_size=3, shortcut=True, g=1, e=0.5, ratio=16):\n+        \"\"\"\n+        Initialize the CBAM (Convolutional Block Attention Module) .\n+\n+        Args:\n+            c1 (int): Number of input channels.\n+            c2 (int): Number of output channels.\n+            kernel_size (int): Size of the convolutional kernel.\n+            shortcut (bool): Whether to use a shortcut connection.\n+            g (int): Number of groups for grouped convolutions.\n+            e (float): Expansion factor for hidden channels.\n+            ratio (int): Reduction ratio for the hidden channels in the channel attention block.\n+        \"\"\"\n+        super().__init__()\n+        c_ = int(c2 * e)  # hidden channels\n+        self.cv1 = Conv(c1, c_, 1, 1)\n+        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n+        self.add = shortcut and c1 == c2\n+        self.channel_attention = ChannelAttention(c2, ratio)\n+        self.spatial_attention = SpatialAttention(kernel_size)\n+\n+    def forward(self, x):\n+        \"\"\"\n+        Forward pass of the CBAM .\n+\n+        Args:\n+            x (torch.Tensor): Input tensor.\n+\n+        Returns:\n+            out (torch.Tensor): Output tensor after applying the CBAM bottleneck.\n+        \"\"\"\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore')\n+            x2 = self.cv2(self.cv1(x))\n+            out = self.channel_attention(x2) * x2\n+            out = self.spatial_attention(out) * out\n+            return x + out if self.add else out\n+\n+\n+# contributed by @aash1999\n+class Involution(nn.Module):\n+\n+    def __init__(self, c1, c2, kernel_size, stride):\n+        \"\"\"\n+        Initialize the Involution module.\n+\n+        Args:\n+            c1 (int): Number of input channels.\n+            c2 (int): Number of output channels.\n+            kernel_size (int): Size of the involution kernel.\n+            stride (int): Stride for the involution operation.\n+        \"\"\"\n+        super().__init__()\n+        self.kernel_size = kernel_size\n+        self.stride = stride\n+        self.c1 = c1\n+        reduction_ratio = 1\n+        self.group_channels = 16\n+        self.groups = self.c1 // self.group_channels\n+        self.conv1 = Conv(c1, c1 // reduction_ratio, 1)\n+        self.conv2 = Conv(c1 // reduction_ratio, kernel_size ** 2 * self.groups, 1, 1)\n+\n+        if stride > 1:\n+            self.avgpool = nn.AvgPool2d(stride, stride)\n+        self.unfold = nn.Unfold(kernel_size, 1, (kernel_size - 1) // 2, stride)\n+\n+    def forward(self, x):\n+        \"\"\"\n+        Forward pass of the Involution module.\n+\n+        Args:\n+            x (torch.Tensor): Input tensor.\n+\n+        Returns:\n+            out (torch.Tensor): Output tensor after applying the involution operation.\n+        \"\"\"\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore')\n+            weight = self.conv2(x)\n+            b, c, h, w = weight.shape\n+            weight = weight.view(b, self.groups, self.kernel_size ** 2, h, w).unsqueeze(2)\n+            out = self.unfold(x).view(b, self.groups, self.group_channels, self.kernel_size ** 2, h, w)\n+            out = (weight * out).sum(dim=3).view(b, self.c1, h, w)\n+\n+            return out\ndiff --git a/models/hub/yolov5s-cbam-involution.yaml b/models/hub/yolov5s-cbam-involution.yaml\nnew file mode 100644\nindex 000000000000..9ac132e1cd78\n--- /dev/null\n+++ b/models/hub/yolov5s-cbam-involution.yaml\n@@ -0,0 +1,60 @@\n+# YOLOv5  by Ultralytics, GPL-3.0 license\n+\n+# Parameters\n+nc: 10  # number of classes\n+depth_multiple: 0.33  # model depth multiple\n+width_multiple: 0.50  # layer channel multiple\n+anchors:\n+  - [2.9434,4.0435, 3.8626,8.5592, 6.8534, 5.9391]\n+  - [10,13, 16,30, 33,23]  # P3/8\n+  - [30,61, 62,45, 59,119]  # P4/16\n+  - [116,90, 156,198, 373,326]  # P5/32\n+\n+# YOLOv5 v6.0 backbone\n+backbone:\n+  # [from, number, module, args]\n+  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n+   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n+   [-1, 3, C3, [128]],\n+   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n+   [-1, 6, C3, [256]],\n+   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n+   [-1, 9, C3, [512]],\n+   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n+   [-1, 3, C3, [1024]],\n+   [-1, 3, CBAM, [1024, 3]],\n+   [-1, 1, SPPF, [1024, 5]],  # 10\n+  ]\n+\n+# YOLOv5 v6.0 head\n+head:\n+  [[-1, 1, Involution, [1024, 1, 1]],\n+   [-1, 1, Conv, [512, 1, 1]],\n+   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n+   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n+   [-1, 3, C3, [512, False]],  # 15\n+\n+   [-1, 1, Conv, [512, 1, 1]],\n+   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n+   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n+   [-1, 3, C3, [512, False]],  # 19\n+\n+   [-1, 1, Conv, [256, 1, 1]],\n+   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n+   [[-1, 2], 1, Concat, [1]],\n+   [-1, 3, C3, [256, False]], # 23 160*160  p2 head\n+\n+   [-1, 1, Conv, [256, 3, 2]],\n+   [[-1, 19], 1, Concat, [1]],\n+   [-1, 3, C3, [512, False]],  # 26  80*80 p3 head\n+\n+   [-1, 1, Conv, [256, 3, 2]],\n+   [[-1, 15], 1, Concat, [1]],\n+   [-1, 3, C3, [256, False]], # 29 40*40 p4 head\n+\n+   [-1, 1, Conv, [512, 3, 2]],\n+   [[-1, 11], 1, Concat, [1]],\n+   [-1, 3, C3, [1024, False]],  # 32 20*20 p5 head\n+\n+   [[23, 26, 29, 32], 1, Detect, [nc, anchors]],  # Detect(P2, P3, P4, P5)\n+  ]\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR introduces the integration of CBAM and Involution modules into the YOLOv5 model, along with two configuration files, which appear well-documented and aligned with performance enhancement goals. While the modifications are promising, ensure all open issues are resolved and that tests pass successfully before merging. The user also encountered checks failing due to trailing whitespace and yapf formattingremoving unnecessary spaces and running the yapf formatter should resolve these issues. Overall, solid work, but focus on these cleanup tasks."
      }
    ],
    "codediff_length": 8133
  },
  {
    "index": 64,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/utils/metrics.py b/utils/metrics.py\nindex 5646f40e9860..c95af58c714c 100644\n--- a/utils/metrics.py\n+++ b/utils/metrics.py\n@@ -123,6 +124,171 @@ def compute_ap(recall, precision):\n     return ap, mpre, mrec\n \n \n+class AUROC:\n+    \"\"\" Compute the auroc scores, given the auc for each class.\n+    \"\"\"\n+\n+    def __init__(self, nc, conf=0.25, iou_thres=0.45):\n+        \"\"\"\n+        different thresholds (like prediction confidence, Iou) will have a great impact on the results of AUC.\n+        Default: conf=0.25, iou_thres=0.45\n+        \"\"\"\n+        self.auc_scores = np.zeros(nc)  # Store the AUROC score for each cls\n+        self.nc = nc  # number of cls\n+        self.conf = conf  # confidence threshold\n+        self.iou_thres = iou_thres  # IoU threshold\n+\n+        self.pred = [[] for _ in range(nc)]  # list to store model predictions for each class\n+        self.true = [[] for _ in range(nc)]  # list to store ground truth labels for each class\n+\n+        import subprocess\n+        subprocess.check_call(['pip', 'install', 'scikit-learn'])\n+        subprocess.check_call(['pip', 'install', 'plotly', 'kaleido'])\n+\n+    def process_batch(self, detections, labels):\n+        \"\"\"\n+        Return /\n+        Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n+        Arguments:\n+            detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n+            labels (Array[M, 5]), class, x1, y1, x2, y2\n+        Returns:\n+            None, updates pred[list] and true[list] accordingly\n+        \"\"\"\n+        if detections is None:\n+            # If there is no prediction result, all ground truths are considered to be negative samples,\n+            # Ignored during calculating auc\n+            return\n+\n+        t = 0\n+\n+        detections = detections[detections[:, 4] > self.conf]\n+        # Filter out prediction db boxes with low confidence (similar to nms)\n+        gt_classes = labels[:, 0].int()  # All gt box categories (int) cls, may be repeated\n+        detection_classes = detections[:, 5].int()  # All pred box cls (int) categories, may repeat positive + negative\n+        iou = box_iou(labels[:, 1:], detections[:, :4])  # # Find the iou of all gt boxes and all pred boxes\n+\n+        x = torch.where(iou > self.iou_thres)  # Filtered by iou threshold\n+\n+        if x[0].shape[0]:  # When have iou > iou threshold\n+            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\n+            # cat gt_index+pred_index+iou\n+            if x[0].shape[0] > 1:\n+                matches = matches[matches[:, 2].argsort()[::-1]]\n+                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n+                matches = matches[matches[:, 2].argsort()[::-1]]\n+                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n+                # finally get the one with the largest iou of each db pred and all db gt (greater than the iou_thres)\n+                # Each db gt will only correspond to the only one db pred. The filtered preds are all positive samples _> (tp or fp)\n+        else:\n+            matches = np.zeros((0, 3))\n+\n+        n = matches.shape[0] > 0\n+        m0, m1, _ = matches.transpose().astype(int)\n+        for class_id in range(self.nc):\n+            for i, gc in enumerate(gt_classes):\n+                if gc == class_id:\n+                    j = m0 == i\n+                    if n and sum(j) == 1:\n+                        if detection_classes[m1[j]] == gc:\n+                            # same cls -> True Positive\n+                            self.pred[class_id].append(detections[m1[j], 4].item())  # save conf\n+                            self.true[class_id].append(1)  # True Positive set 1\n+                        else:\n+                            # diff cls -> False Positive\n+                            self.pred[class_id].append(detections[m1[j], 4].item())  # save conf\n+                            self.true[class_id].append(0)  # False Positive set 0\n+                        t = t + 1\n+                    '''\n+                    # Ignored during calculating auc\n+                    else:\n+                        self.pred[class_id].append(0.0)\n+                        self.true[class_id].append(-1)\n+                    '''\n+\n+    def out(self):\n+        '''\n+        Computes the AUROC score for each category and returns it.\n+        '''\n+        from sklearn.metrics import roc_auc_score, roc_curve\n+        auc_scores = np.zeros(self.nc)\n+        fpr_ = [[] for _ in range(self.nc)]\n+        tpr_ = [[] for _ in range(self.nc)]\n+\n+        for class_id in range(self.nc):\n+            labels = self.true[class_id]\n+            preds = self.pred[class_id]\n+            try:\n+                fpr_class, tpr_class, _ = roc_curve(labels, preds)\n+                auc_scores[class_id] = roc_auc_score(labels, preds)\n+                fpr_[class_id] = fpr_class\n+                tpr_[class_id] = tpr_class\n+\n+            except ValueError:\n+                # No pred = set auc to 0\n+                # print('No pred db for cls ' + str(class_id) + ', Set the auc value to 0 ...')\n+                auc_scores[class_id] = 0\n+\n+        return auc_scores, fpr_, tpr_\n+\n+    def plot_polar_chart(self, auc_scores, save_dir='', names=()):\n+        '''\n+        Generate polar_chart for auc scores.\n+        auc_scores : [dict] auc_scores\n+        names : [list] cls names\n+        return None\n+        save img at Path(save_dir) / 'polar_chart.png'\n+        '''\n+        import plotly.graph_objects as go\n+        mauc = auc_scores.mean()\n+        auc_scores_name = dict(zip(names, auc_scores))\n+        auc_scores_name['mAUC'] = mauc\n+        df = pd.DataFrame.from_dict(auc_scores_name, orient='index')\n+        columns = list(df.index)\n+        fig = go.Figure(\n+            data=[go.Scatterpolar(r=(df[0] * 100).round(0), fill='toself', name='diseases', theta=columns)],\n+            layout=go.Layout(\n+                # title=go.layout.Title(text='Class AUC'),\n+                polar={\n+                    'radialaxis': {\n+                        'range': [0, 100],\n+                        'tickvals': [0, 25, 50, 75, 100],\n+                        'ticktext': ['0%', '25%', '50%', '75%', '100%'],\n+                        'visible': True, }},\n+                showlegend=True,\n+                template='plotly_dark',\n+            ),\n+        )\n+        file_name = Path(save_dir) / 'polar_chart.png'\n+        fig.write_image(file_name)\n+        # print('plot_polar_chart DONE')\n+\n+    def plot_auroc_curve(self, fpr_, tpr_, auc_scores, save_dir='', names=()):\n+        # AUROC curve\n+        fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n+\n+        if 0 < len(names) < 21:  # display per-class legend if < 21 classes\n+            for i in range(len(names)):\n+                ax.plot(fpr_[i], tpr_[i], linewidth=1, label=f'{names[i]} {auc_scores[i]:.3f}')  # plot(F_PR, T_PR)\n+        else:\n+            for i in range(len(names)):\n+                ax.plot(fpr_[i], tpr_[i], linewidth=1, color='grey')  # plot(F_PR, T_PR)\n+\n+        ax.plot([0, 1], [0, 1], linestyle='--', color='black', linewidth=1)  # diagonal line\n+        ax.set_xlabel('False Positive Rate')\n+        ax.set_ylabel('True Positive Rate')\n+        ax.set_xlim(0, 1)\n+        ax.set_ylim(0, 1)\n+        ax.legend(bbox_to_anchor=(1.04, 1), loc='upper left')\n+        ax.set_title('AUROC Curve')\n+        if save_dir:\n+            save_path = Path(save_dir) / 'auroc_curve.png'\n+            fig.savefig(save_path, dpi=250)\n+            # print(f'Saved AUROC curve at: {save_path}')\n+        plt.close(fig)\n+        # print('plot_auroc_curve DONE')\n+\n+\n class ConfusionMatrix:\n     # Updated version of https://github.com/kaanakan/object_detection_confusion_matrix\n     def __init__(self, nc, conf=0.25, iou_thres=0.45):\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The MAUC score being 0 across iterations suggests potential dataset issues or errors in the calculation logic. Check the confusion matrix; a zero true negative count indicates problems, especially with imbalanced datasets or single-class detection. Before adjusting hyperparameters, ensure AUC calculation aligns with expectationsrefer to the AUROC class in metrics.py for details. Also, to match the research paper, update all occurrences of the IOU threshold in the code. Adjustments in dataset handling might be necessary for better class segmentation."
      }
    ],
    "codediff_length": 7856
  },
  {
    "index": 65,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/gpt_engineer/chat_to_files.py b/gpt_engineer/chat_to_files.py\nindex 8e251cbdfd..0223a18fd1 100644\n--- a/gpt_engineer/chat_to_files.py\n+++ b/gpt_engineer/chat_to_files.py\n@@ -40,3 +41,57 @@ def to_files(chat, workspace):\n     files = parse_chat(chat)\n     for file_name, file_content in files:\n         workspace[file_name] = file_content\n+\n+\n+def overwrite_files(chat, dbs, replace_files):\n+    \"\"\"\n+    Replace the AI files to the older local files.\n+    \"\"\"\n+    dbs.workspace[\"all_output.txt\"] = chat\n+\n+    files = parse_chat(chat)\n+    for file_name, file_content in files:\n+        # Verify if the file created by the AI agent was in the input list\n+        if file_name in replace_files:\n+            # If the AI created a file from our input list, we replace it.\n+            with open(replace_files[file_name], \"w\") as text_file:\n+                text_file.write(file_content)\n+        else:\n+            # If the AI create a new file I don't know where to put it yet\n+            # maybe we can think in a smarter solution for this in the future\n+            # like asking the AI where to put it.\n+            #\n+            # by now, just add this to the workspace inside .gpteng folder\n+            print(\n+                f\"Could not find file path for '{file_name}', creating file in workspace\"\n+            )\n+            dbs.workspace[file_name] = file_content\n+\n+\n+def get_code_strings(input) -> dict[str, str]:\n+    \"\"\"\n+    Read file_list.txt and return file names and its content.\n+    \"\"\"\n+    files_paths = input[\"file_list.txt\"].strip().split(\"\\n\")\n+    files_dict = {}\n+    for file_path in files_paths:\n+        file_data = \"\"\n+        with open(file_path, \"r\") as file:\n+            file_data = file.read()\n+        if file_data:\n+            file_name = os.path.basename(file_path).split(\"/\")[-1]\n+            files_dict[file_name] = file_data\n+    return files_dict\n+\n+\n+def format_file_to_input(file_name: str, file_content: str) -> str:\n+    \"\"\"\n+    Format a file string to use as input to AI agent\n+    \"\"\"\n+    file_str = f\"\"\"\n+    {file_name}\n+    ```\n+    {file_content}\n+    ```\n+    \"\"\"\n+    return file_str\ndiff --git a/gpt_engineer/file_selector.py b/gpt_engineer/file_selector.py\nnew file mode 100644\nindex 0000000000..357b8f9094\n--- /dev/null\n+++ b/gpt_engineer/file_selector.py\n@@ -0,0 +1,318 @@\n+import os\n+import re\n+import sys\n+import tkinter as tk\n+import tkinter.filedialog as fd\n+\n+from pathlib import Path\n+from typing import List, Union\n+\n+\n+class DisplayablePath(object):\n+    \"\"\"\n+    A class representing a displayable path in a file explorer.\n+    \"\"\"\n+\n+    display_filename_prefix_middle = \" \"\n+    display_filename_prefix_last = \" \"\n+    display_parent_prefix_middle = \"    \"\n+    display_parent_prefix_last = \"   \"\n+\n+    def __init__(\n+        self, path: Union[str, Path], parent_path: \"DisplayablePath\", is_last: bool\n+    ):\n+        \"\"\"\n+        Initialize a DisplayablePath object.\n+\n+        Args:\n+            path (Union[str, Path]): The path of the file or directory.\n+            parent_path (DisplayablePath): The parent path of the file or directory.\n+            is_last (bool): Whether the file or directory is the last child of its parent.\n+        \"\"\"\n+        self.depth: int = 0\n+        self.path = Path(str(path))\n+        self.parent = parent_path\n+        self.is_last = is_last\n+        if self.parent:\n+            self.depth = self.parent.depth + 1\n+\n+    @property\n+    def display_name(self) -> str:\n+        \"\"\"\n+        Get the display name of the file or directory.\n+\n+        Returns:\n+            str: The display name.\n+        \"\"\"\n+        if self.path.is_dir():\n+            return self.path.name + \"/\"\n+        return self.path.name\n+\n+    @classmethod\n+    def make_tree(cls, root: Union[str, Path], parent=None, is_last=False, criteria=None):\n+        \"\"\"\n+        Generate a tree of DisplayablePath objects.\n+\n+        Args:\n+            root: The root path of the tree.\n+            parent: The parent path of the root path. Defaults to None.\n+            is_last: Whether the root path is the last child of its parent.\n+            criteria: The criteria function to filter the paths. Defaults to None.\n+\n+        Yields:\n+            DisplayablePath: The DisplayablePath objects in the tree.\n+        \"\"\"\n+        root = Path(str(root))\n+        criteria = criteria or cls._default_criteria\n+\n+        displayable_root = cls(root, parent, is_last)\n+        yield displayable_root\n+\n+        children = sorted(\n+            list(path for path in root.iterdir() if criteria(path)),\n+            key=lambda s: str(s).lower(),\n+        )\n+        count = 1\n+        for path in children:\n+            is_last = count == len(children)\n+            if path.is_dir():\n+                yield from cls.make_tree(\n+                    path, parent=displayable_root, is_last=is_last, criteria=criteria\n+                )\n+            else:\n+                yield cls(path, displayable_root, is_last)\n+            count += 1\n+\n+    @classmethod\n+    def _default_criteria(cls, path: Path) -> bool:\n+        \"\"\"\n+        The default criteria function to filter the paths.\n+\n+        Args:\n+            path: The path to check.\n+\n+        Returns:\n+            bool: True if the path should be included, False otherwise.\n+        \"\"\"\n+        return True\n+\n+    def displayable(self) -> str:\n+        \"\"\"\n+        Get the displayable string representation of the file or directory.\n+\n+        Returns:\n+            str: The displayable string representation.\n+        \"\"\"\n+        if self.parent is None:\n+            return self.display_name\n+\n+        _filename_prefix = (\n+            self.display_filename_prefix_last\n+            if self.is_last\n+            else self.display_filename_prefix_middle\n+        )\n+\n+        parts = [\"{!s} {!s}\".format(_filename_prefix, self.display_name)]\n+\n+        parent = self.parent\n+        while parent and parent.parent is not None:\n+            parts.append(\n+                self.display_parent_prefix_middle\n+                if parent.is_last\n+                else self.display_parent_prefix_last\n+            )\n+            parent = parent.parent\n+\n+        return \"\".join(reversed(parts))\n+\n+\n+class TerminalFileSelector:\n+    def __init__(self, root_folder_path: Path) -> None:\n+        self.number_of_selectable_items = 0\n+        self.selectable_file_paths: dict[int, str] = {}\n+        self.file_path_list: list = []\n+        self.db_paths = DisplayablePath.make_tree(\n+            root_folder_path, parent=None, criteria=self.is_in_ignoring_extensions\n+        )\n+\n+    def display(self):\n+        \"\"\"\n+        Select files from a directory and display the selected files.\n+        \"\"\"\n+        count = 0\n+        file_path_enumeration = {}\n+        file_path_list = []\n+        for path in self.db_paths:\n+            n_digits = len(str(count))\n+            n_spaces = 3 - n_digits\n+            if n_spaces < 0:\n+                # We can only print 1000 aligned files. I think it is decent enough\n+                n_spaces = 0\n+            spaces_str = \" \" * n_spaces\n+            if not path.path.is_dir():\n+                print(f\"{count}. {spaces_str}{path.displayable()}\")\n+                file_path_enumeration[count] = path.path\n+                file_path_list.append(path.path)\n+                count += 1\n+            else:\n+                # By now we do not accept selecting entire dirs.\n+                # But could add that in the future. Just need to add more functions\n+                # and remove this else block...\n+                number_space = \" \" * n_digits\n+                print(f\"{number_space}  {spaces_str}{path.displayable()}\")\n+\n+        self.number_of_selectable_items = count\n+        self.file_path_list = file_path_list\n+        self.selectable_file_paths = file_path_enumeration\n+\n+    def ask_for_selection(self) -> List[str]:\n+        \"\"\"\n+        Ask user to select files from the terminal after displaying it\n+\n+        Returns:\n+            List[str]: list of selected paths\n+        \"\"\"\n+        user_input = input(\n+            \"\\nSelect files by entering the numbers separated by commas/spaces or \"\n+            + \"specify range with a dash. \"\n+            + \"Example: 1,2,3-5,7,9,13-15,18,20 (enter 'all' to select everything)\"\n+            + \"\\n\\nSelect files:\"\n+        )\n+        selected_paths = []\n+        regex = r\"\\d+(-\\d+)?([, ]\\d+(-\\d+)?)*\"\n+\n+        if user_input.lower() == \"all\":\n+            selected_paths = self.file_path_list\n+        elif re.match(regex, user_input):\n+            try:\n+                user_input = (\n+                    user_input.replace(\"\", \",\") if \" \" in user_input else user_input\n+                )\n+                selected_files = user_input.split(\",\")\n+                for file_number_str in selected_files:\n+                    if \"-\" in file_number_str:\n+                        start_str, end_str = file_number_str.split(\"-\")\n+                        start = int(start_str)\n+                        end = int(end_str)\n+                        for num in range(start, end + 1):\n+                            selected_paths.append(str(self.selectable_file_paths[num]))\n+                    else:\n+                        num = int(file_number_str)\n+                        selected_paths.append(str(self.selectable_file_paths[num]))\n+\n+            except ValueError:\n+                pass\n+        else:\n+            print(\"Please use a valid number/series of numbers.\\n\")\n+            sys.exit(1)\n+\n+        return selected_paths\n+\n+    def is_in_ignoring_extensions(self, path: Path) -> bool:\n+        \"\"\"\n+        Check if a path is not hidden or in the __pycache__ directory.\n+\n+        Args:\n+            path: The path to check.\n+\n+        Returns:\n+            bool: True if the path is not in ignored rules. False otherwise.\n+        \"\"\"\n+        is_hidden = not path.name.startswith(\".\")\n+        is_pycache = \"__pycache__\" not in path.name\n+        return is_hidden and is_pycache\n+\n+\n+def ask_for_files(db_input) -> dict[str, str]:\n+    \"\"\"\n+    Ask user to select files to improve.\n+    It can be done by terminal, gui, or using the old selection.\n+\n+    Returns:\n+        dict[str, str]: Dictionary where key = file name and value = file path\n+    \"\"\"\n+    use_last_string = \"\"\n+    selection_number = 0\n+    is_valid_selection = False\n+    can_use_last = False\n+    if \"file_list.txt\" in db_input:\n+        can_use_last = True\n+        use_last_string = (\n+            \"2. Use previous file list (available at \"\n+            + f\"{os.path.join(db_input.path, 'file_list.txt')})\\n\"\n+        )\n+    selection_str = f\"\"\"How do you want to select the files?\n+\n+0. Use Command-Line.\n+1. Use File explorer.\n+{use_last_string if len(use_last_string) > 1 else \"\"}\n+Select option and press Enter (default={selection_number}): \"\"\"\n+    file_path_list = []\n+    selected_number_str = input(selection_str)\n+    if selected_number_str:\n+        try:\n+            selection_number = int(selected_number_str)\n+        except ValueError:\n+            print(\"Invalid number. Select a number from the list above.\\n\")\n+            sys.exit(1)\n+    if selection_number == 0:\n+        # Open terminal selection\n+        file_path_list = terminal_file_selector()\n+        is_valid_selection = True\n+    elif selection_number == 1:\n+        # Open GUI selection\n+        file_path_list = gui_file_selector()\n+        is_valid_selection = True\n+    else:\n+        if can_use_last and selection_number == 2:\n+            # Use previous file list\n+            is_valid_selection = True\n+    if not is_valid_selection:\n+        print(\"Invalid number. Select a number from the list above.\\n\")\n+        sys.exit(1)\n+\n+    file_list_string = \"\"\n+    file_path_info = {}\n+    if not selection_number == 2:\n+        # New files\n+        for file_path in file_path_list:\n+            file_list_string += str(file_path) + \"\\n\"\n+            # Return a dict with key=file_name and value=file_path\n+            file_path_info[os.path.basename(file_path).split(\"/\")[-1]] = file_path\n+        # Write in file_list so the user can edit and remember what was done\n+        db_input[\"file_list.txt\"] = file_list_string\n+    else:\n+        # If using the the previous file list, we don't need to write file_list.txt\n+        file_list_string = db_input[\"file_list.txt\"]\n+        for file_path in file_path_list:\n+            # Return a dict with key=file_name and value=file_path\n+            file_path_info[os.path.basename(file_path).split(\"/\")[-1]] = file_path\n+\n+    return file_path_info\n+\n+\n+def gui_file_selector() -> List[str]:\n+    \"\"\"\n+    Display a tkinter file selection window to select context files.\n+    \"\"\"\n+    root = tk.Tk()\n+    root.withdraw()\n+    root.call(\"wm\", \"attributes\", \".\", \"-topmost\", True)\n+    file_list = list(\n+        fd.askopenfilenames(\n+            parent=root,\n+            initialdir=os.getcwd(),\n+            title=\"Select files to improve (or give context):\",\n+        )\n+    )\n+    return file_list\n+\n+\n+def terminal_file_selector() -> List[str]:\n+    \"\"\"\n+    Display a terminal file selection to select context files.\n+    \"\"\"\n+    file_selector = TerminalFileSelector(Path(os.getcwd()))\n+    file_selector.display()\n+    selected_list = file_selector.ask_for_selection()\n+    return selected_list\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "This PR offers a solid enhancement, but it lacks unit tests and integration tests to ensure new functionality. Consider adding exception handling in file operations to prevent crashes due to file permission issues. The `ask_for_files` function should be refactored for clarity, and `improve_existing_code` would benefit from modular decomposition. Lastly, simplify user prompts and streamline directory usage to enhance user experience. Great progress overall; just a few refinements needed before merging."
      }
    ],
    "codediff_length": 13367
  },
  {
    "index": 66,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/gpt_engineer/core/chat_to_files.py b/gpt_engineer/core/chat_to_files.py\nindex 385d941bdc..2a166fabac 100644\n--- a/gpt_engineer/core/chat_to_files.py\n+++ b/gpt_engineer/core/chat_to_files.py\n@@ -199,3 +203,75 @@ def format_file_to_input(file_name: str, file_content: str) -> str:\n     ```\n     \"\"\"\n     return file_str\n+\n+\n+def overwrite_files_with_edits(chat: str, dbs: DBs):\n+    edits = parse_edits(chat)\n+    apply_edits(edits, dbs.workspace)\n+\n+\n+@dataclass\n+class Edit:\n+    filename: str\n+    before: str\n+    after: str\n+\n+\n+def parse_edits(llm_response):\n+    def parse_one_edit(lines):\n+        HEAD = \"<<<<<<< HEAD\"\n+        DIVIDER = \"=======\"\n+        UPDATE = \">>>>>>> updated\"\n+\n+        filename = lines.pop(0)\n+        text = \"\\n\".join(lines)\n+        splits = text.split(DIVIDER)\n+        if len(splits) != 2:\n+            raise ValueError(f\"Could not parse following text as code edit: \\n{text}\")\n+        before, after = splits\n+\n+        before = before.replace(HEAD, \"\").strip()\n+        after = after.replace(UPDATE, \"\").strip()\n+\n+        return Edit(filename, before, after)\n+\n+    def parse_all_edits(txt):\n+        edits = []\n+        current_edit = []\n+        in_fence = False\n+\n+        for line in txt.split(\"\\n\"):\n+            if line.startswith(\"```\") and in_fence:\n+                edits.append(parse_one_edit(current_edit))\n+                current_edit = []\n+                in_fence = False\n+                continue\n+            elif line.startswith(\"```\") and not in_fence:\n+                in_fence = True\n+                continue\n+\n+            if in_fence:\n+                current_edit.append(line)\n+\n+        return edits\n+\n+    return parse_all_edits(llm_response)\n+\n+\n+def apply_edits(edits: List[Edit], workspace: DB):\n+    for edit in edits:\n+        filename = edit.filename\n+        if edit.before == \"\":\n+            if workspace.get(filename) is not None:\n+                logger.warn(\n+                    f\"The edit to be applied wants to create a new file `{filename}`, but that already exists. The file will be overwritten. See `.gpteng/memory` for previous version.\"\n+                )\n+            workspace[filename] = edit.after  # new file\n+        else:\n+            if workspace[filename].count(edit.before) > 1:\n+                logger.warn(\n+                    f\"While applying an edit to `{filename}`, the code block to be replaced was found multiple times. All instances will be replaced.\"\n+                )\n+            workspace[filename] = workspace[filename].replace(\n+                edit.before, edit.after\n+            )  # existing file\ndiff --git a/gpt_engineer/preprompts/improve b/gpt_engineer/preprompts/improve\nindex 4f51f25b8f..95f2a03548 100644\n--- a/gpt_engineer/preprompts/improve\n+++ b/gpt_engineer/preprompts/improve\n@@ -1,12 +1,78 @@\n-You are a expert developer.\n-You will receive some (but not all) relevant files in the existing codebase to work on.\n-Based on a request, you need to comprehend what needs to be modified by understanding the implementation of the received code and the interconnections between each file, function and classes.\n+Act as an expert software developer.\n+Always use best practices when coding.\n+When you edit or add code, respect and use existing conventions, libraries, etc.\n \n-Think step by step and reason yourself to correct decisions to make sure we get it right.\n+Take requests for changes to the supplied code, and then you MUST\n+1. (planning) Think step-by-step and explain the needed changes. Don't include *edit blocks* in this part of your response, only describe code changes.\n+2. (output) Describe each change with an *edit block* per the example below.\n \n-Adapt to the style and frameworks used in the existing codebase.\n-Make sure that code in different files are compatible with each other.\n-Ensure to implement the requested modification, if you are unsure, write a plausible implementation.\n-Before you finish, double check that all parts of the necessary architecture for the request is present in the files.\n+You MUST format EVERY code change with an *edit block* like this:\n+```python\n+some/dir/example.py\n+<<<<<<< HEAD\n+    # some comment\n+    # Func to multiply\n+    def mul(a,b)\n+=======\n+    # updated comment\n+    # Function to add\n+    def add(a,b):\n+>>>>>>> updated\n+```\n+Remember, you can use multiple *edit blocks* per file.\n \n-FILE_FORMAT\n+Here is an example reponse:\n+---\n+PLANNING:\n+We need to change ... because ..., therefore I will add the line `a=a+1` to the function `add_one`.\n+Also, in the class `DB`, we need to update the ...\n+\n+OUTPUT:\n+```python\n+some/dir/example_1.py\n+<<<<<<< HEAD\n+    def mul(a,b)\n+=======\n+    def add(a,b):\n+>>>>>>> updated\n+```\n+\n+```python\n+some/dir/example_1.py\n+<<<<<<< HEAD\n+    def add_one(a,b):\n+        a = a+2\n+=======\n+    def add_one(a,b):\n+        a = a+1\n+>>>>>>> updated\n+```\n+\n+```python\n+some/dir/example_2.py\n+<<<<<<< HEAD\n+    class DBS:\n+        db = 'aaa'\n+=======\n+    class DBS:\n+        db = 'bbb'\n+>>>>>>> updated\n+```\n+---\n+\n+A program will parse the edit blocks you generate and replace the `HEAD` lines with the `updated` lines.\n+So edit blocks must be precise and unambiguous!\n+\n+Every *edit block* must be fenced with ```...``` with the correct code language.\n+\n+The `HEAD` section must be an *exact set of sequential lines* from the file! This is very important. Otherwise the parser won't work.\n+NEVER SKIP LINES in the `HEAD` section!\n+NEVER ELIDE LINES AND REPLACE THEM WITH A COMMENT!\n+NEVER OMIT ANY WHITESPACE in the `HEAD` section!\n+\n+Edits to different parts of a file each need their own *edit block*.\n+\n+If you want to put code in a new file, use an edit block with:\n+- A new file path, including dir name if needed\n+- An empty `HEAD` section\n+- The new file's contents in the `updated` section\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "It's crucial to test with `gpt3.5-turbo` since outputs may vary, affecting your logic. I recommend consolidating the parsing logic in `parse_all_edits` by reusing `parse_chat` to maintain clean code. Also, consider how existing files interact with new features; the model should add new files without wiping the workspace. Lastly, address error handling for additional outputs like bash commands to avoid `ValueError`. Ensure clarity on expectations for multiple suggested editscurrently, only the first is processed."
      }
    ],
    "codediff_length": 5833
  },
  {
    "index": 67,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/gpt_engineer/applications/cli/file_selector.py b/gpt_engineer/applications/cli/file_selector.py\nindex dad72dbcc5..c98e4c77fd 100644\n--- a/gpt_engineer/applications/cli/file_selector.py\n+++ b/gpt_engineer/applications/cli/file_selector.py\n@@ -1,86 +1,267 @@\n \"\"\"\n-This module provides functionalities for selecting files from both a graphical file\n-explorer and terminal-based file explorer.\n-\n-It allows the user to choose files for the purpose of context improvement. This module\n-provides a tree-based display in the terminal to enable file selection with support for\n-navigating through directories and ignoring specified directories.\n-\n-Features:\n-    - Supports both graphical (using `tkinter`) and terminal-based file selection.\n-    - Provides a tree-based display of directories and files.\n-    - Allows for custom filtering of displayed files and directories.\n-    - Support to reuse a previous file selection list.\n-    - Option to ignore specific directories (e.g. \"site-packages\", \"node_modules\", \"venv\").\n-\n-Classes:\n-    - DisplayablePath: Represents a displayable path in a file explorer, allowing for a\n-      tree structure display in the terminal.\n-    - TerminalFileSelector: Enables terminal-based file selection.\n-\n-Functions:\n-    - is_in_ignoring_extensions: Checks if a path should be ignored based on predefined rules.\n-    - ask_for_files: Asks user to select files from either GUI or terminal or uses a previous\n-      file list.\n-    - gui_file_selector: Displays a GUI for file selection.\n-    - terminal_file_selector: Displays a terminal interface for file selection.\n-\n-Dependencies:\n-    - os\n-    - re\n-    - sys\n-    - tkinter\n-    - pathlib\n-    - typing\n-\n-Note:\n-    This module is built on top of `gpt_engineer.core.db` and assumes existence and\n-    functionalities provided by DB and DBs classes.\n+file_selector.py\n+\n+This module offers interactive file selection for projects. Leveraging a terminal-based,\n+tree-structured display, users can navigate and select files for editing or processing.\n+It integrates with system editors for direct file modification and supports saving\n+selections for later use. Designed for efficient workflow enhancement in file-intensive\n+environments, it offers customizable file filtering and seamless editor integration.\n+\n+Key Components:\n+- FileSelector: Manages file selection and interaction.\n+- DisplayablePath: Provides a structured view of file paths.\n+\n+Usage:\n+Typically used in project setup or management phases for selecting specific files.\n+It operates within the GPT-Engineer environment, relying on core functionalities for\n+file handling and persistence.\n+\n \"\"\"\n \n import os\n-import re\n-import sys\n-import tkinter as tk\n-import tkinter.filedialog as fd\n+import subprocess\n \n from pathlib import Path\n-from typing import List, Union\n+from typing import Any, Dict, List, Union\n+\n+import toml\n \n from gpt_engineer.core.default.disk_memory import DiskMemory\n from gpt_engineer.core.default.paths import metadata_path\n from gpt_engineer.core.files_dict import FilesDict\n \n-IGNORE_FOLDERS = {\"site-packages\", \"node_modules\", \"venv\"}\n-FILE_LIST_NAME = \"file_list.txt\"\n+\n+class FileSelector:\n+    def __init__(self, project_path: Union[str, Path]):\n+        self.IGNORE_FOLDERS = {\"site-packages\", \"node_modules\", \"venv\", \".gpteng\"}\n+        self.FILE_LIST_NAME = \"file_selection.toml\"\n+        self.COMMENT = (\n+            \"# Change 'selected' from false to true to include files in the edit. \"\n+            \"GPT-engineer can only read and edit the files that set to true. \"\n+            \"Including irrelevant files will degrade coding performance, \"\n+            \"cost additional tokens and potentially lead to violations \"\n+            \"of the token limit, resulting in runtime errors.\\n\\n\"\n+        )\n+\n+        self.project_path = project_path\n+        self.metadata_db = DiskMemory(metadata_path(self.project_path))\n+        self.toml_path = self.metadata_db.path / self.FILE_LIST_NAME\n+\n+    def ask_for_files(self) -> FilesDict:\n+        \"\"\"\n+        Asks the user to select files for the purpose of context improvement.\n+        It supports selection from the terminal or using a previously saved list.\n+        \"\"\"\n+        if os.getenv(\"GPTE_TEST_MODE\"):\n+            # In test mode, retrieve files from a predefined TOML configuration\n+            assert self.FILE_LIST_NAME in self.metadata_db\n+            selected_files = self.get_files_from_toml(self.project_path, self.toml_path)\n+        else:\n+            # Otherwise, use the editor file selector for interactive selection\n+            if self.FILE_LIST_NAME in self.metadata_db:\n+                print(\n+                    f\"File list detected at {self.toml_path}. Edit or delete it if you want to select new files.\"\n+                )\n+                selected_files = self.editor_file_selector(self.project_path, False)\n+            else:\n+                selected_files = self.editor_file_selector(self.project_path, True)\n+\n+        content_dict = {}\n+        for file_path in selected_files:\n+            # selected files contains paths that are relative to the project path\n+            try:\n+                # to open the file we need the path from the cwd\n+                with open(Path(self.project_path) / file_path, \"r\") as content:\n+                    content_dict[str(file_path)] = content.read()\n+            except FileNotFoundError:\n+                print(f\"Warning: File not found {file_path}\")\n+        return FilesDict(content_dict)\n+\n+    def editor_file_selector(self, input_path: str, init: bool = True) -> List[str]:\n+        \"\"\"\n+        Provides an interactive file selection interface by generating a tree representation in a .toml file.\n+        Allows users to select or deselect files for the context improvement process.\n+        \"\"\"\n+        root_path = Path(input_path)\n+        tree_dict = {\n+            \"files\": {}\n+        }  # Initialize the dictionary to hold file selection state\n+        toml_file = DiskMemory(metadata_path(input_path)).path / \"file_selection.toml\"\n+        # Define the toml file path\n+\n+        # Initialize .toml file with file tree if in initial state\n+        if init:\n+            for path in DisplayablePath.make_tree(\n+                root_path\n+            ):  # Create a tree structure from the root path\n+                if path.path.is_dir() or not self.is_utf8(path.path):\n+                    continue\n+                relative_path = os.path.relpath(\n+                    path.path, input_path\n+                )  # Get the relative path of the file\n+                tree_dict[\"files\"][relative_path] = {\n+                    \"selected\": False\n+                }  # Initialize file selection as False\n+\n+            # Write instructions and file selection states to .toml file\n+\n+            with open(toml_file, \"w\") as f:\n+                f.write(self.COMMENT)\n+                toml.dump(tree_dict, f)\n+        else:\n+            # Load existing files from the .toml configuration\n+            with open(toml_file, \"r\") as file:\n+                existing_files = toml.load(file)\n+                merged_files = self.merge_file_lists(\n+                    existing_files[\"files\"], self.get_current_files(root_path)\n+                )\n+\n+            # Write the merged list back to the .toml for user review and modification\n+            with open(toml_file, \"w\") as file:\n+                file.write(self.COMMENT)  # Ensure to write the comment\n+                toml.dump({\"files\": merged_files}, file)\n+\n+        print(\n+            \"Please select(true) and deselect(false) files, save it, and close it to continue...\"\n+        )\n+        self.open_with_default_editor(\n+            toml_file\n+        )  # Open the .toml file in the default editor for user modification\n+        return self.get_files_from_toml(\n+            input_path, toml_file\n+        )  # Return the list of selected files after user edits\n+\n+    def open_with_default_editor(self, file_path):\n+        \"\"\"\n+        Attempts to open the specified file using the system's default text editor or a common fallback editor.\n+        \"\"\"\n+        editors = [\n+            \"gedit\",\n+            \"notepad\",\n+            \"write\",\n+            \"nano\",\n+            \"vim\",\n+            \"emacs\",\n+        ]  # Putting the beginner-friendly text editor forward\n+        chosen_editor = os.environ.get(\"EDITOR\")\n+\n+        # Try the preferred editor first, then fallback to common editors\n+        if chosen_editor:\n+            try:\n+                subprocess.run([chosen_editor, file_path])\n+                return\n+            except Exception:\n+                pass\n+\n+        for editor in editors:\n+            try:\n+                subprocess.run([editor, file_path])\n+                return\n+            except Exception:\n+                continue\n+        print(\"No suitable text editor found. Please edit the file manually.\")\n+\n+    def is_utf8(self, file_path):\n+        \"\"\"\n+        Determines if the file is UTF-8 encoded by trying to read and decode it.\n+        Useful for ensuring that files are in a readable and compatible format.\n+        \"\"\"\n+        try:\n+            with open(file_path, \"rb\") as file:\n+                file.read().decode(\"utf-8\")\n+                return True\n+        except UnicodeDecodeError:\n+            return False\n+\n+    def get_files_from_toml(self, input_path, toml_file):\n+        \"\"\"\n+        Retrieves the list of files selected by the user from a .toml configuration file.\n+        This function parses the .toml file and returns the list of selected files.\n+        \"\"\"\n+        selected_files = []\n+        edited_tree = toml.load(toml_file)  # Load the edited .toml file\n+\n+        # Iterate through the files in the .toml and append selected files to the list\n+        for file, properties in edited_tree[\"files\"].items():\n+            if properties.get(\"selected\", False):  # Check if the file is selected\n+                selected_files.append(file)\n+\n+        # Ensure that at least one file is selected, or raise an exception\n+        if not selected_files:\n+            raise Exception(\n+                \"No files were selected. Please select at least one file to proceed.\"\n+            )\n+\n+        print(f\"\\nYou have selected the following files:\\n{input_path}\")\n+\n+        project_path = Path(input_path).resolve()\n+        all_paths = set(\n+            project_path.joinpath(file).resolve(strict=False) for file in selected_files\n+        )\n+\n+        try:\n+            for displayable_path in DisplayablePath.make_tree(project_path):\n+                if displayable_path.path in all_paths:\n+                    print(displayable_path.displayable())\n+        except FileNotFoundError:\n+            print(\"Specified path does not exist: \", project_path)\n+        except Exception as e:\n+            print(\"An error occurred while trying to display the file tree:\", e)\n+\n+        print(\"\\n\")\n+        return selected_files\n+\n+    def merge_file_lists(\n+        self, existing_files: Dict[str, Any], new_files: Dict[str, Any]\n+    ) -> Dict[str, Any]:\n+        \"\"\"\n+        Merges the new files list with the existing one, preserving the selection status.\n+        \"\"\"\n+        # Update the existing files with any new files or changes\n+        for file, properties in new_files.items():\n+            if file not in existing_files:\n+                existing_files[file] = properties  # Add new files as unselected\n+            # If you want to update other properties of existing files, you can do so here\n+\n+        return existing_files\n+\n+    def get_current_files(self, project_path: Union[str, Path]) -> Dict[str, Any]:\n+        \"\"\"\n+        Generates a dictionary of all files in the project directory\n+        with their selection status set to False by default.\n+        \"\"\"\n+        all_files = {}\n+        project_path = Path(\n+            project_path\n+        ).resolve()  # Ensure path is absolute and resolved\n+\n+        for path in project_path.glob(\"**/*\"):  # Recursively list all files\n+            if path.is_file():\n+                # Normalize and compare each part of the path\n+                if not any(\n+                    part in self.IGNORE_FOLDERS\n+                    for part in path.relative_to(project_path).parts\n+                ) and not path.name.startswith(\".\"):\n+                    relative_path = str(\n+                        path.relative_to(project_path)\n+                    )  # Store relative paths\n+                    all_files[relative_path] = {\"selected\": False}\n+        return all_files\n+\n+    def is_in_ignoring_extensions(self, path: Path) -> bool:\n+        \"\"\"\n+        Check if a path is not hidden or in the '__pycache__' directory.\n+        Helps in filtering out unnecessary files during file selection.\n+        \"\"\"\n+        is_hidden = not path.name.startswith(\".\")\n+        is_pycache = \"__pycache__\" not in path.name\n+        return is_hidden and is_pycache\n \n \n class DisplayablePath(object):\n     \"\"\"\n-    A class that represents a path in a file system and provides functionality\n-    to display it in a tree-like structure similar to that of a file explorer.\n-\n-    Class Attributes:\n-        - display_filename_prefix_middle (str): Prefix for filenames in the middle of a list.\n-        - display_filename_prefix_last (str): Prefix for filenames at the end of a list.\n-        - display_parent_prefix_middle (str): Prefix for parent directories in the middle of a list.\n-        - display_parent_prefix_last (str): Prefix for parent directories at the end of a list.\n-\n-    Attributes:\n-        - depth (int): Depth of the path in relation to the root.\n-        - path (Path): The actual path object.\n-        - parent (DisplayablePath): Parent path. None if it's the root.\n-        - is_last (bool): Flag to check if the current path is the last child of its parent.\n-\n-    Methods:\n-        - display_name: Return the display name for the path, with directories having a trailing '/'.\n-        - make_tree: Class method to generate a tree of DisplayablePath objects for the given root.\n-        - _default_criteria: Default criteria for filtering paths.\n-        - displayable: Generate the displayable string representation of the file or directory.\n-\n-    Note:\n-        It is assumed that the global constant IGNORE_FOLDERS is defined elsewhere,\n-        which lists the folder names to ignore during the tree generation.\n+    Represents a path in a file system and displays it in a tree-like structure.\n+    Useful for displaying file and directory structures like in a file explorer.\n     \"\"\"\n \n     display_filename_prefix_middle = \" \"\n@@ -195,253 +347,4 @@ def displayable(self) -> str:\n             )\n             parent = parent.parent\n \n-        return \"\".join(reversed(parts))\n-\n-\n-class TerminalFileSelector:\n-    \"\"\"\n-    A terminal-based file selector for navigating and selecting files from a specified root folder.\n-\n-    Attributes:\n-        number_of_selectable_items (int): The number of items (files) that can be selected.\n-        selectable_file_paths (dict[int, str]): A mapping from index number to the corresponding file path.\n-        file_path_list (list): A list containing paths of the displayed files.\n-        db_paths: A structured representation of all paths (both files and directories) within the root folder.\n-\n-    Args:\n-        root_folder_path (Path): The root folder path from where files are to be listed and selected.\n-\n-    Methods:\n-        display(): Prints the list of files and directories to the terminal, allowing files to be selectable by number.\n-        ask_for_selection() -> List[str]: Prompts the user to select files by providing index numbers and returns the list of selected file paths.\n-    \"\"\"\n-\n-    def __init__(self, root_folder_path: Path) -> None:\n-        self.number_of_selectable_items = 0\n-        self.selectable_file_paths: dict[int, str] = {}\n-        self.file_path_list: list = []\n-        self.db_paths = DisplayablePath.make_tree(\n-            root_folder_path, parent=None, criteria=is_in_ignoring_extensions\n-        )\n-        self.root_folder_path = root_folder_path\n-\n-    def display(self):\n-        \"\"\"\n-        Displays a list of files from the root folder in the terminal. Files are enumerated for selection,\n-        while directories are simply listed (currently non-selectable).\n-        \"\"\"\n-        count = 0\n-        file_path_enumeration = {}\n-        file_path_list = []\n-        for path in self.db_paths:\n-            n_digits = len(str(count))\n-            n_spaces = 3 - n_digits\n-            if n_spaces < 0:\n-                # We can only print 1000 aligned files. I think it is decent enough\n-                n_spaces = 0\n-            spaces_str = \" \" * n_spaces\n-            if not path.path.is_dir():\n-                print(f\"{count}. {spaces_str}{path.displayable()}\")\n-                file_path_enumeration[count] = path.path\n-                file_path_list.append(path.path)\n-                count += 1\n-            else:\n-                # By now we do not accept selecting entire dirs.\n-                # But could add that in the future. Just need to add more functions\n-                # and remove this else block...\n-                number_space = \" \" * n_digits\n-                print(f\"{number_space}  {spaces_str}{path.displayable()}\")\n-\n-        self.number_of_selectable_items = count\n-        self.file_path_list = file_path_list\n-        self.selectable_file_paths = file_path_enumeration\n-\n-    def ask_for_selection(self, all: bool = False) -> List[str]:\n-        \"\"\"\n-        Prompts the user to select files by providing a series of index numbers, ranges, or 'all' to select everything.\n-\n-        Returns:\n-            List[str]: A list of selected file paths based on user's input.\n-\n-        Notes:\n-            - Users can select files by entering index numbers separated by commas or spaces.\n-            - Ranges can be specified using a dash.\n-            - Example input: 1,2,3-5,7,9,13-15,18,20\n-            - Users can also input 'all' to select all displayed files.\n-        \"\"\"\n-        if all:\n-            user_input = \"all\"\n-        else:\n-            user_input = input(\n-                \"\\n\".join(\n-                    [\n-                        \"Select files by entering the numbers separated by commas/spaces or\",\n-                        \"specify range with a dash. \",\n-                        \"Example: 1,2,3-5,7,9,13-15,18,20 (enter 'all' to select everything)\",\n-                        \"\\n\\nSelect files:\",\n-                    ]\n-                )\n-            )\n-        selected_paths = []\n-        regex = r\"\\d+(-\\d+)?([, ]\\d+(-\\d+)?)*\"\n-\n-        if user_input.lower() == \"all\":\n-            selected_paths = self.file_path_list\n-        elif re.match(regex, user_input):\n-            try:\n-                user_input = (\n-                    user_input.replace(\" \", \",\") if \" \" in user_input else user_input\n-                )\n-                selected_files = user_input.split(\",\")\n-                for file_number_str in selected_files:\n-                    if \"-\" in file_number_str:\n-                        start_str, end_str = file_number_str.split(\"-\")\n-                        start = int(start_str)\n-                        end = int(end_str)\n-                        for num in range(start, end + 1):\n-                            selected_paths.append(str(self.selectable_file_paths[num]))\n-                    else:\n-                        num = int(file_number_str)\n-                        selected_paths.append(str(self.selectable_file_paths[num]))\n-\n-            except ValueError:\n-                pass\n-        else:\n-            print(\"Please use a valid number/series of numbers.\\n\")\n-            sys.exit(1)\n-\n-        return selected_paths\n-\n-\n-def is_in_ignoring_extensions(path: Path) -> bool:\n-    \"\"\"\n-    Check if a path is not hidden or in the __pycache__ directory.\n-\n-    Args:\n-        path: The path to check.\n-\n-    Returns:\n-        bool: True if the path is not in ignored rules. False otherwise.\n-    \"\"\"\n-    is_hidden = not path.name.startswith(\".\")\n-    is_pycache = \"__pycache__\" not in path.name\n-    return is_hidden and is_pycache\n-\n-\n-def ask_for_files(project_path: Union[str, Path]) -> FilesDict:\n-    \"\"\"\n-    Ask user to select files to improve.\n-    It can be done by terminal, gui, or using the old selection.\n-\n-    Returns:\n-        dict[str, str]: Dictionary where key = file name and value = file path\n-    \"\"\"\n-    metadata_db = DiskMemory(metadata_path(project_path))\n-    if FILE_LIST_NAME in metadata_db:\n-        print(\n-            f\"File list detected at {metadata_db.path / FILE_LIST_NAME}. \"\n-            \"Edit or delete it if you want to select new files.\"\n-        )\n-    else:\n-        use_last_string = \"\"\n-        if FILE_LIST_NAME in metadata_db:\n-            use_last_string = (\n-                \"3. Use previous file list (available at \"\n-                + f\"{os.path.join(metadata_db.path, FILE_LIST_NAME)})\\n\"\n-            )\n-            selection_number = 3\n-        else:\n-            selection_number = 1\n-        selection_str = \"\\n\".join(\n-            [\n-                \"How do you want to select the files?\",\n-                \"\",\n-                \"1. Use File explorer.\",\n-                \"2. Use Command-Line.\",\n-                use_last_string if len(use_last_string) > 1 else \"\",\n-                f\"Select option and press Enter (default={selection_number}): \",\n-            ]\n-        )\n-\n-        file_path_list = []\n-        selected_number_str = input(selection_str)\n-        if selected_number_str:\n-            try:\n-                selection_number = int(selected_number_str)\n-            except ValueError:\n-                print(\"Invalid number. Select a number from the list above.\\n\")\n-                sys.exit(1)\n-\n-        if selection_number == 1:\n-            # Open GUI selection\n-            file_path_list = gui_file_selector(project_path)\n-        elif selection_number == 2:\n-            # Open terminal selection\n-            file_path_list = terminal_file_selector(project_path)\n-        if (\n-            selection_number <= 0\n-            or selection_number > 3\n-            or (selection_number == 3 and not use_last_string)\n-        ):\n-            print(\"Invalid number. Select a number from the list above.\\n\")\n-            sys.exit(1)\n-\n-        # ToDO: Replace this hack that makes all file_path relative to the right thing\n-        file_path_list = [\n-            os.path.relpath(file_path.strip(), project_path)\n-            for file_path in file_path_list\n-        ]\n-\n-        if not selection_number == 3:\n-            metadata_db[FILE_LIST_NAME] = \"\\n\".join(\n-                str(file_path) for file_path in file_path_list\n-            )\n-    content_dict = dict()\n-    with open(metadata_db.path / FILE_LIST_NAME, \"r\") as file_list:\n-        for file in file_list:\n-            with open(os.path.join(project_path, file.strip()), \"r\") as content:\n-                content_dict[file.strip()] = content.read()\n-    return FilesDict(content_dict)\n-\n-\n-def get_all_code(project_path: str) -> FilesDict:\n-    file_selection = terminal_file_selector(project_path, all=True)\n-    # ToDO: Replace this hack that makes all file_path relative to the right thing\n-    file_selection = [\n-        os.path.relpath(str(file_path).strip(), project_path)\n-        for file_path in file_selection\n-    ]\n-    content_dict = dict()\n-\n-    for file in file_selection:\n-        with open(os.path.join(project_path, file.strip()), \"r\") as content:\n-            content_dict[file.strip()] = content.read()\n-    return FilesDict(content_dict)\n-\n-\n-def gui_file_selector(input_path: str) -> List[str]:\n-    \"\"\"\n-    Display a tkinter file selection window to select context files.\n-    \"\"\"\n-    root = tk.Tk()\n-    root.withdraw()\n-    root.call(\"wm\", \"attributes\", \".\", \"-topmost\", True)\n-    file_list = list(\n-        fd.askopenfilenames(\n-            parent=root,\n-            initialdir=input_path,\n-            title=\"Select files to improve (or give context):\",\n-        )\n-    )\n-    # ensure right path type\n-    return [str(path) for path in file_list]\n-\n-\n-def terminal_file_selector(input_path: str, all: bool = False) -> List[str]:\n-    \"\"\"\n-    Display a terminal file selection to select context files.\n-    \"\"\"\n-    file_selector = TerminalFileSelector(Path(input_path))\n-    file_selector.display()\n-    file_list = file_selector.ask_for_selection(all=all)\n-    return [str(path) for path in file_list]\n+        return \"\".join(reversed(parts))  # Assemble the parts into the final string\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "Great effort on the improvements! However, I noted a few points for refinement:\n\n1. The `open_with_default_editor` should cycle through a list of potential editors for better usability. \n2. Modify the `.toml` default to `false` for small projects, improving user experience.\n3. Consider filtering files by decoding with \"utf-8\" instead of relying solely on `_is_in_ignoring_extensions_`.\n4. Persist the `.toml` and remove `file_list.txt` to streamline workflows. \n\nThese tweaks can greatly enhance functionality and user experience. Keep up the good work!"
      }
    ],
    "codediff_length": 24700
  },
  {
    "index": 68,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/gpt_engineer/core/chat_to_files.py b/gpt_engineer/core/chat_to_files.py\nindex 3f5fe7e6c6..af2f715c67 100644\n--- a/gpt_engineer/core/chat_to_files.py\n+++ b/gpt_engineer/core/chat_to_files.py\n@@ -1,196 +1,250 @@\n \"\"\"\n-This module provides utilities to handle and process chat content, especially for extracting code blocks\n-and managing them within a specified GPT Engineer project (\"workspace\"). It offers functionalities like parsing chat messages to\n-retrieve code blocks, storing these blocks into a workspace, and overwriting workspace content based on\n-new chat messages. Moreover, it aids in formatting and reading file content for an AI agent's input.\n+This module provides advanced utilities for processing and managing chat content,\n+specifically in GPT Engineer projects (\"workspaces\"). Its main focus is on the extraction\n+and manipulation of code blocks from chat messages. Key functionalities include parsing chat\n+content to identify and extract code blocks, then integrating them into workspaces.\n \n Key Features:\n-- Parse and extract code blocks from chat messages.\n-- Store and overwrite files within a workspace based on chat content.\n-- Format files to be used as inputs for AI agents.\n-- Retrieve files and their content based on a provided list.\n+- Efficient extraction of code blocks from chat messages for workspace integration.\n+- Dynamic updating of workspace files with new chat content.\n+- Automated formatting of files for AI agent processing.\n+- Retrieval of specific files and content for detailed analysis.\n \n Dependencies:\n-- `os` and `pathlib`: For handling OS-level operations and path manipulations.\n-- `re`: For regex-based parsing of chat content.\n-- `gpt_engineer.core.db`: Database handling functionalities for the workspace.\n-- `gpt_engineer.cli.file_selector`: Constants related to file selection.\n-\n-Functions:\n-- parse_chat: Extracts code blocks from chat messages.\n-- to_files_and_memory: Saves chat content to memory and adds extracted files to a workspace.\n-- to_files: Adds extracted files to a workspace.\n-- get_code_strings: Retrieves file names and their content.\n-- format_file_to_input: Formats file content for AI input.\n-- overwrite_files_with_edits: Overwrites workspace files based on parsed edits from chat.\n-- apply_edits: Applies file edits to a workspace.\n+- `os` and `pathlib`: Handle file system operations and path manipulations.\n+- `re`: Employed for regex-based parsing to extract code blocks and edits.\n+- `gpt_engineer.core.db`: Database functionalities for workspace management.\n+- `gpt_engineer.cli.file_selector`: Constants and utilities for file selection.\n+\n+Core Functions:\n+- chat_to_files_dict: Extracts code blocks and organizes them for access.\n+- overwrite_code_with_edits: Updates workspace files with chat-derived edits.\n+- parse_edits: Parses and structures code edits from chats.\n+- apply_edits: Applies edits to workspace files, maintaining their relevance.\n \"\"\"\n \n import logging\n import re\n \n-from dataclasses import dataclass\n-from typing import List\n+from collections import Counter\n \n from gpt_engineer.core.files_dict import FilesDict\n \n+# Configure logging for the module\n logger = logging.getLogger(__name__)\n \n \n def chat_to_files_dict(chat) -> FilesDict:\n     \"\"\"\n-    Extracts all code blocks from a chat and returns them\n-    as a list of (filename, codeblock) tuples.\n+    Extracts code blocks from a chat string and returns a FilesDict object containing\n+    (filename, codeblock) pairs.\n \n     Parameters\n     ----------\n     chat : str\n-        The chat to extract code blocks from.\n+        The chat string to extract code blocks from.\n \n     Returns\n     -------\n-    List[Tuple[str, str]]\n-        A list of tuples, where each tuple contains a filename and a code block.\n+    FilesDict\n+        A FilesDict object with filenames as keys and their respective code blocks as values.\n     \"\"\"\n-    # Get all ``` blocks and preceding filenames\n+    # Regular expression pattern to identify code blocks and preceding filenames\n     regex = r\"(\\S+)\\n\\s*```[^\\n]*\\n(.+?)```\"\n     matches = re.finditer(regex, chat, re.DOTALL)\n \n     files_dict = FilesDict()\n     for match in matches:\n-        # Strip the filename of any non-allowed characters and convert / to \\\n+        # Clean and standardize the file path\n         path = re.sub(r'[\\:<>\"|?*]', \"\", match.group(1))\n-\n-        # Remove leading and trailing brackets\n         path = re.sub(r\"^\\[(.*)\\]$\", r\"\\1\", path)\n-\n-        # Remove leading and trailing backticks\n         path = re.sub(r\"^`(.*)`$\", r\"\\1\", path)\n-\n-        # Remove trailing ]\n         path = re.sub(r\"[\\]\\:]$\", \"\", path)\n \n-        # Get the code\n+        # Extract and clean the code content\n         content = match.group(2)\n \n-        # Add the file to the list\n+        # Add the cleaned path and content to the FilesDict\n         files_dict[path.strip()] = content.strip()\n \n-    return FilesDict(files_dict)\n+    return files_dict\n \n \n def overwrite_code_with_edits(chat: str, files_dict: FilesDict):\n     \"\"\"\n-    Overwrite code with edits extracted from chat.\n-\n-    This function takes a chat string, parses it for edits, and applies those edits\n-    to the provided code object.\n+    Parses edits from a chat and applies them to the provided FilesDict object.\n \n     Parameters\n     ----------\n     chat : str\n         The chat content containing code edits.\n     files_dict : FilesDict\n-        The code object to apply edits to.\n+        The FilesDict object to apply edits to.\n     \"\"\"\n-    edits = parse_edits(chat)\n-    apply_edits(edits, files_dict)\n+    parse_edits(chat)\n+    # apply_edits(files_dict, edits)\n \n \n-@dataclass\n-class Edit:\n-    filename: str\n-    before: str\n-    after: str\n+def parse_edits(chat: str) -> list[dict]:\n+    edits = []\n+    diff_blocks = re.findall(r\"```diff\\n([\\s\\S]*?)\\n```\", chat)\n+    for diff_block in diff_blocks:\n+        lines = diff_block.split(\"\\n\")\n+        file_name = lines[0][4:]  # Using the filename directly\n+        start_line_num = int(lines[2][4:].split(\",\")[0])\n+        current_line_num = start_line_num - 1\n+\n+        edit = []\n+        for line in lines[3:]:\n+            current_line_num += 1  # Increment line number\n+            if line.startswith(\"-\"):\n+                line_data = {\n+                    \"filename\": file_name,\n+                    \"line_number\": str(current_line_num),\n+                    \"change_type\": \"-\",  # Denote removal\n+                }\n+                edit.append(line_data)\n+\n+            elif line.startswith(\"+\"):\n+                current_line_num -= 1  # follow last line\n+                line_data = {\n+                    \"filename\": file_name,\n+                    \"line_number\": str(\n+                        current_line_num\n+                    ),  # Keep the current line number for '+' lines\n+                    \"change_type\": \"+\",  # Denote addition\n+                }\n+                edit.append(line_data)\n+                # Do not increment the line number here for '+' lines\n+\n+        edits.append(edit)\n+    [print(f\"\\n{edit}\") for edit in edits]\n+    return edits\n \n \n-def parse_edits(chat: str):\n+# @dataclass\n+# class Edit:\n+#     filename: str\n+#     line_number: int\n+#     content: str\n+#     is_before: bool\n+#\n+#\n+# def parse_edits(chat: str):\n+#     \"\"\"\n+#     Parses edits from a chat string while preserving indentation, returning a list of Edit objects.\n+#\n+#     Parameters\n+#     ----------\n+#     chat : str\n+#         The chat string containing edits.\n+#\n+#     Returns\n+#     -------\n+#     List[Edit]\n+#         A list of Edit objects representing the parsed edits.\n+#     \"\"\"\n+#     edits = []\n+#     lines = chat.split(\"\\n\")\n+#     filename = \"\"\n+#     for line in lines:\n+#         if line.startswith(\"File:\"):\n+#             filename = line.split(\":\")[1].strip()\n+#         else:\n+#             # Extract line number, edit type, and content with leading spaces\n+#             match = re.match(r\"(\\d+) ([-+])\\s?(.*\\S.*)\", line)\n+#             if match:\n+#                 line_number, symbol, content = match.groups()\n+#                 line_number = int(line_number)\n+#                 is_before = symbol == \"-\"\n+#                 edits.append(Edit(filename, line_number, content, is_before))\n+#\n+#     # Sort edits for sequential application\n+#     edits.sort(key=lambda edit: (edit.filename, not edit.is_before, edit.line_number))\n+#     return edits\n+#\n+#\n+def is_similar(str1, str2):\n     \"\"\"\n-    Parse edits from a chat string.\n-\n-    This function extracts code edits from a chat string and returns them as a list\n-    of Edit objects.\n+    Compares two strings for similarity, ignoring spaces and case.\n \n     Parameters\n     ----------\n-    chat : str\n-        The chat content containing code edits.\n+    str1, str2 : str\n+        The strings to compare.\n \n     Returns\n     -------\n-    List[Edit]\n-        A list of Edit objects representing the parsed code edits.\n-    \"\"\"\n-\n-    def parse_one_edit(lines):\n-        HEAD = \"<<<<<<< HEAD\"\n-        DIVIDER = \"\\n=======\\n\"\n-        UPDATE = \">>>>>>> updated\"\n-\n-        filename = lines.pop(0)\n-        text = \"\\n\".join(lines)\n-        splits = text.split(DIVIDER)\n-        if len(splits) != 2:\n-            raise ValueError(f\"Could not parse following text as code edit: \\n{text}\")\n-        before, after = splits\n-\n-        before = before.replace(HEAD, \"\").strip()\n-        after = after.replace(UPDATE, \"\").strip()\n-\n-        return Edit(filename, before, after)\n-\n-    edits = []\n-    current_edit = []\n-    in_fence = False\n-\n-    for line in chat.split(\"\\n\"):\n-        if line.startswith(\"```\") and in_fence:\n-            edits.append(parse_one_edit(current_edit))\n-            current_edit = []\n-            in_fence = False\n-            continue\n-        elif line.startswith(\"```\") and not in_fence:\n-            in_fence = True\n-            continue\n-\n-        if in_fence:\n-            current_edit.append(line)\n-\n-    return edits\n-\n-\n-def apply_edits(edits: List[Edit], files_dict: FilesDict):\n-    \"\"\"\n-    Apply a list of edits to the given code.\n-\n-    This function takes a list of Edit objects and applies each edit to the code object.\n-    It handles the creation of new files and the modification of existing files based on the edits.\n-\n-    Parameters\n-    ----------\n-    edits : List[Edit]\n-        A list of Edit objects representing the code edits to apply.\n-    files_dict : FilesDict\n-        The code object to apply edits to.\n+    bool\n+        True if the strings are similar, False otherwise.\n     \"\"\"\n-    for edit in edits:\n-        filename = edit.filename\n-        if edit.before == \"\":\n-            if filename in files_dict:\n-                logger.warning(\n-                    f\"The edit to be applied wants to create a new file `{filename}`, but that already exists. The file will be overwritten. See `.gpteng/memory` for previous version.\"\n-                )\n-            files_dict[filename] = edit.after  # new file\n-        else:\n-            occurrences_cnt = files_dict[filename].count(edit.before)\n-            if occurrences_cnt == 0:\n-                logger.warning(\n-                    f\"While applying an edit to `{filename}`, the code block to be replaced was not found. No instances will be replaced.\"\n-                )\n-            if occurrences_cnt > 1:\n-                logger.warning(\n-                    f\"While applying an edit to `{filename}`, the code block to be replaced was found multiple times. All instances will be replaced.\"\n-                )\n-            files_dict[filename] = files_dict[filename].replace(\n-                edit.before, edit.after\n-            )  # existing file\n+    str1, str2 = str1.replace(\" \", \"\").lower(), str2.replace(\" \", \"\").lower()\n+\n+    counter1, counter2 = Counter(str1), Counter(str2)\n+    intersection = sum((counter1 & counter2).values())\n+    longer_length = max(len(str1), len(str2))\n+\n+    return intersection >= 0.9 * longer_length\n+\n+\n+#\n+#\n+# def apply_edits(edits: List[Edit], files_dict: FilesDict):\n+#     \"\"\"\n+#     Applies a list of Edit objects to the provided FilesDict object.\n+#\n+#     Parameters\n+#     ----------\n+#     edits : List[Edit]\n+#         The list of edits to apply.\n+#     files_dict : FilesDict\n+#         The FilesDict object to be modified.\n+#     \"\"\"\n+#     for edit in edits:\n+#         filename = edit.filename\n+#         if filename not in files_dict:\n+#             files_dict[filename] = \"\"\n+#             logger.warning(f\"Created new file: {filename}\")\n+#\n+#         lines = files_dict[filename].split(\"\\n\")\n+#         line_number = min(edit.line_number - 1, len(lines))\n+#\n+#         if line_number < len(lines):\n+#             if edit.is_before:  # Deletion\n+#                 if is_similar(lines[line_number], edit.content):\n+#                     lines[line_number] = \"# Line deleted line by GPT\"\n+#                     logger.warning(\n+#                         f\"Deleted from {filename}, line {edit.line_number}: '{edit.content}'\"\n+#                     )\n+#                 else:\n+#                     logger.warning(\n+#                         f\"line {edit.line_number}: '{edit.content}' not found in {filename} where should be '{lines[line_number]}'\"\n+#                     )\n+#             else:  # Addition\n+#                 if (\n+#                     lines[line_number] == \"# Line deleted line by GPT\"\n+#                     or lines[line_number] == \"\"\n+#                     or len(files_dict[filename]) == 0\n+#                 ):\n+#                     lines[line_number] = edit.content\n+#                     logger.warning(\n+#                         f\"Added to {filename}, line {edit.line_number}: '{edit.content.strip()}'\"\n+#                     )\n+#                 else:\n+#                     logger.warning(\n+#                         f\"The addition of {edit.content} is discarded for wrong line number\"\n+#                     )\n+#         else:\n+#             if not edit.is_before:\n+#                 lines.append(edit.content)\n+#                 logger.warning(\n+#                     f\"Added to {filename}, line {edit.line_number}: '{edit.content.strip()}'\"\n+#                 )\n+#\n+#         files_dict[filename] = \"\\n\".join(lines)\n+#\n+#     # Remove deletion tag\n+#     for filename in files_dict.keys():\n+#         lines = files_dict[filename].split(\"\\n\")\n+#         lines = [line for line in lines if line.strip() != \"# Line deleted line by GPT\"]\n+#         files_dict[filename] = \"\\n\".join(lines)\ndiff --git a/gpt_engineer/preprompts/improve b/gpt_engineer/preprompts/improve\nindex e588e55083..38a7835e7e 100644\n--- a/gpt_engineer/preprompts/improve\n+++ b/gpt_engineer/preprompts/improve\n@@ -1,80 +1,30 @@\n-You are an expert software developer.\n-Respect existing conventions, libraries, etc.\n-\n-You MUST\n-1. (planning) Think step-by-step and explain the needed changes. Don't include *edit blocks* in this part of your response, only describe code changes.\n-2. (output) Describe each change with an *edit block* per the example below.\n-\n-You MUST format EVERY code change with an *edit block* like this:\n-```python\n-example.py\n-<<<<<<< HEAD\n-    # some comment\n-    # Func to multiply\n-    def mul(a,b)\n-=======\n-    # updated comment\n-    # Function to add\n-    def add(a,b):\n->>>>>>> updated\n-```\n-Remember, you can use multiple *edit blocks* per file.\n-\n-Here is an example response:\n----\n-PLANNING:\n-We need to change \"SOMETHING\" because \"SOMETHING\", therefore I will add the line `a=a+1` to the function `add_one`.\n-Also, in the class `DB`, we need to update the \"SOMETHING\"\n-\n-OUTPUT:\n-```python\n-example_1.py\n-<<<<<<< HEAD\n-    def mul(a,b)\n-=======\n-    def add(a,b):\n->>>>>>> updated\n-```\n-\n-```python\n-example_2.py\n-<<<<<<< HEAD\n-    def add_one(a,b):\n-        a = a+2\n-=======\n-    def add_one(a,b):\n-        a = a+1\n->>>>>>> updated\n+Act as an expert software developer.\n+Always use best practices when coding.\n+When you edit or add code, respect and use existing conventions, libraries, etc.\n+\n+Take requests for changes to the supplied code, and then you MUST\n+1. (planning) Think step-by-step and explain the needed changes. Don't include *block*s in this part of your response, only describe code changes.\n+2. (generate patches) Use \"git diff\" syntax to write changes to the code; each \"git diff\" must only contain one hunk\n+3. (check) Make sure the line numbers and code match the original file; add file headers\n+\n+Patches example:\n+```diff\n+--- a.py\n++++ a.py\n+\ndiff --git a/tests/core/test_chat_to_files.py b/tests/core/test_chat_to_files.py\nindex ee1ed0e86e..63f5fdace8 100644\n--- a/tests/core/test_chat_to_files.py\n+++ b/tests/core/test_chat_to_files.py\n@@ -1,219 +1,270 @@\n-import logging\n-\n import pytest\n \n-from gpt_engineer.core.chat_to_files import (\n-    Edit,\n-    apply_edits,\n-    chat_to_files_dict,\n-    logger as parse_logger,\n-    parse_edits,\n-)\n-\n-\n-def test_standard_input():\n-    chat = \"\"\"\n-    Some text describing the code\n-file1.py\n-```python\n-print(\"Hello, World!\")\n-```\n-\n-file2.py\n-```python\n-def add(a, b):\n-    return a + b\n-```\n-    \"\"\"\n-    expected = {\n-        \"file1.py\": 'print(\"Hello, World!\")',\n-        \"file2.py\": \"def add(a, b):\\n    return a + b\",\n-    }\n-    assert chat_to_files_dict(chat) == expected\n-\n-\n-def test_no_code_blocks():\n-    chat = \"Just some regular chat without code.\"\n-    expected = {}\n-    assert chat_to_files_dict(chat) == expected\n-\n-\n-def test_special_characters_in_filename():\n-    chat = \"\"\"\n-    file[1].py\n-    ```python\n-    print(\"File 1\")\n-    ```\n-\n-    file`2`.py\n-    ```python\n-    print(\"File 2\")\n-    ```\n-    \"\"\"\n-    expected = {\"file[1].py\": 'print(\"File 1\")', \"file`2`.py\": 'print(\"File 2\")'}\n-    parsed = chat_to_files_dict(chat)\n-    assert parsed == expected\n-\n-\n-def test_empty_code_blocks():\n-    chat = \"\"\"\n-    empty.py\n-    ```\n-    ```\n-    \"\"\"\n-    expected = {\"empty.py\": \"\"}\n-    assert chat_to_files_dict(chat) == expected\n-\n-\n-def test_mixed_content():\n-    chat = \"\"\"\n-    script.sh\n-    ```bash\n-    echo \"Hello\"\n-    ```\n-\n-    script.py\n-    ```python\n-    print(\"World\")\n-    ```\n-    \"\"\"\n-    expected = {\"script.sh\": 'echo \"Hello\"', \"script.py\": 'print(\"World\")'}\n-    assert chat_to_files_dict(chat) == expected\n-\n-\n-def test_filename_line_break():\n-    chat = \"\"\"\n-    file1.py\n-\n-    ```python\n-    print(\"Hello, World!\")\n-    ```\n-    \"\"\"\n-    expected = {\"file1.py\": 'print(\"Hello, World!\")'}\n-    assert chat_to_files_dict(chat) == expected\n-\n-\n-def test_filename_in_backticks():\n-    chat = \"\"\"\n-    `file1.py`\n-    ```python\n-    print(\"Hello, World!\")\n-    ```\n-    \"\"\"\n-    expected = {\"file1.py\": 'print(\"Hello, World!\")'}\n-    assert chat_to_files_dict(chat) == expected\n-\n-\n-def test_filename_with_file_tag():\n-    chat = \"\"\"\n-    [FILE: file1.py]\n-    ```python\n-    print(\"Hello, World!\")\n-    ```\n-    \"\"\"\n-    expected = {\"file1.py\": 'print(\"Hello, World!\")'}\n-    assert chat_to_files_dict(chat) == expected\n-\n-\n-def test_filename_with_different_extension():\n-    chat = \"\"\"\n-    [id].jsx\n-    ```javascript\n-    console.log(\"Hello, World!\")\n-    ```\n-    \"\"\"\n-    expected = {\"[id].jsx\": 'console.log(\"Hello, World!\")'}\n-    assert chat_to_files_dict(chat) == expected\n-\n-\n-# Helper function to capture log messages\n-@pytest.fixture\n-def log_capture():\n-    class LogCaptureHandler(logging.Handler):\n-        def __init__(self):\n-            super().__init__()\n-            self.messages = []\n-\n-        def emit(self, record):\n-            self.messages.append(record.getMessage())\n-\n-    handler = LogCaptureHandler()\n-    parse_logger.addHandler(handler)\n-    yield handler\n-    parse_logger.removeHandler(handler)\n-\n-\n-def test_parse_with_additional_text():\n-    chat = \"\"\"\n-Some introductory text.\n-\n-```python\n-some/dir/example_1.py\n-<<<<<<< HEAD\n-    def mul(a,b)\n-=======\n-    def add(a,b):\n->>>>>>> updated\n-```\n-\n-Text between patches.\n-\n-```python\n-some/dir/example_2.py\n-<<<<<<< HEAD\n-    class DBS:\n-        db = 'aaa'\n-=======\n-    class DBS:\n-        db = 'bbb'\n->>>>>>> updated\n-```\n-\n-Ending text.\n-    \"\"\"\n-    expected = [\n-        Edit(\"some/dir/example_1.py\", \"def mul(a,b)\", \"def add(a,b):\"),\n-        Edit(\n-            \"some/dir/example_2.py\",\n-            \"class DBS:\\n        db = 'aaa'\",\n-            \"class DBS:\\n        db = 'bbb'\",\n-        ),\n-    ]\n-    parsed = parse_edits(chat)\n-    assert parsed == expected\n-\n-\n-def test_apply_overwrite_existing_file(log_capture):\n-    edits = [Edit(\"existing_file.py\", \"\", \"print('Hello, World!')\")]\n-    code = {\"existing_file.py\": \"some content\"}\n-    apply_edits(edits, code)\n-    assert code == {\"existing_file.py\": \"print('Hello, World!')\"}\n-    assert \"file will be overwritten\" in log_capture.messages[0]\n-\n-\n-def test_apply_edit_new_file(log_capture):\n-    edits = [Edit(\"new_file.py\", \"\", \"print('Hello, World!')\")]\n-    code = {}\n-    apply_edits(edits, code)\n-    assert code == {\"new_file.py\": \"print('Hello, World!')\"}\n-\n-\n-def test_apply_edit_no_match(log_capture):\n-    edits = [Edit(\"file.py\", \"non-existent content\", \"new content\")]\n-    code = {\"file.py\": \"some content\"}\n-    apply_edits(edits, code)\n-    assert code == {\"file.py\": \"some content\"}  # No change\n-    assert \"code block to be replaced was not found\" in log_capture.messages[0]\n-\n-\n-def test_apply_edit_multiple_matches(log_capture):\n-    edits = [Edit(\"file.py\", \"repeat\", \"new\")]\n-    code = {\"file.py\": \"repeat repeat repeat\"}\n-    apply_edits(edits, code)\n-    assert code == {\"file.py\": \"new new new\"}\n-    assert (\n-        \"code block to be replaced was found multiple times\" in log_capture.messages[0]\n-    )\n+# def test_standard_input():\n+#     chat = \"\"\"\n+#     Some text describing the code\n+# file1.py\n+# ```python\n+# 1 + print(\"Hello, World!\")\n+# ```\n+#\n+# file2.py\n+# ```python\n+# 1 + def add(a, b):\n+# 2 +    return a + b\n+# ```\n+#     \"\"\"\n+#     expected = {\n+#         \"file1.py\": '1 + print(\"Hello, World!\")',\n+#         \"file2.py\": \"1 + def add(a, b):\\n2 +    return a + b\",\n+#     }\n+#     assert chat_to_files_dict(chat) == expected\n+#\n+#\n+# def test_no_code_blocks():\n+#     chat = \"Just some regular chat without code.\"\n+#     expected = {}\n+#     assert chat_to_files_dict(chat) == expected\n+#\n+#\n+# def test_special_characters_in_filename():\n+#     chat = \"\"\"\n+#     file[1].py\n+#     ```python\n+#     1 + print(\"File 1\")\n+#     ```\n+#\n+#     file`2`.py\n+#     ```python\n+#     1 + print(\"File 2\")\n+#     ```\n+#     \"\"\"\n+#     expected = {\n+#         \"file[1].py\": '1 + print(\"File 1\")',\n+#         \"file`2`.py\": '1 + print(\"File 2\")',\n+#     }\n+#     parsed = chat_to_files_dict(chat)\n+#     assert parsed == expected\n+#\n+#\n+# def test_empty_code_blocks():\n+#     chat = \"\"\"\n+#     empty.py\n+#     ```\n+#     ```\n+#     \"\"\"\n+#     expected = {\"empty.py\": \"\"}\n+#     assert chat_to_files_dict(chat) == expected\n+#\n+#\n+# def test_mixed_content():\n+#     chat = \"\"\"\n+#     script.sh\n+#     ```bash\n+#     1 + echo \"Hello\"\n+#     ```\n+#\n+#     script.py\n+#     ```python\n+#     1 + print(\"World\")\n+#     ```\n+#     \"\"\"\n+#     expected = {\"script.sh\": '1 + echo \"Hello\"', \"script.py\": '1 + print(\"World\")'}\n+#     assert chat_to_files_dict(chat) == expected\n+#\n+#\n+# def test_filename_line_break():\n+#     chat = \"\"\"\n+#     file1.py\n+#\n+#     ```python\n+#     1 + print(\"Hello, World!\")\n+#     ```\n+#     \"\"\"\n+#     expected = {\"file1.py\": '1 + print(\"Hello, World!\")'}\n+#     assert chat_to_files_dict(chat) == expected\n+#\n+#\n+# def test_filename_in_backticks():\n+#     chat = \"\"\"\n+#     `file1.py`\n+#     ```python\n+#     1 + print(\"Hello, World!\")\n+#     ```\n+#     \"\"\"\n+#     expected = {\"file1.py\": '1 + print(\"Hello, World!\")'}\n+#     assert chat_to_files_dict(chat) == expected\n+#\n+#\n+# def test_filename_with_file_tag():\n+#     chat = \"\"\"\n+#     [FILE: file1.py]\n+#     ```python\n+#     1 + print(\"Hello, World!\")\n+#     ```\n+#     \"\"\"\n+#     expected = {\"file1.py\": '1 + print(\"Hello, World!\")'}\n+#     assert chat_to_files_dict(chat) == expected\n+#\n+#\n+# def test_filename_with_different_extension():\n+#     chat = \"\"\"\n+#     [id].jsx\n+#     ```javascript\n+#     1 + console.log(\"Hello, World!\")\n+#     ```\n+#     \"\"\"\n+#     expected = {\"[id].jsx\": '1 + console.log(\"Hello, World!\")'}\n+#     assert chat_to_files_dict(chat) == expected\n+#\n+#\n+# # Helper function to capture log messages\n+# @pytest.fixture\n+# def log_capture():\n+#     class LogCaptureHandler(logging.Handler):\n+#         def __init__(self):\n+#             super().__init__()\n+#             self.messages = []\n+#\n+#         def emit(self, record):\n+#             self.messages.append(record.getMessage())\n+#\n+#     handler = LogCaptureHandler()\n+#     parse_logger.addHandler(handler)\n+#     yield handler\n+#     parse_logger.removeHandler(handler)\n+#\n+#\n+# def test_parse_with_additional_text():\n+#     chat = \"\"\"\n+# Some introductory text.\n+#\n+# ```python\n+# File: some/dir/example_1.py\n+# 1 -     def mul(a,b)\n+# 1 +     def add(a,b):\n+# ```\n+#\n+# Text between patches.\n+#\n+# ```python\n+# File: some/dir/example_2.py\n+# 1 -     class DBS:\n+# 2 -         db = 'aaa'\n+# 1 +     class DBS:\n+# 2 +        db = 'bbb'\n+# ```\n+#\n+# Ending text.\n+#     \"\"\"\n+#     expected = [\n+#         Edit(\n+#             filename=\"some/dir/example_1.py\",\n+#             line_number=1,\n+#             content=\"    def mul(a,b)\",\n+#             is_before=True,\n+#         ),\n+#         Edit(\n+#             filename=\"some/dir/example_1.py\",\n+#             line_number=1,\n+#             content=\"    def add(a,b):\",\n+#             is_before=False,\n+#         ),\n+#         Edit(\n+#             filename=\"some/dir/example_2.py\",\n+#             line_number=1,\n+#             content=\"    class DBS:\",\n+#             is_before=True,\n+#         ),\n+#         Edit(\n+#             filename=\"some/dir/example_2.py\",\n+#             line_number=1,\n+#             content=\"    class DBS:\",\n+#             is_before=False,\n+#         ),\n+#         Edit(\n+#             filename=\"some/dir/example_2.py\",\n+#             line_number=2,\n+#             content=\"        db = 'aaa'\",\n+#             is_before=True,\n+#         ),\n+#         Edit(\n+#             filename=\"some/dir/example_2.py\",\n+#             line_number=2,\n+#             content=\"       db = 'bbb'\",\n+#             is_before=False,\n+#         ),\n+#     ]\n+#     parsed = parse_edits(chat)\n+#     assert parsed == expected\n+#\n+#\n+# def test_apply_overwrite_existing_file(log_capture):\n+#     edits = [\n+#         Edit(filename=\"existing_file.py\", line_number=1, content=\"\", is_before=True),\n+#         Edit(\n+#             filename=\"existing_file.py\",\n+#             line_number=1,\n+#             content=\"print('Hello, World!')\",\n+#             is_before=False,\n+#         ),\n+#     ]\n+#     code = {\"existing_file.py\": \"some content\"}\n+#     apply_edits(edits, code)\n+#     assert code == {\"existing_file.py\": \"some content\"}\n+#     assert \"not found in existing_file.py where should be\" in log_capture.messages[0]\n+#     assert \"is discarded for wrong line number\" in log_capture.messages[1]\n+#\n+#\n+# def test_apply_edit_new_file(log_capture):\n+#     edits = [\n+#         Edit(filename=\"new_file.py\", line_number=1, content=\"\", is_before=True),\n+#         Edit(\n+#             filename=\"new_file.py\",\n+#             line_number=1,\n+#             content=\"print('Hello, World!')\",\n+#             is_before=False,\n+#         ),\n+#     ]\n+#     code = {}\n+#     apply_edits(edits, code)\n+#     assert code == {\"new_file.py\": \"print('Hello, World!')\"}\n+#\n+#\n+# def test_apply_edit_no_match(log_capture):\n+#     edits = [\n+#         Edit(\n+#             filename=\"file.py\",\n+#             line_number=1,\n+#             content=\"non-existent content\",\n+#             is_before=True,\n+#         ),\n+#         Edit(filename=\"file.py\", line_number=1, content=\"new content\", is_before=False),\n+#     ]\n+#     code = {\"file.py\": \"some content\"}\n+#     apply_edits(edits, code)\n+#     assert code == {\"file.py\": \"some content\"}  # No change\n+#     assert \"not found\" in log_capture.messages[0]\n+#\n+#\n+# def test_basic_similarity():\n+#     assert is_similar(\"abc\", \"cab\")\n+#     assert not is_similar(\"abc\", \"def\")\n+#\n+#\n+# def test_case_insensitivity_and_whitespace():\n+#     assert is_similar(\"A b C\", \"c a b\")\n+#     assert not is_similar(\"Abc\", \"D e F\")\n+#\n+#\n+# def test_length_and_character_frequency():\n+#     assert is_similar(\"aabbc\", \"bacba\")\n+#     assert not is_similar(\"aabbcc\", \"abbcc\")\n+#\n+#\n+# def test_edge_cases():\n+#     assert not is_similar(\"\", \"a\")\n+#     assert is_similar(\"a\", \"a\")\n \n \n if __name__ == \"__main__\":\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The updates to `file_dict` and `improve` function are showing promise, especially with error corrections and the new `main.py` implementation. However, issues remain with parsing chat content back into the file structure. The editing process is unreliable, possibly due to the recent commit's changes. I suggest refining the logic for validating editsfocus on line numbers rather than exact content matches to reduce inconsistencies. Also, consider using Black for formatting across the board to tackle indentation issues. Lets set up a separate PR for that integration."
      }
    ],
    "codediff_length": 28580
  },
  {
    "index": 69,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/lib/ModelAE.py b/lib/ModelAE.py\ndeleted file mode 100644\nindex 78658ca81b..0000000000\n--- a/lib/ModelAE.py\n+++ /dev/null\n@@ -1,91 +0,0 @@\n-# AutoEncoder base classes\r\n-\r\n-import time\r\n-import numpy\r\n-from lib.training_data import TrainingDataGenerator, stack_images\r\n-\r\n-encoderH5 = 'encoder.h5'\r\n-decoder_AH5 = 'decoder_A.h5'\r\n-decoder_BH5 = 'decoder_B.h5'\r\n-\r\n-class ModelAE:\r\n-    def __init__(self, model_dir):\r\n-        self.model_dir = model_dir\r\n-\r\n-        self.encoder = self.Encoder()\r\n-        self.decoder_A = self.Decoder()\r\n-        self.decoder_B = self.Decoder()\r\n-\r\n-        self.initModel()\r\n-\r\n-    def load(self, swapped):\r\n-        (face_A,face_B) = (decoder_AH5, decoder_BH5) if not swapped else (decoder_BH5, decoder_AH5)\r\n-\r\n-        try:\r\n-            self.encoder.load_weights(str(self.model_dir / encoderH5))\r\n-            self.decoder_A.load_weights(str(self.model_dir / face_A))\r\n-            self.decoder_B.load_weights(str(self.model_dir / face_B))\r\n-            print('loaded model weights')\r\n-            return True\r\n-        except Exception as e:\r\n-            print('Failed loading existing training data.')\r\n-            print(e)\r\n-            return False\r\n-\r\n-    def save_weights(self):\r\n-        self.encoder.save_weights(str(self.model_dir / encoderH5))\r\n-        self.decoder_A.save_weights(str(self.model_dir / decoder_AH5))\r\n-        self.decoder_B.save_weights(str(self.model_dir / decoder_BH5))\r\n-        print('saved model weights')\r\n-\r\n-class TrainerAE():\r\n-    random_transform_args = {\r\n-        'rotation_range': 10,\r\n-        'zoom_range': 0.05,\r\n-        'shift_range': 0.05,\r\n-        'random_flip': 0.4,\r\n-    }\r\n-\r\n-    def __init__(self, model, fn_A, fn_B, batch_size=64):\r\n-        self.batch_size = batch_size\r\n-        self.model = model\r\n-\r\n-        generator = TrainingDataGenerator(self.random_transform_args, 160)\r\n-        self.images_A = generator.minibatchAB(fn_A, self.batch_size)\r\n-        self.images_B = generator.minibatchAB(fn_B, self.batch_size)\r\n-\r\n-    def train_one_step(self, iter, viewer):\r\n-        epoch, warped_A, target_A = next(self.images_A)\r\n-        epoch, warped_B, target_B = next(self.images_B)\r\n-\r\n-        loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)\r\n-        loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)\r\n-        print(\"[{0}] [#{1:05d}] loss_A: {2:.5f}, loss_B: {3:.5f}\".format(time.strftime(\"%H:%M:%S\"), iter, loss_A, loss_B),\r\n-            end='\\r')\r\n-\r\n-        if viewer is not None:\r\n-            viewer(self.show_sample(target_A[0:14], target_B[0:14]), \"training\")\r\n-\r\n-    def show_sample(self, test_A, test_B):\r\n-        figure_A = numpy.stack([\r\n-            test_A,\r\n-            self.model.autoencoder_A.predict(test_A),\r\n-            self.model.autoencoder_B.predict(test_A),\r\n-        ], axis=1)\r\n-        figure_B = numpy.stack([\r\n-            test_B,\r\n-            self.model.autoencoder_B.predict(test_B),\r\n-            self.model.autoencoder_A.predict(test_B),\r\n-        ], axis=1)\r\n-\r\n-        if test_A.shape[0] % 2 == 1:\r\n-            figure_A = numpy.concatenate ([figure_A, numpy.expand_dims(figure_A[0],0) ])\r\n-            figure_B = numpy.concatenate ([figure_B, numpy.expand_dims(figure_B[0],0) ])\r\n-        \r\n-        figure = numpy.concatenate([figure_A, figure_B], axis=0)\r\n-        w = 4\r\n-        h = int( figure.shape[0] / w)        \r\n-        figure = figure.reshape((w, h) + figure.shape[1:])        \r\n-        figure = stack_images(figure)\r\n-        \r\n-        return numpy.clip(figure * 255, 0, 255).astype('uint8')\r\ndiff --git a/plugins/Convert_Masked.py b/plugins/Convert_Masked.py\nindex d2f6fc2c22..22918069b2 100644\n--- a/plugins/Convert_Masked.py\n+++ b/plugins/Convert_Masked.py\n@@ -51,21 +53,77 @@ def apply_new_face(self, image, new_face, image_mask, mat, image_size, size):\n               masky = int(minx+(lenx//2))\r\n               maskx = int(miny+(leny//2))\r\n               outimage = cv2.seamlessClone(new_image.astype(numpy.uint8),base_image.astype(numpy.uint8),unitMask,(masky,maskx) , cv2.NORMAL_CLONE )\r\n-              \r\n+\r\n               return outimage\r\n-              \r\n+\r\n         foreground = cv2.multiply(image_mask, new_image.astype(float))\r\n         background = cv2.multiply(1.0 - image_mask, base_image.astype(float))\r\n         outimage = cv2.add(foreground, background)\r\n \r\n         return outimage\r\n \r\n+    def hist_match(self, source, template, mask=None):\r\n+        # Code borrowed from:\r\n+        # https://stackoverflow.com/questions/32655686/histogram-matching-of-two-images-in-python-2-x\r\n+        masked_source = source\r\n+        masked_template = template\r\n+\r\n+        if mask is not None:\r\n+            masked_source = source * mask\r\n+            masked_template = template * mask\r\n+\r\n+        oldshape = source.shape\r\n+        source = source.ravel()\r\n+        template = template.ravel()\r\n+        masked_source = masked_source.ravel()\r\n+        masked_template = masked_template.ravel()\r\n+        s_values, bin_idx, s_counts = numpy.unique(source, return_inverse=True,\r\n+                                                return_counts=True)\r\n+        t_values, t_counts = numpy.unique(template, return_counts=True)\r\n+        ms_values, mbin_idx, ms_counts = numpy.unique(source, return_inverse=True,\r\n+                                                return_counts=True)\r\n+        mt_values, mt_counts = numpy.unique(template, return_counts=True)\r\n+\r\n+        s_quantiles = numpy.cumsum(s_counts).astype(numpy.float64)\r\n+        s_quantiles /= s_quantiles[-1]\r\n+        t_quantiles = numpy.cumsum(t_counts).astype(numpy.float64)\r\n+        t_quantiles /= t_quantiles[-1]\r\n+        interp_t_values = numpy.interp(s_quantiles, t_quantiles, t_values)\r\n+\r\n+        return interp_t_values[bin_idx].reshape(oldshape)\r\n+\r\n+    def color_hist_match(self, src_im, tar_im, mask):\r\n+        matched_R = self.hist_match(src_im[:,:,0], tar_im[:,:,0], mask)\r\n+        matched_G = self.hist_match(src_im[:,:,1], tar_im[:,:,1], mask)\r\n+        matched_B = self.hist_match(src_im[:,:,2], tar_im[:,:,2], mask)\r\n+        matched = numpy.stack((matched_R, matched_G, matched_B), axis=2).astype(src_im.dtype)\r\n+        return matched\r\n+\r\n     def get_new_face(self, image, mat, size):\r\n         face = cv2.warpAffine( image, mat, (size,size) )\r\n         face = numpy.expand_dims( face, 0 )\r\n-        new_face = self.encoder( face / 255.0 )[0]\r\n-\r\n-        return numpy.clip( new_face * 255, 0, 255 ).astype( image.dtype )\r\n+        face_clipped = numpy.clip(face[0], 0, 255).astype( image.dtype )\r\n+        new_face = None\r\n+        mask = None\r\n+\r\n+        if \"GAN\" not in self.trainer:\r\n+            normalized_face = face / 255.0\r\n+            new_face = self.encoder(normalized_face)[0]\r\n+            new_face = numpy.clip( new_face * 255, 0, 255 ).astype( image.dtype )\r\n+        else:\r\n+            normalized_face = face / 255.0 * 2 - 1\r\n+            fake_output = self.encoder(normalized_face)\r\n+            if \"128\" in self.trainer: # TODO: Another hack to switch between 64 and 128\r\n+                fake_output = fake_output[0]\r\n+            mask = fake_output[:,:,:, :1]\r\n+            new_face = fake_output[:,:,:, 1:]\r\n+            new_face = mask * new_face + (1 - mask) * normalized_face\r\n+            new_face = numpy.clip((new_face[0] + 1) * 255 / 2, 0, 255).astype( image.dtype )\r\n+\r\n+        if self.match_histogram:\r\n+            new_face = self.color_hist_match(new_face, face_clipped, mask)\r\n+\r\n+        return new_face\r\n \r\n     def get_image_mask(self, image, new_face, face_detected, mat, image_size):\r\n \r\ndiff --git a/plugins/Model_GAN/Trainer.py b/plugins/Model_GAN/Trainer.py\nindex 01b16dd56a..0c270430ec 100644\n--- a/plugins/Model_GAN/Trainer.py\n+++ b/plugins/Model_GAN/Trainer.py\n@@ -14,140 +20,241 @@ def color_adjust(self, img):\n class Trainer():\r\n     random_transform_args = {\r\n         'rotation_range': 20,\r\n-        'zoom_range': 0.05,\r\n+        'zoom_range': 0.1,\r\n         'shift_range': 0.05,\r\n         'random_flip': 0.5,\r\n         }\r\n \r\n-    def __init__(self, model, fn_A, fn_B, batch_size):\r\n+    def __init__(self, model, fn_A, fn_B, batch_size, perceptual_loss):\r\n+        K.set_learning_phase(1)\r\n+\r\n         assert batch_size % 2 == 0, \"batch_size must be an even number\"\r\n         self.batch_size = batch_size\r\n         self.model = model\r\n-        \r\n+\r\n+        self.use_lsgan = True\r\n         self.use_mixup = True\r\n         self.mixup_alpha = 0.2\r\n+        self.use_perceptual_loss = perceptual_loss\r\n+        self.use_instancenorm = False\r\n \r\n-        generator = GANTrainingDataGenerator(self.random_transform_args, 220)\r\n+        self.lrD = 1e-4 # Discriminator learning rate\r\n+        self.lrG = 1e-4 # Generator learning rate\r\n+\r\n+        generator = GANTrainingDataGenerator(self.random_transform_args, 220, 6, 1)\r\n         self.train_batchA = generator.minibatchAB(fn_A, batch_size)\r\n         self.train_batchB = generator.minibatchAB(fn_B, batch_size)\r\n-    \r\n+\r\n+        self.avg_counter = self.errDA_sum = self.errDB_sum = self.errGA_sum = self.errGB_sum = 0\r\n+\r\n+        self.setup()\r\n+\r\n+    def setup(self):\r\n+        distorted_A, fake_A, mask_A, self.path_A, self.path_mask_A, self.path_abgr_A, self.path_bgr_A = self.cycle_variables(self.model.netGA)\r\n+        distorted_B, fake_B, mask_B, self.path_B, self.path_mask_B, self.path_abgr_B, self.path_bgr_B = self.cycle_variables(self.model.netGB)\r\n+        real_A = Input(shape=self.model.img_shape)\r\n+        real_B = Input(shape=self.model.img_shape)\r\n+\r\n+        if self.use_lsgan:\r\n+            self.loss_fn = lambda output, target : K.mean(K.abs(K.square(output-target)))\r\n+        else:\r\n+            self.loss_fn = lambda output, target : -K.mean(K.log(output+1e-12)*target+K.log(1-output+1e-12)*(1-target))\r\n+\r\n+        # ========== Define Perceptual Loss Model==========\r\n+        if self.use_perceptual_loss:\r\n+            from keras.models import Model\r\n+            from keras_vggface.vggface import VGGFace\r\n+            vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\r\n+            vggface.trainable = False\r\n+            out_size55 = vggface.layers[36].output\r\n+            out_size28 = vggface.layers[78].output\r\n+            out_size7 = vggface.layers[-2].output\r\n+            vggface_feat = Model(vggface.input, [out_size55, out_size28, out_size7])\r\n+            vggface_feat.trainable = False\r\n+        else:\r\n+            vggface_feat = None\r\n+\r\n+        #TODO check \"Tips for mask refinement (optional after >15k iters)\" => https://render.githubusercontent.com/view/ipynb?commit=87d6e7a28ce754acd38d885367b6ceb0be92ec54&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7368616f616e6c752f66616365737761702d47414e2f383764366537613238636537353461636433386438383533363762366365623062653932656335342f46616365537761705f47414e5f76325f737a3132385f747261696e2e6970796e62&nwo=shaoanlu%2Ffaceswap-GAN&path=FaceSwap_GAN_v2_sz128_train.ipynb&repository_id=115182783&repository_type=Repository#Tips-for-mask-refinement-(optional-after-%3E15k-iters)\r\n+        loss_DA, loss_GA = self.define_loss(self.model.netDA, real_A, fake_A, distorted_A, vggface_feat)\r\n+        loss_DB, loss_GB = self.define_loss(self.model.netDB, real_B, fake_B, distorted_B, vggface_feat)\r\n+\r\n+        loss_GA += 1e-3 * K.mean(K.abs(mask_A))\r\n+        loss_GB += 1e-3 * K.mean(K.abs(mask_B))\r\n+\r\n+        w_fo = 0.01\r\n+        loss_GA += w_fo * K.mean(self.first_order(mask_A, axis=1))\r\n+        loss_GA += w_fo * K.mean(self.first_order(mask_A, axis=2))\r\n+        loss_GB += w_fo * K.mean(self.first_order(mask_B, axis=1))\r\n+        loss_GB += w_fo * K.mean(self.first_order(mask_B, axis=2))\r\n+\r\n+        weightsDA = self.model.netDA.trainable_weights\r\n+        weightsGA = self.model.netGA.trainable_weights\r\n+        weightsDB = self.model.netDB.trainable_weights\r\n+        weightsGB = self.model.netGB.trainable_weights\r\n+\r\n+        # Adam(..).get_updates(...)\r\n+        training_updates = Adam(lr=self.lrD, beta_1=0.5).get_updates(weightsDA,[],loss_DA)\r\n+        self.netDA_train = K.function([distorted_A, real_A],[loss_DA], training_updates)\r\n+        training_updates = Adam(lr=self.lrG, beta_1=0.5).get_updates(weightsGA,[], loss_GA)\r\n+        self.netGA_train = K.function([distorted_A, real_A], [loss_GA], training_updates)\r\n+\r\n+        training_updates = Adam(lr=self.lrD, beta_1=0.5).get_updates(weightsDB,[],loss_DB)\r\n+        self.netDB_train = K.function([distorted_B, real_B],[loss_DB], training_updates)\r\n+        training_updates = Adam(lr=self.lrG, beta_1=0.5).get_updates(weightsGB,[], loss_GB)\r\n+        self.netGB_train = K.function([distorted_B, real_B], [loss_GB], training_updates)\r\n+\r\n+    def first_order(self, x, axis=1):\r\n+        img_nrows = x.shape[1]\r\n+        img_ncols = x.shape[2]\r\n+        if axis == 1:\r\n+            return K.abs(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\r\n+        elif axis == 2:\r\n+            return K.abs(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\r\n+        else:\r\n+            return None\r\n+\r\n     def train_one_step(self, iter, viewer):\r\n         # ---------------------\r\n         #  Train Discriminators\r\n         # ---------------------\r\n \r\n         # Select a random half batch of images\r\n-        epoch, warped_A, target_A = next(self.train_batchA) \r\n-        epoch, warped_B, target_B = next(self.train_batchB) \r\n-\r\n-        # Generate a half batch of new images\r\n-        gen_alphasA, gen_imgsA = self.model.netGA.predict(warped_A)\r\n-        gen_alphasB, gen_imgsB = self.model.netGB.predict(warped_B)\r\n-        #gen_masked_imgsA = gen_alphasA * gen_imgsA + (1 - gen_alphasA) * warped_A\r\n-        #gen_masked_imgsB = gen_alphasB * gen_imgsB + (1 - gen_alphasB) * warped_B\r\n-        gen_masked_imgsA = np.array([gen_alphasA[i] * gen_imgsA[i] + (1 - gen_alphasA[i]) * warped_A[i] \r\n-                                     for i in range(self.batch_size)])\r\n-        gen_masked_imgsB = np.array([gen_alphasB[i] * gen_imgsB[i] + (1 - gen_alphasB[i]) * warped_B[i]\r\n-                                     for i in range (self.batch_size)])\r\n-\r\n-        valid = np.ones((self.batch_size, ) + self.model.netDA.output_shape[1:])\r\n-        fake = np.zeros((self.batch_size, ) + self.model.netDA.output_shape[1:])\r\n-\r\n-        concat_real_inputA = np.array([np.concatenate([target_A[i], warped_A[i]], axis=-1) \r\n-                                       for i in range(self.batch_size)])\r\n-        concat_real_inputB = np.array([np.concatenate([target_B[i], warped_B[i]], axis=-1) \r\n-                                       for i in range(self.batch_size)])\r\n-        concat_fake_inputA = np.array([np.concatenate([gen_masked_imgsA[i], warped_A[i]], axis=-1) \r\n-                                       for i in range(self.batch_size)])\r\n-        concat_fake_inputB = np.array([np.concatenate([gen_masked_imgsB[i], warped_B[i]], axis=-1) \r\n-                                       for i in range(self.batch_size)])\r\n-        if self.use_mixup:\r\n-            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\r\n-            mixup_A = lam * concat_real_inputA + (1 - lam) * concat_fake_inputA\r\n-            mixup_B = lam * concat_real_inputB + (1 - lam) * concat_fake_inputB\r\n+        epoch, warped_A, target_A = next(self.train_batchA)\r\n+        epoch, warped_B, target_B = next(self.train_batchB)\r\n \r\n-        # Train the discriminators\r\n-        #print (\"Train the discriminators.\")\r\n-        if self.use_mixup:\r\n-            d_lossA = self.model.netDA.train_on_batch(mixup_A, lam * valid)\r\n-            d_lossB = self.model.netDB.train_on_batch(mixup_B, lam * valid)\r\n-        else:\r\n-            d_lossA = self.model.netDA.train_on_batch(np.concatenate([concat_real_inputA, concat_fake_inputA], axis=0), \r\n-                                                np.concatenate([valid, fake], axis=0))\r\n-            d_lossB = self.model.netDB.train_on_batch(np.concatenate([concat_real_inputB, concat_fake_inputB], axis=0),\r\n-                                                np.concatenate([valid, fake], axis=0))\r\n+        # Train dicriminators for one batch\r\n+        errDA  = self.netDA_train([warped_A, target_A])\r\n+        errDB  = self.netDB_train([warped_B, target_B])\r\n \r\n-        # ---------------------\r\n-        #  Train Generators\r\n-        # ---------------------\r\n+        # Train generators for one batch\r\n+        errGA = self.netGA_train([warped_A, target_A])\r\n+        errGB = self.netGB_train([warped_B, target_B])\r\n+\r\n+        # For calculating average losses\r\n+        self.errDA_sum += errDA[0]\r\n+        self.errDB_sum += errDB[0]\r\n+        self.errGA_sum += errGA[0]\r\n+        self.errGB_sum += errGB[0]\r\n+        self.avg_counter += 1\r\n \r\n-        # Train the generators\r\n-        #print (\"Train the generators.\")\r\n-        g_lossA = self.model.adversarial_autoencoderA.train_on_batch(warped_A, [target_A, valid])\r\n-        g_lossB = self.model.adversarial_autoencoderB.train_on_batch(warped_B, [target_B, valid])            \r\n-        \r\n         print('[%s] [%d/%s][%d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f'\r\n-              % (time.strftime(\"%H:%M:%S\"), epoch, \"num_epochs\", iter, d_lossA[0], d_lossB[0], g_lossA[0], g_lossB[0]),\r\n+              % (time.strftime(\"%H:%M:%S\"), epoch, \"num_epochs\", iter, self.errDA_sum/self.avg_counter, self.errDB_sum/self.avg_counter, self.errGA_sum/self.avg_counter, self.errGB_sum/self.avg_counter),\r\n               end='\\r')\r\n-        \r\n+\r\n         if viewer is not None:\r\n             self.show_sample(viewer)\r\n-    \r\n+\r\n+    def cycle_variables(self, netG):\r\n+        distorted_input = netG.inputs[0]\r\n+        fake_output = netG.outputs[0]\r\n+        alpha = Lambda(lambda x: x[:,:,:, :1])(fake_output)\r\n+        rgb = Lambda(lambda x: x[:,:,:, 1:])(fake_output)\r\n+\r\n+        masked_fake_output = alpha * rgb + (1-alpha) * distorted_input\r\n+\r\n+        fn_generate = K.function([distorted_input], [masked_fake_output])\r\n+        fn_mask = K.function([distorted_input], [concatenate([alpha, alpha, alpha])])\r\n+        fn_abgr = K.function([distorted_input], [concatenate([alpha, rgb])])\r\n+        fn_bgr = K.function([distorted_input], [rgb])\r\n+        return distorted_input, fake_output, alpha, fn_generate, fn_mask, fn_abgr, fn_bgr\r\n+\r\n+    def define_loss(self, netD, real, fake_argb, distorted, vggface_feat=None):\r\n+        alpha = Lambda(lambda x: x[:,:,:, :1])(fake_argb)\r\n+        fake_rgb = Lambda(lambda x: x[:,:,:, 1:])(fake_argb)\r\n+        fake = alpha * fake_rgb + (1-alpha) * distorted\r\n+\r\n+        if self.use_mixup:\r\n+            dist = Beta(self.mixup_alpha, self.mixup_alpha)\r\n+            lam = dist.sample()\r\n+            # ==========\r\n+            mixup = lam * concatenate([real, distorted]) + (1 - lam) * concatenate([fake, distorted])\r\n+            # ==========\r\n+            output_mixup = netD(mixup)\r\n+            loss_D = self.loss_fn(output_mixup, lam * K.ones_like(output_mixup))\r\n+            output_fake = netD(concatenate([fake, distorted])) # dummy\r\n+            loss_G = .5 * self.loss_fn(output_mixup, (1 - lam) * K.ones_like(output_mixup))\r\n+        else:\r\n+            output_real = netD(concatenate([real, distorted])) # positive sample\r\n+            output_fake = netD(concatenate([fake, distorted])) # negative sample\r\n+            loss_D_real = self.loss_fn(output_real, K.ones_like(output_real))\r\n+            loss_D_fake = self.loss_fn(output_fake, K.zeros_like(output_fake))\r\n+            loss_D = loss_D_real + loss_D_fake\r\n+            loss_G = .5 * self.loss_fn(output_fake, K.ones_like(output_fake))\r\n+        # ==========\r\n+        loss_G += K.mean(K.abs(fake_rgb - real))\r\n+        # ==========\r\n+\r\n+        # Edge loss (similar with total variation loss)\r\n+        loss_G += 1 * K.mean(K.abs(self.first_order(fake_rgb, axis=1) - self.first_order(real, axis=1)))\r\n+        loss_G += 1 * K.mean(K.abs(self.first_order(fake_rgb, axis=2) - self.first_order(real, axis=2)))\r\n+\r\n+\r\n+        # Perceptual Loss\r\n+        if not vggface_feat is None:\r\n+            def preprocess_vggface(x):\r\n+                x = (x + 1)/2 * 255 # channel order: BGR\r\n+                #x[..., 0] -= 93.5940\r\n+                #x[..., 1] -= 104.7624\r\n+                #x[..., 2] -= 129.\r\n+                x -= [91.4953, 103.8827, 131.0912]\r\n+                return x\r\n+            pl_params = (0.011, 0.11, 0.1919)\r\n+            real_sz224 = tf.image.resize_images(real, [224, 224])\r\n+            real_sz224 = Lambda(preprocess_vggface)(real_sz224)\r\n+            # ==========\r\n+            fake_sz224 = tf.image.resize_images(fake_rgb, [224, 224])\r\n+            fake_sz224 = Lambda(preprocess_vggface)(fake_sz224)\r\n+            # ==========\r\n+            real_feat55, real_feat28, real_feat7 = vggface_feat(real_sz224)\r\n+            fake_feat55, fake_feat28, fake_feat7  = vggface_feat(fake_sz224)\r\n+            loss_G += pl_params[0] * K.mean(K.abs(fake_feat7 - real_feat7))\r\n+            loss_G += pl_params[1] * K.mean(K.abs(fake_feat28 - real_feat28))\r\n+            loss_G += pl_params[2] * K.mean(K.abs(fake_feat55 - real_feat55))\r\n+\r\n+        return loss_D, loss_G\r\n+\r\n     def show_sample(self, display_fn):\r\n         _, wA, tA = next(self.train_batchA)\r\n         _, wB, tB = next(self.train_batchB)\r\n-        self.showG(tA, tB, display_fn)\r\n-\r\n-    def showG(self, test_A, test_B, display_fn):\r\n-        def display_fig(name, figure_A, figure_B):\r\n-            figure = np.concatenate([figure_A, figure_B], axis=0 )\r\n-            columns = 4\r\n-            elements = figure.shape[0]\r\n-            figure = figure.reshape((columns,(elements//columns)) + figure.shape[1:])\r\n-            figure = stack_images(figure)\r\n-            figure = np.clip((figure + 1) * 255 / 2, 0, 255).astype('uint8')\r\n-            display_fn(figure, name)\r\n-\r\n-        out_test_A_netGA = self.model.netGA.predict(test_A)\r\n-        out_test_A_netGB = self.model.netGB.predict(test_A)\r\n-        out_test_B_netGA = self.model.netGA.predict(test_B)\r\n-        out_test_B_netGB = self.model.netGB.predict(test_B)\r\n+        display_fn(self.showG(tA, tB, self.path_A, self.path_B), \"raw\")\r\n+        display_fn(self.showG(tA, tB, self.path_bgr_A, self.path_bgr_B), \"masked\")\r\n+        display_fn(self.showG_mask(tA, tB, self.path_mask_A, self.path_mask_B), \"mask\")\r\n+        # Reset the averages\r\n+        self.errDA_sum = self.errDB_sum = self.errGA_sum = self.errGB_sum = 0\r\n+        self.avg_counter = 0\r\n \r\n+    def showG(self, test_A, test_B, path_A, path_B):\r\n         figure_A = np.stack([\r\n             test_A,\r\n-            out_test_A_netGA[1],\r\n-            out_test_A_netGB[1],\r\n+            np.squeeze(np.array([path_A([test_A[i:i+1]]) for i in range(test_A.shape[0])])),\r\n+            np.squeeze(np.array([path_B([test_A[i:i+1]]) for i in range(test_A.shape[0])])),\r\n             ], axis=1 )\r\n         figure_B = np.stack([\r\n             test_B,\r\n-            out_test_B_netGB[1],\r\n-            out_test_B_netGA[1],\r\n+            np.squeeze(np.array([path_B([test_B[i:i+1]]) for i in range(test_B.shape[0])])),\r\n+            np.squeeze(np.array([path_A([test_B[i:i+1]]) for i in range(test_B.shape[0])])),\r\n             ], axis=1 )\r\n-        \r\n-        display_fig(\"raw\", figure_A, figure_B)       \r\n \r\n-        figure_A = np.stack([\r\n-            test_A,\r\n-            np.tile(out_test_A_netGA[0],3) * 2 - 1,\r\n-            np.tile(out_test_A_netGB[0],3) * 2 - 1,\r\n-            ], axis=1 )\r\n-        figure_B = np.stack([\r\n-            test_B,\r\n-            np.tile(out_test_B_netGB[0],3) * 2 - 1,\r\n-            np.tile(out_test_B_netGA[0],3) * 2 - 1,\r\n-            ], axis=1 )\r\n-\r\n-        display_fig(\"alpha_masks\", figure_A, figure_B)\r\n+        figure = np.concatenate([figure_A, figure_B], axis=0 )\r\n+        figure = figure.reshape((4,self.batch_size // 2) + figure.shape[1:])\r\n+        figure = stack_images(figure)\r\n+        figure = np.clip((figure + 1) * 255 / 2, 0, 255).astype('uint8')\r\n+        return figure\r\n \r\n+    def showG_mask(self, test_A, test_B, path_A, path_B):\r\n         figure_A = np.stack([\r\n             test_A,\r\n-            out_test_A_netGA[0] * out_test_A_netGA[1] + (1 - out_test_A_netGA[0]) * test_A,\r\n-            out_test_A_netGB[0] * out_test_A_netGB[1] + (1 - out_test_A_netGB[0]) * test_A,\r\n+            (np.squeeze(np.array([path_A([test_A[i:i+1]]) for i in range(test_A.shape[0])])))*2-1,\r\n+            (np.squeeze(np.array([path_B([test_A[i:i+1]]) for i in range(test_A.shape[0])])))*2-1,\r\n             ], axis=1 )\r\n         figure_B = np.stack([\r\n             test_B,\r\n-            out_test_B_netGB[0] * out_test_B_netGB[1] + (1 - out_test_B_netGB[0]) * test_B,\r\n-            out_test_B_netGA[0] * out_test_B_netGA[1] + (1 - out_test_B_netGA[0]) * test_B,\r\n+            (np.squeeze(np.array([path_B([test_B[i:i+1]]) for i in range(test_B.shape[0])])))*2-1,\r\n+            (np.squeeze(np.array([path_A([test_B[i:i+1]]) for i in range(test_B.shape[0])])))*2-1,\r\n             ], axis=1 )\r\n-        \r\n-        display_fig(\"masked\", figure_A, figure_B)\r\n+\r\n+        figure = np.concatenate([figure_A, figure_B], axis=0 )\r\n+        figure = figure.reshape((4,self.batch_size // 2) + figure.shape[1:])\r\n+        figure = stack_images(figure)\r\n+        figure = np.clip((figure + 1) * 255 / 2, 0, 255).astype('uint8')\r\n+        return figure\r\ndiff --git a/plugins/Model_GAN/instance_normalization.py b/plugins/Model_GAN/instance_normalization.py\nnew file mode 100644\nindex 0000000000..9c0616af27\n--- /dev/null\n+++ b/plugins/Model_GAN/instance_normalization.py\n@@ -0,0 +1,145 @@\n+from keras.engine import Layer, InputSpec\n+from keras import initializers, regularizers, constraints\n+from keras import backend as K\n+from keras.utils.generic_utils import get_custom_objects\n+\n+import numpy as np\n+\n+\n+class InstanceNormalization(Layer):\n+    \"\"\"Instance normalization layer (Lei Ba et al, 2016, Ulyanov et al., 2016).\n+    Normalize the activations of the previous layer at each step,\n+    i.e. applies a transformation that maintains the mean activation\n+    close to 0 and the activation standard deviation close to 1.\n+    # Arguments\n+        axis: Integer, the axis that should be normalized\n+            (typically the features axis).\n+            For instance, after a `Conv2D` layer with\n+            `data_format=\"channels_first\"`,\n+            set `axis=1` in `InstanceNormalization`.\n+            Setting `axis=None` will normalize all values in each instance of the batch.\n+            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n+        epsilon: Small float added to variance to avoid dividing by zero.\n+        center: If True, add offset of `beta` to normalized tensor.\n+            If False, `beta` is ignored.\n+        scale: If True, multiply by `gamma`.\n+            If False, `gamma` is not used.\n+            When the next layer is linear (also e.g. `nn.relu`),\n+            this can be disabled since the scaling\n+            will be done by the next layer.\n+        beta_initializer: Initializer for the beta weight.\n+        gamma_initializer: Initializer for the gamma weight.\n+        beta_regularizer: Optional regularizer for the beta weight.\n+        gamma_regularizer: Optional regularizer for the gamma weight.\n+        beta_constraint: Optional constraint for the beta weight.\n+        gamma_constraint: Optional constraint for the gamma weight.\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+    # Output shape\n+        Same shape as input.\n+    # References\n+        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n+        - [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)\n+    \"\"\"\n+    def __init__(self,\n+                 axis=None,\n+                 epsilon=1e-3,\n+                 center=True,\n+                 scale=True,\n+                 beta_initializer='zeros',\n+                 gamma_initializer='ones',\n+                 beta_regularizer=None,\n+                 gamma_regularizer=None,\n+                 beta_constraint=None,\n+                 gamma_constraint=None,\n+                 **kwargs):\n+        super(InstanceNormalization, self).__init__(**kwargs)\n+        self.supports_masking = True\n+        self.axis = axis\n+        self.epsilon = epsilon\n+        self.center = center\n+        self.scale = scale\n+        self.beta_initializer = initializers.get(beta_initializer)\n+        self.gamma_initializer = initializers.get(gamma_initializer)\n+        self.beta_regularizer = regularizers.get(beta_regularizer)\n+        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n+        self.beta_constraint = constraints.get(beta_constraint)\n+        self.gamma_constraint = constraints.get(gamma_constraint)\n+\n+    def build(self, input_shape):\n+        ndim = len(input_shape)\n+        if self.axis == 0:\n+            raise ValueError('Axis cannot be zero')\n+\n+        if (self.axis is not None) and (ndim == 2):\n+            raise ValueError('Cannot specify axis for rank 1 tensor')\n+\n+        self.input_spec = InputSpec(ndim=ndim)\n+\n+        if self.axis is None:\n+            shape = (1,)\n+        else:\n+            shape = (input_shape[self.axis],)\n+\n+        if self.scale:\n+            self.gamma = self.add_weight(shape=shape,\n+                                         name='gamma',\n+                                         initializer=self.gamma_initializer,\n+                                         regularizer=self.gamma_regularizer,\n+                                         constraint=self.gamma_constraint)\n+        else:\n+            self.gamma = None\n+        if self.center:\n+            self.beta = self.add_weight(shape=shape,\n+                                        name='beta',\n+                                        initializer=self.beta_initializer,\n+                                        regularizer=self.beta_regularizer,\n+                                        constraint=self.beta_constraint)\n+        else:\n+            self.beta = None\n+        self.built = True\n+\n+    def call(self, inputs, training=None):\n+        input_shape = K.int_shape(inputs)\n+        reduction_axes = list(range(0, len(input_shape)))\n+\n+        if (self.axis is not None):\n+            del reduction_axes[self.axis]\n+\n+        del reduction_axes[0]\n+\n+        mean = K.mean(inputs, reduction_axes, keepdims=True)\n+        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n+        normed = (inputs - mean) / stddev\n+\n+        broadcast_shape = [1] * len(input_shape)\n+        if self.axis is not None:\n+            broadcast_shape[self.axis] = input_shape[self.axis]\n+\n+        if self.scale:\n+            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n+            normed = normed * broadcast_gamma\n+        if self.center:\n+            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n+            normed = normed + broadcast_beta\n+        return normed\n+\n+    def get_config(self):\n+        config = {\n+            'axis': self.axis,\n+            'epsilon': self.epsilon,\n+            'center': self.center,\n+            'scale': self.scale,\n+            'beta_initializer': initializers.serialize(self.beta_initializer),\n+            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n+            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n+            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n+            'beta_constraint': constraints.serialize(self.beta_constraint),\n+            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n+        }\n+        base_config = super(InstanceNormalization, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+get_custom_objects().update({'InstanceNormalization': InstanceNormalization})\ndiff --git a/plugins/Model_GAN128/Model.py b/plugins/Model_GAN128/Model.py\nnew file mode 100644\nindex 0000000000..9d14850101\n--- /dev/null\n+++ b/plugins/Model_GAN128/Model.py\n@@ -0,0 +1,177 @@\n+# Based on the https://github.com/shaoanlu/faceswap-GAN repo \r\n+# source : https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2_sz128_train.ipynbtemp/faceswap_GAN_keras.ipynb\r\n+\r\n+from keras.models import Model\r\n+from keras.layers import *\r\n+from keras.layers.advanced_activations import LeakyReLU\r\n+from keras.activations import relu\r\n+from keras.initializers import RandomNormal\r\n+from keras.applications import *\r\n+from keras.optimizers import Adam\r\n+\r\n+from lib.PixelShuffler import PixelShuffler\r\n+from .instance_normalization import InstanceNormalization\r\n+\r\n+netGAH5 = 'netGA_GAN128.h5'\r\n+netGBH5 = 'netGB_GAN128.h5'\r\n+netDAH5 = 'netDA_GAN128.h5'\r\n+netDBH5 = 'netDB_GAN128.h5'\r\n+\r\n+def __conv_init(a):\r\n+    print(\"conv_init\", a)\r\n+    k = RandomNormal(0, 0.02)(a) # for convolution kernel\r\n+    k.conv_weight = True    \r\n+    return k\r\n+\r\n+#def batchnorm():\r\n+#    return BatchNormalization(momentum=0.9, axis=channel_axis, epsilon=1.01e-5, gamma_initializer = gamma_init)\r\n+\r\n+def inst_norm():\r\n+    return InstanceNormalization()\r\n+\r\n+conv_init = RandomNormal(0, 0.02)\r\n+gamma_init = RandomNormal(1., 0.02) # for batch normalization\r\n+\r\n+class GANModel():\r\n+    img_size = 128\r\n+    channels = 3\r\n+    img_shape = (img_size, img_size, channels)\r\n+    encoded_dim = 1024\r\n+    nc_in = 3 # number of input channels of generators\r\n+    nc_D_inp = 6 # number of input channels of discriminators\r\n+\r\n+    def __init__(self, model_dir):\r\n+        self.model_dir = model_dir\r\n+\r\n+        optimizer = Adam(1e-4, 0.5)\r\n+\r\n+        # Build and compile the discriminator\r\n+        self.netDA, self.netDB = self.build_discriminator()\r\n+\r\n+        # Build and compile the generator\r\n+        self.netGA, self.netGB = self.build_generator()\r\n+\r\n+    def converter(self, swap):\r\n+        predictor = self.netGB if not swap else self.netGA\r\n+        return lambda img: predictor.predict(img)\r\n+\r\n+    def build_generator(self):\r\n+\r\n+        def conv_block(input_tensor, f, use_instance_norm=True):\r\n+            x = input_tensor\r\n+            x = SeparableConv2D(f, kernel_size=3, strides=2, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(x)\r\n+            if use_instance_norm:\r\n+                x = inst_norm()(x)\r\n+            x = Activation(\"relu\")(x)\r\n+            return x\r\n+\r\n+        def res_block(input_tensor, f, dilation=1):\r\n+            x = input_tensor\r\n+            x = Conv2D(f, kernel_size=3, kernel_initializer=conv_init, use_bias=False, padding=\"same\", dilation_rate=dilation)(x)\r\n+            x = LeakyReLU(alpha=0.2)(x)\r\n+            x = Conv2D(f, kernel_size=3, kernel_initializer=conv_init, use_bias=False, padding=\"same\", dilation_rate=dilation)(x)\r\n+            x = add([x, input_tensor])\r\n+            #x = LeakyReLU(alpha=0.2)(x)\r\n+            return x\r\n+\r\n+        def upscale_ps(filters, use_instance_norm=True):\r\n+            def block(x, use_instance_norm=use_instance_norm):\r\n+                x = Conv2D(filters*4, kernel_size=3, use_bias=False, kernel_initializer=RandomNormal(0, 0.02), padding='same')(x)\r\n+                if use_instance_norm:\r\n+                    x = inst_norm()(x)\r\n+                x = LeakyReLU(0.1)(x)\r\n+                x = PixelShuffler()(x)\r\n+                return x\r\n+            return block\r\n+\r\n+        def Encoder(nc_in=3, input_size=128):\r\n+            inp = Input(shape=(input_size, input_size, nc_in))\r\n+            x = Conv2D(32, kernel_size=5, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(inp)\r\n+            x = conv_block(x,64, use_instance_norm=False)\r\n+            x = conv_block(x,128)\r\n+            x = conv_block(x,256)\r\n+            x = conv_block(x,512) \r\n+            x = conv_block(x,1024)\r\n+            x = Dense(1024)(Flatten()(x))\r\n+            x = Dense(4*4*1024)(x)\r\n+            x = Reshape((4, 4, 1024))(x)\r\n+            out = upscale_ps(512)(x)\r\n+            return Model(inputs=inp, outputs=out)\r\n+\r\n+        def Decoder_ps(nc_in=512, input_size=8):\r\n+            input_ = Input(shape=(input_size, input_size, nc_in))\r\n+            x = input_\r\n+            x = upscale_ps(256)(x)\r\n+            x = upscale_ps(128)(x)    \r\n+            x = upscale_ps(64)(x)\r\n+            x = res_block(x, 64, dilation=2)      \r\n+            \r\n+            out64 = Conv2D(64, kernel_size=3, padding='same')(x)\r\n+            out64 = LeakyReLU(alpha=0.1)(out64)\r\n+            out64 = Conv2D(3, kernel_size=5, padding='same', activation=\"tanh\")(out64)\r\n+            \r\n+            x = upscale_ps(32)(x)\r\n+            x = res_block(x, 32)\r\n+            x = res_block(x, 32)\r\n+            alpha = Conv2D(1, kernel_size=5, padding='same', activation=\"sigmoid\")(x)\r\n+            rgb = Conv2D(3, kernel_size=5, padding='same', activation=\"tanh\")(x)\r\n+            out = concatenate([alpha, rgb])\r\n+            return Model(input_, [out, out64] )\r\n+        \r\n+        encoder = Encoder()\r\n+        decoder_A = Decoder_ps()\r\n+        decoder_B = Decoder_ps()\r\n+        x = Input(shape=self.img_shape)\r\n+        netGA = Model(x, decoder_A(encoder(x)))\r\n+        netGB = Model(x, decoder_B(encoder(x)))\r\n+        try:\r\n+            netGA.load_weights(str(self.model_dir / netGAH5))\r\n+            netGB.load_weights(str(self.model_dir / netGBH5))\r\n+            print (\"Generator models loaded.\")\r\n+        except:\r\n+            print (\"Generator weights files not found.\")\r\n+            pass\r\n+        return netGA, netGB\r\n+\r\n+    def build_discriminator(self):\r\n+        def conv_block_d(input_tensor, f, use_instance_norm=True):\r\n+            x = input_tensor\r\n+            x = Conv2D(f, kernel_size=4, strides=2, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(x)\r\n+            if use_instance_norm:\r\n+                x = inst_norm()(x)\r\n+            x = LeakyReLU(alpha=0.2)(x)\r\n+            return x\r\n+\r\n+        def Discriminator(nc_in, input_size=128):\r\n+            inp = Input(shape=(input_size, input_size, nc_in))\r\n+            #x = GaussianNoise(0.05)(inp)\r\n+            x = conv_block_d(inp, 64, False)\r\n+            x = conv_block_d(x, 128, True)\r\n+            x = conv_block_d(x, 256, True)\r\n+            x = conv_block_d(x, 512, True)\r\n+            out = Conv2D(1, kernel_size=4, kernel_initializer=conv_init, use_bias=False, padding=\"same\", activation=\"sigmoid\")(x)   \r\n+            return Model(inputs=[inp], outputs=out)\r\n+        \r\n+        netDA = Discriminator(self.nc_D_inp)\r\n+        netDB = Discriminator(self.nc_D_inp)\r\n+        try:\r\n+            netDA.load_weights(str(self.model_dir / netDAH5))\r\n+            netDB.load_weights(str(self.model_dir / netDBH5))\r\n+            print (\"Discriminator models loaded.\")\r\n+        except:\r\n+            print (\"Discriminator weights files not found.\")\r\n+            pass\r\n+        return netDA, netDB    \r\n+    \r\n+    def load(self, swapped):\r\n+        if swapped:\r\n+            print(\"swapping not supported on GAN\")\r\n+            # TODO load is done in __init__ => look how to swap if possible\r\n+        return True\r\n+    \r\n+    def save_weights(self):\r\n+        self.netGA.save_weights(str(self.model_dir / netGAH5))\r\n+        self.netGB.save_weights(str(self.model_dir / netGBH5))\r\n+        self.netDA.save_weights(str(self.model_dir / netDAH5))\r\n+        self.netDB.save_weights(str(self.model_dir / netDBH5))\r\n+        print (\"Models saved.\")\n\\ No newline at end of file\ndiff --git a/plugins/Model_GAN128/Trainer.py b/plugins/Model_GAN128/Trainer.py\nnew file mode 100644\nindex 0000000000..87aeb8b9f8\n--- /dev/null\n+++ b/plugins/Model_GAN128/Trainer.py\n@@ -0,0 +1,253 @@\n+import time\r\n+import cv2\r\n+import numpy as np\r\n+\r\n+from keras.layers import *\r\n+from tensorflow.contrib.distributions import Beta\r\n+import tensorflow as tf\r\n+from keras.optimizers import Adam\r\n+from keras import backend as K\r\n+\r\n+from lib.training_data import TrainingDataGenerator, stack_images\r\n+\r\n+class GANTrainingDataGenerator(TrainingDataGenerator):\r\n+    def __init__(self, random_transform_args, coverage, scale, zoom):\r\n+        super().__init__(random_transform_args, coverage, scale, zoom)\r\n+\r\n+    def color_adjust(self, img):\r\n+        return img / 255.0 * 2 - 1\r\n+\r\n+class Trainer():\r\n+    random_transform_args = {\r\n+        'rotation_range': 20,\r\n+        'zoom_range': 0.1,\r\n+        'shift_range': 0.05,\r\n+        'random_flip': 0.5,\r\n+        }\r\n+\r\n+    def __init__(self, model, fn_A, fn_B, batch_size, perceptual_loss):\r\n+        K.set_learning_phase(1)\r\n+\r\n+        assert batch_size % 2 == 0, \"batch_size must be an even number\"\r\n+        self.batch_size = batch_size\r\n+        self.model = model\r\n+\r\n+        self.use_lsgan = True\r\n+        self.use_mixup = True\r\n+        self.mixup_alpha = 0.2\r\n+        self.use_perceptual_loss = perceptual_loss\r\n+\r\n+        self.lrD = 1e-4 # Discriminator learning rate\r\n+        self.lrG = 1e-4 # Generator learning rate\r\n+\r\n+        generator = GANTrainingDataGenerator(self.random_transform_args, 220, 6, 2)\r\n+        self.train_batchA = generator.minibatchAB(fn_A, batch_size)\r\n+        self.train_batchB = generator.minibatchAB(fn_B, batch_size)\r\n+\r\n+        self.avg_counter = self.errDA_sum = self.errDB_sum = self.errGA_sum = self.errGB_sum = 0\r\n+\r\n+        self.setup()\r\n+\r\n+    def setup(self):\r\n+        distorted_A, fake_A, fake_sz64_A, mask_A, self.path_A, self.path_mask_A, self.path_abgr_A, self.path_bgr_A = self.cycle_variables(self.model.netGA)\r\n+        distorted_B, fake_B, fake_sz64_B, mask_B, self.path_B, self.path_mask_B, self.path_abgr_B, self.path_bgr_B = self.cycle_variables(self.model.netGB)\r\n+        real_A = Input(shape=self.model.img_shape)\r\n+        real_B = Input(shape=self.model.img_shape)\r\n+\r\n+        if self.use_lsgan:\r\n+            self.loss_fn = lambda output, target : K.mean(K.abs(K.square(output-target)))\r\n+        else:\r\n+            self.loss_fn = lambda output, target : -K.mean(K.log(output+1e-12)*target+K.log(1-output+1e-12)*(1-target))\r\n+\r\n+        # ========== Define Perceptual Loss Model==========\r\n+        if self.use_perceptual_loss:\r\n+            from keras.models import Model\r\n+            from keras_vggface.vggface import VGGFace\r\n+            vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\r\n+            vggface.trainable = False\r\n+            out_size55 = vggface.layers[36].output\r\n+            out_size28 = vggface.layers[78].output\r\n+            out_size7 = vggface.layers[-2].output\r\n+            vggface_feat = Model(vggface.input, [out_size55, out_size28, out_size7])\r\n+            vggface_feat.trainable = False\r\n+        else:\r\n+            vggface_feat = None\r\n+\r\n+        #TODO check \"Tips for mask refinement (optional after >15k iters)\" => https://render.githubusercontent.com/view/ipynb?commit=87d6e7a28ce754acd38d885367b6ceb0be92ec54&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7368616f616e6c752f66616365737761702d47414e2f383764366537613238636537353461636433386438383533363762366365623062653932656335342f46616365537761705f47414e5f76325f737a3132385f747261696e2e6970796e62&nwo=shaoanlu%2Ffaceswap-GAN&path=FaceSwap_GAN_v2_sz128_train.ipynb&repository_id=115182783&repository_type=Repository#Tips-for-mask-refinement-(optional-after-%3E15k-iters)\r\n+        loss_DA, loss_GA = self.define_loss(self.model.netDA, real_A, fake_A, fake_sz64_A, distorted_A, vggface_feat)\r\n+        loss_DB, loss_GB = self.define_loss(self.model.netDB, real_B, fake_B, fake_sz64_B, distorted_B, vggface_feat)\r\n+\r\n+        loss_GA += 3e-3 * K.mean(K.abs(mask_A))\r\n+        loss_GB += 3e-3 * K.mean(K.abs(mask_B))\r\n+\r\n+        w_fo = 0.01\r\n+        loss_GA += w_fo * K.mean(self.first_order(mask_A, axis=1))\r\n+        loss_GA += w_fo * K.mean(self.first_order(mask_A, axis=2))\r\n+        loss_GB += w_fo * K.mean(self.first_order(mask_B, axis=1))\r\n+        loss_GB += w_fo * K.mean(self.first_order(mask_B, axis=2))\r\n+\r\n+        weightsDA = self.model.netDA.trainable_weights\r\n+        weightsGA = self.model.netGA.trainable_weights\r\n+        weightsDB = self.model.netDB.trainable_weights\r\n+        weightsGB = self.model.netGB.trainable_weights\r\n+\r\n+        # Adam(..).get_updates(...)\r\n+        training_updates = Adam(lr=self.lrD, beta_1=0.5).get_updates(weightsDA,[],loss_DA)\r\n+        self.netDA_train = K.function([distorted_A, real_A],[loss_DA], training_updates)\r\n+        training_updates = Adam(lr=self.lrG, beta_1=0.5).get_updates(weightsGA,[], loss_GA)\r\n+        self.netGA_train = K.function([distorted_A, real_A], [loss_GA], training_updates)\r\n+\r\n+        training_updates = Adam(lr=self.lrD, beta_1=0.5).get_updates(weightsDB,[],loss_DB)\r\n+        self.netDB_train = K.function([distorted_B, real_B],[loss_DB], training_updates)\r\n+        training_updates = Adam(lr=self.lrG, beta_1=0.5).get_updates(weightsGB,[], loss_GB)\r\n+        self.netGB_train = K.function([distorted_B, real_B], [loss_GB], training_updates)\r\n+\r\n+    def first_order(self, x, axis=1):\r\n+        img_nrows = x.shape[1]\r\n+        img_ncols = x.shape[2]\r\n+        if axis == 1:\r\n+            return K.abs(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\r\n+        elif axis == 2:\r\n+            return K.abs(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\r\n+        else:\r\n+            return None\r\n+\r\n+    def train_one_step(self, iter, viewer):\r\n+        # ---------------------\r\n+        #  Train Discriminators\r\n+        # ---------------------\r\n+\r\n+        # Select a random half batch of images\r\n+        epoch, warped_A, target_A = next(self.train_batchA)\r\n+        epoch, warped_B, target_B = next(self.train_batchB)\r\n+\r\n+        # Train dicriminators for one batch\r\n+        errDA  = self.netDA_train([warped_A, target_A])\r\n+        errDB  = self.netDB_train([warped_B, target_B])\r\n+\r\n+        # Train generators for one batch\r\n+        errGA = self.netGA_train([warped_A, target_A])\r\n+        errGB = self.netGB_train([warped_B, target_B])\r\n+\r\n+        # For calculating average losses\r\n+        self.errDA_sum += errDA[0]\r\n+        self.errDB_sum += errDB[0]\r\n+        self.errGA_sum += errGA[0]\r\n+        self.errGB_sum += errGB[0]\r\n+        self.avg_counter += 1\r\n+\r\n+        print('[%s] [%d/%s][%d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f'\r\n+              % (time.strftime(\"%H:%M:%S\"), epoch, \"num_epochs\", iter, self.errDA_sum/self.avg_counter, self.errDB_sum/self.avg_counter, self.errGA_sum/self.avg_counter, self.errGB_sum/self.avg_counter),\r\n+              end='\\r')\r\n+\r\n+        if viewer is not None:\r\n+            self.show_sample(viewer)\r\n+\r\n+    def cycle_variables(self, netG):\r\n+        distorted_input = netG.inputs[0]\r\n+        fake_output = netG.outputs[0]\r\n+        fake_output64 = netG.outputs[1]\r\n+        alpha = Lambda(lambda x: x[:,:,:, :1])(fake_output)\r\n+        rgb = Lambda(lambda x: x[:,:,:, 1:])(fake_output)\r\n+\r\n+        masked_fake_output = alpha * rgb + (1-alpha) * distorted_input\r\n+\r\n+        fn_generate = K.function([distorted_input], [masked_fake_output])\r\n+        fn_mask = K.function([distorted_input], [concatenate([alpha, alpha, alpha])])\r\n+        fn_abgr = K.function([distorted_input], [concatenate([alpha, rgb])])\r\n+        fn_bgr = K.function([distorted_input], [rgb])\r\n+        return distorted_input, fake_output, fake_output64, alpha, fn_generate, fn_mask, fn_abgr, fn_bgr\r\n+\r\n+    def define_loss(self, netD, real, fake_argb, fake_sz64, distorted, vggface_feat=None):\r\n+        alpha = Lambda(lambda x: x[:,:,:, :1])(fake_argb)\r\n+        fake_rgb = Lambda(lambda x: x[:,:,:, 1:])(fake_argb)\r\n+        fake = alpha * fake_rgb + (1-alpha) * distorted\r\n+\r\n+        if self.use_mixup:\r\n+            dist = Beta(self.mixup_alpha, self.mixup_alpha)\r\n+            lam = dist.sample()\r\n+            # ==========\r\n+            mixup = lam * concatenate([real, distorted]) + (1 - lam) * concatenate([fake, distorted])\r\n+            # ==========\r\n+            output_mixup = netD(mixup)\r\n+            loss_D = self.loss_fn(output_mixup, lam * K.ones_like(output_mixup))\r\n+            #output_fake = netD(concatenate([fake, distorted])) # dummy\r\n+            loss_G = 1 * self.loss_fn(output_mixup, (1 - lam) * K.ones_like(output_mixup))\r\n+        else:\r\n+            output_real = netD(concatenate([real, distorted])) # positive sample\r\n+            output_fake = netD(concatenate([fake, distorted])) # negative sample\r\n+            loss_D_real = self.loss_fn(output_real, K.ones_like(output_real))\r\n+            loss_D_fake = self.loss_fn(output_fake, K.zeros_like(output_fake))\r\n+            loss_D = loss_D_real + loss_D_fake\r\n+            loss_G = 1 * self.loss_fn(output_fake, K.ones_like(output_fake))\r\n+        # ==========\r\n+        loss_G += K.mean(K.abs(fake_rgb - real))\r\n+        loss_G += K.mean(K.abs(fake_sz64 - tf.image.resize_images(real, [64, 64])))\r\n+        # ==========\r\n+\r\n+        # Perceptual Loss\r\n+        if not vggface_feat is None:\r\n+            def preprocess_vggface(x):\r\n+                x = (x + 1)/2 * 255 # channel order: BGR\r\n+                x -= [93.5940, 104.7624, 129.]\r\n+                return x\r\n+            pl_params = (0.02, 0.3, 0.5)\r\n+            real_sz224 = tf.image.resize_images(real, [224, 224])\r\n+            real_sz224 = Lambda(preprocess_vggface)(real_sz224)\r\n+            # ==========\r\n+            fake_sz224 = tf.image.resize_images(fake_rgb, [224, 224])\r\n+            fake_sz224 = Lambda(preprocess_vggface)(fake_sz224)\r\n+            # ==========\r\n+            real_feat55, real_feat28, real_feat7 = vggface_feat(real_sz224)\r\n+            fake_feat55, fake_feat28, fake_feat7  = vggface_feat(fake_sz224)\r\n+            loss_G += pl_params[0] * K.mean(K.abs(fake_feat7 - real_feat7))\r\n+            loss_G += pl_params[1] * K.mean(K.abs(fake_feat28 - real_feat28))\r\n+            loss_G += pl_params[2] * K.mean(K.abs(fake_feat55 - real_feat55))\r\n+\r\n+        return loss_D, loss_G\r\n+\r\n+    def show_sample(self, display_fn):\r\n+        _, wA, tA = next(self.train_batchA)\r\n+        _, wB, tB = next(self.train_batchB)\r\n+        display_fn(self.showG(tA, tB, self.path_A, self.path_B), \"raw\")\r\n+        display_fn(self.showG(tA, tB, self.path_bgr_A, self.path_bgr_B), \"masked\")\r\n+        display_fn(self.showG_mask(tA, tB, self.path_mask_A, self.path_mask_B), \"mask\")\r\n+        # Reset the averages\r\n+        self.errDA_sum = self.errDB_sum = self.errGA_sum = self.errGB_sum = 0\r\n+        self.avg_counter = 0\r\n+\r\n+    def showG(self, test_A, test_B, path_A, path_B):\r\n+        figure_A = np.stack([\r\n+            test_A,\r\n+            np.squeeze(np.array([path_A([test_A[i:i+1]]) for i in range(test_A.shape[0])])),\r\n+            np.squeeze(np.array([path_B([test_A[i:i+1]]) for i in range(test_A.shape[0])])),\r\n+            ], axis=1 )\r\n+        figure_B = np.stack([\r\n+            test_B,\r\n+            np.squeeze(np.array([path_B([test_B[i:i+1]]) for i in range(test_B.shape[0])])),\r\n+            np.squeeze(np.array([path_A([test_B[i:i+1]]) for i in range(test_B.shape[0])])),\r\n+            ], axis=1 )\r\n+\r\n+        figure = np.concatenate([figure_A, figure_B], axis=0 )\r\n+        figure = figure.reshape((4,self.batch_size // 2) + figure.shape[1:])\r\n+        figure = stack_images(figure)\r\n+        figure = np.clip((figure + 1) * 255 / 2, 0, 255).astype('uint8')\r\n+        return figure\r\n+\r\n+    def showG_mask(self, test_A, test_B, path_A, path_B):\r\n+        figure_A = np.stack([\r\n+            test_A,\r\n+            (np.squeeze(np.array([path_A([test_A[i:i+1]]) for i in range(test_A.shape[0])])))*2-1,\r\n+            (np.squeeze(np.array([path_B([test_A[i:i+1]]) for i in range(test_A.shape[0])])))*2-1,\r\n+            ], axis=1 )\r\n+        figure_B = np.stack([\r\n+            test_B,\r\n+            (np.squeeze(np.array([path_B([test_B[i:i+1]]) for i in range(test_B.shape[0])])))*2-1,\r\n+            (np.squeeze(np.array([path_A([test_B[i:i+1]]) for i in range(test_B.shape[0])])))*2-1,\r\n+            ], axis=1 )\r\n+\r\n+        figure = np.concatenate([figure_A, figure_B], axis=0 )\r\n+        figure = figure.reshape((4,self.batch_size // 2) + figure.shape[1:])\r\n+        figure = stack_images(figure)\r\n+        figure = np.clip((figure + 1) * 255 / 2, 0, 255).astype('uint8')\r\n+        return figure\r\ndiff --git a/plugins/Model_GAN128/instance_normalization.py b/plugins/Model_GAN128/instance_normalization.py\nnew file mode 100644\nindex 0000000000..9c0616af27\n--- /dev/null\n+++ b/plugins/Model_GAN128/instance_normalization.py\n@@ -0,0 +1,145 @@\n+from keras.engine import Layer, InputSpec\n+from keras import initializers, regularizers, constraints\n+from keras import backend as K\n+from keras.utils.generic_utils import get_custom_objects\n+\n+import numpy as np\n+\n+\n+class InstanceNormalization(Layer):\n+    \"\"\"Instance normalization layer (Lei Ba et al, 2016, Ulyanov et al., 2016).\n+    Normalize the activations of the previous layer at each step,\n+    i.e. applies a transformation that maintains the mean activation\n+    close to 0 and the activation standard deviation close to 1.\n+    # Arguments\n+        axis: Integer, the axis that should be normalized\n+            (typically the features axis).\n+            For instance, after a `Conv2D` layer with\n+            `data_format=\"channels_first\"`,\n+            set `axis=1` in `InstanceNormalization`.\n+            Setting `axis=None` will normalize all values in each instance of the batch.\n+            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n+        epsilon: Small float added to variance to avoid dividing by zero.\n+        center: If True, add offset of `beta` to normalized tensor.\n+            If False, `beta` is ignored.\n+        scale: If True, multiply by `gamma`.\n+            If False, `gamma` is not used.\n+            When the next layer is linear (also e.g. `nn.relu`),\n+            this can be disabled since the scaling\n+            will be done by the next layer.\n+        beta_initializer: Initializer for the beta weight.\n+        gamma_initializer: Initializer for the gamma weight.\n+        beta_regularizer: Optional regularizer for the beta weight.\n+        gamma_regularizer: Optional regularizer for the gamma weight.\n+        beta_constraint: Optional constraint for the beta weight.\n+        gamma_constraint: Optional constraint for the gamma weight.\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+    # Output shape\n+        Same shape as input.\n+    # References\n+        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n+        - [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)\n+    \"\"\"\n+    def __init__(self,\n+                 axis=None,\n+                 epsilon=1e-3,\n+                 center=True,\n+                 scale=True,\n+                 beta_initializer='zeros',\n+                 gamma_initializer='ones',\n+                 beta_regularizer=None,\n+                 gamma_regularizer=None,\n+                 beta_constraint=None,\n+                 gamma_constraint=None,\n+                 **kwargs):\n+        super(InstanceNormalization, self).__init__(**kwargs)\n+        self.supports_masking = True\n+        self.axis = axis\n+        self.epsilon = epsilon\n+        self.center = center\n+        self.scale = scale\n+        self.beta_initializer = initializers.get(beta_initializer)\n+        self.gamma_initializer = initializers.get(gamma_initializer)\n+        self.beta_regularizer = regularizers.get(beta_regularizer)\n+        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n+        self.beta_constraint = constraints.get(beta_constraint)\n+        self.gamma_constraint = constraints.get(gamma_constraint)\n+\n+    def build(self, input_shape):\n+        ndim = len(input_shape)\n+        if self.axis == 0:\n+            raise ValueError('Axis cannot be zero')\n+\n+        if (self.axis is not None) and (ndim == 2):\n+            raise ValueError('Cannot specify axis for rank 1 tensor')\n+\n+        self.input_spec = InputSpec(ndim=ndim)\n+\n+        if self.axis is None:\n+            shape = (1,)\n+        else:\n+            shape = (input_shape[self.axis],)\n+\n+        if self.scale:\n+            self.gamma = self.add_weight(shape=shape,\n+                                         name='gamma',\n+                                         initializer=self.gamma_initializer,\n+                                         regularizer=self.gamma_regularizer,\n+                                         constraint=self.gamma_constraint)\n+        else:\n+            self.gamma = None\n+        if self.center:\n+            self.beta = self.add_weight(shape=shape,\n+                                        name='beta',\n+                                        initializer=self.beta_initializer,\n+                                        regularizer=self.beta_regularizer,\n+                                        constraint=self.beta_constraint)\n+        else:\n+            self.beta = None\n+        self.built = True\n+\n+    def call(self, inputs, training=None):\n+        input_shape = K.int_shape(inputs)\n+        reduction_axes = list(range(0, len(input_shape)))\n+\n+        if (self.axis is not None):\n+            del reduction_axes[self.axis]\n+\n+        del reduction_axes[0]\n+\n+        mean = K.mean(inputs, reduction_axes, keepdims=True)\n+        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n+        normed = (inputs - mean) / stddev\n+\n+        broadcast_shape = [1] * len(input_shape)\n+        if self.axis is not None:\n+            broadcast_shape[self.axis] = input_shape[self.axis]\n+\n+        if self.scale:\n+            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n+            normed = normed * broadcast_gamma\n+        if self.center:\n+            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n+            normed = normed + broadcast_beta\n+        return normed\n+\n+    def get_config(self):\n+        config = {\n+            'axis': self.axis,\n+            'epsilon': self.epsilon,\n+            'center': self.center,\n+            'scale': self.scale,\n+            'beta_initializer': initializers.serialize(self.beta_initializer),\n+            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n+            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n+            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n+            'beta_constraint': constraints.serialize(self.beta_constraint),\n+            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n+        }\n+        base_config = super(InstanceNormalization, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+get_custom_objects().update({'InstanceNormalization': InstanceNormalization})\ndiff --git a/plugins/Model_Original/Trainer.py b/plugins/Model_Original/Trainer.py\nnew file mode 100644\nindex 0000000000..5b1c918f62\n--- /dev/null\n+++ b/plugins/Model_Original/Trainer.py\n@@ -0,0 +1,50 @@\n+\n+import time\n+import numpy\n+from lib.training_data import TrainingDataGenerator, stack_images\n+\n+class Trainer():\n+    random_transform_args = {\n+        'rotation_range': 10,\n+        'zoom_range': 0.05,\n+        'shift_range': 0.05,\n+        'random_flip': 0.4,\n+    }\n+\n+    def __init__(self, model, fn_A, fn_B, batch_size, *args):\n+        self.batch_size = batch_size\n+        self.model = model\n+\n+        generator = TrainingDataGenerator(self.random_transform_args, 160)\n+        self.images_A = generator.minibatchAB(fn_A, self.batch_size)\n+        self.images_B = generator.minibatchAB(fn_B, self.batch_size)\n+\n+    def train_one_step(self, iter, viewer):\n+        epoch, warped_A, target_A = next(self.images_A)\n+        epoch, warped_B, target_B = next(self.images_B)\n+\n+        loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)\n+        loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)\n+        print(\"[{0}] [#{1:05d}] loss_A: {2:.5f}, loss_B: {3:.5f}\".format(time.strftime(\"%H:%M:%S\"), iter, loss_A, loss_B),\n+            end='\\r')\n+\n+        if viewer is not None:\n+            viewer(self.show_sample(target_A[0:14], target_B[0:14]), \"training\")\n+\n+    def show_sample(self, test_A, test_B):\n+        figure_A = numpy.stack([\n+            test_A,\n+            self.model.autoencoder_A.predict(test_A),\n+            self.model.autoencoder_B.predict(test_A),\n+        ], axis=1)\n+        figure_B = numpy.stack([\n+            test_B,\n+            self.model.autoencoder_B.predict(test_B),\n+            self.model.autoencoder_A.predict(test_B),\n+        ], axis=1)\n+\n+        figure = numpy.concatenate([figure_A, figure_B], axis=0)\n+        figure = figure.reshape((4, 7) + figure.shape[1:])\n+        figure = stack_images(figure)\n+\n+        return numpy.clip(figure * 255, 0, 255).astype('uint8')\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The stack trace suggests an issue with the `first_order` function due to a missing `self`. Ensure all method calls include `self` to avoid `NameError`. Your change addresses that, but double-check the implementation across affected classes. The crash may also relate to an outdated Keras version. Since you've confirmed that wasnt the case, it's worth keeping an eye on package dependencies to prevent compatibility issues. Good job resolving the GPU issue with the TensorFlow reinstall."
      }
    ],
    "codediff_length": 61242
  },
  {
    "index": 70,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/plugins/Convert_DFaker.py b/plugins/Convert_DFaker.py\nnew file mode 100644\nindex 0000000000..b1566505b4\n--- /dev/null\n+++ b/plugins/Convert_DFaker.py\n@@ -0,0 +1,263 @@\n+import argparse\n+import cv2\n+import json\n+import numpy\n+from pathlib import Path\n+from tqdm import tqdm\n+from scipy import ndimage\n+# from model import autoencoder_A\n+# from model import autoencoder_B\n+# from model import encoder, decoder_A, decoder_B\n+\n+# encoder  .load_weights( \"models/encoder.h5\"   )\n+# decoder_A.load_weights( \"models/decoder_A.h5\" )\n+# decoder_B.load_weights( \"models/decoder_B.h5\" )\n+import time\n+\n+n=0\n+\n+imageSize = 256\n+croppedSize = 240 \n+\n+import cv2\n+import numpy\n+\n+from lib.aligner import get_align_mat\n+\n+class Convert():\n+    def __init__(self, encoder, trainer, blur_size=2, seamless_clone=False, mask_type=\"facehullandrect\", erosion_kernel_size=None, match_histogram=False, sharpen_image=None, double_pass=None, **kwargs):\n+        self.encoder_A = encoder(False)\n+        self.encoder_B = encoder(True)\n+        self.trainer = trainer\n+        self.erosion_kernel = None\n+        self.erosion_kernel_size = erosion_kernel_size\n+        if erosion_kernel_size is not None:\n+            self.erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(abs(erosion_kernel_size),abs(erosion_kernel_size)))\n+        self.blur_size = blur_size\n+        self.seamless_clone = seamless_clone\n+        self.sharpen_image = sharpen_image\n+        self.match_histogram = match_histogram\n+        self.mask_type = mask_type.lower() # Choose in 'FaceHullAndRect','FaceHull','Rect'\n+        self.double_pass = double_pass\n+\n+    def patch_image( self, image, face_detected, size ):\n+        mat = numpy.array(get_align_mat(face_detected, size, should_align_eyes=False)).reshape(2,3)\n+\n+        global n\n+        n+=1\n+        size = 64\n+        image_size = image.shape[1], image.shape[0]\n+\n+        sourceMat = mat.copy()\n+\n+        sourceMat = sourceMat*(240+(16*2))\n+        sourceMat[:,2] += 48\n+\n+        face = cv2.warpAffine( image, sourceMat, (240+(48+16)*2,240+(48+16)*2) )\n+        \n+        sourceFace = face.copy()\n+        sourceFace = cv2.resize(sourceFace,(128,128),cv2.INTER_CUBIC)\n+        \n+\n+        face = cv2.resize(face,(64,64),cv2.INTER_AREA)\n+        face = numpy.expand_dims( face, 0 )\n+\n+        new_face_rgb,new_face_m = self.encoder_A( face / 255.0 )\n+\n+        if self.double_pass:\n+            #feed the original prediction back into the network for a second round.\n+            new_face_rgb = new_face_rgb.reshape((128, 128, 3))\n+            new_face_rgb = cv2.resize( new_face_rgb , (64,64))\n+            new_face_rgb = numpy.expand_dims( new_face_rgb, 0 )\n+            new_face_rgb,_ = self.encoder_A( new_face_rgb )\n+\n+        _,other_face_m = self.encoder_B( face / 255.0 ) # NOTE: we get the image from other autoencoder!\n+\n+        new_face_m = numpy.maximum(new_face_m, other_face_m )\n+\n+\n+        new_face_rgb = numpy.clip( new_face_rgb[0] * 255, 0, 255 ).astype( image.dtype )\n+        new_face_m   = numpy.clip( new_face_m[0]  , 0, 1 ).astype( float ) * numpy.ones((new_face_m.shape[0],new_face_m.shape[1],3))\n+\n+\n+        base_image = numpy.copy( image )\n+        new_image = numpy.copy( image ) \n+\n+        transmat =  mat * (64-16) *2\n+        transmat[::,2] += 8*2\n+\n+        self.adjust_avg_color(sourceFace,new_face_rgb)\n+\n+        cv2.warpAffine( new_face_rgb, transmat, image_size, new_image, cv2.WARP_INVERSE_MAP | cv2.INTER_CUBIC, cv2.BORDER_TRANSPARENT )\n+        \n+\n+        image_mask = numpy.zeros_like(new_image, dtype=float)\n+\n+\n+        cv2.warpAffine( new_face_m, transmat, image_size, image_mask, cv2.WARP_INVERSE_MAP | cv2.INTER_CUBIC, cv2.BORDER_TRANSPARENT )\n+        \n+\n+        #slightly enlarge the mask area\n+        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n+        image_mask = cv2.dilate(image_mask,kernel,iterations = 1)\n+\n+        if self.seamless_clone:\n+\n+            unitMask = numpy.clip( image_mask * 365, 0, 255 ).astype(numpy.uint8)\n+\n+            maxregion = numpy.argwhere(unitMask==255)\n+\n+            if maxregion.size > 0:\n+\n+                miny,minx = maxregion.min(axis=0)[:2]\n+                maxy,maxx = maxregion.max(axis=0)[:2]\n+                lenx = maxx - minx;\n+                leny = maxy - miny;\n+                masky = int(minx+(lenx//2))\n+                maskx = int(miny+(leny//2))\n+\n+                new_image = cv2.seamlessClone(new_image.astype(numpy.uint8),base_image.astype(numpy.uint8),unitMask,(masky,maskx) , cv2.NORMAL_CLONE )\n+\n+\n+        image_mask = cv2.GaussianBlur(image_mask,(11,11),0)\n+\n+        foreground = cv2.multiply(image_mask, new_image.astype(float))\n+        background = cv2.multiply(1.0 - image_mask, base_image.astype(float))\n+        \n+        output = numpy.add(background,foreground)\n+\n+        # cv2.imshow(\"output\", output.astype(numpy.uint8) )\n+\n+        # if cv2.waitKey(1)==ord('q'):\n+        #   exit()\n+        return output\n+\n+    def adjust_avg_color(self, img_old,img_new):\n+        w,h,c = img_new.shape\n+        for i in range(img_new.shape[-1]):\n+            old_avg = img_old[:, :, i].mean()\n+            new_avg = img_new[:, :, i].mean()\n+            diff_int = (int)(old_avg - new_avg)\n+            for m in range(img_new.shape[0]):\n+                for n in range(img_new.shape[1]):\n+                    temp = (img_new[m,n,i] + diff_int)\n+                    if temp < 0:\n+                        img_new[m,n,i] = 0\n+                    elif temp > 255:\n+                        img_new[m,n,i] = 255\n+                    else:\n+                        img_new[m,n,i] = temp\n+\n+    # def transfer_avg_color(self, img_old,img_new):\n+    #     assert(img_old.shape==img_new.shape) \n+    #     source = cv2.cvtColor(img_old, cv2.COLOR_BGR2LAB).astype(\"float32\")\n+    #     target = cv2.cvtColor(img_new, cv2.COLOR_BGR2LAB).astype(\"float32\")\n+\n+    #     (lMeanSrc, lStdSrc, aMeanSrc, aStdSrc, bMeanSrc, bStdSrc) = self.image_stats(source)\n+    #     (lMeanTar, lStdTar, aMeanTar, aStdTar, bMeanTar, bStdTar) = self.image_stats(target)\n+\n+    #     (l, a, b) = cv2.split(target)\n+\n+    #     l -= lMeanTar\n+    #     a -= aMeanTar\n+    #     b -= bMeanTar\n+\n+    #     l = (lStdTar / lStdSrc) * l\n+    #     a = (aStdTar / aStdSrc) * a\n+    #     b = (bStdTar / bStdSrc) * b\n+\n+    #     l += lMeanSrc\n+    #     a += aMeanSrc\n+    #     b += bMeanSrc\n+\n+    #     l = numpy.clip(l, 0, 255)\n+    #     a = numpy.clip(a, 0, 255)\n+    #     b = numpy.clip(b, 0, 255)\n+\n+    #     transfer = cv2.merge([l, a, b])\n+    #     transfer = cv2.cvtColor(transfer.astype(\"uint8\"), cv2.COLOR_LAB2BGR)\n+\n+    #     return transfer\n+\n+    # def image_stats(self, image):\n+    #     (l, a, b) = cv2.split(image)\n+    #     (lMean, lStd) = (l.mean(), l.std())\n+    #     (aMean, aStd) = (a.mean(), a.std())\n+    #     (bMean, bStd) = (b.mean(), b.std())\n+\n+    #     return (lMean, lStd, aMean, aStd, bMean, bStd)\n+\n+# def main( args ):\n+\n+\n+#     input_dir = Path( args.input_dir )\n+#     assert input_dir.is_dir()\n+\n+#     alignments = input_dir / args.alignments\n+#     with alignments.open() as f:\n+#         alignments = json.load(f)\n+\n+#     output_dir = input_dir / args.output_dir\n+#     output_dir.mkdir( parents=True, exist_ok=True )\n+\n+#     args.direction = 'AtoB'\n+#     if args.direction == 'AtoB': autoencoder,otherautoencoder = autoencoder_B,autoencoder_A\n+#     if args.direction == 'BtoA': autoencoder,otherautoencoder = autoencoder_A,autoencoder_B\n+    \n+#     if args.blurSize % 2 == 0:\n+#       args.blurSize+=1\n+\n+#     if args.erosionKernelSize>0:\n+#       erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(args.erosionKernelSize,args.erosionKernelSize))\n+#     else:\n+#       erosion_kernel = None\n+\n+#     for e in alignments:    \n+#       if len(e)<4:\n+#         raise LookupError('This script expects new format json files with face points included.')\n+\n+\n+#     for image_file, face_file, mat,facepoints in tqdm( alignments[args.startframe::args.frameSkip] ):\n+#         image = cv2.imread( str( input_dir / image_file ) )\n+#         face  = cv2.imread( str( input_dir / face_file  ) )\n+\n+\n+#         mat = numpy.array(mat).reshape(2,3)\n+\n+#         if image is None: continue\n+#         if face  is None: continue\n+\n+\n+#         new_image = convert_one_image( autoencoder, otherautoencoder, image, mat, facepoints, erosion_kernel, args.blurSize, args.seamlessClone, args.maskType, args.doublePass)\n+\n+#         output_file = output_dir / Path(image_file).name\n+#         cv2.imwrite( str(output_file), new_image )\n+\n+# def str2bool(v):\n+#   if v.lower() in ('yes', 'true', 't', 'y', '1'):\n+#     return True\n+#   elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n+#     return False\n+#   else:\n+#     raise argparse.ArgumentTypeError('Boolean value expected.')\n+\n+# if __name__ == '__main__':\n+#     parser = argparse.ArgumentParser()\n+#     parser.add_argument( \"input_dir\", type=str, nargs='?' )\n+#     parser.add_argument( \"alignments\", type=str, nargs='?', default='alignments.json' )\n+#     parser.add_argument( \"output_dir\", type=str, nargs='?', default='merged' )\n+\n+#     parser.add_argument(\"--seamlessClone\", type=str2bool, nargs='?', const=False, default='False', help=\"Attempt to use opencv seamlessClone.\")\n+\n+#     parser.add_argument(\"--doublePass\", type=str2bool, nargs='?', const=False, default='False', help=\"Pass the original prediction output back through for a second pass.\")\n+\n+\n+#     parser.add_argument('--maskType', type=str, default='FaceHullAndRect' ,choices=['FaceHullAndRect','FaceHull','Rect'], help=\"The type of masking to use around the face.\")\n+\n+#     parser.add_argument( \"--startframe\",        type=int, default='0' )\n+#     parser.add_argument( \"--frameSkip\",        type=int, default='1' )\n+#     parser.add_argument( \"--blurSize\",          type=int, default='4' )\n+#     parser.add_argument( \"--erosionKernelSize\", type=int, default='2' )\n+#     parser.add_argument( \"--direction\",         type=str, default=\"AtoB\", choices=[\"AtoB\", \"BtoA\"])\n+#     main( parser.parse_args() )\n+\ndiff --git a/plugins/Model_DFaker/Model.py b/plugins/Model_DFaker/Model.py\nnew file mode 100644\nindex 0000000000..c24f92c581\n--- /dev/null\n+++ b/plugins/Model_DFaker/Model.py\n@@ -0,0 +1,173 @@\n+from keras.models import Model as KerasModel\n+from keras.layers import Input, Dense, Flatten, Reshape, Dropout, Add,Concatenate, Lambda\n+from keras.layers.advanced_activations import LeakyReLU\n+from keras.layers.convolutional import Conv2D\n+from keras.initializers import RandomNormal\n+from keras.optimizers import Adam\n+\n+from .AutoEncoder import AutoEncoder\n+from lib.PixelShuffler import PixelShuffler\n+\n+import tensorflow as tf\n+\n+from keras_contrib.losses import DSSIMObjective\n+from keras import losses\n+\n+from keras.utils import multi_gpu_model\n+import numpy\n+\n+class penalized_loss(object):\n+\n+  def __init__(self,mask,lossFunc,maskProp= 1.0):\n+    self.mask = mask\n+    self.lossFunc=lossFunc\n+    self.maskProp = maskProp\n+    self.maskaskinvProp = 1-maskProp  \n+\n+  def __call__(self,y_true, y_pred):\n+\n+    tro, tgo, tbo = tf.split(y_true,3, 3 )\n+    pro, pgo, pbo = tf.split(y_pred,3, 3 )\n+\n+    tr = tro\n+    tg = tgo\n+    tb = tbo\n+\n+    pr = pro\n+    pg = pgo\n+    pb = pbo\n+    m  = self.mask \n+\n+    m   = m*self.maskProp\n+    m  += self.maskaskinvProp\n+    tr *= m\n+    tg *= m\n+    tb *= m\n+\n+    pr *= m\n+    pg *= m\n+    pb *= m\n+\n+\n+    y = tf.concat([tr, tg, tb],3)\n+    p = tf.concat([pr, pg, pb],3)\n+\n+    #yo = tf.stack([tro,tgo,tbo],3)\n+    #po = tf.stack([pro,pgo,pbo],3)\n+\n+    return self.lossFunc(y,p)\n+\n+IMAGE_SHAPE = (64,64,3)\n+\n+ENCODER_DIM = 1024\n+conv_init = RandomNormal(0, 0.02)\n+gamma_init = RandomNormal(1., 0.02)\n+\n+def __conv_init(a):\n+    print(\"conv_init\", a)\n+    k = RandomNormal(0, 0.02)(a) # for convolution kernel\n+    k.conv_weight = True    \n+    return k\n+\n+class Model(AutoEncoder):\n+    def initModel(self):\n+        optimizer = Adam( lr=5e-5, beta_1=0.5, beta_2=0.999 )\n+\n+        # print(self.encoder.summary()) \n+        # print(self.decoder_A.summary())\n+        # print(self.decoder_B.summary())\n+\n+        x1 = Input( shape=IMAGE_SHAPE )\n+        x2 = Input( shape=IMAGE_SHAPE )\n+        m1 = Input( shape=(64*2,64*2,1) )\n+        m2 = Input( shape=(64*2,64*2,1) )\n+\n+        self.autoencoder_A = KerasModel( [x1,m1], self.decoder_A( self.encoder(x1) ) )\n+        self.autoencoder_B = KerasModel( [x2,m2], self.decoder_B( self.encoder(x2) ) )\n+\n+        # print(self.autoencoder_A.summary())\n+        # print(self.autoencoder_B.summary())\n+\n+        if self.gpus > 1:\n+            self.autoencoder_A = multi_gpu_model( self.autoencoder_A ,self.gpus)\n+            self.autoencoder_B = multi_gpu_model( self.autoencoder_B ,self.gpus)\n+\n+        # o1,om1  = self.decoder_A( self.encoder(x1))\n+        # o2,om2  = self.decoder_B( self.encoder(x2))\n+\n+        DSSIM = DSSIMObjective()\n+        self.autoencoder_A.compile( optimizer=optimizer, loss=[ penalized_loss(m1, DSSIM),'mse'] )\n+        self.autoencoder_B.compile( optimizer=optimizer, loss=[ penalized_loss(m2, DSSIM),'mse'] )\n+\n+    def converter(self, swap):\n+        zmask = numpy.zeros((1,128, 128,1),float)\n+        autoencoder = self.autoencoder_B if not swap else self.autoencoder_A \n+        return lambda img: autoencoder.predict([img, zmask])\n+\n+    # def upscale_ps(self, filters, use_norm=True):\n+    #     def block(x):\n+    #         x = Conv2D(filters*4, kernel_size=3, use_bias=False, kernel_initializer=RandomNormal(0, 0.02), padding='same' )(x)\n+    #         x = LeakyReLU(0.1)(x)\n+    #         x = PixelShuffler()(x)\n+    #         return x\n+    #     return block\n+\n+    def res_block(self, input_tensor, f):\n+        x = input_tensor\n+        x = Conv2D(f, kernel_size=3, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(x)\n+        x = LeakyReLU(alpha=0.2)(x)\n+        x = Conv2D(f, kernel_size=3, kernel_initializer=conv_init, use_bias=False, padding=\"same\")(x)\n+        x = Add()([x, input_tensor])\n+        x = LeakyReLU(alpha=0.2)(x)\n+        return x\n+\n+    def conv( self, filters ):\n+        def block(x):\n+            x = Conv2D( filters, kernel_size=5, strides=2, padding='same' )(x)\n+            x = LeakyReLU(0.1)(x)\n+            return x\n+        return block\n+\n+    def upscale( self, filters ):\n+        def block(x):\n+            x = Conv2D( filters*4, kernel_size=3, padding='same' )(x)\n+            x = LeakyReLU(0.1)(x)\n+            x = PixelShuffler()(x)\n+            return x\n+        return block\n+\n+    def Encoder(self):\n+        input_ = Input( shape=IMAGE_SHAPE )\n+        x = self.conv( 128)(input_)\n+        x = self.conv( 256)(x)\n+        x = self.conv( 512)(x)\n+\n+        x = self.conv(1024)(x)\n+        x = Dense( ENCODER_DIM )( Flatten()(x) )\n+        x = Dense(4*4*1024)(x)\n+        x = Reshape((4,4,1024))(x)\n+        x = self.upscale(512)(x)\n+        return KerasModel( input_, [x] )\n+\n+    def Decoder(self, name):\n+        input_ = Input( shape=(8,8,512) )\n+        #skip_in = Input( shape=(8,8,512) )\n+\n+        x = input_\n+        x = self.upscale(512)(x)\n+        x = self.res_block(x, 512)\n+        x = self.upscale(256)(x)\n+        x = self.res_block(x, 256)\n+        x = self.upscale(128)(x)\n+        x = self.res_block(x, 128)\n+        x = self.upscale(64)(x)\n+        x = Conv2D( 3, kernel_size=5, padding='same', activation='sigmoid' )(x)\n+\n+        y = input_\n+        y = self.upscale(512)(y)\n+        y = self.upscale(256)(y)\n+        y = self.upscale(128)(y)\n+        y = self.upscale(64)(y)\n+        y = Conv2D( 1, kernel_size=5, padding='same', activation='sigmoid' )(y)\n+\n+        return KerasModel( [input_], outputs=[x,y] )\ndiff --git a/plugins/Model_DFaker/Trainer.py b/plugins/Model_DFaker/Trainer.py\nnew file mode 100644\nindex 0000000000..ea5927eedf\n--- /dev/null\n+++ b/plugins/Model_DFaker/Trainer.py\n@@ -0,0 +1,143 @@\n+import cv2\r\n+import time\r\n+import numpy\r\n+import random\r\n+\r\n+from lib.utils import BackgroundGenerator\r\n+from lib.training_data import TrainingDataGenerator, stack_images\r\n+from .image_augmentation import random_transform, random_warp_src_dest\r\n+from .utils import load_images_aligned\r\n+\r\n+class DFTrainingDataGenerator(TrainingDataGenerator):\r\n+    def __init__(self, random_transform_args, coverage):\r\n+        super().__init__(random_transform_args, coverage)\r\n+\r\n+    def minibatchAB(self, images, srcPoints, dstPoints, batch_size ):\r\n+        batch = BackgroundGenerator(self.minibatch(images, srcPoints, dstPoints, batch_size), 1)\r\n+        return batch.iterator()\r\n+\r\n+    def minibatch(self, images, srcPoints, dstPoints, batch_size):\r\n+        epoch = 0\r\n+        while True:\r\n+            epoch+=1\r\n+            indices = numpy.random.choice(range(0,images.shape[0]),size=batch_size,replace=False)\r\n+            for i,index in enumerate(indices):\r\n+                image = images[index]\r\n+                image = random_transform( image, **self.random_transform_args )\r\n+\r\n+                closest = ( numpy.mean(numpy.square(srcPoints[index]-dstPoints),axis=(1,2)) ).argsort()[:10]\r\n+                closest = numpy.random.choice(closest)\r\n+                warped_img, target_img, mask_image = random_warp_src_dest( image,srcPoints[index],dstPoints[ closest ] )\r\n+        \r\n+                if numpy.random.random() < 0.5:\r\n+                    warped_img = warped_img[:,::-1]\r\n+                    target_img = target_img[:,::-1]\r\n+                    mask_image = mask_image[:,::-1]\r\n+\r\n+                if i == 0:\r\n+                    warped_images = numpy.empty( (batch_size,) + warped_img.shape, warped_img.dtype )\r\n+                    target_images = numpy.empty( (batch_size,) + target_img.shape, warped_img.dtype )\r\n+                    mask_images = numpy.empty( (batch_size,)   + mask_image.shape, mask_image.dtype )\r\n+\r\n+                warped_images[i] = warped_img\r\n+                target_images[i] = target_img\r\n+                mask_images[i]   = mask_image\r\n+\r\n+            yield epoch, warped_images, target_images, mask_images\r\n+\r\n+class Trainer():\r\n+    random_transform_args = {\r\n+        'rotation_range': 10,\r\n+        'zoom_range': 0.05,\r\n+        'shift_range': 0.05,\r\n+        'random_flip': 0.0,\r\n+        }\r\n+\r\n+    def __init__(self, model, fn_A, fn_B, batch_size, *args):\r\n+        self.batch_size = batch_size\r\n+        self.model = model\r\n+\r\n+        generator = DFTrainingDataGenerator(self.random_transform_args, 160)\r\n+        \r\n+        minImages = 2000#min(len(fn_A),len(fn_B))*20\r\n+\r\n+        random.shuffle(fn_A)\r\n+        random.shuffle(fn_B)\r\n+\r\n+        #NOTE this loads all images so it may be memory intensive! (cumber of images is maxed to 'minImages')\r\n+        images_A, landmarks_A = load_images_aligned(fn_A[:minImages])\r\n+        images_B, landmarks_B = load_images_aligned(fn_B[:minImages])\r\n+\r\n+        images_A = images_A/255.0\r\n+        images_B = images_B/255.0\r\n+\r\n+        images_A[:,:,:3] += images_B[:,:,:3].mean( axis=(0,1,2) ) - images_A[:,:,:3].mean( axis=(0,1,2) )\r\n+\r\n+        self.images_A = generator.minibatchAB(images_A, landmarks_A, landmarks_B, self.batch_size)\r\n+        self.images_B = generator.minibatchAB(images_B, landmarks_B, landmarks_A, self.batch_size)\r\n+\r\n+    def train_one_step(self, iter, viewer):\r\n+        epoch, warped_A, target_A, mask_A = next(self.images_A)\r\n+        epoch, warped_B, target_B, mask_B = next(self.images_B)\r\n+      \r\n+        #omask = numpy.ones((target_A.shape[0],64,64,1),float)\r\n+\r\n+        loss_A = self.model.autoencoder_A.train_on_batch([warped_A,mask_A], [target_A,mask_A])\r\n+        loss_B = self.model.autoencoder_B.train_on_batch([warped_B,mask_B], [target_B,mask_B])\r\n+\r\n+        print(\"Loss A [{}] Loss B [{}]\".format(loss_A,loss_B),\r\n+            end='\\r')\r\n+\r\n+        if viewer is not None:\r\n+            viewer(self.show_sample(target_A[0:8,:,:,:3], target_B[0:8,:,:,:3]), \"training\")\r\n+            viewer(self.show_warped(warped_A[:6],warped_B[:6]), \"warped\")\r\n+\r\n+    def show_warped(self, warped_A, warped_B):\r\n+        figWarped = numpy.stack([warped_A, warped_B],axis=0 )\r\n+        figWarped = numpy.clip( figWarped * 255, 0, 255 ).astype('uint8')\r\n+        figWarped = stack_images( figWarped )\r\n+        return figWarped\r\n+\r\n+    def show_sample(self, test_A, test_B):\r\n+        test_A_i = []\r\n+        test_B_i = []\r\n+        \r\n+        for i in test_A:\r\n+            test_A_i.append(cv2.resize(i,(64,64),cv2.INTER_AREA))\r\n+        test_A_i = numpy.array(test_A_i).reshape((-1,64,64,3))\r\n+\r\n+        for i in test_B:\r\n+            test_B_i.append(cv2.resize(i,(64,64),cv2.INTER_AREA))\r\n+        test_B_i = numpy.array(test_B_i).reshape((-1,64,64,3))\r\n+        \r\n+        zmask = numpy.zeros((test_A.shape[0],128,128,1),float)\r\n+\r\n+        pred_a_a,pred_a_a_m = self.model.autoencoder_A.predict([test_A_i,zmask])\r\n+        pred_b_a,pred_b_a_m = self.model.autoencoder_B.predict([test_A_i,zmask])\r\n+\r\n+        pred_a_b,pred_a_b_m = self.model.autoencoder_A.predict([test_B_i,zmask])\r\n+        pred_b_b,pred_b_b_m = self.model.autoencoder_B.predict([test_B_i,zmask])\r\n+\r\n+        pred_a_a = pred_a_a[0:18,:,:,:3]\r\n+        pred_a_b = pred_a_b[0:18,:,:,:3]\r\n+        pred_b_a = pred_b_a[0:18,:,:,:3]\r\n+        pred_b_b = pred_b_b[0:18,:,:,:3]\r\n+\r\n+        figure_A = numpy.stack([\r\n+            test_A,\r\n+            pred_a_a,\r\n+            pred_b_a,\r\n+            ], axis=1 )\r\n+        figure_B = numpy.stack([\r\n+            test_B,\r\n+            pred_b_b,\r\n+            pred_a_b,\r\n+            ], axis=1 )\r\n+\r\n+\r\n+        figure = numpy.concatenate( [ figure_A, figure_B ], axis=0 )\r\n+        figure = figure.reshape( (4,4) + figure.shape[1:] )\r\n+        figure = stack_images( figure )\r\n+\r\n+        return numpy.clip( figure * 255, 0, 255 ).astype('uint8')\r\n+\r\ndiff --git a/plugins/Model_DFaker/image_augmentation.py b/plugins/Model_DFaker/image_augmentation.py\nnew file mode 100644\nindex 0000000000..636ce65876\n--- /dev/null\n+++ b/plugins/Model_DFaker/image_augmentation.py\n@@ -0,0 +1,106 @@\n+import cv2\n+import numpy\n+\n+from lib.umeyama import umeyama\n+\n+def random_transform( image, rotation_range, zoom_range, shift_range, random_flip ):\n+    h,w = image.shape[0:2]\n+    rotation = numpy.random.uniform( -rotation_range, rotation_range )\n+    scale = numpy.random.uniform( 1 - zoom_range, 1 + zoom_range )\n+    tx = numpy.random.uniform( -shift_range, shift_range ) * w\n+    ty = numpy.random.uniform( -shift_range, shift_range ) * h\n+    mat = cv2.getRotationMatrix2D( (w//2,h//2), rotation, scale )\n+    mat[:,2] += (tx,ty)\n+    result = cv2.warpAffine( image, mat, (w,h), borderMode=cv2.BORDER_REPLICATE )\n+    if numpy.random.random() < random_flip:\n+        result = result[:,::-1]\n+    return result\n+\n+maskColour = numpy.array([127,127,127])/255.0\n+\n+\n+from scipy.interpolate import griddata\n+\n+\n+grid_x, grid_y = numpy.mgrid[0:255:256j, 0:255:256j]\n+edgeAnchors = [ (0,0), (0,255), (255,255), (255,0), (127,0), (127,255), (255,127), (0,127) ]\n+\n+def random_warp_src_dest( in_image,srcPoints,destPoints ):\n+\n+  source = srcPoints\n+  destination = (destPoints.copy().astype('float')+numpy.random.normal( size=(destPoints.shape), scale= 2 ))\n+  destination = destination.astype('uint8')\n+\n+  faceCore =cv2.convexHull( numpy.concatenate([source[17:],destination[17:]], axis=0).astype(int) )\n+\n+  source = [(y,x) for x,y in source]+edgeAnchors\n+  destination = [(y,x) for x,y in destination]+edgeAnchors\n+\n+\n+  indeciesToRemove = set()\n+  for fpl in source,destination:\n+    for i,(y,x) in enumerate(fpl):\n+      if i>17:\n+        break\n+      elif cv2.pointPolygonTest(faceCore,(y,x),False) >= 0:\n+        indeciesToRemove.add(i)\n+\n+  for i in sorted(indeciesToRemove,reverse=True):\n+    source.pop(i)\n+    destination.pop(i)\n+\n+  grid_z = griddata( destination,source, (grid_x, grid_y), method='linear')\n+  map_x = numpy.append([], [ar[:,1] for ar in grid_z]).reshape(256,256)\n+  map_y = numpy.append([], [ar[:,0] for ar in grid_z]).reshape(256,256)\n+  map_x_32 = map_x.astype('float32')\n+  map_y_32 = map_y.astype('float32')\n+\n+  warped = cv2.remap(in_image[:,:,:3], map_x_32, map_y_32, cv2.INTER_LINEAR,cv2.BORDER_TRANSPARENT )\n+\n+  target_mask = in_image[:,:,3].reshape((256,256,1))\n+  target_image = in_image[:,:,:3]\n+\n+  warped       = cv2.resize( warped[ 128-120:128+120,128-120:128+120,:]         ,(64,64),cv2.INTER_AREA)\n+  target_image = cv2.resize( target_image[ 128-120:128+120,128-120:128+120,: ]  ,(64*2,64*2),cv2.INTER_AREA)\n+  target_mask  = cv2.resize( target_mask[ 128-120:128+120,128-120:128+120,: ]   ,(64*2,64*2),cv2.INTER_AREA).reshape((64*2,64*2,1))\n+\n+  return warped,target_image,target_mask\n+\n+# # get pair of random warped images from aligened face image\n+# def random_warp( in_image ):\n+#     assert in_image.shape[:2] == (256,256)\n+\n+#     image = in_image.copy()\n+\n+\n+#     scale = 5\n+\n+#     range_ = numpy.linspace( 128-120, 128+120, scale )\n+#     mapx = numpy.broadcast_to( range_, (scale,scale) )\n+#     mapy = mapx.T\n+\n+#     mapx = mapx + numpy.random.normal( size=(scale,scale), scale= 6 )\n+#     mapy = mapy + numpy.random.normal( size=(scale,scale), scale= 6 )\n+\n+#     interp_mapx = cv2.resize( mapx, (80,80) )[8:72,8:72].astype('float32')\n+#     interp_mapy = cv2.resize( mapy, (80,80) )[8:72,8:72].astype('float32')\n+\n+#     warped_image = cv2.remap( image[:,:,:3], interp_mapx, interp_mapy, cv2.INTER_CUBIC )\n+\n+#     src_points = numpy.stack( [ mapx.ravel(), mapy.ravel() ], axis=-1 )\n+#     dst_points = numpy.mgrid[0:65:16,0:65:16].T.reshape(-1,2)\n+#     mat = umeyama( src_points, dst_points, True )[0:2]\n+\n+#     target_image = cv2.warpAffine( image, mat, (64,64) )\n+\n+#     target_mask = target_image[:,:,3].reshape((64,64,1))\n+#     target_image = target_image[:,:,:3]\n+\n+\n+#     if len(target_image.shape)>2:\n+#       return ( warped_image, \n+#                target_image, \n+#                target_mask )\n+#     else:\n+#       return ( warped_image, \n+#                target_image )\n\\ No newline at end of file\ndiff --git a/plugins/Model_DFaker/utils.py b/plugins/Model_DFaker/utils.py\nnew file mode 100644\nindex 0000000000..016c234ab0\n--- /dev/null\n+++ b/plugins/Model_DFaker/utils.py\n@@ -0,0 +1,62 @@\n+import cv2\n+import numpy\n+import os\n+import json\n+\n+from lib.faces_detect import DetectedFace\n+\n+from tqdm import tqdm\n+from lib.aligner import get_align_mat\n+#guideShades = numpy.linspace(20,250,68)\n+\n+def load_images_aligned(image_paths):\n+  basePath = os.path.split(image_paths[0])[0]\n+  alignments = os.path.join(basePath,'alignments.json')\n+  alignments = json.loads( open(alignments).read() )\n+\n+  all_images    = []\n+  landmarks     = []\n+\n+\n+  pbar = tqdm(alignments)\n+  for original in pbar:\n+    faces = alignments[original]\n+    for rawface in faces:\n+        detected_face = DetectedFace(**rawface)\n+        cropped = detected_face.cropped\n+        points = detected_face.landmarksXY\n+        mat = get_align_mat( detected_face, 0, False )\n+        if os.path.exists(cropped):\n+            image, facepoints = load_image( cropped, mat, points )\n+            all_images.append( image  )\n+            landmarks.append( facepoints )\n+\n+  return numpy.array(all_images),numpy.array(landmarks)\n+\n+def load_image(cropped, mat, points):\n+    cropped = cv2.imread(cropped).astype(float)\n+\n+    mat = numpy.array(mat).reshape(2,3)\n+    points = numpy.array(points).reshape((-1,2))\n+\n+    mat = mat*160\n+    mat[:,2] += 42\n+\n+    facepoints = numpy.array( points ).reshape((-1,2))\n+\n+    mask = numpy.zeros_like(cropped,dtype=numpy.uint8)\n+\n+    hull = cv2.convexHull( facepoints.astype(int) )\n+    hull = cv2.transform( hull.reshape(1,-1,2) , mat).reshape(-1,2).astype(int)\n+\n+    cv2.fillConvexPoly( mask,hull,(255,255,255) )\n+\n+    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(15,15))\n+\n+    mask = cv2.dilate(mask,kernel,iterations = 1,borderType=cv2.BORDER_REFLECT )\n+\n+    facepoints = cv2.transform( numpy.array( points ).reshape((1,-1,2)) , mat).reshape(-1,2).astype(int)\n+\n+    mask = mask[:,:,0]\n+\n+    return numpy.dstack([cropped,mask]).astype(numpy.uint8), facepoints\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "You're running into multiple issues while training. First, the `ModuleNotFoundError` for `keras_contrib` suggests a missing dependency. Ensure that this package is installed as it's critical. The syntax error in `Trainer.py` indicates a missing colon in the `show_warped` method; fix it to resolve that issue. \n\nRegarding the OOM errors during extraction, it seems that the resize code might have been removed, leading to a lack of memory management compared to the master branch. Lastly, check why the alignment files are not found; it seems essential for training."
      }
    ],
    "codediff_length": 28142
  },
  {
    "index": 71,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/plugins/ConvertVideo_BoundingBox.py b/plugins/ConvertVideo_BoundingBox.py\nnew file mode 100644\nindex 0000000000..1b014563d6\n--- /dev/null\n+++ b/plugins/ConvertVideo_BoundingBox.py\n@@ -0,0 +1,133 @@\n+# Based on the https://github.com/shaoanlu/faceswap-GAN repo (master/FaceSwap_GAN_v2_train.ipynb)\r\n+\r\n+# # 12. Make video clips\r\n+# \r\n+# Given a video as input, the following cells will detect face for each frame using dlib's cnn model. And use trained GAN model to transform detected face into target face. Then output a video with swapped faces.\r\n+\r\n+# Download ffmpeg if need, which is required by moviepy.\r\n+\r\n+#import imageio\r\n+#imageio.plugins.ffmpeg.download()\r\n+\r\n+# # 13. Make video clips w/o face alignment\r\n+\r\n+\r\n+import face_recognition\r\n+from moviepy.editor import VideoFileClip\r\n+\r\n+use_smoothed_mask = True\r\n+use_smoothed_bbox = True\r\n+\r\n+def get_smoothed_coord(x0, x1, y0, y1):\r\n+    global prev_x0, prev_x1, prev_y0, prev_y1\r\n+    x0 = int(0.65*prev_x0 + 0.35*x0)\r\n+    x1 = int(0.65*prev_x1 + 0.35*x1)\r\n+    y1 = int(0.65*prev_y1 + 0.35*y1)\r\n+    y0 = int(0.65*prev_y0 + 0.35*y0)\r\n+    return x0, x1, y0, y1    \r\n+    \r\n+def set_global_coord(x0, x1, y0, y1):\r\n+    global prev_x0, prev_x1, prev_y0, prev_y1\r\n+    prev_x0 = x0\r\n+    prev_x1 = x1\r\n+    prev_y1 = y1\r\n+    prev_y0 = y0\r\n+\r\n+def process_video(input_img):   \r\n+    # modify this line to reduce input size\r\n+    #input_img = input_img[:, input_img.shape[1]//3:2*input_img.shape[1]//3,:] \r\n+    image = input_img\r\n+    faces = face_recognition.face_locations(image, model=\"cnn\")\r\n+    \r\n+    if len(faces) == 0:\r\n+        comb_img = np.zeros([input_img.shape[0], input_img.shape[1]*2,input_img.shape[2]])\r\n+        comb_img[:, :input_img.shape[1], :] = input_img\r\n+        comb_img[:, input_img.shape[1]:, :] = input_img\r\n+        triple_img = np.zeros([input_img.shape[0], input_img.shape[1]*3,input_img.shape[2]])\r\n+        triple_img[:, :input_img.shape[1], :] = input_img\r\n+        triple_img[:, input_img.shape[1]:input_img.shape[1]*2, :] = input_img      \r\n+        triple_img[:, input_img.shape[1]*2:, :] = (input_img * .15).astype('uint8')\r\n+    \r\n+    mask_map = np.zeros_like(image)\r\n+    \r\n+    global prev_x0, prev_x1, prev_y0, prev_y1\r\n+    global frames    \r\n+    for (x0, y1, x1, y0) in faces:\r\n+        h = x1 - x0\r\n+        w = y1 - y0\r\n+        \r\n+        # smoothing bounding box\r\n+        if use_smoothed_bbox:\r\n+            if frames != 0:\r\n+                x0, x1, y0, y1 = get_smoothed_coord(x0, x1, y0, y1)\r\n+                set_global_coord(x0, x1, y0, y1)\r\n+            else:\r\n+                set_global_coord(x0, x1, y0, y1)\r\n+                frames += 1\r\n+            \r\n+        cv2_img = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n+        roi_image = cv2_img[x0+h//15:x1-h//15,y0+w//15:y1-w//15,:]\r\n+        roi_size = roi_image.shape  \r\n+        \r\n+        # smoothing mask\r\n+        if use_smoothed_mask:\r\n+            mask = np.zeros_like(roi_image)\r\n+            mask[h//15:-h//15,w//15:-w//15,:] = 255\r\n+            mask = cv2.GaussianBlur(mask,(15,15),10)\r\n+            orig_img = cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB)\r\n+        \r\n+        ae_input = cv2.resize(roi_image, (64,64))/255. * 2 - 1        \r\n+        result = np.squeeze(np.array([path_abgr_A([[ae_input]])])) # Change path_A/path_B here\r\n+        result_a = result[:,:,0] * 255\r\n+        result_bgr = np.clip( (result[:,:,1:] + 1) * 255 / 2, 0, 255 )\r\n+        result_a = cv2.GaussianBlur(result_a ,(7,7),6)\r\n+        result_a = np.expand_dims(result_a, axis=2)\r\n+        result = (result_a/255 * result_bgr + (1 - result_a/255) * ((ae_input + 1) * 255 / 2)).astype('uint8')\r\n+        result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\r\n+        \r\n+        mask_map[x0+h//15:x1-h//15, y0+w//15:y1-w//15,:] = np.expand_dims(cv2.resize(result_a, (roi_size[1],roi_size[0])), axis=2)\r\n+        mask_map = np.clip(mask_map + .15 * input_img, 0, 255 )\r\n+        \r\n+        result = cv2.resize(result, (roi_size[1],roi_size[0]))\r\n+        comb_img = np.zeros([input_img.shape[0], input_img.shape[1]*2,input_img.shape[2]])\r\n+        comb_img[:, :input_img.shape[1], :] = input_img\r\n+        comb_img[:, input_img.shape[1]:, :] = input_img\r\n+        \r\n+        if use_smoothed_mask:\r\n+            comb_img[x0+h//15:x1-h//15, input_img.shape[1]+y0+w//15:input_img.shape[1]+y1-w//15,:] = mask/255*result + (1-mask/255)*orig_img\r\n+        else:\r\n+            comb_img[x0+h//15:x1-h//15, input_img.shape[1]+y0+w//15:input_img.shape[1]+y1-w//15,:] = result\r\n+            \r\n+        triple_img = np.zeros([input_img.shape[0], input_img.shape[1]*3,input_img.shape[2]])\r\n+        triple_img[:, :input_img.shape[1]*2, :] = comb_img\r\n+        triple_img[:, input_img.shape[1]*2:, :] = mask_map\r\n+    \r\n+    # ========== Change rthe following line to ==========\r\n+    # return comb_img[:, input_img.shape[1]:, :]  # return only result image\r\n+    # return comb_img  # return input and result image combined as one\r\n+    return triple_img #return input,result and mask heatmap image combined as one\r\n+\r\n+\r\n+# In[ ]:\r\n+\r\n+\r\n+# Variables for smoothing bounding box\r\n+global prev_x0, prev_x1, prev_y0, prev_y1\r\n+global frames\r\n+prev_x0 = prev_x1 = prev_y0 = prev_y1 = 0\r\n+frames = 0\r\n+\r\n+output = 'OUTPUT_VIDEO.mp4'\r\n+clip1 = VideoFileClip(\"INPUT_VIDEO.mp4\")\r\n+clip = clip1.fl_image(process_video)#.subclip(11, 13) #NOTE: this function expects color images!!\r\n+get_ipython().run_line_magic('time', 'clip.write_videofile(output, audio=False)')\r\n+\r\n+\r\n+# ### gc.collect() sometimes solves memory error\r\n+\r\n+# In[103]:\r\n+\r\n+\r\n+import gc\r\n+gc.collect()\r\n+\r\ndiff --git a/plugins/Model_GAN/Model.py b/plugins/Model_GAN/Model.py\nnew file mode 100644\nindex 0000000000..df86693b72\n--- /dev/null\n+++ b/plugins/Model_GAN/Model.py\n@@ -0,0 +1,178 @@\n+# Based on the https://github.com/shaoanlu/faceswap-GAN repo (master/temp/faceswap_GAN_keras.ipynb)\r\n+\r\n+from keras.models import Model\r\n+from keras.layers import *\r\n+from keras.layers.advanced_activations import LeakyReLU\r\n+from keras.activations import relu\r\n+from keras.initializers import RandomNormal\r\n+from keras.applications import *\r\n+from keras.optimizers import Adam\r\n+\r\n+from lib.PixelShuffler import PixelShuffler\r\n+\r\n+netGAH5 = '/netGA_GAN.h5'\r\n+netGBH5 = '/netGB_GAN.h5'\r\n+netDAH5 = '/netDA_GAN.h5'\r\n+netDBH5 = '/netDB_GAN.h5'\r\n+\r\n+class GANModel():\r\n+    img_size = 64 \r\n+    channels = 3\r\n+    img_shape = (img_size, img_size, channels)\r\n+    encoded_dim = 1024\r\n+    \r\n+    def __init__(self, model_dir):\r\n+        self.model_dir = model_dir\r\n+\r\n+        optimizer = Adam(1e-4, 0.5)\r\n+\r\n+        # Build and compile the discriminator\r\n+        self.netDA, self.netDB = self.build_discriminator()\r\n+        self.netDA.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\r\n+        self.netDB.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\r\n+\r\n+        # Build and compile the generator\r\n+        self.netGA, self.netGB = self.build_generator()\r\n+        self.netGA.compile(loss=['mae', 'mse'], optimizer=optimizer)\r\n+        self.netGB.compile(loss=['mae', 'mse'], optimizer=optimizer)\r\n+\r\n+        img = Input(shape=self.img_shape)\r\n+        alphaA, reconstructed_imgA = self.netGA(img)\r\n+        alphaB, reconstructed_imgB = self.netGB(img)\r\n+\r\n+        # For the adversarial_autoencoder model we will only train the generator\r\n+        self.netDA.trainable = False\r\n+        self.netDB.trainable = False\r\n+\r\n+        def one_minus(x): return 1 - x\r\n+        # masked_img = alpha * reconstructed_img + (1 - alpha) * img\r\n+        masked_imgA = add([multiply([alphaA, reconstructed_imgA]), multiply([Lambda(one_minus)(alphaA), img])])\r\n+        masked_imgB = add([multiply([alphaB, reconstructed_imgB]), multiply([Lambda(one_minus)(alphaB), img])])\r\n+        out_discriminatorA = self.netDA(concatenate([masked_imgA, img], axis=-1))\r\n+        out_discriminatorB = self.netDB(concatenate([masked_imgB, img], axis=-1))\r\n+\r\n+        # The adversarial_autoencoder model  (stacked generator and discriminator) takes\r\n+        # img as input => generates encoded represenation and reconstructed image => determines validity \r\n+        self.adversarial_autoencoderA = Model(img, [reconstructed_imgA, out_discriminatorA])\r\n+        self.adversarial_autoencoderB = Model(img, [reconstructed_imgB, out_discriminatorB])\r\n+        self.adversarial_autoencoderA.compile(loss=['mae', 'mse'],\r\n+                                              loss_weights=[1, 0.5],\r\n+                                              optimizer=optimizer)\r\n+        self.adversarial_autoencoderB.compile(loss=['mae', 'mse'],\r\n+                                              loss_weights=[1, 0.5],\r\n+                                              optimizer=optimizer)\r\n+\r\n+    def converter(self, swap):\r\n+        predictor = self.netGB if not swap else self.netGA\r\n+        return lambda img: predictor.predict(img)[1] #Warning! GAN converter returns an image + a mask TODO handle mask in convert!\r\n+\r\n+    def build_generator(self):\r\n+        def conv_block(input_tensor, f):\r\n+            x = input_tensor\r\n+            x = Conv2D(f, kernel_size=3, strides=2, kernel_initializer=RandomNormal(0, 0.02), \r\n+                       use_bias=False, padding=\"same\")(x)\r\n+            x = LeakyReLU(alpha=0.2)(x)\r\n+            return x\r\n+\r\n+        def res_block(input_tensor, f):\r\n+            x = input_tensor\r\n+            x = Conv2D(f, kernel_size=3, kernel_initializer=RandomNormal(0, 0.02), \r\n+                       use_bias=False, padding=\"same\")(x)\r\n+            x = LeakyReLU(alpha=0.2)(x)\r\n+            x = Conv2D(f, kernel_size=3, kernel_initializer=RandomNormal(0, 0.02), \r\n+                       use_bias=False, padding=\"same\")(x)\r\n+            x = add([x, input_tensor])\r\n+            x = LeakyReLU(alpha=0.2)(x)\r\n+            return x\r\n+\r\n+        def upscale_ps(filters, use_norm=True):\r\n+            def block(x):\r\n+                x = Conv2D(filters*4, kernel_size=3, use_bias=False, \r\n+                           kernel_initializer=RandomNormal(0, 0.02), padding='same' )(x)\r\n+                x = LeakyReLU(0.1)(x)\r\n+                x = PixelShuffler()(x)\r\n+                return x\r\n+            return block\r\n+\r\n+        def Encoder(img_shape):\r\n+            inp = Input(shape=img_shape)\r\n+            x = Conv2D(64, kernel_size=5, kernel_initializer=RandomNormal(0, 0.02), \r\n+                       use_bias=False, padding=\"same\")(inp)\r\n+            x = conv_block(x,128)\r\n+            x = conv_block(x,256)\r\n+            x = conv_block(x,512) \r\n+            x = conv_block(x,1024)\r\n+            x = Dense(1024)(Flatten()(x))\r\n+            x = Dense(4*4*1024)(x)\r\n+            x = Reshape((4, 4, 1024))(x)\r\n+            out = upscale_ps(512)(x)\r\n+            return Model(inputs=inp, outputs=out)\r\n+\r\n+        def Decoder_ps(img_shape):\r\n+            nc_in = 512\r\n+            input_size = img_shape[0]//8\r\n+            inp = Input(shape=(input_size, input_size, nc_in))\r\n+            x = inp\r\n+            x = upscale_ps(256)(x)\r\n+            x = upscale_ps(128)(x)\r\n+            x = upscale_ps(64)(x)\r\n+            x = res_block(x, 64)\r\n+            x = res_block(x, 64)\r\n+            alpha = Conv2D(1, kernel_size=5, padding='same', activation=\"sigmoid\")(x)\r\n+            rgb = Conv2D(3, kernel_size=5, padding='same', activation=\"tanh\")(x)\r\n+            return Model(inp, [alpha, rgb])\r\n+        \r\n+        encoder = Encoder(self.img_shape)\r\n+        decoder_A = Decoder_ps(self.img_shape)\r\n+        decoder_B = Decoder_ps(self.img_shape)    \r\n+        x = Input(shape=self.img_shape)\r\n+        netGA = Model(x, decoder_A(encoder(x)))\r\n+        netGB = Model(x, decoder_B(encoder(x)))           \r\n+        try:\r\n+            netGA.load_weights(self.model_dir + netGAH5)\r\n+            netGB.load_weights(self.model_dir + netGBH5)\r\n+            print (\"Generator models loaded.\")\r\n+        except:\r\n+            print (\"Generator weights files not found.\")\r\n+            pass\r\n+        return netGA, netGB, \r\n+\r\n+    def build_discriminator(self):  \r\n+        def conv_block_d(input_tensor, f, use_instance_norm=True):\r\n+            x = input_tensor\r\n+            x = Conv2D(f, kernel_size=4, strides=2, kernel_initializer=RandomNormal(0, 0.02), \r\n+                       use_bias=False, padding=\"same\")(x)\r\n+            x = LeakyReLU(alpha=0.2)(x)\r\n+            return x   \r\n+        def Discriminator(img_shape):\r\n+            inp = Input(shape=(img_shape[0], img_shape[1], img_shape[2]*2))\r\n+            x = conv_block_d(inp, 64, False)\r\n+            x = conv_block_d(x, 128, False)\r\n+            x = conv_block_d(x, 256, False)\r\n+            out = Conv2D(1, kernel_size=4, kernel_initializer=RandomNormal(0, 0.02), \r\n+                         use_bias=False, padding=\"same\", activation=\"sigmoid\")(x)   \r\n+            return Model(inputs=[inp], outputs=out) \r\n+        \r\n+        netDA = Discriminator(self.img_shape)\r\n+        netDB = Discriminator(self.img_shape)        \r\n+        try:\r\n+            netDA.load_weights(self.model_dir + netDAH5) \r\n+            netDB.load_weights(self.model_dir + netDBH5) \r\n+            print (\"Discriminator models loaded.\")\r\n+        except:\r\n+            print (\"Discriminator weights files not found.\")\r\n+            pass\r\n+        return netDA, netDB    \r\n+    \r\n+    def load(self, swapped):\r\n+        if swapped:\r\n+            print(\"swapping not supported on GAN\")\r\n+            # TODO load is done in __init__ => look how to swap if possible\r\n+        return True\r\n+    \r\n+    def save_weights(self):\r\n+        self.netGA.save_weights(self.model_dir + netGAH5)\r\n+        self.netGB.save_weights(self.model_dir + netGBH5)\r\n+        self.netDA.save_weights(self.model_dir + netDAH5)\r\n+        self.netDB.save_weights(self.model_dir + netDBH5)\r\n+        print (\"Models saved.\")\n\\ No newline at end of file\ndiff --git a/plugins/Model_GAN/Trainer.py b/plugins/Model_GAN/Trainer.py\nnew file mode 100644\nindex 0000000000..190c6f37ef\n--- /dev/null\n+++ b/plugins/Model_GAN/Trainer.py\n@@ -0,0 +1,142 @@\n+import time\r\n+import cv2\r\n+import numpy as np\r\n+\r\n+from lib.training_data import minibatchAB, stack_images\r\n+\r\n+class Trainer():\r\n+    random_transform_args = {\r\n+        'rotation_range': 20,\r\n+        'zoom_range': 0.05,\r\n+        'shift_range': 0.05,\r\n+        'random_flip': 0.5,\r\n+        }\r\n+    def __init__(self, model, fn_A, fn_B, batch_size=8):\r\n+        assert batch_size % 2 == 0, \"batch_size must be an even number\"\r\n+        self.model = model\r\n+        self.train_batchA = minibatchAB(fn_A, batch_size, self.random_transform_args)\r\n+        self.train_batchB = minibatchAB(fn_B, batch_size, self.random_transform_args)\r\n+        self.batch_size = batch_size\r\n+        \r\n+        self.use_mixup = True\r\n+        self.mixup_alpha = 0.2\r\n+    \r\n+    def train_one_step(self, iter, viewer):\r\n+        # ---------------------\r\n+        #  Train Discriminators\r\n+        # ---------------------\r\n+\r\n+        # Select a random half batch of images\r\n+        epoch, warped_A, target_A = next(self.train_batchA) \r\n+        epoch, warped_B, target_B = next(self.train_batchB) \r\n+\r\n+        # Generate a half batch of new images\r\n+        gen_alphasA, gen_imgsA = self.model.netGA.predict(warped_A)\r\n+        gen_alphasB, gen_imgsB = self.model.netGB.predict(warped_B)\r\n+        #gen_masked_imgsA = gen_alphasA * gen_imgsA + (1 - gen_alphasA) * warped_A\r\n+        #gen_masked_imgsB = gen_alphasB * gen_imgsB + (1 - gen_alphasB) * warped_B\r\n+        gen_masked_imgsA = np.array([gen_alphasA[i] * gen_imgsA[i] + (1 - gen_alphasA[i]) * warped_A[i] \r\n+                                     for i in range(self.batch_size)])\r\n+        gen_masked_imgsB = np.array([gen_alphasB[i] * gen_imgsB[i] + (1 - gen_alphasB[i]) * warped_B[i]\r\n+                                     for i in range (self.batch_size)])\r\n+\r\n+        valid = np.ones((self.batch_size, ) + self.model.netDA.output_shape[1:])\r\n+        fake = np.zeros((self.batch_size, ) + self.model.netDA.output_shape[1:])\r\n+\r\n+        concat_real_inputA = np.array([np.concatenate([target_A[i], warped_A[i]], axis=-1) \r\n+                                       for i in range(self.batch_size)])\r\n+        concat_real_inputB = np.array([np.concatenate([target_B[i], warped_B[i]], axis=-1) \r\n+                                       for i in range(self.batch_size)])\r\n+        concat_fake_inputA = np.array([np.concatenate([gen_masked_imgsA[i], warped_A[i]], axis=-1) \r\n+                                       for i in range(self.batch_size)])\r\n+        concat_fake_inputB = np.array([np.concatenate([gen_masked_imgsB[i], warped_B[i]], axis=-1) \r\n+                                       for i in range(self.batch_size)])\r\n+        if self.use_mixup:\r\n+            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\r\n+            mixup_A = lam * concat_real_inputA + (1 - lam) * concat_fake_inputA\r\n+            mixup_B = lam * concat_real_inputB + (1 - lam) * concat_fake_inputB\r\n+\r\n+        # Train the discriminators\r\n+        #print (\"Train the discriminators.\")\r\n+        if self.use_mixup:\r\n+            d_lossA = self.model.netDA.train_on_batch(mixup_A, lam * valid)\r\n+            d_lossB = self.model.netDB.train_on_batch(mixup_B, lam * valid)\r\n+        else:\r\n+            d_lossA = self.model.netDA.train_on_batch(np.concatenate([concat_real_inputA, concat_fake_inputA], axis=0), \r\n+                                                np.concatenate([valid, fake], axis=0))\r\n+            d_lossB = self.model.netDB.train_on_batch(np.concatenate([concat_real_inputB, concat_fake_inputB], axis=0),\r\n+                                                np.concatenate([valid, fake], axis=0))\r\n+\r\n+        # ---------------------\r\n+        #  Train Generators\r\n+        # ---------------------\r\n+\r\n+        # Train the generators\r\n+        #print (\"Train the generators.\")\r\n+        g_lossA = self.model.adversarial_autoencoderA.train_on_batch(warped_A, [target_A, valid])\r\n+        g_lossB = self.model.adversarial_autoencoderB.train_on_batch(warped_B, [target_B, valid])            \r\n+        \r\n+        print('[%s] [%d/%s][%d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f'\r\n+              % (time.strftime(\"%H:%M:%S\"), epoch, \"num_epochs\", iter, d_lossA[0], d_lossB[0], g_lossA[0], g_lossB[0]))\r\n+        \r\n+        if viewer is not None:\r\n+            self.show_sample(viewer)\r\n+    \r\n+    def show_sample(self, display_fn):\r\n+        _, wA, tA = next(self.train_batchA)\r\n+        _, wB, tB = next(self.train_batchB)\r\n+        self.showG(tA, tB, display_fn)\r\n+\r\n+    def showG(self, test_A, test_B, display_fn):\r\n+        def display_fig(name, figure_A, figure_B):\r\n+            figure = np.concatenate([figure_A, figure_B], axis=0 )\r\n+            columns = 4\r\n+            elements = figure.shape[0]\r\n+            figure = figure.reshape((columns,(elements//columns)) + figure.shape[1:])\r\n+            figure = stack_images(figure)\r\n+            figure = np.clip((figure + 1) * 255 / 2, 0, 255).astype('uint8')\r\n+            display_fn(figure, name)\r\n+\r\n+        out_test_A_netGA = self.model.netGA.predict(test_A)\r\n+        out_test_A_netGB = self.model.netGB.predict(test_A)\r\n+        out_test_B_netGA = self.model.netGA.predict(test_B)\r\n+        out_test_B_netGB = self.model.netGB.predict(test_B)\r\n+\r\n+        figure_A = np.stack([\r\n+            test_A,\r\n+            out_test_A_netGA[0] * out_test_A_netGA[1] + (1 - out_test_A_netGA[0]) * test_A,\r\n+            out_test_A_netGB[0] * out_test_A_netGB[1] + (1 - out_test_A_netGB[0]) * test_A,\r\n+            ], axis=1 )\r\n+        figure_B = np.stack([\r\n+            test_B,\r\n+            out_test_B_netGB[0] * out_test_B_netGB[1] + (1 - out_test_B_netGB[0]) * test_B,\r\n+            out_test_B_netGA[0] * out_test_B_netGA[1] + (1 - out_test_B_netGA[0]) * test_B,\r\n+            ], axis=1 )\r\n+        \r\n+        display_fig(\"masked\", figure_A, figure_B)\r\n+\r\n+        figure_A = np.stack([\r\n+            test_A,\r\n+            out_test_A_netGA[1],\r\n+            out_test_A_netGB[1],\r\n+            ], axis=1 )\r\n+        figure_B = np.stack([\r\n+            test_B,\r\n+            out_test_B_netGB[1],\r\n+            out_test_B_netGA[1],\r\n+            ], axis=1 )\r\n+        \r\n+        display_fig(\"raw\", figure_A, figure_B)       \r\n+\r\n+        figure_A = np.stack([\r\n+            test_A,\r\n+            np.tile(out_test_A_netGA[0],3) * 2 - 1,\r\n+            np.tile(out_test_A_netGB[0],3) * 2 - 1,\r\n+            ], axis=1 )\r\n+        figure_B = np.stack([\r\n+            test_B,\r\n+            np.tile(out_test_B_netGB[0],3) * 2 - 1,\r\n+            np.tile(out_test_B_netGA[0],3) * 2 - 1,\r\n+            ], axis=1 )\r\n+\r\n+        display_fig(\"alpha_masks\", figure_A, figure_B)\n\\ No newline at end of file\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "While the plugin architecture enhances concurrency, adherence to its formal structure is crucial for maintainability. The exception you're encountering arises from improperly defined parameters in your `minibatch` function. Ensure that it accepts three arguments as expected. Additionally, the `random_transform_args` needs proper definition to resolve the `NameError`. I recommend validating your changes against a test script to catch errors before pushing. Finally, address the warning about trainable weights by ensuring you call `model.compile` after setting `model.trainable`. Consider forking for better compatibility and collaboration."
      }
    ],
    "codediff_length": 20767
  },
  {
    "index": 72,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/lib/Serializer.py b/lib/Serializer.py\nnew file mode 100644\nindex 0000000000..aafef801a2\n--- /dev/null\n+++ b/lib/Serializer.py\n@@ -0,0 +1,83 @@\n+import json, pickle\n+class Serializer(object):\n+    ext = \"wtf\"\n+    woptions = \"????\"\n+    roptions = \"????\"\n+\n+    @classmethod\n+    def marshal(cls, input_data):\n+        raise NotImplementedError()\n+\n+    @classmethod\n+    def unmarshal(cls, input_string):\n+        raise NotImplementedError()\n+\n+\n+class YAMLSerializer(Serializer):\n+    ext = \"yml\"\n+    woptions = \"w\"\n+    roptions = \"r\"\n+\n+    @classmethod\n+    def marshal(cls, input_data):\n+        try:\n+            import yaml\n+        except ImportError:\n+            print(\"You must have PyYAML installed to use YAMLSerializer\")\n+        return yaml.dump(input_data, default_flow_style=False)\n+\n+    @classmethod\n+    def unmarshal(cls, input_string):\n+        try:\n+            import yaml\n+        except ImportError:\n+            print(\"You must have PyYAML installed to use YAMLSerializer\")\n+        return yaml.load(input_string)\n+\n+\n+\n+class JSONSerializer(Serializer):\n+    ext = \"json\"\n+    woptions = \"w\"\n+    roptions = \"r\"\n+\n+    @classmethod\n+    def marshal(cls, input_data):\n+        return json.dumps(input_data, indent=2)\n+\n+    @classmethod\n+    def unmarshal(cls, input_string):\n+        return json.loads(input_string)\n+\n+\n+class PickleSerializer(Serializer):\n+    ext = \"p\"\n+    woptions = \"wb\"\n+    roptions = \"rb\"\n+\n+    @classmethod\n+    def marshal(cls, input_data):\n+        return pickle.dumps(input_data)\n+\n+    @classmethod\n+    def unmarshal(cls, input_bytes):\n+        return pickle.loads(input_bytes)\n+\n+def get_serializer(serializer):\n+    if serializer == \"yaml\":\n+        return YAMLSerializer\n+    if serializer == \"json\":\n+        return JSONSerializer\n+    if serializer == \"pickle\":\n+        return PickleSerializer\n+    raise NotImplementedError()\n+\n+\n+def get_serializer_fromext(ext):\n+    if ext in (\".yaml\", \".yml\"):\n+        return YAMLSerializer\n+    if ext == \".json\":\n+        return JSONSerializer\n+    if ext == \".p\":\n+        return PickleSerializer\n+    raise NotImplementedError(\"No serializer matching that file type.\")\ndiff --git a/lib/cli.py b/lib/cli.py\nindex 2cfe23f4f0..848ee2234c 100644\n--- a/lib/cli.py\n+++ b/lib/cli.py\n@@ -49,27 +61,81 @@ def process_arguments(self, arguments):\n         self.filter = self.load_filter()\n         self.process()\n         self.finalize()\n-        \n+\n+    def read_alignments(self):\n+\n+        fn = os.path.join(str(self.arguments.input_dir),\"alignments.{}\".format(self.serializer.ext))\n+        if self.arguments.alignments_path is not None:\n+            fn = self.arguments.alignments_path\n+\n+        try:\n+            print(\"Reading alignments from: {}\".format(fn))\n+            with open(fn, self.serializer.roptions) as f:\n+                self.faces_detected = self.serializer.unmarshal(f.read())\n+        except Exception as e:\n+            print(\"{} not read!\".format(fn))\n+            print(str(e))\n+            self.faces_detected = dict()\n+\n+    def write_alignments(self):\n+\n+        fn = os.path.join(str(self.arguments.input_dir), \"alignments.{}\".format(self.serializer.ext))\n+        if self.arguments.alignments_path is not None:\n+            fn = self.arguments.alignments_path\n+        try:\n+            print(\"Writing alignments to: {}\".format(fn))\n+            with open(fn, self.serializer.woptions) as fh:\n+                fh.write(self.serializer.marshal(self.faces_detected))\n+        except Exception as e:\n+            print(\"{} not written!\".format(fn))\n+            print(str(e))\n+            self.faces_detected = dict()\n+\n     def read_directory(self):\n         self.images_found = len(self.input_dir)\n         return self.input_dir\n-    \n-    def get_faces(self, image):\n+\n+    def have_face(self, filename):\n+        return os.path.basename(filename) in self.faces_detected\n+\n+    def have_alignments(self):\n+        fn = os.path.join(str(self.arguments.input_dir), \"alignments.{}\".format(self.serializer.ext))\n+        return os.path.exists(fn)\n+\n+    def get_faces_alignments(self, filename, image):\n         faces_count = 0\n-        for face in detect_faces(image, self.arguments.detector):\n+        faces = self.faces_detected[os.path.basename(filename)]\n+        for rawface in faces:\n+            face = DetectedFace(**rawface)\n+            face.image = image[face.y : face.y + face.h, face.x : face.x + face.w]\n             if self.filter is not None and not self.filter.check(face):\n                 print('Skipping not recognized face!')\n                 continue\n \n             yield faces_count, face\n+            self.num_faces_detected += 1\n+            faces_count += 1\n+        if faces_count > 1 and self.arguments.verbose:\n+            print('Note: Found more than one face in an image!')\n+            self.verify_output = True\n+\n+    def get_faces(self, image):\n+        faces_count = 0\n+        faces = detect_faces(image, self.arguments.detector)\n+\n+        for face in faces:\n+            if self.filter is not None and not self.filter.check(face):\n+                print('Skipping not recognized face!')\n+                continue\n+            yield faces_count, face\n \n-            self.faces_detected = self.faces_detected + 1\n+            self.num_faces_detected += 1\n             faces_count += 1\n-        \n+\n         if faces_count > 1 and self.arguments.verbose:\n             print('Note: Found more than one face in an image!')\n             self.verify_output = True\n-    \n+\n     def load_filter(self):\n         filter_file = self.arguments.filter\n         if Path(filter_file).exists():\n\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR addresses multiple issues, primarily around the handling of paths and image processing using OpenCV. The type error with `join()` on 'WindowsPath' can be resolved by converting paths to strings. The `convert` function raises an assertion error due to image dimensions, which seems linked to the use of `alignments.json`. It works correctly only with the \"Masked\" converter. Additionally, the implementation misses the `face.image` field required for certain converters. Suggested fixes improve face alignment handling, but further attention is needed on memory usage with the CNN detector on GPU."
      }
    ],
    "codediff_length": 5661
  },
  {
    "index": 73,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/lib/gpu_stats/apple_silicon.py b/lib/gpu_stats/apple_silicon.py\nnew file mode 100644\nindex 0000000000..7dc39b106c\n--- /dev/null\n+++ b/lib/gpu_stats/apple_silicon.py\n@@ -0,0 +1,126 @@\n+#!/usr/bin/env python3\n+\"\"\" Collects and returns Information on available Apple Silicon SoCs in Apple Macs. \"\"\"\n+from typing import List\n+\n+import lib.metal as metal\n+\n+from lib.utils import FaceswapError\n+\n+from ._base import GPUStats\n+\n+\n+class AppleSiliconStats(GPUStats):\n+    \"\"\" Holds information and statistics about Apple Silicon SoC(s) available on the currently\n+    running Apple system.\n+\n+    Parameters\n+    ----------\n+    log: bool, optional\n+        Whether the class should output information to the logger. There may be occasions where the\n+        logger has not yet been set up when this class is queried. Attempting to log in these\n+        instances will raise an error. If GPU stats are being queried prior to the logger being\n+        available then this parameter should be set to ``False``. Otherwise set to ``True``.\n+        Default: ``True``\n+    \"\"\"\n+\n+    def _initialize(self) -> None:\n+        \"\"\" Initialize Metal for Apple Silicon SoC(s).\n+\n+        If :attr:`_is_initialized` is ``True`` then this function just returns performing no\n+        action. Otherwise :attr:`is_initialized` is set to ``True`` after successfully\n+        initializing Metal.\n+\n+        Raises\n+        ------\n+        FaceswapError\n+            If the Metal library could not be successfully loaded\n+        \"\"\"\n+        if self._is_initialized:\n+            return\n+        self._log(\"debug\", \"Initializing Metal for Apple Silicon SoC.\")\n+        try:\n+            metal.init()  # pylint:disable=no-member\n+        except RuntimeError as err:\n+            msg = (\"An unhandled exception occured initializing the device via Metal\"\n+                   f\"Library. Original error: {str(err)}\")\n+            raise FaceswapError(msg) from err\n+        super()._initialize()\n+\n+    def _get_device_count(self) -> int:\n+        \"\"\" Detect the number of SoCs attached to the system.\n+\n+        Returns\n+        -------\n+        int\n+            The total number of SoCs available\n+        \"\"\"\n+        retval = metal.get_device_count()  # pylint:disable=no-member\n+        self._log(\"debug\", f\"GPU Device count: {retval}\")\n+        return retval\n+\n+    def _get_handles(self) -> list:\n+        \"\"\" Obtain the device handles for all available Apple Silicon SoCs.\n+\n+        Returns\n+        -------\n+        list\n+            The list of pointers for available Apple Silicon SoCs\n+        \"\"\"\n+        handles = metal.get_handles()  # pylint:disable=no-member\n+        self._log(\"debug\", f\"GPU Handles found: {len(handles)}\")\n+        return handles\n+\n+    def _get_driver(self) -> str:\n+        \"\"\" Obtain the Apple Silicon driver version currently in use.\n+\n+        Returns\n+        -------\n+        str\n+            The current SoC driver version\n+        \"\"\"\n+        driver = metal.get_driver_version()  # pylint:disable=no-member\n+        self._log(\"debug\", f\"GPU Driver: {driver}\")\n+        return driver\n+\n+    def _get_device_names(self) -> List[str]:\n+        \"\"\" Obtain the list of names of available Apple Silicon SoC(s) as identified in :attr:`_handles`.\n+\n+        Returns\n+        -------\n+        list\n+            The list of available Apple Silicon SoC names\n+        \"\"\"\n+        names = metal.get_device_names()\n+        self._log(\"debug\", f\"GPU Devices: {names}\")\n+        return names\n+\n+    def _get_vram(self) -> List[float]:\n+        \"\"\" Obtain the VRAM in Megabytes for each available Apple Silicon SoC(s) as identified in\n+        :attr:`_handles`.\n+\n+        Returns\n+        -------\n+        list\n+            The VRAM in Megabytes for each available Apple Silicon SoC\n+        \"\"\"\n+        vram = [\n+            metal.get_memory_info(i) / (1024 * 1024)\n+            for i in range(self._device_count)]\n+        self._log(\"debug\", f\"GPU VRAM: {vram}\")\n+        return vram\n+\n+    def _get_free_vram(self) -> List[float]:\n+        \"\"\" Obtain the amount of VRAM that is available, in Megabytes, for each available Apple Silicon\n+        SoC.\n+\n+        Returns\n+        -------\n+        list\n+             List of `float`s containing the amount of VRAM available, in Megabytes, for each\n+             available SoC as corresponding to the values in :attr:`_handles\n+        \"\"\"\n+        vram = [\n+            metal.get_memory_info(i) / (1024 * 1024)\n+            for i in range(self._device_count)]\n+        self._log(\"debug\", f\"GPU VRAM free: {vram}\")\n+        return vram\ndiff --git a/lib/metal/__init__.py b/lib/metal/__init__.py\nnew file mode 100644\nindex 0000000000..738353c39b\n--- /dev/null\n+++ b/lib/metal/__init__.py\n@@ -0,0 +1,76 @@\n+from typing import List\n+import os\n+import psutil  # used for getting GPU memory\n+import tensorflow as tf\n+\n+\n+class Constants:\n+    class System:\n+        ARCH = 'arm64'\n+        DEVICE_TYPE = 'GPU'\n+        SET_MEMORY_GROWTH = True\n+\n+    class CUDA:\n+        DRIVER_VERSION_UNSUPPORTED = 0\n+\n+\n+def _dbg_check_mem():\n+    print(\"==========================================\")\n+    print(tf.config.experimental.get_memory_info('GPU:0'))\n+    print(tf.config.list_logical_devices())\n+    print(\"==========================================\")\n+\n+\n+def _validate_metal():\n+    # Validate a GPU exists\n+    assert(len(tf.config.experimental.list_physical_devices('GPU')) > 0)\n+\n+    # Validate Metal device is working\n+    with tf.device('GPU:0'):\n+        assert(tf.math.add(1.0, 2.0) == 3.0)\n+\n+\n+def init(device_type: str = 'GPU') -> None:\n+    # _validate_metal()\n+\n+    os.environ['DISPLAY'] = ':0'\n+    try:\n+        os.system('open -a XQuartz')\n+    except Exception:\n+        pass\n+    Constants.System.DEVICE_TYPE = device_type\n+\n+    # for device in get_devices():\n+    #    tf.config.experimental.set_memory_growth(device, Constants.System.SET_MEMORY_GROWTH)\n+\n+    _dbg_check_mem()\n+\n+\n+def get_devices() -> List[tf.config.PhysicalDevice]:\n+    return tf.config.list_physical_devices(device_type=Constants.System.DEVICE_TYPE)\n+\n+\n+def get_device_count() -> int:\n+    return len(get_devices())\n+\n+\n+def get_handles() -> list:\n+    return list(range(get_device_count()))\n+\n+\n+def get_driver_version() -> int:\n+    # https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART____VERSION.html\n+    return Constants.CUDA.DRIVER_VERSION_UNSUPPORTED\n+\n+\n+def get_device_names() -> List[str]:\n+    return [d.name for d in get_devices()]\n+\n+\n+def get_memory_info(handle: int) -> int:\n+    # Does not work:\n+    #   tf.config.experimental.get_memory_info('GPU:0')\n+    # So, using psutil instead.\n+    # We can just grab the total memory, as it's shared between\n+    # the CPU and the GPU. There is no dedicated VRAM.\n+    return psutil.virtual_memory().total / get_device_count()\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The PR shows promising updates, but it has several PEP 8 violations that need attention. Key issues include lines being too long and missing whitespace around comments in `apple_silicon.py` and `__init__.py`. Additionally, there are inconsistent blank line requirements that should be corrected. While its great to see the GPUStats refactor plan, ensure that the code adheres to PEP 8 standards for consistency. Consider using a linter to automate formatting fixes. Also, think about implementing the suggested install script for easier setup."
      }
    ],
    "codediff_length": 6851
  },
  {
    "index": 74,
    "prompt": [
      {
        "role": "user",
        "content": "diff --git a/requests/structures.py b/requests/structures.py\nindex 8d02ea67b6..bb1f3bc2c0 100644\n--- a/requests/structures.py\n+++ b/requests/structures.py\n@@ -126,3 +126,603 @@ def __getitem__(self, key):\n \n     def get(self, key, default=None):\n         return self.__dict__.get(key, default)\n+\n+# from mitsuihiko/werkzeug\n+BadRequestKeyError = KeyError\n+\n+# from mitsuhiko/werkzeug (slightly altered to use items() instead of iteritems(), and duck-typing for MultiDict)\n+def iter_multi_items(mapping):\n+    \"\"\"Iterates over the items of a mapping yielding keys and values\n+    without dropping any from more complex structures.\n+    \"\"\"\n+    if isinstance(mapping, collections.Mapping):\n+        try:\n+            for item in mapping.items(multi=True):\n+                yield item\n+        except TypeError:\n+            for key, value in mapping.items():\n+                if isinstance(value, (tuple, list)):\n+                    for value in value:\n+                        yield key, value\n+                else:\n+                    yield key, value\n+    else:\n+        for item in mapping:\n+            yield item\n+\n+\n+# from mitsuhiko/werkzeug\n+class _omd_bucket(object):\n+    \"\"\"Wraps values in the :class:`OrderedMultiDict`.  This makes it\n+    possible to keep an order over multiple different keys.  It requires\n+    a lot of extra memory and slows down access a lot, but makes it\n+    possible to access elements in O(1) and iterate in O(n).\n+    \"\"\"\n+    __slots__ = ('prev', 'key', 'value', 'next')\n+\n+    def __init__(self, omd, key, value):\n+        self.prev = omd._last_bucket\n+        self.key = key\n+        self.value = value\n+        self.next = None\n+\n+        if omd._first_bucket is None:\n+            omd._first_bucket = self\n+        if omd._last_bucket is not None:\n+            omd._last_bucket.next = self\n+        omd._last_bucket = self\n+\n+    def unlink(self, omd):\n+        if self.prev:\n+            self.prev.next = self.next\n+        if self.next:\n+            self.next.prev = self.prev\n+        if omd._first_bucket is self:\n+            omd._first_bucket = self.next\n+        if omd._last_bucket is self:\n+            omd._last_bucket = self.prev\n+\n+\n+# from mitsuhiko/werkzeug\n+class _Missing(object):\n+\n+    def __repr__(self):\n+        return 'no value'\n+\n+    def __reduce__(self):\n+        return '_missing'\n+\n+_missing = _Missing()\n+\n+\n+# from mitsuhiko/werkzeug\n+class TypeConversionDict(dict):\n+    \"\"\"Works like a regular dict but the :meth:`get` method can perform\n+    type conversions.  :class:`MultiDict` and :class:`CombinedMultiDict`\n+    are subclasses of this class and provide the same feature.\n+\n+    .. versionadded:: 0.5\n+    \"\"\"\n+\n+    def get(self, key, default=None, type=None):\n+        \"\"\"Return the default value if the requested data doesn't exist.\n+        If `type` is provided and is a callable it should convert the value,\n+        return it or raise a :exc:`ValueError` if that is not possible.  In\n+        this case the function will return the default as if the value was not\n+        found:\n+\n+        >>> d = TypeConversionDict(foo='42', bar='blub')\n+        >>> d.get('foo', type=int)\n+        42\n+        >>> d.get('bar', -1, type=int)\n+        -1\n+\n+        :param key: The key to be looked up.\n+        :param default: The default value to be returned if the key can't\n+                        be looked up.  If not further specified `None` is\n+                        returned.\n+        :param type: A callable that is used to cast the value in the\n+                     :class:`MultiDict`.  If a :exc:`ValueError` is raised\n+                     by this callable the default value is returned.\n+        \"\"\"\n+        try:\n+            rv = self[key]\n+            if type is not None:\n+                rv = type(rv)\n+        except (KeyError, ValueError):\n+            rv = default\n+        return rv\n+\n+\n+# from mitsuhiko/werkzeug\n+class MultiDict(TypeConversionDict):\n+    \"\"\"A :class:`MultiDict` is a dictionary subclass customized to deal with\n+    multiple values for the same key which is for example used by the parsing\n+    functions in the wrappers.  This is necessary because some HTML form\n+    elements pass multiple values for the same key.\n+\n+    :class:`MultiDict` implements all standard dictionary methods.\n+    Internally, it saves all values for a key as a list, but the standard dict\n+    access methods will only return the first value for a key. If you want to\n+    gain access to the other values, too, you have to use the `list` methods as\n+    explained below.\n+\n+    Basic Usage:\n+\n+    >>> d = MultiDict([('a', 'b'), ('a', 'c')])\n+    >>> d\n+    MultiDict([('a', 'b'), ('a', 'c')])\n+    >>> d['a']\n+    'b'\n+    >>> d.getlist('a')\n+    ['b', 'c']\n+    >>> 'a' in d\n+    True\n+\n+    It behaves like a normal dict thus all dict functions will only return the\n+    first value when multiple values for one key are found.\n+\n+    From Werkzeug 0.3 onwards, the `KeyError` raised by this class is also a\n+    subclass of the :exc:`~exceptions.BadRequest` HTTP exception and will\n+    render a page for a ``400 BAD REQUEST`` if caught in a catch-all for HTTP\n+    exceptions.\n+\n+    A :class:`MultiDict` can be constructed from an iterable of\n+    ``(key, value)`` tuples, a dict, a :class:`MultiDict` or from Werkzeug 0.2\n+    onwards some keyword parameters.\n+\n+    :param mapping: the initial value for the :class:`MultiDict`.  Either a\n+                    regular dict, an iterable of ``(key, value)`` tuples\n+                    or `None`.\n+    \"\"\"\n+\n+    def __init__(self, mapping=None):\n+        if isinstance(mapping, collections.Mapping) and hasattr(mapping, 'iterlists'):\n+            dict.__init__(self, ((k, l[:]) for k, l in mapping.iterlists()))\n+        elif isinstance(mapping, collections.Mapping):\n+            tmp = {}\n+            for key, value in mapping.iteritems():\n+                if isinstance(value, (tuple, list)):\n+                    value = list(value)\n+                else:\n+                    value = [value]\n+                tmp[key] = value\n+            dict.__init__(self, tmp)\n+        else:\n+            tmp = {}\n+            for key, value in mapping or ():\n+                tmp.setdefault(key, []).append(value)\n+            dict.__init__(self, tmp)\n+\n+    def __getstate__(self):\n+        return dict(self.lists())\n+\n+    def __setstate__(self, value):\n+        dict.clear(self)\n+        dict.update(self, value)\n+\n+    def __iter__(self):\n+        return self.iterkeys()\n+\n+    def __getitem__(self, key):\n+        \"\"\"Return the first data value for this key;\n+        raises KeyError if not found.\n+\n+        :param key: The key to be looked up.\n+        :raise KeyError: if the key does not exist.\n+        \"\"\"\n+        if key in self:\n+            return dict.__getitem__(self, key)[0]\n+        raise BadRequestKeyError(key)\n+\n+    def __setitem__(self, key, value):\n+        \"\"\"Like :meth:`add` but removes an existing key first.\n+\n+        :param key: the key for the value.\n+        :param value: the value to set.\n+        \"\"\"\n+        dict.__setitem__(self, key, [value])\n+\n+    def add(self, key, value):\n+        \"\"\"Adds a new value for the key.\n+\n+        .. versionadded:: 0.6\n+\n+        :param key: the key for the value.\n+        :param value: the value to add.\n+        \"\"\"\n+        dict.setdefault(self, key, []).append(value)\n+\n+    def getlist(self, key, type=None):\n+        \"\"\"Return the list of items for a given key. If that key is not in the\n+        `MultiDict`, the return value will be an empty list.  Just as `get`\n+        `getlist` accepts a `type` parameter.  All items will be converted\n+        with the callable defined there.\n+\n+        :param key: The key to be looked up.\n+        :param type: A callable that is used to cast the value in the\n+                     :class:`MultiDict`.  If a :exc:`ValueError` is raised\n+                     by this callable the value will be removed from the list.\n+        :return: a :class:`list` of all the values for the key.\n+        \"\"\"\n+        try:\n+            rv = dict.__getitem__(self, key)\n+        except KeyError:\n+            return []\n+        if type is None:\n+            return list(rv)\n+        result = []\n+        for item in rv:\n+            try:\n+                result.append(type(item))\n+            except ValueError:\n+                pass\n+        return result\n+\n+    def setlist(self, key, new_list):\n+        \"\"\"Remove the old values for a key and add new ones.  Note that the list\n+        you pass the values in will be shallow-copied before it is inserted in\n+        the dictionary.\n+\n+        >>> d = MultiDict()\n+        >>> d.setlist('foo', ['1', '2'])\n+        >>> d['foo']\n+        '1'\n+        >>> d.getlist('foo')\n+        ['1', '2']\n+\n+        :param key: The key for which the values are set.\n+        :param new_list: An iterable with the new values for the key.  Old values\n+                         are removed first.\n+        \"\"\"\n+        dict.__setitem__(self, key, list(new_list))\n+\n+    def setdefault(self, key, default=None):\n+        \"\"\"Returns the value for the key if it is in the dict, otherwise it\n+        returns `default` and sets that value for `key`.\n+\n+        :param key: The key to be looked up.\n+        :param default: The default value to be returned if the key is not\n+                        in the dict.  If not further specified it's `None`.\n+        \"\"\"\n+        if key not in self:\n+            self[key] = default\n+        else:\n+            default = self[key]\n+        return default\n+\n+    def setlistdefault(self, key, default_list=None):\n+        \"\"\"Like `setdefault` but sets multiple values.  The list returned\n+        is not a copy, but the list that is actually used internally.  This\n+        means that you can put new values into the dict by appending items\n+        to the list:\n+\n+        >>> d = MultiDict({\"foo\": 1})\n+        >>> d.setlistdefault(\"foo\").extend([2, 3])\n+        >>> d.getlist(\"foo\")\n+        [1, 2, 3]\n+\n+        :param key: The key to be looked up.\n+        :param default: An iterable of default values.  It is either copied\n+                        (in case it was a list) or converted into a list\n+                        before returned.\n+        :return: a :class:`list`\n+        \"\"\"\n+        if key not in self:\n+            default_list = list(default_list or ())\n+            dict.__setitem__(self, key, default_list)\n+        else:\n+            default_list = dict.__getitem__(self, key)\n+        return default_list\n+\n+    def items(self, multi=True):\n+        \"\"\"Return a list of ``(key, value)`` pairs.\n+\n+        :param multi: If set to `True` the list returned will have a\n+                      pair for each value of each key.  Otherwise it\n+                      will only contain pairs for the first value of\n+                      each key.\n+\n+        :return: a :class:`list`\n+        \"\"\"\n+        return list(self.iteritems(multi))\n+\n+    def lists(self):\n+        \"\"\"Return a list of ``(key, values)`` pairs, where values is the list of\n+        all values associated with the key.\n+\n+        :return: a :class:`list`\n+        \"\"\"\n+        return list(self.iterlists())\n+\n+    def values(self):\n+        \"\"\"Returns a list of the first value on every key's value list.\n+\n+        :return: a :class:`list`.\n+        \"\"\"\n+        return [self[key] for key in self.iterkeys()]\n+\n+    def listvalues(self):\n+        \"\"\"Return a list of all values associated with a key.  Zipping\n+        :meth:`keys` and this is the same as calling :meth:`lists`:\n+\n+        >>> d = MultiDict({\"foo\": [1, 2, 3]})\n+        >>> zip(d.keys(), d.listvalues()) == d.lists()\n+        True\n+\n+        :return: a :class:`list`\n+        \"\"\"\n+        return list(self.iterlistvalues())\n+\n+    def iteritems(self, multi=True):\n+        \"\"\"Like :meth:`items` but returns an iterator.\"\"\"\n+        for key, values in dict.iteritems(self):\n+            if multi:\n+                for value in values:\n+                    yield key, value\n+            else:\n+                yield key, values[0]\n+\n+    def iterlists(self):\n+        \"\"\"Like :meth:`items` but returns an iterator.\"\"\"\n+        for key, values in dict.iteritems(self):\n+            yield key, list(values)\n+\n+    def itervalues(self):\n+        \"\"\"Like :meth:`values` but returns an iterator.\"\"\"\n+        for values in dict.itervalues(self):\n+            yield values[0]\n+\n+    def iterlistvalues(self):\n+        \"\"\"Like :meth:`listvalues` but returns an iterator.\"\"\"\n+        return dict.itervalues(self)\n+\n+    def copy(self):\n+        \"\"\"Return a shallow copy of this object.\"\"\"\n+        return self.__class__(self)\n+\n+    def to_dict(self, flat=True):\n+        \"\"\"Return the contents as regular dict.  If `flat` is `True` the\n+        returned dict will only have the first item present, if `flat` is\n+        `False` all values will be returned as lists.\n+\n+        :param flat: If set to `False` the dict returned will have lists\n+                     with all the values in it.  Otherwise it will only\n+                     contain the first value for each key.\n+        :return: a :class:`dict`\n+        \"\"\"\n+        if flat:\n+            return dict(self.iteritems())\n+        return dict(self.lists())\n+\n+    def update(self, other_dict):\n+        \"\"\"update() extends rather than replaces existing key lists.\"\"\"\n+        for key, value in iter_multi_items(other_dict):\n+            MultiDict.add(self, key, value)\n+\n+    def pop(self, key, default=_missing):\n+        \"\"\"Pop the first item for a list on the dict.  Afterwards the\n+        key is removed from the dict, so additional values are discarded:\n+\n+        >>> d = MultiDict({\"foo\": [1, 2, 3]})\n+        >>> d.pop(\"foo\")\n+        1\n+        >>> \"foo\" in d\n+        False\n+\n+        :param key: the key to pop.\n+        :param default: if provided the value to return if the key was\n+                        not in the dictionary.\n+        \"\"\"\n+        try:\n+            return dict.pop(self, key)[0]\n+        except KeyError as e:\n+            if default is not _missing:\n+                return default\n+            raise BadRequestKeyError(str(e))\n+\n+    def popitem(self):\n+        \"\"\"Pop an item from the dict.\"\"\"\n+        try:\n+            item = dict.popitem(self)\n+            return (item[0], item[1][0])\n+        except KeyError as e:\n+            raise BadRequestKeyError(str(e))\n+\n+    def poplist(self, key):\n+        \"\"\"Pop the list for a key from the dict.  If the key is not in the dict\n+        an empty list is returned.\n+\n+        .. versionchanged:: 0.5\n+           If the key does no longer exist a list is returned instead of\n+           raising an error.\n+        \"\"\"\n+        return dict.pop(self, key, [])\n+\n+    def popitemlist(self):\n+        \"\"\"Pop a ``(key, list)`` tuple from the dict.\"\"\"\n+        try:\n+            return dict.popitem(self)\n+        except KeyError as e:\n+            raise BadRequestKeyError(str(e))\n+\n+    def __copy__(self):\n+        return self.copy()\n+\n+    def __repr__(self):\n+        return '%s(%r)' % (self.__class__.__name__, self.items(multi=True))\n+\n+\n+# from mitsuhiko/werkzeug (with the exception of all iterators, which default to multi=True, instead of werkzeug's\n+#                          multi=False)\n+class OrderedMultiDict(MultiDict):\n+    \"\"\"Works like a regular :class:`MultiDict` but preserves the\n+    order of the fields.  To convert the ordered multi dict into a\n+    list you can use the :meth:`items` method and pass it ``multi=True``.\n+\n+    In general an :class:`OrderedMultiDict` is an order of magnitude\n+    slower than a :class:`MultiDict`.\n+\n+    .. admonition:: note\n+\n+       Due to a limitation in Python you cannot convert an ordered\n+       multi dict into a regular dict by using ``dict(multidict)``.\n+       Instead you have to use the :meth:`to_dict` method, otherwise\n+       the internal bucket objects are exposed.\n+    \"\"\"\n+\n+    def __init__(self, mapping=None):\n+        dict.__init__(self)\n+        self._first_bucket = self._last_bucket = None\n+        if mapping is not None:\n+            OrderedMultiDict.update(self, mapping)\n+\n+    def __eq__(self, other):\n+        if not hasattr(other, 'iterlists'):\n+            return NotImplemented\n+        if isinstance(other, OrderedMultiDict):\n+            iter1 = self.iteritems(multi=True)\n+            iter2 = other.iteritems(multi=True)\n+            try:\n+                for k1, v1 in iter1:\n+                    k2, v2 = iter2.next()\n+                    if k1 != k2 or v1 != v2:\n+                        return False\n+            except StopIteration:\n+                return False\n+            try:\n+                iter2.next()\n+            except StopIteration:\n+                return True\n+            return False\n+        if len(self) != len(other):\n+            return False\n+        for key, values in self.iterlists():\n+            if other.getlist(key) != values:\n+                return False\n+        return True\n+\n+    def __ne__(self, other):\n+        return not self.__eq__(other)\n+\n+    def __reduce_ex__(self, protocol):\n+        return type(self), (self.items(multi=True),)\n+\n+    def __getstate__(self):\n+        return self.items(multi=True)\n+\n+    def __setstate__(self, values):\n+        dict.clear(self)\n+        for key, value in values:\n+            self.add(key, value)\n+\n+    def __getitem__(self, key):\n+        if key in self:\n+            return dict.__getitem__(self, key)[0].value\n+        raise BadRequestKeyError(key)\n+\n+    def __setitem__(self, key, value):\n+        self.poplist(key)\n+        self.add(key, value)\n+\n+    def __delitem__(self, key):\n+        self.pop(key)\n+\n+    def iterkeys(self):\n+        return (key for key, value in self.iteritems())\n+\n+    def itervalues(self):\n+        return (value for key, value in self.iteritems())\n+\n+    def iteritems(self, multi=True):\n+        ptr = self._first_bucket\n+        if multi:\n+            while ptr is not None:\n+                yield ptr.key, ptr.value\n+                ptr = ptr.next\n+        else:\n+            returned_keys = set()\n+            while ptr is not None:\n+                if ptr.key not in returned_keys:\n+                    returned_keys.add(ptr.key)\n+                    yield ptr.key, ptr.value\n+                ptr = ptr.next\n+\n+    def iterlists(self):\n+        returned_keys = set()\n+        ptr = self._first_bucket\n+        while ptr is not None:\n+            if ptr.key not in returned_keys:\n+                yield ptr.key, self.getlist(ptr.key)\n+                returned_keys.add(ptr.key)\n+            ptr = ptr.next\n+\n+    def iterlistvalues(self):\n+        for key, values in self.iterlists():\n+            yield values\n+\n+    def add(self, key, value):\n+        dict.setdefault(self, key, []).append(_omd_bucket(self, key, value))\n+\n+    def getlist(self, key, type=None):\n+        try:\n+            rv = dict.__getitem__(self, key)\n+        except KeyError:\n+            return []\n+        if type is None:\n+            return [x.value for x in rv]\n+        result = []\n+        for item in rv:\n+            try:\n+                result.append(type(item.value))\n+            except ValueError:\n+                pass\n+        return result\n+\n+    def setlist(self, key, new_list):\n+        self.poplist(key)\n+        for value in new_list:\n+            self.add(key, value)\n+\n+    def setlistdefault(self, key, default_list=None):\n+        raise TypeError('setlistdefault is unsupported for '\n+                        'ordered multi dicts')\n+\n+    def update(self, mapping):\n+        for key, value in iter_multi_items(mapping):\n+            OrderedMultiDict.add(self, key, value)\n+\n+    def poplist(self, key):\n+        buckets = dict.pop(self, key, ())\n+        for bucket in buckets:\n+            bucket.unlink(self)\n+        return [x.value for x in buckets]\n+\n+    def pop(self, key, default=_missing):\n+        try:\n+            buckets = dict.pop(self, key)\n+        except KeyError as e:\n+            if default is not _missing:\n+                return default\n+            raise BadRequestKeyError(str(e))\n+        for bucket in buckets:\n+            bucket.unlink(self)\n+        return buckets[0].value\n+\n+    def popitem(self):\n+        try:\n+            key, buckets = dict.popitem(self)\n+        except KeyError as e:\n+            raise BadRequestKeyError(str(e))\n+        for bucket in buckets:\n+            bucket.unlink(self)\n+        return key, buckets[0].value\n+\n+    def popitemlist(self):\n+        try:\n+            key, buckets = dict.popitem(self)\n+        except KeyError as e:\n+            raise BadRequestKeyError(str(e))\n+        for bucket in buckets:\n+            bucket.unlink(self)\n+        return key, [x.value for x in buckets]\n\\ No newline at end of file\n\n"
      }
    ],
    "completion": [
      {
        "role": "assistant",
        "content": "The implementation of `to_key_val_list` needs adjustments. It mishandles non-standard mappings like `CaseInsensitiveDict` and fails with sequences that dont contain 2-tuples. Switching from `dict` checks to `collections.Mapping` improves compatibility. I recommend simplifying type checks and utilizing a list comprehension for clarity. Regarding the `.update` method, it should overwrite keys, aligning with expected behavior, rather than adding duplicates. Lets clarify whether to allow updates for existing session keys while considering potential use case variations."
      }
    ],
    "codediff_length": 20822
  }
]