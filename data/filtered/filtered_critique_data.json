[
  {
    "index": 1,
    "filtered_comments": [
      "Note I have only added loading the profile key on the first post block, whilst I check im doing it the correct way.\r\n\r\nI need to add the frontend button next as well.",
      "Thank you for this thorough Ayrshare integration PR! The code looks well-structured with comprehensive implementation across both frontend and backend components.\n\nBefore this PR can be merged, there are a few items that need to be addressed:\n\n1. **Missing Checklist**: Please add the standard PR checklist from our template and check off the relevant items. Since this is a significant code change introducing new functionality, we need to ensure you've tested the implementation thoroughly.\n\n2. **Merge Conflicts**: The PR has the 'conflicts' label, indicating there are merge conflicts that need to be resolved before merging.\n\n3. **PR Title Scope**: Consider updating the PR title to use 'platform/blocks' as the scope instead of just 'block' to better align with our conventional commit format and labeled scopes.\n\n4. **Test Plan**: Please provide a test plan detailing how you've verified this integration works correctly. For example:\n   - Connecting to different social media platforms via Ayrshare\n   - Posting content to each supported platform\n   - Handling error cases (e.g., when profile key is missing)\n\nThe implementation itself looks solid, with proper security considerations and a clean architecture. Once the above items are addressed, this PR should be ready for final review.",
      "Thanks for this comprehensive PR adding Ayrshare integration for social media posting! The implementation looks solid with good support across multiple platforms.\n\nI've reviewed your changes and have a couple of items that need addressing before this can be merged:\n\n1. **PR Title Format**: The scope in your title should be `blocks` instead of `block` to match our conventional commit format standards. Please update to: `feat(blocks): Added Ayrshare integration for social media posting`\n\n2. **Missing Checklist**: Your PR description is very detailed, which is great, but it's missing the required checklist section. Please add the standard PR checklist that includes items like:\n   - Confirming you've tested your changes\n   - Your test plan\n   - Any configuration changes\n\nThe code implementation itself looks well-structured with comprehensive support for various social media platforms. I particularly like how you've organized the different posting blocks with platform-specific options and validations.\n\nOnce you've addressed these two items, this PR should be ready for another review.",
      "Thanks for the comprehensive Ayrshare integration PR! The implementation looks well-designed and thoroughly documented.\n\n### What Looks Good\n- Great PR description with detailed explanations of all components\n- Clean implementation of both backend and frontend components\n- Good separation of concerns with platform-specific posting blocks\n- Environment variables correctly added to `.env.example`\n- Proper handling of user_id in credential store operations\n- The new AYRSHARE block type is added in the correct alphabetical location\n\n### What Needs Addressing\n- **Missing checklist**: Please add the required checklist to the PR description. As this is a code change, we need a complete checklist including a test plan to verify the functionality works correctly.\n\n### Testing Considerations\nSince this is a complex integration, please ensure your test plan includes:\n- Creating and connecting to Ayrshare accounts\n- Posting to various social media platforms\n- Handling error cases (e.g., invalid credentials, failed posts)\n- Verifying the SSO flow works correctly\n\nOnce you've added the checklist with a proper test plan, this PR should be ready for final review."
    ],
    "code_diff": "diff --git a/autogpt_platform/backend/.env.example b/autogpt_platform/backend/.env.example\nindex e223efa52557..21aef0c95fc9 100644\n--- a/autogpt_platform/backend/.env.example\n+++ b/autogpt_platform/backend/.env.example\n@@ -197,6 +197,10 @@ SMARTLEAD_API_KEY=\n # ZeroBounce\n ZEROBOUNCE_API_KEY=\n \n+# Ayrshare\n+AYRSHARE_API_KEY=\n+AYRSHARE_JWT_KEY=\n+\n ## ===== OPTIONAL API KEYS END ===== ##\n \n # Block Error Rate Monitoring\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/__init__.py b/autogpt_platform/backend/backend/blocks/ayrshare/__init__.py\nnew file mode 100644\nindex 000000000000..94a567109e60\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/__init__.py\n@@ -0,0 +1,15 @@\n+AYRSHARE_BLOCK_IDS = [\n+    \"cbd52c2a-06d2-43ed-9560-6576cc163283\",  # PostToBlueskyBlock\n+    \"3352f512-3524-49ed-a08f-003042da2fc1\",  # PostToFacebookBlock\n+    \"9e8f844e-b4a5-4b25-80f2-9e1dd7d67625\",  # PostToXBlock\n+    \"589af4e4-507f-42fd-b9ac-a67ecef25811\",  # PostToLinkedInBlock\n+    \"89b02b96-a7cb-46f4-9900-c48b32fe1552\",  # PostToInstagramBlock\n+    \"0082d712-ff1b-4c3d-8a8d-6c7721883b83\",  # PostToYouTubeBlock\n+    \"c7733580-3c72-483e-8e47-a8d58754d853\",  # PostToRedditBlock\n+    \"47bc74eb-4af2-452c-b933-af377c7287df\",  # PostToTelegramBlock\n+    \"2c38c783-c484-4503-9280-ef5d1d345a7e\",  # PostToGMBBlock\n+    \"3ca46e05-dbaa-4afb-9e95-5a429c4177e6\",  # PostToPinterestBlock\n+    \"7faf4b27-96b0-4f05-bf64-e0de54ae74e1\",  # PostToTikTokBlock\n+    \"f8c3b2e1-9d4a-4e5f-8c7b-6a9e8d2f1c3b\",  # PostToThreadsBlock\n+    \"a9d7f854-2c83-4e96-b3a1-7f2e9c5d4b8e\",  # PostToSnapchatBlock\n+]\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/_util.py b/autogpt_platform/backend/backend/blocks/ayrshare/_util.py\nnew file mode 100644\nindex 000000000000..1d2b24e67d76\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/_util.py\n@@ -0,0 +1,144 @@\n+from datetime import datetime\n+from typing import Optional\n+\n+from pydantic import BaseModel, Field\n+\n+from backend.data.block import BlockSchema\n+from backend.data.model import SchemaField\n+from backend.integrations.ayrshare import AyrshareClient\n+from backend.util.exceptions import MissingConfigError\n+\n+\n+class BaseAyrshareInput(BlockSchema):\n+    \"\"\"Base input model for Ayrshare social media posts with common fields.\"\"\"\n+\n+    post: str = SchemaField(\n+        description=\"The post text to be published\", default=\"\", advanced=False\n+    )\n+    media_urls: list[str] = SchemaField(\n+        description=\"Optional list of media URLs to include. Set is_video in advanced settings to true if you want to upload videos.\",\n+        default_factory=list,\n+        advanced=False,\n+    )\n+    is_video: bool = SchemaField(\n+        description=\"Whether the media is a video\", default=False, advanced=True\n+    )\n+    schedule_date: Optional[datetime] = SchemaField(\n+        description=\"UTC datetime for scheduling (YYYY-MM-DDThh:mm:ssZ)\",\n+        default=None,\n+        advanced=True,\n+    )\n+    disable_comments: bool = SchemaField(\n+        description=\"Whether to disable comments\", default=False, advanced=True\n+    )\n+    shorten_links: bool = SchemaField(\n+        description=\"Whether to shorten links\", default=False, advanced=True\n+    )\n+    unsplash: Optional[str] = SchemaField(\n+        description=\"Unsplash image configuration\", default=None, advanced=True\n+    )\n+    requires_approval: bool = SchemaField(\n+        description=\"Whether to enable approval workflow\",\n+        default=False,\n+        advanced=True,\n+    )\n+    random_post: bool = SchemaField(\n+        description=\"Whether to generate random post text\",\n+        default=False,\n+        advanced=True,\n+    )\n+    random_media_url: bool = SchemaField(\n+        description=\"Whether to generate random media\", default=False, advanced=True\n+    )\n+    notes: Optional[str] = SchemaField(\n+        description=\"Additional notes for the post\", default=None, advanced=True\n+    )\n+\n+\n+class CarouselItem(BaseModel):\n+    \"\"\"Model for Facebook carousel items.\"\"\"\n+\n+    name: str = Field(..., description=\"The name of the item\")\n+    link: str = Field(..., description=\"The link of the item\")\n+    picture: str = Field(..., description=\"The picture URL of the item\")\n+\n+\n+class CallToAction(BaseModel):\n+    \"\"\"Model for Google My Business Call to Action.\"\"\"\n+\n+    action_type: str = Field(\n+        ..., description=\"Type of action (book, order, shop, learn_more, sign_up, call)\"\n+    )\n+    url: Optional[str] = Field(\n+        description=\"URL for the action (not required for 'call' action)\"\n+    )\n+\n+\n+class EventDetails(BaseModel):\n+    \"\"\"Model for Google My Business Event details.\"\"\"\n+\n+    title: str = Field(..., description=\"Event title\")\n+    start_date: str = Field(..., description=\"Event start date (ISO format)\")\n+    end_date: str = Field(..., description=\"Event end date (ISO format)\")\n+\n+\n+class OfferDetails(BaseModel):\n+    \"\"\"Model for Google My Business Offer details.\"\"\"\n+\n+    title: str = Field(..., description=\"Offer title\")\n+    start_date: str = Field(..., description=\"Offer start date (ISO format)\")\n+    end_date: str = Field(..., description=\"Offer end date (ISO format)\")\n+    coupon_code: str = Field(..., description=\"Coupon code (max 58 characters)\")\n+    redeem_online_url: str = Field(..., description=\"URL to redeem the offer\")\n+    terms_conditions: str = Field(..., description=\"Terms and conditions\")\n+\n+\n+class InstagramUserTag(BaseModel):\n+    \"\"\"Model for Instagram user tags.\"\"\"\n+\n+    username: str = Field(..., description=\"Instagram username (without @)\")\n+    x: Optional[float] = Field(description=\"X coordinate (0.0-1.0) for image posts\")\n+    y: Optional[float] = Field(description=\"Y coordinate (0.0-1.0) for image posts\")\n+\n+\n+class LinkedInTargeting(BaseModel):\n+    \"\"\"Model for LinkedIn audience targeting.\"\"\"\n+\n+    countries: Optional[list[str]] = Field(\n+        description=\"Country codes (e.g., ['US', 'IN', 'DE', 'GB'])\"\n+    )\n+    seniorities: Optional[list[str]] = Field(\n+        description=\"Seniority levels (e.g., ['Senior', 'VP'])\"\n+    )\n+    degrees: Optional[list[str]] = Field(description=\"Education degrees\")\n+    fields_of_study: Optional[list[str]] = Field(description=\"Fields of study\")\n+    industries: Optional[list[str]] = Field(description=\"Industry categories\")\n+    job_functions: Optional[list[str]] = Field(description=\"Job function categories\")\n+    staff_count_ranges: Optional[list[str]] = Field(description=\"Company size ranges\")\n+\n+\n+class PinterestCarouselOption(BaseModel):\n+    \"\"\"Model for Pinterest carousel image options.\"\"\"\n+\n+    title: Optional[str] = Field(description=\"Image title\")\n+    link: Optional[str] = Field(description=\"External destination link for the image\")\n+    description: Optional[str] = Field(description=\"Image description\")\n+\n+\n+class YouTubeTargeting(BaseModel):\n+    \"\"\"Model for YouTube country targeting.\"\"\"\n+\n+    block: Optional[list[str]] = Field(\n+        description=\"Country codes to block (e.g., ['US', 'CA'])\"\n+    )\n+    allow: Optional[list[str]] = Field(\n+        description=\"Country codes to allow (e.g., ['GB', 'AU'])\"\n+    )\n+\n+\n+def create_ayrshare_client():\n+    \"\"\"Create an Ayrshare client instance.\"\"\"\n+    try:\n+        return AyrshareClient()\n+    except MissingConfigError:\n+        return None\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_bluesky.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_bluesky.py\nnew file mode 100644\nindex 000000000000..4f4a0683140c\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_bluesky.py\n@@ -0,0 +1,113 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToBlueskyBlock(Block):\n+    \"\"\"Block for posting to Bluesky with Bluesky-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for Bluesky posts.\"\"\"\n+\n+        # Override post field to include character limit information\n+        post: str = SchemaField(\n+            description=\"The post text to be published (max 300 characters for Bluesky)\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include Bluesky-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Optional list of media URLs to include. Bluesky supports up to 4 images or 1 video.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # Bluesky-specific options\n+        alt_text: list[str] = SchemaField(\n+            description=\"Alt text for each media item (accessibility)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"cbd52c2a-06d2-43ed-9560-6576cc163283\",\n+            description=\"Post to Bluesky using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToBlueskyBlock.Input,\n+            output_schema=PostToBlueskyBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToBlueskyBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to Bluesky with Bluesky-specific options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate character limit for Bluesky\n+        if len(input_data.post) > 300:\n+            yield \"error\", f\"Post text exceeds Bluesky's 300 character limit ({len(input_data.post)} characters)\"\n+            return\n+\n+        # Validate media constraints for Bluesky\n+        if len(input_data.media_urls) > 4:\n+            yield \"error\", \"Bluesky supports a maximum of 4 images or 1 video\"\n+            return\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build Bluesky-specific options\n+        bluesky_options = {}\n+        if input_data.alt_text:\n+            bluesky_options[\"altText\"] = input_data.alt_text\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.BLUESKY],\n+            media_urls=input_data.media_urls,\n+            is_video=input_data.is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            bluesky_options=bluesky_options if bluesky_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_facebook.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_facebook.py\nnew file mode 100644\nindex 000000000000..250be5522fda\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_facebook.py\n@@ -0,0 +1,207 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, CarouselItem, create_ayrshare_client\n+\n+\n+class PostToFacebookBlock(Block):\n+    \"\"\"Block for posting to Facebook with Facebook-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for Facebook posts.\"\"\"\n+\n+        # Facebook-specific options\n+        is_carousel: bool = SchemaField(\n+            description=\"Whether to post a carousel\", default=False, advanced=True\n+        )\n+        carousel_link: str = SchemaField(\n+            description=\"The URL for the 'See More At' button in the carousel\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        carousel_items: list[CarouselItem] = SchemaField(\n+            description=\"List of carousel items with name, link and picture URLs. Min 2, max 10 items.\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        is_reels: bool = SchemaField(\n+            description=\"Whether to post to Facebook Reels\",\n+            default=False,\n+            advanced=True,\n+        )\n+        reels_title: str = SchemaField(\n+            description=\"Title for the Reels video (max 255 chars)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        reels_thumbnail: str = SchemaField(\n+            description=\"Thumbnail URL for Reels video (JPEG/PNG, <10MB)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        is_story: bool = SchemaField(\n+            description=\"Whether to post as a Facebook Story\",\n+            default=False,\n+            advanced=True,\n+        )\n+        media_captions: list[str] = SchemaField(\n+            description=\"Captions for each media item\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        location_id: str = SchemaField(\n+            description=\"Facebook Page ID or name for location tagging\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        age_min: int = SchemaField(\n+            description=\"Minimum age for audience targeting (13,15,18,21,25)\",\n+            default=0,\n+            advanced=True,\n+        )\n+        target_countries: list[str] = SchemaField(\n+            description=\"List of country codes to target (max 25)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        alt_text: list[str] = SchemaField(\n+            description=\"Alt text for each media item\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        video_title: str = SchemaField(\n+            description=\"Title for video post\", default=\"\", advanced=True\n+        )\n+        video_thumbnail: str = SchemaField(\n+            description=\"Thumbnail URL for video post\", default=\"\", advanced=True\n+        )\n+        is_draft: bool = SchemaField(\n+            description=\"Save as draft in Meta Business Suite\",\n+            default=False,\n+            advanced=True,\n+        )\n+        scheduled_publish_date: str = SchemaField(\n+            description=\"Schedule publish time in Meta Business Suite (UTC)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        preview_link: str = SchemaField(\n+            description=\"URL for custom link preview\", default=\"\", advanced=True\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"3352f512-3524-49ed-a08f-003042da2fc1\",\n+            description=\"Post to Facebook using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToFacebookBlock.Input,\n+            output_schema=PostToFacebookBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToFacebookBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to Facebook with Facebook-specific options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build Facebook-specific options\n+        facebook_options = {}\n+        if input_data.is_carousel:\n+            facebook_options[\"isCarousel\"] = True\n+            if input_data.carousel_link:\n+                facebook_options[\"carouselLink\"] = input_data.carousel_link\n+            if input_data.carousel_items:\n+                facebook_options[\"carouselItems\"] = [\n+                    item.dict() for item in input_data.carousel_items\n+                ]\n+\n+        if input_data.is_reels:\n+            facebook_options[\"isReels\"] = True\n+            if input_data.reels_title:\n+                facebook_options[\"reelsTitle\"] = input_data.reels_title\n+            if input_data.reels_thumbnail:\n+                facebook_options[\"reelsThumbnail\"] = input_data.reels_thumbnail\n+\n+        if input_data.is_story:\n+            facebook_options[\"isStory\"] = True\n+\n+        if input_data.media_captions:\n+            facebook_options[\"mediaCaptions\"] = input_data.media_captions\n+\n+        if input_data.location_id:\n+            facebook_options[\"locationId\"] = input_data.location_id\n+\n+        if input_data.age_min > 0:\n+            facebook_options[\"ageMin\"] = input_data.age_min\n+\n+        if input_data.target_countries:\n+            facebook_options[\"targetCountries\"] = input_data.target_countries\n+\n+        if input_data.alt_text:\n+            facebook_options[\"altText\"] = input_data.alt_text\n+\n+        if input_data.video_title:\n+            facebook_options[\"videoTitle\"] = input_data.video_title\n+\n+        if input_data.video_thumbnail:\n+            facebook_options[\"videoThumbnail\"] = input_data.video_thumbnail\n+\n+        if input_data.is_draft:\n+            facebook_options[\"isDraft\"] = True\n+\n+        if input_data.scheduled_publish_date:\n+            facebook_options[\"scheduledPublishDate\"] = input_data.scheduled_publish_date\n+\n+        if input_data.preview_link:\n+            facebook_options[\"previewLink\"] = input_data.preview_link\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.FACEBOOK],\n+            media_urls=input_data.media_urls,\n+            is_video=input_data.is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            facebook_options=facebook_options if facebook_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_gmb.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_gmb.py\nnew file mode 100644\nindex 000000000000..d534a4ae2bf2\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_gmb.py\n@@ -0,0 +1,210 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToGMBBlock(Block):\n+    \"\"\"Block for posting to Google My Business with GMB-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for Google My Business posts.\"\"\"\n+\n+        # Override media_urls to include GMB-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Optional list of media URLs. GMB supports only one image or video per post.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # GMB-specific options\n+        is_photo_video: bool = SchemaField(\n+            description=\"Whether this is a photo/video post (appears in Photos section)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        photo_category: str = SchemaField(\n+            description=\"Category for photo/video: cover, profile, logo, exterior, interior, product, at_work, food_and_drink, menu, common_area, rooms, teams\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        # Call to action options (flattened from CallToAction object)\n+        call_to_action_type: str = SchemaField(\n+            description=\"Type of action button: 'book', 'order', 'shop', 'learn_more', 'sign_up', or 'call'\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        call_to_action_url: str = SchemaField(\n+            description=\"URL for the action button (not required for 'call' action)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        # Event details options (flattened from EventDetails object)\n+        event_title: str = SchemaField(\n+            description=\"Event title for event posts\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        event_start_date: str = SchemaField(\n+            description=\"Event start date in ISO format (e.g., '2024-03-15T09:00:00Z')\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        event_end_date: str = SchemaField(\n+            description=\"Event end date in ISO format (e.g., '2024-03-15T17:00:00Z')\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        # Offer details options (flattened from OfferDetails object)\n+        offer_title: str = SchemaField(\n+            description=\"Offer title for promotional posts\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        offer_start_date: str = SchemaField(\n+            description=\"Offer start date in ISO format (e.g., '2024-03-15T00:00:00Z')\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        offer_end_date: str = SchemaField(\n+            description=\"Offer end date in ISO format (e.g., '2024-04-15T23:59:59Z')\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        offer_coupon_code: str = SchemaField(\n+            description=\"Coupon code for the offer (max 58 characters)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        offer_redeem_online_url: str = SchemaField(\n+            description=\"URL where customers can redeem the offer online\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        offer_terms_conditions: str = SchemaField(\n+            description=\"Terms and conditions for the offer\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"2c38c783-c484-4503-9280-ef5d1d345a7e\",\n+            description=\"Post to Google My Business using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToGMBBlock.Input,\n+            output_schema=PostToGMBBlock.Output,\n+        )\n+\n+    async def run(\n+        self, input_data: \"PostToGMBBlock.Input\", *, profile_key: SecretStr, **kwargs\n+    ) -> BlockOutput:\n+        \"\"\"Post to Google My Business with GMB-specific options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate GMB constraints\n+        if len(input_data.media_urls) > 1:\n+            yield \"error\", \"Google My Business supports only one image or video per post\"\n+            return\n+\n+        # Validate offer coupon code length\n+        if input_data.offer_coupon_code and len(input_data.offer_coupon_code) > 58:\n+            yield \"error\", \"GMB offer coupon code cannot exceed 58 characters\"\n+            return\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build GMB-specific options\n+        gmb_options = {}\n+\n+        # Photo/Video post options\n+        if input_data.is_photo_video:\n+            gmb_options[\"isPhotoVideo\"] = True\n+            if input_data.photo_category:\n+                gmb_options[\"category\"] = input_data.photo_category\n+\n+        # Call to Action (from flattened fields)\n+        if input_data.call_to_action_type:\n+            cta_dict = {\"actionType\": input_data.call_to_action_type}\n+            # URL not required for 'call' action type\n+            if (\n+                input_data.call_to_action_type != \"call\"\n+                and input_data.call_to_action_url\n+            ):\n+                cta_dict[\"url\"] = input_data.call_to_action_url\n+            gmb_options[\"callToAction\"] = cta_dict\n+\n+        # Event details (from flattened fields)\n+        if (\n+            input_data.event_title\n+            and input_data.event_start_date\n+            and input_data.event_end_date\n+        ):\n+            gmb_options[\"event\"] = {\n+                \"title\": input_data.event_title,\n+                \"startDate\": input_data.event_start_date,\n+                \"endDate\": input_data.event_end_date,\n+            }\n+\n+        # Offer details (from flattened fields)\n+        if (\n+            input_data.offer_title\n+            and input_data.offer_start_date\n+            and input_data.offer_end_date\n+            and input_data.offer_coupon_code\n+            and input_data.offer_redeem_online_url\n+            and input_data.offer_terms_conditions\n+        ):\n+            gmb_options[\"offer\"] = {\n+                \"title\": input_data.offer_title,\n+                \"startDate\": input_data.offer_start_date,\n+                \"endDate\": input_data.offer_end_date,\n+                \"couponCode\": input_data.offer_coupon_code,\n+                \"redeemOnlineUrl\": input_data.offer_redeem_online_url,\n+                \"termsConditions\": input_data.offer_terms_conditions,\n+            }\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.GOOGLE_MY_BUSINESS],\n+            media_urls=input_data.media_urls,\n+            is_video=input_data.is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            gmb_options=gmb_options if gmb_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_instagram.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_instagram.py\nnew file mode 100644\nindex 000000000000..77337a9568b6\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_instagram.py\n@@ -0,0 +1,221 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, InstagramUserTag, create_ayrshare_client\n+\n+\n+class PostToInstagramBlock(Block):\n+    \"\"\"Block for posting to Instagram with Instagram-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for Instagram posts.\"\"\"\n+\n+        # Override post field to include Instagram-specific information\n+        post: str = SchemaField(\n+            description=\"The post text (max 2,200 chars, up to 30 hashtags, 3 @mentions)\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include Instagram-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Optional list of media URLs. Instagram supports up to 10 images/videos in a carousel.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # Instagram-specific options\n+        is_story: bool = SchemaField(\n+            description=\"Whether to post as Instagram Story (24-hour expiration)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        share_reels_feed: bool = SchemaField(\n+            description=\"Whether Reel should appear in both Feed and Reels tabs\",\n+            default=True,\n+            advanced=True,\n+        )\n+        audio_name: str = SchemaField(\n+            description=\"Audio name for Reels (e.g., 'The Weeknd - Blinding Lights')\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        thumbnail: str = SchemaField(\n+            description=\"Thumbnail URL for Reel video\", default=\"\", advanced=True\n+        )\n+        thumbnail_offset: int = SchemaField(\n+            description=\"Thumbnail frame offset in milliseconds (default: 0)\",\n+            default=0,\n+            advanced=True,\n+        )\n+        alt_text: list[str] = SchemaField(\n+            description=\"Alt text for each media item (up to 1,000 chars each, accessibility feature)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        location_id: str = SchemaField(\n+            description=\"Facebook Page ID or name for location tagging (e.g., '7640348500' or '@guggenheimmuseum')\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        user_tags: list[InstagramUserTag] = SchemaField(\n+            description=\"List of users to tag with coordinates for images\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        collaborators: list[str] = SchemaField(\n+            description=\"Instagram usernames to invite as collaborators (max 3, public accounts only)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        auto_resize: bool = SchemaField(\n+            description=\"Auto-resize images to 1080x1080px for Instagram\",\n+            default=False,\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"89b02b96-a7cb-46f4-9900-c48b32fe1552\",\n+            description=\"Post to Instagram using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToInstagramBlock.Input,\n+            output_schema=PostToInstagramBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToInstagramBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to Instagram with Instagram-specific options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate Instagram constraints\n+        if len(input_data.post) > 2200:\n+            yield \"error\", f\"Instagram post text exceeds 2,200 character limit ({len(input_data.post)} characters)\"\n+            return\n+\n+        if len(input_data.media_urls) > 10:\n+            yield \"error\", \"Instagram supports a maximum of 10 images/videos in a carousel\"\n+            return\n+\n+        if len(input_data.collaborators) > 3:\n+            yield \"error\", \"Instagram supports a maximum of 3 collaborators\"\n+            return\n+\n+        # Count hashtags and mentions\n+        hashtag_count = input_data.post.count(\"#\")\n+        mention_count = input_data.post.count(\"@\")\n+\n+        if hashtag_count > 30:\n+            yield \"error\", f\"Instagram allows maximum 30 hashtags ({hashtag_count} found)\"\n+            return\n+\n+        if mention_count > 3:\n+            yield \"error\", f\"Instagram allows maximum 3 @mentions ({mention_count} found)\"\n+            return\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build Instagram-specific options\n+        instagram_options = {}\n+\n+        # Stories\n+        if input_data.is_story:\n+            instagram_options[\"stories\"] = True\n+\n+        # Reels options\n+        if input_data.share_reels_feed is not None:\n+            instagram_options[\"shareReelsFeed\"] = input_data.share_reels_feed\n+\n+        if input_data.audio_name:\n+            instagram_options[\"audioName\"] = input_data.audio_name\n+\n+        if input_data.thumbnail:\n+            instagram_options[\"thumbNail\"] = input_data.thumbnail\n+        elif input_data.thumbnail_offset > 0:\n+            instagram_options[\"thumbNailOffset\"] = input_data.thumbnail_offset\n+\n+        # Alt text\n+        if input_data.alt_text:\n+            # Validate alt text length\n+            for i, alt in enumerate(input_data.alt_text):\n+                if len(alt) > 1000:\n+                    yield \"error\", f\"Alt text {i+1} exceeds 1,000 character limit ({len(alt)} characters)\"\n+                    return\n+            instagram_options[\"altText\"] = input_data.alt_text\n+\n+        # Location\n+        if input_data.location_id:\n+            instagram_options[\"locationId\"] = input_data.location_id\n+\n+        # User tags\n+        if input_data.user_tags:\n+            user_tags_list = []\n+            for tag in input_data.user_tags:\n+                tag_dict: dict[str, float | str] = {\"username\": tag.username}\n+                if tag.x is not None and tag.y is not None:\n+                    # Validate coordinates\n+                    if not (0.0 <= tag.x <= 1.0) or not (0.0 <= tag.y <= 1.0):\n+                        yield \"error\", f\"User tag coordinates must be between 0.0 and 1.0 (user: {tag.username})\"\n+                        return\n+                    tag_dict[\"x\"] = tag.x\n+                    tag_dict[\"y\"] = tag.y\n+                user_tags_list.append(tag_dict)\n+            instagram_options[\"userTags\"] = user_tags_list\n+\n+        # Collaborators\n+        if input_data.collaborators:\n+            instagram_options[\"collaborators\"] = input_data.collaborators\n+\n+        # Auto resize\n+        if input_data.auto_resize:\n+            instagram_options[\"autoResize\"] = True\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.INSTAGRAM],\n+            media_urls=input_data.media_urls,\n+            is_video=input_data.is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            instagram_options=instagram_options if instagram_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_linkedin.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_linkedin.py\nnew file mode 100644\nindex 000000000000..c3b01989f0be\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_linkedin.py\n@@ -0,0 +1,222 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToLinkedInBlock(Block):\n+    \"\"\"Block for posting to LinkedIn with LinkedIn-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for LinkedIn posts.\"\"\"\n+\n+        # Override post field to include LinkedIn-specific information\n+        post: str = SchemaField(\n+            description=\"The post text (max 3,000 chars, hashtags supported with #)\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include LinkedIn-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Optional list of media URLs. LinkedIn supports up to 9 images, videos, or documents (PPT, PPTX, DOC, DOCX, PDF <100MB, <300 pages).\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # LinkedIn-specific options\n+        visibility: str = SchemaField(\n+            description=\"Post visibility: 'public' (default), 'connections' (personal only), 'loggedin'\",\n+            default=\"public\",\n+            advanced=True,\n+        )\n+        alt_text: list[str] = SchemaField(\n+            description=\"Alt text for each image (accessibility feature, not supported for videos/documents)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        titles: list[str] = SchemaField(\n+            description=\"Title/caption for each image or video\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        document_title: str = SchemaField(\n+            description=\"Title for document posts (max 400 chars, uses filename if not specified)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        thumbnail: str = SchemaField(\n+            description=\"Thumbnail URL for video (PNG/JPG, same dimensions as video, <10MB)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        # LinkedIn targeting options (flattened from LinkedInTargeting object)\n+        targeting_countries: list[str] | None = SchemaField(\n+            description=\"Country codes for targeting (e.g., ['US', 'IN', 'DE', 'GB']). Requires 300+ followers in target audience.\",\n+            default=None,\n+            advanced=True,\n+        )\n+        targeting_seniorities: list[str] | None = SchemaField(\n+            description=\"Seniority levels for targeting (e.g., ['Senior', 'VP']). Requires 300+ followers in target audience.\",\n+            default=None,\n+            advanced=True,\n+        )\n+        targeting_degrees: list[str] | None = SchemaField(\n+            description=\"Education degrees for targeting. Requires 300+ followers in target audience.\",\n+            default=None,\n+            advanced=True,\n+        )\n+        targeting_fields_of_study: list[str] | None = SchemaField(\n+            description=\"Fields of study for targeting. Requires 300+ followers in target audience.\",\n+            default=None,\n+            advanced=True,\n+        )\n+        targeting_industries: list[str] | None = SchemaField(\n+            description=\"Industry categories for targeting. Requires 300+ followers in target audience.\",\n+            default=None,\n+            advanced=True,\n+        )\n+        targeting_job_functions: list[str] | None = SchemaField(\n+            description=\"Job function categories for targeting. Requires 300+ followers in target audience.\",\n+            default=None,\n+            advanced=True,\n+        )\n+        targeting_staff_count_ranges: list[str] | None = SchemaField(\n+            description=\"Company size ranges for targeting. Requires 300+ followers in target audience.\",\n+            default=None,\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            id=\"589af4e4-507f-42fd-b9ac-a67ecef25811\",\n+            description=\"Post to LinkedIn using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToLinkedInBlock.Input,\n+            output_schema=PostToLinkedInBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToLinkedInBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to LinkedIn with LinkedIn-specific options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate LinkedIn constraints\n+        if len(input_data.post) > 3000:\n+            yield \"error\", f\"LinkedIn post text exceeds 3,000 character limit ({len(input_data.post)} characters)\"\n+            return\n+\n+        if len(input_data.media_urls) > 9:\n+            yield \"error\", \"LinkedIn supports a maximum of 9 images/videos/documents\"\n+            return\n+\n+        if input_data.document_title and len(input_data.document_title) > 400:\n+            yield \"error\", f\"LinkedIn document title exceeds 400 character limit ({len(input_data.document_title)} characters)\"\n+            return\n+\n+        # Validate visibility option\n+        valid_visibility = [\"public\", \"connections\", \"loggedin\"]\n+        if input_data.visibility not in valid_visibility:\n+            yield \"error\", f\"LinkedIn visibility must be one of: {', '.join(valid_visibility)}\"\n+            return\n+\n+        # Check for document extensions\n+        document_extensions = [\".ppt\", \".pptx\", \".doc\", \".docx\", \".pdf\"]\n+        has_documents = any(\n+            any(url.lower().endswith(ext) for ext in document_extensions)\n+            for url in input_data.media_urls\n+        )\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build LinkedIn-specific options\n+        linkedin_options = {}\n+\n+        # Visibility\n+        if input_data.visibility != \"public\":\n+            linkedin_options[\"visibility\"] = input_data.visibility\n+\n+        # Alt text (not supported for videos or documents)\n+        if input_data.alt_text and not has_documents:\n+            linkedin_options[\"altText\"] = input_data.alt_text\n+\n+        # Titles/captions\n+        if input_data.titles:\n+            linkedin_options[\"titles\"] = input_data.titles\n+\n+        # Document title\n+        if input_data.document_title and has_documents:\n+            linkedin_options[\"title\"] = input_data.document_title\n+\n+        # Video thumbnail\n+        if input_data.thumbnail:\n+            linkedin_options[\"thumbNail\"] = input_data.thumbnail\n+\n+        # Audience targeting (from flattened fields)\n+        targeting_dict = {}\n+        if input_data.targeting_countries:\n+            targeting_dict[\"countries\"] = input_data.targeting_countries\n+        if input_data.targeting_seniorities:\n+            targeting_dict[\"seniorities\"] = input_data.targeting_seniorities\n+        if input_data.targeting_degrees:\n+            targeting_dict[\"degrees\"] = input_data.targeting_degrees\n+        if input_data.targeting_fields_of_study:\n+            targeting_dict[\"fieldsOfStudy\"] = input_data.targeting_fields_of_study\n+        if input_data.targeting_industries:\n+            targeting_dict[\"industries\"] = input_data.targeting_industries\n+        if input_data.targeting_job_functions:\n+            targeting_dict[\"jobFunctions\"] = input_data.targeting_job_functions\n+        if input_data.targeting_staff_count_ranges:\n+            targeting_dict[\"staffCountRanges\"] = input_data.targeting_staff_count_ranges\n+\n+        if targeting_dict:\n+            linkedin_options[\"targeting\"] = targeting_dict\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.LINKEDIN],\n+            media_urls=input_data.media_urls,\n+            is_video=input_data.is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            linkedin_options=linkedin_options if linkedin_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_pinterest.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_pinterest.py\nnew file mode 100644\nindex 000000000000..c6fab03b32a9\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_pinterest.py\n@@ -0,0 +1,209 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, PinterestCarouselOption, create_ayrshare_client\n+\n+\n+class PostToPinterestBlock(Block):\n+    \"\"\"Block for posting to Pinterest with Pinterest-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for Pinterest posts.\"\"\"\n+\n+        # Override post field to include Pinterest-specific information\n+        post: str = SchemaField(\n+            description=\"Pin description (max 500 chars, links not clickable - use link field instead)\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include Pinterest-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Required image/video URLs. Pinterest requires at least one image. Videos need thumbnail. Up to 5 images for carousel.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # Pinterest-specific options\n+        pin_title: str = SchemaField(\n+            description=\"Pin title displayed in 'Add your title' section (max 100 chars)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        link: str = SchemaField(\n+            description=\"Clickable destination URL when users click the pin (max 2048 chars)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        board_id: str = SchemaField(\n+            description=\"Pinterest Board ID to post to (from /user/details endpoint, uses default board if not specified)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        note: str = SchemaField(\n+            description=\"Private note for the pin (only visible to you and board collaborators)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        thumbnail: str = SchemaField(\n+            description=\"Required thumbnail URL for video pins (must have valid image Content-Type)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        carousel_options: list[PinterestCarouselOption] = SchemaField(\n+            description=\"Options for each image in carousel (title, link, description per image)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        alt_text: list[str] = SchemaField(\n+            description=\"Alt text for each image/video (max 500 chars each, accessibility feature)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"3ca46e05-dbaa-4afb-9e95-5a429c4177e6\",\n+            description=\"Post to Pinterest using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToPinterestBlock.Input,\n+            output_schema=PostToPinterestBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToPinterestBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to Pinterest with Pinterest-specific options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate Pinterest constraints\n+        if len(input_data.post) > 500:\n+            yield \"error\", f\"Pinterest pin description exceeds 500 character limit ({len(input_data.post)} characters)\"\n+            return\n+\n+        if len(input_data.pin_title) > 100:\n+            yield \"error\", f\"Pinterest pin title exceeds 100 character limit ({len(input_data.pin_title)} characters)\"\n+            return\n+\n+        if len(input_data.link) > 2048:\n+            yield \"error\", f\"Pinterest link URL exceeds 2048 character limit ({len(input_data.link)} characters)\"\n+            return\n+\n+        if len(input_data.media_urls) == 0:\n+            yield \"error\", \"Pinterest requires at least one image or video\"\n+            return\n+\n+        if len(input_data.media_urls) > 5:\n+            yield \"error\", \"Pinterest supports a maximum of 5 images in a carousel\"\n+            return\n+\n+        # Check if video is included and thumbnail is provided\n+        video_extensions = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\", \".webm\"]\n+        has_video = any(\n+            any(url.lower().endswith(ext) for ext in video_extensions)\n+            for url in input_data.media_urls\n+        )\n+\n+        if (has_video or input_data.is_video) and not input_data.thumbnail:\n+            yield \"error\", \"Pinterest video pins require a thumbnail URL\"\n+            return\n+\n+        # Validate alt text length\n+        for i, alt in enumerate(input_data.alt_text):\n+            if len(alt) > 500:\n+                yield \"error\", f\"Pinterest alt text {i+1} exceeds 500 character limit ({len(alt)} characters)\"\n+                return\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build Pinterest-specific options\n+        pinterest_options = {}\n+\n+        # Pin title\n+        if input_data.pin_title:\n+            pinterest_options[\"title\"] = input_data.pin_title\n+\n+        # Clickable link\n+        if input_data.link:\n+            pinterest_options[\"link\"] = input_data.link\n+\n+        # Board ID\n+        if input_data.board_id:\n+            pinterest_options[\"boardId\"] = input_data.board_id\n+\n+        # Private note\n+        if input_data.note:\n+            pinterest_options[\"note\"] = input_data.note\n+\n+        # Video thumbnail\n+        if input_data.thumbnail:\n+            pinterest_options[\"thumbNail\"] = input_data.thumbnail\n+\n+        # Carousel options\n+        if input_data.carousel_options:\n+            carousel_list = []\n+            for option in input_data.carousel_options:\n+                carousel_dict = {}\n+                if option.title:\n+                    carousel_dict[\"title\"] = option.title\n+                if option.link:\n+                    carousel_dict[\"link\"] = option.link\n+                if option.description:\n+                    carousel_dict[\"description\"] = option.description\n+                if carousel_dict:  # Only add if not empty\n+                    carousel_list.append(carousel_dict)\n+            if carousel_list:\n+                pinterest_options[\"carouselOptions\"] = carousel_list\n+\n+        # Alt text\n+        if input_data.alt_text:\n+            pinterest_options[\"altText\"] = input_data.alt_text\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.PINTEREST],\n+            media_urls=input_data.media_urls,\n+            is_video=input_data.is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            pinterest_options=pinterest_options if pinterest_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_reddit.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_reddit.py\nnew file mode 100644\nindex 000000000000..519e7d0d3e9c\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_reddit.py\n@@ -0,0 +1,69 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToRedditBlock(Block):\n+    \"\"\"Block for posting to Reddit.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for Reddit posts.\"\"\"\n+\n+        pass  # Uses all base fields\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"c7733580-3c72-483e-8e47-a8d58754d853\",\n+            description=\"Post to Reddit using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToRedditBlock.Input,\n+            output_schema=PostToRedditBlock.Output,\n+        )\n+\n+    async def run(\n+        self, input_data: \"PostToRedditBlock.Input\", *, profile_key: SecretStr, **kwargs\n+    ) -> BlockOutput:\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured.\"\n+            return\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.REDDIT],\n+            media_urls=input_data.media_urls,\n+            is_video=input_data.is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_snapchat.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_snapchat.py\nnew file mode 100644\nindex 000000000000..819899d5e116\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_snapchat.py\n@@ -0,0 +1,129 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToSnapchatBlock(Block):\n+    \"\"\"Block for posting to Snapchat with Snapchat-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for Snapchat posts.\"\"\"\n+\n+        # Override post field to include Snapchat-specific information\n+        post: str = SchemaField(\n+            description=\"The post text (optional for video-only content)\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include Snapchat-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Required video URL for Snapchat posts. Snapchat only supports video content.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # Snapchat-specific options\n+        story_type: str = SchemaField(\n+            description=\"Type of Snapchat content: 'story' (24-hour Stories), 'saved_story' (Saved Stories), or 'spotlight' (Spotlight posts)\",\n+            default=\"story\",\n+            advanced=True,\n+        )\n+        video_thumbnail: str = SchemaField(\n+            description=\"Thumbnail URL for video content (optional, auto-generated if not provided)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"a9d7f854-2c83-4e96-b3a1-7f2e9c5d4b8e\",\n+            description=\"Post to Snapchat using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToSnapchatBlock.Input,\n+            output_schema=PostToSnapchatBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToSnapchatBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to Snapchat with Snapchat-specific options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate Snapchat constraints\n+        if not input_data.media_urls:\n+            yield \"error\", \"Snapchat requires at least one video URL\"\n+            return\n+\n+        if len(input_data.media_urls) > 1:\n+            yield \"error\", \"Snapchat supports only one video per post\"\n+            return\n+\n+        # Validate story type\n+        valid_story_types = [\"story\", \"saved_story\", \"spotlight\"]\n+        if input_data.story_type not in valid_story_types:\n+            yield \"error\", f\"Snapchat story type must be one of: {', '.join(valid_story_types)}\"\n+            return\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build Snapchat-specific options\n+        snapchat_options = {}\n+\n+        # Story type\n+        if input_data.story_type != \"story\":\n+            snapchat_options[\"storyType\"] = input_data.story_type\n+\n+        # Video thumbnail\n+        if input_data.video_thumbnail:\n+            snapchat_options[\"videoThumbnail\"] = input_data.video_thumbnail\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.SNAPCHAT],\n+            media_urls=input_data.media_urls,\n+            is_video=True,  # Snapchat only supports video\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            snapchat_options=snapchat_options if snapchat_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_telegram.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_telegram.py\nnew file mode 100644\nindex 000000000000..7ecf3c38d37d\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_telegram.py\n@@ -0,0 +1,116 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToTelegramBlock(Block):\n+    \"\"\"Block for posting to Telegram with Telegram-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for Telegram posts.\"\"\"\n+\n+        # Override post field to include Telegram-specific information\n+        post: str = SchemaField(\n+            description=\"The post text (empty string allowed). Use @handle to mention other Telegram users.\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include Telegram-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Optional list of media URLs. For animated GIFs, only one URL is allowed. Telegram will auto-preview links unless image/video is included.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # Override is_video to include GIF-specific information\n+        is_video: bool = SchemaField(\n+            description=\"Whether the media is a video. Set to true for animated GIFs that don't end in .gif/.GIF extension.\",\n+            default=False,\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"47bc74eb-4af2-452c-b933-af377c7287df\",\n+            description=\"Post to Telegram using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToTelegramBlock.Input,\n+            output_schema=PostToTelegramBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToTelegramBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to Telegram with Telegram-specific validation.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate Telegram constraints\n+        # Check for animated GIFs - only one URL allowed\n+        gif_extensions = [\".gif\", \".GIF\"]\n+        has_gif = any(\n+            any(url.endswith(ext) for ext in gif_extensions)\n+            for url in input_data.media_urls\n+        )\n+\n+        if has_gif and len(input_data.media_urls) > 1:\n+            yield \"error\", \"Telegram animated GIFs support only one URL per post\"\n+            return\n+\n+        # Auto-detect if we need to set is_video for GIFs without proper extension\n+        detected_is_video = input_data.is_video\n+        if input_data.media_urls and not has_gif and not input_data.is_video:\n+            # Check if this might be a GIF without proper extension\n+            # This is just informational - user needs to set is_video manually\n+            pass\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.TELEGRAM],\n+            media_urls=input_data.media_urls,\n+            is_video=detected_is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_threads.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_threads.py\nnew file mode 100644\nindex 000000000000..ac3ad2984cc3\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_threads.py\n@@ -0,0 +1,111 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToThreadsBlock(Block):\n+    \"\"\"Block for posting to Threads with Threads-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for Threads posts.\"\"\"\n+\n+        # Override post field to include Threads-specific information\n+        post: str = SchemaField(\n+            description=\"The post text (max 500 chars, empty string allowed). Only 1 hashtag allowed. Use @handle to mention users.\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include Threads-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Optional list of media URLs. Supports up to 20 images/videos in a carousel. Auto-preview links unless media is included.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"f8c3b2e1-9d4a-4e5f-8c7b-6a9e8d2f1c3b\",\n+            description=\"Post to Threads using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToThreadsBlock.Input,\n+            output_schema=PostToThreadsBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToThreadsBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to Threads with Threads-specific validation.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate Threads constraints\n+        if len(input_data.post) > 500:\n+            yield \"error\", f\"Threads post text exceeds 500 character limit ({len(input_data.post)} characters)\"\n+            return\n+\n+        if len(input_data.media_urls) > 20:\n+            yield \"error\", \"Threads supports a maximum of 20 images/videos in a carousel\"\n+            return\n+\n+        # Count hashtags (only 1 allowed)\n+        hashtag_count = input_data.post.count(\"#\")\n+        if hashtag_count > 1:\n+            yield \"error\", f\"Threads allows only 1 hashtag per post ({hashtag_count} found)\"\n+            return\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build Threads-specific options\n+        threads_options = {}\n+        # Note: Based on the documentation, Threads doesn't seem to have specific options\n+        # beyond the standard ones. The main constraints are validation-based.\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.THREADS],\n+            media_urls=input_data.media_urls,\n+            is_video=input_data.is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            threads_options=threads_options if threads_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_tiktok.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_tiktok.py\nnew file mode 100644\nindex 000000000000..ae0f9dc4b03a\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_tiktok.py\n@@ -0,0 +1,243 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToTikTokBlock(Block):\n+    \"\"\"Block for posting to TikTok with TikTok-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for TikTok posts.\"\"\"\n+\n+        # Override post field to include TikTok-specific information\n+        post: str = SchemaField(\n+            description=\"The post text (max 2,200 chars, empty string allowed). Use @handle to mention users. Line breaks will be ignored.\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include TikTok-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Required media URLs. Either 1 video OR up to 35 images (JPG/JPEG/WEBP only). Cannot mix video and images.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # TikTok-specific options\n+        auto_add_music: bool = SchemaField(\n+            description=\"Automatically add recommended music to image posts\",\n+            default=False,\n+            advanced=True,\n+        )\n+        disable_comments: bool = SchemaField(\n+            description=\"Disable comments on the published post\",\n+            default=False,\n+            advanced=True,\n+        )\n+        disable_duet: bool = SchemaField(\n+            description=\"Disable duets on published video (video only)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        disable_stitch: bool = SchemaField(\n+            description=\"Disable stitch on published video (video only)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        is_ai_generated: bool = SchemaField(\n+            description=\"Label content as AI-generated (video only)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        is_branded_content: bool = SchemaField(\n+            description=\"Label as branded content (paid partnership)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        is_brand_organic: bool = SchemaField(\n+            description=\"Label as brand organic content (promotional)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        image_cover_index: int = SchemaField(\n+            description=\"Index of image to use as cover (0-based, image posts only)\",\n+            default=0,\n+            advanced=True,\n+        )\n+        title: str = SchemaField(\n+            description=\"Title for image posts\", default=\"\", advanced=True\n+        )\n+        thumbnail_offset: int = SchemaField(\n+            description=\"Video thumbnail frame offset in milliseconds (video only)\",\n+            default=0,\n+            advanced=True,\n+        )\n+        visibility: str = SchemaField(\n+            description=\"Post visibility: 'public', 'private', 'followers', or 'friends'\",\n+            default=\"public\",\n+            advanced=True,\n+        )\n+        draft: bool = SchemaField(\n+            description=\"Create as draft post (video only)\",\n+            default=False,\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"7faf4b27-96b0-4f05-bf64-e0de54ae74e1\",\n+            description=\"Post to TikTok using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToTikTokBlock.Input,\n+            output_schema=PostToTikTokBlock.Output,\n+        )\n+\n+    async def run(\n+        self, input_data: \"PostToTikTokBlock.Input\", *, profile_key: SecretStr, **kwargs\n+    ) -> BlockOutput:\n+        \"\"\"Post to TikTok with TikTok-specific validation and options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate TikTok constraints\n+        if len(input_data.post) > 2200:\n+            yield \"error\", f\"TikTok post text exceeds 2,200 character limit ({len(input_data.post)} characters)\"\n+            return\n+\n+        if not input_data.media_urls:\n+            yield \"error\", \"TikTok requires at least one media URL (either 1 video or up to 35 images)\"\n+            return\n+\n+        # Check for video vs image constraints\n+        video_extensions = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\", \".webm\"]\n+        image_extensions = [\".jpg\", \".jpeg\", \".webp\"]\n+\n+        has_video = input_data.is_video or any(\n+            any(url.lower().endswith(ext) for ext in video_extensions)\n+            for url in input_data.media_urls\n+        )\n+\n+        has_images = any(\n+            any(url.lower().endswith(ext) for ext in image_extensions)\n+            for url in input_data.media_urls\n+        )\n+\n+        if has_video and has_images:\n+            yield \"error\", \"TikTok does not support mixing video and images in the same post\"\n+            return\n+\n+        if has_video and len(input_data.media_urls) > 1:\n+            yield \"error\", \"TikTok supports only 1 video per post\"\n+            return\n+\n+        if has_images and len(input_data.media_urls) > 35:\n+            yield \"error\", \"TikTok supports a maximum of 35 images per post\"\n+            return\n+\n+        # Validate image cover index\n+        if has_images and input_data.image_cover_index >= len(input_data.media_urls):\n+            yield \"error\", f\"Image cover index {input_data.image_cover_index} is out of range (max: {len(input_data.media_urls) - 1})\"\n+            return\n+\n+        # Validate visibility option\n+        valid_visibility = [\"public\", \"private\", \"followers\", \"friends\"]\n+        if input_data.visibility not in valid_visibility:\n+            yield \"error\", f\"TikTok visibility must be one of: {', '.join(valid_visibility)}\"\n+            return\n+\n+        # Check for PNG files (not supported)\n+        has_png = any(url.lower().endswith(\".png\") for url in input_data.media_urls)\n+        if has_png:\n+            yield \"error\", \"TikTok does not support PNG files. Please use JPG, JPEG, or WEBP for images.\"\n+            return\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build TikTok-specific options\n+        tiktok_options = {}\n+\n+        # Common options\n+        if input_data.auto_add_music and has_images:\n+            tiktok_options[\"autoAddMusic\"] = True\n+\n+        if input_data.disable_comments:\n+            tiktok_options[\"disableComments\"] = True\n+\n+        if input_data.is_branded_content:\n+            tiktok_options[\"isBrandedContent\"] = True\n+\n+        if input_data.is_brand_organic:\n+            tiktok_options[\"isBrandOrganic\"] = True\n+\n+        # Video-specific options\n+        if has_video:\n+            if input_data.disable_duet:\n+                tiktok_options[\"disableDuet\"] = True\n+\n+            if input_data.disable_stitch:\n+                tiktok_options[\"disableStitch\"] = True\n+\n+            if input_data.is_ai_generated:\n+                tiktok_options[\"isAIGenerated\"] = True\n+\n+            if input_data.thumbnail_offset > 0:\n+                tiktok_options[\"thumbNailOffset\"] = input_data.thumbnail_offset\n+\n+            if input_data.draft:\n+                tiktok_options[\"draft\"] = True\n+\n+        # Image-specific options\n+        if has_images:\n+            if input_data.image_cover_index > 0:\n+                tiktok_options[\"imageCoverIndex\"] = input_data.image_cover_index\n+\n+            if input_data.title:\n+                tiktok_options[\"title\"] = input_data.title\n+\n+            if input_data.visibility != \"public\":\n+                tiktok_options[\"visibility\"] = input_data.visibility\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.TIKTOK],\n+            media_urls=input_data.media_urls,\n+            is_video=has_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            tiktok_options=tiktok_options if tiktok_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_x.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_x.py\nnew file mode 100644\nindex 000000000000..9c9fb760e6f5\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_x.py\n@@ -0,0 +1,241 @@\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToXBlock(Block):\n+    \"\"\"Block for posting to X / Twitter with Twitter-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for X / Twitter posts.\"\"\"\n+\n+        # Override post field to include X-specific information\n+        post: str = SchemaField(\n+            description=\"The post text (max 280 chars, up to 25,000 for Premium users). Use @handle to mention users. Use \\\\n\\\\n for thread breaks.\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include X-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Optional list of media URLs. X supports up to 4 images or videos per tweet. Auto-preview links unless media is included.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # X-specific options\n+        reply_to_id: str | None = SchemaField(\n+            description=\"ID of the tweet to reply to\",\n+            default=None,\n+            advanced=True,\n+        )\n+        quote_tweet_id: str | None = SchemaField(\n+            description=\"ID of the tweet to quote (low-level Tweet ID)\",\n+            default=None,\n+            advanced=True,\n+        )\n+        poll_options: list[str] = SchemaField(\n+            description=\"Poll options (2-4 choices)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        poll_duration: int = SchemaField(\n+            description=\"Poll duration in minutes (1-10080)\",\n+            default=1440,\n+            advanced=True,\n+        )\n+        alt_text: list[str] = SchemaField(\n+            description=\"Alt text for each image (max 1,000 chars each, not supported for videos)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        is_thread: bool = SchemaField(\n+            description=\"Whether to automatically break post into thread based on line breaks\",\n+            default=False,\n+            advanced=True,\n+        )\n+        thread_number: bool = SchemaField(\n+            description=\"Add thread numbers (1/n format) to each thread post\",\n+            default=False,\n+            advanced=True,\n+        )\n+        thread_media_urls: list[str] = SchemaField(\n+            description=\"Media URLs for thread posts (one per thread, use 'null' to skip)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        long_post: bool = SchemaField(\n+            description=\"Force long form post (requires Premium X account)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        long_video: bool = SchemaField(\n+            description=\"Enable long video upload (requires approval and Business/Enterprise plan)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        subtitle_url: str = SchemaField(\n+            description=\"URL to SRT subtitle file for videos (must be HTTPS and end in .srt)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        subtitle_language: str = SchemaField(\n+            description=\"Language code for subtitles (default: 'en')\",\n+            default=\"en\",\n+            advanced=True,\n+        )\n+        subtitle_name: str = SchemaField(\n+            description=\"Name of caption track (max 150 chars, default: 'English')\",\n+            default=\"English\",\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            id=\"9e8f844e-b4a5-4b25-80f2-9e1dd7d67625\",\n+            description=\"Post to X / Twitter using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToXBlock.Input,\n+            output_schema=PostToXBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToXBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to X / Twitter with enhanced X-specific options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate X constraints\n+        if not input_data.long_post and len(input_data.post) > 280:\n+            yield \"error\", f\"X post text exceeds 280 character limit ({len(input_data.post)} characters). Enable 'long_post' for Premium accounts.\"\n+            return\n+\n+        if input_data.long_post and len(input_data.post) > 25000:\n+            yield \"error\", f\"X long post text exceeds 25,000 character limit ({len(input_data.post)} characters)\"\n+            return\n+\n+        if len(input_data.media_urls) > 4:\n+            yield \"error\", \"X supports a maximum of 4 images or videos per tweet\"\n+            return\n+\n+        # Validate poll options\n+        if input_data.poll_options:\n+            if len(input_data.poll_options) < 2 or len(input_data.poll_options) > 4:\n+                yield \"error\", \"X polls require 2-4 options\"\n+                return\n+\n+            if input_data.poll_duration < 1 or input_data.poll_duration > 10080:\n+                yield \"error\", \"X poll duration must be between 1 and 10,080 minutes (7 days)\"\n+                return\n+\n+        # Validate alt text\n+        if input_data.alt_text:\n+            for i, alt in enumerate(input_data.alt_text):\n+                if len(alt) > 1000:\n+                    yield \"error\", f\"X alt text {i+1} exceeds 1,000 character limit ({len(alt)} characters)\"\n+                    return\n+\n+        # Validate subtitle settings\n+        if input_data.subtitle_url:\n+            if not input_data.subtitle_url.startswith(\n+                \"https://\"\n+            ) or not input_data.subtitle_url.endswith(\".srt\"):\n+                yield \"error\", \"Subtitle URL must start with https:// and end with .srt\"\n+                return\n+\n+            if len(input_data.subtitle_name) > 150:\n+                yield \"error\", f\"Subtitle name exceeds 150 character limit ({len(input_data.subtitle_name)} characters)\"\n+                return\n+\n+        # Convert datetime to ISO format if provided\n+        iso_date = (\n+            input_data.schedule_date.isoformat() if input_data.schedule_date else None\n+        )\n+\n+        # Build X-specific options\n+        twitter_options = {}\n+\n+        # Basic options\n+        if input_data.reply_to_id:\n+            twitter_options[\"replyToId\"] = input_data.reply_to_id\n+\n+        if input_data.quote_tweet_id:\n+            twitter_options[\"quoteTweetId\"] = input_data.quote_tweet_id\n+\n+        if input_data.long_post:\n+            twitter_options[\"longPost\"] = True\n+\n+        if input_data.long_video:\n+            twitter_options[\"longVideo\"] = True\n+\n+        # Poll options\n+        if input_data.poll_options:\n+            twitter_options[\"poll\"] = {\n+                \"duration\": input_data.poll_duration,\n+                \"options\": input_data.poll_options,\n+            }\n+\n+        # Alt text for images\n+        if input_data.alt_text:\n+            twitter_options[\"altText\"] = input_data.alt_text\n+\n+        # Thread options\n+        if input_data.is_thread:\n+            twitter_options[\"thread\"] = True\n+\n+            if input_data.thread_number:\n+                twitter_options[\"threadNumber\"] = True\n+\n+            if input_data.thread_media_urls:\n+                twitter_options[\"mediaUrls\"] = input_data.thread_media_urls\n+\n+        # Video subtitle options\n+        if input_data.subtitle_url:\n+            twitter_options[\"subTitleUrl\"] = input_data.subtitle_url\n+            twitter_options[\"subTitleLanguage\"] = input_data.subtitle_language\n+            twitter_options[\"subTitleName\"] = input_data.subtitle_name\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.TWITTER],\n+            media_urls=input_data.media_urls,\n+            is_video=input_data.is_video,\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            twitter_options=twitter_options if twitter_options else None,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/ayrshare/post_to_youtube.py b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_youtube.py\nnew file mode 100644\nindex 000000000000..16dc945da1ce\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/ayrshare/post_to_youtube.py\n@@ -0,0 +1,305 @@\n+from typing import Any\n+\n+from backend.integrations.ayrshare import PostIds, PostResponse, SocialPlatform\n+from backend.sdk import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockType,\n+    SchemaField,\n+    SecretStr,\n+)\n+\n+from ._util import BaseAyrshareInput, create_ayrshare_client\n+\n+\n+class PostToYouTubeBlock(Block):\n+    \"\"\"Block for posting to YouTube with YouTube-specific options.\"\"\"\n+\n+    class Input(BaseAyrshareInput):\n+        \"\"\"Input schema for YouTube posts.\"\"\"\n+\n+        # Override post field to include YouTube-specific information\n+        post: str = SchemaField(\n+            description=\"Video description (max 5,000 chars, empty string allowed). Cannot contain < or > characters.\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # Override media_urls to include YouTube-specific constraints\n+        media_urls: list[str] = SchemaField(\n+            description=\"Required video URL. YouTube only supports 1 video per post.\",\n+            default_factory=list,\n+            advanced=False,\n+        )\n+\n+        # YouTube-specific required options\n+        title: str = SchemaField(\n+            description=\"Video title (max 100 chars, required). Cannot contain < or > characters.\",\n+            default=\"\",\n+            advanced=False,\n+        )\n+\n+        # YouTube-specific optional options\n+        visibility: str = SchemaField(\n+            description=\"Video visibility: 'private' (default), 'public', or 'unlisted'\",\n+            default=\"private\",\n+            advanced=True,\n+        )\n+        thumbnail: str = SchemaField(\n+            description=\"Thumbnail URL (JPEG/PNG under 2MB, must end in .png/.jpg/.jpeg). Requires phone verification.\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        playlist_id: str = SchemaField(\n+            description=\"Playlist ID to add video (user must own playlist)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        tags: list[str] = SchemaField(\n+            description=\"Video tags (min 2 chars each, max 500 chars total)\",\n+            default_factory=list,\n+            advanced=True,\n+        )\n+        made_for_kids: bool = SchemaField(\n+            description=\"Self-declared kids content\", default=False, advanced=True\n+        )\n+        is_shorts: bool = SchemaField(\n+            description=\"Post as YouTube Short (max 3 minutes, adds #shorts)\",\n+            default=False,\n+            advanced=True,\n+        )\n+        notify_subscribers: bool = SchemaField(\n+            description=\"Send notification to subscribers\", default=True, advanced=True\n+        )\n+        category_id: int = SchemaField(\n+            description=\"Video category ID (e.g., 24 = Entertainment)\",\n+            default=0,\n+            advanced=True,\n+        )\n+        contains_synthetic_media: bool = SchemaField(\n+            description=\"Disclose realistic AI/synthetic content\",\n+            default=False,\n+            advanced=True,\n+        )\n+        publish_at: str = SchemaField(\n+            description=\"UTC publish time (YouTube controlled, format: 2022-10-08T21:18:36Z)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        # YouTube targeting options (flattened from YouTubeTargeting object)\n+        targeting_block_countries: list[str] | None = SchemaField(\n+            description=\"Country codes to block from viewing (e.g., ['US', 'CA'])\",\n+            default=None,\n+            advanced=True,\n+        )\n+        targeting_allow_countries: list[str] | None = SchemaField(\n+            description=\"Country codes to allow viewing (e.g., ['GB', 'AU'])\",\n+            default=None,\n+            advanced=True,\n+        )\n+        subtitle_url: str = SchemaField(\n+            description=\"URL to SRT or SBV subtitle file (must be HTTPS and end in .srt/.sbv, under 100MB)\",\n+            default=\"\",\n+            advanced=True,\n+        )\n+        subtitle_language: str = SchemaField(\n+            description=\"Language code for subtitles (default: 'en')\",\n+            default=\"en\",\n+            advanced=True,\n+        )\n+        subtitle_name: str = SchemaField(\n+            description=\"Name of caption track (max 150 chars, default: 'English')\",\n+            default=\"English\",\n+            advanced=True,\n+        )\n+\n+    class Output(BlockSchema):\n+        post_result: PostResponse = SchemaField(description=\"The result of the post\")\n+        post: PostIds = SchemaField(description=\"The result of the post\")\n+\n+    def __init__(self):\n+        super().__init__(\n+            disabled=True,\n+            id=\"0082d712-ff1b-4c3d-8a8d-6c7721883b83\",\n+            description=\"Post to YouTube using Ayrshare\",\n+            categories={BlockCategory.SOCIAL},\n+            block_type=BlockType.AYRSHARE,\n+            input_schema=PostToYouTubeBlock.Input,\n+            output_schema=PostToYouTubeBlock.Output,\n+        )\n+\n+    async def run(\n+        self,\n+        input_data: \"PostToYouTubeBlock.Input\",\n+        *,\n+        profile_key: SecretStr,\n+        **kwargs,\n+    ) -> BlockOutput:\n+        \"\"\"Post to YouTube with YouTube-specific validation and options.\"\"\"\n+        if not profile_key:\n+            yield \"error\", \"Please link a social account via Ayrshare\"\n+            return\n+\n+        client = create_ayrshare_client()\n+        if not client:\n+            yield \"error\", \"Ayrshare integration is not configured. Please set up the AYRSHARE_API_KEY.\"\n+            return\n+\n+        # Validate YouTube constraints\n+        if not input_data.title:\n+            yield \"error\", \"YouTube requires a video title\"\n+            return\n+\n+        if len(input_data.title) > 100:\n+            yield \"error\", f\"YouTube title exceeds 100 character limit ({len(input_data.title)} characters)\"\n+            return\n+\n+        if len(input_data.post) > 5000:\n+            yield \"error\", f\"YouTube description exceeds 5,000 character limit ({len(input_data.post)} characters)\"\n+            return\n+\n+        # Check for forbidden characters\n+        forbidden_chars = [\"<\", \">\"]\n+        for char in forbidden_chars:\n+            if char in input_data.title:\n+                yield \"error\", f\"YouTube title cannot contain '{char}' character\"\n+                return\n+            if char in input_data.post:\n+                yield \"error\", f\"YouTube description cannot contain '{char}' character\"\n+                return\n+\n+        if not input_data.media_urls:\n+            yield \"error\", \"YouTube requires exactly one video URL\"\n+            return\n+\n+        if len(input_data.media_urls) > 1:\n+            yield \"error\", \"YouTube supports only 1 video per post\"\n+            return\n+\n+        # Validate visibility option\n+        valid_visibility = [\"private\", \"public\", \"unlisted\"]\n+        if input_data.visibility not in valid_visibility:\n+            yield \"error\", f\"YouTube visibility must be one of: {', '.join(valid_visibility)}\"\n+            return\n+\n+        # Validate thumbnail URL format\n+        if input_data.thumbnail:\n+            valid_extensions = [\".png\", \".jpg\", \".jpeg\"]\n+            if not any(\n+                input_data.thumbnail.lower().endswith(ext) for ext in valid_extensions\n+            ):\n+                yield \"error\", \"YouTube thumbnail must end in .png, .jpg, or .jpeg\"\n+                return\n+\n+        # Validate tags\n+        if input_data.tags:\n+            total_tag_length = sum(len(tag) for tag in input_data.tags)\n+            if total_tag_length > 500:\n+                yield \"error\", f\"YouTube tags total length exceeds 500 characters ({total_tag_length} characters)\"\n+                return\n+\n+            for tag in input_data.tags:\n+                if len(tag) < 2:\n+                    yield \"error\", f\"YouTube tag '{tag}' is too short (minimum 2 characters)\"\n+                    return\n+\n+        # Validate subtitle URL\n+        if input_data.subtitle_url:\n+            if not input_data.subtitle_url.startswith(\"https://\"):\n+                yield \"error\", \"YouTube subtitle URL must start with https://\"\n+                return\n+\n+            valid_subtitle_extensions = [\".srt\", \".sbv\"]\n+            if not any(\n+                input_data.subtitle_url.lower().endswith(ext)\n+                for ext in valid_subtitle_extensions\n+            ):\n+                yield \"error\", \"YouTube subtitle URL must end in .srt or .sbv\"\n+                return\n+\n+        if len(input_data.subtitle_name) > 150:\n+            yield \"error\", f\"YouTube subtitle name exceeds 150 character limit ({len(input_data.subtitle_name)} characters)\"\n+            return\n+\n+        # Validate publish_at format if provided\n+        if input_data.publish_at and input_data.schedule_date:\n+            yield \"error\", \"Cannot use both 'publish_at' and 'schedule_date'. Use 'publish_at' for YouTube-controlled publishing.\"\n+            return\n+\n+        # Convert datetime to ISO format if provided (only if not using publish_at)\n+        iso_date = None\n+        if not input_data.publish_at and input_data.schedule_date:\n+            iso_date = input_data.schedule_date.isoformat()\n+\n+        # Build YouTube-specific options\n+        youtube_options: dict[str, Any] = {\"title\": input_data.title}\n+\n+        # Basic options\n+        if input_data.visibility != \"private\":\n+            youtube_options[\"visibility\"] = input_data.visibility\n+\n+        if input_data.thumbnail:\n+            youtube_options[\"thumbNail\"] = input_data.thumbnail\n+\n+        if input_data.playlist_id:\n+            youtube_options[\"playListId\"] = input_data.playlist_id\n+\n+        if input_data.tags:\n+            youtube_options[\"tags\"] = input_data.tags\n+\n+        if input_data.made_for_kids:\n+            youtube_options[\"madeForKids\"] = True\n+\n+        if input_data.is_shorts:\n+            youtube_options[\"shorts\"] = True\n+\n+        if not input_data.notify_subscribers:\n+            youtube_options[\"notifySubscribers\"] = False\n+\n+        if input_data.category_id > 0:\n+            youtube_options[\"categoryId\"] = input_data.category_id\n+\n+        if input_data.contains_synthetic_media:\n+            youtube_options[\"containsSyntheticMedia\"] = True\n+\n+        if input_data.publish_at:\n+            youtube_options[\"publishAt\"] = input_data.publish_at\n+\n+        # Country targeting (from flattened fields)\n+        targeting_dict = {}\n+        if input_data.targeting_block_countries:\n+            targeting_dict[\"block\"] = input_data.targeting_block_countries\n+        if input_data.targeting_allow_countries:\n+            targeting_dict[\"allow\"] = input_data.targeting_allow_countries\n+\n+        if targeting_dict:\n+            youtube_options[\"targeting\"] = targeting_dict\n+\n+        # Subtitle options\n+        if input_data.subtitle_url:\n+            youtube_options[\"subTitleUrl\"] = input_data.subtitle_url\n+            youtube_options[\"subTitleLanguage\"] = input_data.subtitle_language\n+            youtube_options[\"subTitleName\"] = input_data.subtitle_name\n+\n+        response = await client.create_post(\n+            post=input_data.post,\n+            platforms=[SocialPlatform.YOUTUBE],\n+            media_urls=input_data.media_urls,\n+            is_video=True,  # YouTube only supports videos\n+            schedule_date=iso_date,\n+            disable_comments=input_data.disable_comments,\n+            shorten_links=input_data.shorten_links,\n+            unsplash=input_data.unsplash,\n+            requires_approval=input_data.requires_approval,\n+            random_post=input_data.random_post,\n+            random_media_url=input_data.random_media_url,\n+            notes=input_data.notes,\n+            youtube_options=youtube_options,\n+            profile_key=profile_key.get_secret_value(),\n+        )\n+        yield \"post_result\", response\n+        if response.postIds:\n+            for p in response.postIds:\n+                yield \"post\", p\ndiff --git a/autogpt_platform/backend/backend/blocks/exa/answers.py b/autogpt_platform/backend/backend/blocks/exa/answers.py\nindex 237edccd650f..fa3f6b403f44 100644\n--- a/autogpt_platform/backend/backend/blocks/exa/answers.py\n+++ b/autogpt_platform/backend/backend/blocks/exa/answers.py\n@@ -119,6 +119,3 @@ async def run(\n \n         except Exception as e:\n             yield \"error\", str(e)\n-            yield \"answer\", \"\"\n-            yield \"citations\", []\n-            yield \"cost_dollars\", {}\ndiff --git a/autogpt_platform/backend/backend/data/block.py b/autogpt_platform/backend/backend/data/block.py\nindex 5f28fefc879d..2d77cb04ebee 100644\n--- a/autogpt_platform/backend/backend/data/block.py\n+++ b/autogpt_platform/backend/backend/data/block.py\n@@ -55,6 +55,7 @@ class BlockType(Enum):\n     WEBHOOK_MANUAL = \"Webhook (manual)\"\n     AGENT = \"Agent\"\n     AI = \"AI\"\n+    AYRSHARE = \"Ayrshare\"\n \n \n class BlockCategory(Enum):\ndiff --git a/autogpt_platform/backend/backend/data/model.py b/autogpt_platform/backend/backend/data/model.py\nindex 17ba987f14f1..c750b80482b9 100644\n--- a/autogpt_platform/backend/backend/data/model.py\n+++ b/autogpt_platform/backend/backend/data/model.py\n@@ -14,7 +14,6 @@\n     Generic,\n     Literal,\n     Optional,\n-    TypedDict,\n     TypeVar,\n     cast,\n     get_args,\n@@ -38,6 +37,7 @@\n     ValidationError,\n     core_schema,\n )\n+from typing_extensions import TypedDict\n \n from backend.integrations.providers import ProviderName\n from backend.util.settings import Secrets\n@@ -316,15 +316,32 @@ class OAuthState(BaseModel):\n \n class UserMetadata(BaseModel):\n     integration_credentials: list[Credentials] = Field(default_factory=list)\n+    \"\"\" Deprecated; use `UserIntegrations.credentials` instead\"\"\"\n     integration_oauth_states: list[OAuthState] = Field(default_factory=list)\n+    \"\"\" Deprecated; use `UserIntegrations.oauth_states` instead\"\"\"\n \n \n class UserMetadataRaw(TypedDict, total=False):\n     integration_credentials: list[dict]\n+    \"\"\" Deprecated; use `UserIntegrations.credentials` instead\"\"\"\n     integration_oauth_states: list[dict]\n+    \"\"\" Deprecated; use `UserIntegrations.oauth_states` instead\"\"\"\n \n \n class UserIntegrations(BaseModel):\n+\n+    class ManagedCredentials(BaseModel):\n+        \"\"\"Integration credentials managed by us, rather than by the user\"\"\"\n+\n+        ayrshare_profile_key: Optional[SecretStr] = None\n+\n+        @field_serializer(\"*\")\n+        def dump_secret_strings(value: Any, _info):\n+            if isinstance(value, SecretStr):\n+                return value.get_secret_value()\n+            return value\n+\n+    managed_credentials: ManagedCredentials = Field(default_factory=ManagedCredentials)\n     credentials: list[Credentials] = Field(default_factory=list)\n     oauth_states: list[OAuthState] = Field(default_factory=list)\n \ndiff --git a/autogpt_platform/backend/backend/executor/manager.py b/autogpt_platform/backend/backend/executor/manager.py\nindex df246abb05a3..c9797ec77791 100644\n--- a/autogpt_platform/backend/backend/executor/manager.py\n+++ b/autogpt_platform/backend/backend/executor/manager.py\n@@ -35,6 +35,7 @@\n from prometheus_client import Gauge, start_http_server\n \n from backend.blocks.agent import AgentExecutorBlock\n+from backend.blocks.ayrshare import AYRSHARE_BLOCK_IDS\n from backend.data import redis_client as redis\n from backend.data.block import (\n     BlockData,\n@@ -182,6 +183,17 @@ async def execute_node(\n         )\n         extra_exec_kwargs[field_name] = credentials\n \n+    if node_block.id in AYRSHARE_BLOCK_IDS:\n+        profile_key = await creds_manager.store.get_ayrshare_profile_key(user_id)\n+        if not profile_key:\n+            logger.error(\n+                \"Ayrshare profile not configured. Please link a social account via Ayrshare integration first.\"\n+            )\n+            raise ValueError(\n+                \"Ayrshare profile not configured. Please link a social account via Ayrshare integration first.\"\n+            )\n+        extra_exec_kwargs[\"profile_key\"] = profile_key\n+\n     output_size = 0\n     try:\n         async for output_name, output_data in node_block.execute(\ndiff --git a/autogpt_platform/backend/backend/integrations/ayrshare.py b/autogpt_platform/backend/backend/integrations/ayrshare.py\nnew file mode 100644\nindex 000000000000..42e069b4aca5\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/integrations/ayrshare.py\n@@ -0,0 +1,515 @@\n+from __future__ import annotations\n+\n+import json\n+import logging\n+from enum import Enum\n+from typing import Any, Optional\n+\n+from pydantic import BaseModel\n+\n+from backend.util.exceptions import MissingConfigError\n+from backend.util.request import Requests\n+from backend.util.settings import Settings\n+\n+logger = logging.getLogger(__name__)\n+\n+settings = Settings()\n+\n+\n+class AyrshareAPIException(Exception):\n+    def __init__(self, message: str, status_code: int):\n+        super().__init__(message)\n+        self.status_code = status_code\n+\n+\n+class SocialPlatform(str, Enum):\n+    BLUESKY = \"bluesky\"\n+    FACEBOOK = \"facebook\"\n+    TWITTER = \"twitter\"\n+    LINKEDIN = \"linkedin\"\n+    INSTAGRAM = \"instagram\"\n+    YOUTUBE = \"youtube\"\n+    REDDIT = \"reddit\"\n+    TELEGRAM = \"telegram\"\n+    GOOGLE_MY_BUSINESS = \"gmb\"\n+    PINTEREST = \"pinterest\"\n+    TIKTOK = \"tiktok\"\n+    SNAPCHAT = \"snapchat\"\n+    THREADS = \"threads\"\n+\n+\n+class EmailConfig(BaseModel):\n+    to: str\n+    subject: Optional[str] = None\n+    body: Optional[str] = None\n+    from_name: Optional[str] = None\n+    from_email: Optional[str] = None\n+\n+\n+class JWTResponse(BaseModel):\n+    status: str\n+    title: str\n+    token: str\n+    url: str\n+    emailSent: Optional[bool] = None\n+    expiresIn: Optional[str] = None\n+\n+\n+class ProfileResponse(BaseModel):\n+    status: str\n+    title: str\n+    refId: str\n+    profileKey: str\n+    messagingActive: Optional[bool] = None\n+\n+\n+class PostResponse(BaseModel):\n+    status: str\n+    id: str\n+    refId: str\n+    profileTitle: str\n+    post: str\n+    postIds: Optional[list[PostIds]] = None\n+    scheduleDate: Optional[str] = None\n+    errors: Optional[list[str]] = None\n+\n+\n+class PostIds(BaseModel):\n+    status: str\n+    id: str\n+    postUrl: str\n+    platform: str\n+\n+\n+class AutoHashtag(BaseModel):\n+    max: Optional[int] = None\n+    position: Optional[str] = None\n+\n+\n+class FirstComment(BaseModel):\n+    text: str\n+    platforms: Optional[list[SocialPlatform]] = None\n+\n+\n+class AutoSchedule(BaseModel):\n+    interval: str\n+    platforms: Optional[list[SocialPlatform]] = None\n+    startDate: Optional[str] = None\n+    endDate: Optional[str] = None\n+\n+\n+class AutoRepost(BaseModel):\n+    interval: str\n+    platforms: Optional[list[SocialPlatform]] = None\n+    startDate: Optional[str] = None\n+    endDate: Optional[str] = None\n+\n+\n+class AyrshareClient:\n+    \"\"\"Client for the Ayrshare Social Media Post API\"\"\"\n+\n+    API_URL = \"https://api.ayrshare.com/api\"\n+    POST_ENDPOINT = f\"{API_URL}/post\"\n+    PROFILES_ENDPOINT = f\"{API_URL}/profiles\"\n+    JWT_ENDPOINT = f\"{PROFILES_ENDPOINT}/generateJWT\"\n+\n+    def __init__(\n+        self,\n+        custom_requests: Optional[Requests] = None,\n+    ):\n+        if not settings.secrets.ayrshare_api_key:\n+            raise MissingConfigError(\"AYRSHARE_API_KEY is not configured\")\n+\n+        headers: dict[str, str] = {\n+            \"Content-Type\": \"application/json\",\n+            \"Authorization\": f\"Bearer {settings.secrets.ayrshare_api_key}\",\n+        }\n+        self.headers = headers\n+\n+        if custom_requests:\n+            self._requests = custom_requests\n+        else:\n+            self._requests = Requests(\n+                extra_headers=headers,\n+                trusted_origins=[\"https://api.ayrshare.com\"],\n+            )\n+\n+    async def generate_jwt(\n+        self,\n+        private_key: str,\n+        profile_key: str,\n+        logout: Optional[bool] = None,\n+        redirect: Optional[str] = None,\n+        allowed_social: Optional[list[SocialPlatform]] = None,\n+        verify: Optional[bool] = None,\n+        base64: Optional[bool] = None,\n+        expires_in: Optional[int] = None,\n+        email: Optional[EmailConfig] = None,\n+    ) -> JWTResponse:\n+        \"\"\"\n+        Generate a JSON Web Token (JWT) for use with single sign on.\n+\n+        Docs: https://www.ayrshare.com/docs/apis/profiles/generate-jwt-overview\n+\n+        Args:\n+            domain: Domain of app. Must match the domain given during onboarding.\n+            private_key: Private Key used for encryption.\n+            profile_key: User Profile Key (not the API Key).\n+            logout: Automatically logout the current session.\n+            redirect: URL to redirect to when the \"Done\" button or logo is clicked.\n+            allowed_social: List of social networks to display in the linking page.\n+            verify: Verify that the generated token is valid (recommended for non-production).\n+            base64: Whether the private key is base64 encoded.\n+            expires_in: Token longevity in minutes (1-2880).\n+            email: Configuration for sending Connect Accounts email.\n+\n+        Returns:\n+            JWTResponse object containing the JWT token and URL.\n+\n+        Raises:\n+            AyrshareAPIException: If the API request fails or private key is invalid.\n+        \"\"\"\n+        payload: dict[str, Any] = {\n+            \"domain\": \"id-pojeg\",\n+            \"privateKey\": private_key,\n+            \"profileKey\": profile_key,\n+        }\n+\n+        headers = self.headers\n+        headers[\"Profile-Key\"] = profile_key\n+        if logout is not None:\n+            payload[\"logout\"] = logout\n+        if redirect is not None:\n+            payload[\"redirect\"] = redirect\n+        if allowed_social is not None:\n+            payload[\"allowedSocial\"] = [p.value for p in allowed_social]\n+        if verify is not None:\n+            payload[\"verify\"] = verify\n+        if base64 is not None:\n+            payload[\"base64\"] = base64\n+        if expires_in is not None:\n+            payload[\"expiresIn\"] = expires_in\n+        if email is not None:\n+            payload[\"email\"] = email.model_dump(exclude_none=True)\n+\n+        response = await self._requests.post(\n+            self.JWT_ENDPOINT, json=payload, headers=headers\n+        )\n+\n+        if not response.ok:\n+            try:\n+                error_data = response.json()\n+                error_message = error_data.get(\"message\", \"Unknown error\")\n+            except json.JSONDecodeError:\n+                error_message = response.text()\n+\n+            raise AyrshareAPIException(\n+                f\"Ayrshare API request failed ({response.status}): {error_message}\",\n+                response.status,\n+            )\n+\n+        response_data = response.json()\n+        if response_data.get(\"status\") != \"success\":\n+            raise AyrshareAPIException(\n+                f\"Ayrshare API returned error: {response_data.get('message', 'Unknown error')}\",\n+                response.status,\n+            )\n+\n+        return JWTResponse(**response_data)\n+\n+    async def create_profile(\n+        self,\n+        title: str,\n+        messaging_active: Optional[bool] = None,\n+        hide_top_header: Optional[bool] = None,\n+        top_header: Optional[str] = None,\n+        disable_social: Optional[list[SocialPlatform]] = None,\n+        team: Optional[bool] = None,\n+        email: Optional[str] = None,\n+        sub_header: Optional[str] = None,\n+        tags: Optional[list[str]] = None,\n+    ) -> ProfileResponse:\n+        \"\"\"\n+        Create a new User Profile under your Primary Profile.\n+\n+        Docs: https://www.ayrshare.com/docs/apis/profiles/create-profile\n+\n+        Args:\n+            title: Title of the new profile. Must be unique.\n+            messaging_active: Set to true to activate messaging for this user profile.\n+            hide_top_header: Hide the top header on the social accounts linkage page.\n+            top_header: Change the header on the social accounts linkage page.\n+            disable_social: Array of social networks that are disabled for this user's profile.\n+            team: Create a new user profile as a team member.\n+            email: Email address for team member invite (required if team is true).\n+            sub_header: Change the sub header on the social accounts linkage page.\n+            tags: Array of strings to tag user profiles.\n+\n+        Returns:\n+            ProfileResponse object containing the profile details and profile key.\n+\n+        Raises:\n+            AyrshareAPIException: If the API request fails or profile title already exists.\n+        \"\"\"\n+        payload: dict[str, Any] = {\n+            \"title\": title,\n+        }\n+\n+        if messaging_active is not None:\n+            payload[\"messagingActive\"] = messaging_active\n+        if hide_top_header is not None:\n+            payload[\"hideTopHeader\"] = hide_top_header\n+        if top_header is not None:\n+            payload[\"topHeader\"] = top_header\n+        if disable_social is not None:\n+            payload[\"disableSocial\"] = [p.value for p in disable_social]\n+        if team is not None:\n+            payload[\"team\"] = team\n+        if email is not None:\n+            payload[\"email\"] = email\n+        if sub_header is not None:\n+            payload[\"subHeader\"] = sub_header\n+        if tags is not None:\n+            payload[\"tags\"] = tags\n+\n+        response = await self._requests.post(self.PROFILES_ENDPOINT, json=payload)\n+\n+        if not response.ok:\n+            try:\n+                error_data = response.json()\n+                error_message = error_data.get(\"message\", \"Unknown error\")\n+            except json.JSONDecodeError:\n+                error_message = response.text()\n+\n+            raise AyrshareAPIException(\n+                f\"Ayrshare API request failed ({response.status}): {error_message}\",\n+                response.status,\n+            )\n+\n+        response_data = response.json()\n+        if response_data.get(\"status\") != \"success\":\n+            raise AyrshareAPIException(\n+                f\"Ayrshare API returned error: {response_data.get('message', 'Unknown error')}\",\n+                response.status,\n+            )\n+\n+        return ProfileResponse(**response_data)\n+\n+    async def create_post(\n+        self,\n+        post: str,\n+        platforms: list[SocialPlatform],\n+        *,\n+        media_urls: Optional[list[str]] = None,\n+        is_video: Optional[bool] = None,\n+        schedule_date: Optional[str] = None,\n+        validate_schedule: Optional[bool] = None,\n+        first_comment: Optional[FirstComment] = None,\n+        disable_comments: Optional[bool] = None,\n+        shorten_links: Optional[bool] = None,\n+        auto_schedule: Optional[AutoSchedule] = None,\n+        auto_repost: Optional[AutoRepost] = None,\n+        auto_hashtag: Optional[AutoHashtag | bool] = None,\n+        unsplash: Optional[str] = None,\n+        bluesky_options: Optional[dict[str, Any]] = None,\n+        facebook_options: Optional[dict[str, Any]] = None,\n+        gmb_options: Optional[dict[str, Any]] = None,\n+        instagram_options: Optional[dict[str, Any]] = None,\n+        linkedin_options: Optional[dict[str, Any]] = None,\n+        pinterest_options: Optional[dict[str, Any]] = None,\n+        reddit_options: Optional[dict[str, Any]] = None,\n+        snapchat_options: Optional[dict[str, Any]] = None,\n+        telegram_options: Optional[dict[str, Any]] = None,\n+        threads_options: Optional[dict[str, Any]] = None,\n+        tiktok_options: Optional[dict[str, Any]] = None,\n+        twitter_options: Optional[dict[str, Any]] = None,\n+        youtube_options: Optional[dict[str, Any]] = None,\n+        requires_approval: Optional[bool] = None,\n+        random_post: Optional[bool] = None,\n+        random_media_url: Optional[bool] = None,\n+        idempotency_key: Optional[str] = None,\n+        notes: Optional[str] = None,\n+        profile_key: Optional[str] = None,\n+    ) -> PostResponse:\n+        \"\"\"\n+        Create a post across multiple social media platforms.\n+\n+        Docs: https://www.ayrshare.com/docs/apis/post/post\n+\n+        Args:\n+            post: The post text to be published - required\n+            platforms: List of platforms to post to (e.g. [SocialPlatform.TWITTER, SocialPlatform.FACEBOOK]) - required\n+            media_urls: Optional list of media URLs to include - required if is_video is true\n+            is_video: Whether the media is a video - default is false (in api docs)\n+            schedule_date: UTC datetime for scheduling (YYYY-MM-DDThh:mm:ssZ) - default is None (in api docs)\n+            validate_schedule: Whether to validate the schedule date - default is false (in api docs)\n+            first_comment: Configuration for first comment - default is None (in api docs)\n+            disable_comments: Whether to disable comments - default is false (in api docs)\n+            shorten_links: Whether to shorten links - default is false (in api docs)\n+            auto_schedule: Configuration for automatic scheduling - default is None (in api docs https://www.ayrshare.com/docs/apis/auto-schedule/overview)\n+            auto_repost: Configuration for automatic reposting - default is None (in api docs https://www.ayrshare.com/docs/apis/post/overview#auto-repost)\n+            auto_hashtag: Configuration for automatic hashtags - default is None (in api docs https://www.ayrshare.com/docs/apis/post/overview#auto-hashtags)\n+            unsplash: Unsplash image configuration - default is None (in api docs https://www.ayrshare.com/docs/apis/post/overview#unsplash)\n+\n+            ------------------------------------------------------------\n+\n+            bluesky_options: Bluesky-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/bluesky\n+            facebook_options: Facebook-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/facebook\n+            gmb_options: Google Business Profile options - https://www.ayrshare.com/docs/apis/post/social-networks/google\n+            instagram_options: Instagram-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/instagram\n+            linkedin_options: LinkedIn-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/linkedin\n+            pinterest_options: Pinterest-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/pinterest\n+            reddit_options: Reddit-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/reddit\n+            snapchat_options: Snapchat-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/snapchat\n+            telegram_options: Telegram-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/telegram\n+            threads_options: Threads-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/threads\n+            tiktok_options: TikTok-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/tiktok\n+            twitter_options: Twitter-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/twitter\n+            youtube_options: YouTube-specific options - https://www.ayrshare.com/docs/apis/post/social-networks/youtube\n+\n+            ------------------------------------------------------------\n+\n+\n+            requires_approval: Whether to enable approval workflow - default is false (in api docs)\n+            random_post: Whether to generate random post text - default is false (in api docs)\n+            random_media_url: Whether to generate random media - default is false (in api docs)\n+            idempotency_key: Unique ID for the post - default is None (in api docs)\n+            notes: Additional notes for the post - default is None (in api docs)\n+\n+        Returns:\n+            PostResponse object containing the post details and status\n+\n+        Raises:\n+            AyrshareAPIException: If the API request fails\n+        \"\"\"\n+\n+        payload: dict[str, Any] = {\n+            \"post\": post,\n+            \"platforms\": [p.value for p in platforms],\n+        }\n+\n+        # Add optional parameters if provided\n+        if media_urls:\n+            payload[\"mediaUrls\"] = media_urls\n+        if is_video is not None:\n+            payload[\"isVideo\"] = is_video\n+        if schedule_date:\n+            payload[\"scheduleDate\"] = schedule_date\n+        if validate_schedule is not None:\n+            payload[\"validateSchedule\"] = validate_schedule\n+        if first_comment:\n+            first_comment_dict = first_comment.model_dump(exclude_none=True)\n+            if first_comment.platforms:\n+                first_comment_dict[\"platforms\"] = [\n+                    p.value for p in first_comment.platforms\n+                ]\n+            payload[\"firstComment\"] = first_comment_dict\n+        if disable_comments is not None:\n+            payload[\"disableComments\"] = disable_comments\n+        if shorten_links is not None:\n+            payload[\"shortenLinks\"] = shorten_links\n+        if auto_schedule:\n+            auto_schedule_dict = auto_schedule.model_dump(exclude_none=True)\n+            if auto_schedule.platforms:\n+                auto_schedule_dict[\"platforms\"] = [\n+                    p.value for p in auto_schedule.platforms\n+                ]\n+            payload[\"autoSchedule\"] = auto_schedule_dict\n+        if auto_repost:\n+            auto_repost_dict = auto_repost.model_dump(exclude_none=True)\n+            if auto_repost.platforms:\n+                auto_repost_dict[\"platforms\"] = [p.value for p in auto_repost.platforms]\n+            payload[\"autoRepost\"] = auto_repost_dict\n+        if auto_hashtag:\n+            payload[\"autoHashtag\"] = (\n+                auto_hashtag.model_dump(exclude_none=True)\n+                if isinstance(auto_hashtag, AutoHashtag)\n+                else auto_hashtag\n+            )\n+        if unsplash:\n+            payload[\"unsplash\"] = unsplash\n+        if bluesky_options:\n+            payload[\"blueskyOptions\"] = bluesky_options\n+        if facebook_options:\n+            payload[\"faceBookOptions\"] = facebook_options\n+        if gmb_options:\n+            payload[\"gmbOptions\"] = gmb_options\n+        if instagram_options:\n+            payload[\"instagramOptions\"] = instagram_options\n+        if linkedin_options:\n+            payload[\"linkedInOptions\"] = linkedin_options\n+        if pinterest_options:\n+            payload[\"pinterestOptions\"] = pinterest_options\n+        if reddit_options:\n+            payload[\"redditOptions\"] = reddit_options\n+        if snapchat_options:\n+            payload[\"snapchatOptions\"] = snapchat_options\n+        if telegram_options:\n+            payload[\"telegramOptions\"] = telegram_options\n+        if threads_options:\n+            payload[\"threadsOptions\"] = threads_options\n+        if tiktok_options:\n+            payload[\"tikTokOptions\"] = tiktok_options\n+        if twitter_options:\n+            payload[\"twitterOptions\"] = twitter_options\n+        if youtube_options:\n+            payload[\"youTubeOptions\"] = youtube_options\n+        if requires_approval is not None:\n+            payload[\"requiresApproval\"] = requires_approval\n+        if random_post is not None:\n+            payload[\"randomPost\"] = random_post\n+        if random_media_url is not None:\n+            payload[\"randomMediaUrl\"] = random_media_url\n+        if idempotency_key:\n+            payload[\"idempotencyKey\"] = idempotency_key\n+        if notes:\n+            payload[\"notes\"] = notes\n+\n+        headers = self.headers\n+        if profile_key:\n+            headers[\"Profile-Key\"] = profile_key\n+\n+        response = await self._requests.post(\n+            self.POST_ENDPOINT, json=payload, headers=headers\n+        )\n+        logger.warning(f\"Ayrshare request: {payload} and headers: {headers}\")\n+        if not response.ok:\n+            logger.error(\n+                f\"Ayrshare API request failed ({response.status}): {response.text()}\"\n+            )\n+            try:\n+                error_data = response.json()\n+                error_message = error_data.get(\"message\", \"Unknown error\")\n+            except json.JSONDecodeError:\n+                error_message = response.text()\n+\n+            raise AyrshareAPIException(\n+                f\"Ayrshare API request failed ({response.status}): {error_message}\",\n+                response.status,\n+            )\n+\n+        response_data = response.json()\n+        if response_data.get(\"status\") != \"success\":\n+            logger.error(\n+                f\"Ayrshare API returned error: {response_data.get('message', 'Unknown error')}\"\n+            )\n+            raise AyrshareAPIException(\n+                f\"Ayrshare API returned error: {response_data.get('message', 'Unknown error')}\",\n+                response.status,\n+            )\n+\n+        # Ayrshare returns an array of posts even for single posts\n+        # It seems like there is only ever one post in the array, and within that\n+        # there are multiple postIds\n+\n+        # There is a seperate endpoint for bulk posting, so feels safe to just take\n+        # the first post from the array\n+\n+        if len(response_data[\"posts\"]) == 0:\n+            logger.error(\"Ayrshare API returned no posts\")\n+            raise AyrshareAPIException(\n+                \"Ayrshare API returned no posts\",\n+                response.status,\n+            )\n+        logger.warn(f\"Ayrshare API returned posts: {response_data['posts']}\")\n+        return PostResponse(**response_data[\"posts\"][0])\ndiff --git a/autogpt_platform/backend/backend/integrations/credentials_store.py b/autogpt_platform/backend/backend/integrations/credentials_store.py\nindex f321da82fabf..033bfa97fe33 100644\n--- a/autogpt_platform/backend/backend/integrations/credentials_store.py\n+++ b/autogpt_platform/backend/backend/integrations/credentials_store.py\n@@ -1,6 +1,7 @@\n import base64\n import hashlib\n import secrets\n+from contextlib import asynccontextmanager\n from datetime import datetime, timedelta, timezone\n from typing import Optional\n \n@@ -240,6 +241,7 @@ def db_manager(self):\n \n             return get_service_client(DatabaseManagerAsyncClient)\n \n+    # =============== USER-MANAGED CREDENTIALS =============== #\n     async def add_creds(self, user_id: str, credentials: Credentials) -> None:\n         async with await self.locked_user_integrations(user_id):\n             if await self.get_creds_by_id(user_id, credentials.id):\n@@ -359,6 +361,39 @@ async def delete_creds_by_id(self, user_id: str, credentials_id: str) -> None:\n             ]\n             await self._set_user_integration_creds(user_id, filtered_credentials)\n \n+    # ============== SYSTEM-MANAGED CREDENTIALS ============== #\n+\n+    async def get_ayrshare_profile_key(self, user_id: str) -> SecretStr | None:\n+        \"\"\"Get the Ayrshare profile key for a user.\n+\n+        The profile key is used to authenticate API requests to Ayrshare's social media posting service.\n+        See https://www.ayrshare.com/docs/apis/profiles/overview for more details.\n+\n+        Args:\n+            user_id: The ID of the user to get the profile key for\n+\n+        Returns:\n+            The profile key as a SecretStr if set, None otherwise\n+        \"\"\"\n+        user_integrations = await self._get_user_integrations(user_id)\n+        return user_integrations.managed_credentials.ayrshare_profile_key\n+\n+    async def set_ayrshare_profile_key(self, user_id: str, profile_key: str) -> None:\n+        \"\"\"Set the Ayrshare profile key for a user.\n+\n+        The profile key is used to authenticate API requests to Ayrshare's social media posting service.\n+        See https://www.ayrshare.com/docs/apis/profiles/overview for more details.\n+\n+        Args:\n+            user_id: The ID of the user to set the profile key for\n+            profile_key: The profile key to set\n+        \"\"\"\n+        _profile_key = SecretStr(profile_key)\n+        async with self.edit_user_integrations(user_id) as user_integrations:\n+            user_integrations.managed_credentials.ayrshare_profile_key = _profile_key\n+\n+    # ===================== OAUTH STATES ===================== #\n+\n     async def store_state_token(\n         self, user_id: str, provider: str, scopes: list[str], use_pkce: bool = False\n     ) -> tuple[str, str]:\n@@ -375,6 +410,9 @@ async def store_state_token(\n             scopes=scopes,\n         )\n \n+        async with self.edit_user_integrations(user_id) as user_integrations:\n+            user_integrations.oauth_states.append(state)\n+\n         async with await self.locked_user_integrations(user_id):\n \n             user_integrations = await self._get_user_integrations(user_id)\n@@ -428,6 +466,17 @@ async def verify_state_token(\n \n         return None\n \n+    # =================== GET/SET HELPERS =================== #\n+\n+    @asynccontextmanager\n+    async def edit_user_integrations(self, user_id: str):\n+        async with await self.locked_user_integrations(user_id):\n+            user_integrations = await self._get_user_integrations(user_id)\n+            yield user_integrations  # yield to allow edits\n+            await self.db_manager.update_user_integrations(\n+                user_id=user_id, data=user_integrations\n+            )\n+\n     async def _set_user_integration_creds(\n         self, user_id: str, credentials: list[Credentials]\n     ) -> None:\ndiff --git a/autogpt_platform/backend/backend/server/integrations/router.py b/autogpt_platform/backend/backend/server/integrations/router.py\nindex 0e71d7f0ceb4..c54a6fb8248e 100644\n--- a/autogpt_platform/backend/backend/server/integrations/router.py\n+++ b/autogpt_platform/backend/backend/server/integrations/router.py\n@@ -1,5 +1,6 @@\n import asyncio\n import logging\n+from datetime import datetime, timedelta, timezone\n from typing import TYPE_CHECKING, Annotated, Awaitable, List, Literal\n \n from fastapi import (\n@@ -12,7 +13,8 @@\n     Request,\n     status,\n )\n-from pydantic import BaseModel, Field\n+from pydantic import BaseModel, Field, SecretStr\n+from starlette.status import HTTP_500_INTERNAL_SERVER_ERROR, HTTP_502_BAD_GATEWAY\n \n from backend.data.graph import get_graph, set_node_webhook\n from backend.data.integrations import (\n@@ -29,6 +31,7 @@\n     OAuth2Credentials,\n )\n from backend.executor.utils import add_graph_execution\n+from backend.integrations.ayrshare import AyrshareClient, SocialPlatform\n from backend.integrations.creds_manager import IntegrationCredentialsManager\n from backend.integrations.oauth import CREDENTIALS_BY_PROVIDER, HANDLERS_BY_NAME\n from backend.integrations.providers import ProviderName\n@@ -39,7 +42,7 @@\n     get_all_provider_names,\n )\n from backend.server.v2.library.db import set_preset_webhook, update_preset\n-from backend.util.exceptions import NeedConfirmation, NotFoundError\n+from backend.util.exceptions import MissingConfigError, NeedConfirmation, NotFoundError\n from backend.util.settings import Settings\n \n if TYPE_CHECKING:\n@@ -271,6 +274,11 @@ class CredentialsDeletionNeedsConfirmationResponse(BaseModel):\n     message: str\n \n \n+class AyrshareSSOResponse(BaseModel):\n+    sso_url: str = Field(..., description=\"The SSO URL for Ayrshare integration\")\n+    expires_at: datetime = Field(..., description=\"ISO timestamp when the URL expires\")\n+\n+\n @router.delete(\"/{provider}/credentials/{cred_id}\")\n async def delete_credentials(\n     request: Request,\n@@ -548,9 +556,90 @@ def _get_provider_oauth_handler(\n     )\n \n \n-# === PROVIDER DISCOVERY ENDPOINTS ===\n+@router.get(\"/ayrshare/sso_url\")\n+async def get_ayrshare_sso_url(\n+    user_id: Annotated[str, Depends(get_user_id)],\n+) -> AyrshareSSOResponse:\n+    \"\"\"\n+    Generate an SSO URL for Ayrshare social media integration.\n+\n+    Returns:\n+        dict: Contains the SSO URL for Ayrshare integration\n+    \"\"\"\n+    try:\n+        client = AyrshareClient()\n+    except MissingConfigError:\n+        raise HTTPException(\n+            status_code=HTTP_500_INTERNAL_SERVER_ERROR,\n+            detail=\"Ayrshare integration is not configured\",\n+        )\n+\n+    # Ayrshare profile key is stored in the credentials store\n+    # It is generated when creating a new profile, if there is no profile key,\n+    # we create a new profile and store the profile key in the credentials store\n+    profile_key = await creds_manager.store.get_ayrshare_profile_key(user_id)\n+    if not profile_key:\n+        logger.debug(f\"Creating new Ayrshare profile for user {user_id}\")\n+        try:\n+            profile = await client.create_profile(\n+                title=f\"User {user_id}\", messaging_active=True\n+            )\n+            profile_key = profile.profileKey\n+            await creds_manager.store.set_ayrshare_profile_key(user_id, profile_key)\n+        except Exception as e:\n+            logger.error(f\"Error creating Ayrshare profile for user {user_id}: {e}\")\n+            raise HTTPException(\n+                status_code=HTTP_502_BAD_GATEWAY,\n+                detail=\"Failed to create Ayrshare profile\",\n+            )\n+    else:\n+        logger.debug(f\"Using existing Ayrshare profile for user {user_id}\")\n+\n+    profile_key_str = (\n+        profile_key.get_secret_value()\n+        if isinstance(profile_key, SecretStr)\n+        else str(profile_key)\n+    )\n+\n+    private_key = settings.secrets.ayrshare_jwt_key\n+    # Ayrshare JWT expiry is 2880 minutes (48 hours)\n+    max_expiry_minutes = 2880\n+    try:\n+        logger.debug(f\"Generating Ayrshare JWT for user {user_id}\")\n+        jwt_response = await client.generate_jwt(\n+            private_key=private_key,\n+            profile_key=profile_key_str,\n+            allowed_social=[\n+                # NOTE: We are enabling platforms one at a time\n+                # to speed up the development process\n+                # SocialPlatform.FACEBOOK,\n+                SocialPlatform.TWITTER,\n+                SocialPlatform.LINKEDIN,\n+                # SocialPlatform.INSTAGRAM,\n+                # SocialPlatform.YOUTUBE,\n+                # SocialPlatform.REDDIT,\n+                # SocialPlatform.TELEGRAM,\n+                # SocialPlatform.GOOGLE_MY_BUSINESS,\n+                # SocialPlatform.PINTEREST,\n+                # SocialPlatform.TIKTOK,\n+                # SocialPlatform.BLUESKY,\n+                # SocialPlatform.SNAPCHAT,\n+                # SocialPlatform.THREADS,\n+            ],\n+            expires_in=max_expiry_minutes,\n+            verify=True,\n+        )\n+    except Exception as e:\n+        logger.error(f\"Error generating Ayrshare JWT for user {user_id}: {e}\")\n+        raise HTTPException(\n+            status_code=HTTP_502_BAD_GATEWAY, detail=\"Failed to generate JWT\"\n+        )\n+\n+    expires_at = datetime.now(timezone.utc) + timedelta(minutes=max_expiry_minutes)\n+    return AyrshareSSOResponse(sso_url=jwt_response.url, expires_at=expires_at)\n \n \n+# === PROVIDER DISCOVERY ENDPOINTS ===\n @router.get(\"/providers\", response_model=List[str])\n async def list_providers() -> List[str]:\n     \"\"\"\ndiff --git a/autogpt_platform/backend/backend/util/settings.py b/autogpt_platform/backend/backend/util/settings.py\nindex fdf35128e572..c7685bfc7979 100644\n--- a/autogpt_platform/backend/backend/util/settings.py\n+++ b/autogpt_platform/backend/backend/util/settings.py\n@@ -495,7 +495,8 @@ class Secrets(UpdateTrackingModel[\"Secrets\"], BaseSettings):\n     apollo_api_key: str = Field(default=\"\", description=\"Apollo API Key\")\n     smartlead_api_key: str = Field(default=\"\", description=\"SmartLead API Key\")\n     zerobounce_api_key: str = Field(default=\"\", description=\"ZeroBounce API Key\")\n-\n+    ayrshare_api_key: str = Field(default=\"\", description=\"Ayrshare API Key\")\n+    ayrshare_jwt_key: str = Field(default=\"\", description=\"Ayrshare private Key\")\n     # Add more secret fields as needed\n \n     model_config = SettingsConfigDict(\ndiff --git a/autogpt_platform/frontend/src/app/api/__generated__/endpoints/files/files.ts b/autogpt_platform/frontend/src/app/api/__generated__/endpoints/files/files.ts\nnew file mode 100644\nindex 000000000000..71a103c17667\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/app/api/__generated__/endpoints/files/files.ts\n@@ -0,0 +1,184 @@\n+/**\n+ * Generated by orval v7.10.0 \n+ * Do not edit manually.\n+ * AutoGPT Agent Server\n+ * This server is used to execute agents that are created by the AutoGPT system.\n+ * OpenAPI spec version: 0.1\n+ */\n+import { useMutation } from \"@tanstack/react-query\";\n+import type {\n+  MutationFunction,\n+  QueryClient,\n+  UseMutationOptions,\n+  UseMutationResult,\n+} from \"@tanstack/react-query\";\n+\n+import type { BodyPostV1UploadFileToCloudStorage } from \"../../models/bodyPostV1UploadFileToCloudStorage\";\n+\n+import type { HTTPValidationError } from \"../../models/hTTPValidationError\";\n+\n+import type { PostV1UploadFileToCloudStorageParams } from \"../../models/postV1UploadFileToCloudStorageParams\";\n+\n+import type { UploadFileResponse } from \"../../models/uploadFileResponse\";\n+\n+import { customMutator } from \"../../../mutators/custom-mutator\";\n+\n+type SecondParameter<T extends (...args: never) => unknown> = Parameters<T>[1];\n+\n+/**\n+ * Upload a file to cloud storage and return a storage key that can be used\n+with FileStoreBlock and AgentFileInputBlock.\n+\n+Args:\n+    file: The file to upload\n+    user_id: The user ID\n+    provider: Cloud storage provider (\"gcs\", \"s3\", \"azure\")\n+    expiration_hours: Hours until file expires (1-48)\n+\n+Returns:\n+    Dict containing the cloud storage path and signed URL\n+ * @summary Upload file to cloud storage\n+ */\n+export type postV1UploadFileToCloudStorageResponse200 = {\n+  data: UploadFileResponse;\n+  status: 200;\n+};\n+\n+export type postV1UploadFileToCloudStorageResponse422 = {\n+  data: HTTPValidationError;\n+  status: 422;\n+};\n+\n+export type postV1UploadFileToCloudStorageResponseComposite =\n+  | postV1UploadFileToCloudStorageResponse200\n+  | postV1UploadFileToCloudStorageResponse422;\n+\n+export type postV1UploadFileToCloudStorageResponse =\n+  postV1UploadFileToCloudStorageResponseComposite & {\n+    headers: Headers;\n+  };\n+\n+export const getPostV1UploadFileToCloudStorageUrl = (\n+  params?: PostV1UploadFileToCloudStorageParams,\n+) => {\n+  const normalizedParams = new URLSearchParams();\n+\n+  Object.entries(params || {}).forEach(([key, value]) => {\n+    if (value !== undefined) {\n+      normalizedParams.append(key, value === null ? \"null\" : value.toString());\n+    }\n+  });\n+\n+  const stringifiedParams = normalizedParams.toString();\n+\n+  return stringifiedParams.length > 0\n+    ? `/api/files/upload?${stringifiedParams}`\n+    : `/api/files/upload`;\n+};\n+\n+export const postV1UploadFileToCloudStorage = async (\n+  bodyPostV1UploadFileToCloudStorage: BodyPostV1UploadFileToCloudStorage,\n+  params?: PostV1UploadFileToCloudStorageParams,\n+  options?: RequestInit,\n+): Promise<postV1UploadFileToCloudStorageResponse> => {\n+  const formData = new FormData();\n+  formData.append(`file`, bodyPostV1UploadFileToCloudStorage.file);\n+\n+  return customMutator<postV1UploadFileToCloudStorageResponse>(\n+    getPostV1UploadFileToCloudStorageUrl(params),\n+    {\n+      ...options,\n+      method: \"POST\",\n+      body: formData,\n+    },\n+  );\n+};\n+\n+export const getPostV1UploadFileToCloudStorageMutationOptions = <\n+  TError = HTTPValidationError,\n+  TContext = unknown,\n+>(options?: {\n+  mutation?: UseMutationOptions<\n+    Awaited<ReturnType<typeof postV1UploadFileToCloudStorage>>,\n+    TError,\n+    {\n+      data: BodyPostV1UploadFileToCloudStorage;\n+      params?: PostV1UploadFileToCloudStorageParams;\n+    },\n+    TContext\n+  >;\n+  request?: SecondParameter<typeof customMutator>;\n+}): UseMutationOptions<\n+  Awaited<ReturnType<typeof postV1UploadFileToCloudStorage>>,\n+  TError,\n+  {\n+    data: BodyPostV1UploadFileToCloudStorage;\n+    params?: PostV1UploadFileToCloudStorageParams;\n+  },\n+  TContext\n+> => {\n+  const mutationKey = [\"postV1UploadFileToCloudStorage\"];\n+  const { mutation: mutationOptions, request: requestOptions } = options\n+    ? options.mutation &&\n+      \"mutationKey\" in options.mutation &&\n+      options.mutation.mutationKey\n+      ? options\n+      : { ...options, mutation: { ...options.mutation, mutationKey } }\n+    : { mutation: { mutationKey }, request: undefined };\n+\n+  const mutationFn: MutationFunction<\n+    Awaited<ReturnType<typeof postV1UploadFileToCloudStorage>>,\n+    {\n+      data: BodyPostV1UploadFileToCloudStorage;\n+      params?: PostV1UploadFileToCloudStorageParams;\n+    }\n+  > = (props) => {\n+    const { data, params } = props ?? {};\n+\n+    return postV1UploadFileToCloudStorage(data, params, requestOptions);\n+  };\n+\n+  return { mutationFn, ...mutationOptions };\n+};\n+\n+export type PostV1UploadFileToCloudStorageMutationResult = NonNullable<\n+  Awaited<ReturnType<typeof postV1UploadFileToCloudStorage>>\n+>;\n+export type PostV1UploadFileToCloudStorageMutationBody =\n+  BodyPostV1UploadFileToCloudStorage;\n+export type PostV1UploadFileToCloudStorageMutationError = HTTPValidationError;\n+\n+/**\n+ * @summary Upload file to cloud storage\n+ */\n+export const usePostV1UploadFileToCloudStorage = <\n+  TError = HTTPValidationError,\n+  TContext = unknown,\n+>(\n+  options?: {\n+    mutation?: UseMutationOptions<\n+      Awaited<ReturnType<typeof postV1UploadFileToCloudStorage>>,\n+      TError,\n+      {\n+        data: BodyPostV1UploadFileToCloudStorage;\n+        params?: PostV1UploadFileToCloudStorageParams;\n+      },\n+      TContext\n+    >;\n+    request?: SecondParameter<typeof customMutator>;\n+  },\n+  queryClient?: QueryClient,\n+): UseMutationResult<\n+  Awaited<ReturnType<typeof postV1UploadFileToCloudStorage>>,\n+  TError,\n+  {\n+    data: BodyPostV1UploadFileToCloudStorage;\n+    params?: PostV1UploadFileToCloudStorageParams;\n+  },\n+  TContext\n+> => {\n+  const mutationOptions =\n+    getPostV1UploadFileToCloudStorageMutationOptions(options);\n+\n+  return useMutation(mutationOptions, queryClient);\n+};\ndiff --git a/autogpt_platform/frontend/src/app/api/__generated__/endpoints/integrations/integrations.ts b/autogpt_platform/frontend/src/app/api/__generated__/endpoints/integrations/integrations.ts\nindex 9525a6a90e51..bdcdea8b323a 100644\n--- a/autogpt_platform/frontend/src/app/api/__generated__/endpoints/integrations/integrations.ts\n+++ b/autogpt_platform/frontend/src/app/api/__generated__/endpoints/integrations/integrations.ts\n@@ -21,6 +21,8 @@ import type {\n   UseQueryResult,\n } from \"@tanstack/react-query\";\n \n+import type { AyrshareSSOResponse } from \"../../models/ayrshareSSOResponse\";\n+\n import type { BodyPostV1Callback } from \"../../models/bodyPostV1Callback\";\n \n import type { CredentialsMetaResponse } from \"../../models/credentialsMetaResponse\";\n@@ -1530,6 +1532,210 @@ export const usePostV1WebhookPing = <\n \n   return useMutation(mutationOptions, queryClient);\n };\n+/**\n+ * Generate an SSO URL for Ayrshare social media integration.\n+\n+Returns:\n+    dict: Contains the SSO URL for Ayrshare integration\n+ * @summary Get Ayrshare Sso Url\n+ */\n+export type getV1GetAyrshareSsoUrlResponse200 = {\n+  data: AyrshareSSOResponse;\n+  status: 200;\n+};\n+\n+export type getV1GetAyrshareSsoUrlResponseComposite =\n+  getV1GetAyrshareSsoUrlResponse200;\n+\n+export type getV1GetAyrshareSsoUrlResponse =\n+  getV1GetAyrshareSsoUrlResponseComposite & {\n+    headers: Headers;\n+  };\n+\n+export const getGetV1GetAyrshareSsoUrlUrl = () => {\n+  return `/api/integrations/ayrshare/sso_url`;\n+};\n+\n+export const getV1GetAyrshareSsoUrl = async (\n+  options?: RequestInit,\n+): Promise<getV1GetAyrshareSsoUrlResponse> => {\n+  return customMutator<getV1GetAyrshareSsoUrlResponse>(\n+    getGetV1GetAyrshareSsoUrlUrl(),\n+    {\n+      ...options,\n+      method: \"GET\",\n+    },\n+  );\n+};\n+\n+export const getGetV1GetAyrshareSsoUrlQueryKey = () => {\n+  return [`/api/integrations/ayrshare/sso_url`] as const;\n+};\n+\n+export const getGetV1GetAyrshareSsoUrlQueryOptions = <\n+  TData = Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+  TError = unknown,\n+>(options?: {\n+  query?: Partial<\n+    UseQueryOptions<\n+      Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+      TError,\n+      TData\n+    >\n+  >;\n+  request?: SecondParameter<typeof customMutator>;\n+}) => {\n+  const { query: queryOptions, request: requestOptions } = options ?? {};\n+\n+  const queryKey =\n+    queryOptions?.queryKey ?? getGetV1GetAyrshareSsoUrlQueryKey();\n+\n+  const queryFn: QueryFunction<\n+    Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>\n+  > = ({ signal }) => getV1GetAyrshareSsoUrl({ signal, ...requestOptions });\n+\n+  return { queryKey, queryFn, ...queryOptions } as UseQueryOptions<\n+    Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+    TError,\n+    TData\n+  > & { queryKey: DataTag<QueryKey, TData, TError> };\n+};\n+\n+export type GetV1GetAyrshareSsoUrlQueryResult = NonNullable<\n+  Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>\n+>;\n+export type GetV1GetAyrshareSsoUrlQueryError = unknown;\n+\n+export function useGetV1GetAyrshareSsoUrl<\n+  TData = Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+  TError = unknown,\n+>(\n+  options: {\n+    query: Partial<\n+      UseQueryOptions<\n+        Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+        TError,\n+        TData\n+      >\n+    > &\n+      Pick<\n+        DefinedInitialDataOptions<\n+          Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+          TError,\n+          Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>\n+        >,\n+        \"initialData\"\n+      >;\n+    request?: SecondParameter<typeof customMutator>;\n+  },\n+  queryClient?: QueryClient,\n+): DefinedUseQueryResult<TData, TError> & {\n+  queryKey: DataTag<QueryKey, TData, TError>;\n+};\n+export function useGetV1GetAyrshareSsoUrl<\n+  TData = Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+  TError = unknown,\n+>(\n+  options?: {\n+    query?: Partial<\n+      UseQueryOptions<\n+        Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+        TError,\n+        TData\n+      >\n+    > &\n+      Pick<\n+        UndefinedInitialDataOptions<\n+          Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+          TError,\n+          Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>\n+        >,\n+        \"initialData\"\n+      >;\n+    request?: SecondParameter<typeof customMutator>;\n+  },\n+  queryClient?: QueryClient,\n+): UseQueryResult<TData, TError> & {\n+  queryKey: DataTag<QueryKey, TData, TError>;\n+};\n+export function useGetV1GetAyrshareSsoUrl<\n+  TData = Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+  TError = unknown,\n+>(\n+  options?: {\n+    query?: Partial<\n+      UseQueryOptions<\n+        Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+        TError,\n+        TData\n+      >\n+    >;\n+    request?: SecondParameter<typeof customMutator>;\n+  },\n+  queryClient?: QueryClient,\n+): UseQueryResult<TData, TError> & {\n+  queryKey: DataTag<QueryKey, TData, TError>;\n+};\n+/**\n+ * @summary Get Ayrshare Sso Url\n+ */\n+\n+export function useGetV1GetAyrshareSsoUrl<\n+  TData = Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+  TError = unknown,\n+>(\n+  options?: {\n+    query?: Partial<\n+      UseQueryOptions<\n+        Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+        TError,\n+        TData\n+      >\n+    >;\n+    request?: SecondParameter<typeof customMutator>;\n+  },\n+  queryClient?: QueryClient,\n+): UseQueryResult<TData, TError> & {\n+  queryKey: DataTag<QueryKey, TData, TError>;\n+} {\n+  const queryOptions = getGetV1GetAyrshareSsoUrlQueryOptions(options);\n+\n+  const query = useQuery(queryOptions, queryClient) as UseQueryResult<\n+    TData,\n+    TError\n+  > & { queryKey: DataTag<QueryKey, TData, TError> };\n+\n+  query.queryKey = queryOptions.queryKey;\n+\n+  return query;\n+}\n+\n+/**\n+ * @summary Get Ayrshare Sso Url\n+ */\n+export const prefetchGetV1GetAyrshareSsoUrlQuery = async <\n+  TData = Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+  TError = unknown,\n+>(\n+  queryClient: QueryClient,\n+  options?: {\n+    query?: Partial<\n+      UseQueryOptions<\n+        Awaited<ReturnType<typeof getV1GetAyrshareSsoUrl>>,\n+        TError,\n+        TData\n+      >\n+    >;\n+    request?: SecondParameter<typeof customMutator>;\n+  },\n+): Promise<QueryClient> => {\n+  const queryOptions = getGetV1GetAyrshareSsoUrlQueryOptions(options);\n+\n+  await queryClient.prefetchQuery(queryOptions);\n+\n+  return queryClient;\n+};\n+\n /**\n  * Get a list of all available provider names.\n \ndiff --git a/autogpt_platform/frontend/src/app/api/__generated__/models/ayrshareSSOResponse.ts b/autogpt_platform/frontend/src/app/api/__generated__/models/ayrshareSSOResponse.ts\nnew file mode 100644\nindex 000000000000..bd05f62512be\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/app/api/__generated__/models/ayrshareSSOResponse.ts\n@@ -0,0 +1,14 @@\n+/**\n+ * Generated by orval v7.10.0 \n+ * Do not edit manually.\n+ * AutoGPT Agent Server\n+ * This server is used to execute agents that are created by the AutoGPT system.\n+ * OpenAPI spec version: 0.1\n+ */\n+\n+export interface AyrshareSSOResponse {\n+  /** The SSO URL for Ayrshare integration */\n+  sso_url: string;\n+  /** ISO timestamp when the URL expires */\n+  expires_at: string;\n+}\ndiff --git a/autogpt_platform/frontend/src/app/api/__generated__/models/bodyPostV1UploadFileToCloudStorage.ts b/autogpt_platform/frontend/src/app/api/__generated__/models/bodyPostV1UploadFileToCloudStorage.ts\nnew file mode 100644\nindex 000000000000..46bfff9a7885\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/app/api/__generated__/models/bodyPostV1UploadFileToCloudStorage.ts\n@@ -0,0 +1,11 @@\n+/**\n+ * Generated by orval v7.10.0 \n+ * Do not edit manually.\n+ * AutoGPT Agent Server\n+ * This server is used to execute agents that are created by the AutoGPT system.\n+ * OpenAPI spec version: 0.1\n+ */\n+\n+export interface BodyPostV1UploadFileToCloudStorage {\n+  file: Blob;\n+}\ndiff --git a/autogpt_platform/frontend/src/app/api/__generated__/models/postV1UploadFileToCloudStorageParams.ts b/autogpt_platform/frontend/src/app/api/__generated__/models/postV1UploadFileToCloudStorageParams.ts\nnew file mode 100644\nindex 000000000000..878eb768ec32\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/app/api/__generated__/models/postV1UploadFileToCloudStorageParams.ts\n@@ -0,0 +1,12 @@\n+/**\n+ * Generated by orval v7.10.0 \n+ * Do not edit manually.\n+ * AutoGPT Agent Server\n+ * This server is used to execute agents that are created by the AutoGPT system.\n+ * OpenAPI spec version: 0.1\n+ */\n+\n+export type PostV1UploadFileToCloudStorageParams = {\n+  provider?: string;\n+  expiration_hours?: number;\n+};\ndiff --git a/autogpt_platform/frontend/src/app/api/__generated__/models/uploadFileResponse.ts b/autogpt_platform/frontend/src/app/api/__generated__/models/uploadFileResponse.ts\nnew file mode 100644\nindex 000000000000..78fa6c6c50aa\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/app/api/__generated__/models/uploadFileResponse.ts\n@@ -0,0 +1,15 @@\n+/**\n+ * Generated by orval v7.10.0 \n+ * Do not edit manually.\n+ * AutoGPT Agent Server\n+ * This server is used to execute agents that are created by the AutoGPT system.\n+ * OpenAPI spec version: 0.1\n+ */\n+\n+export interface UploadFileResponse {\n+  file_uri: string;\n+  file_name: string;\n+  size: number;\n+  content_type: string;\n+  expires_in_hours: number;\n+}\ndiff --git a/autogpt_platform/frontend/src/app/api/openapi.json b/autogpt_platform/frontend/src/app/api/openapi.json\nindex c63413e9ac01..a03f764e0de4 100644\n--- a/autogpt_platform/frontend/src/app/api/openapi.json\n+++ b/autogpt_platform/frontend/src/app/api/openapi.json\n@@ -443,6 +443,24 @@\n         }\n       }\n     },\n+    \"/api/integrations/ayrshare/sso_url\": {\n+      \"get\": {\n+        \"tags\": [\"v1\", \"integrations\"],\n+        \"summary\": \"Get Ayrshare Sso Url\",\n+        \"description\": \"Generate an SSO URL for Ayrshare social media integration.\\n\\nReturns:\\n    dict: Contains the SSO URL for Ayrshare integration\",\n+        \"operationId\": \"getV1GetAyrshareSsoUrl\",\n+        \"responses\": {\n+          \"200\": {\n+            \"description\": \"Successful Response\",\n+            \"content\": {\n+              \"application/json\": {\n+                \"schema\": { \"$ref\": \"#/components/schemas/AyrshareSSOResponse\" }\n+              }\n+            }\n+          }\n+        }\n+      }\n+    },\n     \"/api/integrations/providers\": {\n       \"get\": {\n         \"tags\": [\"v1\", \"integrations\"],\n@@ -823,6 +841,64 @@\n         }\n       }\n     },\n+    \"/api/files/upload\": {\n+      \"post\": {\n+        \"tags\": [\"v1\", \"files\"],\n+        \"summary\": \"Upload file to cloud storage\",\n+        \"description\": \"Upload a file to cloud storage and return a storage key that can be used\\nwith FileStoreBlock and AgentFileInputBlock.\\n\\nArgs:\\n    file: The file to upload\\n    user_id: The user ID\\n    provider: Cloud storage provider (\\\"gcs\\\", \\\"s3\\\", \\\"azure\\\")\\n    expiration_hours: Hours until file expires (1-48)\\n\\nReturns:\\n    Dict containing the cloud storage path and signed URL\",\n+        \"operationId\": \"postV1Upload file to cloud storage\",\n+        \"parameters\": [\n+          {\n+            \"name\": \"provider\",\n+            \"in\": \"query\",\n+            \"required\": false,\n+            \"schema\": {\n+              \"type\": \"string\",\n+              \"default\": \"gcs\",\n+              \"title\": \"Provider\"\n+            }\n+          },\n+          {\n+            \"name\": \"expiration_hours\",\n+            \"in\": \"query\",\n+            \"required\": false,\n+            \"schema\": {\n+              \"type\": \"integer\",\n+              \"default\": 24,\n+              \"title\": \"Expiration Hours\"\n+            }\n+          }\n+        ],\n+        \"requestBody\": {\n+          \"required\": true,\n+          \"content\": {\n+            \"multipart/form-data\": {\n+              \"schema\": {\n+                \"$ref\": \"#/components/schemas/Body_postV1Upload_file_to_cloud_storage\"\n+              }\n+            }\n+          }\n+        },\n+        \"responses\": {\n+          \"200\": {\n+            \"description\": \"Successful Response\",\n+            \"content\": {\n+              \"application/json\": {\n+                \"schema\": { \"$ref\": \"#/components/schemas/UploadFileResponse\" }\n+              }\n+            }\n+          },\n+          \"422\": {\n+            \"description\": \"Validation Error\",\n+            \"content\": {\n+              \"application/json\": {\n+                \"schema\": { \"$ref\": \"#/components/schemas/HTTPValidationError\" }\n+              }\n+            }\n+          }\n+        }\n+      }\n+    },\n     \"/api/credits\": {\n       \"get\": {\n         \"tags\": [\"v1\", \"credits\"],\n@@ -3790,6 +3866,24 @@\n         \"required\": [\"amount\", \"threshold\"],\n         \"title\": \"AutoTopUpConfig\"\n       },\n+      \"AyrshareSSOResponse\": {\n+        \"properties\": {\n+          \"sso_url\": {\n+            \"type\": \"string\",\n+            \"title\": \"Sso Url\",\n+            \"description\": \"The SSO URL for Ayrshare integration\"\n+          },\n+          \"expires_at\": {\n+            \"type\": \"string\",\n+            \"format\": \"date-time\",\n+            \"title\": \"Expires At\",\n+            \"description\": \"ISO timestamp when the URL expires\"\n+          }\n+        },\n+        \"type\": \"object\",\n+        \"required\": [\"sso_url\", \"expires_at\"],\n+        \"title\": \"AyrshareSSOResponse\"\n+      },\n       \"BaseGraph-Input\": {\n         \"properties\": {\n           \"id\": { \"type\": \"string\", \"title\": \"Id\" },\n@@ -3934,6 +4028,14 @@\n         \"required\": [\"type\", \"data\", \"data_index\"],\n         \"title\": \"Body_postV1LogRawAnalytics\"\n       },\n+      \"Body_postV1Upload_file_to_cloud_storage\": {\n+        \"properties\": {\n+          \"file\": { \"type\": \"string\", \"format\": \"binary\", \"title\": \"File\" }\n+        },\n+        \"type\": \"object\",\n+        \"required\": [\"file\"],\n+        \"title\": \"Body_postV1Upload file to cloud storage\"\n+      },\n       \"Body_postV2Add_credits_to_user\": {\n         \"properties\": {\n           \"user_id\": { \"type\": \"string\", \"title\": \"User Id\" },\n@@ -6348,6 +6450,24 @@\n         \"required\": [\"permissions\"],\n         \"title\": \"UpdatePermissionsRequest\"\n       },\n+      \"UploadFileResponse\": {\n+        \"properties\": {\n+          \"file_uri\": { \"type\": \"string\", \"title\": \"File Uri\" },\n+          \"file_name\": { \"type\": \"string\", \"title\": \"File Name\" },\n+          \"size\": { \"type\": \"integer\", \"title\": \"Size\" },\n+          \"content_type\": { \"type\": \"string\", \"title\": \"Content Type\" },\n+          \"expires_in_hours\": { \"type\": \"integer\", \"title\": \"Expires In Hours\" }\n+        },\n+        \"type\": \"object\",\n+        \"required\": [\n+          \"file_uri\",\n+          \"file_name\",\n+          \"size\",\n+          \"content_type\",\n+          \"expires_in_hours\"\n+        ],\n+        \"title\": \"UploadFileResponse\"\n+      },\n       \"UserHistoryResponse\": {\n         \"properties\": {\n           \"history\": {\ndiff --git a/autogpt_platform/frontend/src/components/CustomNode.tsx b/autogpt_platform/frontend/src/components/CustomNode.tsx\nindex bf5df316c7c5..360689829418 100644\n--- a/autogpt_platform/frontend/src/components/CustomNode.tsx\n+++ b/autogpt_platform/frontend/src/components/CustomNode.tsx\n@@ -31,7 +31,7 @@ import {\n   parseKeys,\n   setNestedProperty,\n } from \"@/lib/utils\";\n-import { Button } from \"@/components/ui/button\";\n+import { Button } from \"@/components/atoms/Button/Button\";\n import { Switch } from \"@/components/ui/switch\";\n import { TextRenderer } from \"@/components/ui/render\";\n import { history } from \"./history\";\n@@ -54,8 +54,10 @@ import {\n   CopyIcon,\n   ExitIcon,\n } from \"@radix-ui/react-icons\";\n-\n+import { Key } from \"@phosphor-icons/react\";\n import useCredits from \"@/hooks/useCredits\";\n+import { getV1GetAyrshareSsoUrl } from \"@/app/api/__generated__/endpoints/integrations/integrations\";\n+import { toast } from \"@/components/molecules/Toast/use-toast\";\n \n export type ConnectionData = Array<{\n   edge_id: string;\n@@ -112,6 +114,8 @@ export const CustomNode = React.memo(\n     const flowContext = useContext(FlowContext);\n     const api = useBackendAPI();\n     const { formatCredits } = useCredits();\n+    const [isLoading, setIsLoading] = useState(false);\n+\n     let nodeFlowId = \"\";\n \n     if (data.uiType === BlockUIType.AGENT) {\n@@ -241,6 +245,59 @@ export const CustomNode = React.memo(\n       return renderHandles(schema.properties);\n     };\n \n+    const generateAyrshareSSOHandles = () => {\n+      const handleSSOLogin = async () => {\n+        setIsLoading(true);\n+        try {\n+          const {\n+            data: { sso_url },\n+          } = await getV1GetAyrshareSsoUrl();\n+          const popup = window.open(sso_url, \"_blank\", \"popup=true\");\n+          if (!popup) {\n+            throw new Error(\n+              \"Please allow popups for this site to be able to login with Ayrshare\",\n+            );\n+          }\n+        } catch (error) {\n+          toast({\n+            title: \"Error\",\n+            description: `Error getting SSO URL: ${error}`,\n+            variant: \"destructive\",\n+          });\n+        } finally {\n+          setIsLoading(false);\n+        }\n+      };\n+\n+      return (\n+        <div className=\"flex flex-col gap-2\">\n+          <Button\n+            type=\"button\"\n+            variant=\"outline\"\n+            className=\"w-full\"\n+            onClick={handleSSOLogin}\n+            disabled={isLoading}\n+          >\n+            {isLoading ? (\n+              \"Loading...\"\n+            ) : (\n+              <>\n+                <Key className=\"mr-2 h-4 w-4\" />\n+                Connect Social Media Accounts\n+              </>\n+            )}\n+          </Button>\n+          <NodeHandle\n+            title=\"SSO Token\"\n+            keyName=\"sso_token\"\n+            isConnected={false}\n+            schema={{ type: \"string\" }}\n+            side=\"right\"\n+          />\n+        </div>\n+      );\n+    };\n+\n     const generateInputHandles = (\n       schema: BlockIORootSchema,\n       nodeType: BlockUIType,\n@@ -827,8 +884,18 @@ export const CustomNode = React.memo(\n                       (A Webhook URL will be generated when you save the agent)\n                     </p>\n                   ))}\n-                {data.inputSchema &&\n-                  generateInputHandles(data.inputSchema, data.uiType)}\n+                {data.uiType === BlockUIType.AYRSHARE ? (\n+                  <>\n+                    {generateAyrshareSSOHandles()}\n+                    {generateInputHandles(\n+                      data.inputSchema,\n+                      BlockUIType.STANDARD,\n+                    )}\n+                  </>\n+                ) : (\n+                  data.inputSchema &&\n+                  generateInputHandles(data.inputSchema, data.uiType)\n+                )}\n               </div>\n             </div>\n           ) : (\ndiff --git a/autogpt_platform/frontend/src/components/integrations/credentials-provider.tsx b/autogpt_platform/frontend/src/components/integrations/credentials-provider.tsx\nindex b113aa22f5cb..abf081ab6d3b 100644\n--- a/autogpt_platform/frontend/src/components/integrations/credentials-provider.tsx\n+++ b/autogpt_platform/frontend/src/components/integrations/credentials-provider.tsx\n@@ -230,7 +230,7 @@ export default function CredentialsProvider({\n \n   useEffect(() => {\n     if (!isLoggedIn || providerNames.length === 0) {\n-      if (isLoggedIn == false) setProviders(null);\n+      if (isLoggedIn == false) setProviders({});\n       return;\n     }\n \ndiff --git a/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts b/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\nindex 4082f3839df9..dca88d90bedb 100644\n--- a/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\n+++ b/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\n@@ -604,6 +604,7 @@ export enum BlockUIType {\n   WEBHOOK_MANUAL = \"Webhook (manual)\",\n   AGENT = \"Agent\",\n   AI = \"AI\",\n+  AYRSHARE = \"Ayrshare\",\n }\n \n export enum SpecialBlockID {\n"
  },
  {
    "index": 2,
    "filtered_comments": [
      "The PR fails to meet some basic requirements but shows a good effort. Issues:\n1. The PR template is not properly filled out - missing test plan and checklist items\n2. While the scope is clear (frontend) and shown in the title correctly, documentation of changes could be clearer\n3. No clear explanation in PR body about 'why' these changes were needed\n4. No test plan or testing notes provided for such a large UI change\n\nHowever, the code itself looks well-structured with clear component organization and documentation. The TODO comments also show good forward planning.",
      "## Feedback for PR Improvement\n\n### Missing PR Template & Checklist\nPlease fill out the complete PR template, including the checklist section. This is required for all PRs and helps reviewers understand what's being changed and how it's been tested.\n\n### Scope Mismatch\nYour PR title mentions only frontend changes (`feat(frontend)`), but the diff shows significant backend additions including:\n- New API routes in `/api/builder/`\n- New DB functions and models\n- Changes to store routes\n\nPlease update your title to reflect both frontend and backend changes, or better explain in the description why backend changes were needed for this frontend redesign.\n\n### User ID Validation\nSome of the new backend functions in `backend/server/v2/builder/db.py` don't appear to validate user permissions via user_id. Please review these functions to ensure proper authorization is in place or provide an explanation for why user_id checks aren't needed.\n\n### Merge Conflicts\nThis PR has merge conflicts that need to be resolved before it can be merged.\n\n### Testing\nPlease include information about how you've tested these changes, particularly since they span both frontend and backend components.\n\nOnce you've addressed these issues, the PR should be ready for re-review.",
      "Thank you for your work on redesigning the block menu. This looks like a substantial change that will improve the builder page experience. \n\nHowever, there are a few things that need to be addressed before this PR can be merged:\n\n1. **Missing checklist**: Please complete the PR template checklist. For code changes like these, we need to know that you've tested your changes with a clear test plan.\n\n2. **Scope discrepancy**: Your PR title mentions only frontend changes, but the diff includes significant backend changes - new routes, endpoints, and database access functions. Please update your PR title to reflect that this is both a frontend and backend feature, perhaps something like `feat(frontend,backend): Redesign block menu on builder page`.\n\n3. **Implementation details**: Your PR description lists the components you've added but doesn't explain how they work together or what specific improvements they bring. Please enhance the description with more details about the implementation and how it improves upon the previous design.\n\n4. **Test plan**: Please add a test plan to your PR description explaining how you've tested these changes.\n\nThe code itself looks well-structured with proper authentication handling and user_id checks. I particularly like how you've organized the new components and the backend routes to support them. Once you address the above points, this PR should be ready for another review.",
      "Thank you for your work on redesigning the block menu! I have a few comments before this can be merged:\n\n## Description and Checklist\n- Please update your PR description to include a more detailed explanation of the changes you've made\n- The required PR checklist is missing. Please add the checklist from the PR template and check off the appropriate items\n\n## Scope Mismatch\nYour PR title indicates this is a frontend feature (`feat(frontend)`), but the changes include substantial backend code, including:\n- New backend routes in `/api/builder`\n- New database functions in `backend/server/v2/builder/db.py`\n- New models in `backend/server/v2/builder/model.py`\n\nConsider updating your PR title to reflect that this involves both frontend and backend changes, perhaps something like `feat(platform): Redesign block menu on builder page with backend support`\n\n## Test Plan\nPlease include a test plan in your PR description to explain how you've tested these changes.\n\nOnce these items are addressed, we can proceed with reviewing the technical aspects of the implementation. The component list and structure looks well-organized!",
      "Thank you for your work on redesigning the block menu! I have a few concerns about this PR that need to be addressed before it can be merged:\n\n1. **PR Description:** While you've listed the components you've added, the PR description should explain why these changes are needed and provide more context about the redesign goals.\n\n2. **Missing Checklist:** Your PR description is missing the required checklist section that outlines your test plan and confirms you've tested the changes appropriately. Please update your PR description to include the complete checklist.\n\n3. **Scope Mismatch:** The PR title indicates this is a frontend change, but the diff shows substantial backend changes including:\n   - New backend routes in `/api/builder`\n   - New database functions and models\n   - Changes to the store API\n\n   Please update either the PR title to reflect the full scope of changes (e.g., `feat(frontend,backend): Redesign block menu and add builder API endpoints`) or separate the backend changes into a different PR.\n\n4. **Dependencies:** You've added several new dependencies (`react-timeago`, `tailwind-scrollbar`, `tailwind-scrollbar-hide`). Please ensure these are necessary and document their purpose in the PR description.\n\n5. **Testing:** Please describe how you've tested these changes, especially the new backend routes and functionality.\n\nAdditionally, I see there's a conflicts label on this PR. Please resolve any merge conflicts before this can be approved.\n\nOnce you've addressed these issues, I'll be happy to review the PR again.",
      "Thank you for your PR to redesign the block menu! The component list you provided is helpful for understanding what's been added. I have some feedback to help get this PR ready for merging:\n\n1. **Missing checklist**: This PR contains significant code changes but is missing the required checklist. Please add the checklist from the PR template and ensure you've tested all changes appropriately.\n\n2. **Scope mismatch**: While your title mentions frontend changes, a significant portion of this PR adds backend functionality (new routes, models, and database operations for the builder). Please update your PR title to reflect both frontend and backend changes, e.g., `feat(frontend,backend): Redesign block menu and add builder API endpoints`.\n\n3. **PR description improvements**: Consider expanding your description to mention the backend API endpoints you've added to support the block menu functionality.\n\nOnce you've addressed these items, we'll be able to review your PR again. The code changes themselves look good - I like the organized approach with the reusable components and the proper handling of user_id in the backend routes.",
      "Thanks for the PR to redesign the block menu! The implementation looks comprehensive with both frontend components and backend support.\n\n### Feedback\n\n1. **PR Description Improvements**:\n   - The PR description is missing the required checklist from the template. Please add the checklist sections for code changes and make sure all items are checked off.\n   - While you've described the frontend components well, please also mention the significant backend changes (new API routes in `/api/builder`, pagination model extensions, etc.) that support this feature.\n\n2. **Implementation Notes**:\n   - The changes include backend routes and models to support the block menu functionality, which is good but should be mentioned in the PR description.\n   - I notice you've added several new npm packages (`react-timeago`, `tailwind-scrollbar`, etc.) - please include these in your description of changes.\n\n3. **Code Quality**:\n   - The backend implementation looks solid with well-structured endpoints and models.\n   - Good job on the TODO markers for future work.\n\nPlease update the PR description to include the missing checklist and mention the backend changes that support this feature. Once these items are addressed, the PR will be ready for another review.",
      "Thank you for your PR to redesign the block menu! I've identified a few issues that need to be addressed before this can be merged:\n\n1. **Scope mismatch**: While your PR title indicates this is a frontend change, approximately half of the changes are backend-related (new API endpoints, models, database functions). The PR title and description should accurately reflect all major components being changed.\n\n2. **Missing checklist**: For substantial code changes like this, the PR template checklist needs to be completed to ensure proper testing and quality checks. Please add the checklist and check off the appropriate items.\n\n3. **Backend changes documentation**: The PR description doesn't mention any of the backend changes. Please update the description to include details about the new backend/server/v2/builder files and endpoints you're adding.\n\nAdditionally, I notice you've marked some items with \"BLOCK MENU TODO:\" for future work. If these are part of this redesign effort but will be addressed in separate PRs, that's fine, but please make that clear in the description.\n\nOnce you've updated the PR title to accurately reflect both frontend and backend changes, completed the required checklist, and expanded the description to cover all major components being changed, we can proceed with the review process.",
      "Thanks for your PR on redesigning the block menu. I noticed a few things that need to be addressed before this can be merged:\n\n1. **Missing Checklist**: Please include the complete PR checklist in your description. Since your changes include significant code modifications, you'll need to add the checklist with appropriate test plans.\n\n2. **Scope Mismatch**: Your PR title only mentions frontend changes (`feat(frontend)`), but the diff shows substantial backend additions including:\n   - New API routes in `/api/builder`\n   - New database functionality\n   - New models and response types\n\n   Please update your PR title to reflect both the frontend and backend changes, perhaps something like `feat(frontend,backend): Redesign block menu on builder page`.\n\n3. **PR Description**: Consider enhancing your description to explain more about the purpose of the backend changes and how they relate to the frontend block menu redesign.\n\nThe changes themselves look promising, but we need to ensure the PR follows our submission guidelines before proceeding with the review of the actual implementation. Please make these updates so we can move forward with the review.",
      "Thank you for your work on redesigning the block menu! I have a few concerns that need to be addressed before this PR can be merged:\n\n1. **Missing Checklist**: The PR description is missing the required checklist. Please update your description to include the standard checklist and fill it out completely.\n\n2. **Scope Mismatch**: The PR title mentions only frontend changes (`feat(frontend)`), but the diff shows significant backend changes as well. Please update the PR title to accurately reflect the scope of your changes. Something like `feat(platform): Redesign block menu on builder page` or `feat(platform/frontend,platform/backend): Redesign block menu on builder page` would be more appropriate.\n\n3. **Backend Changes**: The PR includes substantial backend additions:\n   - New builder routes and endpoints\n   - New database functions and models\n   - Changes to the store API\n\nPlease make sure to include these backend changes in your test plan when you add the checklist.\n\nOnce these items are addressed, we can proceed with the review of the code itself. The overall direction of the redesign looks promising!",
      "Thank you for working on redesigning the block menu. I see you've created several reusable components for this feature.\n\nI have two main concerns with this PR:\n\n1. **Missing Checklist**: Your PR description is missing the required checklist that should cover testing plan and other verification steps. Please update your description to include the standard checklist and fill it out appropriately.\n\n2. **Scope Mismatch**: Your PR title mentions only a frontend redesign, but the changes include significant backend work. I see new backend files for a builder API, including routes, models, and database functionality. The PR title should reflect both frontend and backend changes, something like: `feat(frontend,backend): Redesign block menu and add builder API`\n\nPlease update your PR to address these issues so we can proceed with the review. The code changes themselves look well-structured, but we need to ensure the PR follows our process requirements.\n\nAlso, I noticed you've marked some items with `BLOCK MENU TODO:` for future work. That's a good approach for tracking remaining tasks.",
      "Hi @Abhi1992002, thanks for your work on redesigning the block menu. I have some feedback to help get this PR ready for merging:\n\n## Required Changes\n\n1. **PR Description & Checklist**\n   - Please include the complete PR checklist from our template, including your test plan\n   - The description should explain both the frontend and backend changes\n\n2. **PR Scope**\n   - Your PR title mentions only frontend changes, but there are significant backend changes\n   - Please update the title to reflect both (e.g., `feat(frontend,backend): Redesign block menu on builder page`)\n\n3. **Backend Testing**\n   - The new API endpoints should have proper testing documentation\n   - Please describe how you've tested the new endpoints\n\n4. **User ID Security**\n   - For the new backend routes, please confirm that user_id checks are properly implemented\n\n## Questions\n\n1. What testing have you done for both the frontend components and backend endpoints?\n2. Are there any configuration changes needed for these new components?\n3. How do the new endpoints interact with the redesigned block menu?\n\nOnce these items are addressed, I'd be happy to re-review the PR. Thanks!",
      "Thanks for working on redesigning the block menu for the builder page! This is an important improvement that will enhance the user experience. However, there are a couple of issues that need to be addressed before we can merge this PR:\n\n1. **Missing Checklist**: The PR template requires a filled-out checklist for all code changes. Please update your PR description to include the checklist from the template with all applicable items checked.\n\n2. **Scope Mismatch**: Your PR title indicates this is a frontend change, but the diff contains significant backend changes including:\n   - Adding new backend routes under `/api/builder`\n   - Creating new backend models and database functions\n   - Modifying the store API\n\n   Please either:\n   - Update the PR title to something like `feat(platform): Redesign block menu with backend support` to accurately reflect both frontend and backend changes, or\n   - Split this into separate PRs for frontend and backend changes\n\nAdditionally, while your PR description lists the components you've added, it would be helpful to include a brief explanation of the backend changes as well.\n\nOnce these items are addressed, we can proceed with the review of the implementation details. The component organization looks good, and I appreciate your marking future tasks with the `BLOCK MENU TODO:` tag for easy reference.",
      "Thanks for working on the block menu redesign! This PR adds important components and functionality for the builder page.\n\n### Key Issues to Address\n\n1. **Missing Checklist:** Please add the required checklist to your PR description. This is mandatory for code changes and should include details about your test plan.\n\n2. **Scope Clarification:** Your PR title focuses on frontend changes, but includes significant backend changes (new endpoints, models, DB functions). While these backend changes appear necessary to support the frontend redesign, consider either:\n   - Updating the PR title to reflect both frontend and backend changes (e.g., `feat(platform): Redesign block menu on builder page with supporting API`)  \n   - OR splitting the PR into separate frontend and backend PRs if that makes sense\n\n3. **PR Conflicts:** There's a conflicts label on your PR - please resolve these merge conflicts.\n\n### Additional Notes\n\n- The TODO comments you've marked with `BLOCK MENU TODO:` are fine as placeholders for future work.\n- Good job properly implementing auth middleware on the new API routes.\n- Consider adding more details about what the redesigned block menu includes and what improvements it brings.\n\nOnce you've addressed these issues, particularly adding the required checklist, the PR should be ready for another review.",
      "Thank you for your PR to redesign the block menu! The implementation looks thorough with many new components created for the builder page.\n\nHowever, I've noticed a couple of issues that need to be addressed before this can be merged:\n\n1. **Missing checklist**: The PR description is missing the required checklist from our PR template. Since this is a significant code change, please update your PR description to include a completed checklist with items like:\n   - Confirmation that you've tested your changes\n   - A test plan outlining how you verified your implementation works correctly\n\n2. **Scope mismatch**: Your PR title mentions only frontend changes, but the PR includes substantial backend implementations including:\n   - A new backend/server/v2/builder module with model, db, and routes files\n   - New API endpoints\n   - Modifications to existing store functionality\n\n   Please update your PR title to reflect both frontend and backend changes, perhaps something like: `feat(frontend,backend): Redesign block menu and add builder API endpoints`\n\nOnce these issues are addressed, we can proceed with the review of your implementation. The code itself looks well-structured with appropriate user_id handling in the backend functions.",
      "Thanks for your PR on redesigning the block menu! There are a few items that need to be addressed before this can be merged:\n\n## Missing Required Information\n\n1. **Checklist**: Your PR is missing the required checklist section. Since this PR involves significant code changes, we need to ensure all necessary checks have been completed.\n\n2. **Test Plan**: Please include details on how you've tested these changes.\n\n## Scope Clarification\n\nYour PR title mentions frontend redesign, but includes significant backend changes (new API endpoints, models, etc.). While these backend changes appear to support the frontend redesign, it would be helpful to:\n\n- Update your PR title to reflect both frontend and backend changes, e.g., `feat(platform): Redesign block menu with supporting backend APIs`\n- OR clarify in your description how the backend changes are necessary for the frontend redesign\n\n## Documentation\n\nPlease add some brief documentation on the new backend endpoints you've created to help other developers understand their purpose and how they relate to the block menu redesign.\n\nThe code itself looks good - I see you've properly protected all routes with auth middleware and correctly handle user_id passing where needed. Once you've addressed these items, we can proceed with the review.",
      "Thank you for working on redesigning the block menu! This is an important improvement for the builder page UI. I've identified a couple of issues that need to be addressed before this PR can be merged:\n\n1. The PR description is missing the required checklist. Please update your description to include the checklist from our template, with all applicable items checked off.\n\n2. The PR title mentions only frontend changes (`feat(frontend)`), but the PR includes significant backend additions (new routes, models, and DB functions in `/backend/server/v2/builder/`). Either:\n   - Update the PR title to reflect both frontend and backend changes (e.g., `feat(platform): Redesign block menu with backend support`)\n   - Or split this into separate PRs for frontend and backend changes\n\nYour implementation looks good overall - I can see you've built reusable components and made sure the backend functions properly handle user_id for authentication. Please address the issues above so we can proceed with the review.\n\nNote: There's also a conflict label on this PR that will need to be resolved before merging.",
      "Thank you for your PR to redesign the block menu on the builder page! This looks like a substantial piece of work that adds many components and backend support.  \n\nHowever, I've noticed a few issues that need to be addressed before this can be merged:\n\n1. **Missing Checklist**: Your PR is missing the required checklist. Please add the complete checklist from the PR template and check off the items you've completed. Given the scope of your changes, the checklist is necessary to ensure everything has been properly tested.\n\n2. **PR Title/Scope Mismatch**: Your PR title only mentions frontend changes, but there are significant backend changes as well (new API routes, database functions, models). Please update your PR title to reflect both aspects, perhaps something like: `feat(frontend,backend): Redesign block menu on builder page with API support`\n\n3. **Test Plan**: Please provide a test plan detailing how you've verified that both the frontend components and backend API routes work correctly.\n\n4. **PR Description**: Consider expanding your description to briefly explain the purpose of the backend changes and how they support the frontend redesign.\n\nYour component organization looks good, and I appreciate that you've marked future tasks with `BLOCK MENU TODO:`. Once you address these items, we'll be able to move forward with the review process.",
      "Thanks for your contribution to redesign the block menu! This is a substantial PR with changes across both frontend and backend.\n\n### Missing Required Checklist\n\nPlease add the standard PR checklist to your description and complete it. This is required for all PRs with material code changes. The checklist helps ensure you've tested your changes appropriately and considered all necessary factors.\n\n### Scope Consideration\n\nYour PR title mentions frontend changes, but there are significant backend changes as well (new API endpoints, database functions, models). Consider either:\n1. Updating the PR title to reflect both frontend and backend changes, or\n2. Splitting this into two PRs if the changes are separable (one for backend support, one for frontend implementation)\n\n### TODOs in Code\n\nYou mentioned \"Some tasks are planned for the near future. I've marked them with `BLOCK MENU TODO:` so they can be found easily.\" Please ensure that these TODOs are addressed before the PR is merged, or create follow-up issues to track them.\n\n### Other Notes\n\n- The new backend endpoints look well-structured with appropriate auth middleware\n- The implementation seems comprehensive with search functionality, filtering, and pagination\n- Good job on adding new dependencies in package.json for the enhanced UI components\n\nPlease update your PR description with the required checklist and consider the scope recommendation.",
      "Thank you for your PR! I noticed a few issues that need to be addressed before this can be merged:\n\n## Missing Checklist\nYour PR contains significant code changes but doesn't include the required checklist. Please add the standard checklist to your PR description and ensure all items are checked off.\n\n## Scope Mismatch\nYour PR title mentions only frontend changes (`feat(frontend): Redesign block menu on builder page`), but the actual changes include extensive backend work:\n- New backend routes in `/api/builder/`\n- New database functions and models for builder functionality\n- Changes to store and library endpoints\n\nPlease update your PR title to reflect both frontend and backend changes, for example: `feat(frontend,backend): Redesign block menu and add builder API endpoints`\n\n## PR Description Needs Expansion\nPlease provide more details in your PR description:\n- What problem does this redesign solve?\n- What are the key changes in both frontend and backend?\n- How should reviewers test these changes?\n\nThe current description lists small components but doesn't explain the overall architecture or the backend additions.\n\nOnce these issues are addressed, I'll be happy to review this PR again. Let me know if you need any clarification!",
      "Thanks for your PR on redesigning the block menu! I have a couple of important issues that need to be addressed before this can be merged:\n\n## Required Changes\n\n1. **Missing Checklist**: The PR template requires a checklist to be completed for code changes. Please update your PR description to include the standard checklist and complete it.\n\n2. **PR Title/Scope Mismatch**: Your PR title indicates this is a frontend change (`feat(frontend)`), but the vast majority of changes are actually backend-related. The PR adds several new backend files (db.py, model.py, routes.py) and modifies existing backend code.\n\n   Please update your PR title to accurately reflect the scope of changes, such as `feat(platform): Redesign block menu with backend support` or similar that reflects both the frontend and backend work.\n\n3. **PR Description**: Consider expanding your PR description to explain why this redesign was needed and how it improves the current implementation. \n\n## Additional Notes\n\n- Your backend code changes look good from a security perspective - the functions properly use user_id for access control.\n- I noticed you've added a TODO comment \"BLOCK MENU TODO:\" for future work. Make sure these are tracked separately if they won't be addressed in this PR.\n- You've added some new dependencies in package.json (tailwind-scrollbar, react-timeago, etc.) - it would be helpful to mention these in the PR description.\n\nPlease update your PR with these changes so we can proceed with the review process. Let me know if you need any clarification!",
      "Thanks for your work on redesigning the block menu! I noticed a few issues that need to be addressed before this PR can be merged:\n\n## Description and Scope\n\n- Your PR title indicates only frontend changes, but there are significant backend changes (new API routes, models, etc.). The scope in the title should reflect all areas being modified, such as `feat(platform)` or both frontend and backend.\n- The PR description focuses on frontend components but doesn't mention the backend API work. Please update your description to include details about all changes made.\n\n## Missing Required Checklist\n\n- The PR is missing the required checklist that should be filled out completely. Since this includes substantial code changes, please add the checklist and check off all applicable items.\n- Please include a test plan detailing how you've verified the changes work correctly.\n\n## Suggested Changes\n\n1. Update the PR title to correctly represent the scope of changes (e.g., `feat(platform): Redesign block menu on builder page`)\n2. Expand the PR description to include details about the backend API changes\n3. Add the required checklist template and fill it out completely\n4. Include a test plan that describes how you've tested these changes\n\nOnce these issues are addressed, we can proceed with reviewing the code changes in detail.",
      "Thank you for your PR to redesign the block menu! I've reviewed your changes and have a few concerns that need to be addressed before this can be merged:\n\n## Missing Checklist\nThe PR description is missing the required checklist for code changes. Please update your description to include the standard checklist that confirms you've tested your changes properly.\n\n## Scope Mismatch\nYour PR title indicates this is a frontend change (`feat(frontend): Redesign block menu on builder page`), but the changes include significant backend additions:\n- New builder API endpoints\n- New database functions\n- New models and data structures\n\nThese backend changes appear to be more than 20% of the total changes, which exceeds our scope guidelines. You have two options:\n\n1. Split this into two PRs: one for frontend and one for backend changes\n2. Update the PR title to reflect both areas (e.g., `feat(frontend,backend): Redesign block menu with supporting API endpoints`)\n\nThe new backend code looks good in terms of security and user_id handling, but we need to ensure the PR accurately represents the work being done.\n\nPlease update your PR accordingly, and let me know if you need any help or clarification!",
      "Thanks for the PR to redesign the block menu! The components structure you've listed looks well organized.\n\nI noticed a couple of issues that need to be addressed before this can be merged:\n\n1. **Missing checklist**: The PR template requires a checklist section to be completed for code changes. Please add the checklist from the template and fill it out appropriately, especially including your test plan.\n\n2. **Scope mismatch**: Your PR title indicates this is purely a frontend change (`feat(frontend)`), but the PR contains substantial backend changes including:\n   - New backend API routes in `/api/builder`\n   - New backend models and database methods\n   - Changes to existing backend services\n\nPlease update either:\n   - The PR title to reflect both frontend and backend changes (e.g., `feat(platform): Redesign block menu and add builder API endpoints`)\n   - OR split this into separate PRs for frontend and backend changes\n\nAlso, I notice you've marked some tasks with \"BLOCK MENU TODO\" for future work. It would be helpful to clarify in the PR description which parts are complete in this PR and which parts are planned for future PRs.\n\nLet me know if you need any help with these changes!",
      "Thank you for your work on redesigning the block menu for the builder page. However, there are a couple of issues that need to be addressed before this PR can be merged:\n\n1. **Missing Checklist**: Your PR is missing the required checklist section. Please update your PR description to include the standard checklist and fill it out completely. This helps ensure all necessary steps have been completed before merging.\n\n2. **Scope Mismatch**: Your PR title mentions only frontend changes (`feat(frontend): Redesign block menu on builder page`), but the PR includes extensive backend changes as well. I see new backend routes, models, and database functions being added in:\n   - `backend/server/v2/builder/db.py`\n   - `backend/server/v2/builder/model.py`\n   - `backend/server/v2/builder/routes.py`\n   - And modifications to several other backend files\n\nYou have two options to address this:\n   - Update your PR title to reflect both frontend and backend changes, such as `feat(platform): Redesign block menu with supporting backend APIs`\n   - Or split this into separate PRs - one for frontend and one for backend changes\n\nPlease make these adjustments so we can proceed with the review. The changes themselves look valuable, but we need to ensure the PR follows our standards."
    ],
    "code_diff": "diff --git a/autogpt_platform/backend/backend/blocks/agent.py b/autogpt_platform/backend/backend/blocks/agent.py\nindex c25d99458d6d..406bff9e06bf 100644\n--- a/autogpt_platform/backend/backend/blocks/agent.py\n+++ b/autogpt_platform/backend/backend/blocks/agent.py\n@@ -23,6 +23,9 @@ class Input(BlockSchema):\n         user_id: str = SchemaField(description=\"User ID\")\n         graph_id: str = SchemaField(description=\"Graph ID\")\n         graph_version: int = SchemaField(description=\"Graph Version\")\n+        agent_name: Optional[str] = SchemaField(\n+            default=None, description=\"Name to display in the Builder UI\"\n+        )\n \n         inputs: BlockInput = SchemaField(description=\"Input data for the graph\")\n         input_schema: dict = SchemaField(description=\"Input schema for the graph\")\ndiff --git a/autogpt_platform/backend/backend/server/model.py b/autogpt_platform/backend/backend/server/model.py\nindex 8b7293b65ae7..c7b63634729f 100644\n--- a/autogpt_platform/backend/backend/server/model.py\n+++ b/autogpt_platform/backend/backend/server/model.py\n@@ -74,6 +74,15 @@ class Pagination(pydantic.BaseModel):\n         description=\"Number of items per page.\", examples=[25]\n     )\n \n+    @staticmethod\n+    def empty() -> \"Pagination\":\n+        return Pagination(\n+            total_items=0,\n+            total_pages=0,\n+            current_page=0,\n+            page_size=0,\n+        )\n+\n \n class RequestTopUp(pydantic.BaseModel):\n     credit_amount: int\ndiff --git a/autogpt_platform/backend/backend/server/rest_api.py b/autogpt_platform/backend/backend/server/rest_api.py\nindex 7ebbab9c4a38..5a885b9606a1 100644\n--- a/autogpt_platform/backend/backend/server/rest_api.py\n+++ b/autogpt_platform/backend/backend/server/rest_api.py\n@@ -23,6 +23,8 @@\n import backend.server.routers.v1\n import backend.server.v2.admin.credit_admin_routes\n import backend.server.v2.admin.store_admin_routes\n+import backend.server.v2.builder\n+import backend.server.v2.builder.routes\n import backend.server.v2.library.db\n import backend.server.v2.library.model\n import backend.server.v2.library.routes\n@@ -144,6 +146,9 @@ async def validation_error_handler(\n app.include_router(\n     backend.server.v2.store.routes.router, tags=[\"v2\"], prefix=\"/api/store\"\n )\n+app.include_router(\n+    backend.server.v2.builder.routes.router, tags=[\"v2\"], prefix=\"/api/builder\"\n+)\n app.include_router(\n     backend.server.v2.admin.store_admin_routes.router,\n     tags=[\"v2\", \"admin\"],\ndiff --git a/autogpt_platform/backend/backend/server/v2/builder/db.py b/autogpt_platform/backend/backend/server/v2/builder/db.py\nnew file mode 100644\nindex 000000000000..8cdbf9d9d4c7\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/server/v2/builder/db.py\n@@ -0,0 +1,370 @@\n+import functools\n+import logging\n+\n+import prisma\n+\n+import backend.data.block\n+import backend.server.model as server_model\n+from backend.blocks import load_all_blocks\n+from backend.blocks.llm import LlmModel\n+from backend.data.block import Block, BlockCategory, BlockSchema\n+from backend.data.credit import get_block_costs\n+from backend.integrations.providers import ProviderName\n+from backend.server.v2.builder.model import (\n+    BlockCategoryResponse,\n+    BlockData,\n+    BlockResponse,\n+    BlockType,\n+    CountResponse,\n+    Provider,\n+    ProviderResponse,\n+    SearchBlocksResponse,\n+)\n+\n+logger = logging.getLogger(__name__)\n+llm_models = [name.name.lower().replace(\"_\", \" \") for name in LlmModel]\n+_static_counts_cache: dict | None = None\n+_suggested_blocks: list[BlockData] | None = None\n+\n+\n+def get_block_categories(category_blocks: int = 3) -> list[BlockCategoryResponse]:\n+    categories: dict[BlockCategory, BlockCategoryResponse] = {}\n+\n+    for block_type in load_all_blocks().values():\n+        block: Block[BlockSchema, BlockSchema] = block_type()\n+        # Skip disabled blocks\n+        if block.disabled:\n+            continue\n+        # Skip blocks that don't have categories (all should have at least one)\n+        if not block.categories:\n+            continue\n+\n+        # Add block to the categories\n+        for category in block.categories:\n+            if category not in categories:\n+                categories[category] = BlockCategoryResponse(\n+                    name=category.name.lower(),\n+                    total_blocks=0,\n+                    blocks=[],\n+                )\n+\n+            categories[category].total_blocks += 1\n+\n+            # Append if the category has less than the specified number of blocks\n+            if len(categories[category].blocks) < category_blocks:\n+                categories[category].blocks.append(block.to_dict())\n+\n+    # Sort categories by name\n+    return sorted(categories.values(), key=lambda x: x.name)\n+\n+\n+def get_blocks(\n+    *,\n+    category: str | None = None,\n+    type: BlockType | None = None,\n+    provider: ProviderName | None = None,\n+    page: int = 1,\n+    page_size: int = 50,\n+) -> BlockResponse:\n+    \"\"\"\n+    Get blocks based on either category, type or provider.\n+    Providing nothing fetches all block types.\n+    \"\"\"\n+    # Only one of category, type, or provider can be specified\n+    if (category and type) or (category and provider) or (type and provider):\n+        raise ValueError(\"Only one of category, type, or provider can be specified\")\n+\n+    blocks: list[Block[BlockSchema, BlockSchema]] = []\n+    skip = (page - 1) * page_size\n+    take = page_size\n+    total = 0\n+\n+    for block_type in load_all_blocks().values():\n+        block: Block[BlockSchema, BlockSchema] = block_type()\n+        # Skip disabled blocks\n+        if block.disabled:\n+            continue\n+        # Skip blocks that don't match the category\n+        if category and category not in {c.name.lower() for c in block.categories}:\n+            continue\n+        # Skip blocks that don't match the type\n+        if (\n+            (type == \"input\" and block.block_type.value != \"Input\")\n+            or (type == \"output\" and block.block_type.value != \"Output\")\n+            or (type == \"action\" and block.block_type.value in (\"Input\", \"Output\"))\n+        ):\n+            continue\n+        # Skip blocks that don't match the provider\n+        if provider:\n+            credentials_info = block.input_schema.get_credentials_fields_info().values()\n+            if not any(provider in info.provider for info in credentials_info):\n+                continue\n+\n+        total += 1\n+        if skip > 0:\n+            skip -= 1\n+            continue\n+        if take > 0:\n+            take -= 1\n+            blocks.append(block)\n+\n+    costs = get_block_costs()\n+\n+    return BlockResponse(\n+        blocks=[{**b.to_dict(), \"costs\": costs.get(b.id, [])} for b in blocks],\n+        pagination=server_model.Pagination(\n+            total_items=total,\n+            total_pages=total // page_size + (1 if total % page_size > 0 else 0),\n+            current_page=page,\n+            page_size=page_size,\n+        ),\n+    )\n+\n+\n+def search_blocks(\n+    include_blocks: bool = True,\n+    include_integrations: bool = True,\n+    query: str = \"\",\n+    page: int = 1,\n+    page_size: int = 50,\n+) -> SearchBlocksResponse:\n+    \"\"\"\n+    Get blocks based on the filter and query.\n+    `providers` only applies for `integrations` filter.\n+    \"\"\"\n+    blocks: list[Block[BlockSchema, BlockSchema]] = []\n+    query = query.lower()\n+\n+    total = 0\n+    skip = (page - 1) * page_size\n+    take = page_size\n+    block_count = 0\n+    integration_count = 0\n+\n+    for block_type in load_all_blocks().values():\n+        block: Block[BlockSchema, BlockSchema] = block_type()\n+        # Skip disabled blocks\n+        if block.disabled:\n+            continue\n+        # Skip blocks that don't match the query\n+        if (\n+            query not in block.name.lower()\n+            and query not in block.description.lower()\n+            and not _matches_llm_model(block.input_schema, query)\n+        ):\n+            continue\n+        keep = False\n+        credentials = list(block.input_schema.get_credentials_fields().values())\n+        if include_integrations and len(credentials) > 0:\n+            keep = True\n+            integration_count += 1\n+        if include_blocks and len(credentials) == 0:\n+            keep = True\n+            block_count += 1\n+\n+        if not keep:\n+            continue\n+\n+        total += 1\n+        if skip > 0:\n+            skip -= 1\n+            continue\n+        if take > 0:\n+            take -= 1\n+            blocks.append(block)\n+\n+    costs = get_block_costs()\n+\n+    return SearchBlocksResponse(\n+        blocks=BlockResponse(\n+            blocks=[{**b.to_dict(), \"costs\": costs.get(b.id, [])} for b in blocks],\n+            pagination=server_model.Pagination(\n+                total_items=total,\n+                total_pages=total // page_size + (1 if total % page_size > 0 else 0),\n+                current_page=page,\n+                page_size=page_size,\n+            ),\n+        ),\n+        total_block_count=block_count,\n+        total_integration_count=integration_count,\n+    )\n+\n+\n+def get_providers(\n+    query: str = \"\",\n+    page: int = 1,\n+    page_size: int = 50,\n+) -> ProviderResponse:\n+    providers = []\n+    query = query.lower()\n+\n+    skip = (page - 1) * page_size\n+    take = page_size\n+\n+    all_providers = _get_all_providers()\n+\n+    for provider in all_providers.values():\n+        if (\n+            query not in provider.name.value.lower()\n+            and query not in provider.description.lower()\n+        ):\n+            continue\n+        if skip > 0:\n+            skip -= 1\n+            continue\n+        if take > 0:\n+            take -= 1\n+            providers.append(provider)\n+\n+    total = len(all_providers)\n+\n+    return ProviderResponse(\n+        providers=providers,\n+        pagination=server_model.Pagination(\n+            total_items=total,\n+            total_pages=total // page_size + (1 if total % page_size > 0 else 0),\n+            current_page=page,\n+            page_size=page_size,\n+        ),\n+    )\n+\n+\n+async def get_counts(user_id: str) -> CountResponse:\n+    my_agents = await prisma.models.LibraryAgent.prisma().count(\n+        where={\n+            \"userId\": user_id,\n+            \"isDeleted\": False,\n+            \"isArchived\": False,\n+        }\n+    )\n+    counts = await _get_static_counts()\n+    return CountResponse(\n+        my_agents=my_agents,\n+        **counts,\n+    )\n+\n+\n+async def _get_static_counts():\n+    \"\"\"\n+    Get counts of blocks, integrations, and marketplace agents.\n+    This is cached to avoid unnecessary database queries and calculations.\n+    Can't use functools.cache here because the function is async.\n+    \"\"\"\n+    global _static_counts_cache\n+    if _static_counts_cache is not None:\n+        return _static_counts_cache\n+\n+    all_blocks = 0\n+    input_blocks = 0\n+    action_blocks = 0\n+    output_blocks = 0\n+    integrations = 0\n+\n+    for block_type in load_all_blocks().values():\n+        block: Block[BlockSchema, BlockSchema] = block_type()\n+        if block.disabled:\n+            continue\n+\n+        all_blocks += 1\n+\n+        if block.block_type.value == \"Input\":\n+            input_blocks += 1\n+        elif block.block_type.value == \"Output\":\n+            output_blocks += 1\n+        else:\n+            action_blocks += 1\n+\n+        credentials = list(block.input_schema.get_credentials_fields().values())\n+        if len(credentials) > 0:\n+            integrations += 1\n+\n+    marketplace_agents = await prisma.models.StoreAgent.prisma().count()\n+\n+    _static_counts_cache = {\n+        \"all_blocks\": all_blocks,\n+        \"input_blocks\": input_blocks,\n+        \"action_blocks\": action_blocks,\n+        \"output_blocks\": output_blocks,\n+        \"integrations\": integrations,\n+        \"marketplace_agents\": marketplace_agents,\n+    }\n+\n+    return _static_counts_cache\n+\n+\n+def _matches_llm_model(schema_cls: type[BlockSchema], query: str) -> bool:\n+    for field in schema_cls.model_fields.values():\n+        if field.annotation == LlmModel:\n+            # Check if query matches any value in llm_models\n+            if any(query in name for name in llm_models):\n+                return True\n+    return False\n+\n+\n+@functools.cache\n+def _get_all_providers() -> dict[ProviderName, Provider]:\n+    providers: dict[ProviderName, Provider] = {}\n+\n+    for block_type in load_all_blocks().values():\n+        block: Block[BlockSchema, BlockSchema] = block_type()\n+        if block.disabled:\n+            continue\n+\n+        credentials_info = block.input_schema.get_credentials_fields_info().values()\n+        for info in credentials_info:\n+            for provider in info.provider:  # provider is a ProviderName enum member\n+                if provider in providers:\n+                    providers[provider].integration_count += 1\n+                else:\n+                    providers[provider] = Provider(\n+                        name=provider, description=\"\", integration_count=1\n+                    )\n+    return providers\n+\n+\n+async def get_suggested_blocks(count: int = 5) -> list[BlockData]:\n+    global _suggested_blocks\n+\n+    if _suggested_blocks is not None and len(_suggested_blocks) >= count:\n+        return _suggested_blocks[:count]\n+\n+    _suggested_blocks = []\n+    # Sum the number of executions for each block type\n+    # Prisma cannot group by nested relations, so we do a raw query\n+    results = await prisma.get_client().query_raw(\n+        \"\"\"\n+        SELECT\n+            agent_node.\"agentBlockId\" AS block_id,\n+            COUNT(execution.id) AS execution_count\n+        FROM \"AgentNodeExecution\" execution\n+        JOIN \"AgentNode\" agent_node ON execution.\"agentNodeId\" = agent_node.id\n+        GROUP BY agent_node.\"agentBlockId\"\n+        ORDER BY execution_count DESC;\n+        \"\"\"\n+    )\n+\n+    # Get the top blocks based on execution count\n+    # But ignore Input and Output blocks\n+    blocks: list[tuple[BlockData, int]] = []\n+\n+    for block_type in load_all_blocks().values():\n+        block: Block[BlockSchema, BlockSchema] = block_type()\n+        if block.disabled or block.block_type in (\n+            backend.data.block.BlockType.INPUT,\n+            backend.data.block.BlockType.OUTPUT,\n+            backend.data.block.BlockType.AGENT,\n+        ):\n+            continue\n+        # Find the execution count for this block\n+        execution_count = next(\n+            (row[\"execution_count\"] for row in results if row[\"block_id\"] == block.id),\n+            0,\n+        )\n+        blocks.append((block.to_dict(), execution_count))\n+    # Sort blocks by execution count\n+    blocks.sort(key=lambda x: x[1], reverse=True)\n+\n+    _suggested_blocks = [block[0] for block in blocks]\n+\n+    # Return the top blocks\n+    return _suggested_blocks[:count]\ndiff --git a/autogpt_platform/backend/backend/server/v2/builder/model.py b/autogpt_platform/backend/backend/server/v2/builder/model.py\nnew file mode 100644\nindex 000000000000..475010642d59\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/server/v2/builder/model.py\n@@ -0,0 +1,87 @@\n+from typing import Any, Literal\n+\n+from pydantic import BaseModel\n+\n+import backend.server.model as server_model\n+import backend.server.v2.library.model as library_model\n+import backend.server.v2.store.model as store_model\n+from backend.integrations.providers import ProviderName\n+\n+FilterType = Literal[\n+    \"blocks\",\n+    \"integrations\",\n+    \"marketplace_agents\",\n+    \"my_agents\",\n+]\n+\n+BlockType = Literal[\"all\", \"input\", \"action\", \"output\"]\n+\n+BlockData = dict[str, Any]\n+\n+\n+# Suggestions\n+class SuggestionsResponse(BaseModel):\n+    otto_suggestions: list[str]\n+    recent_searches: list[str]\n+    providers: list[ProviderName]\n+    top_blocks: list[BlockData]\n+\n+\n+# All blocks\n+class BlockCategoryResponse(BaseModel):\n+    name: str\n+    total_blocks: int\n+    blocks: list[BlockData]\n+\n+    model_config = {\"use_enum_values\": False}  # <== use enum names like \"AI\"\n+\n+\n+# Input/Action/Output and see all for block categories\n+class BlockResponse(BaseModel):\n+    blocks: list[BlockData]\n+    pagination: server_model.Pagination\n+\n+\n+# Providers\n+class Provider(BaseModel):\n+    name: ProviderName\n+    description: str\n+    integration_count: int\n+\n+\n+class ProviderResponse(BaseModel):\n+    providers: list[Provider]\n+    pagination: server_model.Pagination\n+\n+\n+# Search\n+class SearchRequest(BaseModel):\n+    search_query: str | None = None\n+    filter: list[FilterType] | None = None\n+    by_creator: list[str] | None = None\n+    search_id: str | None = None\n+    page: int | None = None\n+    page_size: int | None = None\n+\n+\n+class SearchBlocksResponse(BaseModel):\n+    blocks: BlockResponse\n+    total_block_count: int\n+    total_integration_count: int\n+\n+\n+class SearchResponse(BaseModel):\n+    items: list[BlockData | library_model.LibraryAgent | store_model.StoreAgent]\n+    total_items: dict[FilterType, int]\n+    page: int\n+    more_pages: bool\n+\n+\n+class CountResponse(BaseModel):\n+    all_blocks: int\n+    input_blocks: int\n+    action_blocks: int\n+    output_blocks: int\n+    integrations: int\n+    marketplace_agents: int\n+    my_agents: int\ndiff --git a/autogpt_platform/backend/backend/server/v2/builder/routes.py b/autogpt_platform/backend/backend/server/v2/builder/routes.py\nnew file mode 100644\nindex 000000000000..57934203892a\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/server/v2/builder/routes.py\n@@ -0,0 +1,227 @@\n+import logging\n+from typing import Annotated, Sequence\n+\n+import fastapi\n+from autogpt_libs.auth.depends import auth_middleware, get_user_id\n+\n+import backend.server.model as server_model\n+import backend.server.v2.builder.db as builder_db\n+import backend.server.v2.builder.model as builder_model\n+import backend.server.v2.library.db as library_db\n+import backend.server.v2.library.model as library_model\n+import backend.server.v2.store.db as store_db\n+import backend.server.v2.store.model as store_model\n+from backend.integrations.providers import ProviderName\n+\n+logger = logging.getLogger(__name__)\n+\n+router = fastapi.APIRouter()\n+\n+\n+# Taken from backend/server/v2/store/db.py\n+def sanitize_query(query: str | None) -> str | None:\n+    if query is None:\n+        return query\n+    query = query.strip()[:100]\n+    return (\n+        query.replace(\"\\\\\", \"\\\\\\\\\")\n+        .replace(\"%\", \"\\\\%\")\n+        .replace(\"_\", \"\\\\_\")\n+        .replace(\"[\", \"\\\\[\")\n+        .replace(\"]\", \"\\\\]\")\n+        .replace(\"'\", \"\\\\'\")\n+        .replace('\"', '\\\\\"')\n+        .replace(\";\", \"\\\\;\")\n+        .replace(\"--\", \"\\\\--\")\n+        .replace(\"/*\", \"\\\\/*\")\n+        .replace(\"*/\", \"\\\\*/\")\n+    )\n+\n+\n+@router.get(\n+    \"/suggestions\",\n+    dependencies=[fastapi.Depends(auth_middleware)],\n+)\n+async def get_suggestions(\n+    user_id: Annotated[str, fastapi.Depends(get_user_id)],\n+) -> builder_model.SuggestionsResponse:\n+    \"\"\"\n+    Get all suggestions for the Blocks Menu.\n+    \"\"\"\n+    return builder_model.SuggestionsResponse(\n+        otto_suggestions=[\n+            \"What blocks do I need to get started?\",\n+            \"Help me create a list\",\n+            \"Help me feed my data to Google Maps\",\n+        ],\n+        recent_searches=[\n+            \"image generation\",\n+            \"deepfake\",\n+            \"competitor analysis\",\n+        ],\n+        providers=[\n+            ProviderName.TWITTER,\n+            ProviderName.GITHUB,\n+            ProviderName.NOTION,\n+            ProviderName.GOOGLE,\n+            ProviderName.DISCORD,\n+            ProviderName.GOOGLE_MAPS,\n+        ],\n+        top_blocks=await builder_db.get_suggested_blocks(),\n+    )\n+\n+\n+@router.get(\n+    \"/categories\",\n+    dependencies=[fastapi.Depends(auth_middleware)],\n+)\n+async def get_block_categories(\n+    category_blocks: Annotated[int, fastapi.Query()] = 3,\n+) -> Sequence[builder_model.BlockCategoryResponse]:\n+    \"\"\"\n+    Get all block categories with a specified number of blocks per category.\n+    \"\"\"\n+    return builder_db.get_block_categories(category_blocks)\n+\n+\n+@router.get(\n+    \"/blocks\",\n+    dependencies=[fastapi.Depends(auth_middleware)],\n+)\n+async def get_blocks(\n+    category: Annotated[str | None, fastapi.Query()] = None,\n+    type: Annotated[builder_model.BlockType | None, fastapi.Query()] = None,\n+    provider: Annotated[ProviderName | None, fastapi.Query()] = None,\n+    page: Annotated[int, fastapi.Query()] = 1,\n+    page_size: Annotated[int, fastapi.Query()] = 50,\n+) -> builder_model.BlockResponse:\n+    \"\"\"\n+    Get blocks based on either category, type, or provider.\n+    \"\"\"\n+    return builder_db.get_blocks(\n+        category=category,\n+        type=type,\n+        provider=provider,\n+        page=page,\n+        page_size=page_size,\n+    )\n+\n+\n+@router.get(\n+    \"/providers\",\n+    dependencies=[fastapi.Depends(auth_middleware)],\n+)\n+async def get_providers(\n+    page: Annotated[int, fastapi.Query()] = 1,\n+    page_size: Annotated[int, fastapi.Query()] = 50,\n+) -> builder_model.ProviderResponse:\n+    \"\"\"\n+    Get all integration providers with their block counts.\n+    \"\"\"\n+    return builder_db.get_providers(\n+        page=page,\n+        page_size=page_size,\n+    )\n+\n+\n+@router.post(\n+    \"/search\",\n+    tags=[\"store\", \"private\"],\n+    dependencies=[fastapi.Depends(auth_middleware)],\n+)\n+async def search(\n+    options: builder_model.SearchRequest,\n+    user_id: Annotated[str, fastapi.Depends(get_user_id)],\n+) -> builder_model.SearchResponse:\n+    \"\"\"\n+    Search for blocks (including integrations), marketplace agents, and user library agents.\n+    \"\"\"\n+    # If no filters are provided, then we will return all types\n+    if not options.filter:\n+        options.filter = [\n+            \"blocks\",\n+            \"integrations\",\n+            \"marketplace_agents\",\n+            \"my_agents\",\n+        ]\n+    options.search_query = sanitize_query(options.search_query)\n+    options.page = options.page or 1\n+    options.page_size = options.page_size or 50\n+\n+    # Blocks&Integrations\n+    blocks = builder_model.SearchBlocksResponse(\n+        blocks=builder_model.BlockResponse(\n+            blocks=[],\n+            pagination=server_model.Pagination.empty(),\n+        ),\n+        total_block_count=0,\n+        total_integration_count=0,\n+    )\n+    if \"blocks\" in options.filter or \"integrations\" in options.filter:\n+        blocks = builder_db.search_blocks(\n+            include_blocks=\"blocks\" in options.filter,\n+            include_integrations=\"integrations\" in options.filter,\n+            query=options.search_query or \"\",\n+            page=options.page,\n+            page_size=options.page_size,\n+        )\n+\n+    # Library Agents\n+    my_agents = library_model.LibraryAgentResponse(\n+        agents=[],\n+        pagination=server_model.Pagination.empty(),\n+    )\n+    if \"my_agents\" in options.filter:\n+        my_agents = await library_db.list_library_agents(\n+            user_id=user_id,\n+            search_term=options.search_query,\n+            page=options.page,\n+            page_size=options.page_size,\n+        )\n+\n+    # Marketplace Agents\n+    marketplace_agents = store_model.StoreAgentsResponse(\n+        agents=[],\n+        pagination=server_model.Pagination.empty(),\n+    )\n+    if \"marketplace_agents\" in options.filter:\n+        marketplace_agents = await store_db.get_store_agents(\n+            creators=options.by_creator,\n+            search_query=options.search_query,\n+            page=options.page,\n+            page_size=options.page_size,\n+        )\n+\n+    more_pages = False\n+    if (\n+        blocks.blocks.pagination.current_page < blocks.blocks.pagination.total_pages\n+        or my_agents.pagination.current_page < my_agents.pagination.total_pages\n+        or marketplace_agents.pagination.current_page\n+        < marketplace_agents.pagination.total_pages\n+    ):\n+        more_pages = True\n+\n+    return builder_model.SearchResponse(\n+        items=blocks.blocks.blocks + my_agents.agents + marketplace_agents.agents,\n+        total_items={\n+            \"blocks\": blocks.total_block_count,\n+            \"integrations\": blocks.total_integration_count,\n+            \"marketplace_agents\": marketplace_agents.pagination.total_items,\n+            \"my_agents\": my_agents.pagination.total_items,\n+        },\n+        page=options.page,\n+        more_pages=more_pages,\n+    )\n+\n+\n+@router.get(\n+    \"/counts\",\n+    dependencies=[fastapi.Depends(auth_middleware)],\n+)\n+async def get_counts(\n+    user_id: Annotated[str, fastapi.Depends(get_user_id)],\n+) -> builder_model.CountResponse:\n+    \"\"\"\n+    Get item counts for the menu categories in the Blocks Menu.\n+    \"\"\"\n+    return await builder_db.get_counts(user_id)\ndiff --git a/autogpt_platform/backend/backend/server/v2/library/db.py b/autogpt_platform/backend/backend/server/v2/library/db.py\nindex b0f8f7a755c9..d4e6d4f4a476 100644\n--- a/autogpt_platform/backend/backend/server/v2/library/db.py\n+++ b/autogpt_platform/backend/backend/server/v2/library/db.py\n@@ -448,7 +448,7 @@ async def add_store_agent_to_library(\n                             \"agentGraphVersion\": graph.version,\n                         }\n                     },\n-                    include={\"AgentGraph\": True},\n+                    include=library_agent_include(user_id),\n                 )\n             )\n             if existing_library_agent:\ndiff --git a/autogpt_platform/backend/backend/server/v2/library/model.py b/autogpt_platform/backend/backend/server/v2/library/model.py\nindex 8c39f3dc3b04..2ea8798361eb 100644\n--- a/autogpt_platform/backend/backend/server/v2/library/model.py\n+++ b/autogpt_platform/backend/backend/server/v2/library/model.py\n@@ -42,6 +42,7 @@ class LibraryAgent(pydantic.BaseModel):\n \n     # Made input_schema and output_schema match GraphMeta's type\n     input_schema: dict[str, Any]  # Should be BlockIOObjectSubSchema in frontend\n+    output_schema: dict[str, Any]\n \n     # Indicates whether there's a new output (based on recent runs)\n     new_output: bool\n@@ -106,6 +107,7 @@ def from_db(agent: prisma.models.LibraryAgent) -> \"LibraryAgent\":\n             name=graph.name,\n             description=graph.description,\n             input_schema=graph.input_schema,\n+            output_schema=graph.output_schema,\n             new_output=new_output,\n             can_access_graph=can_access_graph,\n             is_latest_version=is_latest_version,\ndiff --git a/autogpt_platform/backend/backend/server/v2/library/routes_test.py b/autogpt_platform/backend/backend/server/v2/library/routes_test.py\nindex 6cfb1ffce035..99d9bd4f1af2 100644\n--- a/autogpt_platform/backend/backend/server/v2/library/routes_test.py\n+++ b/autogpt_platform/backend/backend/server/v2/library/routes_test.py\n@@ -50,6 +50,7 @@ async def test_get_library_agents_success(\n                 creator_name=\"Test Creator\",\n                 creator_image_url=\"\",\n                 input_schema={\"type\": \"object\", \"properties\": {}},\n+                output_schema={\"type\": \"object\", \"properties\": {}},\n                 status=library_model.LibraryAgentStatus.COMPLETED,\n                 new_output=False,\n                 can_access_graph=True,\n@@ -66,6 +67,7 @@ async def test_get_library_agents_success(\n                 creator_name=\"Test Creator\",\n                 creator_image_url=\"\",\n                 input_schema={\"type\": \"object\", \"properties\": {}},\n+                output_schema={\"type\": \"object\", \"properties\": {}},\n                 status=library_model.LibraryAgentStatus.COMPLETED,\n                 new_output=False,\n                 can_access_graph=False,\ndiff --git a/autogpt_platform/backend/backend/server/v2/store/db.py b/autogpt_platform/backend/backend/server/v2/store/db.py\nindex dac8cce8a520..707e20befd9b 100644\n--- a/autogpt_platform/backend/backend/server/v2/store/db.py\n+++ b/autogpt_platform/backend/backend/server/v2/store/db.py\n@@ -37,7 +37,7 @@ def sanitize_query(query: str | None) -> str | None:\n \n async def get_store_agents(\n     featured: bool = False,\n-    creator: str | None = None,\n+    creators: list[str] | None = None,\n     sorted_by: str | None = None,\n     search_query: str | None = None,\n     category: str | None = None,\n@@ -48,15 +48,15 @@ async def get_store_agents(\n     Get PUBLIC store agents from the StoreAgent view\n     \"\"\"\n     logger.debug(\n-        f\"Getting store agents. featured={featured}, creator={creator}, sorted_by={sorted_by}, search={search_query}, category={category}, page={page}\"\n+        f\"Getting store agents. featured={featured}, creator={creators}, sorted_by={sorted_by}, search={search_query}, category={category}, page={page}\"\n     )\n     sanitized_query = sanitize_query(search_query)\n \n     where_clause = {}\n     if featured:\n         where_clause[\"featured\"] = featured\n-    if creator:\n-        where_clause[\"creator_username\"] = creator\n+    if creators:\n+        where_clause[\"creator_username\"] = {\"in\": creators}\n     if category:\n         where_clause[\"categories\"] = {\"has\": category}\n \ndiff --git a/autogpt_platform/backend/backend/server/v2/store/routes.py b/autogpt_platform/backend/backend/server/v2/store/routes.py\nindex 41795f5d887b..f71665d93794 100644\n--- a/autogpt_platform/backend/backend/server/v2/store/routes.py\n+++ b/autogpt_platform/backend/backend/server/v2/store/routes.py\n@@ -158,7 +158,7 @@ async def get_agents(\n     try:\n         agents = await backend.server.v2.store.db.get_store_agents(\n             featured=featured,\n-            creator=creator,\n+            creators=[creator] if creator else None,\n             sorted_by=sorted_by,\n             search_query=search_query,\n             category=category,\ndiff --git a/autogpt_platform/frontend/package.json b/autogpt_platform/frontend/package.json\nindex b8abd0baf04c..60e79c2472b1 100644\n--- a/autogpt_platform/frontend/package.json\n+++ b/autogpt_platform/frontend/package.json\n@@ -82,9 +82,12 @@\n     \"react-markdown\": \"9.0.3\",\n     \"react-modal\": \"3.16.3\",\n     \"react-shepherd\": \"6.1.8\",\n+    \"react-timeago\": \"^8.2.0\",\n     \"recharts\": \"2.15.3\",\n     \"shepherd.js\": \"14.5.0\",\n     \"tailwind-merge\": \"2.6.0\",\n+    \"tailwind-scrollbar\": \"^4.0.2\",\n+    \"tailwind-scrollbar-hide\": \"^2.0.0\",\n     \"tailwindcss-animate\": \"1.0.7\",\n     \"uuid\": \"11.1.0\",\n     \"zod\": \"3.25.56\"\ndiff --git a/autogpt_platform/frontend/pnpm-lock.yaml b/autogpt_platform/frontend/pnpm-lock.yaml\nindex e418b8bb879f..e6bea7076a8f 100644\n--- a/autogpt_platform/frontend/pnpm-lock.yaml\n+++ b/autogpt_platform/frontend/pnpm-lock.yaml\n@@ -79,7 +79,7 @@ importers:\n         version: 1.2.7(@types/react-dom@18.3.5(@types/react@18.3.17))(@types/react@18.3.17)(react-dom@18.3.1(react@18.3.1))(react@18.3.1)\n       '@sentry/nextjs':\n         specifier: 9.27.0\n-        version: 9.27.0(@opentelemetry/context-async-hooks@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/core@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/instrumentation@0.57.2(@opentelemetry/api@1.9.0))(@opentelemetry/sdk-trace-base@1.30.1(@opentelemetry/api@1.9.0))(next@15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1))(react@18.3.1)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+        version: 9.27.0(@opentelemetry/context-async-hooks@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/core@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/instrumentation@0.57.2(@opentelemetry/api@1.9.0))(@opentelemetry/sdk-trace-base@1.30.1(@opentelemetry/api@1.9.0))(next@15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1))(react@18.3.1)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       '@supabase/ssr':\n         specifier: 0.6.1\n         version: 0.6.1(@supabase/supabase-js@2.50.0)\n@@ -182,6 +182,9 @@ importers:\n       react-shepherd:\n         specifier: 6.1.8\n         version: 6.1.8(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(typescript@5.8.3)\n+      react-timeago:\n+        specifier: ^8.2.0\n+        version: 8.2.0(react@18.3.1)\n       recharts:\n         specifier: 2.15.3\n         version: 2.15.3(react-dom@18.3.1(react@18.3.1))(react@18.3.1)\n@@ -191,6 +194,12 @@ importers:\n       tailwind-merge:\n         specifier: 2.6.0\n         version: 2.6.0\n+      tailwind-scrollbar:\n+        specifier: ^4.0.2\n+        version: 4.0.2(react@18.3.1)(tailwindcss@3.4.17)\n+      tailwind-scrollbar-hide:\n+        specifier: ^2.0.0\n+        version: 2.0.0(tailwindcss@3.4.17)\n       tailwindcss-animate:\n         specifier: 1.0.7\n         version: 1.0.7(tailwindcss@3.4.17)\n@@ -233,7 +242,7 @@ importers:\n         version: 8.6.14(storybook@8.6.14(prettier@3.5.3))\n       '@storybook/nextjs':\n         specifier: 8.6.14\n-        version: 8.6.14(@swc/core@1.12.1)(esbuild@0.25.5)(next@15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1))(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(type-fest@4.41.0)(typescript@5.8.3)(webpack-hot-middleware@2.26.1)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+        version: 8.6.14(@swc/core@1.11.31)(esbuild@0.24.2)(next@15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1))(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(type-fest@4.41.0)(typescript@5.8.3)(webpack-hot-middleware@2.26.1)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       '@storybook/react':\n         specifier: 8.6.14\n         version: 8.6.14(@storybook/test@8.6.14(storybook@8.6.14(prettier@3.5.3)))(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)\n@@ -269,7 +278,7 @@ importers:\n         version: 3.16.3\n       axe-playwright:\n         specifier: 2.1.0\n-        version: 2.1.0(playwright@1.53.0)\n+        version: 2.1.0(playwright@1.52.0)\n       chromatic:\n         specifier: 11.25.2\n         version: 11.25.2\n@@ -991,152 +1000,152 @@ packages:\n   '@emotion/unitless@0.8.1':\n     resolution: {integrity: sha512-KOEGMu6dmJZtpadb476IsZBclKvILjopjUii3V+7MnXIQCYh8W3NgNcgwo21n9LXZX6EDIKvqfjYxXebDwxKmQ==}\n \n-  '@esbuild/aix-ppc64@0.25.5':\n-    resolution: {integrity: sha512-9o3TMmpmftaCMepOdA5k/yDw8SfInyzWWTjYTFCX3kPSDJMROQTb8jg+h9Cnwnmm1vOzvxN7gIfB5V2ewpjtGA==}\n+  '@esbuild/aix-ppc64@0.24.2':\n+    resolution: {integrity: sha512-thpVCb/rhxE/BnMLQ7GReQLLN8q9qbHmI55F4489/ByVg2aQaQ6kbcLb6FHkocZzQhxc4gx0sCk0tJkKBFzDhA==}\n     engines: {node: '>=18'}\n     cpu: [ppc64]\n     os: [aix]\n \n-  '@esbuild/android-arm64@0.25.5':\n-    resolution: {integrity: sha512-VGzGhj4lJO+TVGV1v8ntCZWJktV7SGCs3Pn1GRWI1SBFtRALoomm8k5E9Pmwg3HOAal2VDc2F9+PM/rEY6oIDg==}\n+  '@esbuild/android-arm64@0.24.2':\n+    resolution: {integrity: sha512-cNLgeqCqV8WxfcTIOeL4OAtSmL8JjcN6m09XIgro1Wi7cF4t/THaWEa7eL5CMoMBdjoHOTh/vwTO/o2TRXIyzg==}\n     engines: {node: '>=18'}\n     cpu: [arm64]\n     os: [android]\n \n-  '@esbuild/android-arm@0.25.5':\n-    resolution: {integrity: sha512-AdJKSPeEHgi7/ZhuIPtcQKr5RQdo6OO2IL87JkianiMYMPbCtot9fxPbrMiBADOWWm3T2si9stAiVsGbTQFkbA==}\n+  '@esbuild/android-arm@0.24.2':\n+    resolution: {integrity: sha512-tmwl4hJkCfNHwFB3nBa8z1Uy3ypZpxqxfTQOcHX+xRByyYgunVbZ9MzUUfb0RxaHIMnbHagwAxuTL+tnNM+1/Q==}\n     engines: {node: '>=18'}\n     cpu: [arm]\n     os: [android]\n \n-  '@esbuild/android-x64@0.25.5':\n-    resolution: {integrity: sha512-D2GyJT1kjvO//drbRT3Hib9XPwQeWd9vZoBJn+bu/lVsOZ13cqNdDeqIF/xQ5/VmWvMduP6AmXvylO/PIc2isw==}\n+  '@esbuild/android-x64@0.24.2':\n+    resolution: {integrity: sha512-B6Q0YQDqMx9D7rvIcsXfmJfvUYLoP722bgfBlO5cGvNVb5V/+Y7nhBE3mHV9OpxBf4eAS2S68KZztiPaWq4XYw==}\n     engines: {node: '>=18'}\n     cpu: [x64]\n     os: [android]\n \n-  '@esbuild/darwin-arm64@0.25.5':\n-    resolution: {integrity: sha512-GtaBgammVvdF7aPIgH2jxMDdivezgFu6iKpmT+48+F8Hhg5J/sfnDieg0aeG/jfSvkYQU2/pceFPDKlqZzwnfQ==}\n+  '@esbuild/darwin-arm64@0.24.2':\n+    resolution: {integrity: sha512-kj3AnYWc+CekmZnS5IPu9D+HWtUI49hbnyqk0FLEJDbzCIQt7hg7ucF1SQAilhtYpIujfaHr6O0UHlzzSPdOeA==}\n     engines: {node: '>=18'}\n     cpu: [arm64]\n     os: [darwin]\n \n-  '@esbuild/darwin-x64@0.25.5':\n-    resolution: {integrity: sha512-1iT4FVL0dJ76/q1wd7XDsXrSW+oLoquptvh4CLR4kITDtqi2e/xwXwdCVH8hVHU43wgJdsq7Gxuzcs6Iq/7bxQ==}\n+  '@esbuild/darwin-x64@0.24.2':\n+    resolution: {integrity: sha512-WeSrmwwHaPkNR5H3yYfowhZcbriGqooyu3zI/3GGpF8AyUdsrrP0X6KumITGA9WOyiJavnGZUwPGvxvwfWPHIA==}\n     engines: {node: '>=18'}\n     cpu: [x64]\n     os: [darwin]\n \n-  '@esbuild/freebsd-arm64@0.25.5':\n-    resolution: {integrity: sha512-nk4tGP3JThz4La38Uy/gzyXtpkPW8zSAmoUhK9xKKXdBCzKODMc2adkB2+8om9BDYugz+uGV7sLmpTYzvmz6Sw==}\n+  '@esbuild/freebsd-arm64@0.24.2':\n+    resolution: {integrity: sha512-UN8HXjtJ0k/Mj6a9+5u6+2eZ2ERD7Edt1Q9IZiB5UZAIdPnVKDoG7mdTVGhHJIeEml60JteamR3qhsr1r8gXvg==}\n     engines: {node: '>=18'}\n     cpu: [arm64]\n     os: [freebsd]\n \n-  '@esbuild/freebsd-x64@0.25.5':\n-    resolution: {integrity: sha512-PrikaNjiXdR2laW6OIjlbeuCPrPaAl0IwPIaRv+SMV8CiM8i2LqVUHFC1+8eORgWyY7yhQY+2U2fA55mBzReaw==}\n+  '@esbuild/freebsd-x64@0.24.2':\n+    resolution: {integrity: sha512-TvW7wE/89PYW+IevEJXZ5sF6gJRDY/14hyIGFXdIucxCsbRmLUcjseQu1SyTko+2idmCw94TgyaEZi9HUSOe3Q==}\n     engines: {node: '>=18'}\n     cpu: [x64]\n     os: [freebsd]\n \n-  '@esbuild/linux-arm64@0.25.5':\n-    resolution: {integrity: sha512-Z9kfb1v6ZlGbWj8EJk9T6czVEjjq2ntSYLY2cw6pAZl4oKtfgQuS4HOq41M/BcoLPzrUbNd+R4BXFyH//nHxVg==}\n+  '@esbuild/linux-arm64@0.24.2':\n+    resolution: {integrity: sha512-7HnAD6074BW43YvvUmE/35Id9/NB7BeX5EoNkK9obndmZBUk8xmJJeU7DwmUeN7tkysslb2eSl6CTrYz6oEMQg==}\n     engines: {node: '>=18'}\n     cpu: [arm64]\n     os: [linux]\n \n-  '@esbuild/linux-arm@0.25.5':\n-    resolution: {integrity: sha512-cPzojwW2okgh7ZlRpcBEtsX7WBuqbLrNXqLU89GxWbNt6uIg78ET82qifUy3W6OVww6ZWobWub5oqZOVtwolfw==}\n+  '@esbuild/linux-arm@0.24.2':\n+    resolution: {integrity: sha512-n0WRM/gWIdU29J57hJyUdIsk0WarGd6To0s+Y+LwvlC55wt+GT/OgkwoXCXvIue1i1sSNWblHEig00GBWiJgfA==}\n     engines: {node: '>=18'}\n     cpu: [arm]\n     os: [linux]\n \n-  '@esbuild/linux-ia32@0.25.5':\n-    resolution: {integrity: sha512-sQ7l00M8bSv36GLV95BVAdhJ2QsIbCuCjh/uYrWiMQSUuV+LpXwIqhgJDcvMTj+VsQmqAHL2yYaasENvJ7CDKA==}\n+  '@esbuild/linux-ia32@0.24.2':\n+    resolution: {integrity: sha512-sfv0tGPQhcZOgTKO3oBE9xpHuUqguHvSo4jl+wjnKwFpapx+vUDcawbwPNuBIAYdRAvIDBfZVvXprIj3HA+Ugw==}\n     engines: {node: '>=18'}\n     cpu: [ia32]\n     os: [linux]\n \n-  '@esbuild/linux-loong64@0.25.5':\n-    resolution: {integrity: sha512-0ur7ae16hDUC4OL5iEnDb0tZHDxYmuQyhKhsPBV8f99f6Z9KQM02g33f93rNH5A30agMS46u2HP6qTdEt6Q1kg==}\n+  '@esbuild/linux-loong64@0.24.2':\n+    resolution: {integrity: sha512-CN9AZr8kEndGooS35ntToZLTQLHEjtVB5n7dl8ZcTZMonJ7CCfStrYhrzF97eAecqVbVJ7APOEe18RPI4KLhwQ==}\n     engines: {node: '>=18'}\n     cpu: [loong64]\n     os: [linux]\n \n-  '@esbuild/linux-mips64el@0.25.5':\n-    resolution: {integrity: sha512-kB/66P1OsHO5zLz0i6X0RxlQ+3cu0mkxS3TKFvkb5lin6uwZ/ttOkP3Z8lfR9mJOBk14ZwZ9182SIIWFGNmqmg==}\n+  '@esbuild/linux-mips64el@0.24.2':\n+    resolution: {integrity: sha512-iMkk7qr/wl3exJATwkISxI7kTcmHKE+BlymIAbHO8xanq/TjHaaVThFF6ipWzPHryoFsesNQJPE/3wFJw4+huw==}\n     engines: {node: '>=18'}\n     cpu: [mips64el]\n     os: [linux]\n \n-  '@esbuild/linux-ppc64@0.25.5':\n-    resolution: {integrity: sha512-UZCmJ7r9X2fe2D6jBmkLBMQetXPXIsZjQJCjgwpVDz+YMcS6oFR27alkgGv3Oqkv07bxdvw7fyB71/olceJhkQ==}\n+  '@esbuild/linux-ppc64@0.24.2':\n+    resolution: {integrity: sha512-shsVrgCZ57Vr2L8mm39kO5PPIb+843FStGt7sGGoqiiWYconSxwTiuswC1VJZLCjNiMLAMh34jg4VSEQb+iEbw==}\n     engines: {node: '>=18'}\n     cpu: [ppc64]\n     os: [linux]\n \n-  '@esbuild/linux-riscv64@0.25.5':\n-    resolution: {integrity: sha512-kTxwu4mLyeOlsVIFPfQo+fQJAV9mh24xL+y+Bm6ej067sYANjyEw1dNHmvoqxJUCMnkBdKpvOn0Ahql6+4VyeA==}\n+  '@esbuild/linux-riscv64@0.24.2':\n+    resolution: {integrity: sha512-4eSFWnU9Hhd68fW16GD0TINewo1L6dRrB+oLNNbYyMUAeOD2yCK5KXGK1GH4qD/kT+bTEXjsyTCiJGHPZ3eM9Q==}\n     engines: {node: '>=18'}\n     cpu: [riscv64]\n     os: [linux]\n \n-  '@esbuild/linux-s390x@0.25.5':\n-    resolution: {integrity: sha512-K2dSKTKfmdh78uJ3NcWFiqyRrimfdinS5ErLSn3vluHNeHVnBAFWC8a4X5N+7FgVE1EjXS1QDZbpqZBjfrqMTQ==}\n+  '@esbuild/linux-s390x@0.24.2':\n+    resolution: {integrity: sha512-S0Bh0A53b0YHL2XEXC20bHLuGMOhFDO6GN4b3YjRLK//Ep3ql3erpNcPlEFed93hsQAjAQDNsvcK+hV90FubSw==}\n     engines: {node: '>=18'}\n     cpu: [s390x]\n     os: [linux]\n \n-  '@esbuild/linux-x64@0.25.5':\n-    resolution: {integrity: sha512-uhj8N2obKTE6pSZ+aMUbqq+1nXxNjZIIjCjGLfsWvVpy7gKCOL6rsY1MhRh9zLtUtAI7vpgLMK6DxjO8Qm9lJw==}\n+  '@esbuild/linux-x64@0.24.2':\n+    resolution: {integrity: sha512-8Qi4nQcCTbLnK9WoMjdC9NiTG6/E38RNICU6sUNqK0QFxCYgoARqVqxdFmWkdonVsvGqWhmm7MO0jyTqLqwj0Q==}\n     engines: {node: '>=18'}\n     cpu: [x64]\n     os: [linux]\n \n-  '@esbuild/netbsd-arm64@0.25.5':\n-    resolution: {integrity: sha512-pwHtMP9viAy1oHPvgxtOv+OkduK5ugofNTVDilIzBLpoWAM16r7b/mxBvfpuQDpRQFMfuVr5aLcn4yveGvBZvw==}\n+  '@esbuild/netbsd-arm64@0.24.2':\n+    resolution: {integrity: sha512-wuLK/VztRRpMt9zyHSazyCVdCXlpHkKm34WUyinD2lzK07FAHTq0KQvZZlXikNWkDGoT6x3TD51jKQ7gMVpopw==}\n     engines: {node: '>=18'}\n     cpu: [arm64]\n     os: [netbsd]\n \n-  '@esbuild/netbsd-x64@0.25.5':\n-    resolution: {integrity: sha512-WOb5fKrvVTRMfWFNCroYWWklbnXH0Q5rZppjq0vQIdlsQKuw6mdSihwSo4RV/YdQ5UCKKvBy7/0ZZYLBZKIbwQ==}\n+  '@esbuild/netbsd-x64@0.24.2':\n+    resolution: {integrity: sha512-VefFaQUc4FMmJuAxmIHgUmfNiLXY438XrL4GDNV1Y1H/RW3qow68xTwjZKfj/+Plp9NANmzbH5R40Meudu8mmw==}\n     engines: {node: '>=18'}\n     cpu: [x64]\n     os: [netbsd]\n \n-  '@esbuild/openbsd-arm64@0.25.5':\n-    resolution: {integrity: sha512-7A208+uQKgTxHd0G0uqZO8UjK2R0DDb4fDmERtARjSHWxqMTye4Erz4zZafx7Di9Cv+lNHYuncAkiGFySoD+Mw==}\n+  '@esbuild/openbsd-arm64@0.24.2':\n+    resolution: {integrity: sha512-YQbi46SBct6iKnszhSvdluqDmxCJA+Pu280Av9WICNwQmMxV7nLRHZfjQzwbPs3jeWnuAhE9Jy0NrnJ12Oz+0A==}\n     engines: {node: '>=18'}\n     cpu: [arm64]\n     os: [openbsd]\n \n-  '@esbuild/openbsd-x64@0.25.5':\n-    resolution: {integrity: sha512-G4hE405ErTWraiZ8UiSoesH8DaCsMm0Cay4fsFWOOUcz8b8rC6uCvnagr+gnioEjWn0wC+o1/TAHt+It+MpIMg==}\n+  '@esbuild/openbsd-x64@0.24.2':\n+    resolution: {integrity: sha512-+iDS6zpNM6EnJyWv0bMGLWSWeXGN/HTaF/LXHXHwejGsVi+ooqDfMCCTerNFxEkM3wYVcExkeGXNqshc9iMaOA==}\n     engines: {node: '>=18'}\n     cpu: [x64]\n     os: [openbsd]\n \n-  '@esbuild/sunos-x64@0.25.5':\n-    resolution: {integrity: sha512-l+azKShMy7FxzY0Rj4RCt5VD/q8mG/e+mDivgspo+yL8zW7qEwctQ6YqKX34DTEleFAvCIUviCFX1SDZRSyMQA==}\n+  '@esbuild/sunos-x64@0.24.2':\n+    resolution: {integrity: sha512-hTdsW27jcktEvpwNHJU4ZwWFGkz2zRJUz8pvddmXPtXDzVKTTINmlmga3ZzwcuMpUvLw7JkLy9QLKyGpD2Yxig==}\n     engines: {node: '>=18'}\n     cpu: [x64]\n     os: [sunos]\n \n-  '@esbuild/win32-arm64@0.25.5':\n-    resolution: {integrity: sha512-O2S7SNZzdcFG7eFKgvwUEZ2VG9D/sn/eIiz8XRZ1Q/DO5a3s76Xv0mdBzVM5j5R639lXQmPmSo0iRpHqUUrsxw==}\n+  '@esbuild/win32-arm64@0.24.2':\n+    resolution: {integrity: sha512-LihEQ2BBKVFLOC9ZItT9iFprsE9tqjDjnbulhHoFxYQtQfai7qfluVODIYxt1PgdoyQkz23+01rzwNwYfutxUQ==}\n     engines: {node: '>=18'}\n     cpu: [arm64]\n     os: [win32]\n \n-  '@esbuild/win32-ia32@0.25.5':\n-    resolution: {integrity: sha512-onOJ02pqs9h1iMJ1PQphR+VZv8qBMQ77Klcsqv9CNW2w6yLqoURLcgERAIurY6QE63bbLuqgP9ATqajFLK5AMQ==}\n+  '@esbuild/win32-ia32@0.24.2':\n+    resolution: {integrity: sha512-q+iGUwfs8tncmFC9pcnD5IvRHAzmbwQ3GPS5/ceCyHdjXubwQWI12MKWSNSMYLJMq23/IUCvJMS76PDqXe1fxA==}\n     engines: {node: '>=18'}\n     cpu: [ia32]\n     os: [win32]\n \n-  '@esbuild/win32-x64@0.25.5':\n-    resolution: {integrity: sha512-TXv6YnJ8ZMVdX+SXWVBo/0p8LTcrUYngpWjvm91TMjjBQii7Oz11Lw5lbDV5Y0TzuhSJHwiH4hEtC1I42mMS0g==}\n+  '@esbuild/win32-x64@0.24.2':\n+    resolution: {integrity: sha512-7VTgWzgMGvup6aSqDPLiW5zHaxYJGTO4OokMjIlrCtf+VpEL+cXKtCvg723iguPYI5oaUNdS+/V7OU2gvXVWEg==}\n     engines: {node: '>=18'}\n     cpu: [x64]\n     os: [win32]\n@@ -1567,8 +1576,8 @@ packages:\n     resolution: {integrity: sha512-RuzCup9Ct91Y7V79xwCb146RaBRHZ7NBbrIUySumd1rpKqHL5OonaqrGIbug5hNwP/fRyxFMA6ISgw4FTtYFYg==}\n     engines: {node: '>=18'}\n \n-  '@napi-rs/wasm-runtime@0.2.11':\n-    resolution: {integrity: sha512-9DPkXtvHydrcOsopiYpUgPHpmj0HWZKMUnL2dZqpvC42lsratuBG06V5ipyno0fUek5VlFsNQ+AcFATSrJXgMA==}\n+  '@napi-rs/wasm-runtime@0.2.10':\n+    resolution: {integrity: sha512-bCsCyeZEwVErsGmyPNSzwfwFn4OdxBj0mmv6hOFucB/k81Ojdu68RbZdxYsRQUPc9l6SU5F/cG+bXgWs3oUgsQ==}\n \n   '@next/env@15.3.3':\n     resolution: {integrity: sha512-OdiMrzCl2Xi0VTjiQQUK0Xh7bJHnOuET2s+3V+Y40WJBAXrJeGA3f+I8MZJ/YQ3mVGi5XGR1L66oFlgqXhQ4Vw==}\n@@ -2907,68 +2916,68 @@ packages:\n   '@supabase/supabase-js@2.50.0':\n     resolution: {integrity: sha512-M1Gd5tPaaghYZ9OjeO1iORRqbTWFEz/cF3pPubRnMPzA+A8SiUsXXWDP+DWsASZcjEcVEcVQIAF38i5wrijYOg==}\n \n-  '@swc/core-darwin-arm64@1.12.1':\n-    resolution: {integrity: sha512-nUjWVcJ3YS2N40ZbKwYO2RJ4+o2tWYRzNOcIQp05FqW0+aoUCVMdAUUzQinPDynfgwVshDAXCKemY8X7nN5MaA==}\n+  '@swc/core-darwin-arm64@1.11.31':\n+    resolution: {integrity: sha512-NTEaYOts0OGSbJZc0O74xsji+64JrF1stmBii6D5EevWEtrY4wlZhm8SiP/qPrOB+HqtAihxWIukWkP2aSdGSQ==}\n     engines: {node: '>=10'}\n     cpu: [arm64]\n     os: [darwin]\n \n-  '@swc/core-darwin-x64@1.12.1':\n-    resolution: {integrity: sha512-OGm4a4d3OeJn+tRt8H/eiHgTFrJbS6r8mi/Ob65tAEXZGHN900T2kR7c5ALr0V2hBOQ8BfhexwPoQlGQP/B95w==}\n+  '@swc/core-darwin-x64@1.11.31':\n+    resolution: {integrity: sha512-THSGaSwT96JwXDwuXQ6yFBbn+xDMdyw7OmBpnweAWsh5DhZmQkALEm1DgdQO3+rrE99MkmzwAfclc0UmYro/OA==}\n     engines: {node: '>=10'}\n     cpu: [x64]\n     os: [darwin]\n \n-  '@swc/core-linux-arm-gnueabihf@1.12.1':\n-    resolution: {integrity: sha512-76YeeQKyK0EtNkQiNBZ0nbVGooPf9IucY0WqVXVpaU4wuG7ZyLEE2ZAIgXafIuzODGQoLfetue7I8boMxh1/MA==}\n+  '@swc/core-linux-arm-gnueabihf@1.11.31':\n+    resolution: {integrity: sha512-laKtQFnW7KHgE57Hx32os2SNAogcuIDxYE+3DYIOmDMqD7/1DCfJe6Rln2N9WcOw6HuDbDpyQavIwZNfSAa8vQ==}\n     engines: {node: '>=10'}\n     cpu: [arm]\n     os: [linux]\n \n-  '@swc/core-linux-arm64-gnu@1.12.1':\n-    resolution: {integrity: sha512-BxJDIJPq1+aCh9UsaSAN6wo3tuln8UhNXruOrzTI8/ElIig/3sAueDM6Eq7GvZSGGSA7ljhNATMJ0elD7lFatQ==}\n+  '@swc/core-linux-arm64-gnu@1.11.31':\n+    resolution: {integrity: sha512-T+vGw9aPE1YVyRxRr1n7NAdkbgzBzrXCCJ95xAZc/0+WUwmL77Z+js0J5v1KKTRxw4FvrslNCOXzMWrSLdwPSA==}\n     engines: {node: '>=10'}\n     cpu: [arm64]\n     os: [linux]\n \n-  '@swc/core-linux-arm64-musl@1.12.1':\n-    resolution: {integrity: sha512-NhLdbffSXvY0/FwUSAl4hKBlpe5GHQGXK8DxTo3HHjLsD9sCPYieo3vG0NQoUYAy4ZUY1WeGjyxeq4qZddJzEQ==}\n+  '@swc/core-linux-arm64-musl@1.11.31':\n+    resolution: {integrity: sha512-Mztp5NZkyd5MrOAG+kl+QSn0lL4Uawd4CK4J7wm97Hs44N9DHGIG5nOz7Qve1KZo407Y25lTxi/PqzPKHo61zQ==}\n     engines: {node: '>=10'}\n     cpu: [arm64]\n     os: [linux]\n \n-  '@swc/core-linux-x64-gnu@1.12.1':\n-    resolution: {integrity: sha512-CrYnV8SZIgArQ9LKH0xEF95PKXzX9WkRSc5j55arOSBeDCeDUQk1Bg/iKdnDiuj5HC1hZpvzwMzSBJjv+Z70jA==}\n+  '@swc/core-linux-x64-gnu@1.11.31':\n+    resolution: {integrity: sha512-DDVE0LZcXOWwOqFU1Xi7gdtiUg3FHA0vbGb3trjWCuI1ZtDZHEQYL4M3/2FjqKZtIwASrDvO96w91okZbXhvMg==}\n     engines: {node: '>=10'}\n     cpu: [x64]\n     os: [linux]\n \n-  '@swc/core-linux-x64-musl@1.12.1':\n-    resolution: {integrity: sha512-BQMl3d0HaGB0/h2xcKlGtjk/cGRn2tnbsaChAKcjFdCepblKBCz1pgO/mL7w5iXq3s57wMDUn++71/a5RAkZOA==}\n+  '@swc/core-linux-x64-musl@1.11.31':\n+    resolution: {integrity: sha512-mJA1MzPPRIfaBUHZi0xJQ4vwL09MNWDeFtxXb0r4Yzpf0v5Lue9ymumcBPmw/h6TKWms+Non4+TDquAsweuKSw==}\n     engines: {node: '>=10'}\n     cpu: [x64]\n     os: [linux]\n \n-  '@swc/core-win32-arm64-msvc@1.12.1':\n-    resolution: {integrity: sha512-b7NeGnpqTfmIGtUqXBl0KqoSmOnH64nRZoT5l4BAGdvwY7nxitWR94CqZuwyLPty/bLywmyDA9uO12Kvgb3+gg==}\n+  '@swc/core-win32-arm64-msvc@1.11.31':\n+    resolution: {integrity: sha512-RdtakUkNVAb/FFIMw3LnfNdlH1/ep6KgiPDRlmyUfd0WdIQ3OACmeBegEFNFTzi7gEuzy2Yxg4LWf4IUVk8/bg==}\n     engines: {node: '>=10'}\n     cpu: [arm64]\n     os: [win32]\n \n-  '@swc/core-win32-ia32-msvc@1.12.1':\n-    resolution: {integrity: sha512-iU/29X2D7cHBp1to62cUg/5Xk8K+lyOJiKIGGW5rdzTW/c2zz3d/ehgpzVP/rqC4NVr88MXspqHU4il5gmDajw==}\n+  '@swc/core-win32-ia32-msvc@1.11.31':\n+    resolution: {integrity: sha512-hErXdCGsg7swWdG1fossuL8542I59xV+all751mYlBoZ8kOghLSKObGQTkBbuNvc0sUKWfWg1X0iBuIhAYar+w==}\n     engines: {node: '>=10'}\n     cpu: [ia32]\n     os: [win32]\n \n-  '@swc/core-win32-x64-msvc@1.12.1':\n-    resolution: {integrity: sha512-+Zh+JKDwiFqV5N9yAd2DhYVGPORGh9cfenu1ptr9yge+eHAf7vZJcC3rnj6QMR1QJh0Y5VC9+YBjRFjZVA7XDw==}\n+  '@swc/core-win32-x64-msvc@1.11.31':\n+    resolution: {integrity: sha512-5t7SGjUBMMhF9b5j17ml/f/498kiBJNf4vZFNM421UGUEETdtjPN9jZIuQrowBkoFGJTCVL/ECM4YRtTH30u/A==}\n     engines: {node: '>=10'}\n     cpu: [x64]\n     os: [win32]\n \n-  '@swc/core@1.12.1':\n-    resolution: {integrity: sha512-aKXdDTqxTVFl/bKQZ3EQUjEMBEoF6JBv29moMZq0kbVO43na6u/u+3Vcbhbrh+A2N0X5OL4RaveuWfAjEgOmeA==}\n+  '@swc/core@1.11.31':\n+    resolution: {integrity: sha512-mAby9aUnKRjMEA7v8cVZS9Ah4duoRBnX7X6r5qrhTxErx+68MoY1TPrVwj/66/SWN3Bl+jijqAqoB8Qx0QE34A==}\n     engines: {node: '>=10'}\n     peerDependencies:\n       '@swc/helpers': '>=0.5.17'\n@@ -2988,8 +2997,8 @@ packages:\n     peerDependencies:\n       '@swc/core': '*'\n \n-  '@swc/types@0.1.23':\n-    resolution: {integrity: sha512-u1iIVZV9Q0jxY+yM2vw/hZGDNudsN85bBpTqzAQ9rzkxW9D+e3aEM4Han+ow518gSewkXgjmEK0BD79ZcNVgPw==}\n+  '@swc/types@0.1.21':\n+    resolution: {integrity: sha512-2YEtj5HJVbKivud9N4bpPBAyZhj4S2Ipe5LkUG94alTpr7in/GU/EARgPAd3BwU+YOmFVJC2+kjqhGRi3r0ZpQ==}\n \n   '@tanstack/react-table@8.21.3':\n     resolution: {integrity: sha512-5nNMTSETP4ykGegmVkhjcS8tTLW6Vl4axfEGQN3v0zdHYbK4UfoqfPChclTrJ4EoK9QynqAu9oUf8VEmrpZ5Ww==}\n@@ -3100,8 +3109,8 @@ packages:\n   '@types/estree@1.0.6':\n     resolution: {integrity: sha512-AYnb1nQyY49te+VRAVgmzfcgjYS91mY5P0TKUDCLEM+gNnA+3T6rWITXRLYCpahpqSQbN5cE+gHpnPyXjHWxcw==}\n \n-  '@types/estree@1.0.8':\n-    resolution: {integrity: sha512-dWHzHa2WqEXI/O1E9OjrocMTKJl2mSrEolh1Iomrv6U+JuNwaHXsXx9bLu5gG7BUWFIN0skIQJQ/L1rIex4X6w==}\n+  '@types/estree@1.0.7':\n+    resolution: {integrity: sha512-w28IoSUCJpidD/TGviZwwMJckNESJZXFu7NBZ5YJ4mEUnNraUn9Pm8HSZm/jDF1pDWYKspWE7oVphigUPRakIQ==}\n \n   '@types/graceful-fs@4.1.9':\n     resolution: {integrity: sha512-olP3sd1qOEe5dXTSaFvQG+02VdRXcdytWLAZsAq1PecU8uqQAhkrnbli7DagjtXKW/Bl7YJbUsa8MPcuc8LHEQ==}\n@@ -3166,8 +3175,11 @@ packages:\n   '@types/phoenix@1.6.6':\n     resolution: {integrity: sha512-PIzZZlEppgrpoT2QgbnDU+MMzuR6BbCjllj0bM70lWoejMeNJAxCchxnv7J3XFkI8MpygtRpzXrIlmWUBclP5A==}\n \n-  '@types/prop-types@15.7.15':\n-    resolution: {integrity: sha512-F6bEyamV9jKGAFBEmlQnesRPGOQqS2+Uwi0Em15xenOxHaf2hv6L8YCVn3rPdPJOiJfPiCnLIRyvwVaqMY3MIw==}\n+  '@types/prismjs@1.26.5':\n+    resolution: {integrity: sha512-AUZTa7hQ2KY5L7AmtSiqxlhWxb4ina0yd8hNbl4TWuqnv/pFP0nDMb3YrfSBf4hJVGLh2YEIBfKaBW/9UEl6IQ==}\n+\n+  '@types/prop-types@15.7.14':\n+    resolution: {integrity: sha512-gNMvNH49DJ7OJYv+KAKn0Xp45p8PLl6zo2YnvDIbTd4J6MER2BmWN49TG7n9LvkyihINxeKW8+3bfS2yDC9dzQ==}\n \n   '@types/react-dom@18.3.5':\n     resolution: {integrity: sha512-P4t6saawp+b/dFrUr2cvkVsfvPguwsxtH6dNIYRllMsefqFzkZk5UIjzyDOv5g1dXIPdG4Sp1yCR4Z6RCUsG/Q==}\n@@ -3192,8 +3204,8 @@ packages:\n   '@types/stack-utils@2.0.3':\n     resolution: {integrity: sha512-9aEbYZ3TbYMznPdcdr3SmIrLXwC/AKZXQeCf9Pgao5CKb8CyHuEX5jzWPTkvregvhRJHcpRO6BFoGW9ycaOkYw==}\n \n-  '@types/statuses@2.0.6':\n-    resolution: {integrity: sha512-xMAgYwceFhRA2zY+XbEA7mxYbA093wdiW8Vu6gZPGWy9cmOyU9XesH1tNcEWsKFd5Vzrqx5T3D38PWx1FIIXkA==}\n+  '@types/statuses@2.0.5':\n+    resolution: {integrity: sha512-jmIUGWrAiwu3dZpxntxieC+1n/5c3mjrImkmOSQ2NC5uP6cYO4aAZDdSmRcI5C1oiTmqlZGHC+/NmJrKogbP5A==}\n \n   '@types/stylis@4.2.5':\n     resolution: {integrity: sha512-1Xve+NMN7FWjY14vLoY5tL3BVEQ/n42YLwaqJIPYhotZ9uBHt87VceMwWQpzmdEt2TNXIorIFG+YeCUUW7RInw==}\n@@ -3225,160 +3237,150 @@ packages:\n   '@types/yargs@17.0.33':\n     resolution: {integrity: sha512-WpxBCKWPLr4xSsHgz511rFJAM+wS28w2zEO1QDNY5zM/S8ok70NNfztH0xwhqKyaK0OHCbN98LDAZuy1ctxDkA==}\n \n-  '@typescript-eslint/eslint-plugin@8.34.1':\n-    resolution: {integrity: sha512-STXcN6ebF6li4PxwNeFnqF8/2BNDvBupf2OPx2yWNzr6mKNGF7q49VM00Pz5FaomJyqvbXpY6PhO+T9w139YEQ==}\n+  '@typescript-eslint/eslint-plugin@8.33.1':\n+    resolution: {integrity: sha512-TDCXj+YxLgtvxvFlAvpoRv9MAncDLBV2oT9Bd7YBGC/b/sEURoOYuIwLI99rjWOfY3QtDzO+mk0n4AmdFExW8A==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n     peerDependencies:\n-      '@typescript-eslint/parser': ^8.34.1\n+      '@typescript-eslint/parser': ^8.33.1\n       eslint: ^8.57.0 || ^9.0.0\n       typescript: '>=4.8.4 <5.9.0'\n \n-  '@typescript-eslint/parser@8.34.1':\n-    resolution: {integrity: sha512-4O3idHxhyzjClSMJ0a29AcoK0+YwnEqzI6oz3vlRf3xw0zbzt15MzXwItOlnr5nIth6zlY2RENLsOPvhyrKAQA==}\n+  '@typescript-eslint/parser@8.33.1':\n+    resolution: {integrity: sha512-qwxv6dq682yVvgKKp2qWwLgRbscDAYktPptK4JPojCwwi3R9cwrvIxS4lvBpzmcqzR4bdn54Z0IG1uHFskW4dA==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n     peerDependencies:\n       eslint: ^8.57.0 || ^9.0.0\n       typescript: '>=4.8.4 <5.9.0'\n \n-  '@typescript-eslint/project-service@8.34.1':\n-    resolution: {integrity: sha512-nuHlOmFZfuRwLJKDGQOVc0xnQrAmuq1Mj/ISou5044y1ajGNp2BNliIqp7F2LPQ5sForz8lempMFCovfeS1XoA==}\n+  '@typescript-eslint/project-service@8.33.1':\n+    resolution: {integrity: sha512-DZR0efeNklDIHHGRpMpR5gJITQpu6tLr9lDJnKdONTC7vvzOlLAG/wcfxcdxEWrbiZApcoBCzXqU/Z458Za5Iw==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n     peerDependencies:\n       typescript: '>=4.8.4 <5.9.0'\n \n-  '@typescript-eslint/scope-manager@8.34.1':\n-    resolution: {integrity: sha512-beu6o6QY4hJAgL1E8RaXNC071G4Kso2MGmJskCFQhRhg8VOH/FDbC8soP8NHN7e/Hdphwp8G8cE6OBzC8o41ZA==}\n+  '@typescript-eslint/scope-manager@8.33.1':\n+    resolution: {integrity: sha512-dM4UBtgmzHR9bS0Rv09JST0RcHYearoEoo3pG5B6GoTR9XcyeqX87FEhPo+5kTvVfKCvfHaHrcgeJQc6mrDKrA==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n \n-  '@typescript-eslint/tsconfig-utils@8.34.1':\n-    resolution: {integrity: sha512-K4Sjdo4/xF9NEeA2khOb7Y5nY6NSXBnod87uniVYW9kHP+hNlDV8trUSFeynA2uxWam4gIWgWoygPrv9VMWrYg==}\n+  '@typescript-eslint/tsconfig-utils@8.33.1':\n+    resolution: {integrity: sha512-STAQsGYbHCF0/e+ShUQ4EatXQ7ceh3fBCXkNU7/MZVKulrlq1usH7t2FhxvCpuCi5O5oi1vmVaAjrGeL71OK1g==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n     peerDependencies:\n       typescript: '>=4.8.4 <5.9.0'\n \n-  '@typescript-eslint/type-utils@8.34.1':\n-    resolution: {integrity: sha512-Tv7tCCr6e5m8hP4+xFugcrwTOucB8lshffJ6zf1mF1TbU67R+ntCc6DzLNKM+s/uzDyv8gLq7tufaAhIBYeV8g==}\n+  '@typescript-eslint/type-utils@8.33.1':\n+    resolution: {integrity: sha512-1cG37d9xOkhlykom55WVwG2QRNC7YXlxMaMzqw2uPeJixBFfKWZgaP/hjAObqMN/u3fr5BrTwTnc31/L9jQ2ww==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n     peerDependencies:\n       eslint: ^8.57.0 || ^9.0.0\n       typescript: '>=4.8.4 <5.9.0'\n \n-  '@typescript-eslint/types@8.34.1':\n-    resolution: {integrity: sha512-rjLVbmE7HR18kDsjNIZQHxmv9RZwlgzavryL5Lnj2ujIRTeXlKtILHgRNmQ3j4daw7zd+mQgy+uyt6Zo6I0IGA==}\n+  '@typescript-eslint/types@8.33.1':\n+    resolution: {integrity: sha512-xid1WfizGhy/TKMTwhtVOgalHwPtV8T32MS9MaH50Cwvz6x6YqRIPdD2WvW0XaqOzTV9p5xdLY0h/ZusU5Lokg==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n \n-  '@typescript-eslint/typescript-estree@8.34.1':\n-    resolution: {integrity: sha512-rjCNqqYPuMUF5ODD+hWBNmOitjBWghkGKJg6hiCHzUvXRy6rK22Jd3rwbP2Xi+R7oYVvIKhokHVhH41BxPV5mA==}\n+  '@typescript-eslint/typescript-estree@8.33.1':\n+    resolution: {integrity: sha512-+s9LYcT8LWjdYWu7IWs7FvUxpQ/DGkdjZeE/GGulHvv8rvYwQvVaUZ6DE+j5x/prADUgSbbCWZ2nPI3usuVeOA==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n     peerDependencies:\n       typescript: '>=4.8.4 <5.9.0'\n \n-  '@typescript-eslint/utils@8.34.1':\n-    resolution: {integrity: sha512-mqOwUdZ3KjtGk7xJJnLbHxTuWVn3GO2WZZuM+Slhkun4+qthLdXx32C8xIXbO1kfCECb3jIs3eoxK3eryk7aoQ==}\n+  '@typescript-eslint/utils@8.33.1':\n+    resolution: {integrity: sha512-52HaBiEQUaRYqAXpfzWSR2U3gxk92Kw006+xZpElaPMg3C4PgM+A5LqwoQI1f9E5aZ/qlxAZxzm42WX+vn92SQ==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n     peerDependencies:\n       eslint: ^8.57.0 || ^9.0.0\n       typescript: '>=4.8.4 <5.9.0'\n \n-  '@typescript-eslint/visitor-keys@8.34.1':\n-    resolution: {integrity: sha512-xoh5rJ+tgsRKoXnkBPFRLZ7rjKM0AfVbC68UZ/ECXoDbfggb9RbEySN359acY1vS3qZ0jVTVWzbtfapwm5ztxw==}\n+  '@typescript-eslint/visitor-keys@8.33.1':\n+    resolution: {integrity: sha512-3i8NrFcZeeDHJ+7ZUuDkGT+UHq+XoFGsymNK2jZCOHcfEzRQ0BdpRtdpSx/Iyf3MHLWIcLS0COuOPibKQboIiQ==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n \n   '@ungap/structured-clone@1.3.0':\n     resolution: {integrity: sha512-WmoN8qaIAo7WTYWbAZuG8PYEhn5fkz7dZrqTBZ7dtt//lL2Gwms1IcnQ5yHqjDfX8Ft5j4YzDM23f87zBfDe9g==}\n \n-  '@unrs/resolver-binding-android-arm-eabi@1.9.0':\n-    resolution: {integrity: sha512-h1T2c2Di49ekF2TE8ZCoJkb+jwETKUIPDJ/nO3tJBKlLFPu+fyd93f0rGP/BvArKx2k2HlRM4kqkNarj3dvZlg==}\n-    cpu: [arm]\n-    os: [android]\n-\n-  '@unrs/resolver-binding-android-arm64@1.9.0':\n-    resolution: {integrity: sha512-sG1NHtgXtX8owEkJ11yn34vt0Xqzi3k9TJ8zppDmyG8GZV4kVWw44FHwKwHeEFl07uKPeC4ZoyuQaGh5ruJYPA==}\n-    cpu: [arm64]\n-    os: [android]\n-\n-  '@unrs/resolver-binding-darwin-arm64@1.9.0':\n-    resolution: {integrity: sha512-nJ9z47kfFnCxN1z/oYZS7HSNsFh43y2asePzTEZpEvK7kGyuShSl3RRXnm/1QaqFL+iP+BjMwuB+DYUymOkA5A==}\n+  '@unrs/resolver-binding-darwin-arm64@1.7.10':\n+    resolution: {integrity: sha512-ABsM3eEiL3yu903G0uxgvGAoIw011XjTzyEk//gGtuVY1PuXP2IJG6novd6DBjm7MaWmRV/CZFY1rWBXSlSVVw==}\n     cpu: [arm64]\n     os: [darwin]\n \n-  '@unrs/resolver-binding-darwin-x64@1.9.0':\n-    resolution: {integrity: sha512-TK+UA1TTa0qS53rjWn7cVlEKVGz2B6JYe0C++TdQjvWYIyx83ruwh0wd4LRxYBM5HeuAzXcylA9BH2trARXJTw==}\n+  '@unrs/resolver-binding-darwin-x64@1.7.10':\n+    resolution: {integrity: sha512-lGVWy4FQEDo/PuI1VQXaQCY0XUg4xUJilf3fQ8NY4wtsQTm9lbasbUYf3nkoma+O2/do90jQTqkb02S3meyTDg==}\n     cpu: [x64]\n     os: [darwin]\n \n-  '@unrs/resolver-binding-freebsd-x64@1.9.0':\n-    resolution: {integrity: sha512-6uZwzMRFcD7CcCd0vz3Hp+9qIL2jseE/bx3ZjaLwn8t714nYGwiE84WpaMCYjU+IQET8Vu/+BNAGtYD7BG/0yA==}\n+  '@unrs/resolver-binding-freebsd-x64@1.7.10':\n+    resolution: {integrity: sha512-g9XLCHzNGatY79JJNgxrUH6uAAfBDj2NWIlTnqQN5odwGKjyVfFZ5tFL1OxYPcxTHh384TY5lvTtF+fuEZNvBQ==}\n     cpu: [x64]\n     os: [freebsd]\n \n-  '@unrs/resolver-binding-linux-arm-gnueabihf@1.9.0':\n-    resolution: {integrity: sha512-bPUBksQfrgcfv2+mm+AZinaKq8LCFvt5PThYqRotqSuuZK1TVKkhbVMS/jvSRfYl7jr3AoZLYbDkItxgqMKRkg==}\n+  '@unrs/resolver-binding-linux-arm-gnueabihf@1.7.10':\n+    resolution: {integrity: sha512-zV0ZMNy50sJFJapsjec8onyL9YREQKT88V8KwMoOA+zki/duFUP0oyTlbax1jGKdh8rQnruvW9VYkovGvdBAsw==}\n     cpu: [arm]\n     os: [linux]\n \n-  '@unrs/resolver-binding-linux-arm-musleabihf@1.9.0':\n-    resolution: {integrity: sha512-uT6E7UBIrTdCsFQ+y0tQd3g5oudmrS/hds5pbU3h4s2t/1vsGWbbSKhBSCD9mcqaqkBwoqlECpUrRJCmldl8PA==}\n+  '@unrs/resolver-binding-linux-arm-musleabihf@1.7.10':\n+    resolution: {integrity: sha512-jQxgb1DIDI7goyrabh4uvyWWBrFRfF+OOnS9SbF15h52g3Qjn/u8zG7wOQ0NjtcSMftzO75TITu9aHuI7FcqQQ==}\n     cpu: [arm]\n     os: [linux]\n \n-  '@unrs/resolver-binding-linux-arm64-gnu@1.9.0':\n-    resolution: {integrity: sha512-vdqBh911wc5awE2bX2zx3eflbyv8U9xbE/jVKAm425eRoOVv/VseGZsqi3A3SykckSpF4wSROkbQPvbQFn8EsA==}\n+  '@unrs/resolver-binding-linux-arm64-gnu@1.7.10':\n+    resolution: {integrity: sha512-9wVVlO6+aNlm90YWitwSI++HyCyBkzYCwMi7QbuGrTxDFm2pAgtpT0OEliaI7tLS8lAWYuDbzRRCJDgsdm6nwg==}\n     cpu: [arm64]\n     os: [linux]\n \n-  '@unrs/resolver-binding-linux-arm64-musl@1.9.0':\n-    resolution: {integrity: sha512-/8JFZ/SnuDr1lLEVsxsuVwrsGquTvT51RZGvyDB/dOK3oYK2UqeXzgeyq6Otp8FZXQcEYqJwxb9v+gtdXn03eQ==}\n+  '@unrs/resolver-binding-linux-arm64-musl@1.7.10':\n+    resolution: {integrity: sha512-FtFweORChdXOes0RAAyTZp6I4PodU2cZiSILAbGaEKDXp378UOumD2vaAkWHNxpsreQUKRxG5O1uq9EoV1NiVQ==}\n     cpu: [arm64]\n     os: [linux]\n \n-  '@unrs/resolver-binding-linux-ppc64-gnu@1.9.0':\n-    resolution: {integrity: sha512-FkJjybtrl+rajTw4loI3L6YqSOpeZfDls4SstL/5lsP2bka9TiHUjgMBjygeZEis1oC8LfJTS8FSgpKPaQx2tQ==}\n+  '@unrs/resolver-binding-linux-ppc64-gnu@1.7.10':\n+    resolution: {integrity: sha512-B+hOjpG2ncCR96a9d9ww1dWVuRVC2NChD0bITgrUhEWBhpdv2o/Mu2l8MsB2fzjdV/ku+twaQhr8iLHBoZafZQ==}\n     cpu: [ppc64]\n     os: [linux]\n \n-  '@unrs/resolver-binding-linux-riscv64-gnu@1.9.0':\n-    resolution: {integrity: sha512-w/NZfHNeDusbqSZ8r/hp8iL4S39h4+vQMc9/vvzuIKMWKppyUGKm3IST0Qv0aOZ1rzIbl9SrDeIqK86ZpUK37w==}\n+  '@unrs/resolver-binding-linux-riscv64-gnu@1.7.10':\n+    resolution: {integrity: sha512-DS6jFDoQCFsnsdLXlj3z3THakQLBic63B6A0rpQ1kpkyKa3OzEfqhwRNVaywuUuOKP9bX55Jk2uqpvn/hGjKCg==}\n     cpu: [riscv64]\n     os: [linux]\n \n-  '@unrs/resolver-binding-linux-riscv64-musl@1.9.0':\n-    resolution: {integrity: sha512-bEPBosut8/8KQbUixPry8zg/fOzVOWyvwzOfz0C0Rw6dp+wIBseyiHKjkcSyZKv/98edrbMknBaMNJfA/UEdqw==}\n+  '@unrs/resolver-binding-linux-riscv64-musl@1.7.10':\n+    resolution: {integrity: sha512-A82SB6yEaA8EhIW2r0I7P+k5lg7zPscFnGs1Gna5rfPwoZjeUAGX76T55+DiyTiy08VFKUi79PGCulXnfjDq0g==}\n     cpu: [riscv64]\n     os: [linux]\n \n-  '@unrs/resolver-binding-linux-s390x-gnu@1.9.0':\n-    resolution: {integrity: sha512-LDtMT7moE3gK753gG4pc31AAqGUC86j3AplaFusc717EUGF9ZFJ356sdQzzZzkBk1XzMdxFyZ4f/i35NKM/lFA==}\n+  '@unrs/resolver-binding-linux-s390x-gnu@1.7.10':\n+    resolution: {integrity: sha512-J+VmOPH16U69QshCp9WS+Zuiuu9GWTISKchKIhLbS/6JSCEfw2A4N02whv2VmrkXE287xxZbhW1p6xlAXNzwqg==}\n     cpu: [s390x]\n     os: [linux]\n \n-  '@unrs/resolver-binding-linux-x64-gnu@1.9.0':\n-    resolution: {integrity: sha512-WmFd5KINHIXj8o1mPaT8QRjA9HgSXhN1gl9Da4IZihARihEnOylu4co7i/yeaIpcfsI6sYs33cNZKyHYDh0lrA==}\n+  '@unrs/resolver-binding-linux-x64-gnu@1.7.10':\n+    resolution: {integrity: sha512-bYTdDltcB/V3fEqpx8YDwDw8ta9uEg8TUbJOtek6JM42u9ciJ7R/jBjNeAOs+QbyxGDd2d6xkBaGwty1HzOz3Q==}\n     cpu: [x64]\n     os: [linux]\n \n-  '@unrs/resolver-binding-linux-x64-musl@1.9.0':\n-    resolution: {integrity: sha512-CYuXbANW+WgzVRIl8/QvZmDaZxrqvOldOwlbUjIM4pQ46FJ0W5cinJ/Ghwa/Ng1ZPMJMk1VFdsD/XwmCGIXBWg==}\n+  '@unrs/resolver-binding-linux-x64-musl@1.7.10':\n+    resolution: {integrity: sha512-NYZ1GvSuTokJ28lqcjrMTnGMySoo4dVcNK/nsNCKCXT++1zekZtJaE+N+4jc1kR7EV0fc1OhRrOGcSt7FT9t8w==}\n     cpu: [x64]\n     os: [linux]\n \n-  '@unrs/resolver-binding-wasm32-wasi@1.9.0':\n-    resolution: {integrity: sha512-6Rp2WH0OoitMYR57Z6VE8Y6corX8C6QEMWLgOV6qXiJIeZ1F9WGXY/yQ8yDC4iTraotyLOeJ2Asea0urWj2fKQ==}\n+  '@unrs/resolver-binding-wasm32-wasi@1.7.10':\n+    resolution: {integrity: sha512-MRjJhTaQzLoX8OtzRBQDJ84OJ8IX1FqpRAUSxp/JtPeak+fyDfhXaEjcA/fhfgrACUnvC+jWC52f/V6MixSKCQ==}\n     engines: {node: '>=14.0.0'}\n     cpu: [wasm32]\n \n-  '@unrs/resolver-binding-win32-arm64-msvc@1.9.0':\n-    resolution: {integrity: sha512-rknkrTRuvujprrbPmGeHi8wYWxmNVlBoNW8+4XF2hXUnASOjmuC9FNF1tGbDiRQWn264q9U/oGtixyO3BT8adQ==}\n+  '@unrs/resolver-binding-win32-arm64-msvc@1.7.10':\n+    resolution: {integrity: sha512-Cgw6qhdsfzXJnHb006CzqgaX8mD445x5FGKuueaLeH1ptCxDbzRs8wDm6VieOI7rdbstfYBaFtaYN7zBT5CUPg==}\n     cpu: [arm64]\n     os: [win32]\n \n-  '@unrs/resolver-binding-win32-ia32-msvc@1.9.0':\n-    resolution: {integrity: sha512-Ceymm+iBl+bgAICtgiHyMLz6hjxmLJKqBim8tDzpX61wpZOx2bPK6Gjuor7I2RiUynVjvvkoRIkrPyMwzBzF3A==}\n+  '@unrs/resolver-binding-win32-ia32-msvc@1.7.10':\n+    resolution: {integrity: sha512-Z7oECyIT2/HsrWpJ6wi2b+lVbPmWqQHuW5zeatafoRXizk1+2wUl+aSop1PF58XcyBuwPP2YpEUUpMZ8ILV4fA==}\n     cpu: [ia32]\n     os: [win32]\n \n-  '@unrs/resolver-binding-win32-x64-msvc@1.9.0':\n-    resolution: {integrity: sha512-k59o9ZyeyS0hAlcaKFezYSH2agQeRFEB7KoQLXl3Nb3rgkqT1NY9Vwy+SqODiLmYnEjxWJVRE/yq2jFVqdIxZw==}\n+  '@unrs/resolver-binding-win32-x64-msvc@1.7.10':\n+    resolution: {integrity: sha512-DGAOo5asNvDsmFgwkb7xsgxNyN0If6XFYwDIC1QlRE7kEYWIMRChtWJyHDf30XmGovDNOs/37krxhnga/nm/4w==}\n     cpu: [x64]\n     os: [win32]\n \n@@ -3474,8 +3476,8 @@ packages:\n     peerDependencies:\n       acorn: ^6.0.0 || ^7.0.0 || ^8.0.0\n \n-  acorn@8.15.0:\n-    resolution: {integrity: sha512-NZyJarBfL7nWwIq+FDL6Zp/yHEhePMNnnJ0y3qfieCrmNvYct8uvtiV41UvlSe6apAfk0fY1FbWx+NwfmpvtTg==}\n+  acorn@8.14.1:\n+    resolution: {integrity: sha512-OvQ/2pUDKmgfCg++xsTX1wGxfTaszcHVcTctW4UJB4hibJx2HXxxO5UmVgyjMa+ZDsiaf5wWLXYpRWMmBI0QHg==}\n     engines: {node: '>=0.4.0'}\n     hasBin: true\n \n@@ -3666,8 +3668,8 @@ packages:\n     peerDependencies:\n       playwright: '>1.0.0'\n \n-  axios@1.10.0:\n-    resolution: {integrity: sha512-/1xYAC4MP/HEG+3duIhFr4ZQXR4sQXOIe+o6sdqzeykGLx6Upp/1p8MHqhINOvGeP7xyNHe7tsiJByc4SSVUxw==}\n+  axios@1.9.0:\n+    resolution: {integrity: sha512-re4CqKTJaURpzbLHtIi6XpDv20/CnpXOtjRY5/CU32L8gU8ek9UIivcfvSWvmKEngmVbrUtPpdDwWDWL7DNHvg==}\n \n   axobject-query@4.1.0:\n     resolution: {integrity: sha512-qIj0G9wZbMGNLjLmg1PT6v2mE9AH2zlnADJD/2tC6E00hgmhUOfEB6greHPAfLRSufHqROIUTkw6E+M3lH0PTQ==}\n@@ -3752,11 +3754,11 @@ packages:\n   boring-avatars@1.11.2:\n     resolution: {integrity: sha512-3+wkwPeObwS4R37FGXMYViqc4iTrIRj5yzfX9Qy4mnpZ26sX41dGMhsAgmKks1r/uufY1pl4vpgzMWHYfJRb2A==}\n \n-  brace-expansion@1.1.12:\n-    resolution: {integrity: sha512-9T9UjW3r0UW5c1Q7GTwllptXwhvYmEzFhzMfZ9H7FQWt+uZePjZPjBP/W1ZEyZ1twGWom5/56TF4lPcqjnDHcg==}\n+  brace-expansion@1.1.11:\n+    resolution: {integrity: sha512-iCuPHDFgrHX7H2vEI/5xpz07zSHB00TpugqhmYtVmMO6518mCuRMoOYFldEBl0g187ufozdaHgWKcYFb61qGiA==}\n \n-  brace-expansion@2.0.2:\n-    resolution: {integrity: sha512-Jt0vHyM+jmUBqojB7E1NIYadt0vI0Qxjxd2TErW94wDz+E2LAm5vKMXXwg6ZZBTHPuUlDgQHKXvjGBdfcF1ZDQ==}\n+  brace-expansion@2.0.1:\n+    resolution: {integrity: sha512-XnAIvQ8eM+kC6aULx6wuQiwVsnzsi9d3WxzV3FpWTGA19F621kwdbsAcFKXgKUHZWsy+mY6iL1sHTxWEFCytDA==}\n \n   braces@3.0.3:\n     resolution: {integrity: sha512-yQbXgO/OSZVD2IsiLlro+7Hf6Q18EJrKSEsdoMzKePKXct3gvD8oLcOQdIzGupr5Fj+EDe8gO/lxc1BzfMpxvA==}\n@@ -3850,8 +3852,8 @@ packages:\n   camelize@1.0.1:\n     resolution: {integrity: sha512-dU+Tx2fsypxTgtLoE36npi3UqcjSSMNYfkqgmoEhtZrraP5VWq0K7FkWVTYa8eMPtnU/G2txVsfdCJTn9uzpuQ==}\n \n-  caniuse-lite@1.0.30001723:\n-    resolution: {integrity: sha512-1R/elMjtehrFejxwmexeXAtae5UO9iSyFn6G/I806CYC/BLyyBk1EPhrKBkWhy6wM6Xnm47dSJQec+tLJ39WHw==}\n+  caniuse-lite@1.0.30001721:\n+    resolution: {integrity: sha512-cOuvmUVtKrtEaoKiO0rSc29jcjwMwX5tOHDy4MgVFEWiUXj4uBMJkwI8MDySkgXidpMiHUcviogAvFi4pA2hDQ==}\n \n   case-sensitive-paths-webpack-plugin@2.4.0:\n     resolution: {integrity: sha512-roIFONhcxog0JSSWbvVAh3OocukmSgpqOH6YpMkCvav/ySIV3JKg4Dc8vYtQjYi/UxpNE36r/9v+VqTQqgkYmw==}\n@@ -4062,11 +4064,11 @@ packages:\n     resolution: {integrity: sha512-9Kr/j4O16ISv8zBBhJoi4bXOYNTkFLOqSL3UDB0njXxCXNezjeyVrJyGOWtgfs/q2km1gwBcfH8q1yEGoMYunA==}\n     engines: {node: '>=18'}\n \n-  core-js-compat@3.43.0:\n-    resolution: {integrity: sha512-2GML2ZsCc5LR7hZYz4AXmjQw8zuy2T//2QntwdnpuYI7jteT6GVYJL7F6C2C57R7gSYrcqVW3lAALefdbhBLDA==}\n+  core-js-compat@3.42.0:\n+    resolution: {integrity: sha512-bQasjMfyDGyaeWKBIu33lHh9qlSR0MFE/Nmc6nMjf/iU9b3rSMdAYz1Baxrv4lPdGUsTqZudHA4jIGSJy0SWZQ==}\n \n-  core-js-pure@3.43.0:\n-    resolution: {integrity: sha512-i/AgxU2+A+BbJdMxh3v7/vxi2SbFqxiFmg6VsDwYB4jkucrd1BZNA9a9gphC0fYMG5IBSgQcbQnk865VCLe7xA==}\n+  core-js-pure@3.42.0:\n+    resolution: {integrity: sha512-007bM04u91fF4kMgwom2I5cQxAFIy8jVulgr9eozILl/SZE53QOqnW/+vviC+wQWLv+AunBG+8Q0TLoeSsSxRQ==}\n \n   core-util-is@1.0.3:\n     resolution: {integrity: sha512-ZQBvi1DcpJ4GDqanjucZ2Hj3wEO5pZDS89BWbkcrvdxksJorwUDDZamX9ldFkp9aw2lmBDLgkObEA4DWNJ9FYQ==}\n@@ -4258,8 +4260,8 @@ packages:\n   decimal.js-light@2.5.1:\n     resolution: {integrity: sha512-qIMFpTMZmny+MMIitAB6D7iVPEorVw6YQRWkvarTkT4tBeSLLiHzcwj6q0MmYSFCiVpiqPJTJEYIrpcPzVEIvg==}\n \n-  decode-named-character-reference@1.2.0:\n-    resolution: {integrity: sha512-c6fcElNV6ShtZXmsgNgFFV5tVX2PaV4g+MOAkb8eXHvn6sryJBrZa9r0zV6+dtTyoCKxtDy5tyQ5ZwQuidtd+Q==}\n+  decode-named-character-reference@1.1.0:\n+    resolution: {integrity: sha512-Wy+JTSbFThEOXQIR2L6mxJvEs+veIzpmqD7ynWxMXGpnk3smkHQOp6forLdHsKpAMW9iJpaBBIxz285t1n1C3w==}\n \n   dedent@0.7.0:\n     resolution: {integrity: sha512-Q6fKUPqnAHAyhiUgFU7BUzLiv0kd8saH9al7tnu5Q/okj6dnupxyTgFIBjVzJATdfIAm9NAsvXNzjaKa+bxVyA==}\n@@ -4407,8 +4409,8 @@ packages:\n   eastasianwidth@0.2.0:\n     resolution: {integrity: sha512-I88TYZWc9XiYHRQ4/3c5rjjfgkjhLyW2luGIheGERbNQ6OY7yTybanSpDXZa8y7VUP9YmDcYa+eyq4ca7iLqWA==}\n \n-  electron-to-chromium@1.5.169:\n-    resolution: {integrity: sha512-q7SQx6mkLy0GTJK9K9OiWeaBMV4XQtBSdf6MJUzDB/H/5tFXfIiX38Lci1Kl6SsgiEhz1SQI1ejEOU5asWEhwQ==}\n+  electron-to-chromium@1.5.165:\n+    resolution: {integrity: sha512-naiMx1Z6Nb2TxPU6fiFrUrDTjyPMLdTtaOd2oLmG8zVSg2hCWGkhPyxwk+qRmZ1ytwVqUv0u7ZcDA5+ALhaUtw==}\n \n   elliptic@6.6.1:\n     resolution: {integrity: sha512-RaddvvMatK2LJHqFJ+YA4WysVN5Ita9E35botqIYspQ4TkRAlCicdzKOjlyv/1Za5RyTNn7di//eEV0uTAfe3g==}\n@@ -4506,8 +4508,8 @@ packages:\n     peerDependencies:\n       esbuild: '>=0.12 <1'\n \n-  esbuild@0.25.5:\n-    resolution: {integrity: sha512-P8OtKZRv/5J5hhz0cUAdu/cLuPIKXpQl1R9pZtvmHWQvrAUVd0UNIPT4IB4W3rNOqVO0rlqHmCIbSwxh/c9yUQ==}\n+  esbuild@0.24.2:\n+    resolution: {integrity: sha512-+9egpBW8I3CD5XPe0n6BfT5fxLzxrlDzqydF3aviG+9ni1lDC/OvMHcxqEFV0+LANZG5R1bFMWfUrjVsdwxJvA==}\n     engines: {node: '>=18'}\n     hasBin: true\n \n@@ -4619,8 +4621,8 @@ packages:\n     resolution: {integrity: sha512-wpc+LXeiyiisxPlEkUzU6svyS1frIO3Mgxj1fdy7Pm8Ygzguax2N3Fa/D/ag1WqbOprdI+uY6wMUl8/a2G+iag==}\n     engines: {node: ^12.22.0 || ^14.17.0 || >=16.0.0}\n \n-  eslint-visitor-keys@4.2.1:\n-    resolution: {integrity: sha512-Uhdk5sfqcee/9H/rCOJikYz67o0a2Tw2hGRPOG2Y1R2dg7brRe1uG0yaNQDHu+TO/uQPF/5eCapvYSmHUjt7JQ==}\n+  eslint-visitor-keys@4.2.0:\n+    resolution: {integrity: sha512-UyLnSehNt62FFhSwjZlHmeokpRK59rcz29j+F1/aDgbkbRTk7wIc9XzdoasMUbRNKDM0qQt/+BJ4BrpFeABemw==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n \n   eslint@8.57.1:\n@@ -4742,8 +4744,8 @@ packages:\n   fb-watchman@2.0.2:\n     resolution: {integrity: sha512-p5161BqbuCaSnB8jIbzQHOlpgsPmK5rJVDfDKO91Axs5NC1uu3HRQm6wt9cd9/+GtQQIO53JdGXXoyDpTAsgYA==}\n \n-  fdir@6.4.6:\n-    resolution: {integrity: sha512-hiFoqpyZcfNm1yc4u8oWCf9A2c4D3QjCrks3zmoVKVxpQRzmPNar1hUJcBG2RQHvEVGDN+Jm81ZheVLAQMK6+w==}\n+  fdir@6.4.5:\n+    resolution: {integrity: sha512-4BG7puHpVsIYxZUbiUE3RqGloLaSSwzYie5jvasC4LWuBWzZawynvYouhjbQKw2JuIGYdm0DzIxl8iVidKlUEw==}\n     peerDependencies:\n       picomatch: ^3 || ^4\n     peerDependenciesMeta:\n@@ -4833,8 +4835,8 @@ packages:\n       typescript: '>3.6.0'\n       webpack: ^5.11.0\n \n-  form-data@4.0.3:\n-    resolution: {integrity: sha512-qsITQPfmvMOSAdeyZ+12I1c+CKSstAFAwu+97zrnWAbIr5u8wfsExUzCesVLC8NgHuRUqNN4Zy6UPWUTRGslcA==}\n+  form-data@4.0.2:\n+    resolution: {integrity: sha512-hGfm/slu0ZabnNt4oaRZ6uREyfCj6P4fT/n6A1rGV+Z0VdGXjfOhVUpkn6qVQONHGIFwmveGXyDs75+nr6FM8w==}\n     engines: {node: '>= 6'}\n \n   forwarded-parse@2.1.2:\n@@ -5711,8 +5713,8 @@ packages:\n     resolution: {integrity: sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==}\n     hasBin: true\n \n-  loupe@3.1.4:\n-    resolution: {integrity: sha512-wJzkKwJrheKtknCOKNEtDK4iqg/MxmZheEMtSTYvnzRdEYaZzmgH976nenp8WdJRdx5Vc1X/9MO0Oszl6ezeXg==}\n+  loupe@3.1.3:\n+    resolution: {integrity: sha512-kkIp7XSkP78ZxJEsSxW3712C6teJVoeHHwgo9zJ380de7IYyJ2ISlxojcH2pC5OFLewESmnRi/+XCDIEEVyoug==}\n \n   lower-case@2.0.2:\n     resolution: {integrity: sha512-7fm3l3NAF9WfN6W3JOmf5drwpVqX78JtoGJ3A6W0a6ZnldM41w2fV5D490psKFTpMds8TJse/eHLFFsNHHjHgg==}\n@@ -5924,11 +5926,11 @@ packages:\n   moment@2.30.1:\n     resolution: {integrity: sha512-uEmtNhbDOrWPFS+hdjFCBfy9f2YoyzRpwcl+DqpC6taX21FzsTLQVbMV/W7PzNSX6x/bhC1zA3c2UQ5NzH6how==}\n \n-  motion-dom@12.18.1:\n-    resolution: {integrity: sha512-dR/4EYT23Snd+eUSLrde63Ws3oXQtJNw/krgautvTfwrN/2cHfCZMdu6CeTxVfRRWREW3Fy1f5vobRDiBb/q+w==}\n+  motion-dom@12.16.0:\n+    resolution: {integrity: sha512-Z2nGwWrrdH4egLEtgYMCEN4V2qQt1qxlKy/uV7w691ztyA41Q5Rbn0KNGbsNVDZr9E8PD2IOQ3hSccRnB6xWzw==}\n \n-  motion-utils@12.18.1:\n-    resolution: {integrity: sha512-az26YDU4WoDP0ueAkUtABLk2BIxe28d8NH1qWT8jPGhPyf44XTdDUh8pDk9OPphaSrR9McgpcJlgwSOIw/sfkA==}\n+  motion-utils@12.12.1:\n+    resolution: {integrity: sha512-f9qiqUHm7hWSLlNW8gS9pisnsN7CRFRD58vNjptKdsqFLpkVnX00TNeD6Q0d27V9KzT7ySFyK1TZ/DShfVOv6w==}\n \n   ms@2.1.3:\n     resolution: {integrity: sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==}\n@@ -6275,21 +6277,11 @@ packages:\n     engines: {node: '>=18'}\n     hasBin: true\n \n-  playwright-core@1.53.0:\n-    resolution: {integrity: sha512-mGLg8m0pm4+mmtB7M89Xw/GSqoNC+twivl8ITteqvAndachozYe2ZA7srU6uleV1vEdAHYqjq+SV8SNxRRFYBw==}\n-    engines: {node: '>=18'}\n-    hasBin: true\n-\n   playwright@1.52.0:\n     resolution: {integrity: sha512-JAwMNMBlxJ2oD1kce4KPtMkDeKGHQstdpFPcPH3maElAXon/QZeTvtsfXmTMRyO9TslfoYOXkSsvao2nE1ilTw==}\n     engines: {node: '>=18'}\n     hasBin: true\n \n-  playwright@1.53.0:\n-    resolution: {integrity: sha512-ghGNnIEYZC4E+YtclRn4/p6oYbdPiASELBIYkBXfaTVKreQUYbMUYQDwS12a8F0/HtIjr/CkGjtwABeFPGcS4Q==}\n-    engines: {node: '>=18'}\n-    hasBin: true\n-\n   pnp-webpack-plugin@1.7.0:\n     resolution: {integrity: sha512-2Rb3vm+EXble/sMXNSu6eoBx8e79gKqhNq9F5ZWW6ERNCTE/Q0wQNne5541tE5vKjfM8hpNCYL+LGc1YTfI0dg==}\n     engines: {node: '>=6'}\n@@ -6483,6 +6475,11 @@ packages:\n     resolution: {integrity: sha512-Pdlw/oPxN+aXdmM9R00JVC9WVFoCLTKJvDVLgmJ+qAffBMxsV85l/Lu7sNx4zSzPyoL2euImuEwHhOXdEgNFZQ==}\n     engines: {node: ^14.15.0 || ^16.10.0 || >=18.0.0}\n \n+  prism-react-renderer@2.4.1:\n+    resolution: {integrity: sha512-ey8Ls/+Di31eqzUxC46h8MksNuGx/n0AAC8uKpwFau4RPDYLuE3EXTp8N8G2vX2N7UC/+IXeNUnlWBGGcAG+Ig==}\n+    peerDependencies:\n+      react: '>=16.0.0'\n+\n   process-nextick-args@2.0.1:\n     resolution: {integrity: sha512-3ouUOpQhtgrbOa17J7+uxOTpITYWaGP7/AhoR3+A+/1e9skrzelGi/dXzEYyvbxubEF6Wn2ypscTKiKJFFn1ag==}\n \n@@ -6566,8 +6563,8 @@ packages:\n     peerDependencies:\n       react: '>=16.8.0'\n \n-  react-docgen-typescript@2.4.0:\n-    resolution: {integrity: sha512-ZtAp5XTO5HRzQctjPU0ybY0RRCQO19X/8fxn3w7y2VVTUbGHDKULPTL4ky3vB05euSgG5NpALhEhDPvQ56wvXg==}\n+  react-docgen-typescript@2.2.2:\n+    resolution: {integrity: sha512-tvg2ZtOpOi6QDwsb3GZhOjDkkX0h8Z2gipvTg6OVMUyoYoURhEiRNePT8NZItTVCDh39JJHnLdfCOkzoLbFnTg==}\n     peerDependencies:\n       typescript: '>= 4.3.x'\n \n@@ -6668,6 +6665,11 @@ packages:\n       '@types/react':\n         optional: true\n \n+  react-timeago@8.2.0:\n+    resolution: {integrity: sha512-RWDlG3Jj+iwv+yNEDweA/Qk1mxE8i/Oc4oW8Irp29ZfBp+eNpqqYPMLPYQJyfRMJcGB8CmWkEGMYhB4fW8eZlQ==}\n+    peerDependencies:\n+      react: ^16.0.0 || ^17.0.0 || ^18.0.0 || ^19.0.0\n+\n   react-transition-group@4.4.5:\n     resolution: {integrity: sha512-pZcd1MCJoiKiBR2NRxeCRg13uCXbydPnmB4EOeRrY7480qNWO8IIgQG6zlDkm6uRMsURXPuKq0GWtiM59a5Q6g==}\n     peerDependencies:\n@@ -7037,8 +7039,8 @@ packages:\n     resolution: {integrity: sha512-WjlahMgHmCJpqzU8bIBy4qtsZdU9lRlcZE3Lvyej6t4tuOuv1vk57OW3MBrj6hXBFx/nNoC9MPMTcr5YA7NQbg==}\n     engines: {node: '>=6'}\n \n-  statuses@2.0.2:\n-    resolution: {integrity: sha512-DvEy55V3DB7uknRo+4iOGT5fP1slR8wQohVdknigZPMpMstaKJQWhwiYBACJE3Ul2pTnATihhBYnRhZQHGBiRw==}\n+  statuses@2.0.1:\n+    resolution: {integrity: sha512-RwNA9Z/7PrK06rYLIzFMlaF+l73iwpzsqRIFgbMLbTcLD6cOao82TaWefPXQvB2fOC4AjuYSEndS7N/mTCbkdQ==}\n     engines: {node: '>= 0.8'}\n \n   stop-iteration-iterator@1.1.0:\n@@ -7153,14 +7155,14 @@ packages:\n     peerDependencies:\n       webpack: ^5.0.0\n \n-  style-to-js@1.1.17:\n-    resolution: {integrity: sha512-xQcBGDxJb6jjFCTzvQtfiPn6YvvP2O8U1MDIPNfJQlWMYfktPy+iGsHE7cssjs7y84d9fQaK4UF3RIJaAHSoYA==}\n+  style-to-js@1.1.16:\n+    resolution: {integrity: sha512-/Q6ld50hKYPH3d/r6nr117TZkHR0w0kGGIVfpG9N6D8NymRPM9RqCUv4pRpJ62E5DqOYx2AFpbZMyCPnjQCnOw==}\n \n-  style-to-object@1.0.9:\n-    resolution: {integrity: sha512-G4qppLgKu/k6FwRpHiGiKPaPTFcG3g4wNVX/Qsfu+RqQM30E7Tyu/TEgxcL9PNLF5pdRLwQdE3YKKf+KF2Dzlw==}\n+  style-to-object@1.0.8:\n+    resolution: {integrity: sha512-xT47I/Eo0rwJmaXC4oilDGDWLohVhR6o/xAQcPQN8q6QBuZVL8qMYL85kLmST5cPjAorwvqIA4qXTRQoYHaL6g==}\n \n-  styled-components@6.1.19:\n-    resolution: {integrity: sha512-1v/e3Dl1BknC37cXMhwGomhO8AkYmN41CqyX9xhUDxry1ns3BFQy2lLDRQXJRdVVWB9OHemv/53xaStimvWyuA==}\n+  styled-components@6.1.18:\n+    resolution: {integrity: sha512-Mvf3gJFzZCkhjY2Y/Fx9z1m3dxbza0uI9H1CbNZm/jSHCojzJhQ0R7bByrlFJINnMzz/gPulpoFFGymNwrsMcw==}\n     engines: {node: '>= 16'}\n     peerDependencies:\n       react: '>= 16.8.0'\n@@ -7219,6 +7221,17 @@ packages:\n   tailwind-merge@2.6.0:\n     resolution: {integrity: sha512-P+Vu1qXfzediirmHOC3xKGAYeZtPcV9g76X+xg2FD4tYgR71ewMA35Y3sCz3zhiN/dwefRpJX0yBcgwi1fXNQA==}\n \n+  tailwind-scrollbar-hide@2.0.0:\n+    resolution: {integrity: sha512-lqiIutHliEiODwBRHy4G2+Tcayo2U7+3+4frBmoMETD72qtah+XhOk5XcPzC1nJvXhXUdfl2ajlMhUc2qC6CIg==}\n+    peerDependencies:\n+      tailwindcss: '>=3.0.0 || >= 4.0.0 || >= 4.0.0-beta.8 || >= 4.0.0-alpha.20'\n+\n+  tailwind-scrollbar@4.0.2:\n+    resolution: {integrity: sha512-wAQiIxAPqk0MNTPptVe/xoyWi27y+NRGnTwvn4PQnbvB9kp8QUBiGl/wsfoVBHnQxTmhXJSNt9NHTmcz9EivFA==}\n+    engines: {node: '>=12.13.0'}\n+    peerDependencies:\n+      tailwindcss: 4.x\n+\n   tailwindcss-animate@1.0.7:\n     resolution: {integrity: sha512-bl6mpH3T7I3UFxuvDEXLxy/VuFxBk5bbzplh7tXI68mwMokNYd1t9qPBHlnyTwfa4JGC4zP516I1hYYtQ/vspA==}\n     peerDependencies:\n@@ -7249,8 +7262,8 @@ packages:\n       uglify-js:\n         optional: true\n \n-  terser@5.42.0:\n-    resolution: {integrity: sha512-UYCvU9YQW2f/Vwl+P0GfhxJxbUGLwd+5QrrGgLajzWAtC/23AX0vcise32kkP7Eu0Wu9VlzzHAXkLObgjQfFlQ==}\n+  terser@5.41.0:\n+    resolution: {integrity: sha512-H406eLPXpZbAX14+B8psIuvIr8+3c+2hkuYzpMkoE0ij+NdsVATbA78vb8neA/eqrj7rywa2pIkdmWRsXW6wmw==}\n     engines: {node: '>=10'}\n     hasBin: true\n \n@@ -7471,8 +7484,8 @@ packages:\n     resolution: {integrity: sha512-4/u/j4FrCKdi17jaxuJA0jClGxB1AvU2hw/IuayPc4ay1XGaJs/rbb4v5WKwAjNifjmXK9PIFyuPiaK8azyR9w==}\n     engines: {node: '>=14.0.0'}\n \n-  unrs-resolver@1.9.0:\n-    resolution: {integrity: sha512-wqaRu4UnzBD2ABTC1kLfBjAqIDZ5YUTr/MLGa7By47JV1bJDSW7jq/ZSLigB7enLe7ubNaJhtnBXgrc/50cEhg==}\n+  unrs-resolver@1.7.10:\n+    resolution: {integrity: sha512-CJEMJcz6vuwRK6xxWc+uf8AGi0OyfoVtHs5mExtNecS0HZq3a3Br1JC/InwwTn6uy+qkAdAdK+nJUYO9FPtgZw==}\n \n   update-browserslist-db@1.1.3:\n     resolution: {integrity: sha512-UxhIZQ+QInVdunkDAaiazvvT/+fXL5Osr0JZlJulepYu6Jd7qJtDZjlur0emRlT71EN3ScPoE7gvsuIKKNavKw==}\n@@ -8503,7 +8516,7 @@ snapshots:\n       babel-plugin-polyfill-corejs2: 0.4.13(@babel/core@7.27.4)\n       babel-plugin-polyfill-corejs3: 0.11.1(@babel/core@7.27.4)\n       babel-plugin-polyfill-regenerator: 0.6.4(@babel/core@7.27.4)\n-      core-js-compat: 3.43.0\n+      core-js-compat: 3.42.0\n       semver: 6.3.1\n     transitivePeerDependencies:\n       - supports-color\n@@ -8571,7 +8584,7 @@ snapshots:\n \n   '@bundled-es-modules/statuses@1.0.1':\n     dependencies:\n-      statuses: 2.0.2\n+      statuses: 2.0.1\n \n   '@bundled-es-modules/tough-cookie@0.1.6':\n     dependencies:\n@@ -8617,79 +8630,79 @@ snapshots:\n \n   '@emotion/unitless@0.8.1': {}\n \n-  '@esbuild/aix-ppc64@0.25.5':\n+  '@esbuild/aix-ppc64@0.24.2':\n     optional: true\n \n-  '@esbuild/android-arm64@0.25.5':\n+  '@esbuild/android-arm64@0.24.2':\n     optional: true\n \n-  '@esbuild/android-arm@0.25.5':\n+  '@esbuild/android-arm@0.24.2':\n     optional: true\n \n-  '@esbuild/android-x64@0.25.5':\n+  '@esbuild/android-x64@0.24.2':\n     optional: true\n \n-  '@esbuild/darwin-arm64@0.25.5':\n+  '@esbuild/darwin-arm64@0.24.2':\n     optional: true\n \n-  '@esbuild/darwin-x64@0.25.5':\n+  '@esbuild/darwin-x64@0.24.2':\n     optional: true\n \n-  '@esbuild/freebsd-arm64@0.25.5':\n+  '@esbuild/freebsd-arm64@0.24.2':\n     optional: true\n \n-  '@esbuild/freebsd-x64@0.25.5':\n+  '@esbuild/freebsd-x64@0.24.2':\n     optional: true\n \n-  '@esbuild/linux-arm64@0.25.5':\n+  '@esbuild/linux-arm64@0.24.2':\n     optional: true\n \n-  '@esbuild/linux-arm@0.25.5':\n+  '@esbuild/linux-arm@0.24.2':\n     optional: true\n \n-  '@esbuild/linux-ia32@0.25.5':\n+  '@esbuild/linux-ia32@0.24.2':\n     optional: true\n \n-  '@esbuild/linux-loong64@0.25.5':\n+  '@esbuild/linux-loong64@0.24.2':\n     optional: true\n \n-  '@esbuild/linux-mips64el@0.25.5':\n+  '@esbuild/linux-mips64el@0.24.2':\n     optional: true\n \n-  '@esbuild/linux-ppc64@0.25.5':\n+  '@esbuild/linux-ppc64@0.24.2':\n     optional: true\n \n-  '@esbuild/linux-riscv64@0.25.5':\n+  '@esbuild/linux-riscv64@0.24.2':\n     optional: true\n \n-  '@esbuild/linux-s390x@0.25.5':\n+  '@esbuild/linux-s390x@0.24.2':\n     optional: true\n \n-  '@esbuild/linux-x64@0.25.5':\n+  '@esbuild/linux-x64@0.24.2':\n     optional: true\n \n-  '@esbuild/netbsd-arm64@0.25.5':\n+  '@esbuild/netbsd-arm64@0.24.2':\n     optional: true\n \n-  '@esbuild/netbsd-x64@0.25.5':\n+  '@esbuild/netbsd-x64@0.24.2':\n     optional: true\n \n-  '@esbuild/openbsd-arm64@0.25.5':\n+  '@esbuild/openbsd-arm64@0.24.2':\n     optional: true\n \n-  '@esbuild/openbsd-x64@0.25.5':\n+  '@esbuild/openbsd-x64@0.24.2':\n     optional: true\n \n-  '@esbuild/sunos-x64@0.25.5':\n+  '@esbuild/sunos-x64@0.24.2':\n     optional: true\n \n-  '@esbuild/win32-arm64@0.25.5':\n+  '@esbuild/win32-arm64@0.24.2':\n     optional: true\n \n-  '@esbuild/win32-ia32@0.25.5':\n+  '@esbuild/win32-ia32@0.24.2':\n     optional: true\n \n-  '@esbuild/win32-x64@0.25.5':\n+  '@esbuild/win32-x64@0.24.2':\n     optional: true\n \n   '@eslint-community/eslint-utils@4.7.0(eslint@8.57.1)':\n@@ -9161,7 +9174,7 @@ snapshots:\n       outvariant: 1.4.3\n       strict-event-emitter: 0.5.1\n \n-  '@napi-rs/wasm-runtime@0.2.11':\n+  '@napi-rs/wasm-runtime@0.2.10':\n     dependencies:\n       '@emnapi/core': 1.4.3\n       '@emnapi/runtime': 1.4.3\n@@ -9481,17 +9494,17 @@ snapshots:\n     dependencies:\n       playwright: 1.52.0\n \n-  '@pmmmwh/react-refresh-webpack-plugin@0.5.16(react-refresh@0.14.2)(type-fest@4.41.0)(webpack-hot-middleware@2.26.1)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))':\n+  '@pmmmwh/react-refresh-webpack-plugin@0.5.16(react-refresh@0.14.2)(type-fest@4.41.0)(webpack-hot-middleware@2.26.1)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))':\n     dependencies:\n       ansi-html: 0.0.9\n-      core-js-pure: 3.43.0\n+      core-js-pure: 3.42.0\n       error-stack-parser: 2.1.4\n       html-entities: 2.6.0\n       loader-utils: 2.0.4\n       react-refresh: 0.14.2\n       schema-utils: 4.3.2\n       source-map: 0.7.4\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n     optionalDependencies:\n       type-fest: 4.41.0\n       webpack-hot-middleware: 2.26.1\n@@ -10047,7 +10060,7 @@ snapshots:\n       '@rollup/pluginutils': 5.1.4(rollup@4.35.0)\n       commondir: 1.0.1\n       estree-walker: 2.0.2\n-      fdir: 6.4.6(picomatch@4.0.2)\n+      fdir: 6.4.5(picomatch@4.0.2)\n       is-reference: 1.2.1\n       magic-string: 0.30.17\n       picomatch: 4.0.2\n@@ -10056,7 +10069,7 @@ snapshots:\n \n   '@rollup/pluginutils@5.1.4(rollup@4.35.0)':\n     dependencies:\n-      '@types/estree': 1.0.8\n+      '@types/estree': 1.0.7\n       estree-walker: 2.0.2\n       picomatch: 4.0.2\n     optionalDependencies:\n@@ -10209,7 +10222,7 @@ snapshots:\n \n   '@sentry/core@9.27.0': {}\n \n-  '@sentry/nextjs@9.27.0(@opentelemetry/context-async-hooks@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/core@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/instrumentation@0.57.2(@opentelemetry/api@1.9.0))(@opentelemetry/sdk-trace-base@1.30.1(@opentelemetry/api@1.9.0))(next@15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1))(react@18.3.1)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))':\n+  '@sentry/nextjs@9.27.0(@opentelemetry/context-async-hooks@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/core@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/instrumentation@0.57.2(@opentelemetry/api@1.9.0))(@opentelemetry/sdk-trace-base@1.30.1(@opentelemetry/api@1.9.0))(next@15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1))(react@18.3.1)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))':\n     dependencies:\n       '@opentelemetry/api': 1.9.0\n       '@opentelemetry/semantic-conventions': 1.34.0\n@@ -10220,7 +10233,7 @@ snapshots:\n       '@sentry/opentelemetry': 9.27.0(@opentelemetry/api@1.9.0)(@opentelemetry/context-async-hooks@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/core@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/instrumentation@0.57.2(@opentelemetry/api@1.9.0))(@opentelemetry/sdk-trace-base@1.30.1(@opentelemetry/api@1.9.0))(@opentelemetry/semantic-conventions@1.34.0)\n       '@sentry/react': 9.27.0(react@18.3.1)\n       '@sentry/vercel-edge': 9.27.0\n-      '@sentry/webpack-plugin': 3.5.0(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      '@sentry/webpack-plugin': 3.5.0(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       chalk: 3.0.0\n       next: 15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1)\n       resolve: 1.22.8\n@@ -10297,12 +10310,12 @@ snapshots:\n       '@opentelemetry/api': 1.9.0\n       '@sentry/core': 9.27.0\n \n-  '@sentry/webpack-plugin@3.5.0(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))':\n+  '@sentry/webpack-plugin@3.5.0(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))':\n     dependencies:\n       '@sentry/bundler-plugin-core': 3.5.0\n       unplugin: 1.0.1\n       uuid: 9.0.1\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n     transitivePeerDependencies:\n       - encoding\n       - supports-color\n@@ -10443,7 +10456,7 @@ snapshots:\n       react: 18.3.1\n       react-dom: 18.3.1(react@18.3.1)\n \n-  '@storybook/builder-webpack5@8.6.14(@swc/core@1.12.1)(esbuild@0.25.5)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)':\n+  '@storybook/builder-webpack5@8.6.14(@swc/core@1.11.31)(esbuild@0.24.2)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)':\n     dependencies:\n       '@storybook/core-webpack': 8.6.14(storybook@8.6.14(prettier@3.5.3))\n       '@types/semver': 7.7.0\n@@ -10451,23 +10464,23 @@ snapshots:\n       case-sensitive-paths-webpack-plugin: 2.4.0\n       cjs-module-lexer: 1.4.3\n       constants-browserify: 1.0.0\n-      css-loader: 6.11.0(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      css-loader: 6.11.0(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       es-module-lexer: 1.7.0\n-      fork-ts-checker-webpack-plugin: 8.0.0(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n-      html-webpack-plugin: 5.6.3(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      fork-ts-checker-webpack-plugin: 8.0.0(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n+      html-webpack-plugin: 5.6.3(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       magic-string: 0.30.17\n       path-browserify: 1.0.1\n       process: 0.11.10\n       semver: 7.7.2\n       storybook: 8.6.14(prettier@3.5.3)\n-      style-loader: 3.3.4(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n-      terser-webpack-plugin: 5.3.14(@swc/core@1.12.1)(esbuild@0.25.5)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      style-loader: 3.3.4(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n+      terser-webpack-plugin: 5.3.14(@swc/core@1.11.31)(esbuild@0.24.2)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       ts-dedent: 2.2.0\n       url: 0.11.4\n       util: 0.12.5\n       util-deprecate: 1.0.2\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n-      webpack-dev-middleware: 6.1.3(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n+      webpack-dev-middleware: 6.1.3(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       webpack-hot-middleware: 2.26.1\n       webpack-virtual-modules: 0.6.2\n     optionalDependencies:\n@@ -10493,8 +10506,8 @@ snapshots:\n       '@storybook/theming': 8.6.14(storybook@8.6.14(prettier@3.5.3))\n       better-opn: 3.0.2\n       browser-assert: 1.2.1\n-      esbuild: 0.25.5\n-      esbuild-register: 3.6.0(esbuild@0.25.5)\n+      esbuild: 0.24.2\n+      esbuild-register: 3.6.0(esbuild@0.24.2)\n       jsdoc-type-pratt-parser: 4.1.0\n       process: 0.11.10\n       recast: 0.23.11\n@@ -10535,7 +10548,7 @@ snapshots:\n     dependencies:\n       storybook: 8.6.14(prettier@3.5.3)\n \n-  '@storybook/nextjs@8.6.14(@swc/core@1.12.1)(esbuild@0.25.5)(next@15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1))(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(type-fest@4.41.0)(typescript@5.8.3)(webpack-hot-middleware@2.26.1)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))':\n+  '@storybook/nextjs@8.6.14(@swc/core@1.11.31)(esbuild@0.24.2)(next@15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1))(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(type-fest@4.41.0)(typescript@5.8.3)(webpack-hot-middleware@2.26.1)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))':\n     dependencies:\n       '@babel/core': 7.27.4\n       '@babel/plugin-syntax-bigint': 7.8.3(@babel/core@7.27.4)\n@@ -10550,30 +10563,30 @@ snapshots:\n       '@babel/preset-react': 7.27.1(@babel/core@7.27.4)\n       '@babel/preset-typescript': 7.27.1(@babel/core@7.27.4)\n       '@babel/runtime': 7.27.6\n-      '@pmmmwh/react-refresh-webpack-plugin': 0.5.16(react-refresh@0.14.2)(type-fest@4.41.0)(webpack-hot-middleware@2.26.1)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n-      '@storybook/builder-webpack5': 8.6.14(@swc/core@1.12.1)(esbuild@0.25.5)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)\n-      '@storybook/preset-react-webpack': 8.6.14(@storybook/test@8.6.14(storybook@8.6.14(prettier@3.5.3)))(@swc/core@1.12.1)(esbuild@0.25.5)(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)\n+      '@pmmmwh/react-refresh-webpack-plugin': 0.5.16(react-refresh@0.14.2)(type-fest@4.41.0)(webpack-hot-middleware@2.26.1)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n+      '@storybook/builder-webpack5': 8.6.14(@swc/core@1.11.31)(esbuild@0.24.2)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)\n+      '@storybook/preset-react-webpack': 8.6.14(@storybook/test@8.6.14(storybook@8.6.14(prettier@3.5.3)))(@swc/core@1.11.31)(esbuild@0.24.2)(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)\n       '@storybook/react': 8.6.14(@storybook/test@8.6.14(storybook@8.6.14(prettier@3.5.3)))(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)\n       '@storybook/test': 8.6.14(storybook@8.6.14(prettier@3.5.3))\n       '@types/semver': 7.7.0\n-      babel-loader: 9.2.1(@babel/core@7.27.4)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n-      css-loader: 6.11.0(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      babel-loader: 9.2.1(@babel/core@7.27.4)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n+      css-loader: 6.11.0(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       find-up: 5.0.0\n       image-size: 1.2.1\n       loader-utils: 3.3.1\n       next: 15.3.3(@babel/core@7.27.4)(@opentelemetry/api@1.9.0)(@playwright/test@1.52.0)(react-dom@18.3.1(react@18.3.1))(react@18.3.1)\n-      node-polyfill-webpack-plugin: 2.0.1(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      node-polyfill-webpack-plugin: 2.0.1(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       pnp-webpack-plugin: 1.7.0(typescript@5.8.3)\n       postcss: 8.5.4\n-      postcss-loader: 8.1.1(postcss@8.5.4)(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      postcss-loader: 8.1.1(postcss@8.5.4)(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       react: 18.3.1\n       react-dom: 18.3.1(react@18.3.1)\n       react-refresh: 0.14.2\n       resolve-url-loader: 5.0.0\n-      sass-loader: 14.2.1(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      sass-loader: 14.2.1(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       semver: 7.7.2\n       storybook: 8.6.14(prettier@3.5.3)\n-      style-loader: 3.3.4(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      style-loader: 3.3.4(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       styled-jsx: 5.1.7(@babel/core@7.27.4)(react@18.3.1)\n       ts-dedent: 2.2.0\n       tsconfig-paths: 4.2.0\n@@ -10581,7 +10594,7 @@ snapshots:\n     optionalDependencies:\n       sharp: 0.33.5\n       typescript: 5.8.3\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n     transitivePeerDependencies:\n       - '@rspack/core'\n       - '@swc/core'\n@@ -10600,11 +10613,11 @@ snapshots:\n       - webpack-hot-middleware\n       - webpack-plugin-serve\n \n-  '@storybook/preset-react-webpack@8.6.14(@storybook/test@8.6.14(storybook@8.6.14(prettier@3.5.3)))(@swc/core@1.12.1)(esbuild@0.25.5)(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)':\n+  '@storybook/preset-react-webpack@8.6.14(@storybook/test@8.6.14(storybook@8.6.14(prettier@3.5.3)))(@swc/core@1.11.31)(esbuild@0.24.2)(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)':\n     dependencies:\n       '@storybook/core-webpack': 8.6.14(storybook@8.6.14(prettier@3.5.3))\n       '@storybook/react': 8.6.14(@storybook/test@8.6.14(storybook@8.6.14(prettier@3.5.3)))(react-dom@18.3.1(react@18.3.1))(react@18.3.1)(storybook@8.6.14(prettier@3.5.3))(typescript@5.8.3)\n-      '@storybook/react-docgen-typescript-plugin': 1.0.6--canary.9.0c3f3b7.0(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      '@storybook/react-docgen-typescript-plugin': 1.0.6--canary.9.0c3f3b7.0(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       '@types/semver': 7.7.0\n       find-up: 5.0.0\n       magic-string: 0.30.17\n@@ -10615,7 +10628,7 @@ snapshots:\n       semver: 7.7.2\n       storybook: 8.6.14(prettier@3.5.3)\n       tsconfig-paths: 4.2.0\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n     optionalDependencies:\n       typescript: 5.8.3\n     transitivePeerDependencies:\n@@ -10630,17 +10643,17 @@ snapshots:\n     dependencies:\n       storybook: 8.6.14(prettier@3.5.3)\n \n-  '@storybook/react-docgen-typescript-plugin@1.0.6--canary.9.0c3f3b7.0(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))':\n+  '@storybook/react-docgen-typescript-plugin@1.0.6--canary.9.0c3f3b7.0(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))':\n     dependencies:\n       debug: 4.4.1\n       endent: 2.1.0\n       find-cache-dir: 3.3.2\n       flat-cache: 3.2.0\n       micromatch: 4.0.8\n-      react-docgen-typescript: 2.4.0(typescript@5.8.3)\n+      react-docgen-typescript: 2.2.2(typescript@5.8.3)\n       tslib: 2.8.1\n       typescript: 5.8.3\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n     transitivePeerDependencies:\n       - supports-color\n \n@@ -10673,8 +10686,8 @@ snapshots:\n       '@babel/types': 7.27.6\n       '@jest/types': 29.6.3\n       '@storybook/csf': 0.1.13\n-      '@swc/core': 1.12.1\n-      '@swc/jest': 0.2.38(@swc/core@1.12.1)\n+      '@swc/core': 1.11.31\n+      '@swc/jest': 0.2.38(@swc/core@1.11.31)\n       expect-playwright: 0.8.0\n       jest: 29.7.0(@types/node@22.15.30)\n       jest-circus: 29.7.0\n@@ -10685,7 +10698,7 @@ snapshots:\n       jest-serializer-html: 7.1.0\n       jest-watch-typeahead: 2.2.2(jest@29.7.0(@types/node@22.15.30))\n       nyc: 15.1.0\n-      playwright: 1.53.0\n+      playwright: 1.52.0\n       storybook: 8.6.14(prettier@3.5.3)\n     transitivePeerDependencies:\n       - '@swc/helpers'\n@@ -10758,51 +10771,51 @@ snapshots:\n       - bufferutil\n       - utf-8-validate\n \n-  '@swc/core-darwin-arm64@1.12.1':\n+  '@swc/core-darwin-arm64@1.11.31':\n     optional: true\n \n-  '@swc/core-darwin-x64@1.12.1':\n+  '@swc/core-darwin-x64@1.11.31':\n     optional: true\n \n-  '@swc/core-linux-arm-gnueabihf@1.12.1':\n+  '@swc/core-linux-arm-gnueabihf@1.11.31':\n     optional: true\n \n-  '@swc/core-linux-arm64-gnu@1.12.1':\n+  '@swc/core-linux-arm64-gnu@1.11.31':\n     optional: true\n \n-  '@swc/core-linux-arm64-musl@1.12.1':\n+  '@swc/core-linux-arm64-musl@1.11.31':\n     optional: true\n \n-  '@swc/core-linux-x64-gnu@1.12.1':\n+  '@swc/core-linux-x64-gnu@1.11.31':\n     optional: true\n \n-  '@swc/core-linux-x64-musl@1.12.1':\n+  '@swc/core-linux-x64-musl@1.11.31':\n     optional: true\n \n-  '@swc/core-win32-arm64-msvc@1.12.1':\n+  '@swc/core-win32-arm64-msvc@1.11.31':\n     optional: true\n \n-  '@swc/core-win32-ia32-msvc@1.12.1':\n+  '@swc/core-win32-ia32-msvc@1.11.31':\n     optional: true\n \n-  '@swc/core-win32-x64-msvc@1.12.1':\n+  '@swc/core-win32-x64-msvc@1.11.31':\n     optional: true\n \n-  '@swc/core@1.12.1':\n+  '@swc/core@1.11.31':\n     dependencies:\n       '@swc/counter': 0.1.3\n-      '@swc/types': 0.1.23\n+      '@swc/types': 0.1.21\n     optionalDependencies:\n-      '@swc/core-darwin-arm64': 1.12.1\n-      '@swc/core-darwin-x64': 1.12.1\n-      '@swc/core-linux-arm-gnueabihf': 1.12.1\n-      '@swc/core-linux-arm64-gnu': 1.12.1\n-      '@swc/core-linux-arm64-musl': 1.12.1\n-      '@swc/core-linux-x64-gnu': 1.12.1\n-      '@swc/core-linux-x64-musl': 1.12.1\n-      '@swc/core-win32-arm64-msvc': 1.12.1\n-      '@swc/core-win32-ia32-msvc': 1.12.1\n-      '@swc/core-win32-x64-msvc': 1.12.1\n+      '@swc/core-darwin-arm64': 1.11.31\n+      '@swc/core-darwin-x64': 1.11.31\n+      '@swc/core-linux-arm-gnueabihf': 1.11.31\n+      '@swc/core-linux-arm64-gnu': 1.11.31\n+      '@swc/core-linux-arm64-musl': 1.11.31\n+      '@swc/core-linux-x64-gnu': 1.11.31\n+      '@swc/core-linux-x64-musl': 1.11.31\n+      '@swc/core-win32-arm64-msvc': 1.11.31\n+      '@swc/core-win32-ia32-msvc': 1.11.31\n+      '@swc/core-win32-x64-msvc': 1.11.31\n \n   '@swc/counter@0.1.3': {}\n \n@@ -10810,14 +10823,14 @@ snapshots:\n     dependencies:\n       tslib: 2.8.1\n \n-  '@swc/jest@0.2.38(@swc/core@1.12.1)':\n+  '@swc/jest@0.2.38(@swc/core@1.11.31)':\n     dependencies:\n       '@jest/create-cache-key-function': 29.7.0\n-      '@swc/core': 1.12.1\n+      '@swc/core': 1.11.31\n       '@swc/counter': 0.1.3\n       jsonc-parser: 3.3.1\n \n-  '@swc/types@0.1.23':\n+  '@swc/types@0.1.21':\n     dependencies:\n       '@swc/counter': 0.1.3\n \n@@ -10938,20 +10951,20 @@ snapshots:\n   '@types/eslint-scope@3.7.7':\n     dependencies:\n       '@types/eslint': 9.6.1\n-      '@types/estree': 1.0.8\n+      '@types/estree': 1.0.7\n \n   '@types/eslint@9.6.1':\n     dependencies:\n-      '@types/estree': 1.0.8\n+      '@types/estree': 1.0.7\n       '@types/json-schema': 7.0.15\n \n   '@types/estree-jsx@1.0.5':\n     dependencies:\n-      '@types/estree': 1.0.8\n+      '@types/estree': 1.0.7\n \n   '@types/estree@1.0.6': {}\n \n-  '@types/estree@1.0.8': {}\n+  '@types/estree@1.0.7': {}\n \n   '@types/graceful-fs@4.1.9':\n     dependencies:\n@@ -11015,7 +11028,9 @@ snapshots:\n \n   '@types/phoenix@1.6.6': {}\n \n-  '@types/prop-types@15.7.15': {}\n+  '@types/prismjs@1.26.5': {}\n+\n+  '@types/prop-types@15.7.14': {}\n \n   '@types/react-dom@18.3.5(@types/react@18.3.17)':\n     dependencies:\n@@ -11027,7 +11042,7 @@ snapshots:\n \n   '@types/react@18.3.17':\n     dependencies:\n-      '@types/prop-types': 15.7.15\n+      '@types/prop-types': 15.7.14\n       csstype: 3.1.3\n \n   '@types/resolve@1.20.6': {}\n@@ -11038,7 +11053,7 @@ snapshots:\n \n   '@types/stack-utils@2.0.3': {}\n \n-  '@types/statuses@2.0.6': {}\n+  '@types/statuses@2.0.5': {}\n \n   '@types/stylis@4.2.5': {}\n \n@@ -11068,14 +11083,14 @@ snapshots:\n     dependencies:\n       '@types/yargs-parser': 21.0.3\n \n-  '@typescript-eslint/eslint-plugin@8.34.1(@typescript-eslint/parser@8.34.1(eslint@8.57.1)(typescript@5.8.3))(eslint@8.57.1)(typescript@5.8.3)':\n+  '@typescript-eslint/eslint-plugin@8.33.1(@typescript-eslint/parser@8.33.1(eslint@8.57.1)(typescript@5.8.3))(eslint@8.57.1)(typescript@5.8.3)':\n     dependencies:\n       '@eslint-community/regexpp': 4.12.1\n-      '@typescript-eslint/parser': 8.34.1(eslint@8.57.1)(typescript@5.8.3)\n-      '@typescript-eslint/scope-manager': 8.34.1\n-      '@typescript-eslint/type-utils': 8.34.1(eslint@8.57.1)(typescript@5.8.3)\n-      '@typescript-eslint/utils': 8.34.1(eslint@8.57.1)(typescript@5.8.3)\n-      '@typescript-eslint/visitor-keys': 8.34.1\n+      '@typescript-eslint/parser': 8.33.1(eslint@8.57.1)(typescript@5.8.3)\n+      '@typescript-eslint/scope-manager': 8.33.1\n+      '@typescript-eslint/type-utils': 8.33.1(eslint@8.57.1)(typescript@5.8.3)\n+      '@typescript-eslint/utils': 8.33.1(eslint@8.57.1)(typescript@5.8.3)\n+      '@typescript-eslint/visitor-keys': 8.33.1\n       eslint: 8.57.1\n       graphemer: 1.4.0\n       ignore: 7.0.5\n@@ -11085,40 +11100,40 @@ snapshots:\n     transitivePeerDependencies:\n       - supports-color\n \n-  '@typescript-eslint/parser@8.34.1(eslint@8.57.1)(typescript@5.8.3)':\n+  '@typescript-eslint/parser@8.33.1(eslint@8.57.1)(typescript@5.8.3)':\n     dependencies:\n-      '@typescript-eslint/scope-manager': 8.34.1\n-      '@typescript-eslint/types': 8.34.1\n-      '@typescript-eslint/typescript-estree': 8.34.1(typescript@5.8.3)\n-      '@typescript-eslint/visitor-keys': 8.34.1\n+      '@typescript-eslint/scope-manager': 8.33.1\n+      '@typescript-eslint/types': 8.33.1\n+      '@typescript-eslint/typescript-estree': 8.33.1(typescript@5.8.3)\n+      '@typescript-eslint/visitor-keys': 8.33.1\n       debug: 4.4.1\n       eslint: 8.57.1\n       typescript: 5.8.3\n     transitivePeerDependencies:\n       - supports-color\n \n-  '@typescript-eslint/project-service@8.34.1(typescript@5.8.3)':\n+  '@typescript-eslint/project-service@8.33.1(typescript@5.8.3)':\n     dependencies:\n-      '@typescript-eslint/tsconfig-utils': 8.34.1(typescript@5.8.3)\n-      '@typescript-eslint/types': 8.34.1\n+      '@typescript-eslint/tsconfig-utils': 8.33.1(typescript@5.8.3)\n+      '@typescript-eslint/types': 8.33.1\n       debug: 4.4.1\n       typescript: 5.8.3\n     transitivePeerDependencies:\n       - supports-color\n \n-  '@typescript-eslint/scope-manager@8.34.1':\n+  '@typescript-eslint/scope-manager@8.33.1':\n     dependencies:\n-      '@typescript-eslint/types': 8.34.1\n-      '@typescript-eslint/visitor-keys': 8.34.1\n+      '@typescript-eslint/types': 8.33.1\n+      '@typescript-eslint/visitor-keys': 8.33.1\n \n-  '@typescript-eslint/tsconfig-utils@8.34.1(typescript@5.8.3)':\n+  '@typescript-eslint/tsconfig-utils@8.33.1(typescript@5.8.3)':\n     dependencies:\n       typescript: 5.8.3\n \n-  '@typescript-eslint/type-utils@8.34.1(eslint@8.57.1)(typescript@5.8.3)':\n+  '@typescript-eslint/type-utils@8.33.1(eslint@8.57.1)(typescript@5.8.3)':\n     dependencies:\n-      '@typescript-eslint/typescript-estree': 8.34.1(typescript@5.8.3)\n-      '@typescript-eslint/utils': 8.34.1(eslint@8.57.1)(typescript@5.8.3)\n+      '@typescript-eslint/typescript-estree': 8.33.1(typescript@5.8.3)\n+      '@typescript-eslint/utils': 8.33.1(eslint@8.57.1)(typescript@5.8.3)\n       debug: 4.4.1\n       eslint: 8.57.1\n       ts-api-utils: 2.1.0(typescript@5.8.3)\n@@ -11126,14 +11141,14 @@ snapshots:\n     transitivePeerDependencies:\n       - supports-color\n \n-  '@typescript-eslint/types@8.34.1': {}\n+  '@typescript-eslint/types@8.33.1': {}\n \n-  '@typescript-eslint/typescript-estree@8.34.1(typescript@5.8.3)':\n+  '@typescript-eslint/typescript-estree@8.33.1(typescript@5.8.3)':\n     dependencies:\n-      '@typescript-eslint/project-service': 8.34.1(typescript@5.8.3)\n-      '@typescript-eslint/tsconfig-utils': 8.34.1(typescript@5.8.3)\n-      '@typescript-eslint/types': 8.34.1\n-      '@typescript-eslint/visitor-keys': 8.34.1\n+      '@typescript-eslint/project-service': 8.33.1(typescript@5.8.3)\n+      '@typescript-eslint/tsconfig-utils': 8.33.1(typescript@5.8.3)\n+      '@typescript-eslint/types': 8.33.1\n+      '@typescript-eslint/visitor-keys': 8.33.1\n       debug: 4.4.1\n       fast-glob: 3.3.3\n       is-glob: 4.0.3\n@@ -11144,81 +11159,75 @@ snapshots:\n     transitivePeerDependencies:\n       - supports-color\n \n-  '@typescript-eslint/utils@8.34.1(eslint@8.57.1)(typescript@5.8.3)':\n+  '@typescript-eslint/utils@8.33.1(eslint@8.57.1)(typescript@5.8.3)':\n     dependencies:\n       '@eslint-community/eslint-utils': 4.7.0(eslint@8.57.1)\n-      '@typescript-eslint/scope-manager': 8.34.1\n-      '@typescript-eslint/types': 8.34.1\n-      '@typescript-eslint/typescript-estree': 8.34.1(typescript@5.8.3)\n+      '@typescript-eslint/scope-manager': 8.33.1\n+      '@typescript-eslint/types': 8.33.1\n+      '@typescript-eslint/typescript-estree': 8.33.1(typescript@5.8.3)\n       eslint: 8.57.1\n       typescript: 5.8.3\n     transitivePeerDependencies:\n       - supports-color\n \n-  '@typescript-eslint/visitor-keys@8.34.1':\n+  '@typescript-eslint/visitor-keys@8.33.1':\n     dependencies:\n-      '@typescript-eslint/types': 8.34.1\n-      eslint-visitor-keys: 4.2.1\n+      '@typescript-eslint/types': 8.33.1\n+      eslint-visitor-keys: 4.2.0\n \n   '@ungap/structured-clone@1.3.0': {}\n \n-  '@unrs/resolver-binding-android-arm-eabi@1.9.0':\n-    optional: true\n-\n-  '@unrs/resolver-binding-android-arm64@1.9.0':\n+  '@unrs/resolver-binding-darwin-arm64@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-darwin-arm64@1.9.0':\n+  '@unrs/resolver-binding-darwin-x64@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-darwin-x64@1.9.0':\n+  '@unrs/resolver-binding-freebsd-x64@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-freebsd-x64@1.9.0':\n+  '@unrs/resolver-binding-linux-arm-gnueabihf@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-arm-gnueabihf@1.9.0':\n+  '@unrs/resolver-binding-linux-arm-musleabihf@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-arm-musleabihf@1.9.0':\n+  '@unrs/resolver-binding-linux-arm64-gnu@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-arm64-gnu@1.9.0':\n+  '@unrs/resolver-binding-linux-arm64-musl@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-arm64-musl@1.9.0':\n+  '@unrs/resolver-binding-linux-ppc64-gnu@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-ppc64-gnu@1.9.0':\n+  '@unrs/resolver-binding-linux-riscv64-gnu@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-riscv64-gnu@1.9.0':\n+  '@unrs/resolver-binding-linux-riscv64-musl@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-riscv64-musl@1.9.0':\n+  '@unrs/resolver-binding-linux-s390x-gnu@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-s390x-gnu@1.9.0':\n+  '@unrs/resolver-binding-linux-x64-gnu@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-x64-gnu@1.9.0':\n+  '@unrs/resolver-binding-linux-x64-musl@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-linux-x64-musl@1.9.0':\n-    optional: true\n-\n-  '@unrs/resolver-binding-wasm32-wasi@1.9.0':\n+  '@unrs/resolver-binding-wasm32-wasi@1.7.10':\n     dependencies:\n-      '@napi-rs/wasm-runtime': 0.2.11\n+      '@napi-rs/wasm-runtime': 0.2.10\n     optional: true\n \n-  '@unrs/resolver-binding-win32-arm64-msvc@1.9.0':\n+  '@unrs/resolver-binding-win32-arm64-msvc@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-win32-ia32-msvc@1.9.0':\n+  '@unrs/resolver-binding-win32-ia32-msvc@1.7.10':\n     optional: true\n \n-  '@unrs/resolver-binding-win32-x64-msvc@1.9.0':\n+  '@unrs/resolver-binding-win32-x64-msvc@1.7.10':\n     optional: true\n \n   '@vitest/expect@2.0.5':\n@@ -11244,13 +11253,13 @@ snapshots:\n     dependencies:\n       '@vitest/pretty-format': 2.0.5\n       estree-walker: 3.0.3\n-      loupe: 3.1.4\n+      loupe: 3.1.3\n       tinyrainbow: 1.2.0\n \n   '@vitest/utils@2.1.9':\n     dependencies:\n       '@vitest/pretty-format': 2.1.9\n-      loupe: 3.1.4\n+      loupe: 3.1.3\n       tinyrainbow: 1.2.0\n \n   '@webassemblyjs/ast@1.14.1':\n@@ -11358,15 +11367,15 @@ snapshots:\n     dependencies:\n       event-target-shim: 5.0.1\n \n-  acorn-import-attributes@1.9.5(acorn@8.15.0):\n+  acorn-import-attributes@1.9.5(acorn@8.14.1):\n     dependencies:\n-      acorn: 8.15.0\n+      acorn: 8.14.1\n \n-  acorn-jsx@5.3.2(acorn@8.15.0):\n+  acorn-jsx@5.3.2(acorn@8.14.1):\n     dependencies:\n-      acorn: 8.15.0\n+      acorn: 8.14.1\n \n-  acorn@8.15.0: {}\n+  acorn@8.14.1: {}\n \n   adjust-sourcemap-loader@4.0.0:\n     dependencies:\n@@ -11572,19 +11581,19 @@ snapshots:\n       axe-core: 4.10.3\n       mustache: 4.2.0\n \n-  axe-playwright@2.1.0(playwright@1.53.0):\n+  axe-playwright@2.1.0(playwright@1.52.0):\n     dependencies:\n       '@types/junit-report-builder': 3.0.2\n       axe-core: 4.10.3\n       axe-html-reporter: 2.2.11(axe-core@4.10.3)\n       junit-report-builder: 5.1.1\n       picocolors: 1.1.1\n-      playwright: 1.53.0\n+      playwright: 1.52.0\n \n-  axios@1.10.0:\n+  axios@1.9.0:\n     dependencies:\n       follow-redirects: 1.15.9\n-      form-data: 4.0.3\n+      form-data: 4.0.2\n       proxy-from-env: 1.1.0\n     transitivePeerDependencies:\n       - debug\n@@ -11604,12 +11613,12 @@ snapshots:\n     transitivePeerDependencies:\n       - supports-color\n \n-  babel-loader@9.2.1(@babel/core@7.27.4)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  babel-loader@9.2.1(@babel/core@7.27.4)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n       '@babel/core': 7.27.4\n       find-cache-dir: 4.0.0\n       schema-utils: 4.3.2\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n \n   babel-plugin-istanbul@6.1.1:\n     dependencies:\n@@ -11641,7 +11650,7 @@ snapshots:\n     dependencies:\n       '@babel/core': 7.27.4\n       '@babel/helper-define-polyfill-provider': 0.6.4(@babel/core@7.27.4)\n-      core-js-compat: 3.43.0\n+      core-js-compat: 3.42.0\n     transitivePeerDependencies:\n       - supports-color\n \n@@ -11699,12 +11708,12 @@ snapshots:\n \n   boring-avatars@1.11.2: {}\n \n-  brace-expansion@1.1.12:\n+  brace-expansion@1.1.11:\n     dependencies:\n       balanced-match: 1.0.2\n       concat-map: 0.0.1\n \n-  brace-expansion@2.0.2:\n+  brace-expansion@2.0.1:\n     dependencies:\n       balanced-match: 1.0.2\n \n@@ -11763,8 +11772,8 @@ snapshots:\n \n   browserslist@4.25.0:\n     dependencies:\n-      caniuse-lite: 1.0.30001723\n-      electron-to-chromium: 1.5.169\n+      caniuse-lite: 1.0.30001721\n+      electron-to-chromium: 1.5.165\n       node-releases: 2.0.19\n       update-browserslist-db: 1.1.3(browserslist@4.25.0)\n \n@@ -11826,7 +11835,7 @@ snapshots:\n \n   camelize@1.0.1: {}\n \n-  caniuse-lite@1.0.30001723: {}\n+  caniuse-lite@1.0.30001721: {}\n \n   case-sensitive-paths-webpack-plugin@2.4.0: {}\n \n@@ -11837,7 +11846,7 @@ snapshots:\n       assertion-error: 2.0.1\n       check-error: 2.1.1\n       deep-eql: 5.0.2\n-      loupe: 3.1.4\n+      loupe: 3.1.3\n       pathval: 2.0.0\n \n   chalk@2.4.2:\n@@ -12013,11 +12022,11 @@ snapshots:\n \n   cookie@1.0.2: {}\n \n-  core-js-compat@3.43.0:\n+  core-js-compat@3.42.0:\n     dependencies:\n       browserslist: 4.25.0\n \n-  core-js-pure@3.43.0: {}\n+  core-js-pure@3.42.0: {}\n \n   core-util-is@1.0.3: {}\n \n@@ -12098,7 +12107,7 @@ snapshots:\n \n   css-color-keywords@1.0.0: {}\n \n-  css-loader@6.11.0(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  css-loader@6.11.0(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n       icss-utils: 5.1.0(postcss@8.5.4)\n       postcss: 8.5.4\n@@ -12109,7 +12118,7 @@ snapshots:\n       postcss-value-parser: 4.2.0\n       semver: 7.7.2\n     optionalDependencies:\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n \n   css-select@4.3.0:\n     dependencies:\n@@ -12238,7 +12247,7 @@ snapshots:\n \n   decimal.js-light@2.5.1: {}\n \n-  decode-named-character-reference@1.2.0:\n+  decode-named-character-reference@1.1.0:\n     dependencies:\n       character-entities: 2.0.2\n \n@@ -12380,7 +12389,7 @@ snapshots:\n \n   eastasianwidth@0.2.0: {}\n \n-  electron-to-chromium@1.5.169: {}\n+  electron-to-chromium@1.5.165: {}\n \n   elliptic@6.6.1:\n     dependencies:\n@@ -12542,40 +12551,40 @@ snapshots:\n \n   es6-error@4.1.1: {}\n \n-  esbuild-register@3.6.0(esbuild@0.25.5):\n+  esbuild-register@3.6.0(esbuild@0.24.2):\n     dependencies:\n       debug: 4.4.1\n-      esbuild: 0.25.5\n+      esbuild: 0.24.2\n     transitivePeerDependencies:\n       - supports-color\n \n-  esbuild@0.25.5:\n+  esbuild@0.24.2:\n     optionalDependencies:\n-      '@esbuild/aix-ppc64': 0.25.5\n-      '@esbuild/android-arm': 0.25.5\n-      '@esbuild/android-arm64': 0.25.5\n-      '@esbuild/android-x64': 0.25.5\n-      '@esbuild/darwin-arm64': 0.25.5\n-      '@esbuild/darwin-x64': 0.25.5\n-      '@esbuild/freebsd-arm64': 0.25.5\n-      '@esbuild/freebsd-x64': 0.25.5\n-      '@esbuild/linux-arm': 0.25.5\n-      '@esbuild/linux-arm64': 0.25.5\n-      '@esbuild/linux-ia32': 0.25.5\n-      '@esbuild/linux-loong64': 0.25.5\n-      '@esbuild/linux-mips64el': 0.25.5\n-      '@esbuild/linux-ppc64': 0.25.5\n-      '@esbuild/linux-riscv64': 0.25.5\n-      '@esbuild/linux-s390x': 0.25.5\n-      '@esbuild/linux-x64': 0.25.5\n-      '@esbuild/netbsd-arm64': 0.25.5\n-      '@esbuild/netbsd-x64': 0.25.5\n-      '@esbuild/openbsd-arm64': 0.25.5\n-      '@esbuild/openbsd-x64': 0.25.5\n-      '@esbuild/sunos-x64': 0.25.5\n-      '@esbuild/win32-arm64': 0.25.5\n-      '@esbuild/win32-ia32': 0.25.5\n-      '@esbuild/win32-x64': 0.25.5\n+      '@esbuild/aix-ppc64': 0.24.2\n+      '@esbuild/android-arm': 0.24.2\n+      '@esbuild/android-arm64': 0.24.2\n+      '@esbuild/android-x64': 0.24.2\n+      '@esbuild/darwin-arm64': 0.24.2\n+      '@esbuild/darwin-x64': 0.24.2\n+      '@esbuild/freebsd-arm64': 0.24.2\n+      '@esbuild/freebsd-x64': 0.24.2\n+      '@esbuild/linux-arm': 0.24.2\n+      '@esbuild/linux-arm64': 0.24.2\n+      '@esbuild/linux-ia32': 0.24.2\n+      '@esbuild/linux-loong64': 0.24.2\n+      '@esbuild/linux-mips64el': 0.24.2\n+      '@esbuild/linux-ppc64': 0.24.2\n+      '@esbuild/linux-riscv64': 0.24.2\n+      '@esbuild/linux-s390x': 0.24.2\n+      '@esbuild/linux-x64': 0.24.2\n+      '@esbuild/netbsd-arm64': 0.24.2\n+      '@esbuild/netbsd-x64': 0.24.2\n+      '@esbuild/openbsd-arm64': 0.24.2\n+      '@esbuild/openbsd-x64': 0.24.2\n+      '@esbuild/sunos-x64': 0.24.2\n+      '@esbuild/win32-arm64': 0.24.2\n+      '@esbuild/win32-ia32': 0.24.2\n+      '@esbuild/win32-x64': 0.24.2\n \n   escalade@3.2.0: {}\n \n@@ -12589,12 +12598,12 @@ snapshots:\n     dependencies:\n       '@next/eslint-plugin-next': 15.3.3\n       '@rushstack/eslint-patch': 1.11.0\n-      '@typescript-eslint/eslint-plugin': 8.34.1(@typescript-eslint/parser@8.34.1(eslint@8.57.1)(typescript@5.8.3))(eslint@8.57.1)(typescript@5.8.3)\n-      '@typescript-eslint/parser': 8.34.1(eslint@8.57.1)(typescript@5.8.3)\n+      '@typescript-eslint/eslint-plugin': 8.33.1(@typescript-eslint/parser@8.33.1(eslint@8.57.1)(typescript@5.8.3))(eslint@8.57.1)(typescript@5.8.3)\n+      '@typescript-eslint/parser': 8.33.1(eslint@8.57.1)(typescript@5.8.3)\n       eslint: 8.57.1\n       eslint-import-resolver-node: 0.3.9\n       eslint-import-resolver-typescript: 3.10.1(eslint-plugin-import@2.31.0)(eslint@8.57.1)\n-      eslint-plugin-import: 2.31.0(@typescript-eslint/parser@8.34.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1)\n+      eslint-plugin-import: 2.31.0(@typescript-eslint/parser@8.33.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1)\n       eslint-plugin-jsx-a11y: 6.10.2(eslint@8.57.1)\n       eslint-plugin-react: 7.37.5(eslint@8.57.1)\n       eslint-plugin-react-hooks: 5.2.0(eslint@8.57.1)\n@@ -12622,24 +12631,24 @@ snapshots:\n       is-bun-module: 2.0.0\n       stable-hash: 0.0.5\n       tinyglobby: 0.2.14\n-      unrs-resolver: 1.9.0\n+      unrs-resolver: 1.7.10\n     optionalDependencies:\n-      eslint-plugin-import: 2.31.0(@typescript-eslint/parser@8.34.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1)\n+      eslint-plugin-import: 2.31.0(@typescript-eslint/parser@8.33.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1)\n     transitivePeerDependencies:\n       - supports-color\n \n-  eslint-module-utils@2.12.0(@typescript-eslint/parser@8.34.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-node@0.3.9)(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1):\n+  eslint-module-utils@2.12.0(@typescript-eslint/parser@8.33.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-node@0.3.9)(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1):\n     dependencies:\n       debug: 3.2.7\n     optionalDependencies:\n-      '@typescript-eslint/parser': 8.34.1(eslint@8.57.1)(typescript@5.8.3)\n+      '@typescript-eslint/parser': 8.33.1(eslint@8.57.1)(typescript@5.8.3)\n       eslint: 8.57.1\n       eslint-import-resolver-node: 0.3.9\n       eslint-import-resolver-typescript: 3.10.1(eslint-plugin-import@2.31.0)(eslint@8.57.1)\n     transitivePeerDependencies:\n       - supports-color\n \n-  eslint-plugin-import@2.31.0(@typescript-eslint/parser@8.34.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1):\n+  eslint-plugin-import@2.31.0(@typescript-eslint/parser@8.33.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1):\n     dependencies:\n       '@rtsao/scc': 1.1.0\n       array-includes: 3.1.9\n@@ -12650,7 +12659,7 @@ snapshots:\n       doctrine: 2.1.0\n       eslint: 8.57.1\n       eslint-import-resolver-node: 0.3.9\n-      eslint-module-utils: 2.12.0(@typescript-eslint/parser@8.34.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-node@0.3.9)(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1)\n+      eslint-module-utils: 2.12.0(@typescript-eslint/parser@8.33.1(eslint@8.57.1)(typescript@5.8.3))(eslint-import-resolver-node@0.3.9)(eslint-import-resolver-typescript@3.10.1)(eslint@8.57.1)\n       hasown: 2.0.2\n       is-core-module: 2.16.1\n       is-glob: 4.0.3\n@@ -12662,7 +12671,7 @@ snapshots:\n       string.prototype.trimend: 1.0.9\n       tsconfig-paths: 3.15.0\n     optionalDependencies:\n-      '@typescript-eslint/parser': 8.34.1(eslint@8.57.1)(typescript@5.8.3)\n+      '@typescript-eslint/parser': 8.33.1(eslint@8.57.1)(typescript@5.8.3)\n     transitivePeerDependencies:\n       - eslint-import-resolver-typescript\n       - eslint-import-resolver-webpack\n@@ -12716,7 +12725,7 @@ snapshots:\n   eslint-plugin-storybook@0.12.0(eslint@8.57.1)(typescript@5.8.3):\n     dependencies:\n       '@storybook/csf': 0.1.13\n-      '@typescript-eslint/utils': 8.34.1(eslint@8.57.1)(typescript@5.8.3)\n+      '@typescript-eslint/utils': 8.33.1(eslint@8.57.1)(typescript@5.8.3)\n       eslint: 8.57.1\n       ts-dedent: 2.2.0\n     transitivePeerDependencies:\n@@ -12735,7 +12744,7 @@ snapshots:\n \n   eslint-visitor-keys@3.4.3: {}\n \n-  eslint-visitor-keys@4.2.1: {}\n+  eslint-visitor-keys@4.2.0: {}\n \n   eslint@8.57.1:\n     dependencies:\n@@ -12782,8 +12791,8 @@ snapshots:\n \n   espree@9.6.1:\n     dependencies:\n-      acorn: 8.15.0\n-      acorn-jsx: 5.3.2(acorn@8.15.0)\n+      acorn: 8.14.1\n+      acorn-jsx: 5.3.2(acorn@8.14.1)\n       eslint-visitor-keys: 3.4.3\n \n   esprima@4.0.1: {}\n@@ -12806,7 +12815,7 @@ snapshots:\n \n   estree-walker@3.0.3:\n     dependencies:\n-      '@types/estree': 1.0.8\n+      '@types/estree': 1.0.7\n \n   esutils@2.0.3: {}\n \n@@ -12891,7 +12900,7 @@ snapshots:\n     dependencies:\n       bser: 2.1.1\n \n-  fdir@6.4.6(picomatch@4.0.2):\n+  fdir@6.4.5(picomatch@4.0.2):\n     optionalDependencies:\n       picomatch: 4.0.2\n \n@@ -12972,7 +12981,7 @@ snapshots:\n       cross-spawn: 7.0.6\n       signal-exit: 4.1.0\n \n-  fork-ts-checker-webpack-plugin@8.0.0(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  fork-ts-checker-webpack-plugin@8.0.0(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n       '@babel/code-frame': 7.27.1\n       chalk: 4.1.2\n@@ -12987,22 +12996,21 @@ snapshots:\n       semver: 7.7.2\n       tapable: 2.2.2\n       typescript: 5.8.3\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n \n-  form-data@4.0.3:\n+  form-data@4.0.2:\n     dependencies:\n       asynckit: 0.4.0\n       combined-stream: 1.0.8\n       es-set-tostringtag: 2.1.0\n-      hasown: 2.0.2\n       mime-types: 2.1.35\n \n   forwarded-parse@2.1.2: {}\n \n   framer-motion@12.16.0(@emotion/is-prop-valid@1.2.2)(react-dom@18.3.1(react@18.3.1))(react@18.3.1):\n     dependencies:\n-      motion-dom: 12.18.1\n-      motion-utils: 12.18.1\n+      motion-dom: 12.16.0\n+      motion-utils: 12.12.1\n       tslib: 2.8.1\n     optionalDependencies:\n       '@emotion/is-prop-valid': 1.2.2\n@@ -13191,7 +13199,7 @@ snapshots:\n \n   hast-util-to-jsx-runtime@2.3.6:\n     dependencies:\n-      '@types/estree': 1.0.8\n+      '@types/estree': 1.0.7\n       '@types/hast': 3.0.4\n       '@types/unist': 3.0.3\n       comma-separated-tokens: 2.0.3\n@@ -13203,7 +13211,7 @@ snapshots:\n       mdast-util-mdxjs-esm: 2.0.1\n       property-information: 7.1.0\n       space-separated-tokens: 2.0.2\n-      style-to-js: 1.1.17\n+      style-to-js: 1.1.16\n       unist-util-position: 5.0.0\n       vfile-message: 4.0.2\n     transitivePeerDependencies:\n@@ -13243,11 +13251,11 @@ snapshots:\n       he: 1.2.0\n       param-case: 3.0.4\n       relateurl: 0.2.7\n-      terser: 5.42.0\n+      terser: 5.41.0\n \n   html-url-attributes@3.0.1: {}\n \n-  html-webpack-plugin@5.6.3(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  html-webpack-plugin@5.6.3(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n       '@types/html-minifier-terser': 6.1.0\n       html-minifier-terser: 6.1.0\n@@ -13255,7 +13263,7 @@ snapshots:\n       pretty-error: 4.0.0\n       tapable: 2.2.2\n     optionalDependencies:\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n \n   htmlparser2@3.10.1:\n     dependencies:\n@@ -13305,8 +13313,8 @@ snapshots:\n \n   import-in-the-middle@1.14.0:\n     dependencies:\n-      acorn: 8.15.0\n-      acorn-import-attributes: 1.9.5(acorn@8.15.0)\n+      acorn: 8.14.1\n+      acorn-import-attributes: 1.9.5(acorn@8.14.1)\n       cjs-module-lexer: 1.4.3\n       module-details-from-path: 1.0.4\n \n@@ -13454,7 +13462,7 @@ snapshots:\n \n   is-reference@1.2.1:\n     dependencies:\n-      '@types/estree': 1.0.8\n+      '@types/estree': 1.0.7\n \n   is-regex@1.2.1:\n     dependencies:\n@@ -13766,7 +13774,7 @@ snapshots:\n       jest-process-manager: 0.4.0\n       jest-runner: 29.7.0\n       nyc: 15.1.0\n-      playwright-core: 1.53.0\n+      playwright-core: 1.52.0\n       rimraf: 3.0.2\n       uuid: 8.3.2\n     transitivePeerDependencies:\n@@ -14107,7 +14115,7 @@ snapshots:\n     dependencies:\n       js-tokens: 4.0.0\n \n-  loupe@3.1.4: {}\n+  loupe@3.1.3: {}\n \n   lower-case@2.0.2:\n     dependencies:\n@@ -14159,7 +14167,7 @@ snapshots:\n     dependencies:\n       '@types/mdast': 4.0.4\n       '@types/unist': 3.0.3\n-      decode-named-character-reference: 1.2.0\n+      decode-named-character-reference: 1.1.0\n       devlop: 1.1.0\n       mdast-util-to-string: 4.0.0\n       micromark: 4.0.2\n@@ -14258,7 +14266,7 @@ snapshots:\n \n   micromark-core-commonmark@2.0.3:\n     dependencies:\n-      decode-named-character-reference: 1.2.0\n+      decode-named-character-reference: 1.1.0\n       devlop: 1.1.0\n       micromark-factory-destination: 2.0.1\n       micromark-factory-label: 2.0.1\n@@ -14333,7 +14341,7 @@ snapshots:\n \n   micromark-util-decode-string@2.0.1:\n     dependencies:\n-      decode-named-character-reference: 1.2.0\n+      decode-named-character-reference: 1.1.0\n       micromark-util-character: 2.1.1\n       micromark-util-decode-numeric-character-reference: 2.0.2\n       micromark-util-symbol: 2.0.1\n@@ -14371,7 +14379,7 @@ snapshots:\n     dependencies:\n       '@types/debug': 4.1.12\n       debug: 4.4.1\n-      decode-named-character-reference: 1.2.0\n+      decode-named-character-reference: 1.1.0\n       devlop: 1.1.0\n       micromark-core-commonmark: 2.0.3\n       micromark-factory-space: 2.0.1\n@@ -14415,15 +14423,15 @@ snapshots:\n \n   minimatch@3.1.2:\n     dependencies:\n-      brace-expansion: 1.1.12\n+      brace-expansion: 1.1.11\n \n   minimatch@8.0.4:\n     dependencies:\n-      brace-expansion: 2.0.2\n+      brace-expansion: 2.0.1\n \n   minimatch@9.0.5:\n     dependencies:\n-      brace-expansion: 2.0.2\n+      brace-expansion: 2.0.1\n \n   minimist@1.2.8: {}\n \n@@ -14437,11 +14445,11 @@ snapshots:\n \n   moment@2.30.1: {}\n \n-  motion-dom@12.18.1:\n+  motion-dom@12.16.0:\n     dependencies:\n-      motion-utils: 12.18.1\n+      motion-utils: 12.12.1\n \n-  motion-utils@12.18.1: {}\n+  motion-utils@12.12.1: {}\n \n   ms@2.1.3: {}\n \n@@ -14460,7 +14468,7 @@ snapshots:\n       '@open-draft/deferred-promise': 2.2.0\n       '@open-draft/until': 2.1.0\n       '@types/cookie': 0.6.0\n-      '@types/statuses': 2.0.6\n+      '@types/statuses': 2.0.5\n       graphql: 16.11.0\n       headers-polyfill: 4.0.3\n       is-node-process: 1.2.0\n@@ -14504,7 +14512,7 @@ snapshots:\n       '@swc/counter': 0.1.3\n       '@swc/helpers': 0.5.15\n       busboy: 1.6.0\n-      caniuse-lite: 1.0.30001723\n+      caniuse-lite: 1.0.30001721\n       postcss: 8.4.31\n       react: 18.3.1\n       react-dom: 18.3.1(react@18.3.1)\n@@ -14538,7 +14546,7 @@ snapshots:\n \n   node-int64@0.4.0: {}\n \n-  node-polyfill-webpack-plugin@2.0.1(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  node-polyfill-webpack-plugin@2.0.1(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n       assert: 2.1.0\n       browserify-zlib: 0.2.0\n@@ -14565,7 +14573,7 @@ snapshots:\n       url: 0.11.4\n       util: 0.12.5\n       vm-browserify: 1.1.2\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n \n   node-preload@0.2.1:\n     dependencies:\n@@ -14765,7 +14773,7 @@ snapshots:\n       '@types/unist': 2.0.11\n       character-entities-legacy: 3.0.0\n       character-reference-invalid: 2.0.1\n-      decode-named-character-reference: 1.2.0\n+      decode-named-character-reference: 1.1.0\n       is-alphanumerical: 2.0.1\n       is-decimal: 2.0.1\n       is-hexadecimal: 2.0.1\n@@ -14849,20 +14857,12 @@ snapshots:\n \n   playwright-core@1.52.0: {}\n \n-  playwright-core@1.53.0: {}\n-\n   playwright@1.52.0:\n     dependencies:\n       playwright-core: 1.52.0\n     optionalDependencies:\n       fsevents: 2.3.2\n \n-  playwright@1.53.0:\n-    dependencies:\n-      playwright-core: 1.53.0\n-    optionalDependencies:\n-      fsevents: 2.3.2\n-\n   pnp-webpack-plugin@1.7.0(typescript@5.8.3):\n     dependencies:\n       ts-pnp: 1.2.0(typescript@5.8.3)\n@@ -14894,14 +14894,14 @@ snapshots:\n     optionalDependencies:\n       postcss: 8.5.4\n \n-  postcss-loader@8.1.1(postcss@8.5.4)(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  postcss-loader@8.1.1(postcss@8.5.4)(typescript@5.8.3)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n       cosmiconfig: 9.0.0(typescript@5.8.3)\n       jiti: 1.21.7\n       postcss: 8.5.4\n       semver: 7.7.2\n     optionalDependencies:\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n     transitivePeerDependencies:\n       - typescript\n \n@@ -14996,6 +14996,12 @@ snapshots:\n       ansi-styles: 5.2.0\n       react-is: 18.3.1\n \n+  prism-react-renderer@2.4.1(react@18.3.1):\n+    dependencies:\n+      '@types/prismjs': 1.26.5\n+      clsx: 2.1.1\n+      react: 18.3.1\n+\n   process-nextick-args@2.0.1: {}\n \n   process-on-spawn@1.1.0:\n@@ -15077,7 +15083,7 @@ snapshots:\n       date-fns-jalali: 4.1.0-0\n       react: 18.3.1\n \n-  react-docgen-typescript@2.4.0(typescript@5.8.3):\n+  react-docgen-typescript@2.2.2(typescript@5.8.3):\n     dependencies:\n       typescript: 5.8.3\n \n@@ -15107,7 +15113,7 @@ snapshots:\n       prop-types: 15.8.1\n       react: 18.3.1\n       react-dom: 18.3.1(react@18.3.1)\n-      styled-components: 6.1.19(react-dom@18.3.1(react@18.3.1))(react@18.3.1)\n+      styled-components: 6.1.18(react-dom@18.3.1(react@18.3.1))(react@18.3.1)\n \n   react-hook-form@7.57.0(react@18.3.1):\n     dependencies:\n@@ -15195,6 +15201,10 @@ snapshots:\n     optionalDependencies:\n       '@types/react': 18.3.17\n \n+  react-timeago@8.2.0(react@18.3.1):\n+    dependencies:\n+      react: 18.3.1\n+\n   react-transition-group@4.4.5(react-dom@18.3.1(react@18.3.1))(react@18.3.1):\n     dependencies:\n       '@babel/runtime': 7.27.6\n@@ -15470,11 +15480,11 @@ snapshots:\n       es-errors: 1.3.0\n       is-regex: 1.2.1\n \n-  sass-loader@14.2.1(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  sass-loader@14.2.1(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n       neo-async: 2.6.2\n     optionalDependencies:\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n \n   scheduler@0.23.2:\n     dependencies:\n@@ -15699,7 +15709,7 @@ snapshots:\n     dependencies:\n       type-fest: 0.7.1\n \n-  statuses@2.0.2: {}\n+  statuses@2.0.1: {}\n \n   stop-iteration-iterator@1.1.0:\n     dependencies:\n@@ -15841,19 +15851,19 @@ snapshots:\n \n   strip-json-comments@3.1.1: {}\n \n-  style-loader@3.3.4(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  style-loader@3.3.4(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n \n-  style-to-js@1.1.17:\n+  style-to-js@1.1.16:\n     dependencies:\n-      style-to-object: 1.0.9\n+      style-to-object: 1.0.8\n \n-  style-to-object@1.0.9:\n+  style-to-object@1.0.8:\n     dependencies:\n       inline-style-parser: 0.2.4\n \n-  styled-components@6.1.19(react-dom@18.3.1(react@18.3.1))(react@18.3.1):\n+  styled-components@6.1.18(react-dom@18.3.1(react@18.3.1))(react@18.3.1):\n     dependencies:\n       '@emotion/is-prop-valid': 1.2.2\n       '@emotion/unitless': 0.8.1\n@@ -15909,6 +15919,17 @@ snapshots:\n \n   tailwind-merge@2.6.0: {}\n \n+  tailwind-scrollbar-hide@2.0.0(tailwindcss@3.4.17):\n+    dependencies:\n+      tailwindcss: 3.4.17\n+\n+  tailwind-scrollbar@4.0.2(react@18.3.1)(tailwindcss@3.4.17):\n+    dependencies:\n+      prism-react-renderer: 2.4.1(react@18.3.1)\n+      tailwindcss: 3.4.17\n+    transitivePeerDependencies:\n+      - react\n+\n   tailwindcss-animate@1.0.7(tailwindcss@3.4.17):\n     dependencies:\n       tailwindcss: 3.4.17\n@@ -15942,22 +15963,22 @@ snapshots:\n \n   tapable@2.2.2: {}\n \n-  terser-webpack-plugin@5.3.14(@swc/core@1.12.1)(esbuild@0.25.5)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  terser-webpack-plugin@5.3.14(@swc/core@1.11.31)(esbuild@0.24.2)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n       '@jridgewell/trace-mapping': 0.3.25\n       jest-worker: 27.5.1\n       schema-utils: 4.3.2\n       serialize-javascript: 6.0.2\n-      terser: 5.42.0\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      terser: 5.41.0\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n     optionalDependencies:\n-      '@swc/core': 1.12.1\n-      esbuild: 0.25.5\n+      '@swc/core': 1.11.31\n+      esbuild: 0.24.2\n \n-  terser@5.42.0:\n+  terser@5.41.0:\n     dependencies:\n       '@jridgewell/source-map': 0.3.6\n-      acorn: 8.15.0\n+      acorn: 8.14.1\n       commander: 2.20.3\n       source-map-support: 0.5.21\n \n@@ -15987,7 +16008,7 @@ snapshots:\n \n   tinyglobby@0.2.14:\n     dependencies:\n-      fdir: 6.4.6(picomatch@4.0.2)\n+      fdir: 6.4.5(picomatch@4.0.2)\n       picomatch: 4.0.2\n \n   tinyrainbow@1.2.0: {}\n@@ -16171,39 +16192,37 @@ snapshots:\n \n   unplugin@1.0.1:\n     dependencies:\n-      acorn: 8.15.0\n+      acorn: 8.14.1\n       chokidar: 3.6.0\n       webpack-sources: 3.3.2\n       webpack-virtual-modules: 0.5.0\n \n   unplugin@1.16.1:\n     dependencies:\n-      acorn: 8.15.0\n+      acorn: 8.14.1\n       webpack-virtual-modules: 0.6.2\n \n-  unrs-resolver@1.9.0:\n+  unrs-resolver@1.7.10:\n     dependencies:\n       napi-postinstall: 0.2.4\n     optionalDependencies:\n-      '@unrs/resolver-binding-android-arm-eabi': 1.9.0\n-      '@unrs/resolver-binding-android-arm64': 1.9.0\n-      '@unrs/resolver-binding-darwin-arm64': 1.9.0\n-      '@unrs/resolver-binding-darwin-x64': 1.9.0\n-      '@unrs/resolver-binding-freebsd-x64': 1.9.0\n-      '@unrs/resolver-binding-linux-arm-gnueabihf': 1.9.0\n-      '@unrs/resolver-binding-linux-arm-musleabihf': 1.9.0\n-      '@unrs/resolver-binding-linux-arm64-gnu': 1.9.0\n-      '@unrs/resolver-binding-linux-arm64-musl': 1.9.0\n-      '@unrs/resolver-binding-linux-ppc64-gnu': 1.9.0\n-      '@unrs/resolver-binding-linux-riscv64-gnu': 1.9.0\n-      '@unrs/resolver-binding-linux-riscv64-musl': 1.9.0\n-      '@unrs/resolver-binding-linux-s390x-gnu': 1.9.0\n-      '@unrs/resolver-binding-linux-x64-gnu': 1.9.0\n-      '@unrs/resolver-binding-linux-x64-musl': 1.9.0\n-      '@unrs/resolver-binding-wasm32-wasi': 1.9.0\n-      '@unrs/resolver-binding-win32-arm64-msvc': 1.9.0\n-      '@unrs/resolver-binding-win32-ia32-msvc': 1.9.0\n-      '@unrs/resolver-binding-win32-x64-msvc': 1.9.0\n+      '@unrs/resolver-binding-darwin-arm64': 1.7.10\n+      '@unrs/resolver-binding-darwin-x64': 1.7.10\n+      '@unrs/resolver-binding-freebsd-x64': 1.7.10\n+      '@unrs/resolver-binding-linux-arm-gnueabihf': 1.7.10\n+      '@unrs/resolver-binding-linux-arm-musleabihf': 1.7.10\n+      '@unrs/resolver-binding-linux-arm64-gnu': 1.7.10\n+      '@unrs/resolver-binding-linux-arm64-musl': 1.7.10\n+      '@unrs/resolver-binding-linux-ppc64-gnu': 1.7.10\n+      '@unrs/resolver-binding-linux-riscv64-gnu': 1.7.10\n+      '@unrs/resolver-binding-linux-riscv64-musl': 1.7.10\n+      '@unrs/resolver-binding-linux-s390x-gnu': 1.7.10\n+      '@unrs/resolver-binding-linux-x64-gnu': 1.7.10\n+      '@unrs/resolver-binding-linux-x64-musl': 1.7.10\n+      '@unrs/resolver-binding-wasm32-wasi': 1.7.10\n+      '@unrs/resolver-binding-win32-arm64-msvc': 1.7.10\n+      '@unrs/resolver-binding-win32-ia32-msvc': 1.7.10\n+      '@unrs/resolver-binding-win32-x64-msvc': 1.7.10\n \n   update-browserslist-db@1.1.3(browserslist@4.25.0):\n     dependencies:\n@@ -16299,7 +16318,7 @@ snapshots:\n \n   wait-on@7.2.0:\n     dependencies:\n-      axios: 1.10.0\n+      axios: 1.9.0\n       joi: 17.13.3\n       lodash: 4.17.21\n       minimist: 1.2.8\n@@ -16330,7 +16349,7 @@ snapshots:\n \n   webidl-conversions@3.0.1: {}\n \n-  webpack-dev-middleware@6.1.3(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)):\n+  webpack-dev-middleware@6.1.3(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)):\n     dependencies:\n       colorette: 2.0.20\n       memfs: 3.5.3\n@@ -16338,7 +16357,7 @@ snapshots:\n       range-parser: 1.2.1\n       schema-utils: 4.3.2\n     optionalDependencies:\n-      webpack: 5.99.9(@swc/core@1.12.1)(esbuild@0.25.5)\n+      webpack: 5.99.9(@swc/core@1.11.31)(esbuild@0.24.2)\n \n   webpack-hot-middleware@2.26.1:\n     dependencies:\n@@ -16352,15 +16371,15 @@ snapshots:\n \n   webpack-virtual-modules@0.6.2: {}\n \n-  webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5):\n+  webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2):\n     dependencies:\n       '@types/eslint-scope': 3.7.7\n-      '@types/estree': 1.0.8\n+      '@types/estree': 1.0.7\n       '@types/json-schema': 7.0.15\n       '@webassemblyjs/ast': 1.14.1\n       '@webassemblyjs/wasm-edit': 1.14.1\n       '@webassemblyjs/wasm-parser': 1.14.1\n-      acorn: 8.15.0\n+      acorn: 8.14.1\n       browserslist: 4.25.0\n       chrome-trace-event: 1.0.4\n       enhanced-resolve: 5.18.1\n@@ -16375,7 +16394,7 @@ snapshots:\n       neo-async: 2.6.2\n       schema-utils: 4.3.2\n       tapable: 2.2.2\n-      terser-webpack-plugin: 5.3.14(@swc/core@1.12.1)(esbuild@0.25.5)(webpack@5.99.9(@swc/core@1.12.1)(esbuild@0.25.5))\n+      terser-webpack-plugin: 5.3.14(@swc/core@1.11.31)(esbuild@0.24.2)(webpack@5.99.9(@swc/core@1.11.31)(esbuild@0.24.2))\n       watchpack: 2.4.4\n       webpack-sources: 3.3.2\n     transitivePeerDependencies:\ndiff --git a/autogpt_platform/frontend/public/integrations/anthropic.png b/autogpt_platform/frontend/public/integrations/anthropic.png\nnew file mode 100644\nindex 000000000000..23cc3ddb9d83\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/anthropic.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/apollo.png b/autogpt_platform/frontend/public/integrations/apollo.png\nnew file mode 100644\nindex 000000000000..cb3750356b32\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/apollo.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/d-id.png b/autogpt_platform/frontend/public/integrations/d-id.png\nindex 17ebf2ddb478..977cd679ceac 100644\nBinary files a/autogpt_platform/frontend/public/integrations/d-id.png and b/autogpt_platform/frontend/public/integrations/d-id.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/d_id.png b/autogpt_platform/frontend/public/integrations/d_id.png\nnew file mode 100644\nindex 000000000000..5a41621838ad\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/d_id.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/discord.png b/autogpt_platform/frontend/public/integrations/discord.png\nindex 4e4ed4662d91..e440b11672b8 100644\nBinary files a/autogpt_platform/frontend/public/integrations/discord.png and b/autogpt_platform/frontend/public/integrations/discord.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/e2b.png b/autogpt_platform/frontend/public/integrations/e2b.png\nnew file mode 100644\nindex 000000000000..986e7cc60ef7\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/e2b.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/exa.png b/autogpt_platform/frontend/public/integrations/exa.png\nnew file mode 100644\nindex 000000000000..35b25b48f827\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/exa.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/fal.png b/autogpt_platform/frontend/public/integrations/fal.png\nnew file mode 100644\nindex 000000000000..95ae1079e574\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/fal.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/github.png b/autogpt_platform/frontend/public/integrations/github.png\nindex 5e0778732890..1b4e59693628 100644\nBinary files a/autogpt_platform/frontend/public/integrations/github.png and b/autogpt_platform/frontend/public/integrations/github.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/google.png b/autogpt_platform/frontend/public/integrations/google.png\nindex 49cc06b98077..5b8b6e658ce3 100644\nBinary files a/autogpt_platform/frontend/public/integrations/google.png and b/autogpt_platform/frontend/public/integrations/google.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/google_maps.png b/autogpt_platform/frontend/public/integrations/google_maps.png\nnew file mode 100644\nindex 000000000000..928c5a558527\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/google_maps.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/groq.png b/autogpt_platform/frontend/public/integrations/groq.png\nnew file mode 100644\nindex 000000000000..4953c67a3547\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/groq.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/hubspot.png b/autogpt_platform/frontend/public/integrations/hubspot.png\nindex de4fb1dbe998..5f25ce0a3ddd 100644\nBinary files a/autogpt_platform/frontend/public/integrations/hubspot.png and b/autogpt_platform/frontend/public/integrations/hubspot.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/ideogram.png b/autogpt_platform/frontend/public/integrations/ideogram.png\nnew file mode 100644\nindex 000000000000..049cee4b28c7\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/ideogram.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/jina.png b/autogpt_platform/frontend/public/integrations/jina.png\nnew file mode 100644\nindex 000000000000..aec95773298e\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/jina.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/linear.png b/autogpt_platform/frontend/public/integrations/linear.png\nindex 5e3a0ccfe1c5..fe64191e6d98 100644\nBinary files a/autogpt_platform/frontend/public/integrations/linear.png and b/autogpt_platform/frontend/public/integrations/linear.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/llama_api.png b/autogpt_platform/frontend/public/integrations/llama_api.png\nnew file mode 100644\nindex 000000000000..5d48d62c1919\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/llama_api.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/maps.png b/autogpt_platform/frontend/public/integrations/maps.png\nindex 70a707dcd986..8bc9fddcb747 100644\nBinary files a/autogpt_platform/frontend/public/integrations/maps.png and b/autogpt_platform/frontend/public/integrations/maps.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/medium.png b/autogpt_platform/frontend/public/integrations/medium.png\nindex 41228fcdce3f..c0eec340d51c 100644\nBinary files a/autogpt_platform/frontend/public/integrations/medium.png and b/autogpt_platform/frontend/public/integrations/medium.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/mem0.png b/autogpt_platform/frontend/public/integrations/mem0.png\nindex 953ae47f3def..21ba2452c501 100644\nBinary files a/autogpt_platform/frontend/public/integrations/mem0.png and b/autogpt_platform/frontend/public/integrations/mem0.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/notion.png b/autogpt_platform/frontend/public/integrations/notion.png\nindex 9b95380a9380..a1e535fe0cb9 100644\nBinary files a/autogpt_platform/frontend/public/integrations/notion.png and b/autogpt_platform/frontend/public/integrations/notion.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/nvidia.jpg b/autogpt_platform/frontend/public/integrations/nvidia.jpg\nindex 46956b6de3fa..77513386f4c7 100644\nBinary files a/autogpt_platform/frontend/public/integrations/nvidia.jpg and b/autogpt_platform/frontend/public/integrations/nvidia.jpg differ\ndiff --git a/autogpt_platform/frontend/public/integrations/nvidia.png b/autogpt_platform/frontend/public/integrations/nvidia.png\nnew file mode 100644\nindex 000000000000..c5bf936fd304\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/nvidia.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/ollama.png b/autogpt_platform/frontend/public/integrations/ollama.png\nnew file mode 100644\nindex 000000000000..7c5f003b495b\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/ollama.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/open_router.png b/autogpt_platform/frontend/public/integrations/open_router.png\nnew file mode 100644\nindex 000000000000..62fe66752836\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/open_router.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/openai.png b/autogpt_platform/frontend/public/integrations/openai.png\nnew file mode 100644\nindex 000000000000..6f16d37ea07d\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/openai.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/openweathermap.png b/autogpt_platform/frontend/public/integrations/openweathermap.png\nindex ae4e47b8bfa9..80ae46e053e8 100644\nBinary files a/autogpt_platform/frontend/public/integrations/openweathermap.png and b/autogpt_platform/frontend/public/integrations/openweathermap.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/pinecone.png b/autogpt_platform/frontend/public/integrations/pinecone.png\nindex ec25827f7aa7..1378c6522270 100644\nBinary files a/autogpt_platform/frontend/public/integrations/pinecone.png and b/autogpt_platform/frontend/public/integrations/pinecone.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/reddit.png b/autogpt_platform/frontend/public/integrations/reddit.png\nindex 6f5777a78963..976a157866e0 100644\nBinary files a/autogpt_platform/frontend/public/integrations/reddit.png and b/autogpt_platform/frontend/public/integrations/reddit.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/replicate.png b/autogpt_platform/frontend/public/integrations/replicate.png\nnew file mode 100644\nindex 000000000000..8e70d93c8a4f\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/replicate.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/revid.png b/autogpt_platform/frontend/public/integrations/revid.png\nnew file mode 100644\nindex 000000000000..a49151b22fa1\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/revid.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/screenshotone.png b/autogpt_platform/frontend/public/integrations/screenshotone.png\nnew file mode 100644\nindex 000000000000..c3c1f9930727\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/screenshotone.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/slant3d.jpeg b/autogpt_platform/frontend/public/integrations/slant3d.jpeg\nindex 8071a6a0b90f..43b3ef88d0a8 100644\nBinary files a/autogpt_platform/frontend/public/integrations/slant3d.jpeg and b/autogpt_platform/frontend/public/integrations/slant3d.jpeg differ\ndiff --git a/autogpt_platform/frontend/public/integrations/slant3d.png b/autogpt_platform/frontend/public/integrations/slant3d.png\nnew file mode 100644\nindex 000000000000..958a30d4f4be\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/slant3d.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/smartlead.png b/autogpt_platform/frontend/public/integrations/smartlead.png\nnew file mode 100644\nindex 000000000000..30ea5d5b6f78\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/smartlead.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/smtp.png b/autogpt_platform/frontend/public/integrations/smtp.png\nindex 1e7c5dd4f045..eea99b046ef4 100644\nBinary files a/autogpt_platform/frontend/public/integrations/smtp.png and b/autogpt_platform/frontend/public/integrations/smtp.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/todoist.png b/autogpt_platform/frontend/public/integrations/todoist.png\nindex 70cc564a11b3..40aa9480d12e 100644\nBinary files a/autogpt_platform/frontend/public/integrations/todoist.png and b/autogpt_platform/frontend/public/integrations/todoist.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/twitter.png b/autogpt_platform/frontend/public/integrations/twitter.png\nnew file mode 100644\nindex 000000000000..8262c4b9cc06\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/twitter.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/unreal-speech.png b/autogpt_platform/frontend/public/integrations/unreal-speech.png\ndeleted file mode 100644\nindex 1baaf64f922f..000000000000\nBinary files a/autogpt_platform/frontend/public/integrations/unreal-speech.png and /dev/null differ\ndiff --git a/autogpt_platform/frontend/public/integrations/unreal_speech.png b/autogpt_platform/frontend/public/integrations/unreal_speech.png\nnew file mode 100644\nindex 000000000000..1446eac432ab\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/unreal_speech.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/x.png b/autogpt_platform/frontend/public/integrations/x.png\nindex 25bbe2c0585d..8262c4b9cc06 100644\nBinary files a/autogpt_platform/frontend/public/integrations/x.png and b/autogpt_platform/frontend/public/integrations/x.png differ\ndiff --git a/autogpt_platform/frontend/public/integrations/zerobounce.png b/autogpt_platform/frontend/public/integrations/zerobounce.png\nnew file mode 100644\nindex 000000000000..78168a853a27\nBinary files /dev/null and b/autogpt_platform/frontend/public/integrations/zerobounce.png differ\ndiff --git a/autogpt_platform/frontend/src/components/Flow.tsx b/autogpt_platform/frontend/src/components/Flow.tsx\nindex 345fc60b5ae8..af5469061c01 100644\n--- a/autogpt_platform/frontend/src/components/Flow.tsx\n+++ b/autogpt_platform/frontend/src/components/Flow.tsx\n@@ -28,6 +28,7 @@ import \"@xyflow/react/dist/style.css\";\n import { CustomNode } from \"./CustomNode\";\n import \"./flow.css\";\n import {\n+  Block,\n   BlockUIType,\n   formatEdgeID,\n   GraphExecutionID,\n@@ -39,7 +40,6 @@ import { CustomEdge } from \"./CustomEdge\";\n import ConnectionLine from \"./ConnectionLine\";\n import { Control, ControlPanel } from \"@/components/edit/control/ControlPanel\";\n import { SaveControl } from \"@/components/edit/control/SaveControl\";\n-import { BlocksControl } from \"@/components/edit/control/BlocksControl\";\n import { IconUndo2, IconRedo2 } from \"@/components/ui/icons\";\n import { startTutorial } from \"./tutorial\";\n import useAgentGraph from \"@/hooks/useAgentGraph\";\n@@ -53,6 +53,7 @@ import OttoChatWidget from \"@/components/OttoChatWidget\";\n import { useToast } from \"@/components/ui/use-toast\";\n import { useCopyPaste } from \"../hooks/useCopyPaste\";\n import { CronScheduler } from \"./cronScheduler\";\n+import { BlockMenu } from \"./builder/block-menu/BlockMenu\";\n \n // This is for the history, this is the minimum distance a block must move before it is logged\n // It helps to prevent spamming the history with small movements especially when pressing on a input in a block\n@@ -101,7 +102,6 @@ const FlowEditor: React.FC<{\n     setAgentDescription,\n     savedAgent,\n     availableNodes,\n-    availableFlows,\n     getOutputType,\n     requestSave,\n     requestSaveAndRun,\n@@ -136,6 +136,10 @@ const FlowEditor: React.FC<{\n   // State to control if save popover should be pinned open\n   const [pinSavePopover, setPinSavePopover] = useState(false);\n \n+  const [blockMenuSelected, setBlockMenuSelected] = useState<\n+    \"save\" | \"block\" | \"\"\n+  >(\"\");\n+\n   const runnerUIRef = useRef<RunnerUIWrapperRef>(null);\n \n   const [openCron, setOpenCron] = useState(false);\n@@ -466,13 +470,7 @@ const FlowEditor: React.FC<{\n   }, [nodes, setViewport, x, y]);\n \n   const addNode = useCallback(\n-    (blockId: string, nodeType: string, hardcodedValues: any = {}) => {\n-      const nodeSchema = availableNodes.find((node) => node.id === blockId);\n-      if (!nodeSchema) {\n-        console.error(`Schema not found for block ID: ${blockId}`);\n-        return;\n-      }\n-\n+    (block: Block) => {\n       /*\n        Calculate a position to the right of the newly added block, allowing for some margin.\n        If adding to the right side causes the new block to collide with an existing block, attempt to place it at the bottom or left.\n@@ -489,7 +487,7 @@ const FlowEditor: React.FC<{\n           ? // we will get all the dimension of nodes, then store\n             findNewlyAddedBlockCoordinates(\n               nodeDimensions,\n-              nodeSchema.uiType == BlockUIType.NOTE ? 300 : 500,\n+              block.uiType == BlockUIType.NOTE ? 300 : 500,\n               60,\n               1.0,\n             )\n@@ -504,19 +502,19 @@ const FlowEditor: React.FC<{\n         type: \"custom\",\n         position: viewportCoordinates, // Set the position to the calculated viewport center\n         data: {\n-          blockType: nodeType,\n-          blockCosts: nodeSchema.costs,\n-          title: `${nodeType} ${nodeId}`,\n-          description: nodeSchema.description,\n-          categories: nodeSchema.categories,\n-          inputSchema: nodeSchema.inputSchema,\n-          outputSchema: nodeSchema.outputSchema,\n-          hardcodedValues: hardcodedValues,\n+          blockType: block.name,\n+          blockCosts: block.costs,\n+          title: `${block.name} ${nodeId}`,\n+          description: block.description,\n+          categories: block.categories,\n+          inputSchema: block.inputSchema,\n+          outputSchema: block.outputSchema,\n+          hardcodedValues: block.hardcodedValues || {},\n           connections: [],\n           isOutputOpen: false,\n-          block_id: blockId,\n-          isOutputStatic: nodeSchema.staticOutput,\n-          uiType: nodeSchema.uiType,\n+          block_id: block.id,\n+          isOutputStatic: block.staticOutput,\n+          uiType: block.uiType,\n         },\n       };\n \n@@ -545,7 +543,6 @@ const FlowEditor: React.FC<{\n     [\n       nodeId,\n       setViewport,\n-      availableNodes,\n       addNodes,\n       nodeDimensions,\n       deleteElements,\n@@ -627,12 +624,12 @@ const FlowEditor: React.FC<{\n   const editorControls: Control[] = [\n     {\n       label: \"Undo\",\n-      icon: <IconUndo2 />,\n+      icon: <IconUndo2 className=\"h-5 w-5\" strokeWidth={2} />,\n       onClick: handleUndo,\n     },\n     {\n       label: \"Redo\",\n-      icon: <IconRedo2 />,\n+      icon: <IconRedo2 className=\"h-5 w-5\" strokeWidth={2} />,\n       onClick: handleRedo,\n     },\n   ];\n@@ -680,15 +677,13 @@ const FlowEditor: React.FC<{\n           <Controls />\n           <Background className=\"dark:bg-slate-800\" />\n           <ControlPanel\n-            className=\"absolute z-20\"\n             controls={editorControls}\n             topChildren={\n-              <BlocksControl\n-                pinBlocksPopover={pinBlocksPopover} // Pass the state to BlocksControl\n-                blocks={availableNodes}\n-                addBlock={addNode}\n-                flows={availableFlows}\n-                nodes={nodes}\n+              <BlockMenu\n+                pinBlocksPopover={pinBlocksPopover}\n+                addNode={addNode}\n+                blockMenuSelected={blockMenuSelected}\n+                setBlockMenuSelected={setBlockMenuSelected}\n               />\n             }\n             botChildren={\n@@ -701,6 +696,8 @@ const FlowEditor: React.FC<{\n                 agentName={agentName}\n                 onNameChange={setAgentName}\n                 pinSavePopover={pinSavePopover}\n+                blockMenuSelected={blockMenuSelected}\n+                setBlockMenuSelected={setBlockMenuSelected}\n               />\n             }\n           ></ControlPanel>\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/Block.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/Block.tsx\nnew file mode 100644\nindex 000000000000..4b4c04f858a5\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/Block.tsx\n@@ -0,0 +1,77 @@\n+import { Button } from \"@/components/ui/button\";\n+import { Skeleton } from \"@/components/ui/skeleton\";\n+import { beautifyString, cn } from \"@/lib/utils\";\n+import { Plus } from \"lucide-react\";\n+import React, { ButtonHTMLAttributes } from \"react\";\n+import { highlightText } from \"./IntegrationBlock\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  title?: string;\n+  description?: string;\n+  highlightedText?: string;\n+}\n+\n+interface BlockComponent extends React.FC<Props> {\n+  Skeleton: React.FC<{ className?: string }>;\n+}\n+\n+export const Block: BlockComponent = ({\n+  title,\n+  description,\n+  highlightedText,\n+  className,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"group flex h-16 w-full min-w-[7.5rem] items-center justify-start space-x-3 whitespace-normal rounded-[0.75rem] bg-zinc-50 px-[0.875rem] py-[0.625rem] text-start shadow-none\",\n+        \"hover:cursor-default hover:bg-zinc-100 focus:ring-0 active:bg-zinc-100 active:ring-1 active:ring-zinc-300 disabled:cursor-not-allowed\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      <div className=\"flex flex-1 flex-col items-start gap-0.5\">\n+        {title && (\n+          <span\n+            className={cn(\n+              \"line-clamp-1 font-sans text-sm font-medium leading-[1.375rem] text-zinc-800 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            {highlightText(beautifyString(title), highlightedText)}\n+          </span>\n+        )}\n+        {description && (\n+          <span\n+            className={cn(\n+              \"line-clamp-1 font-sans text-xs font-normal leading-5 text-zinc-500 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            {highlightText(description, highlightedText)}\n+          </span>\n+        )}\n+      </div>\n+      <div\n+        className={cn(\n+          \"flex h-7 w-7 items-center justify-center rounded-[0.5rem] bg-zinc-700 group-disabled:bg-zinc-400\",\n+        )}\n+      >\n+        <Plus className=\"h-5 w-5 text-zinc-50\" strokeWidth={2} />\n+      </div>\n+    </Button>\n+  );\n+};\n+\n+const BlockSkeleton = () => {\n+  return (\n+    <Skeleton className=\"flex h-16 w-full min-w-[7.5rem] animate-pulse items-center justify-start space-x-3 rounded-[0.75rem] bg-zinc-100 px-[0.875rem] py-[0.625rem]\">\n+      <div className=\"flex flex-1 flex-col items-start gap-0.5\">\n+        <Skeleton className=\"h-[1.375rem] w-24 rounded bg-zinc-200\" />\n+        <Skeleton className=\"h-5 w-32 rounded bg-zinc-200\" />\n+      </div>\n+      <Skeleton className=\"h-7 w-7 rounded-[0.5rem] bg-zinc-200\" />\n+    </Skeleton>\n+  );\n+};\n+\n+Block.Skeleton = BlockSkeleton;\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/BlockMenu.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/BlockMenu.tsx\nnew file mode 100644\nindex 000000000000..d3f16b8fe4c4\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/BlockMenu.tsx\n@@ -0,0 +1,62 @@\n+import React, { useState } from \"react\";\n+import {\n+  Popover,\n+  PopoverContent,\n+  PopoverTrigger,\n+} from \"@/components/ui/popover\";\n+import { ControlPanelButton } from \"@/components/builder/block-menu/ControlPanelButton\";\n+import { ToyBrick } from \"lucide-react\";\n+import { BlockMenuContent } from \"./BlockMenuContent\";\n+import { BlockMenuStateProvider } from \"./block-menu-provider\";\n+import { Block } from \"@/lib/autogpt-server-api\";\n+\n+interface BlockMenuProps {\n+  addNode: (block: Block) => void;\n+  pinBlocksPopover: boolean;\n+  blockMenuSelected: \"save\" | \"block\" | \"\";\n+  setBlockMenuSelected: React.Dispatch<\n+    React.SetStateAction<\"\" | \"save\" | \"block\">\n+  >;\n+}\n+\n+export const BlockMenu: React.FC<BlockMenuProps> = ({\n+  addNode,\n+  pinBlocksPopover,\n+  blockMenuSelected,\n+  setBlockMenuSelected,\n+}) => {\n+  const [open, setOpen] = useState(false);\n+  const onOpen = (newOpen: boolean) => {\n+    if (!pinBlocksPopover) {\n+      setOpen(newOpen);\n+      setBlockMenuSelected(newOpen ? \"block\" : \"\");\n+    }\n+  };\n+\n+  return (\n+    <Popover open={pinBlocksPopover ? true : open} onOpenChange={onOpen}>\n+      <PopoverTrigger className=\"hover:cursor-pointer\">\n+        <ControlPanelButton\n+          data-id=\"blocks-control-popover-trigger\"\n+          data-testid=\"blocks-control-blocks-button\"\n+          selected={blockMenuSelected === \"block\"}\n+          className=\"rounded-none\"\n+        >\n+          <ToyBrick className=\"h-5 w-6\" strokeWidth={2} />\n+        </ControlPanelButton>\n+      </PopoverTrigger>\n+\n+      <PopoverContent\n+        side=\"right\"\n+        align=\"start\"\n+        sideOffset={16}\n+        className=\"absolute h-[75vh] w-[46.625rem] overflow-hidden rounded-[1rem] border-none p-0 shadow-[0_2px_6px_0_rgba(0,0,0,0.05)]\"\n+        data-id=\"blocks-control-popover-content\"\n+      >\n+        <BlockMenuStateProvider addNode={addNode}>\n+          <BlockMenuContent />\n+        </BlockMenuStateProvider>\n+      </PopoverContent>\n+    </Popover>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/BlockMenuContent.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/BlockMenuContent.tsx\nnew file mode 100644\nindex 000000000000..dc7aeb562e3d\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/BlockMenuContent.tsx\n@@ -0,0 +1,18 @@\n+\"use client\";\n+import React from \"react\";\n+import { BlockMenuSearchBar } from \"./BlockMenuSearchBar\";\n+import { BlockMenuSearch } from \"./search-and-filter//BlockMenuSearch\";\n+import { BlockMenuDefault } from \"./default/BlockMenuDefault\";\n+import { Separator } from \"@/components/ui/separator\";\n+import { useBlockMenuContext } from \"./block-menu-provider\";\n+\n+export const BlockMenuContent = () => {\n+  const { searchQuery } = useBlockMenuContext();\n+  return (\n+    <div className=\"flex h-full w-full flex-col\">\n+      <BlockMenuSearchBar />\n+      <Separator className=\"h-[1px] w-full text-zinc-300\" />\n+      {searchQuery ? <BlockMenuSearch /> : <BlockMenuDefault />}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/BlockMenuSearchBar.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/BlockMenuSearchBar.tsx\nnew file mode 100644\nindex 000000000000..3e4e05119b5b\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/BlockMenuSearchBar.tsx\n@@ -0,0 +1,90 @@\n+import { cn } from \"@/lib/utils\";\n+import { Search, X } from \"lucide-react\";\n+import React, { useRef, useState, useEffect, useMemo } from \"react\";\n+import { useBlockMenuContext } from \"./block-menu-provider\";\n+import { Button } from \"@/components/ui/button\";\n+import debounce from \"lodash/debounce\";\n+import { Input } from \"@/components/ui/input\";\n+import { getDefaultFilters } from \"./helpers\";\n+\n+const SEARCH_DEBOUNCE_MS = 500;\n+\n+interface BlockMenuSearchBarProps {\n+  className?: string;\n+}\n+\n+export const BlockMenuSearchBar: React.FC<BlockMenuSearchBarProps> = ({\n+  className = \"\",\n+}) => {\n+  const inputRef = useRef<HTMLInputElement>(null);\n+  const [localQuery, setLocalQuery] = useState(\"\");\n+  const { setSearchQuery, searchId, setSearchId, setFilters } =\n+    useBlockMenuContext();\n+\n+  const searchIdRef = useRef(searchId);\n+  useEffect(() => {\n+    searchIdRef.current = searchId;\n+  }, [searchId]);\n+\n+  const debouncedSetSearchQuery = useMemo(\n+    () =>\n+      debounce((value: string) => {\n+        setSearchQuery(value);\n+        if (value.length === 0) {\n+          setSearchId(undefined);\n+        } else if (!searchIdRef.current) {\n+          setSearchId(crypto.randomUUID());\n+        }\n+      }, SEARCH_DEBOUNCE_MS),\n+    [setSearchQuery, setSearchId],\n+  );\n+\n+  useEffect(() => {\n+    return () => {\n+      debouncedSetSearchQuery.cancel();\n+    };\n+  }, [debouncedSetSearchQuery]);\n+\n+  const handleClear = () => {\n+    setLocalQuery(\"\");\n+    setSearchQuery(\"\");\n+    setSearchId(undefined);\n+    setFilters(getDefaultFilters());\n+    debouncedSetSearchQuery.cancel();\n+  };\n+\n+  return (\n+    <div\n+      className={cn(\n+        \"flex min-h-[3.5625rem] items-center gap-2.5 px-4\",\n+        className,\n+      )}\n+    >\n+      <Search className=\"h-6 w-6 text-zinc-700\" strokeWidth={2} />\n+      <Input\n+        ref={inputRef}\n+        type=\"text\"\n+        value={localQuery}\n+        onChange={(e) => {\n+          setLocalQuery(e.target.value);\n+          debouncedSetSearchQuery(e.target.value);\n+        }}\n+        placeholder={\"Blocks, Agents, Integrations or Keywords...\"}\n+        className={cn(\n+          \"m-0 border-none p-0 font-sans text-base font-normal text-zinc-800 shadow-none outline-none\",\n+          \"placeholder:text-zinc-400 focus:shadow-none focus:outline-none focus:ring-0\",\n+        )}\n+      />\n+      {localQuery.length > 0 && (\n+        <Button\n+          variant=\"ghost\"\n+          size=\"sm\"\n+          onClick={handleClear}\n+          className=\"p-0 hover:bg-transparent\"\n+        >\n+          <X className=\"h-6 w-6 text-zinc-700\" strokeWidth={2} />\n+        </Button>\n+      )}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/ControlPanelButton.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/ControlPanelButton.tsx\nnew file mode 100644\nindex 000000000000..8bf8f2bb5aaa\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/ControlPanelButton.tsx\n@@ -0,0 +1,34 @@\n+// BLOCK MENU TODO: We need a disable state in this, currently it's not in design.\n+\n+import { cn } from \"@/lib/utils\";\n+import React from \"react\";\n+\n+interface Props extends React.HTMLAttributes<HTMLDivElement> {\n+  selected?: boolean;\n+  children?: React.ReactNode; // For icon purpose\n+  disabled?: boolean;\n+}\n+\n+export const ControlPanelButton: React.FC<Props> = ({\n+  selected = false,\n+  children,\n+  disabled,\n+  className,\n+  ...rest\n+}) => {\n+  return (\n+    // Using div instead of button, because it's only for design purposes. We are using this to give design to PopoverTrigger.\n+    <div\n+      className={cn(\n+        \"flex h-[4.25rem] w-[4.25rem] items-center justify-center whitespace-normal bg-white p-[1.38rem] text-zinc-800 shadow-none hover:cursor-pointer hover:bg-zinc-100 hover:text-zinc-950 focus:ring-0\",\n+        selected &&\n+          \"bg-violet-50 text-violet-700 hover:cursor-default hover:bg-violet-50 hover:text-violet-700 active:bg-violet-50 active:text-violet-700\",\n+        disabled && \"cursor-not-allowed\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      {children}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/ErrorState.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/ErrorState.tsx\nnew file mode 100644\nindex 000000000000..ae59e923dea1\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/ErrorState.tsx\n@@ -0,0 +1,54 @@\n+import { Button } from \"@/components/ui/button\";\n+import { cn, parseErrorMessage } from \"@/lib/utils\";\n+import { AlertCircle, RefreshCw } from \"lucide-react\";\n+import React from \"react\";\n+\n+interface ErrorStateProps {\n+  title?: string;\n+  message?: string;\n+  error?: string | Error | null;\n+  onRetry?: () => void;\n+  retryLabel?: string;\n+  className?: string;\n+  showIcon?: boolean;\n+}\n+\n+export const ErrorState: React.FC<ErrorStateProps> = ({\n+  title = \"Something went wrong\",\n+  message,\n+  error,\n+  onRetry,\n+  retryLabel = \"Retry\",\n+  className,\n+  showIcon = true,\n+}) => {\n+  return (\n+    <div\n+      className={cn(\n+        \"flex h-full w-full flex-col items-center justify-center space-y-4 text-center\",\n+        className,\n+      )}\n+    >\n+      {showIcon && <AlertCircle className=\"h-12 w-12\" strokeWidth={1.5} />}\n+\n+      <div className=\"space-y-2\">\n+        <p className=\"text-sm font-medium text-zinc-800\">{title}</p>\n+        <p className=\"text-sm text-zinc-600\">\n+          {parseErrorMessage(error, message)}\n+        </p>\n+      </div>\n+\n+      {onRetry && (\n+        <Button\n+          variant=\"default\"\n+          size=\"sm\"\n+          onClick={onRetry}\n+          className=\"mt-2 h-7 bg-zinc-800 text-xs\"\n+        >\n+          <RefreshCw className=\"mr-1 h-3 w-3\" />\n+          {retryLabel}\n+        </Button>\n+      )}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/FilterChip.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/FilterChip.tsx\nnew file mode 100644\nindex 000000000000..214e5e8b25d2\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/FilterChip.tsx\n@@ -0,0 +1,54 @@\n+import { Button } from \"@/components/ui/button\";\n+import { cn } from \"@/lib/utils\";\n+import { X } from \"lucide-react\";\n+import React, { ButtonHTMLAttributes } from \"react\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  selected?: boolean;\n+  number?: number;\n+  name?: string;\n+}\n+\n+export const FilterChip: React.FC<Props> = ({\n+  selected = false,\n+  number,\n+  name,\n+  className,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"group w-fit space-x-1 rounded-[1.5rem] border border-zinc-300 bg-transparent px-[0.625rem] py-[0.375rem] shadow-none transition-transform duration-300 ease-in-out\",\n+        \"hover:border-violet-500 hover:bg-transparent focus:ring-0 disabled:cursor-not-allowed\",\n+        selected && \"border-0 bg-violet-700 hover:border\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      <span\n+        className={cn(\n+          \"font-sans text-sm font-medium leading-[1.375rem] text-zinc-600 group-hover:text-zinc-600 group-disabled:text-zinc-400\",\n+          selected && \"text-zinc-50\",\n+        )}\n+      >\n+        {name}\n+      </span>\n+      {selected && (\n+        <>\n+          <span className=\"flex h-4 w-4 items-center justify-center rounded-full bg-zinc-50 transition-all duration-300 ease-in-out group-hover:hidden\">\n+            <X\n+              className=\"h-3 w-3 rounded-full text-violet-700\"\n+              strokeWidth={2}\n+            />\n+          </span>\n+          {number !== undefined && (\n+            <span className=\"hidden h-[1.375rem] items-center rounded-[1.25rem] bg-violet-700 p-[0.375rem] text-zinc-50 transition-all duration-300 ease-in-out animate-in fade-in zoom-in group-hover:flex\">\n+              {number > 100 ? \"100+\" : number}\n+            </span>\n+          )}\n+        </>\n+      )}\n+    </Button>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/Integration.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/Integration.tsx\nnew file mode 100644\nindex 000000000000..6082df97c847\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/Integration.tsx\n@@ -0,0 +1,88 @@\n+import { Button } from \"@/components/ui/button\";\n+import { Skeleton } from \"@/components/ui/skeleton\";\n+import { beautifyString, cn } from \"@/lib/utils\";\n+import Image from \"next/image\";\n+import React, { ButtonHTMLAttributes } from \"react\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  title?: string;\n+  description?: string;\n+  icon_url?: string;\n+  number_of_blocks?: number;\n+}\n+\n+interface IntegrationComponent extends React.FC<Props> {\n+  Skeleton: React.FC<{ className?: string }>;\n+}\n+\n+export const Integration: IntegrationComponent = ({\n+  title,\n+  icon_url,\n+  description,\n+  className,\n+  number_of_blocks,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"group flex h-16 w-full min-w-[7.5rem] items-center justify-start space-x-3 whitespace-normal rounded-[0.75rem] bg-zinc-50 px-[0.875rem] py-[0.625rem] text-start shadow-none\",\n+        \"hover:cursor-default hover:bg-zinc-100 focus:ring-0 active:bg-zinc-50 active:ring-1 active:ring-zinc-300 disabled:pointer-events-none\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      <div className=\"relative h-[2.625rem] w-[2.625rem] overflow-hidden rounded-[0.5rem] bg-white\">\n+        {icon_url && (\n+          <Image\n+            src={icon_url}\n+            alt=\"integration-icon\"\n+            fill\n+            sizes=\"2.25rem\"\n+            className=\"w-full rounded-[0.5rem] object-contain group-disabled:opacity-50\"\n+          />\n+        )}\n+      </div>\n+\n+      <div className=\"w-full\">\n+        <div className=\"flex items-center justify-between gap-2\">\n+          {title && (\n+            <p className=\"line-clamp-1 flex-1 font-sans text-sm font-medium leading-[1.375rem] text-zinc-700 group-disabled:text-zinc-400\">\n+              {beautifyString(title)}\n+            </p>\n+          )}\n+          <span className=\"flex h-[1.375rem] w-[1.6875rem] items-center justify-center rounded-[1.25rem] bg-[#f0f0f0] p-1.5 font-sans text-sm leading-[1.375rem] text-zinc-500 group-disabled:text-zinc-400\">\n+            {number_of_blocks}\n+          </span>\n+        </div>\n+        <span className=\"line-clamp-1 font-sans text-xs font-normal leading-5 text-zinc-500 group-disabled:text-zinc-400\">\n+          {description}\n+        </span>\n+      </div>\n+    </Button>\n+  );\n+};\n+\n+const IntegrationSkeleton: React.FC<{ className?: string }> = ({\n+  className,\n+}) => {\n+  return (\n+    <Skeleton\n+      className={cn(\n+        \"flex h-16 w-full min-w-[7.5rem] animate-pulse items-center justify-start space-x-3 rounded-[0.75rem] bg-zinc-100 px-[0.875rem] py-[0.625rem]\",\n+        className,\n+      )}\n+    >\n+      <Skeleton className=\"h-[2.625rem] w-[2.625rem] rounded-[0.5rem] bg-zinc-200\" />\n+      <div className=\"flex flex-1 flex-col items-start gap-0.5\">\n+        <div className=\"flex w-full items-center justify-between\">\n+          <Skeleton className=\"h-[1.375rem] w-24 rounded bg-zinc-200\" />\n+          <Skeleton className=\"h-[1.375rem] w-[1.6875rem] rounded-[1.25rem] bg-zinc-200\" />\n+        </div>\n+        <Skeleton className=\"h-5 w-[80%] rounded bg-zinc-200\" />\n+      </div>\n+    </Skeleton>\n+  );\n+};\n+\n+Integration.Skeleton = IntegrationSkeleton;\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/IntegrationBlock.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/IntegrationBlock.tsx\nnew file mode 100644\nindex 000000000000..b621e805ce9a\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/IntegrationBlock.tsx\n@@ -0,0 +1,119 @@\n+import { Button } from \"@/components/ui/button\";\n+import { Skeleton } from \"@/components/ui/skeleton\";\n+import { beautifyString, cn } from \"@/lib/utils\";\n+import { Plus } from \"lucide-react\";\n+import Image from \"next/image\";\n+import React, { ButtonHTMLAttributes } from \"react\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  title?: string;\n+  description?: string;\n+  icon_url?: string;\n+  highlightedText?: string;\n+}\n+\n+interface IntegrationBlockComponent extends React.FC<Props> {\n+  Skeleton: React.FC<{ className?: string }>;\n+}\n+\n+export const highlightText = (\n+  text: string | undefined,\n+  highlight: string | undefined,\n+) => {\n+  if (!text || !highlight) return text;\n+\n+  function escapeRegExp(s: string) {\n+    return s.replace(/[.*+?^${}()|[\\]\\\\]/g, \"\\\\$&\");\n+  }\n+\n+  const escaped = escapeRegExp(highlight);\n+  const parts = text.split(new RegExp(`(${escaped})`, \"gi\"));\n+  return parts.map((part, i) =>\n+    part.toLowerCase() === highlight?.toLowerCase() ? (\n+      <mark key={i} className=\"bg-transparent font-bold\">\n+        {part}\n+      </mark>\n+    ) : (\n+      part\n+    ),\n+  );\n+};\n+\n+export const IntegrationBlock: IntegrationBlockComponent = ({\n+  title,\n+  icon_url,\n+  description,\n+  className,\n+  highlightedText,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"group flex h-16 w-full min-w-[7.5rem] items-center justify-start gap-3 whitespace-normal rounded-[0.75rem] bg-zinc-50 px-[0.875rem] py-[0.625rem] text-start shadow-none\",\n+        \"hover:cursor-default hover:bg-zinc-100 focus:ring-0 active:bg-zinc-100 active:ring-1 active:ring-zinc-300 disabled:cursor-not-allowed\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      <div className=\"relative h-[2.625rem] w-[2.625rem] rounded-[0.5rem] bg-white\">\n+        {icon_url && (\n+          <Image\n+            src={icon_url}\n+            alt=\"integration-icon\"\n+            fill\n+            sizes=\"2.25rem\"\n+            className=\"w-full object-contain group-disabled:opacity-50\"\n+          />\n+        )}\n+      </div>\n+      <div className=\"flex flex-1 flex-col items-start gap-0.5\">\n+        {title && (\n+          <span\n+            className={cn(\n+              \"line-clamp-1 font-sans text-sm font-medium leading-[1.375rem] text-zinc-800 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            {highlightText(beautifyString(title), highlightedText)}\n+          </span>\n+        )}\n+        {description && (\n+          <span\n+            className={cn(\n+              \"line-clamp-1 font-sans text-xs font-normal leading-5 text-zinc-500 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            {highlightText(description, highlightedText)}\n+          </span>\n+        )}\n+      </div>\n+      <div\n+        className={cn(\n+          \"flex h-7 w-7 items-center justify-center rounded-[0.5rem] bg-zinc-700 group-disabled:bg-zinc-400\",\n+        )}\n+      >\n+        <Plus className=\"h-5 w-5 text-zinc-50\" strokeWidth={2} />\n+      </div>\n+    </Button>\n+  );\n+};\n+\n+const IntegrationBlockSkeleton = ({ className }: { className?: string }) => {\n+  return (\n+    <Skeleton\n+      className={cn(\n+        \"flex h-16 w-full min-w-[7.5rem] animate-pulse items-center justify-start gap-3 rounded-[0.75rem] bg-zinc-100 px-[0.875rem] py-[0.625rem]\",\n+        className,\n+      )}\n+    >\n+      <Skeleton className=\"h-[2.625rem] w-[2.625rem] rounded-[0.5rem] bg-zinc-200\" />\n+      <div className=\"flex flex-1 flex-col items-start gap-0.5\">\n+        <Skeleton className=\"h-[1.375rem] w-24 rounded bg-zinc-200\" />\n+        <Skeleton className=\"h-5 w-32 rounded bg-zinc-200\" />\n+      </div>\n+      <Skeleton className=\"h-7 w-7 rounded-[0.5rem] bg-zinc-200\" />\n+    </Skeleton>\n+  );\n+};\n+\n+IntegrationBlock.Skeleton = IntegrationBlockSkeleton;\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/IntegrationChip.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/IntegrationChip.tsx\nnew file mode 100644\nindex 000000000000..611e283bdb1a\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/IntegrationChip.tsx\n@@ -0,0 +1,60 @@\n+import { Button } from \"@/components/ui/button\";\n+import { Skeleton } from \"@/components/ui/skeleton\";\n+import { beautifyString, cn } from \"@/lib/utils\";\n+import Image from \"next/image\";\n+import React, { ButtonHTMLAttributes } from \"react\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  name?: string;\n+  icon_url?: string;\n+}\n+\n+interface IntegrationChipComponent extends React.FC<Props> {\n+  Skeleton: React.FC;\n+}\n+\n+export const IntegrationChip: IntegrationChipComponent = ({\n+  icon_url,\n+  name,\n+  className,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"flex h-[3.25rem] w-full min-w-[7.5rem] justify-start gap-2 whitespace-normal rounded-[0.5rem] bg-zinc-50 p-2 pr-3 shadow-none\",\n+        \"hover:cursor-default hover:bg-zinc-100 focus:ring-0 active:bg-zinc-100 active:ring-1 active:ring-zinc-300\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      <div className=\"relative h-9 w-9 rounded-[0.5rem] bg-transparent\">\n+        {icon_url && (\n+          <Image\n+            src={icon_url}\n+            alt=\"integration-icon\"\n+            fill\n+            sizes=\"2.25rem\"\n+            className=\"w-full object-contain\"\n+          />\n+        )}\n+      </div>\n+      {name && (\n+        <span className=\"truncate font-sans text-sm font-normal leading-[1.375rem] text-zinc-800\">\n+          {beautifyString(name)}\n+        </span>\n+      )}\n+    </Button>\n+  );\n+};\n+\n+const IntegrationChipSkeleton: React.FC = () => {\n+  return (\n+    <Skeleton className=\"flex h-[3.25rem] w-full min-w-[7.5rem] gap-2 rounded-[0.5rem] bg-zinc-100 p-2 pr-3\">\n+      <Skeleton className=\"h-9 w-12 rounded-[0.5rem] bg-zinc-200\" />\n+      <Skeleton className=\"h-5 w-24 self-center rounded-sm bg-zinc-200\" />\n+    </Skeleton>\n+  );\n+};\n+\n+IntegrationChip.Skeleton = IntegrationChipSkeleton;\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/MarketplaceAgentBlock.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/MarketplaceAgentBlock.tsx\nnew file mode 100644\nindex 000000000000..84184e40d074\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/MarketplaceAgentBlock.tsx\n@@ -0,0 +1,135 @@\n+import { Button } from \"@/components/ui/button\";\n+import { Skeleton } from \"@/components/ui/skeleton\";\n+import { cn } from \"@/lib/utils\";\n+import { ExternalLink, Loader2, Plus } from \"lucide-react\";\n+import Image from \"next/image\";\n+import React, { ButtonHTMLAttributes } from \"react\";\n+import { highlightText } from \"./IntegrationBlock\";\n+import Link from \"next/link\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  title?: string;\n+  creator_name?: string;\n+  number_of_runs?: number;\n+  image_url?: string;\n+  highlightedText?: string;\n+  slug: string;\n+  loading: boolean;\n+}\n+\n+interface MarketplaceAgentBlockComponent extends React.FC<Props> {\n+  Skeleton: React.FC<{ className?: string }>;\n+}\n+\n+export const MarketplaceAgentBlock: MarketplaceAgentBlockComponent = ({\n+  title,\n+  image_url,\n+  creator_name,\n+  number_of_runs,\n+  className,\n+  loading,\n+  highlightedText,\n+  slug,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"group flex h-[4.375rem] w-full min-w-[7.5rem] items-center justify-start gap-3 whitespace-normal rounded-[0.75rem] bg-zinc-50 p-[0.625rem] pr-[0.875rem] text-start shadow-none\",\n+        \"hover:cursor-default hover:bg-zinc-100 focus:ring-0 active:bg-zinc-100 active:ring-1 active:ring-zinc-300 disabled:pointer-events-none\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      <div className=\"relative h-[3.125rem] w-[5.625rem] overflow-hidden rounded-[0.375rem] bg-white\">\n+        {image_url && (\n+          <Image\n+            src={image_url}\n+            alt=\"integration-icon\"\n+            fill\n+            sizes=\"5.625rem\"\n+            className=\"w-full object-contain group-disabled:opacity-50\"\n+          />\n+        )}\n+      </div>\n+      <div className=\"flex flex-1 flex-col items-start gap-0.5\">\n+        {title && (\n+          <span\n+            className={cn(\n+              \"line-clamp-1 font-sans text-sm font-medium leading-[1.375rem] text-zinc-800 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            {highlightText(title, highlightedText)}\n+          </span>\n+        )}\n+        <div className=\"flex items-center space-x-2.5\">\n+          <span\n+            className={cn(\n+              \"truncate font-sans text-xs font-normal leading-5 text-zinc-500 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            By {creator_name}\n+          </span>\n+\n+          <span className=\"font-sans text-zinc-400\"></span>\n+\n+          <span\n+            className={cn(\n+              \"truncate font-sans text-xs font-normal leading-5 text-zinc-500 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            {number_of_runs} runs\n+          </span>\n+          <span className=\"font-sans text-zinc-400\"></span>\n+          <Link\n+            href={`/marketplace/agent/${creator_name}/${slug}`}\n+            className=\"flex gap-0.5 truncate\"\n+            onClick={(e) => e.stopPropagation()}\n+          >\n+            <span className=\"font-sans text-xs leading-5 text-blue-700 underline\">\n+              Agent page\n+            </span>\n+            <ExternalLink className=\"h-4 w-4 text-blue-700\" strokeWidth={1} />\n+          </Link>\n+        </div>\n+      </div>\n+      <div\n+        className={cn(\n+          \"flex h-7 min-w-7 items-center justify-center rounded-[0.5rem] bg-zinc-700 group-disabled:bg-zinc-400\",\n+        )}\n+      >\n+        {!loading ? (\n+          <Plus className=\"h-5 w-5 text-zinc-50\" strokeWidth={2} />\n+        ) : (\n+          <Loader2 className=\"h-5 w-5 animate-spin\" />\n+        )}\n+      </div>\n+    </Button>\n+  );\n+};\n+\n+const MarketplaceAgentBlockSkeleton: React.FC<{ className?: string }> = ({\n+  className,\n+}) => {\n+  return (\n+    <Skeleton\n+      className={cn(\n+        \"flex h-[4.375rem] w-full min-w-[7.5rem] animate-pulse items-center justify-start gap-3 rounded-[0.75rem] bg-zinc-100 p-[0.625rem] pr-[0.875rem]\",\n+        className,\n+      )}\n+    >\n+      <Skeleton className=\"h-[3.125rem] w-[5.625rem] rounded-[0.375rem] bg-zinc-200\" />\n+      <div className=\"flex flex-1 flex-col items-start gap-0.5\">\n+        <Skeleton className=\"h-[1.375rem] w-24 rounded bg-zinc-200\" />\n+        <div className=\"flex items-center gap-1\">\n+          <Skeleton className=\"h-5 w-16 rounded bg-zinc-200\" />\n+\n+          <Skeleton className=\"h-5 w-16 rounded bg-zinc-200\" />\n+        </div>\n+      </div>\n+      <Skeleton className=\"h-7 w-7 rounded-[0.5rem] bg-zinc-200\" />\n+    </Skeleton>\n+  );\n+};\n+\n+MarketplaceAgentBlock.Skeleton = MarketplaceAgentBlockSkeleton;\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/MenuItem.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/MenuItem.tsx\nnew file mode 100644\nindex 000000000000..89407ccfbdc1\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/MenuItem.tsx\n@@ -0,0 +1,40 @@\n+// BLOCK MENU TODO: We need to add a better hover state to it; currently it's not in the design either.\n+\n+import { Button } from \"@/components/ui/button\";\n+import { cn } from \"@/lib/utils\";\n+import React, { ButtonHTMLAttributes } from \"react\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  selected?: boolean;\n+  number?: number;\n+  name?: string;\n+}\n+\n+export const MenuItem: React.FC<Props> = ({\n+  selected = false,\n+  number,\n+  name,\n+  className,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"flex h-[2.375rem] w-[12.875rem] justify-between whitespace-normal rounded-[0.5rem] bg-transparent p-2 pl-3 shadow-none\",\n+        \"hover:cursor-default hover:bg-zinc-100 focus:ring-0\",\n+        selected && \"bg-zinc-100\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      <span className=\"truncate font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\">\n+        {name}\n+      </span>\n+      {number && (\n+        <span className=\"font-sans text-sm font-normal leading-[1.375rem] text-zinc-600\">\n+          {number > 100 ? \"100+\" : number}\n+        </span>\n+      )}\n+    </Button>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/SearchHistoryChip.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/SearchHistoryChip.tsx\nnew file mode 100644\nindex 000000000000..77cad6443a99\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/SearchHistoryChip.tsx\n@@ -0,0 +1,47 @@\n+import { Button } from \"@/components/ui/button\";\n+import { Skeleton } from \"@/components/ui/skeleton\";\n+import { cn } from \"@/lib/utils\";\n+import { ArrowUpRight } from \"lucide-react\";\n+import React, { ButtonHTMLAttributes } from \"react\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  content?: string;\n+}\n+\n+interface SearchHistoryChipComponent extends React.FC<Props> {\n+  Skeleton: React.FC<{ className?: string }>;\n+}\n+\n+export const SearchHistoryChip: SearchHistoryChipComponent = ({\n+  content,\n+  className,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"my-[1px] h-[2.25rem] space-x-1 rounded-[1.5rem] bg-zinc-50 p-[0.375rem] pr-[0.625rem] shadow-none\",\n+        \"hover:cursor-default hover:bg-zinc-100 focus:ring-0 active:bg-zinc-100 active:ring-1 active:ring-zinc-300\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      <ArrowUpRight className=\"h-6 w-6 text-zinc-500\" strokeWidth={1.25} />\n+      <span className=\"font-sans text-sm font-normal leading-[1.375rem] text-zinc-800\">\n+        {content}\n+      </span>\n+    </Button>\n+  );\n+};\n+\n+const SearchHistoryChipSkeleton: React.FC<{ className?: string }> = ({\n+  className,\n+}) => {\n+  return (\n+    <Skeleton\n+      className={cn(\"h-[2.25rem] w-32 rounded-[1.5rem] bg-zinc-100\", className)}\n+    />\n+  );\n+};\n+\n+SearchHistoryChip.Skeleton = SearchHistoryChipSkeleton;\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/UGCAgentBlock.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/UGCAgentBlock.tsx\nnew file mode 100644\nindex 000000000000..7244deae886b\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/UGCAgentBlock.tsx\n@@ -0,0 +1,117 @@\n+import { Button } from \"@/components/ui/button\";\n+import { Skeleton } from \"@/components/ui/skeleton\";\n+import { cn } from \"@/lib/utils\";\n+import { Plus } from \"lucide-react\";\n+import Image from \"next/image\";\n+import React, { ButtonHTMLAttributes } from \"react\";\n+import { highlightText } from \"./IntegrationBlock\";\n+import TimeAgo from \"react-timeago\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  title?: string;\n+  edited_time?: Date;\n+  version?: number;\n+  image_url?: string;\n+  highlightedText?: string;\n+}\n+\n+interface UGCAgentBlockComponent extends React.FC<Props> {\n+  Skeleton: React.FC<{ className?: string }>;\n+}\n+\n+export const UGCAgentBlock: UGCAgentBlockComponent = ({\n+  title,\n+  image_url,\n+  edited_time,\n+  version,\n+  className,\n+  highlightedText,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"group flex h-[4.375rem] w-full min-w-[7.5rem] items-center justify-start gap-3 whitespace-normal rounded-[0.75rem] bg-zinc-50 p-[0.625rem] pr-[0.875rem] text-start shadow-none\",\n+        \"hover:cursor-default hover:bg-zinc-100 focus:ring-0 active:bg-zinc-100 active:ring-1 active:ring-zinc-300 disabled:cursor-not-allowed\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      {image_url && (\n+        <div className=\"relative h-[3.125rem] w-[5.625rem] overflow-hidden rounded-[0.375rem] bg-white\">\n+          <Image\n+            src={image_url}\n+            alt=\"integration-icon\"\n+            fill\n+            sizes=\"5.625rem\"\n+            className=\"w-full object-contain group-disabled:opacity-50\"\n+          />\n+        </div>\n+      )}\n+      <div className=\"flex flex-1 flex-col items-start gap-0.5\">\n+        {title && (\n+          <span\n+            className={cn(\n+              \"line-clamp-1 font-sans text-sm font-medium leading-[1.375rem] text-zinc-800 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            {highlightText(title, highlightedText)}\n+          </span>\n+        )}\n+        <div className=\"flex items-center space-x-1.5\">\n+          {edited_time && (\n+            <span\n+              className={cn(\n+                \"line-clamp-1 font-sans text-xs font-normal leading-5 text-zinc-500 group-disabled:text-zinc-400\",\n+              )}\n+            >\n+              Edited {<TimeAgo date={edited_time} />}\n+            </span>\n+          )}\n+\n+          <span className=\"font-sans text-zinc-400\"></span>\n+\n+          <span\n+            className={cn(\n+              \"line-clamp-1 font-sans text-xs font-normal leading-5 text-zinc-500 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            Version {version}\n+          </span>\n+        </div>\n+      </div>\n+      <div\n+        className={cn(\n+          \"flex h-7 w-7 items-center justify-center rounded-[0.5rem] bg-zinc-700 group-disabled:bg-zinc-400\",\n+        )}\n+      >\n+        <Plus className=\"h-5 w-5 text-zinc-50\" strokeWidth={2} />\n+      </div>\n+    </Button>\n+  );\n+};\n+\n+const UGCAgentBlockSkeleton: React.FC<{ className?: string }> = ({\n+  className,\n+}) => {\n+  return (\n+    <Skeleton\n+      className={cn(\n+        \"flex h-[4.375rem] w-full min-w-[7.5rem] animate-pulse items-center justify-start gap-3 rounded-[0.75rem] bg-zinc-100 p-[0.625rem] pr-[0.875rem]\",\n+        className,\n+      )}\n+    >\n+      <Skeleton className=\"h-[3.125rem] w-[5.625rem] rounded-[0.375rem] bg-zinc-200\" />\n+      <div className=\"flex flex-1 flex-col items-start gap-0.5\">\n+        <Skeleton className=\"h-[1.375rem] w-24 rounded bg-zinc-200\" />\n+        <div className=\"flex items-center gap-1\">\n+          <Skeleton className=\"h-5 w-16 rounded bg-zinc-200\" />\n+          <Skeleton className=\"h-5 w-16 rounded bg-zinc-200\" />\n+        </div>\n+      </div>\n+      <Skeleton className=\"h-7 w-7 rounded-[0.5rem] bg-zinc-200\" />\n+    </Skeleton>\n+  );\n+};\n+\n+UGCAgentBlock.Skeleton = UGCAgentBlockSkeleton;\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/block-menu-provider.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/block-menu-provider.tsx\nnew file mode 100644\nindex 000000000000..8820a3a239eb\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/block-menu-provider.tsx\n@@ -0,0 +1,176 @@\n+\"use client\";\n+\n+import {\n+  Block,\n+  CredentialsProviderName,\n+  LibraryAgent,\n+  Provider,\n+  StoreAgent,\n+} from \"@/lib/autogpt-server-api\";\n+import { createContext, ReactNode, useContext, useState } from \"react\";\n+import { convertLibraryAgentIntoBlock } from \"@/lib/utils\";\n+import { useBackendAPI } from \"@/lib/autogpt-server-api/context\";\n+import { getDefaultFilters } from \"./helpers\";\n+\n+export type SearchItem = Block | Provider | LibraryAgent | StoreAgent;\n+\n+export type DefaultStateType =\n+  | \"suggestion\"\n+  | \"all_blocks\"\n+  | \"input_blocks\"\n+  | \"action_blocks\"\n+  | \"output_blocks\"\n+  | \"integrations\"\n+  | \"marketplace_agents\"\n+  | \"my_agents\";\n+\n+export type CategoryKey =\n+  | \"blocks\"\n+  | \"integrations\"\n+  | \"marketplace_agents\"\n+  | \"my_agents\";\n+\n+export interface Filters {\n+  categories: {\n+    blocks: boolean;\n+    integrations: boolean;\n+    marketplace_agents: boolean;\n+    my_agents: boolean;\n+    providers: boolean;\n+  };\n+  createdBy: string[];\n+}\n+\n+export type CategoryCounts = Record<CategoryKey, number>;\n+\n+interface BlockMenuContextType {\n+  defaultState: DefaultStateType;\n+  setDefaultState: React.Dispatch<React.SetStateAction<DefaultStateType>>;\n+  integration: CredentialsProviderName | null;\n+  setIntegration: React.Dispatch<\n+    React.SetStateAction<CredentialsProviderName | null>\n+  >;\n+  searchQuery: string;\n+  setSearchQuery: React.Dispatch<React.SetStateAction<string>>;\n+  searchId: string | undefined;\n+  setSearchId: React.Dispatch<React.SetStateAction<string | undefined>>;\n+  filters: Filters;\n+  setFilters: React.Dispatch<React.SetStateAction<Filters>>;\n+  searchData: SearchItem[];\n+  setSearchData: React.Dispatch<React.SetStateAction<SearchItem[]>>;\n+  categoryCounts: CategoryCounts;\n+  setCategoryCounts: React.Dispatch<React.SetStateAction<CategoryCounts>>;\n+  addNode: (block: Block) => void;\n+  handleAddStoreAgent: ({\n+    creator_name,\n+    slug,\n+  }: {\n+    creator_name: string;\n+    slug: string;\n+  }) => Promise<void>;\n+  loadingSlug: string | null;\n+  setLoadingSlug: React.Dispatch<React.SetStateAction<string | null>>;\n+}\n+\n+export const BlockMenuContext = createContext<BlockMenuContextType>(\n+  {} as BlockMenuContextType,\n+);\n+\n+interface BlockMenuStateProviderProps {\n+  children: ReactNode;\n+  addNode: (block: Block) => void;\n+}\n+\n+export function BlockMenuStateProvider({\n+  children,\n+  addNode,\n+}: BlockMenuStateProviderProps) {\n+  const [defaultState, setDefaultState] =\n+    useState<DefaultStateType>(\"suggestion\");\n+  const [integration, setIntegration] =\n+    useState<CredentialsProviderName | null>(null);\n+  const [searchQuery, setSearchQuery] = useState(\"\");\n+  const [filters, setFilters] = useState<Filters>(getDefaultFilters());\n+  const [searchData, setSearchData] = useState<SearchItem[]>([]);\n+\n+  const [searchId, setSearchId] = useState<string | undefined>(undefined);\n+\n+  const [categoryCounts, setCategoryCounts] = useState<CategoryCounts>({\n+    blocks: 0,\n+    integrations: 0,\n+    marketplace_agents: 0,\n+    my_agents: 0,\n+  });\n+\n+  const [loadingSlug, setLoadingSlug] = useState<string | null>(null);\n+\n+  const api = useBackendAPI();\n+\n+  const handleAddStoreAgent = async ({\n+    creator_name,\n+    slug,\n+  }: {\n+    creator_name: string;\n+    slug: string;\n+  }) => {\n+    try {\n+      setLoadingSlug(slug);\n+      const details = await api.getStoreAgent(creator_name, slug);\n+\n+      if (!details.active_version_id) {\n+        console.error(\n+          \"Cannot add store agent to library: active version ID is missing or undefined\",\n+        );\n+        return;\n+      }\n+\n+      const libraryAgent = await api.addMarketplaceAgentToLibrary(\n+        details.active_version_id,\n+      );\n+\n+      const block = convertLibraryAgentIntoBlock(libraryAgent);\n+      addNode(block);\n+    } catch (error) {\n+      console.error(\"Failed to add store agent:\", error);\n+    } finally {\n+      setLoadingSlug(null);\n+    }\n+  };\n+\n+  return (\n+    <BlockMenuContext.Provider\n+      value={{\n+        defaultState,\n+        setDefaultState,\n+        integration,\n+        setIntegration,\n+        searchQuery,\n+        setSearchQuery,\n+        searchId,\n+        setSearchId,\n+        filters,\n+        setFilters,\n+        searchData,\n+        setSearchData,\n+        categoryCounts,\n+        setCategoryCounts,\n+        addNode,\n+        handleAddStoreAgent,\n+        loadingSlug,\n+        setLoadingSlug,\n+      }}\n+    >\n+      {children}\n+    </BlockMenuContext.Provider>\n+  );\n+}\n+\n+export function useBlockMenuContext(): BlockMenuContextType {\n+  const context = useContext(BlockMenuContext);\n+  if (!context) {\n+    throw new Error(\n+      \"useBlockMenuContext must be used within a BlockMenuStateProvider\",\n+    );\n+  }\n+  return context;\n+}\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/AllBlocksContent.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/AllBlocksContent.tsx\nnew file mode 100644\nindex 000000000000..fdfb66d75df5\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/AllBlocksContent.tsx\n@@ -0,0 +1,162 @@\n+import React, { useState, useEffect, Fragment, useCallback } from \"react\";\n+import { Block } from \"../Block\";\n+import { Button } from \"@/components/ui/button\";\n+import { Separator } from \"@/components/ui/separator\";\n+import { Skeleton } from \"@/components/ui/skeleton\";\n+import { useBackendAPI } from \"@/lib/autogpt-server-api/context\";\n+import { BlockCategoryResponse } from \"@/lib/autogpt-server-api\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import { ErrorState } from \"../ErrorState\";\n+import { beautifyString } from \"@/lib/utils\";\n+import { scrollbarStyles } from \"@/components/styles/scrollbar\";\n+\n+export const AllBlocksContent = () => {\n+  const { addNode } = useBlockMenuContext();\n+  const [categories, setCategories] = useState<BlockCategoryResponse[]>([]);\n+  const [loading, setLoading] = useState(true);\n+  const [error, setError] = useState<string | null>(null);\n+  const [loadingCategories, setLoadingCategories] = useState<Set<string>>(\n+    new Set(),\n+  );\n+\n+  const api = useBackendAPI();\n+\n+  const fetchBlocks = useCallback(async () => {\n+    try {\n+      setLoading(true);\n+      setError(null);\n+      const response = await api.getBlockCategories();\n+      setCategories(response);\n+    } catch (err) {\n+      console.error(\"Failed to fetch block categories:\", err);\n+      setError(\n+        err instanceof Error ? err.message : \"Failed to load block categories\",\n+      );\n+    } finally {\n+      setLoading(false);\n+    }\n+  }, [api]);\n+\n+  useEffect(() => {\n+    fetchBlocks();\n+  }, [fetchBlocks]);\n+\n+  const fetchMoreBlockOfACategory = async (category: string) => {\n+    try {\n+      setLoadingCategories((prev) => new Set(prev).add(category));\n+      const response = await api.getBuilderBlocks({ category: category });\n+      const updatedCategories = categories.map((cat) => {\n+        if (cat.name === category) {\n+          return {\n+            ...cat,\n+            blocks: [...response.blocks],\n+          };\n+        }\n+        return cat;\n+      });\n+\n+      setCategories(updatedCategories);\n+    } catch (error) {\n+      console.error(`Failed to fetch blocks for category ${category}:`, error);\n+    } finally {\n+      setLoadingCategories((prev) => {\n+        const newSet = new Set(prev);\n+        newSet.delete(category);\n+        return newSet;\n+      });\n+    }\n+  };\n+\n+  if (loading) {\n+    return (\n+      <div className={scrollbarStyles}>\n+        <div className=\"w-full space-y-3 px-4 pb-4\">\n+          {Array.from({ length: 3 }).map((_, categoryIndex) => (\n+            <Fragment key={categoryIndex}>\n+              {categoryIndex > 0 && (\n+                <Skeleton className=\"my-4 h-[1px] w-full text-zinc-100\" />\n+              )}\n+              {[0, 1, 2].map((blockIndex) => (\n+                <Block.Skeleton key={`${categoryIndex}-${blockIndex}`} />\n+              ))}\n+            </Fragment>\n+          ))}\n+        </div>\n+      </div>\n+    );\n+  }\n+\n+  if (error) {\n+    return (\n+      <div className=\"h-full p-4\">\n+        <ErrorState\n+          title=\"Failed to load blocks\"\n+          error={error}\n+          onRetry={fetchBlocks}\n+        />\n+      </div>\n+    );\n+  }\n+\n+  return (\n+    <div className={scrollbarStyles}>\n+      <div className=\"w-full space-y-3 px-4 pb-4\">\n+        {categories.map((category, index) => (\n+          <Fragment key={category.name}>\n+            {index > 0 && (\n+              <Separator className=\"h-[1px] w-full text-zinc-300\" />\n+            )}\n+\n+            {/* Category Section */}\n+            <div className=\"space-y-2.5\">\n+              <div className=\"flex items-center justify-between\">\n+                <p className=\"font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\">\n+                  {category.name && beautifyString(category.name)}\n+                </p>\n+                <span className=\"rounded-full bg-zinc-100 px-[0.375rem] font-sans text-sm leading-[1.375rem] text-zinc-600\">\n+                  {category.total_blocks}\n+                </span>\n+              </div>\n+\n+              <div className=\"space-y-2\">\n+                {category.blocks.map((block) => (\n+                  <Block\n+                    key={`${category.name}-${block.id}`}\n+                    title={block.name}\n+                    description={block.name}\n+                    onClick={() => {\n+                      addNode(block);\n+                    }}\n+                  />\n+                ))}\n+\n+                {loadingCategories.has(category.name) && (\n+                  <>\n+                    {[0, 1, 2, 3, 4].map((skeletonIndex) => (\n+                      <Block.Skeleton\n+                        key={`skeleton-${category.name}-${skeletonIndex}`}\n+                      />\n+                    ))}\n+                  </>\n+                )}\n+\n+                {category.total_blocks > category.blocks.length && (\n+                  <Button\n+                    variant={\"link\"}\n+                    className=\"px-0 font-sans text-sm leading-[1.375rem] text-zinc-600 underline hover:text-zinc-800\"\n+                    disabled={loadingCategories.has(category.name)}\n+                    onClick={() => {\n+                      fetchMoreBlockOfACategory(category.name);\n+                    }}\n+                  >\n+                    see all\n+                  </Button>\n+                )}\n+              </div>\n+            </div>\n+          </Fragment>\n+        ))}\n+      </div>\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/BlockMenuDefault.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/BlockMenuDefault.tsx\nnew file mode 100644\nindex 000000000000..66cecb9c5189\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/BlockMenuDefault.tsx\n@@ -0,0 +1,14 @@\n+import React from \"react\";\n+import { BlockMenuSidebar } from \"./BlockMenuSidebar\";\n+import { Separator } from \"@/components/ui/separator\";\n+import { BlockMenuDefaultContent } from \"./BlockMenuDefaultContent\";\n+\n+export const BlockMenuDefault = () => {\n+  return (\n+    <div className=\"flex flex-1 overflow-y-auto\">\n+      <BlockMenuSidebar />\n+      <Separator className=\"h-full w-[1px] text-zinc-300\" />\n+      <BlockMenuDefaultContent />\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/BlockMenuDefaultContent.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/BlockMenuDefaultContent.tsx\nnew file mode 100644\nindex 000000000000..ef5f3c8d7c38\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/BlockMenuDefaultContent.tsx\n@@ -0,0 +1,43 @@\n+import React from \"react\";\n+import { SuggestionContent } from \"./SuggestionContent\";\n+import { AllBlocksContent } from \"./AllBlocksContent\";\n+import { IntegrationsContent } from \"./IntegrationsContent\";\n+import { MarketplaceAgentsContent } from \"./MarketplaceAgentsContent\";\n+import { MyAgentsContent } from \"./MyAgentsContent\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import { PaginatedBlocksContent } from \"./PaginatedBlocksContent\";\n+\n+export interface ActionBlock {\n+  id: number;\n+  title: string;\n+  description: string;\n+}\n+\n+export interface BlockListType {\n+  id: number;\n+  title: string;\n+  description: string;\n+}\n+\n+export const BlockMenuDefaultContent = () => {\n+  const { defaultState } = useBlockMenuContext();\n+\n+  return (\n+    <div className=\"h-full flex-1 overflow-hidden\">\n+      {defaultState == \"suggestion\" && <SuggestionContent />}\n+      {defaultState == \"all_blocks\" && <AllBlocksContent />}\n+      {defaultState == \"input_blocks\" && (\n+        <PaginatedBlocksContent blockRequest={{ type: \"input\" }} />\n+      )}\n+      {defaultState == \"action_blocks\" && (\n+        <PaginatedBlocksContent blockRequest={{ type: \"action\" }} />\n+      )}\n+      {defaultState == \"output_blocks\" && (\n+        <PaginatedBlocksContent blockRequest={{ type: \"output\" }} />\n+      )}\n+      {defaultState == \"integrations\" && <IntegrationsContent />}\n+      {defaultState == \"marketplace_agents\" && <MarketplaceAgentsContent />}\n+      {defaultState == \"my_agents\" && <MyAgentsContent />}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/BlockMenuSidebar.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/BlockMenuSidebar.tsx\nnew file mode 100644\nindex 000000000000..84591d61b94e\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/BlockMenuSidebar.tsx\n@@ -0,0 +1,117 @@\n+import React, { useEffect, useState } from \"react\";\n+import { MenuItem } from \"../MenuItem\";\n+import { DefaultStateType, useBlockMenuContext } from \"../block-menu-provider\";\n+import { useBackendAPI } from \"@/lib/autogpt-server-api/context\";\n+import { CountResponse } from \"@/lib/autogpt-server-api\";\n+\n+export const BlockMenuSidebar = () => {\n+  const { defaultState, setDefaultState, setIntegration } =\n+    useBlockMenuContext();\n+  const [blockCounts, setBlockCounts] = useState<CountResponse | undefined>(\n+    undefined,\n+  );\n+  const api = useBackendAPI();\n+\n+  useEffect(() => {\n+    const fetchBlockCounts = async () => {\n+      try {\n+        const counts = await api.getBlockCounts();\n+        setBlockCounts(counts);\n+      } catch (error) {\n+        console.error(\"Failed to fetch block counts:\", error);\n+      }\n+    };\n+\n+    fetchBlockCounts();\n+  }, [api]);\n+\n+  const topLevelMenuItems = [\n+    {\n+      name: \"Suggestion\",\n+      type: \"suggestion\",\n+    },\n+    {\n+      name: \"All blocks\",\n+      type: \"all_blocks\",\n+      number: blockCounts?.all_blocks,\n+    },\n+  ];\n+\n+  const subMenuItems = [\n+    {\n+      name: \"Input blocks\",\n+      type: \"input_blocks\",\n+      number: blockCounts?.input_blocks,\n+    },\n+    {\n+      name: \"Action blocks\",\n+      type: \"action_blocks\",\n+      number: blockCounts?.action_blocks,\n+    },\n+    {\n+      name: \"Output blocks\",\n+      type: \"output_blocks\",\n+      number: blockCounts?.output_blocks,\n+    },\n+  ];\n+\n+  const bottomMenuItems = [\n+    {\n+      name: \"Integrations\",\n+      type: \"integrations\",\n+      number: blockCounts?.integrations,\n+      onClick: () => {\n+        setIntegration(null);\n+        setDefaultState(\"integrations\");\n+      },\n+    },\n+    {\n+      name: \"Marketplace Agents\",\n+      type: \"marketplace_agents\",\n+      number: blockCounts?.marketplace_agents,\n+    },\n+    {\n+      name: \"My Agents\",\n+      type: \"my_agents\",\n+      number: blockCounts?.my_agents,\n+    },\n+  ];\n+\n+  return (\n+    <div className=\"w-fit space-y-2 px-4 pt-4\">\n+      {topLevelMenuItems.map((item) => (\n+        <MenuItem\n+          key={item.type}\n+          name={item.name}\n+          number={item.number}\n+          selected={defaultState === item.type}\n+          onClick={() => setDefaultState(item.type as DefaultStateType)}\n+        />\n+      ))}\n+      <div className=\"ml-[0.5365rem] space-y-2 border-l border-black/10 pl-[0.75rem]\">\n+        {subMenuItems.map((item) => (\n+          <MenuItem\n+            key={item.type}\n+            name={item.name}\n+            number={item.number}\n+            className=\"max-w-[11.5339rem]\"\n+            selected={defaultState === item.type}\n+            onClick={() => setDefaultState(item.type as DefaultStateType)}\n+          />\n+        ))}\n+      </div>\n+      {bottomMenuItems.map((item) => (\n+        <MenuItem\n+          key={item.type}\n+          name={item.name}\n+          number={item.number}\n+          selected={defaultState === item.type}\n+          onClick={\n+            item.onClick ||\n+            (() => setDefaultState(item.type as DefaultStateType))\n+          }\n+        />\n+      ))}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/BlocksList.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/BlocksList.tsx\nnew file mode 100644\nindex 000000000000..d230fe6a04be\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/BlocksList.tsx\n@@ -0,0 +1,34 @@\n+import React from \"react\";\n+import { Block } from \"../Block\";\n+import { Block as BlockType } from \"@/lib/autogpt-server-api\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+\n+interface BlocksListProps {\n+  blocks: BlockType[];\n+  loading?: boolean;\n+}\n+\n+export const BlocksList: React.FC<BlocksListProps> = ({\n+  blocks,\n+  loading = false,\n+}) => {\n+  const { addNode } = useBlockMenuContext();\n+  return (\n+    <div className=\"w-full space-y-3 px-4 pb-4\">\n+      {loading\n+        ? Array.from({ length: 7 }).map((_, index) => (\n+            <Block.Skeleton key={index} />\n+          ))\n+        : blocks.map((block) => (\n+            <Block\n+              key={block.id}\n+              title={block.name}\n+              description={block.description}\n+              onClick={() => {\n+                addNode(block);\n+              }}\n+            />\n+          ))}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/IntegrationBlocks.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/IntegrationBlocks.tsx\nnew file mode 100644\nindex 000000000000..72ab5d8731b1\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/IntegrationBlocks.tsx\n@@ -0,0 +1,110 @@\n+import { Button } from \"@/components/ui/button\";\n+import React, { useState, useEffect, Fragment, useCallback } from \"react\";\n+import { IntegrationBlock } from \"../IntegrationBlock\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import { useBackendAPI } from \"@/lib/autogpt-server-api/context\";\n+import { Block } from \"@/lib/autogpt-server-api\";\n+import { ErrorState } from \"../ErrorState\";\n+import { Skeleton } from \"@/components/ui/skeleton\";\n+\n+export const IntegrationBlocks = () => {\n+  const { integration, setIntegration, addNode } = useBlockMenuContext();\n+  const [blocks, setBlocks] = useState<Block[]>([]);\n+  const [loading, setLoading] = useState(true);\n+  const [error, setError] = useState<string | null>(null);\n+\n+  const api = useBackendAPI();\n+\n+  const fetchBlocks = useCallback(async () => {\n+    if (integration) {\n+      try {\n+        setLoading(true);\n+        setError(null);\n+        const response = await api.getBuilderBlocks({ provider: integration });\n+        setBlocks(response.blocks);\n+      } catch (err) {\n+        console.error(\"Failed to fetch integration blocks:\", err);\n+        setError(\n+          err instanceof Error\n+            ? err.message\n+            : \"Failed to load integration blocks\",\n+        );\n+      } finally {\n+        setLoading(false);\n+      }\n+    }\n+  }, [api, integration]);\n+\n+  useEffect(() => {\n+    fetchBlocks();\n+  }, [fetchBlocks]);\n+\n+  if (loading) {\n+    return (\n+      <div className=\"w-full space-y-3 p-4\">\n+        {Array.from({ length: 3 }).map((_, blockIndex) => (\n+          <Fragment key={blockIndex}>\n+            {blockIndex > 0 && (\n+              <Skeleton className=\"my-4 h-[1px] w-full text-zinc-100\" />\n+            )}\n+            {[0, 1, 2].map((index) => (\n+              <IntegrationBlock.Skeleton key={`${blockIndex}-${index}`} />\n+            ))}\n+          </Fragment>\n+        ))}\n+      </div>\n+    );\n+  }\n+\n+  if (error) {\n+    return (\n+      <div className=\"h-full p-4\">\n+        <ErrorState\n+          title=\"Failed to load integration blocks\"\n+          error={error}\n+          onRetry={fetchBlocks}\n+        />\n+      </div>\n+    );\n+  }\n+\n+  return (\n+    <div className=\"space-y-2.5\">\n+      <div className=\"flex items-center justify-between\">\n+        <div className=\"flex items-center gap-1\">\n+          <Button\n+            variant={\"link\"}\n+            className=\"p-0 font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\"\n+            onClick={() => {\n+              setIntegration(null);\n+            }}\n+          >\n+            Integrations\n+          </Button>\n+          <p className=\"font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\">\n+            /\n+          </p>\n+          <p className=\"font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\">\n+            {integration}\n+          </p>\n+        </div>\n+        <span className=\"flex h-[1.375rem] w-[1.6875rem] items-center justify-center rounded-[1.25rem] bg-[#f0f0f0] p-1.5 font-sans text-sm leading-[1.375rem] text-zinc-500 group-disabled:text-zinc-400\">\n+          {blocks.length}\n+        </span>\n+      </div>\n+      <div className=\"space-y-3\">\n+        {blocks.map((block) => (\n+          <IntegrationBlock\n+            key={block.id}\n+            title={block.name}\n+            description={block.description}\n+            icon_url={`/integrations/${integration}.png`}\n+            onClick={() => {\n+              addNode(block);\n+            }}\n+          />\n+        ))}\n+      </div>\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/IntegrationsContent.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/IntegrationsContent.tsx\nnew file mode 100644\nindex 000000000000..c394ab82d977\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/IntegrationsContent.tsx\n@@ -0,0 +1,21 @@\n+import React from \"react\";\n+import { PaginatedIntegrationList } from \"./PaginatedIntegrationList\";\n+import { IntegrationBlocks } from \"./IntegrationBlocks\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import { scrollbarStyles } from \"@/components/styles/scrollbar\";\n+\n+export const IntegrationsContent = () => {\n+  const { integration } = useBlockMenuContext();\n+\n+  if (!integration) {\n+    return <PaginatedIntegrationList />;\n+  }\n+\n+  return (\n+    <div className={scrollbarStyles}>\n+      <div className=\"w-full px-4 pb-4\">\n+        <IntegrationBlocks />\n+      </div>\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/MarketplaceAgentsContent.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/MarketplaceAgentsContent.tsx\nnew file mode 100644\nindex 000000000000..508cf3861149\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/MarketplaceAgentsContent.tsx\n@@ -0,0 +1,77 @@\n+import React from \"react\";\n+import { MarketplaceAgentBlock } from \"../MarketplaceAgentBlock\";\n+import { usePagination } from \"@/hooks/usePagination\";\n+import { ErrorState } from \"../ErrorState\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import { scrollbarStyles } from \"@/components/styles/scrollbar\";\n+\n+export const MarketplaceAgentsContent = () => {\n+  const {\n+    data: agents,\n+    loading,\n+    loadingMore,\n+    hasMore,\n+    error,\n+    scrollRef,\n+    refresh,\n+  } = usePagination({\n+    request: { apiType: \"store-agents\" },\n+    pageSize: 10,\n+  });\n+  const { handleAddStoreAgent, loadingSlug } = useBlockMenuContext();\n+\n+  if (loading) {\n+    return (\n+      <div ref={scrollRef} className={scrollbarStyles}>\n+        <div className=\"w-full space-y-3 px-4 pb-4\">\n+          {Array.from({ length: 5 }).map((_, index) => (\n+            <MarketplaceAgentBlock.Skeleton key={index} />\n+          ))}\n+        </div>\n+      </div>\n+    );\n+  }\n+\n+  if (error) {\n+    return (\n+      <div className=\"h-full p-4\">\n+        <ErrorState\n+          title=\"Failed to load marketplace agents\"\n+          error={error}\n+          onRetry={refresh}\n+        />\n+      </div>\n+    );\n+  }\n+\n+  return (\n+    <div ref={scrollRef} className={scrollbarStyles}>\n+      <div className=\"w-full space-y-3 px-4 pb-4\">\n+        {agents.map((agent) => (\n+          <MarketplaceAgentBlock\n+            key={agent.slug}\n+            slug={agent.slug}\n+            title={agent.agent_name}\n+            image_url={agent.agent_image}\n+            creator_name={agent.creator}\n+            number_of_runs={agent.runs}\n+            loading={loadingSlug === agent.slug}\n+            onClick={() =>\n+              handleAddStoreAgent({\n+                creator_name: agent.creator,\n+                slug: agent.slug,\n+              })\n+            }\n+          />\n+        ))}\n+        {loadingMore && hasMore && (\n+          <>\n+            {Array.from({ length: 3 }).map((_, index) => (\n+              <MarketplaceAgentBlock.Skeleton key={`loading-${index}`} />\n+            ))}\n+          </>\n+        )}\n+      </div>\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/MyAgentsContent.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/MyAgentsContent.tsx\nnew file mode 100644\nindex 000000000000..874a11d106c0\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/MyAgentsContent.tsx\n@@ -0,0 +1,74 @@\n+import React from \"react\";\n+import { UGCAgentBlock } from \"../UGCAgentBlock\";\n+import { usePagination } from \"@/hooks/usePagination\";\n+import { ErrorState } from \"../ErrorState\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import { convertLibraryAgentIntoBlock } from \"@/lib/utils\";\n+import { scrollbarStyles } from \"@/components/styles/scrollbar\";\n+\n+export const MyAgentsContent = () => {\n+  const {\n+    data: agents,\n+    loading,\n+    loadingMore,\n+    hasMore,\n+    error,\n+    scrollRef,\n+    refresh,\n+  } = usePagination({\n+    request: { apiType: \"library-agents\" },\n+    pageSize: 10,\n+  });\n+  const { addNode } = useBlockMenuContext();\n+\n+  if (loading) {\n+    return (\n+      <div ref={scrollRef} className={scrollbarStyles}>\n+        <div className=\"w-full space-y-3 px-4 pb-4\">\n+          {Array.from({ length: 5 }).map((_, index) => (\n+            <UGCAgentBlock.Skeleton key={index} />\n+          ))}\n+        </div>\n+      </div>\n+    );\n+  }\n+\n+  if (error) {\n+    return (\n+      <div className=\"h-full p-4\">\n+        <ErrorState\n+          title=\"Failed to load library agents\"\n+          error={error}\n+          onRetry={refresh}\n+        />\n+      </div>\n+    );\n+  }\n+\n+  return (\n+    <div ref={scrollRef} className={scrollbarStyles}>\n+      <div className=\"w-full space-y-3 px-4 pb-4\">\n+        {agents.map((agent) => (\n+          <UGCAgentBlock\n+            key={agent.id}\n+            title={agent.name}\n+            edited_time={agent.updated_at}\n+            version={agent.graph_version}\n+            image_url={agent.image_url}\n+            onClick={() => {\n+              const block = convertLibraryAgentIntoBlock(agent);\n+              addNode(block);\n+            }}\n+          />\n+        ))}\n+        {loadingMore && hasMore && (\n+          <>\n+            {Array.from({ length: 3 }).map((_, index) => (\n+              <UGCAgentBlock.Skeleton key={`loading-${index}`} />\n+            ))}\n+          </>\n+        )}\n+      </div>\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/PaginatedBlocksContent.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/PaginatedBlocksContent.tsx\nnew file mode 100644\nindex 000000000000..a855bc94e13b\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/PaginatedBlocksContent.tsx\n@@ -0,0 +1,55 @@\n+import React from \"react\";\n+import { BlocksList } from \"./BlocksList\";\n+import { Block } from \"../Block\";\n+import { BlockRequest } from \"@/lib/autogpt-server-api\";\n+import { usePagination } from \"@/hooks/usePagination\";\n+import { ErrorState } from \"../ErrorState\";\n+import { scrollbarStyles } from \"@/components/styles/scrollbar\";\n+\n+interface PaginatedBlocksContentProps {\n+  blockRequest: BlockRequest;\n+  pageSize?: number;\n+}\n+\n+export const PaginatedBlocksContent: React.FC<PaginatedBlocksContentProps> = ({\n+  blockRequest,\n+  pageSize = 10,\n+}) => {\n+  const {\n+    data: blocks,\n+    loading,\n+    loadingMore,\n+    hasMore,\n+    error,\n+    scrollRef,\n+    refresh,\n+  } = usePagination({\n+    request: { apiType: \"blocks\", ...blockRequest },\n+    pageSize,\n+  });\n+\n+  if (error) {\n+    return (\n+      <div className=\"h-full w-full px-4 pb-4\">\n+        <ErrorState\n+          title=\"Failed to load blocks\"\n+          error={error}\n+          onRetry={refresh}\n+        />\n+      </div>\n+    );\n+  }\n+\n+  return (\n+    <div ref={scrollRef} className={scrollbarStyles}>\n+      <BlocksList blocks={blocks} loading={loading} />\n+      {loadingMore && hasMore && (\n+        <div className=\"w-full space-y-3 px-4 pb-4\">\n+          {Array.from({ length: 3 }).map((_, index) => (\n+            <Block.Skeleton key={`loading-${index}`} />\n+          ))}\n+        </div>\n+      )}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/PaginatedIntegrationList.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/PaginatedIntegrationList.tsx\nnew file mode 100644\nindex 000000000000..105d5729e41f\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/PaginatedIntegrationList.tsx\n@@ -0,0 +1,72 @@\n+import React from \"react\";\n+import { Integration } from \"../Integration\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import { usePagination } from \"@/hooks/usePagination\";\n+import { ErrorState } from \"../ErrorState\";\n+import { scrollbarStyles } from \"@/components/styles/scrollbar\";\n+\n+export const PaginatedIntegrationList = () => {\n+  const { setIntegration } = useBlockMenuContext();\n+  const {\n+    data: providers,\n+    loading,\n+    loadingMore,\n+    hasMore,\n+    error,\n+    scrollRef,\n+    refresh,\n+  } = usePagination({\n+    request: { apiType: \"providers\" },\n+    pageSize: 10,\n+  });\n+\n+  if (loading) {\n+    return (\n+      <div ref={scrollRef} className={scrollbarStyles}>\n+        <div className=\"w-full space-y-3 px-4 pb-4\">\n+          {Array.from({ length: 6 }).map((_, integrationIndex) => (\n+            <Integration.Skeleton key={integrationIndex} />\n+          ))}\n+        </div>\n+      </div>\n+    );\n+  }\n+\n+  if (error) {\n+    return (\n+      <div className=\"h-full p-4\">\n+        <ErrorState\n+          title=\"Failed to load integrations\"\n+          error={error}\n+          onRetry={refresh}\n+        />\n+      </div>\n+    );\n+  }\n+\n+  return (\n+    <div ref={scrollRef} className={scrollbarStyles}>\n+      <div className=\"w-full px-4 pb-4\">\n+        <div className=\"space-y-3\">\n+          {providers.map((integration, index) => (\n+            <Integration\n+              key={index}\n+              title={integration.name}\n+              icon_url={`/integrations/${integration.name}.png`}\n+              description={integration.description}\n+              number_of_blocks={integration.integration_count}\n+              onClick={() => setIntegration(integration.name)}\n+            />\n+          ))}\n+          {loadingMore && hasMore && (\n+            <>\n+              {Array.from({ length: 3 }).map((_, index) => (\n+                <Integration.Skeleton key={`loading-${index}`} />\n+              ))}\n+            </>\n+          )}\n+        </div>\n+      </div>\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/default/SuggestionContent.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/default/SuggestionContent.tsx\nnew file mode 100644\nindex 000000000000..125fb8484117\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/default/SuggestionContent.tsx\n@@ -0,0 +1,113 @@\n+import React, { useCallback, useEffect, useState } from \"react\";\n+import { IntegrationChip } from \"../IntegrationChip\";\n+import { Block } from \"../Block\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import {\n+  CredentialsProviderName,\n+  SuggestionsResponse,\n+} from \"@/lib/autogpt-server-api\";\n+import { useBackendAPI } from \"@/lib/autogpt-server-api/context\";\n+import { ErrorState } from \"../ErrorState\";\n+import { scrollbarStyles } from \"@/components/styles/scrollbar\";\n+\n+export const SuggestionContent = () => {\n+  const { setIntegration, setDefaultState, addNode } = useBlockMenuContext();\n+\n+  const [suggestionsData, setSuggestionsData] =\n+    useState<SuggestionsResponse | null>(null);\n+  const [loading, setLoading] = useState<boolean>(true);\n+  const [error, setError] = useState<string | null>(null);\n+\n+  const api = useBackendAPI();\n+\n+  const fetchSuggestions = useCallback(async () => {\n+    try {\n+      setLoading(true);\n+      setError(null);\n+      const response = await api.getSuggestions();\n+      setSuggestionsData(response);\n+    } catch (err) {\n+      console.error(\"Error fetching data:\", err);\n+      setError(\n+        err instanceof Error ? err.message : \"Failed to load suggestions\",\n+      );\n+    } finally {\n+      setLoading(false);\n+    }\n+  }, [api]);\n+\n+  useEffect(() => {\n+    fetchSuggestions();\n+  }, [fetchSuggestions]);\n+\n+  if (error) {\n+    return (\n+      <div className=\"h-full p-4\">\n+        <ErrorState\n+          title=\"Failed to load suggestions\"\n+          error={error}\n+          onRetry={fetchSuggestions}\n+        />\n+      </div>\n+    );\n+  }\n+\n+  return (\n+    <div className={scrollbarStyles}>\n+      <div className=\"w-full space-y-6 pb-4\">\n+        {/* Integrations */}\n+        <div className=\"space-y-2.5 px-4\">\n+          <p className=\"font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\">\n+            Integrations\n+          </p>\n+          <div className=\"grid grid-cols-3 grid-rows-2 gap-2\">\n+            {!loading && suggestionsData\n+              ? suggestionsData.providers.map((provider, index) => (\n+                  <IntegrationChip\n+                    key={`integration-${index}`}\n+                    icon_url={`/integrations/${provider}.png`}\n+                    name={provider}\n+                    onClick={() => {\n+                      setDefaultState(\"integrations\");\n+                      setIntegration(provider as CredentialsProviderName);\n+                    }}\n+                  />\n+                ))\n+              : Array(6)\n+                  .fill(0)\n+                  .map((_, index) => (\n+                    <IntegrationChip.Skeleton\n+                      key={`integration-skeleton-${index}`}\n+                    />\n+                  ))}\n+          </div>\n+        </div>\n+\n+        {/* Top blocks */}\n+        <div className=\"space-y-2.5 px-4\">\n+          <p className=\"font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\">\n+            Top blocks\n+          </p>\n+          <div className=\"space-y-2\">\n+            {!loading && suggestionsData\n+              ? suggestionsData.top_blocks.map((block, index) => (\n+                  <Block\n+                    key={`block-${index}`}\n+                    title={block.name}\n+                    description={block.description}\n+                    onClick={() => {\n+                      addNode(block);\n+                    }}\n+                  />\n+                ))\n+              : Array(3)\n+                  .fill(0)\n+                  .map((_, index) => (\n+                    <Block.Skeleton key={`block-skeleton-${index}`} />\n+                  ))}\n+          </div>\n+        </div>\n+      </div>\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/helpers.ts b/autogpt_platform/frontend/src/components/builder/block-menu/helpers.ts\nnew file mode 100644\nindex 000000000000..eeb2e1f87f9c\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/helpers.ts\n@@ -0,0 +1,12 @@\n+import { Filters } from \"./block-menu-provider\";\n+\n+export const getDefaultFilters = (): Filters => ({\n+  categories: {\n+    blocks: false,\n+    integrations: false,\n+    marketplace_agents: false,\n+    my_agents: false,\n+    providers: false,\n+  },\n+  createdBy: [],\n+});\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/AiBlock.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/AiBlock.tsx\nnew file mode 100644\nindex 000000000000..8b43f2830665\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/AiBlock.tsx\n@@ -0,0 +1,63 @@\n+import { Button } from \"@/components/ui/button\";\n+import { cn } from \"@/lib/utils\";\n+import { Plus } from \"lucide-react\";\n+import { ButtonHTMLAttributes } from \"react\";\n+\n+interface Props extends ButtonHTMLAttributes<HTMLButtonElement> {\n+  title?: string;\n+  description?: string;\n+  ai_name?: string;\n+}\n+\n+export const AiBlock: React.FC<Props> = ({\n+  title,\n+  description,\n+  className,\n+  ai_name,\n+  ...rest\n+}) => {\n+  return (\n+    <Button\n+      className={cn(\n+        \"group flex h-[5.625rem] w-full min-w-[7.5rem] items-center justify-start space-x-3 whitespace-normal rounded-[0.75rem] bg-zinc-50 px-[0.875rem] py-[0.625rem] text-start shadow-none\",\n+        \"hover:bg-zinc-100 focus:ring-0 active:bg-zinc-100 active:ring-1 active:ring-zinc-300 disabled:pointer-events-none\",\n+        className,\n+      )}\n+      {...rest}\n+    >\n+      <div className=\"flex flex-1 flex-col items-start gap-1.5\">\n+        <div className=\"space-y-0.5\">\n+          <span\n+            className={cn(\n+              \"line-clamp-1 font-sans text-sm font-medium leading-[1.375rem] text-zinc-700 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            {title}\n+          </span>\n+          <span\n+            className={cn(\n+              \"line-clamp-1 font-sans text-xs font-normal leading-5 text-zinc-500 group-disabled:text-zinc-400\",\n+            )}\n+          >\n+            {description}\n+          </span>\n+        </div>\n+\n+        <span\n+          className={cn(\n+            \"rounded-[0.75rem] bg-zinc-200 px-[0.5rem] font-sans text-xs leading-[1.25rem] text-zinc-500\",\n+          )}\n+        >\n+          Supports {ai_name}\n+        </span>\n+      </div>\n+      <div\n+        className={cn(\n+          \"flex h-7 w-7 items-center justify-center rounded-[0.5rem] bg-zinc-700 group-disabled:bg-zinc-400\",\n+        )}\n+      >\n+        <Plus className=\"h-5 w-5 text-zinc-50\" strokeWidth={2} />\n+      </div>\n+    </Button>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/BlockMenuSearch.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/BlockMenuSearch.tsx\nnew file mode 100644\nindex 000000000000..21edbaf5f569\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/BlockMenuSearch.tsx\n@@ -0,0 +1,144 @@\n+import React, { useEffect, useState, useCallback, useRef } from \"react\";\n+import { FiltersList } from \"./FiltersList\";\n+import { SearchList } from \"./SearchList\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import { useBackendAPI } from \"@/lib/autogpt-server-api/context\";\n+import { cn } from \"@/lib/utils\";\n+import { scrollbarStyles } from \"@/components/styles/scrollbar\";\n+\n+export const BlockMenuSearch = () => {\n+  const {\n+    searchData,\n+    searchQuery,\n+    searchId,\n+    setSearchData,\n+    filters,\n+    setCategoryCounts,\n+  } = useBlockMenuContext();\n+  const [isLoading, setIsLoading] = useState<boolean>(false);\n+  const [hasMore, setHasMore] = useState<boolean>(true);\n+  const [page, setPage] = useState<number>(1);\n+  const [loadingMore, setLoadingMore] = useState<boolean>(false);\n+  const [error, setError] = useState<string | null>(null);\n+  const scrollRef = useRef<HTMLDivElement>(null);\n+  const api = useBackendAPI();\n+\n+  const pageSize = 10;\n+\n+  const fetchSearchData = useCallback(\n+    async (pageNum: number, isLoadMore: boolean = false) => {\n+      if (isLoadMore) {\n+        setLoadingMore(true);\n+      } else {\n+        setIsLoading(true);\n+      }\n+\n+      try {\n+        const activeCategories = Object.entries(filters.categories)\n+          .filter(([_, isActive]) => isActive)\n+          .map(([category, _]) => category)\n+          .map(\n+            (category) =>\n+              category as\n+                | \"blocks\"\n+                | \"integrations\"\n+                | \"marketplace_agents\"\n+                | \"my_agents\",\n+          );\n+\n+        const response = await api.searchBlocks({\n+          search_query: searchQuery,\n+          search_id: searchId,\n+          page: pageNum,\n+          page_size: pageSize,\n+          filter: activeCategories.length > 0 ? activeCategories : undefined,\n+          by_creator:\n+            filters.createdBy.length > 0 ? filters.createdBy : undefined,\n+        });\n+\n+        setCategoryCounts(response.total_items);\n+\n+        if (isLoadMore) {\n+          setSearchData((prev) => [...prev, ...response.items]);\n+        } else {\n+          setSearchData(response.items);\n+        }\n+\n+        setHasMore(response.more_pages);\n+        setError(null);\n+      } catch (error) {\n+        console.error(\"Error fetching search data:\", error);\n+        setError(\n+          error instanceof Error\n+            ? error.message\n+            : \"Failed to load search results\",\n+        );\n+        if (!isLoadMore) {\n+          setPage(1);\n+        }\n+      } finally {\n+        setIsLoading(false);\n+        setLoadingMore(false);\n+      }\n+    },\n+    [\n+      searchQuery,\n+      searchId,\n+      filters,\n+      api,\n+      setCategoryCounts,\n+      setSearchData,\n+      pageSize,\n+    ],\n+  );\n+\n+  const handleScroll = useCallback(() => {\n+    if (!scrollRef.current || loadingMore || !hasMore) return;\n+\n+    const { scrollTop, scrollHeight, clientHeight } = scrollRef.current;\n+    if (scrollTop + clientHeight >= scrollHeight - 100) {\n+      const nextPage = page + 1;\n+      setPage(nextPage);\n+      fetchSearchData(nextPage, true);\n+    }\n+  }, [loadingMore, hasMore, page, fetchSearchData]);\n+\n+  useEffect(() => {\n+    const scrollElement = scrollRef.current;\n+    if (scrollElement) {\n+      scrollElement.addEventListener(\"scroll\", handleScroll);\n+      return () => scrollElement.removeEventListener(\"scroll\", handleScroll);\n+    }\n+  }, [handleScroll]);\n+\n+  useEffect(() => {\n+    if (searchQuery) {\n+      setPage(1);\n+      setHasMore(true);\n+      setError(null);\n+      fetchSearchData(1, false);\n+    } else {\n+      setSearchData([]);\n+      setError(null);\n+      setPage(1);\n+      setHasMore(true);\n+    }\n+  }, [searchQuery, searchId, filters, fetchSearchData, setSearchData]);\n+\n+  return (\n+    <div ref={scrollRef} className={cn(scrollbarStyles, \"space-y-4 py-4\")}>\n+      {searchData.length !== 0 && <FiltersList />}\n+      <SearchList\n+        isLoading={isLoading}\n+        loadingMore={loadingMore}\n+        hasMore={hasMore}\n+        error={error}\n+        onRetry={() => {\n+          setPage(1);\n+          setError(null);\n+          fetchSearchData(1, false);\n+        }}\n+      />\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/FilterSheet.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/FilterSheet.tsx\nnew file mode 100644\nindex 000000000000..9a1ba5d4ca15\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/FilterSheet.tsx\n@@ -0,0 +1,255 @@\n+import { FilterChip } from \"../FilterChip\";\n+import { useState, useEffect } from \"react\";\n+import { Button } from \"@/components/ui/button\";\n+import { X } from \"lucide-react\";\n+import { cn, getBlockType } from \"@/lib/utils\";\n+import { Separator } from \"@/components/ui/separator\";\n+import { Checkbox } from \"@/components/ui/checkbox\";\n+import {\n+  CategoryKey,\n+  Filters,\n+  useBlockMenuContext,\n+} from \"../block-menu-provider\";\n+import { StoreAgent } from \"@/lib/autogpt-server-api\";\n+import { getDefaultFilters } from \"../helpers\";\n+import { scrollbarStyles } from \"@/components/styles/scrollbar\";\n+\n+const INITIAL_CREATORS_TO_SHOW = 5;\n+\n+export function FilterSheet({\n+  categories,\n+}: {\n+  categories: Array<{ key: CategoryKey; name: string }>;\n+}) {\n+  const { filters, setFilters, searchData } = useBlockMenuContext();\n+  const [isOpen, setIsOpen] = useState(false);\n+  const [isSheetVisible, setIsSheetVisible] = useState(false);\n+  const [localFilters, setLocalFilters] = useState<Filters>(filters);\n+\n+  const [creators, setCreators] = useState<string[]>([]);\n+  const [displayedCreatorsCount, setDisplayedCreatorsCount] = useState(\n+    INITIAL_CREATORS_TO_SHOW,\n+  );\n+\n+  useEffect(() => {\n+    if (isOpen) {\n+      setIsSheetVisible(true);\n+      setLocalFilters(filters);\n+      setDisplayedCreatorsCount(INITIAL_CREATORS_TO_SHOW); // Reset on open\n+\n+      const marketplaceAgents = (searchData?.filter(\n+        (item) => getBlockType(item) === \"store_agent\",\n+      ) || []) as StoreAgent[];\n+\n+      const uniqueCreators = Array.from(\n+        new Set(marketplaceAgents.map((agent) => agent.creator)),\n+      );\n+\n+      setCreators(uniqueCreators);\n+    } else {\n+      const timer = setTimeout(() => {\n+        setIsSheetVisible(false);\n+      }, 300);\n+      return () => clearTimeout(timer);\n+    }\n+  }, [isOpen, filters, searchData]);\n+\n+  const onCategoryChange = (category: CategoryKey) => {\n+    setLocalFilters((prev) => ({\n+      ...prev,\n+      categories: {\n+        ...prev.categories,\n+        [category]: !prev.categories[category],\n+      },\n+    }));\n+  };\n+\n+  const onCreatorChange = (creator: string) => {\n+    setLocalFilters((prev) => {\n+      const updatedCreators = prev.createdBy.includes(creator)\n+        ? prev.createdBy.filter((c) => c !== creator)\n+        : [...prev.createdBy, creator];\n+\n+      return {\n+        ...prev,\n+        createdBy: updatedCreators,\n+      };\n+    });\n+  };\n+\n+  const handleApplyFilters = () => {\n+    setFilters(localFilters);\n+    setIsOpen(false);\n+  };\n+\n+  const handleClearFilters = () => {\n+    const clearedFilters: Filters = getDefaultFilters();\n+    setFilters(clearedFilters);\n+    setIsOpen(false);\n+  };\n+\n+  const hasLocalActiveFilters = () => {\n+    const hasCategoryFilter = Object.values(localFilters.categories).some(\n+      (value) => value,\n+    );\n+    const hasCreatorFilter = localFilters.createdBy.length > 0;\n+\n+    return hasCategoryFilter || hasCreatorFilter;\n+  };\n+\n+  const hasActiveFilters = () => {\n+    const hasCategoryFilter = Object.values(filters.categories).some(\n+      (value) => value,\n+    );\n+    const hasCreatorFilter = filters.createdBy.length > 0;\n+\n+    return hasCategoryFilter || hasCreatorFilter;\n+  };\n+\n+  const handleToggleShowMoreCreators = () => {\n+    if (displayedCreatorsCount < creators.length) {\n+      setDisplayedCreatorsCount(creators.length);\n+    } else {\n+      setDisplayedCreatorsCount(INITIAL_CREATORS_TO_SHOW);\n+    }\n+  };\n+\n+  const visibleCreators = creators.slice(0, displayedCreatorsCount);\n+\n+  return (\n+    <div className=\"m-0 ml-4 inline w-fit p-0\">\n+      <Button\n+        onClick={() => {\n+          setIsSheetVisible(true);\n+          requestAnimationFrame(() => {\n+            requestAnimationFrame(() => {\n+              setIsOpen(true);\n+            });\n+          });\n+        }}\n+        variant={\"link\"}\n+        className=\"m-0 p-0 hover:no-underline\"\n+      >\n+        <FilterChip\n+          name={hasActiveFilters() ? \"Edit filters\" : \"All filters\"}\n+        />\n+      </Button>\n+\n+      {isSheetVisible && (\n+        <>\n+          <div\n+            className={cn(\n+              \"absolute bottom-2 left-2 top-2 z-20 w-3/4 max-w-[22.5rem] space-y-4 overflow-hidden rounded-[0.75rem] bg-white pb-4 shadow-[0_4px_12px_2px_rgba(0,0,0,0.1)] transition-all\",\n+              isOpen\n+                ? \"translate-x-0 duration-300 ease-out\"\n+                : \"-translate-x-full duration-300 ease-out\",\n+            )}\n+          >\n+            <div className={cn(\"flex-1 space-y-4 pb-16\", scrollbarStyles)}>\n+              {/* Top */}\n+              <div className=\"flex items-center justify-between px-5\">\n+                <p className=\"font-sans text-base text-[#040404]\">Filters</p>\n+                <Button\n+                  variant=\"ghost\"\n+                  size=\"icon\"\n+                  onClick={() => setIsOpen(false)}\n+                >\n+                  <X className=\"h-5 w-5\" />\n+                </Button>\n+              </div>\n+\n+              <Separator className=\"h-[1px] w-full text-zinc-300\" />\n+\n+              {/* Categories */}\n+\n+              <div className=\"space-y-4 px-5\">\n+                <p className=\"font-sans text-base font-medium text-zinc-800\">\n+                  Categories\n+                </p>\n+                <div className=\"space-y-2\">\n+                  {categories.map((category) => (\n+                    <div\n+                      key={category.key}\n+                      className=\"flex items-center space-x-2\"\n+                    >\n+                      <Checkbox\n+                        id={category.key}\n+                        checked={localFilters.categories[category.key]}\n+                        onCheckedChange={() => onCategoryChange(category.key)}\n+                        className=\"border border-[#D4D4D4] shadow-none data-[state=checked]:border-none data-[state=checked]:bg-violet-700 data-[state=checked]:text-white\"\n+                      />\n+                      <label\n+                        htmlFor={category.key}\n+                        className=\"font-sans text-sm leading-[1.375rem] text-zinc-600\"\n+                      >\n+                        {category.name}\n+                      </label>\n+                    </div>\n+                  ))}\n+                </div>\n+              </div>\n+\n+              <Separator className=\"h-[1px] w-full text-zinc-300\" />\n+\n+              {/* Created By */}\n+\n+              <div className=\"space-y-4 px-5\">\n+                <p className=\"font-sans text-base font-medium text-zinc-800\">\n+                  Created by\n+                </p>\n+                <div className=\"space-y-2\">\n+                  {visibleCreators.map((creator) => (\n+                    <div key={creator} className=\"flex items-center space-x-2\">\n+                      <Checkbox\n+                        id={`creator-${creator}`}\n+                        checked={localFilters.createdBy.includes(creator)}\n+                        onCheckedChange={() => onCreatorChange(creator)}\n+                        className=\"border border-[#D4D4D4] shadow-none data-[state=checked]:border-none data-[state=checked]:bg-violet-700 data-[state=checked]:text-white\"\n+                      />\n+                      <label\n+                        htmlFor={`creator-${creator}`}\n+                        className=\"font-sans text-sm leading-[1.375rem] text-zinc-600\"\n+                      >\n+                        {creator}\n+                      </label>\n+                    </div>\n+                  ))}\n+                </div>\n+                {creators.length > INITIAL_CREATORS_TO_SHOW && (\n+                  <Button\n+                    variant={\"link\"}\n+                    className=\"m-0 p-0 font-sans text-sm font-medium leading-[1.375rem] text-zinc-800 underline hover:text-zinc-600\"\n+                    onClick={handleToggleShowMoreCreators}\n+                  >\n+                    {displayedCreatorsCount < creators.length ? \"More\" : \"Less\"}\n+                  </Button>\n+                )}\n+              </div>\n+            </div>\n+\n+            {/* Footer buttons */}\n+            <div className=\"fixed bottom-0 flex w-full justify-between gap-3 border-t border-zinc-300 bg-white px-5 py-3\">\n+              <Button\n+                className=\"min-w-[5rem] rounded-[0.5rem] border-none px-1.5 py-2 font-sans text-sm font-medium leading-[1.375rem] text-zinc-800 shadow-none ring-1 ring-zinc-400\"\n+                variant={\"outline\"}\n+                onClick={handleClearFilters}\n+              >\n+                Clear\n+              </Button>\n+\n+              <Button\n+                className={cn(\n+                  \"min-w-[6.25rem] rounded-[0.5rem] border-none px-1.5 py-2 font-sans text-sm font-medium leading-[1.375rem] text-white shadow-none ring-1 disabled:ring-0\",\n+                )}\n+                onClick={handleApplyFilters}\n+                disabled={!hasLocalActiveFilters()}\n+              >\n+                Apply filters\n+              </Button>\n+            </div>\n+          </div>\n+        </>\n+      )}\n+    </div>\n+  );\n+}\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/FiltersList.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/FiltersList.tsx\nnew file mode 100644\nindex 000000000000..da034fcc9f52\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/FiltersList.tsx\n@@ -0,0 +1,63 @@\n+import { useCallback } from \"react\";\n+import { FilterChip } from \"../FilterChip\";\n+import { FilterSheet } from \"./FilterSheet\";\n+import { CategoryKey, useBlockMenuContext } from \"../block-menu-provider\";\n+\n+export const FiltersList = () => {\n+  const { filters, setFilters, categoryCounts } = useBlockMenuContext();\n+  const categories: Array<{ key: CategoryKey; name: string }> = [\n+    { key: \"blocks\", name: \"Blocks\" },\n+    { key: \"integrations\", name: \"Integrations\" },\n+    { key: \"marketplace_agents\", name: \"Marketplace agents\" },\n+    { key: \"my_agents\", name: \"My agents\" },\n+  ];\n+\n+  const handleCategoryFilter = (category: CategoryKey) => {\n+    setFilters({\n+      ...filters,\n+      categories: {\n+        ...filters.categories,\n+        [category]: !filters.categories[category],\n+      },\n+    });\n+  };\n+\n+  const handleCreatorFilter = useCallback(\n+    (creator: string) => {\n+      const updatedCreators = filters.createdBy.includes(creator)\n+        ? filters.createdBy.filter((c) => c !== creator)\n+        : [...filters.createdBy, creator];\n+\n+      setFilters({\n+        ...filters,\n+        createdBy: updatedCreators,\n+      });\n+    },\n+    [filters, setFilters],\n+  );\n+\n+  return (\n+    <div className=\"flex flex-nowrap gap-3 overflow-x-auto scrollbar-hide\">\n+      <FilterSheet categories={categories} />\n+\n+      {filters.createdBy.map((creator) => (\n+        <FilterChip\n+          key={creator}\n+          name={\"Created by \" + creator}\n+          selected={true}\n+          onClick={() => handleCreatorFilter(creator)}\n+        />\n+      ))}\n+\n+      {categories.map((category) => (\n+        <FilterChip\n+          key={category.key}\n+          name={category.name}\n+          number={categoryCounts[category.key]}\n+          selected={filters.categories[category.key]}\n+          onClick={() => handleCategoryFilter(category.key)}\n+        />\n+      ))}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/NoSearchResult.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/NoSearchResult.tsx\nnew file mode 100644\nindex 000000000000..726e3804a1a1\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/NoSearchResult.tsx\n@@ -0,0 +1,17 @@\n+import { Frown } from \"lucide-react\";\n+\n+export const NoSearchResult = () => {\n+  return (\n+    <div className=\"flex h-full w-full flex-col items-center justify-center text-center\">\n+      <Frown className=\"mb-10 h-16 w-16 text-zinc-400\" strokeWidth={1} />\n+      <div className=\"space-y-1\">\n+        <p className=\"font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\">\n+          No match found\n+        </p>\n+        <p className=\"font-sans text-sm font-normal leading-[1.375rem] text-zinc-600\">\n+          Try adjusting your search terms\n+        </p>\n+      </div>\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/SearchList.tsx b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/SearchList.tsx\nnew file mode 100644\nindex 000000000000..aaad3153b454\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/builder/block-menu/search-and-filter/SearchList.tsx\n@@ -0,0 +1,171 @@\n+import React from \"react\";\n+import { MarketplaceAgentBlock } from \"../MarketplaceAgentBlock\";\n+import { Block } from \"../Block\";\n+import { UGCAgentBlock } from \"../UGCAgentBlock\";\n+import { AiBlock } from \"./AiBlock\";\n+import { IntegrationBlock } from \"../IntegrationBlock\";\n+import { useBlockMenuContext } from \"../block-menu-provider\";\n+import { NoSearchResult } from \"./NoSearchResult\";\n+import { Button } from \"@/components/ui/button\";\n+import { convertLibraryAgentIntoBlock, getBlockType } from \"@/lib/utils\";\n+\n+interface SearchListProps {\n+  isLoading: boolean;\n+  loadingMore: boolean;\n+  hasMore: boolean;\n+  error: string | null;\n+  onRetry: () => void;\n+}\n+\n+export const SearchList: React.FC<SearchListProps> = ({\n+  isLoading,\n+  loadingMore,\n+  hasMore,\n+  error,\n+  onRetry,\n+}) => {\n+  const { searchQuery, addNode, loadingSlug, searchData, handleAddStoreAgent } =\n+    useBlockMenuContext();\n+\n+  if (isLoading) {\n+    return (\n+      <div className=\"space-y-2.5 px-4\">\n+        <p className=\"font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\">\n+          Search results\n+        </p>\n+        {Array(6)\n+          .fill(0)\n+          .map((_, i) => (\n+            <Block.Skeleton key={i} />\n+          ))}\n+      </div>\n+    );\n+  }\n+\n+  if (error) {\n+    return (\n+      <div className=\"px-4\">\n+        <div className=\"rounded-lg border border-red-200 bg-red-50 p-3\">\n+          <p className=\"mb-2 text-sm text-red-600\">\n+            Error loading search results: {error}\n+          </p>\n+          <Button\n+            variant=\"outline\"\n+            size=\"sm\"\n+            onClick={onRetry}\n+            className=\"h-7 text-xs\"\n+          >\n+            Retry\n+          </Button>\n+        </div>\n+      </div>\n+    );\n+  }\n+\n+  if (searchData.length === 0) {\n+    return <NoSearchResult />;\n+  }\n+\n+  return (\n+    <div className=\"space-y-2.5 px-4\">\n+      <p className=\"font-sans text-sm font-medium leading-[1.375rem] text-zinc-800\">\n+        Search results\n+      </p>\n+      {searchData.map((item: any, index: number) => {\n+        const blockType = getBlockType(item);\n+\n+        switch (blockType) {\n+          case \"store_agent\":\n+            return (\n+              <MarketplaceAgentBlock\n+                key={index}\n+                slug={item.slug}\n+                highlightedText={searchQuery}\n+                title={item.agent_name}\n+                image_url={item.agent_image}\n+                creator_name={item.creator}\n+                number_of_runs={item.runs}\n+                loading={loadingSlug == item.slug}\n+                onClick={() =>\n+                  handleAddStoreAgent({\n+                    creator_name: item.creator,\n+                    slug: item.slug,\n+                  })\n+                }\n+              />\n+            );\n+          case \"block\":\n+            return (\n+              <Block\n+                key={index}\n+                title={item.name}\n+                highlightedText={searchQuery}\n+                description={item.description}\n+                onClick={() => {\n+                  addNode(item);\n+                }}\n+              />\n+            );\n+          case \"provider\":\n+            return (\n+              <IntegrationBlock\n+                key={index}\n+                title={item.name}\n+                highlightedText={searchQuery}\n+                icon_url={`/integrations/${item.name}.png`}\n+                description={item.description}\n+                onClick={() => {\n+                  addNode(item);\n+                }}\n+              />\n+            );\n+          case \"library_agent\":\n+            return (\n+              <UGCAgentBlock\n+                key={index}\n+                title={item.name}\n+                highlightedText={searchQuery}\n+                image_url={item.image_url}\n+                version={item.graph_version}\n+                edited_time={item.updated_at}\n+                onClick={() => {\n+                  const block = convertLibraryAgentIntoBlock(item);\n+                  addNode(block);\n+                }}\n+              />\n+            );\n+          case \"ai_agent\":\n+            return (\n+              <AiBlock\n+                key={index}\n+                title={item.name}\n+                description={item.description}\n+                ai_name={item.inputSchema.properties.model.enum.find(\n+                  (model: string) =>\n+                    model\n+                      .toLowerCase()\n+                      .includes(searchQuery.toLowerCase().trim()),\n+                )}\n+                onClick={() => {\n+                  const block = convertLibraryAgentIntoBlock(item);\n+                  addNode(block);\n+                }}\n+              />\n+            );\n+\n+          default:\n+            return null;\n+        }\n+      })}\n+      {loadingMore && hasMore && (\n+        <div className=\"space-y-2.5\">\n+          {Array(3)\n+            .fill(0)\n+            .map((_, i) => (\n+              <Block.Skeleton key={`loading-more-${i}`} />\n+            ))}\n+        </div>\n+      )}\n+    </div>\n+  );\n+};\ndiff --git a/autogpt_platform/frontend/src/components/edit/control/ControlPanel.tsx b/autogpt_platform/frontend/src/components/edit/control/ControlPanel.tsx\nindex 870d34582283..6df5d191f95c 100644\n--- a/autogpt_platform/frontend/src/components/edit/control/ControlPanel.tsx\n+++ b/autogpt_platform/frontend/src/components/edit/control/ControlPanel.tsx\n@@ -1,13 +1,7 @@\n-import { Card, CardContent } from \"@/components/ui/card\";\n-import {\n-  Tooltip,\n-  TooltipContent,\n-  TooltipTrigger,\n-} from \"@/components/ui/tooltip\";\n-import { Button } from \"@/components/ui/button\";\n import { Separator } from \"@/components/ui/separator\";\n import { cn } from \"@/lib/utils\";\n import React from \"react\";\n+import { ControlPanelButton } from \"@/components/builder/block-menu/ControlPanelButton\";\n \n /**\n  * Represents a control element for the ControlPanel Component.\n@@ -27,6 +21,7 @@ interface ControlPanelProps {\n   controls: Control[];\n   topChildren?: React.ReactNode;\n   botChildren?: React.ReactNode;\n+\n   className?: string;\n }\n \n@@ -45,42 +40,31 @@ export const ControlPanel = ({\n   className,\n }: ControlPanelProps) => {\n   return (\n-    <Card className={cn(\"m-4 mt-24 w-14 dark:bg-slate-900\", className)}>\n-      <CardContent className=\"p-0\">\n-        <div className=\"flex flex-col items-center gap-3 rounded-xl py-3\">\n-          {topChildren}\n-          <Separator className=\"dark:bg-slate-700\" />\n-          {controls.map((control, index) => (\n-            <Tooltip key={index} delayDuration={500}>\n-              <TooltipTrigger asChild>\n-                <div>\n-                  <Button\n-                    variant=\"ghost\"\n-                    size=\"icon\"\n-                    onClick={() => control.onClick()}\n-                    data-id={`control-button-${index}`}\n-                    data-testid={`blocks-control-${control.label.toLowerCase()}-button`}\n-                    disabled={control.disabled || false}\n-                    className=\"dark:bg-slate-900 dark:text-slate-100 dark:hover:bg-slate-800\"\n-                  >\n-                    {control.icon}\n-                    <span className=\"sr-only\">{control.label}</span>\n-                  </Button>\n-                </div>\n-              </TooltipTrigger>\n-              <TooltipContent\n-                side=\"right\"\n-                className=\"dark:bg-slate-800 dark:text-slate-100\"\n-              >\n-                {control.label}\n-              </TooltipContent>\n-            </Tooltip>\n-          ))}\n-          <Separator className=\"dark:bg-slate-700\" />\n-          {botChildren}\n-        </div>\n-      </CardContent>\n-    </Card>\n+    <section\n+      className={cn(\n+        \"absolute left-4 top-24 z-10 w-[4.25rem] overflow-hidden rounded-[1rem] border-none bg-white p-0 shadow-[0_1px_5px_0_rgba(0,0,0,0.1)]\",\n+        className,\n+      )}\n+    >\n+      <div className=\"flex flex-col items-center justify-center rounded-[1rem] p-0\">\n+        {topChildren}\n+        <Separator className=\"text-[#E1E1E1]\" />\n+        {controls.map((control, index) => (\n+          <ControlPanelButton\n+            key={index}\n+            onClick={() => control.onClick()}\n+            data-id={`control-button-${index}`}\n+            data-testid={`blocks-control-${control.label.toLowerCase()}-button`}\n+            disabled={control.disabled || false}\n+            className=\"rounded-none\"\n+          >\n+            {control.icon}\n+          </ControlPanelButton>\n+        ))}\n+        <Separator className=\"text-[#E1E1E1]\" />\n+        {botChildren}\n+      </div>\n+    </section>\n   );\n };\n export default ControlPanel;\ndiff --git a/autogpt_platform/frontend/src/components/edit/control/SaveControl.tsx b/autogpt_platform/frontend/src/components/edit/control/SaveControl.tsx\nindex bfa0408dbad4..aeb4fc2cbef6 100644\n--- a/autogpt_platform/frontend/src/components/edit/control/SaveControl.tsx\n+++ b/autogpt_platform/frontend/src/components/edit/control/SaveControl.tsx\n@@ -10,12 +10,8 @@ import { Button } from \"@/components/ui/button\";\n import { GraphMeta } from \"@/lib/autogpt-server-api\";\n import { Label } from \"@/components/ui/label\";\n import { IconSave } from \"@/components/ui/icons\";\n-import {\n-  Tooltip,\n-  TooltipContent,\n-  TooltipTrigger,\n-} from \"@/components/ui/tooltip\";\n import { useToast } from \"@/components/ui/use-toast\";\n+import { ControlPanelButton } from \"@/components/builder/block-menu/ControlPanelButton\";\n \n interface SaveControlProps {\n   agentMeta: GraphMeta | null;\n@@ -26,6 +22,11 @@ interface SaveControlProps {\n   onNameChange: (name: string) => void;\n   onDescriptionChange: (description: string) => void;\n   pinSavePopover: boolean;\n+\n+  blockMenuSelected: \"save\" | \"block\" | \"\";\n+  setBlockMenuSelected: React.Dispatch<\n+    React.SetStateAction<\"\" | \"save\" | \"block\">\n+  >;\n }\n \n /**\n@@ -48,6 +49,8 @@ export const SaveControl = ({\n   onNameChange,\n   agentDescription,\n   onDescriptionChange,\n+  blockMenuSelected,\n+  setBlockMenuSelected,\n   pinSavePopover,\n }: SaveControlProps) => {\n   /**\n@@ -82,27 +85,29 @@ export const SaveControl = ({\n   }, [handleSave, toast]);\n \n   return (\n-    <Popover open={pinSavePopover ? true : undefined}>\n-      <Tooltip delayDuration={500}>\n-        <TooltipTrigger asChild>\n-          <PopoverTrigger asChild>\n-            <Button\n-              variant=\"ghost\"\n-              size=\"icon\"\n-              data-id=\"save-control-popover-trigger\"\n-              data-testid=\"blocks-control-save-button\"\n-              name=\"Save\"\n-            >\n-              <IconSave className=\"dark:text-gray-300\" />\n-            </Button>\n-          </PopoverTrigger>\n-        </TooltipTrigger>\n-        <TooltipContent side=\"right\">Save</TooltipContent>\n-      </Tooltip>\n+    <Popover\n+      open={pinSavePopover ? true : undefined}\n+      onOpenChange={(open) => open || setBlockMenuSelected(\"\")}\n+    >\n+      <PopoverTrigger>\n+        <ControlPanelButton\n+          data-id=\"save-control-popover-trigger\"\n+          data-testid=\"blocks-control-save-button\"\n+          selected={blockMenuSelected === \"save\"}\n+          onClick={() => {\n+            setBlockMenuSelected(\"save\");\n+          }}\n+          className=\"rounded-none\"\n+        >\n+          <IconSave className=\"h-5 w-5\" strokeWidth={2} />\n+        </ControlPanelButton>\n+      </PopoverTrigger>\n+\n       <PopoverContent\n         side=\"right\"\n-        sideOffset={15}\n+        sideOffset={16}\n         align=\"start\"\n+        className=\"w-[17rem] rounded-xl border-none p-0 shadow-none md:w-[30rem]\"\n         data-id=\"save-control-popover-content\"\n       >\n         <Card className=\"border-none shadow-none dark:bg-slate-900\">\ndiff --git a/autogpt_platform/frontend/src/components/styles/scrollbar.ts b/autogpt_platform/frontend/src/components/styles/scrollbar.ts\nnew file mode 100644\nindex 000000000000..78a14ec7fb66\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/styles/scrollbar.ts\n@@ -0,0 +1,2 @@\n+export const scrollbarStyles =\n+  \"scrollbar-thumb-rounded h-full overflow-y-auto pt-4 transition-all duration-200 scrollbar-thin scrollbar-track-transparent scrollbar-thumb-transparent hover:scrollbar-thumb-zinc-200\";\ndiff --git a/autogpt_platform/frontend/src/components/ui/checkbox.tsx b/autogpt_platform/frontend/src/components/ui/checkbox.tsx\nindex 81d0befff4e2..5438d5946530 100644\n--- a/autogpt_platform/frontend/src/components/ui/checkbox.tsx\n+++ b/autogpt_platform/frontend/src/components/ui/checkbox.tsx\n@@ -13,7 +13,7 @@ const Checkbox = React.forwardRef<\n   <CheckboxPrimitive.Root\n     ref={ref}\n     className={cn(\n-      \"peer h-4 w-4 shrink-0 rounded-sm border border-neutral-200 border-neutral-900 shadow focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-neutral-950 disabled:cursor-not-allowed disabled:opacity-50 data-[state=checked]:bg-neutral-900 data-[state=checked]:text-neutral-50 dark:border-neutral-50 dark:border-neutral-800 dark:focus-visible:ring-neutral-300 dark:data-[state=checked]:bg-neutral-50 dark:data-[state=checked]:text-neutral-900\",\n+      \"peer h-4 w-4 shrink-0 rounded-sm border border-neutral-900 shadow focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-neutral-950 disabled:cursor-not-allowed disabled:opacity-50 data-[state=checked]:bg-neutral-900 data-[state=checked]:text-neutral-50 dark:border-neutral-50 dark:border-neutral-800 dark:focus-visible:ring-neutral-300 dark:data-[state=checked]:bg-neutral-50 dark:data-[state=checked]:text-neutral-900\",\n       className,\n     )}\n     {...props}\n@@ -21,7 +21,7 @@ const Checkbox = React.forwardRef<\n     <CheckboxPrimitive.Indicator\n       className={cn(\"flex items-center justify-center text-current\")}\n     >\n-      <CheckIcon className=\"h-4 w-4\" />\n+      <CheckIcon className=\"h-4 w-4\" strokeWidth={2} />\n     </CheckboxPrimitive.Indicator>\n   </CheckboxPrimitive.Root>\n ));\ndiff --git a/autogpt_platform/frontend/src/components/ui/multiselect.tsx b/autogpt_platform/frontend/src/components/ui/multiselect.tsx\nindex 876444262194..6c3243015a61 100644\n--- a/autogpt_platform/frontend/src/components/ui/multiselect.tsx\n+++ b/autogpt_platform/frontend/src/components/ui/multiselect.tsx\n@@ -256,7 +256,7 @@ const MultiSelectorList = forwardRef<\n     <CommandList\n       ref={ref}\n       className={cn(\n-        \"scrollbar-thin scrollbar-track-transparent scrollbar-thumb-muted-foreground dark:scrollbar-thumb-muted scrollbar-thumb-rounded-lg absolute top-0 z-10 flex w-full flex-col gap-2 rounded-md border border-muted bg-background p-2 shadow-md transition-colors\",\n+        \"scrollbar-thumb-rounded-lg absolute top-0 z-10 flex w-full flex-col gap-2 rounded-md border border-muted bg-background p-2 shadow-md transition-colors scrollbar-thin scrollbar-track-transparent scrollbar-thumb-muted-foreground dark:scrollbar-thumb-muted\",\n         className,\n       )}\n     >\ndiff --git a/autogpt_platform/frontend/src/hooks/index.ts b/autogpt_platform/frontend/src/hooks/index.ts\nnew file mode 100644\nindex 000000000000..40d59d0e50ce\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/hooks/index.ts\n@@ -0,0 +1 @@\n+export { usePagination } from \"./usePagination\";\ndiff --git a/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx b/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx\nindex 55b7c4d2bdcf..4c3d11153bba 100644\n--- a/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx\n+++ b/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx\n@@ -14,6 +14,7 @@ import BackendAPI, {\n   GraphMeta,\n   NodeExecutionResult,\n   SpecialBlockID,\n+  Node,\n } from \"@/lib/autogpt-server-api\";\n import {\n   deepEquals,\n@@ -177,6 +178,16 @@ export default function useAgentGraph(\n       setAgentName(graph.name);\n       setAgentDescription(graph.description);\n \n+      const getGraphName = (node: Node) => {\n+        if (node.input_default.agent_name) {\n+          return node.input_default.agent_name;\n+        }\n+        return (\n+          availableFlows.find((flow) => flow.id === node.input_default.graph_id)\n+            ?.name || null\n+        );\n+      };\n+\n       setNodes((prevNodes) => {\n         const _newNodes = graph.nodes.map((node) => {\n           const block = availableNodes.find(\n@@ -184,12 +195,8 @@ export default function useAgentGraph(\n           )!;\n           if (!block) return null;\n           const prevNode = prevNodes.find((n) => n.id === node.id);\n-          const flow =\n-            block.uiType == BlockUIType.AGENT\n-              ? availableFlows.find(\n-                  (flow) => flow.id === node.input_default.graph_id,\n-                )\n-              : null;\n+          const graphName =\n+            (block.uiType == BlockUIType.AGENT && getGraphName(node)) || null;\n           const newNode: CustomNode = {\n             id: node.id,\n             type: \"custom\",\n@@ -201,7 +208,7 @@ export default function useAgentGraph(\n               isOutputOpen: false,\n               ...prevNode?.data,\n               block_id: block.id,\n-              blockType: flow?.name || block.name,\n+              blockType: graphName || block.name,\n               blockCosts: block.costs,\n               categories: block.categories,\n               description: block.description,\n@@ -281,15 +288,17 @@ export default function useAgentGraph(\n \n   const getToolFuncName = (nodeId: string) => {\n     const sinkNode = nodes.find((node) => node.id === nodeId);\n-    const sinkNodeName = sinkNode\n-      ? sinkNode.data.block_id === SpecialBlockID.AGENT\n-        ? sinkNode.data.hardcodedValues?.graph_id\n-          ? availableFlows.find(\n-              (flow) => flow.id === sinkNode.data.hardcodedValues.graph_id,\n-            )?.name || \"agentexecutorblock\"\n-          : \"agentexecutorblock\"\n-        : sinkNode.data.title.split(\" \")[0]\n-      : \"\";\n+\n+    if (!sinkNode) return \"\";\n+\n+    const sinkNodeName =\n+      sinkNode.data.block_id === SpecialBlockID.AGENT\n+        ? sinkNode.data.hardcodedValues?.agent_name ||\n+          availableFlows.find(\n+            (flow) => flow.id === sinkNode.data.hardcodedValues.graph_id,\n+          )?.name ||\n+          \"agentexecutorblock\"\n+        : sinkNode.data.title.split(\" \")[0];\n \n     return sinkNodeName;\n   };\n@@ -1120,7 +1129,6 @@ export default function useAgentGraph(\n     setAgentDescription,\n     savedAgent,\n     availableNodes,\n-    availableFlows,\n     getOutputType,\n     requestSave,\n     requestSaveAndRun,\ndiff --git a/autogpt_platform/frontend/src/hooks/usePagination.ts b/autogpt_platform/frontend/src/hooks/usePagination.ts\nnew file mode 100644\nindex 000000000000..bce8e9c96a3d\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/hooks/usePagination.ts\n@@ -0,0 +1,232 @@\n+import { useState, useCallback, useRef, useEffect } from \"react\";\n+import { useBackendAPI } from \"@/lib/autogpt-server-api/context\";\n+import {\n+  Block,\n+  BlockRequest,\n+  Provider,\n+  StoreAgent,\n+  LibraryAgent,\n+  LibraryAgentSortEnum,\n+} from \"@/lib/autogpt-server-api\";\n+\n+type BlocksPaginationRequest = { apiType: \"blocks\" } & BlockRequest;\n+type ProvidersPaginationRequest = { apiType: \"providers\" } & {\n+  page?: number;\n+  page_size?: number;\n+};\n+type StoreAgentsPaginationRequest = { apiType: \"store-agents\" } & {\n+  featured?: boolean;\n+  creator?: string;\n+  sorted_by?: string;\n+  search_query?: string;\n+  category?: string;\n+  page?: number;\n+  page_size?: number;\n+};\n+type LibraryAgentsPaginationRequest = { apiType: \"library-agents\" } & {\n+  search_term?: string;\n+  sort_by?: LibraryAgentSortEnum;\n+  page?: number;\n+  page_size?: number;\n+};\n+\n+type PaginationRequest =\n+  | BlocksPaginationRequest\n+  | ProvidersPaginationRequest\n+  | StoreAgentsPaginationRequest\n+  | LibraryAgentsPaginationRequest;\n+\n+interface UsePaginationOptions<T extends PaginationRequest> {\n+  request: T;\n+  pageSize?: number;\n+  enabled?: boolean;\n+}\n+\n+interface UsePaginationReturn<T> {\n+  data: T[];\n+  loading: boolean;\n+  loadingMore: boolean;\n+  hasMore: boolean;\n+  error: string | null;\n+  scrollRef: React.RefObject<HTMLDivElement>;\n+  refresh: () => void;\n+  loadMore: () => void;\n+}\n+\n+type GetReturnType<T> = T extends BlocksPaginationRequest\n+  ? Block\n+  : T extends ProvidersPaginationRequest\n+    ? Provider\n+    : T extends StoreAgentsPaginationRequest\n+      ? StoreAgent\n+      : T extends LibraryAgentsPaginationRequest\n+        ? LibraryAgent\n+        : never;\n+\n+export const usePagination = <T extends PaginationRequest>({\n+  request,\n+  pageSize = 10,\n+  enabled = true, // to allow pagination or not\n+}: UsePaginationOptions<T>): UsePaginationReturn<GetReturnType<T>> => {\n+  const [data, setData] = useState<GetReturnType<T>[]>([]);\n+  const [loading, setLoading] = useState(true);\n+  const [loadingMore, setLoadingMore] = useState(false);\n+  const [hasMore, setHasMore] = useState(true);\n+  const [currentPage, setCurrentPage] = useState(1);\n+  const [error, setError] = useState<string | null>(null);\n+  const scrollRef = useRef<HTMLDivElement>(null);\n+  const isLoadingRef = useRef(false);\n+  const requestRef = useRef(request);\n+  const api = useBackendAPI();\n+\n+  // because we are using this pagination for multiple components\n+  requestRef.current = request;\n+\n+  const fetchData = useCallback(\n+    async (page: number, isLoadMore = false) => {\n+      if (isLoadingRef.current || !enabled) return;\n+\n+      isLoadingRef.current = true;\n+\n+      if (isLoadMore) {\n+        setLoadingMore(true);\n+      } else {\n+        setLoading(true);\n+      }\n+\n+      setError(null);\n+\n+      try {\n+        let response;\n+        let newData: GetReturnType<T>[];\n+        let pagination;\n+\n+        const currentRequest = requestRef.current;\n+        const requestWithPagination = {\n+          ...currentRequest,\n+          page,\n+          page_size: pageSize,\n+        };\n+\n+        switch (currentRequest.apiType) {\n+          case \"blocks\":\n+            const { apiType: _, ...blockRequest } = requestWithPagination;\n+            response = await api.getBuilderBlocks(blockRequest);\n+            newData = response.blocks as GetReturnType<T>[];\n+            pagination = response.pagination;\n+            break;\n+\n+          case \"providers\":\n+            const { apiType: __, ...providerRequest } = requestWithPagination;\n+            response = await api.getProviders(providerRequest);\n+            newData = response.providers as GetReturnType<T>[];\n+            pagination = response.pagination;\n+            break;\n+\n+          case \"store-agents\":\n+            const { apiType: ___, ...storeAgentRequest } =\n+              requestWithPagination;\n+            response = await api.getStoreAgents(storeAgentRequest);\n+            newData = response.agents as GetReturnType<T>[];\n+            pagination = response.pagination;\n+            break;\n+\n+          case \"library-agents\":\n+            const { apiType: ____, ...libraryAgentRequest } =\n+              requestWithPagination;\n+            response = await api.listLibraryAgents(libraryAgentRequest);\n+            newData = response.agents as GetReturnType<T>[];\n+            pagination = response.pagination;\n+            break;\n+\n+          default:\n+            throw new Error(\n+              `Unknown request type: ${(currentRequest as any).apiType}`,\n+            );\n+        }\n+\n+        if (isLoadMore) {\n+          setData((prev) => [...prev, ...newData]);\n+        } else {\n+          setData(newData);\n+        }\n+\n+        setHasMore(page < pagination.total_pages);\n+        setCurrentPage(page);\n+      } catch (err) {\n+        const errorMessage =\n+          err instanceof Error ? err.message : \"Failed to fetch data\";\n+        setError(errorMessage);\n+        console.error(\"Error fetching data:\", err);\n+      } finally {\n+        setLoading(false);\n+        setLoadingMore(false);\n+        isLoadingRef.current = false;\n+      }\n+    },\n+    [api, pageSize, enabled],\n+  );\n+\n+  const handleScroll = useCallback(() => {\n+    const scrollElement = scrollRef.current;\n+    if (\n+      !scrollElement ||\n+      loadingMore ||\n+      !hasMore ||\n+      isLoadingRef.current ||\n+      !enabled\n+    )\n+      return;\n+\n+    const { scrollTop, scrollHeight, clientHeight } = scrollElement;\n+    const threshold = 100;\n+\n+    if (scrollTop + clientHeight >= scrollHeight - threshold) {\n+      fetchData(currentPage + 1, true);\n+    }\n+  }, [fetchData, currentPage, loadingMore, hasMore, enabled]);\n+\n+  const refresh = useCallback(() => {\n+    setCurrentPage(1);\n+    setHasMore(true);\n+    setError(null);\n+    fetchData(1);\n+  }, [fetchData]);\n+\n+  const loadMore = useCallback(() => {\n+    if (!loadingMore && hasMore && !isLoadingRef.current && enabled) {\n+      fetchData(currentPage + 1, true);\n+    }\n+  }, [fetchData, currentPage, loadingMore, hasMore, enabled]);\n+\n+  const requestString = JSON.stringify(request);\n+\n+  useEffect(() => {\n+    if (enabled) {\n+      setCurrentPage(1);\n+      setHasMore(true);\n+      setError(null);\n+      setData([]);\n+      fetchData(1);\n+    }\n+  }, [requestString, enabled, fetchData]);\n+\n+  useEffect(() => {\n+    const scrollElement = scrollRef.current;\n+    if (scrollElement && enabled) {\n+      scrollElement.addEventListener(\"scroll\", handleScroll);\n+      return () => scrollElement.removeEventListener(\"scroll\", handleScroll);\n+    }\n+  }, [handleScroll, enabled]);\n+\n+  return {\n+    data,\n+    loading,\n+    loadingMore,\n+    hasMore,\n+    error,\n+    scrollRef,\n+    refresh,\n+    loadMore,\n+  };\n+};\ndiff --git a/autogpt_platform/frontend/src/lib/autogpt-server-api/client.ts b/autogpt_platform/frontend/src/lib/autogpt-server-api/client.ts\nindex f82ed8e302a8..a1b0e169a674 100644\n--- a/autogpt_platform/frontend/src/lib/autogpt-server-api/client.ts\n+++ b/autogpt_platform/frontend/src/lib/autogpt-server-api/client.ts\n@@ -9,6 +9,11 @@ import type {\n   APIKeyCredentials,\n   APIKeyPermission,\n   Block,\n+  BlockCategoryResponse,\n+  BlockRequest,\n+  BlockResponse,\n+  BlockSearchResponse,\n+  CountResponse,\n   CreateAPIKeyResponse,\n   CreatorDetails,\n   CreatorsResponse,\n@@ -42,6 +47,7 @@ import type {\n   OttoQuery,\n   OttoResponse,\n   ProfileDetails,\n+  ProviderResponse,\n   RefundRequest,\n   ReviewSubmissionRequest,\n   Schedule,\n@@ -56,6 +62,7 @@ import type {\n   StoreSubmissionRequest,\n   StoreSubmissionsResponse,\n   SubmissionStatus,\n+  SuggestionsResponse,\n   TransactionHistory,\n   User,\n   UserOnboarding,\n@@ -206,6 +213,44 @@ export default class BackendAPI {\n     return this._get(\"/onboarding/enabled\");\n   }\n \n+  ////////////////////////////////////////\n+  //////////////// BUILDER ///////////////\n+  ////////////////////////////////////////\n+\n+  getSuggestions(): Promise<SuggestionsResponse> {\n+    return this._get(\"/builder/suggestions\");\n+  }\n+\n+  getBlockCategories(): Promise<BlockCategoryResponse[]> {\n+    return this._get(\"/builder/categories\");\n+  }\n+\n+  getBuilderBlocks(request?: BlockRequest): Promise<BlockResponse> {\n+    return this._get(\"/builder/blocks\", request);\n+  }\n+\n+  getProviders(request?: {\n+    page?: number;\n+    page_size?: number;\n+  }): Promise<ProviderResponse> {\n+    return this._get(\"/builder/providers\", request);\n+  }\n+\n+  searchBlocks(options: {\n+    search_query?: string;\n+    filter?: (\"blocks\" | \"integrations\" | \"marketplace_agents\" | \"my_agents\")[];\n+    by_creator?: string[];\n+    search_id?: string;\n+    page?: number;\n+    page_size?: number;\n+  }): Promise<BlockSearchResponse> {\n+    return this._request(\"POST\", \"/builder/search\", options);\n+  }\n+\n+  getBlockCounts(): Promise<CountResponse> {\n+    return this._get(\"/builder/counts\");\n+  }\n+\n   ////////////////////////////////////////\n   //////////////// GRAPHS ////////////////\n   ////////////////////////////////////////\ndiff --git a/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts b/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\nindex b4ba851053d0..f00dba21d2a2 100644\n--- a/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\n+++ b/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\n@@ -27,6 +27,71 @@ export type BlockCost = {\n   cost_filter: { [key: string]: any };\n };\n \n+/* Mirror of backend/server/v2/builder/model.py:SuggestionsResponse */\n+export type SuggestionsResponse = {\n+  otto_suggestions: string[];\n+  recent_searches: string[];\n+  providers: string[];\n+  top_blocks: Block[];\n+};\n+\n+/* Mirror of backend/server/v2/builder/model.py:BlockCategoryResponse */\n+export type BlockCategoryResponse = {\n+  name: string;\n+  total_blocks: number;\n+  blocks: Block[];\n+};\n+\n+export type BlockRequest = {\n+  page?: number;\n+  page_size?: number;\n+} & (\n+  | { category?: string }\n+  | { type?: \"all\" | \"input\" | \"action\" | \"output\" }\n+  | { provider?: CredentialsProviderName }\n+);\n+\n+/* Mirror of backend/server/v2/builder/model.py:BlockReponse */\n+export type BlockResponse = {\n+  blocks: Block[];\n+  pagination: Pagination;\n+};\n+\n+/* Mirror of backend/server/v2/builder/model.py:Provider */\n+export type Provider = {\n+  name: CredentialsProviderName;\n+  description: string;\n+  integration_count: number;\n+};\n+\n+/* Mirror of backend/server/v2/builder/model.py:ProviderResponse */\n+export type ProviderResponse = {\n+  providers: Provider[];\n+  pagination: Pagination;\n+};\n+\n+/* Mirror of backend/server/v2/builder/model.py:BlockSearchResponse */\n+export type BlockSearchResponse = {\n+  items: (Block | LibraryAgent | StoreAgent)[];\n+  total_items: Record<\n+    \"blocks\" | \"integrations\" | \"marketplace_agents\" | \"my_agents\",\n+    number\n+  >;\n+  page: number;\n+  more_pages: boolean;\n+};\n+\n+/* Mirror of backend/server/v2/builder/model.py:CountResponse */\n+export type CountResponse = {\n+  all_blocks: number;\n+  input_blocks: number;\n+  action_blocks: number;\n+  output_blocks: number;\n+  integrations: number;\n+  marketplace_agents: number;\n+  my_agents: number;\n+};\n+\n /* Mirror of backend/data/block.py:Block */\n export type Block = {\n   id: string;\n@@ -402,6 +467,7 @@ export type LibraryAgent = {\n   name: string;\n   description: string;\n   input_schema: BlockIOObjectSubSchema;\n+  output_schema: BlockIOObjectSubSchema;\n   new_output: boolean;\n   can_access_graph: boolean;\n   is_latest_version: boolean;\ndiff --git a/autogpt_platform/frontend/src/lib/utils.ts b/autogpt_platform/frontend/src/lib/utils.ts\nindex 3eea21add85f..3be378777830 100644\n--- a/autogpt_platform/frontend/src/lib/utils.ts\n+++ b/autogpt_platform/frontend/src/lib/utils.ts\n@@ -1,7 +1,13 @@\n import { type ClassValue, clsx } from \"clsx\";\n import { twMerge } from \"tailwind-merge\";\n \n-import { Category } from \"@/lib/autogpt-server-api/types\";\n+import {\n+  Block,\n+  BlockUIType,\n+  Category,\n+  LibraryAgent,\n+  SpecialBlockID,\n+} from \"@/lib/autogpt-server-api/types\";\n import { NodeDimension } from \"@/components/Flow\";\n \n export function cn(...inputs: ClassValue[]) {\n@@ -397,7 +403,59 @@ export function isEmptyOrWhitespace(str: string | undefined | null): boolean {\n   return !str || str.trim().length === 0;\n }\n \n-/** Check if a value is an object or not */\n-export function isObject(value: unknown): value is Record<string, unknown> {\n-  return typeof value === \"object\" && value !== null && !Array.isArray(value);\n+export const convertLibraryAgentIntoBlock = (agent: LibraryAgent) => {\n+  const block = {\n+    id: SpecialBlockID.AGENT,\n+    name: agent.name,\n+    description:\n+      `Ver.${agent.graph_version}` +\n+      (agent.description ? ` | ${agent.description}` : \"\"),\n+    categories: [{ category: \"AGENT\", description: \"\" }],\n+    inputSchema: agent.input_schema,\n+    outputSchema: agent.output_schema,\n+    staticOutput: false,\n+    uiType: BlockUIType.AGENT,\n+    uiKey: agent.id,\n+    costs: [],\n+    hardcodedValues: {\n+      graph_id: agent.graph_id,\n+      graph_version: agent.graph_version,\n+      input_schema: agent.input_schema,\n+      output_schema: agent.output_schema,\n+      agent_name: agent.name,\n+    },\n+  } as Block;\n+\n+  return block;\n+};\n+\n+// Need to change it once, we got provider blocks\n+export const getBlockType = (item: any) => {\n+  if (item?.inputSchema?.properties?.model?.title === \"LLM Model\") {\n+    return \"ai_agent\";\n+  }\n+  if (item.id && item.name && item.inputSchema && item.outputSchema) {\n+    return \"block\";\n+  }\n+  if (item.name && typeof item.integration_count === \"number\") {\n+    return \"provider\";\n+  }\n+  if (item.id && item.graph_id && item.status) {\n+    return \"library_agent\";\n+  }\n+  if (item.slug && item.agent_name && item.runs !== undefined) {\n+    return \"store_agent\";\n+  }\n+\n+  return null;\n+};\n+\n+export function parseErrorMessage(error: unknown, message?: string): string {\n+  const errorMessage = error\n+    ? error instanceof Error\n+      ? error.message\n+      : String(error)\n+    : message || \"An unexpected error occurred. Please try again.\";\n+\n+  return errorMessage;\n }\ndiff --git a/autogpt_platform/frontend/src/tests/build.spec.ts b/autogpt_platform/frontend/src/tests/build.spec.ts\nindex 3c6a87ba94b7..4f968242774f 100644\n--- a/autogpt_platform/frontend/src/tests/build.spec.ts\n+++ b/autogpt_platform/frontend/src/tests/build.spec.ts\n@@ -1,304 +1,304 @@\n-// Note: all the comments with //(number)! are for the docs\n-//ignore them when reading the code, but if you change something,\n-//make sure to update the docs! Your autoformmater will break this page,\n-// so don't run it on this file.\n-// --8<-- [start:BuildPageExample]\n-import { test } from \"./fixtures\";\n-import { BuildPage } from \"./pages/build.page\";\n-\n-// Reason Ignore: admonishment is in the wrong place visually with correct prettier rules\n-// prettier-ignore\n-test.describe(\"Build\", () => { //(1)!\n-  let buildPage: BuildPage; //(2)!\n-\n-  // Reason Ignore: admonishment is in the wrong place visually with correct prettier rules\n-  // prettier-ignore\n-  test.beforeEach(async ({ page, loginPage, testUser }) => { //(3)! ts-ignore\n-    buildPage = new BuildPage(page);\n-\n-    // Start each test with login using worker auth\n-    await page.goto(\"/login\"); //(4)!\n-    await loginPage.login(testUser.email, testUser.password);\n-    await test.expect(page).toHaveURL(\"/marketplace\"); //(5)!\n-    await buildPage.navbar.clickBuildLink();\n-  });\n-\n-  // Reason Ignore: admonishment is in the wrong place visually with correct prettier rules\n-  // prettier-ignore\n-  test(\"user can add a block\", async ({ page }) => { //(6)!\n-    // workaround for #8788\n-    await buildPage.navbar.clickBuildLink();\n-    await test.expect(page).toHaveURL(new RegExp(\"/build\"));\n-    await buildPage.waitForPageLoad();\n-    await test.expect(buildPage.isLoaded()).resolves.toBeTruthy(); //(7)!\n-\n-    await buildPage.closeTutorial(); //(9)!\n-    await buildPage.openBlocksPanel(); //(10)!\n-    const block = await buildPage.getDictionaryBlockDetails();\n-\n-    await buildPage.addBlock(block); //(11)!\n-    await buildPage.closeBlocksPanel(); //(12)!\n-    await test.expect(buildPage.hasBlock(block)).resolves.toBeTruthy(); //(13)!\n-  });\n-  // --8<-- [end:BuildPageExample]\n-\n-  test(\"user can add all blocks a-l\", async ({ page }, testInfo) => {\n-    // this test is slow af so we 10x the timeout (sorry future me)\n-    await test.setTimeout(testInfo.timeout * 100);\n-    await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n-    await test.expect(page).toHaveURL(new RegExp(\"/.*build\"));\n-    await buildPage.closeTutorial();\n-    await buildPage.openBlocksPanel();\n-    const blocks = await buildPage.getBlocks();\n-\n-    const blockIdsToSkip = await buildPage.getBlocksToSkip();\n-    const blockTypesToSkip = [\"Input\", \"Output\", \"Agent\", \"AI\"];\n-\n-    // add all the blocks in order except for the agent executor block\n-    for (const block of blocks) {\n-      if (block.name[0].toLowerCase() >= \"m\") {\n-        continue;\n-      }\n-      if (!blockIdsToSkip.some((b) => b === block.id) && !blockTypesToSkip.some((b) => block.type === b)) {\n-        console.log(\"Adding block:\", block.name, block.id, block.type, \" skipping types:\", blockTypesToSkip);\n-        await buildPage.addBlock(block);\n-      }\n-    }\n-    await buildPage.closeBlocksPanel();\n-    // check that all the blocks are visible\n-    for (const block of blocks) {\n-      if (block.name[0].toLowerCase() >= \"m\") {\n-        continue;\n-      }\n-      if (!blockIdsToSkip.some((b) => b === block.id) && !blockTypesToSkip.some((b) => block.type === b)) {\n-        console.log(\"Checking block:\", block.name, block.id, block.type, \" skipping types:\", blockTypesToSkip);\n-        await test.expect(buildPage.hasBlock(block)).resolves.toBeTruthy();\n-      }\n-    }\n-\n-    // check that we can save the agent with all the blocks\n-    await buildPage.saveAgent(\"all blocks test\", \"all blocks test\");\n-    // page should have a url like http://localhost:3000/build?flowID=f4f3a1da-cfb3-430f-a074-a455b047e340\n-    await test.expect(page).toHaveURL(new RegExp(\"/.*build\\\\?flowID=.+\"));\n-  });\n-\n-  test(\"user can add all blocks m-z\", async ({ page }, testInfo) => {\n-    // this test is slow af so we 10x the timeout (sorry future me)\n-    await test.setTimeout(testInfo.timeout * 100);\n-    await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n-    await test.expect(page).toHaveURL(new RegExp(\"/.*build\"));\n-    await buildPage.closeTutorial();\n-    await buildPage.openBlocksPanel();\n-    const blocks = await buildPage.getBlocks();\n-\n-    const blockIdsToSkip = await buildPage.getBlocksToSkip();\n-    const blockTypesToSkip = [\"Input\", \"Output\", \"Agent\", \"AI\"];\n-\n-    // add all the blocks in order except for the agent executor block\n-    for (const block of blocks) {\n-      if (block.name[0].toLowerCase() < \"m\") {\n-        continue;\n-      }\n-      if (!blockIdsToSkip.some((b) => b === block.id) && !blockTypesToSkip.some((b) => block.type === b)) {\n-        console.log(\"Adding block:\", block.name, block.id, block.type, \" skipping types:\", blockTypesToSkip);\n-        await buildPage.addBlock(block);\n-      }\n-    }\n-    await buildPage.closeBlocksPanel();\n-    // check that all the blocks are visible\n-    for (const block of blocks) {\n-      if (block.name[0].toLowerCase() < \"m\") {\n-        continue;\n-      }\n-      if (!blockIdsToSkip.some((b) => b === block.id) && !blockTypesToSkip.some((b) => block.type === b)) {\n-        console.log(\"Checking block:\", block.name, block.id, block.type, \" skipping types:\", blockTypesToSkip);\n-        await test.expect(buildPage.hasBlock(block)).resolves.toBeTruthy();\n-      }\n-    }\n-\n-    // check that we can save the agent with all the blocks\n-    await buildPage.saveAgent(\"all blocks test\", \"all blocks test\");\n-    // page should have a url like http://localhost:3000/build?flowID=f4f3a1da-cfb3-430f-a074-a455b047e340\n-    await test.expect(page).toHaveURL(new RegExp(\"/.*build\\\\?flowID=.+\"));\n-  });\n-\n-  test(\"build navigation is accessible from navbar\", async ({ page }) => {\n-    await buildPage.navbar.clickBuildLink();\n-    await test.expect(page).toHaveURL(new RegExp(\"/build\"));\n-    // workaround for #8788\n-    await page.reload();\n-    await page.reload();\n-    await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n-  });\n-\n-  test(\"user can add two blocks and connect them\", async ({\n-    page,\n-  }, testInfo) => {\n-    await test.setTimeout(testInfo.timeout * 10);\n-\n-    await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n-    await test.expect(page).toHaveURL(new RegExp(\"/.*build\"));\n-    await buildPage.closeTutorial();\n-    await buildPage.openBlocksPanel();\n-\n-    // Define the blocks to add\n-    const block1 = {\n-      id: \"1ff065e9-88e8-4358-9d82-8dc91f622ba9\",\n-      name: \"Store Value 1\",\n-      description: \"Store Value Block 1\",\n-      type: \"Standard\",\n-    };\n-    const block2 = {\n-      id: \"1ff065e9-88e8-4358-9d82-8dc91f622ba9\",\n-      name: \"Store Value 2\",\n-      description: \"Store Value Block 2\",\n-      type: \"Standard\",\n-    };\n-\n-    // Add the blocks\n-    await buildPage.addBlock(block1);\n-    await buildPage.addBlock(block2);\n-    await buildPage.closeBlocksPanel();\n-\n-    // Connect the blocks\n-    await buildPage.connectBlockOutputToBlockInputViaDataId(\n-      \"1-1-output-source\",\n-      \"1-2-input-target\",\n-    );\n-\n-    // Fill in the input for the first block\n-    await buildPage.fillBlockInputByPlaceholder(\n-      block1.id,\n-      \"Enter input\",\n-      \"Test Value\",\n-      \"1\",\n-    );\n-\n-    // Save the agent and wait for the URL to update\n-    await buildPage.saveAgent(\n-      \"Connected Blocks Test\",\n-      \"Testing block connections\",\n-    );\n-    await test.expect(page).toHaveURL(new RegExp(\"/.*build\\\\?flowID=.+\"));\n-\n-    // Wait for the save button to be enabled again\n-    await buildPage.waitForSaveButton();\n-\n-    // Ensure the run button is enabled\n-    await test.expect(buildPage.isRunButtonEnabled()).resolves.toBeTruthy();\n-\n-    // Run the agent\n-    await buildPage.runAgent();\n-\n-    // Wait for processing to complete by checking the completion badge\n-    await buildPage.waitForCompletionBadge();\n-\n-    // Get the first completion badge and verify it's visible\n-    await test\n-      .expect(buildPage.isCompletionBadgeVisible())\n-      .resolves.toBeTruthy();\n-  });\n-\n-  test(\"user can build an agent with inputs and output blocks\", async ({\n-    page,\n-  }) => {\n-    // simple calculator to double input and output it\n-\n-    // load the pages and prep\n-    await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n-    await test.expect(page).toHaveURL(new RegExp(\"/.*build\"));\n-    await buildPage.closeTutorial();\n-    await buildPage.openBlocksPanel();\n-\n-    // find the blocks we want\n-    const blocks = await buildPage.getBlocks();\n-    const inputBlock = blocks.find((b) => b.name === \"Agent Input\");\n-    const outputBlock = blocks.find((b) => b.name === \"Agent Output\");\n-    const calculatorBlock = blocks.find((b) => b.name === \"Calculator\");\n-    if (!inputBlock || !outputBlock || !calculatorBlock) {\n-      throw new Error(\"Input or output block not found\");\n-    }\n-\n-    // add the blocks\n-    await buildPage.addBlock(inputBlock);\n-    await buildPage.addBlock(outputBlock);\n-    await buildPage.addBlock(calculatorBlock);\n-    await buildPage.closeBlocksPanel();\n-\n-    // Wait for blocks to be fully loaded\n-    await page.waitForTimeout(1000);\n-\n-    await test.expect(buildPage.hasBlock(inputBlock)).resolves.toBeTruthy();\n-    await test.expect(buildPage.hasBlock(outputBlock)).resolves.toBeTruthy();\n-    await test\n-      .expect(buildPage.hasBlock(calculatorBlock))\n-      .resolves.toBeTruthy();\n-\n-    // Wait for blocks to be ready for connections\n-    await page.waitForTimeout(1000);\n-\n-    await buildPage.connectBlockOutputToBlockInputViaName(\n-      inputBlock.id,\n-      \"Result\",\n-      calculatorBlock.id,\n-      \"A\",\n-    );\n-    await buildPage.connectBlockOutputToBlockInputViaName(\n-      inputBlock.id,\n-      \"Result\",\n-      calculatorBlock.id,\n-      \"B\",\n-    );\n-    await buildPage.connectBlockOutputToBlockInputViaName(\n-      calculatorBlock.id,\n-      \"Result\",\n-      outputBlock.id,\n-      \"Value\",\n-    );\n-\n-    // Wait for connections to stabilize\n-    await page.waitForTimeout(1000);\n-\n-    await buildPage.fillBlockInputByPlaceholder(\n-      inputBlock.id,\n-      \"Enter Name\",\n-      \"Value\",\n-    );\n-    await buildPage.fillBlockInputByPlaceholder(\n-      outputBlock.id,\n-      \"Enter Name\",\n-      \"Doubled\",\n-    );\n-\n-    // Wait before changing dropdown\n-    await page.waitForTimeout(500);\n-\n-    await buildPage.selectBlockInputValue(\n-      calculatorBlock.id,\n-      \"Operation\",\n-      \"Add\",\n-    );\n-\n-    // Wait before saving\n-    await page.waitForTimeout(1000);\n-\n-    await buildPage.saveAgent(\n-      \"Input and Output Blocks Test\",\n-      \"Testing input and output blocks\",\n-    );\n-    await test.expect(page).toHaveURL(new RegExp(\"/.*build\\\\?flowID=.+\"));\n-\n-    // Wait for save to complete\n-    await page.waitForTimeout(1000);\n-\n-    await buildPage.runAgent();\n-    await buildPage.fillRunDialog({\n-      Value: \"10\",\n-    });\n-    await buildPage.clickRunDialogRunButton();\n-    await buildPage.waitForCompletionBadge();\n-    await test\n-      .expect(buildPage.isCompletionBadgeVisible())\n-      .resolves.toBeTruthy();\n-  });\n-});\n+// // Note: all the comments with //(number)! are for the docs\n+// //ignore them when reading the code, but if you change something,\n+// //make sure to update the docs! Your autoformmater will break this page,\n+// // so don't run it on this file.\n+// // --8<-- [start:BuildPageExample]\n+// import { test } from \"./fixtures\";\n+// import { BuildPage } from \"./pages/build.page\";\n+\n+// // Reason Ignore: admonishment is in the wrong place visually with correct prettier rules\n+// // prettier-ignore\n+// test.describe(\"Build\", () => { //(1)!\n+//   let buildPage: BuildPage; //(2)!\n+\n+//   // Reason Ignore: admonishment is in the wrong place visually with correct prettier rules\n+//   // prettier-ignore\n+//   test.beforeEach(async ({ page, loginPage, testUser }, testInfo) => { //(3)! ts-ignore\n+//     buildPage = new BuildPage(page);\n+\n+//     // Start each test with login using worker auth\n+//     await page.goto(\"/login\"); //(4)!\n+//     await loginPage.login(testUser.email, testUser.password);\n+//     await test.expect(page).toHaveURL(\"/marketplace\"); //(5)!\n+//     await buildPage.navbar.clickBuildLink();\n+//   });\n+\n+//   // Reason Ignore: admonishment is in the wrong place visually with correct prettier rules\n+//   // prettier-ignore\n+//   test(\"user can add a block\", async ({ page }) => { //(6)!\n+//     // workaround for #8788\n+//     await buildPage.navbar.clickBuildLink();\n+//     await test.expect(page).toHaveURL(new RegExp(\"/build\"));\n+//     await buildPage.waitForPageLoad();\n+//     await test.expect(buildPage.isLoaded()).resolves.toBeTruthy(); //(7)!\n+\n+//     await buildPage.closeTutorial(); //(9)!\n+//     await buildPage.openBlocksPanel(); //(10)!\n+//     const block = await buildPage.getDictionaryBlockDetails();\n+\n+//     await buildPage.addBlock(block); //(11)!\n+//     await buildPage.closeBlocksPanel(); //(12)!\n+//     await test.expect(buildPage.hasBlock(block)).resolves.toBeTruthy(); //(13)!\n+//   });\n+//   // --8<-- [end:BuildPageExample]\n+\n+//   test(\"user can add all blocks a-l\", async ({ page }, testInfo) => {\n+//     // this test is slow af so we 10x the timeout (sorry future me)\n+//     await test.setTimeout(testInfo.timeout * 100);\n+//     await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n+//     await test.expect(page).toHaveURL(new RegExp(\"/.*build\"));\n+//     await buildPage.closeTutorial();\n+//     await buildPage.openBlocksPanel();\n+//     const blocks = await buildPage.getBlocks();\n+\n+//     const blockIdsToSkip = await buildPage.getBlocksToSkip();\n+//     const blockTypesToSkip = [\"Input\", \"Output\", \"Agent\", \"AI\"];\n+\n+//     // add all the blocks in order except for the agent executor block\n+//     for (const block of blocks) {\n+//       if (block.name[0].toLowerCase() >= \"m\") {\n+//         continue;\n+//       }\n+//       if (!blockIdsToSkip.some((b) => b === block.id) && !blockTypesToSkip.some((b) => block.type === b)) {\n+//         console.log(\"Adding block:\", block.name, block.id, block.type, \" skipping types:\", blockTypesToSkip);\n+//         await buildPage.addBlock(block);\n+//       }\n+//     }\n+//     await buildPage.closeBlocksPanel();\n+//     // check that all the blocks are visible\n+//     for (const block of blocks) {\n+//       if (block.name[0].toLowerCase() >= \"m\") {\n+//         continue;\n+//       }\n+//       if (!blockIdsToSkip.some((b) => b === block.id) && !blockTypesToSkip.some((b) => block.type === b)) {\n+//         console.log(\"Checking block:\", block.name, block.id, block.type, \" skipping types:\", blockTypesToSkip);\n+//         await test.expect(buildPage.hasBlock(block)).resolves.toBeTruthy();\n+//       }\n+//     }\n+\n+//     // check that we can save the agent with all the blocks\n+//     await buildPage.saveAgent(\"all blocks test\", \"all blocks test\");\n+//     // page should have a url like http://localhost:3000/build?flowID=f4f3a1da-cfb3-430f-a074-a455b047e340\n+//     await test.expect(page).toHaveURL(new RegExp(\"/.*build\\\\?flowID=.+\"));\n+//   });\n+\n+//   test(\"user can add all blocks m-z\", async ({ page }, testInfo) => {\n+//     // this test is slow af so we 10x the timeout (sorry future me)\n+//     await test.setTimeout(testInfo.timeout * 100);\n+//     await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n+//     await test.expect(page).toHaveURL(new RegExp(\"/.*build\"));\n+//     await buildPage.closeTutorial();\n+//     await buildPage.openBlocksPanel();\n+//     const blocks = await buildPage.getBlocks();\n+\n+//     const blockIdsToSkip = await buildPage.getBlocksToSkip();\n+//     const blockTypesToSkip = [\"Input\", \"Output\", \"Agent\", \"AI\"];\n+\n+//     // add all the blocks in order except for the agent executor block\n+//     for (const block of blocks) {\n+//       if (block.name[0].toLowerCase() < \"m\") {\n+//         continue;\n+//       }\n+//       if (!blockIdsToSkip.some((b) => b === block.id) && !blockTypesToSkip.some((b) => block.type === b)) {\n+//         console.log(\"Adding block:\", block.name, block.id, block.type, \" skipping types:\", blockTypesToSkip);\n+//         await buildPage.addBlock(block);\n+//       }\n+//     }\n+//     await buildPage.closeBlocksPanel();\n+//     // check that all the blocks are visible\n+//     for (const block of blocks) {\n+//       if (block.name[0].toLowerCase() < \"m\") {\n+//         continue;\n+//       }\n+//       if (!blockIdsToSkip.some((b) => b === block.id) && !blockTypesToSkip.some((b) => block.type === b)) {\n+//         console.log(\"Checking block:\", block.name, block.id, block.type, \" skipping types:\", blockTypesToSkip);\n+//         await test.expect(buildPage.hasBlock(block)).resolves.toBeTruthy();\n+//       }\n+//     }\n+\n+//     // check that we can save the agent with all the blocks\n+//     await buildPage.saveAgent(\"all blocks test\", \"all blocks test\");\n+//     // page should have a url like http://localhost:3000/build?flowID=f4f3a1da-cfb3-430f-a074-a455b047e340\n+//     await test.expect(page).toHaveURL(new RegExp(\"/.*build\\\\?flowID=.+\"));\n+//   });\n+\n+//   test(\"build navigation is accessible from navbar\", async ({ page }) => {\n+//     await buildPage.navbar.clickBuildLink();\n+//     await test.expect(page).toHaveURL(new RegExp(\"/build\"));\n+//     // workaround for #8788\n+//     await page.reload();\n+//     await page.reload();\n+//     await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n+//   });\n+\n+//   test(\"user can add two blocks and connect them\", async ({\n+//     page,\n+//   }, testInfo) => {\n+//     await test.setTimeout(testInfo.timeout * 10);\n+\n+//     await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n+//     await test.expect(page).toHaveURL(new RegExp(\"/.*build\"));\n+//     await buildPage.closeTutorial();\n+//     await buildPage.openBlocksPanel();\n+\n+//     // Define the blocks to add\n+//     const block1 = {\n+//       id: \"1ff065e9-88e8-4358-9d82-8dc91f622ba9\",\n+//       name: \"Store Value 1\",\n+//       description: \"Store Value Block 1\",\n+//       type: \"Standard\",\n+//     };\n+//     const block2 = {\n+//       id: \"1ff065e9-88e8-4358-9d82-8dc91f622ba9\",\n+//       name: \"Store Value 2\",\n+//       description: \"Store Value Block 2\",\n+//       type: \"Standard\",\n+//     };\n+\n+//     // Add the blocks\n+//     await buildPage.addBlock(block1);\n+//     await buildPage.addBlock(block2);\n+//     await buildPage.closeBlocksPanel();\n+\n+//     // Connect the blocks\n+//     await buildPage.connectBlockOutputToBlockInputViaDataId(\n+//       \"1-1-output-source\",\n+//       \"1-2-input-target\",\n+//     );\n+\n+//     // Fill in the input for the first block\n+//     await buildPage.fillBlockInputByPlaceholder(\n+//       block1.id,\n+//       \"Enter input\",\n+//       \"Test Value\",\n+//       \"1\",\n+//     );\n+\n+//     // Save the agent and wait for the URL to update\n+//     await buildPage.saveAgent(\n+//       \"Connected Blocks Test\",\n+//       \"Testing block connections\",\n+//     );\n+//     await test.expect(page).toHaveURL(new RegExp(\"/.*build\\\\?flowID=.+\"));\n+\n+//     // Wait for the save button to be enabled again\n+//     await buildPage.waitForSaveButton();\n+\n+//     // Ensure the run button is enabled\n+//     await test.expect(buildPage.isRunButtonEnabled()).resolves.toBeTruthy();\n+\n+//     // Run the agent\n+//     await buildPage.runAgent();\n+\n+//     // Wait for processing to complete by checking the completion badge\n+//     await buildPage.waitForCompletionBadge();\n+\n+//     // Get the first completion badge and verify it's visible\n+//     await test\n+//       .expect(buildPage.isCompletionBadgeVisible())\n+//       .resolves.toBeTruthy();\n+//   });\n+\n+//   test(\"user can build an agent with inputs and output blocks\", async ({\n+//     page,\n+//   }) => {\n+//     // simple calculator to double input and output it\n+\n+//     // load the pages and prep\n+//     await test.expect(buildPage.isLoaded()).resolves.toBeTruthy();\n+//     await test.expect(page).toHaveURL(new RegExp(\"/.*build\"));\n+//     await buildPage.closeTutorial();\n+//     await buildPage.openBlocksPanel();\n+\n+//     // find the blocks we want\n+//     const blocks = await buildPage.getBlocks();\n+//     const inputBlock = blocks.find((b) => b.name === \"Agent Input\");\n+//     const outputBlock = blocks.find((b) => b.name === \"Agent Output\");\n+//     const calculatorBlock = blocks.find((b) => b.name === \"Calculator\");\n+//     if (!inputBlock || !outputBlock || !calculatorBlock) {\n+//       throw new Error(\"Input or output block not found\");\n+//     }\n+\n+//     // add the blocks\n+//     await buildPage.addBlock(inputBlock);\n+//     await buildPage.addBlock(outputBlock);\n+//     await buildPage.addBlock(calculatorBlock);\n+//     await buildPage.closeBlocksPanel();\n+\n+//     // Wait for blocks to be fully loaded\n+//     await page.waitForTimeout(1000);\n+\n+//     await test.expect(buildPage.hasBlock(inputBlock)).resolves.toBeTruthy();\n+//     await test.expect(buildPage.hasBlock(outputBlock)).resolves.toBeTruthy();\n+//     await test\n+//       .expect(buildPage.hasBlock(calculatorBlock))\n+//       .resolves.toBeTruthy();\n+\n+//     // Wait for blocks to be ready for connections\n+//     await page.waitForTimeout(1000);\n+\n+//     await buildPage.connectBlockOutputToBlockInputViaName(\n+//       inputBlock.id,\n+//       \"Result\",\n+//       calculatorBlock.id,\n+//       \"A\",\n+//     );\n+//     await buildPage.connectBlockOutputToBlockInputViaName(\n+//       inputBlock.id,\n+//       \"Result\",\n+//       calculatorBlock.id,\n+//       \"B\",\n+//     );\n+//     await buildPage.connectBlockOutputToBlockInputViaName(\n+//       calculatorBlock.id,\n+//       \"Result\",\n+//       outputBlock.id,\n+//       \"Value\",\n+//     );\n+\n+//     // Wait for connections to stabilize\n+//     await page.waitForTimeout(1000);\n+\n+//     await buildPage.fillBlockInputByPlaceholder(\n+//       inputBlock.id,\n+//       \"Enter Name\",\n+//       \"Value\",\n+//     );\n+//     await buildPage.fillBlockInputByPlaceholder(\n+//       outputBlock.id,\n+//       \"Enter Name\",\n+//       \"Doubled\",\n+//     );\n+\n+//     // Wait before changing dropdown\n+//     await page.waitForTimeout(500);\n+\n+//     await buildPage.selectBlockInputValue(\n+//       calculatorBlock.id,\n+//       \"Operation\",\n+//       \"Add\",\n+//     );\n+\n+//     // Wait before saving\n+//     await page.waitForTimeout(1000);\n+\n+//     await buildPage.saveAgent(\n+//       \"Input and Output Blocks Test\",\n+//       \"Testing input and output blocks\",\n+//     );\n+//     await test.expect(page).toHaveURL(new RegExp(\"/.*build\\\\?flowID=.+\"));\n+\n+//     // Wait for save to complete\n+//     await page.waitForTimeout(1000);\n+\n+//     await buildPage.runAgent();\n+//     await buildPage.fillRunDialog({\n+//       Value: \"10\",\n+//     });\n+//     await buildPage.clickRunDialogRunButton();\n+//     await buildPage.waitForCompletionBadge();\n+//     await test\n+//       .expect(buildPage.isCompletionBadgeVisible())\n+//       .resolves.toBeTruthy();\n+//   });\n+// });\ndiff --git a/autogpt_platform/frontend/src/tests/monitor.spec.ts b/autogpt_platform/frontend/src/tests/monitor.spec.ts\nindex a7e588acf0f7..576a7c69183d 100644\n--- a/autogpt_platform/frontend/src/tests/monitor.spec.ts\n+++ b/autogpt_platform/frontend/src/tests/monitor.spec.ts\n@@ -1,126 +1,126 @@\n-import { TestInfo } from \"@playwright/test\";\n-import { test } from \"./fixtures\";\n-import { BuildPage } from \"./pages/build.page\";\n-import { MonitorPage } from \"./pages/monitor.page\";\n-import { v4 as uuidv4 } from \"uuid\";\n-import * as fs from \"fs/promises\";\n-import path from \"path\";\n-// --8<-- [start:AttachAgentId]\n-test.describe(\"Monitor\", () => {\n-  let buildPage: BuildPage;\n-  let monitorPage: MonitorPage;\n+// import { expect, TestInfo } from \"@playwright/test\";\n+// import { test } from \"./fixtures\";\n+// import { BuildPage } from \"./pages/build.page\";\n+// import { MonitorPage } from \"./pages/monitor.page\";\n+// import { v4 as uuidv4 } from \"uuid\";\n+// import * as fs from \"fs/promises\";\n+// import path from \"path\";\n+// // --8<-- [start:AttachAgentId]\n+// test.describe(\"Monitor\", () => {\n+//   let buildPage: BuildPage;\n+//   let monitorPage: MonitorPage;\n \n-  test.beforeEach(async ({ page, loginPage, testUser }, testInfo: TestInfo) => {\n-    buildPage = new BuildPage(page);\n-    monitorPage = new MonitorPage(page);\n+//   test.beforeEach(async ({ page, loginPage, testUser }, testInfo: TestInfo) => {\n+//     buildPage = new BuildPage(page);\n+//     monitorPage = new MonitorPage(page);\n \n-    // Start each test with login using worker auth\n-    await page.goto(\"/login\");\n-    await loginPage.login(testUser.email, testUser.password);\n-    await test.expect(page).toHaveURL(\"/marketplace\");\n+//     // Start each test with login using worker auth\n+//     await page.goto(\"/login\");\n+//     await loginPage.login(testUser.email, testUser.password);\n+//     await test.expect(page).toHaveURL(\"/marketplace\");\n \n-    // add a test agent\n-    const basicBlock = await buildPage.getDictionaryBlockDetails();\n-    const id = uuidv4();\n-    await buildPage.createSingleBlockAgent(\n-      `test-agent-${id}`,\n-      `test-agent-description-${id}`,\n-      basicBlock,\n-    );\n-    await buildPage.runAgent();\n-    // await monitorPage.navbar.clickMonitorLink();\n-    await page.goto(\"/monitoring\"); // Library link now points to /library\n-    await monitorPage.waitForPageLoad();\n-    await test.expect(monitorPage.isLoaded()).resolves.toBeTruthy();\n-    testInfo.attach(\"agent-id\", { body: id });\n-  });\n-  // --8<-- [end:AttachAgentId]\n+//     // add a test agent\n+//     const basicBlock = await buildPage.getDictionaryBlockDetails();\n+//     const id = uuidv4();\n+//     await buildPage.createSingleBlockAgent(\n+//       `test-agent-${id}`,\n+//       `test-agent-description-${id}`,\n+//       basicBlock,\n+//     );\n+//     await buildPage.runAgent();\n+//     // await monitorPage.navbar.clickMonitorLink();\n+//     await page.goto(\"/monitoring\"); // Library link now points to /library\n+//     await monitorPage.waitForPageLoad();\n+//     await test.expect(monitorPage.isLoaded()).resolves.toBeTruthy();\n+//     testInfo.attach(\"agent-id\", { body: id });\n+//   });\n+//   // --8<-- [end:AttachAgentId]\n \n-  test.afterAll(async ({}) => {\n-    // clear out the downloads folder\n-    console.log(\n-      `clearing out the downloads folder ${monitorPage.downloadsFolder}`,\n-    );\n+//   test.afterAll(async ({}) => {\n+//     // clear out the downloads folder\n+//     console.log(\n+//       `clearing out the downloads folder ${monitorPage.downloadsFolder}`,\n+//     );\n \n-    await fs.rm(`${monitorPage.downloadsFolder}/monitor`, {\n-      recursive: true,\n-      force: true,\n-    });\n-  });\n+//     await fs.rm(`${monitorPage.downloadsFolder}/monitor`, {\n+//       recursive: true,\n+//       force: true,\n+//     });\n+//   });\n \n-  test(\"user can view agents\", async () => {\n-    const agents = await monitorPage.listAgents();\n-    // there should be at least one agent\n-    await test.expect(agents.length).toBeGreaterThan(0);\n-  });\n+//   test(\"user can view agents\", async ({ page }) => {\n+//     const agents = await monitorPage.listAgents();\n+//     // there should be at least one agent\n+//     await test.expect(agents.length).toBeGreaterThan(0);\n+//   });\n \n-  test.skip(\"user can export and import agents\", async ({\n-    page,\n-  }, testInfo: TestInfo) => {\n-    // --8<-- [start:ReadAgentId]\n-    if (testInfo.attachments.length === 0 || !testInfo.attachments[0].body) {\n-      throw new Error(\"No agent id attached to the test\");\n-    }\n-    const testAttachName = testInfo.attachments[0].body.toString();\n-    // --8<-- [end:ReadAgentId]\n-    const agents = await monitorPage.listAgents();\n+//   test.skip(\"user can export and import agents\", async ({\n+//     page,\n+//   }, testInfo: TestInfo) => {\n+//     // --8<-- [start:ReadAgentId]\n+//     if (testInfo.attachments.length === 0 || !testInfo.attachments[0].body) {\n+//       throw new Error(\"No agent id attached to the test\");\n+//     }\n+//     const testAttachName = testInfo.attachments[0].body.toString();\n+//     // --8<-- [end:ReadAgentId]\n+//     const agents = await monitorPage.listAgents();\n \n-    const downloadPromise = page.waitForEvent(\"download\");\n-    const agent = agents.find(\n-      (a: any) => a.name === `test-agent-${testAttachName}`,\n-    );\n-    if (!agent) {\n-      throw new Error(`Agent ${testAttachName} not found`);\n-    }\n-    await monitorPage.exportToFile(agent);\n-    const download = await downloadPromise;\n+//     const downloadPromise = page.waitForEvent(\"download\");\n+//     const agent = agents.find(\n+//       (a: any) => a.name === `test-agent-${testAttachName}`,\n+//     );\n+//     if (!agent) {\n+//       throw new Error(`Agent ${testAttachName} not found`);\n+//     }\n+//     await monitorPage.exportToFile(agent);\n+//     const download = await downloadPromise;\n \n-    // Wait for the download process to complete and save the downloaded file somewhere.\n-    await download.saveAs(\n-      `${monitorPage.downloadsFolder}/monitor/${download.suggestedFilename()}`,\n-    );\n-    console.log(`downloaded file to ${download.suggestedFilename()}`);\n-    await test.expect(download.suggestedFilename()).toBeDefined();\n-    // test-agent-uuid-v1.json\n-    await test.expect(download.suggestedFilename()).toContain(\"test-agent-\");\n-    await test.expect(download.suggestedFilename()).toContain(\"v1.json\");\n+//     // Wait for the download process to complete and save the downloaded file somewhere.\n+//     await download.saveAs(\n+//       `${monitorPage.downloadsFolder}/monitor/${download.suggestedFilename()}`,\n+//     );\n+//     console.log(`downloaded file to ${download.suggestedFilename()}`);\n+//     await test.expect(download.suggestedFilename()).toBeDefined();\n+//     // test-agent-uuid-v1.json\n+//     await test.expect(download.suggestedFilename()).toContain(\"test-agent-\");\n+//     await test.expect(download.suggestedFilename()).toContain(\"v1.json\");\n \n-    // import the agent\n-    const preImportAgents = await monitorPage.listAgents();\n-    const filesInFolder = await fs.readdir(\n-      `${monitorPage.downloadsFolder}/monitor`,\n-    );\n-    const importFile = filesInFolder.find((f) => f.includes(testAttachName));\n-    if (!importFile) {\n-      throw new Error(`No import file found for agent ${testAttachName}`);\n-    }\n-    const baseName = importFile.split(\".\")[0];\n-    await monitorPage.importFromFile(\n-      path.resolve(monitorPage.downloadsFolder, \"monitor\"),\n-      importFile,\n-      baseName + \"-imported\",\n-    );\n+//     // import the agent\n+//     const preImportAgents = await monitorPage.listAgents();\n+//     const filesInFolder = await fs.readdir(\n+//       `${monitorPage.downloadsFolder}/monitor`,\n+//     );\n+//     const importFile = filesInFolder.find((f) => f.includes(testAttachName));\n+//     if (!importFile) {\n+//       throw new Error(`No import file found for agent ${testAttachName}`);\n+//     }\n+//     const baseName = importFile.split(\".\")[0];\n+//     await monitorPage.importFromFile(\n+//       path.resolve(monitorPage.downloadsFolder, \"monitor\"),\n+//       importFile,\n+//       baseName + \"-imported\",\n+//     );\n \n-    // You'll be dropped at the build page, so hit run and then go back to monitor\n-    await buildPage.runAgent();\n-    await monitorPage.navbar.clickMonitorLink();\n-    await monitorPage.waitForPageLoad();\n+//     // You'll be dropped at the build page, so hit run and then go back to monitor\n+//     await buildPage.runAgent();\n+//     await monitorPage.navbar.clickMonitorLink();\n+//     await monitorPage.waitForPageLoad();\n \n-    const postImportAgents = await monitorPage.listAgents();\n-    await test\n-      .expect(postImportAgents.length)\n-      .toBeGreaterThan(preImportAgents.length);\n-    console.log(`postImportAgents: ${JSON.stringify(postImportAgents)}`);\n-    const importedAgent = postImportAgents.find(\n-      (a: any) => a.name === `${baseName}-imported`,\n-    );\n-    await test.expect(importedAgent).toBeDefined();\n-  });\n+//     const postImportAgents = await monitorPage.listAgents();\n+//     await test\n+//       .expect(postImportAgents.length)\n+//       .toBeGreaterThan(preImportAgents.length);\n+//     console.log(`postImportAgents: ${JSON.stringify(postImportAgents)}`);\n+//     const importedAgent = postImportAgents.find(\n+//       (a: any) => a.name === `${baseName}-imported`,\n+//     );\n+//     await test.expect(importedAgent).toBeDefined();\n+//   });\n \n-  test(\"user can view runs\", async () => {\n-    const runs = await monitorPage.listRuns();\n-    console.log(runs);\n-    // there should be at least one run\n-    await test.expect(runs.length).toBeGreaterThan(0);\n-  });\n-});\n+//   test(\"user can view runs\", async ({ page }) => {\n+//     const runs = await monitorPage.listRuns();\n+//     console.log(runs);\n+//     // there should be at least one run\n+//     await test.expect(runs.length).toBeGreaterThan(0);\n+//   });\n+// });\ndiff --git a/autogpt_platform/frontend/tailwind.config.ts b/autogpt_platform/frontend/tailwind.config.ts\nindex 9f2a706bcd98..7bf88951de4a 100644\n--- a/autogpt_platform/frontend/tailwind.config.ts\n+++ b/autogpt_platform/frontend/tailwind.config.ts\n@@ -1,4 +1,5 @@\n import type { Config } from \"tailwindcss\";\n+import scrollbarHide from \"tailwind-scrollbar-hide\";\n \n const config = {\n   darkMode: [\"class\"],\n@@ -141,7 +142,11 @@ const config = {\n       },\n     },\n   },\n-  plugins: [require(\"tailwindcss-animate\")],\n+  plugins: [\n+    require(\"tailwindcss-animate\"),\n+    scrollbarHide,\n+    require(\"tailwind-scrollbar\"),\n+  ],\n } satisfies Config;\n \n export default config;\n"
  },
  {
    "index": 3,
    "filtered_comments": [
      "Thank you for your contribution to improve the scheduling UX in the platform library! I see that you've added several important features like API+backend data paths for scheduling, buttons for creating/deleting schedules, and updated the scheduling API.\n\nHowever, I cannot approve this PR yet because:\n\n1. **The checklist is incomplete** - Please check off all the items or remove sections that aren't applicable. At minimum, we need to see that you've:\n   - Clearly listed your changes (which you have)  \n   - Made and executed a test plan\n\n2. **Missing test plan** - Please add details about how you've tested these scheduling features. For example:\n   - Creating a schedule for an agent\n   - Verifying the schedule executes correctly\n   - Deleting a schedule\n   - Testing the UI components for scheduling\n\nOnce you've addressed these items, we can proceed with the review. The changes themselves look appropriate for the scope described in the PR title.",
      "Thank you for working on the scheduling feature implementation! There are a few things that need to be addressed before this PR can be merged:\n\n### Missing Information\n\n1. **Checklist**: Please complete the checklist in the PR description. All items need to be checked off to indicate you've verified them.\n\n2. **Test Plan**: Please add a test plan explaining how you've tested these changes.\n\n3. **Backend Data Changes**: Since there are modifications to backend/data files, please explain how user_id checking is handled in the new/modified functions.\n\n### Scope Alignment\n\nThe PR title mentions \"platform/library\" but the changes span both frontend and backend components (which is also reflected in the PR labels). Consider updating the title to better reflect the scope of changes, perhaps something like: `feat(platform): Scheduling UX implementation`\n\n### Description Enhancements\n\nConsider expanding the description to better capture all the changes made. For example:\n- What specific API endpoints were updated/added?\n- How does the new scheduling flow work from a user perspective?\n- Are there any database changes or migrations needed?\n\nOnce these items are addressed, we'll be happy to review this PR again for merging. Thanks for your contribution to improving the scheduling feature!",
      "Thank you for working on completing the scheduling feature implementation in the Library. There are a few items that need to be addressed before this PR can be merged:\n\n1. **Checklist Completion**: Please complete the checklist in the PR description. Each item needs to be checked off to confirm you've verified these aspects of your changes.\n\n2. **Test Plan**: Include a specific test plan that outlines how you've tested the scheduling functionality. This should cover creating schedules, deleting schedules, and verifying that scheduled runs execute as expected.\n\n3. **PR Scope**: The title mentions only 'platform/library' but your changes span across both frontend and backend components (as evidenced by the file changes and labels). Consider updating the title to better reflect the full scope of changes, such as 'feat(platform): Complete scheduling implementation in Library'.\n\n4. **More Detailed Description**: While your description lists the changes, it would be helpful to provide a bit more context about how these changes improve the scheduling UX and what the expected user experience will be after these changes.\n\nOnce these items are addressed, your PR will be ready for another review.",
      "Thank you for your PR implementing scheduling features in the Library. I have a few points of feedback to help get this ready for merging:\n\n1. **Checklist not completed**: None of the checkboxes in the PR description are checked. Please complete the checklist by checking off all items or removing sections that are not applicable.\n\n2. **Scope in title doesn't match changes**: Your PR title indicates `feat(platform/library)`, but the changes span both frontend and backend components. Consider updating the title to better reflect the scope, perhaps something like `feat(platform): Scheduling UX` or `feat(platform/library,backend): Scheduling UX`.\n\n3. **Backend data modifications**: Since you're modifying files in `backend/data/`, please ensure that any added/changed functions pass the user_id and compare it correctly, or provide an explanation for why this isn't needed.\n\n4. **Test plan**: Please include a test plan that describes how you've tested these scheduling features.\n\nOnce you address these points, we can proceed with the review of the actual code changes.",
      "Thank you for your detailed PR on enhancing the scheduling UX! The description and implementation plan look thorough and well-organized.\n\nHowever, I notice your checklist indicates you haven't completed testing according to your test plan yet. Before this PR can be merged, please:\n\n1. Complete the testing steps you've outlined in your checklist\n2. Check the box to confirm testing has been completed\n\nYour test plan looks comprehensive, covering schedule creation, execution, deletion, and UI state management. Once you've verified all these aspects work correctly, please update the PR.\n\nThe changes themselves look appropriate for the scope defined in your PR title (platform/library scheduling UX improvements), covering both frontend and backend components needed for this feature.",
      "Thanks for this comprehensive PR improving the scheduling functionality! The changes look well-organized and clearly documented.\n\nHowever, I noticed that many of your test plan items are not checked off:\n- You've verified that newly created schedules appear in the list\n- But you haven't confirmed that scheduled runs execute successfully\n- You also haven't verified the deletion functionality works properly\n- And you haven't tested the UI behavior when the last schedule is deleted\n\nBefore we can merge this PR, please complete the testing according to your test plan and update the checklist to reflect the completed tests. This will ensure that all the functionality you've implemented works as expected.\n\nLet me know if you encounter any issues during testing that need to be addressed!"
    ],
    "code_diff": "diff --git a/autogpt_platform/backend/backend/data/__init__.py b/autogpt_platform/backend/backend/data/__init__.py\nnew file mode 100644\nindex 000000000000..7cbc4487be58\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/data/__init__.py\n@@ -0,0 +1,5 @@\n+from .graph import NodeModel\n+from .integrations import Webhook  # noqa: F401\n+\n+# Resolve Webhook <- NodeModel forward reference\n+NodeModel.model_rebuild()\ndiff --git a/autogpt_platform/backend/backend/data/integrations.py b/autogpt_platform/backend/backend/data/integrations.py\nindex d663b7339858..7bc4b3d93f31 100644\n--- a/autogpt_platform/backend/backend/data/integrations.py\n+++ b/autogpt_platform/backend/backend/data/integrations.py\n@@ -77,10 +77,6 @@ def from_db(webhook: IntegrationWebhook):\n         )\n \n \n-# Fix Webhook <- NodeModel relations\n-NodeModel.model_rebuild()\n-\n-\n # --------------------- CRUD functions --------------------- #\n \n \ndiff --git a/autogpt_platform/backend/backend/executor/scheduler.py b/autogpt_platform/backend/backend/executor/scheduler.py\nindex 0c25e58f94c3..e642dbd027a7 100644\n--- a/autogpt_platform/backend/backend/executor/scheduler.py\n+++ b/autogpt_platform/backend/backend/executor/scheduler.py\n@@ -3,6 +3,7 @@\n import os\n from datetime import datetime, timedelta, timezone\n from enum import Enum\n+from typing import Optional\n from urllib.parse import parse_qs, urlencode, urlparse, urlunparse\n \n from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED\n@@ -14,13 +15,16 @@\n from autogpt_libs.utils.cache import thread_cached\n from dotenv import load_dotenv\n from prisma.enums import NotificationType\n-from pydantic import BaseModel, ValidationError\n+from pydantic import BaseModel, Field, ValidationError\n from sqlalchemy import MetaData, create_engine\n \n from backend.data.block import BlockInput\n from backend.data.execution import ExecutionStatus\n+from backend.data.model import CredentialsMetaInput\n from backend.executor import utils as execution_utils\n from backend.notifications.notifications import NotificationManagerClient\n+from backend.util.exceptions import NotAuthorizedError, NotFoundError\n+from backend.util.logging import PrefixFilter\n from backend.util.metrics import sentry_capture_error\n from backend.util.service import (\n     AppService,\n@@ -52,19 +56,19 @@ def _extract_schema_from_url(database_url) -> tuple[str, str]:\n \n \n logger = logging.getLogger(__name__)\n-config = Config()\n-\n+logger.addFilter(PrefixFilter(\"[Scheduler]\"))\n+apscheduler_logger = logger.getChild(\"apscheduler\")\n+apscheduler_logger.addFilter(PrefixFilter(\"[Scheduler] [APScheduler]\"))\n \n-def log(msg, **kwargs):\n-    logger.info(\"[Scheduler] \" + msg, **kwargs)\n+config = Config()\n \n \n def job_listener(event):\n     \"\"\"Logs job execution outcomes for better monitoring.\"\"\"\n     if event.exception:\n-        log(f\"Job {event.job_id} failed.\")\n+        logger.error(f\"Job {event.job_id} failed.\")\n     else:\n-        log(f\"Job {event.job_id} completed successfully.\")\n+        logger.info(f\"Job {event.job_id} completed successfully.\")\n \n \n @thread_cached\n@@ -84,16 +88,17 @@ def execute_graph(**kwargs):\n async def _execute_graph(**kwargs):\n     args = GraphExecutionJobArgs(**kwargs)\n     try:\n-        log(f\"Executing recurring job for graph #{args.graph_id}\")\n+        logger.info(f\"Executing recurring job for graph #{args.graph_id}\")\n         await execution_utils.add_graph_execution(\n-            graph_id=args.graph_id,\n-            inputs=args.input_data,\n             user_id=args.user_id,\n+            graph_id=args.graph_id,\n             graph_version=args.graph_version,\n+            inputs=args.input_data,\n+            graph_credentials_inputs=args.input_credentials,\n             use_db_query=False,\n         )\n     except Exception as e:\n-        logger.exception(f\"Error executing graph {args.graph_id}: {e}\")\n+        logger.error(f\"Error executing graph {args.graph_id}: {e}\")\n \n \n class LateExecutionException(Exception):\n@@ -137,20 +142,20 @@ def report_late_executions() -> str:\n def process_existing_batches(**kwargs):\n     args = NotificationJobArgs(**kwargs)\n     try:\n-        log(\n+        logger.info(\n             f\"Processing existing batches for notification type {args.notification_types}\"\n         )\n         get_notification_client().process_existing_batches(args.notification_types)\n     except Exception as e:\n-        logger.exception(f\"Error processing existing batches: {e}\")\n+        logger.error(f\"Error processing existing batches: {e}\")\n \n \n def process_weekly_summary(**kwargs):\n     try:\n-        log(\"Processing weekly summary\")\n+        logger.info(\"Processing weekly summary\")\n         get_notification_client().queue_weekly_summary()\n     except Exception as e:\n-        logger.exception(f\"Error processing weekly summary: {e}\")\n+        logger.error(f\"Error processing weekly summary: {e}\")\n \n \n class Jobstores(Enum):\n@@ -160,11 +165,12 @@ class Jobstores(Enum):\n \n \n class GraphExecutionJobArgs(BaseModel):\n-    graph_id: str\n-    input_data: BlockInput\n     user_id: str\n+    graph_id: str\n     graph_version: int\n     cron: str\n+    input_data: BlockInput\n+    input_credentials: dict[str, CredentialsMetaInput] = Field(default_factory=dict)\n \n \n class GraphExecutionJobInfo(GraphExecutionJobArgs):\n@@ -247,7 +253,8 @@ def run_service(self):\n                 ),\n                 # These don't really need persistence\n                 Jobstores.WEEKLY_NOTIFICATIONS.value: MemoryJobStore(),\n-            }\n+            },\n+            logger=apscheduler_logger,\n         )\n \n         if self.register_system_tasks:\n@@ -285,34 +292,40 @@ def run_service(self):\n \n     def cleanup(self):\n         super().cleanup()\n-        logger.info(f\"[{self.service_name}]  Shutting down scheduler...\")\n+        logger.info(\" Shutting down scheduler...\")\n         if self.scheduler:\n             self.scheduler.shutdown(wait=False)\n \n     @expose\n     def add_graph_execution_schedule(\n         self,\n+        user_id: str,\n         graph_id: str,\n         graph_version: int,\n         cron: str,\n         input_data: BlockInput,\n-        user_id: str,\n+        input_credentials: dict[str, CredentialsMetaInput],\n+        name: Optional[str] = None,\n     ) -> GraphExecutionJobInfo:\n         job_args = GraphExecutionJobArgs(\n-            graph_id=graph_id,\n-            input_data=input_data,\n             user_id=user_id,\n+            graph_id=graph_id,\n             graph_version=graph_version,\n             cron=cron,\n+            input_data=input_data,\n+            input_credentials=input_credentials,\n         )\n         job = self.scheduler.add_job(\n             execute_graph,\n-            CronTrigger.from_crontab(cron),\n             kwargs=job_args.model_dump(),\n-            replace_existing=True,\n+            name=name,\n+            trigger=CronTrigger.from_crontab(cron),\n             jobstore=Jobstores.EXECUTION.value,\n+            replace_existing=True,\n+        )\n+        logger.info(\n+            f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\"\n         )\n-        log(f\"Added job {job.id} with cron schedule '{cron}' input data: {input_data}\")\n         return GraphExecutionJobInfo.from_db(job_args, job)\n \n     @expose\n@@ -321,14 +334,13 @@ def delete_graph_execution_schedule(\n     ) -> GraphExecutionJobInfo:\n         job = self.scheduler.get_job(schedule_id, jobstore=Jobstores.EXECUTION.value)\n         if not job:\n-            log(f\"Job {schedule_id} not found.\")\n-            raise ValueError(f\"Job #{schedule_id} not found.\")\n+            raise NotFoundError(f\"Job #{schedule_id} not found.\")\n \n         job_args = GraphExecutionJobArgs(**job.kwargs)\n         if job_args.user_id != user_id:\n-            raise ValueError(\"User ID does not match the job's user ID.\")\n+            raise NotAuthorizedError(\"User ID does not match the job's user ID\")\n \n-        log(f\"Deleting job {schedule_id}\")\n+        logger.info(f\"Deleting job {schedule_id}\")\n         job.remove()\n \n         return GraphExecutionJobInfo.from_db(job_args, job)\ndiff --git a/autogpt_platform/backend/backend/executor/scheduler_test.py b/autogpt_platform/backend/backend/executor/scheduler_test.py\nindex 73e6a2e82a49..f6b2b028c2f6 100644\n--- a/autogpt_platform/backend/backend/executor/scheduler_test.py\n+++ b/autogpt_platform/backend/backend/executor/scheduler_test.py\n@@ -27,6 +27,7 @@ async def test_agent_schedule(server: SpinTestServer):\n         graph_version=1,\n         cron=\"0 0 * * *\",\n         input_data={\"input\": \"data\"},\n+        input_credentials={},\n     )\n     assert schedule\n \ndiff --git a/autogpt_platform/backend/backend/server/routers/v1.py b/autogpt_platform/backend/backend/server/routers/v1.py\nindex e177d22d3450..19b8ef07e578 100644\n--- a/autogpt_platform/backend/backend/server/routers/v1.py\n+++ b/autogpt_platform/backend/backend/server/routers/v1.py\n@@ -9,7 +9,7 @@\n from autogpt_libs.auth.middleware import auth_middleware\n from autogpt_libs.feature_flag.client import feature_flag\n from autogpt_libs.utils.cache import thread_cached\n-from fastapi import APIRouter, Body, Depends, HTTPException, Request, Response\n+from fastapi import APIRouter, Body, Depends, HTTPException, Path, Request, Response\n from starlette.status import HTTP_204_NO_CONTENT, HTTP_404_NOT_FOUND\n from typing_extensions import Optional, TypedDict\n \n@@ -72,6 +72,7 @@\n     UpdatePermissionsRequest,\n )\n from backend.server.utils import get_user_id\n+from backend.util.exceptions import NotFoundError\n from backend.util.service import get_service_client\n from backend.util.settings import Settings\n \n@@ -765,68 +766,92 @@ async def delete_graph_execution(\n \n \n class ScheduleCreationRequest(pydantic.BaseModel):\n+    graph_version: Optional[int] = None\n+    name: str\n     cron: str\n-    input_data: dict[Any, Any]\n-    graph_id: str\n-    graph_version: int\n+    inputs: dict[str, Any]\n+    credentials: dict[str, CredentialsMetaInput] = pydantic.Field(default_factory=dict)\n \n \n @v1_router.post(\n-    path=\"/schedules\",\n+    path=\"/graphs/{graph_id}/schedules\",\n     summary=\"Create execution schedule\",\n     tags=[\"schedules\"],\n     dependencies=[Depends(auth_middleware)],\n )\n-async def create_schedule(\n+async def create_graph_execution_schedule(\n     user_id: Annotated[str, Depends(get_user_id)],\n-    schedule: ScheduleCreationRequest,\n+    graph_id: str = Path(..., description=\"ID of the graph to schedule\"),\n+    schedule_params: ScheduleCreationRequest = Body(),\n ) -> scheduler.GraphExecutionJobInfo:\n     graph = await graph_db.get_graph(\n-        schedule.graph_id, schedule.graph_version, user_id=user_id\n+        graph_id=graph_id,\n+        version=schedule_params.graph_version,\n+        user_id=user_id,\n     )\n     if not graph:\n         raise HTTPException(\n             status_code=404,\n-            detail=f\"Graph #{schedule.graph_id} v.{schedule.graph_version} not found.\",\n+            detail=f\"Graph #{graph_id} v{schedule_params.graph_version} not found.\",\n         )\n \n     return await execution_scheduler_client().add_execution_schedule(\n-        graph_id=schedule.graph_id,\n-        graph_version=graph.version,\n-        cron=schedule.cron,\n-        input_data=schedule.input_data,\n         user_id=user_id,\n+        graph_id=graph_id,\n+        graph_version=graph.version,\n+        name=schedule_params.name,\n+        cron=schedule_params.cron,\n+        input_data=schedule_params.inputs,\n+        input_credentials=schedule_params.credentials,\n     )\n \n \n-@v1_router.delete(\n-    path=\"/schedules/{schedule_id}\",\n-    summary=\"Delete execution schedule\",\n+@v1_router.get(\n+    path=\"/graphs/{graph_id}/schedules\",\n+    summary=\"List execution schedules for a graph\",\n     tags=[\"schedules\"],\n     dependencies=[Depends(auth_middleware)],\n )\n-async def delete_schedule(\n-    schedule_id: str,\n+async def list_graph_execution_schedules(\n     user_id: Annotated[str, Depends(get_user_id)],\n-) -> dict[Any, Any]:\n-    await execution_scheduler_client().delete_schedule(schedule_id, user_id=user_id)\n-    return {\"id\": schedule_id}\n+    graph_id: str = Path(),\n+) -> list[scheduler.GraphExecutionJobInfo]:\n+    return await execution_scheduler_client().get_execution_schedules(\n+        user_id=user_id,\n+        graph_id=graph_id,\n+    )\n \n \n @v1_router.get(\n     path=\"/schedules\",\n-    summary=\"List execution schedules\",\n+    summary=\"List execution schedules for a user\",\n     tags=[\"schedules\"],\n     dependencies=[Depends(auth_middleware)],\n )\n-async def get_execution_schedules(\n+async def list_all_graphs_execution_schedules(\n     user_id: Annotated[str, Depends(get_user_id)],\n-    graph_id: str | None = None,\n ) -> list[scheduler.GraphExecutionJobInfo]:\n-    return await execution_scheduler_client().get_execution_schedules(\n-        user_id=user_id,\n-        graph_id=graph_id,\n-    )\n+    return await execution_scheduler_client().get_execution_schedules(user_id=user_id)\n+\n+\n+@v1_router.delete(\n+    path=\"/schedules/{schedule_id}\",\n+    summary=\"Delete execution schedule\",\n+    tags=[\"schedules\"],\n+    dependencies=[Depends(auth_middleware)],\n+)\n+async def delete_graph_execution_schedule(\n+    user_id: Annotated[str, Depends(get_user_id)],\n+    schedule_id: str = Path(..., description=\"ID of the schedule to delete\"),\n+) -> dict[str, Any]:\n+    try:\n+        await execution_scheduler_client().delete_schedule(schedule_id, user_id=user_id)\n+    except NotFoundError:\n+        raise HTTPException(\n+            status_code=HTTP_404_NOT_FOUND,\n+            detail=f\"Schedule #{schedule_id} not found\",\n+        )\n+    return {\"id\": schedule_id}\n \n \n ########################################################\ndiff --git a/autogpt_platform/backend/backend/util/exceptions.py b/autogpt_platform/backend/backend/util/exceptions.py\nindex 0d0fa0cf9638..8f2eacc4eaed 100644\n--- a/autogpt_platform/backend/backend/util/exceptions.py\n+++ b/autogpt_platform/backend/backend/util/exceptions.py\n@@ -10,6 +10,10 @@ class NeedConfirmation(Exception):\n     \"\"\"The user must explicitly confirm that they want to proceed\"\"\"\n \n \n+class NotAuthorizedError(ValueError):\n+    \"\"\"The user is not authorized to perform the requested operation\"\"\"\n+\n+\n class InsufficientBalanceError(ValueError):\n     user_id: str\n     message: str\ndiff --git a/autogpt_platform/backend/backend/util/logging.py b/autogpt_platform/backend/backend/util/logging.py\nindex 9b6a64deac2a..5f822c94a733 100644\n--- a/autogpt_platform/backend/backend/util/logging.py\n+++ b/autogpt_platform/backend/backend/util/logging.py\n@@ -1,4 +1,4 @@\n-from logging import Logger\n+import logging\n \n from backend.util.settings import AppEnvironment, BehaveAs, Settings\n \n@@ -6,8 +6,6 @@\n \n \n def configure_logging():\n-    import logging\n-\n     import autogpt_libs.logging.config\n \n     if (\n@@ -25,7 +23,7 @@ def configure_logging():\n class TruncatedLogger:\n     def __init__(\n         self,\n-        logger: Logger,\n+        logger: logging.Logger,\n         prefix: str = \"\",\n         metadata: dict | None = None,\n         max_length: int = 1000,\n@@ -65,3 +63,13 @@ def _wrap(self, msg: str, **extra):\n         if len(text) > self.max_length:\n             text = text[: self.max_length] + \"...\"\n         return text\n+\n+\n+class PrefixFilter(logging.Filter):\n+    def __init__(self, prefix: str):\n+        super().__init__()\n+        self.prefix = prefix\n+\n+    def filter(self, record):\n+        record.msg = f\"{self.prefix} {record.msg}\"\n+        return True\ndiff --git a/autogpt_platform/backend/backend/util/service.py b/autogpt_platform/backend/backend/util/service.py\nindex abcfd75dd3a8..d9e3e55eb8a6 100644\n--- a/autogpt_platform/backend/backend/util/service.py\n+++ b/autogpt_platform/backend/backend/util/service.py\n@@ -31,7 +31,7 @@\n     wait_exponential_jitter,\n )\n \n-from backend.util.exceptions import InsufficientBalanceError\n+import backend.util.exceptions as exceptions\n from backend.util.json import to_dict\n from backend.util.metrics import sentry_init\n from backend.util.process import AppProcess, get_service_name\n@@ -106,7 +106,13 @@ class RemoteCallError(BaseModel):\n         ValueError,\n         TimeoutError,\n         ConnectionError,\n-        InsufficientBalanceError,\n+        *[\n+            ErrorType\n+            for _, ErrorType in inspect.getmembers(exceptions)\n+            if inspect.isclass(ErrorType)\n+            and issubclass(ErrorType, Exception)\n+            and ErrorType.__module__ == exceptions.__name__\n+        ],\n     ]\n }\n \ndiff --git a/autogpt_platform/frontend/src/api/openapi.json b/autogpt_platform/frontend/src/api/openapi.json\nindex 03acbe4216d1..30b826fad545 100644\n--- a/autogpt_platform/frontend/src/api/openapi.json\n+++ b/autogpt_platform/frontend/src/api/openapi.json\n@@ -3191,6 +3191,48 @@\n         }\n       }\n     },\n+    \"/api/library/agents/by-graph/{graph_id}\": {\n+      \"get\": {\n+        \"tags\": [\"v2\", \"library\", \"private\"],\n+        \"summary\": \"Get Library Agent By Graph Id\",\n+        \"operationId\": \"getV2GetLibraryAgentByGraphId\",\n+        \"parameters\": [\n+          {\n+            \"name\": \"graph_id\",\n+            \"in\": \"path\",\n+            \"required\": true,\n+            \"schema\": { \"type\": \"string\", \"title\": \"Graph Id\" }\n+          },\n+          {\n+            \"name\": \"version\",\n+            \"in\": \"query\",\n+            \"required\": false,\n+            \"schema\": {\n+              \"anyOf\": [{ \"type\": \"integer\" }, { \"type\": \"null\" }],\n+              \"title\": \"Version\"\n+            }\n+          }\n+        ],\n+        \"responses\": {\n+          \"200\": {\n+            \"description\": \"Successful Response\",\n+            \"content\": {\n+              \"application/json\": {\n+                \"schema\": { \"$ref\": \"#/components/schemas/LibraryAgent\" }\n+              }\n+            }\n+          },\n+          \"422\": {\n+            \"description\": \"Validation Error\",\n+            \"content\": {\n+              \"application/json\": {\n+                \"schema\": { \"$ref\": \"#/components/schemas/HTTPValidationError\" }\n+              }\n+            }\n+          }\n+        }\n+      }\n+    },\n     \"/api/library/agents/marketplace/{store_listing_version_id}\": {\n       \"get\": {\n         \"tags\": [\"v2\", \"library\", \"private\", \"store, library\"],\n@@ -5012,6 +5054,7 @@\n           \"AGENT_INPUT\",\n           \"CONGRATS\",\n           \"GET_RESULTS\",\n+          \"RUN_AGENTS\",\n           \"MARKETPLACE_VISIT\",\n           \"MARKETPLACE_ADD_AGENT\",\n           \"MARKETPLACE_RUN_AGENT\",\ndiff --git a/autogpt_platform/frontend/src/app/(platform)/library/agents/[id]/page.tsx b/autogpt_platform/frontend/src/app/(platform)/library/agents/[id]/page.tsx\nindex 506968e7a6c5..84aae2898f2e 100644\n--- a/autogpt_platform/frontend/src/app/(platform)/library/agents/[id]/page.tsx\n+++ b/autogpt_platform/frontend/src/app/(platform)/library/agents/[id]/page.tsx\n@@ -64,9 +64,10 @@ export default function AgentRunsPage(): React.ReactElement {\n   const [selectedRun, setSelectedRun] = useState<\n     GraphExecution | GraphExecutionMeta | null\n   >(null);\n-  const [selectedSchedule, setSelectedSchedule] = useState<Schedule | null>(\n-    null,\n-  );\n+  const selectedSchedule =\n+    selectedView.type == \"schedule\"\n+      ? schedules.find((s) => s.id == selectedView.id)\n+      : null;\n   const [isFirstLoad, setIsFirstLoad] = useState<boolean>(true);\n   const [agentDeleteDialogOpen, setAgentDeleteDialogOpen] =\n     useState<boolean>(false);\n@@ -100,9 +101,8 @@ export default function AgentRunsPage(): React.ReactElement {\n     selectView({ type: \"preset\", id });\n   }, []);\n \n-  const selectSchedule = useCallback((schedule: Schedule) => {\n-    selectView({ type: \"schedule\", id: schedule.id });\n-    setSelectedSchedule(schedule);\n+  const selectSchedule = useCallback((id: ScheduleID) => {\n+    selectView({ type: \"schedule\", id });\n   }, []);\n \n   const graphVersions = useRef<Record<number, Graph>>({});\n@@ -315,11 +315,8 @@ export default function AgentRunsPage(): React.ReactElement {\n   const fetchSchedules = useCallback(async () => {\n     if (!agent) return;\n \n-    // TODO: filter in backend - https://github.com/Significant-Gravitas/AutoGPT/issues/9183\n-    setSchedules(\n-      (await api.listSchedules()).filter((s) => s.graph_id == agent.graph_id),\n-    );\n-  }, [api, agent]);\n+    setSchedules(await api.listGraphExecutionSchedules(agent.graph_id));\n+  }, [api, agent?.graph_id]);\n \n   useEffect(() => {\n     fetchSchedules();\n@@ -358,8 +355,28 @@ export default function AgentRunsPage(): React.ReactElement {\n \n   const deleteSchedule = useCallback(\n     async (scheduleID: ScheduleID) => {\n-      const removedSchedule = await api.deleteSchedule(scheduleID);\n-      setSchedules(schedules.filter((s) => s.id !== removedSchedule.id));\n+      const removedSchedule =\n+        await api.deleteGraphExecutionSchedule(scheduleID);\n+\n+      setSchedules((schedules) => {\n+        const newSchedules = schedules.filter(\n+          (s) => s.id !== removedSchedule.id,\n+        );\n+        if (\n+          selectedView.type == \"schedule\" &&\n+          selectedView.id == removedSchedule.id\n+        ) {\n+          if (newSchedules.length > 0) {\n+            // Select next schedule if available\n+            selectSchedule(newSchedules[0].id);\n+          } else {\n+            // Reset to draft view if current schedule was deleted\n+            openRunDraftView();\n+          }\n+        }\n+        return newSchedules;\n+      });\n+      openRunDraftView();\n     },\n     [schedules, api],\n   );\n@@ -417,6 +434,14 @@ export default function AgentRunsPage(): React.ReactElement {\n     [agent, downloadGraph],\n   );\n \n+  const onCreateSchedule = useCallback(\n+    (schedule: Schedule) => {\n+      setSchedules((prev) => [...prev, schedule]);\n+      selectSchedule(schedule.id);\n+    },\n+    [selectView],\n+  );\n+\n   const onCreatePreset = useCallback(\n     (preset: LibraryAgentPreset) => {\n       setAgentPresets((prev) => [...prev, preset]);\n@@ -454,9 +479,9 @@ export default function AgentRunsPage(): React.ReactElement {\n         onSelectPreset={selectPreset}\n         onSelectSchedule={selectSchedule}\n         onSelectDraftNewRun={openRunDraftView}\n-        onDeleteRun={setConfirmingDeleteAgentRun}\n-        onDeletePreset={setConfirmingDeleteAgentPreset}\n-        onDeleteSchedule={deleteSchedule}\n+        doDeleteRun={setConfirmingDeleteAgentRun}\n+        doDeletePreset={setConfirmingDeleteAgentPreset}\n+        doDeleteSchedule={deleteSchedule}\n       />\n \n       <div className=\"flex-1\">\n@@ -486,6 +511,7 @@ export default function AgentRunsPage(): React.ReactElement {\n           <AgentRunDraftView\n             agent={agent}\n             onRun={selectRun}\n+            onCreateSchedule={onCreateSchedule}\n             onCreatePreset={onCreatePreset}\n             agentActions={agentActions}\n           />\n@@ -497,6 +523,7 @@ export default function AgentRunsPage(): React.ReactElement {\n               agentPresets.find((preset) => preset.id == selectedView.id)!\n             }\n             onRun={selectRun}\n+            onCreateSchedule={onCreateSchedule}\n             onUpdatePreset={onUpdatePreset}\n             doDeletePreset={setConfirmingDeleteAgentPreset}\n             agentActions={agentActions}\n@@ -506,8 +533,10 @@ export default function AgentRunsPage(): React.ReactElement {\n             <AgentScheduleDetailsView\n               graph={graph}\n               schedule={selectedSchedule}\n-              onForcedRun={selectRun}\n+              // agent={agent}\n               agentActions={agentActions}\n+              onForcedRun={selectRun}\n+              doDeleteSchedule={deleteSchedule}\n             />\n           )\n         ) : null) || <LoadingBox className=\"h-[70vh]\" />}\ndiff --git a/autogpt_platform/frontend/src/app/(platform)/monitoring/page.tsx b/autogpt_platform/frontend/src/app/(platform)/monitoring/page.tsx\nindex e6c1f11efb9c..434103bf34b1 100644\n--- a/autogpt_platform/frontend/src/app/(platform)/monitoring/page.tsx\n+++ b/autogpt_platform/frontend/src/app/(platform)/monitoring/page.tsx\n@@ -32,12 +32,13 @@ const Monitor = () => {\n   const api = useBackendAPI();\n \n   const fetchSchedules = useCallback(async () => {\n-    setSchedules(await api.listSchedules());\n+    setSchedules(await api.listAllGraphsExecutionSchedules());\n   }, [api]);\n \n   const removeSchedule = useCallback(\n     async (scheduleId: ScheduleID) => {\n-      const removedSchedule = await api.deleteSchedule(scheduleId);\n+      const removedSchedule =\n+        await api.deleteGraphExecutionSchedule(scheduleId);\n       setSchedules(schedules.filter((s) => s.id !== removedSchedule.id));\n     },\n     [schedules, api],\ndiff --git a/autogpt_platform/frontend/src/components/Flow.tsx b/autogpt_platform/frontend/src/components/Flow.tsx\nindex 345fc60b5ae8..55f324036f30 100644\n--- a/autogpt_platform/frontend/src/components/Flow.tsx\n+++ b/autogpt_platform/frontend/src/components/Flow.tsx\n@@ -52,7 +52,7 @@ import PrimaryActionBar from \"@/components/PrimaryActionButton\";\n import OttoChatWidget from \"@/components/OttoChatWidget\";\n import { useToast } from \"@/components/ui/use-toast\";\n import { useCopyPaste } from \"../hooks/useCopyPaste\";\n-import { CronScheduler } from \"./cronScheduler\";\n+import { CronSchedulerDialog } from \"@/components/cron-scheduler-dialog\";\n \n // This is for the history, this is the minimum distance a block must move before it is logged\n // It helps to prevent spamming the history with small movements especially when pressing on a input in a block\n@@ -639,8 +639,11 @@ const FlowEditor: React.FC<{\n \n   // This function is called after cron expression is created\n   // So you can collect inputs for scheduling\n-  const afterCronCreation = (cronExpression: string) => {\n-    runnerUIRef.current?.collectInputsForScheduling(cronExpression);\n+  const afterCronCreation = (cronExpression: string, scheduleName: string) => {\n+    runnerUIRef.current?.collectInputsForScheduling(\n+      cronExpression,\n+      scheduleName,\n+    );\n   };\n \n   // This function Opens up form for creating cron expression\n@@ -728,10 +731,11 @@ const FlowEditor: React.FC<{\n             requestStopRun={requestStopRun}\n             runAgentTooltip={!isRunning ? \"Run Agent\" : \"Stop Agent\"}\n           />\n-          <CronScheduler\n-            afterCronCreation={afterCronCreation}\n+          <CronSchedulerDialog\n             open={openCron}\n             setOpen={setOpenCron}\n+            afterCronCreation={afterCronCreation}\n+            defaultScheduleName={agentName}\n           />\n         </ReactFlow>\n       </div>\ndiff --git a/autogpt_platform/frontend/src/components/RunnerUIWrapper.tsx b/autogpt_platform/frontend/src/components/RunnerUIWrapper.tsx\nindex 2f727542058c..37ead841481a 100644\n--- a/autogpt_platform/frontend/src/components/RunnerUIWrapper.tsx\n+++ b/autogpt_platform/frontend/src/components/RunnerUIWrapper.tsx\n@@ -36,14 +36,21 @@ interface RunnerUIWrapperProps {\n   isRunning: boolean;\n   isScheduling: boolean;\n   requestSaveAndRun: () => void;\n-  scheduleRunner: (cronExpression: string, input: InputItem[]) => Promise<void>;\n+  scheduleRunner: (\n+    cronExpression: string,\n+    input: InputItem[],\n+    scheduleName: string,\n+  ) => Promise<void>;\n }\n \n export interface RunnerUIWrapperRef {\n   openRunnerInput: () => void;\n   openRunnerOutput: () => void;\n   runOrOpenInput: () => void;\n-  collectInputsForScheduling: (cronExpression: string) => void;\n+  collectInputsForScheduling: (\n+    cronExpression: string,\n+    scheduleName: string,\n+  ) => void;\n }\n \n const RunnerUIWrapper = forwardRef<RunnerUIWrapperRef, RunnerUIWrapperProps>(\n@@ -63,6 +70,7 @@ const RunnerUIWrapper = forwardRef<RunnerUIWrapperRef, RunnerUIWrapperProps>(\n     const [isRunnerOutputOpen, setIsRunnerOutputOpen] = useState(false);\n     const [scheduledInput, setScheduledInput] = useState(false);\n     const [cronExpression, setCronExpression] = useState(\"\");\n+    const [scheduleName, setScheduleName] = useState(\"\");\n \n     const getBlockInputsAndOutputs = useCallback((): {\n       inputs: InputItem[];\n@@ -149,15 +157,19 @@ const RunnerUIWrapper = forwardRef<RunnerUIWrapperRef, RunnerUIWrapperProps>(\n       }\n     };\n \n-    const collectInputsForScheduling = (cron_exp: string) => {\n+    const collectInputsForScheduling = (\n+      cronExpression: string,\n+      scheduleName: string,\n+    ) => {\n       const { inputs } = getBlockInputsAndOutputs();\n-      setCronExpression(cron_exp);\n+      setCronExpression(cronExpression);\n+      setScheduleName(scheduleName);\n \n       if (inputs.length > 0) {\n         setScheduledInput(true);\n         setIsRunnerInputOpen(true);\n       } else {\n-        scheduleRunner(cron_exp, []);\n+        scheduleRunner(cronExpression, [], scheduleName);\n       }\n     };\n \n@@ -186,6 +198,7 @@ const RunnerUIWrapper = forwardRef<RunnerUIWrapperRef, RunnerUIWrapperProps>(\n             await scheduleRunner(\n               cronExpression,\n               getBlockInputsAndOutputs().inputs,\n+              scheduleName,\n             );\n             setIsScheduling(false);\n             setIsRunnerInputOpen(false);\ndiff --git a/autogpt_platform/frontend/src/components/agents/agent-run-draft-view.tsx b/autogpt_platform/frontend/src/components/agents/agent-run-draft-view.tsx\nindex 752f887ec077..23628f0a3183 100644\n--- a/autogpt_platform/frontend/src/components/agents/agent-run-draft-view.tsx\n+++ b/autogpt_platform/frontend/src/components/agents/agent-run-draft-view.tsx\n@@ -9,17 +9,19 @@ import {\n   LibraryAgentPreset,\n   LibraryAgentPresetID,\n   LibraryAgentPresetUpdatable,\n+  Schedule,\n } from \"@/lib/autogpt-server-api\";\n \n import type { ButtonAction } from \"@/components/agptui/types\";\n import { Card, CardContent, CardHeader, CardTitle } from \"@/components/ui/card\";\n import { IconCross, IconPlay, IconSave } from \"@/components/ui/icons\";\n+import { CalendarClockIcon, Trash2Icon } from \"lucide-react\";\n+import { CronSchedulerDialog } from \"@/components/cron-scheduler-dialog\";\n import { CredentialsInput } from \"@/components/integrations/credentials-input\";\n import { TypeBasedInput } from \"@/components/type-based-input\";\n import { useToastOnFail } from \"@/components/ui/use-toast\";\n import ActionButtonGroup from \"@/components/agptui/action-button-group\";\n import { useOnboarding } from \"@/components/onboarding/onboarding-provider\";\n-import { Trash2Icon } from \"lucide-react\";\n import SchemaTooltip from \"@/components/SchemaTooltip\";\n import { useToast } from \"@/components/ui/use-toast\";\n import { isEmpty } from \"lodash\";\n@@ -32,11 +34,13 @@ export default function AgentRunDraftView({\n   onCreatePreset,\n   onUpdatePreset,\n   doDeletePreset,\n+  onCreateSchedule,\n   agentActions,\n }: {\n   agent: LibraryAgent;\n   agentActions: ButtonAction[];\n   onRun: (runID: GraphExecutionID) => void;\n+  onCreateSchedule: (schedule: Schedule) => void;\n } & (\n   | {\n       onCreatePreset: (preset: LibraryAgentPreset) => void;\n@@ -66,6 +70,7 @@ export default function AgentRunDraftView({\n   >(new Set());\n   const { state: onboardingState, completeStep: completeOnboardingStep } =\n     useOnboarding();\n+  const [cronScheduleDialogOpen, setCronScheduleDialogOpen] = useState(false);\n \n   // Update values if agentPreset parameter is changed\n   useEffect(() => {\n@@ -314,6 +319,43 @@ export default function AgentRunDraftView({\n     completeOnboardingStep,\n   ]);\n \n+  const openScheduleDialog = useCallback(() => {\n+    // Scheduling is not supported for webhook-triggered agents\n+    if (agent.has_external_trigger) return;\n+\n+    if (!allRequiredInputsAreSet || !allCredentialsAreSet) {\n+      notifyMissingInputs(false);\n+      return;\n+    }\n+\n+    setCronScheduleDialogOpen(true);\n+  }, [\n+    agent,\n+    allRequiredInputsAreSet,\n+    allCredentialsAreSet,\n+    notifyMissingInputs,\n+  ]);\n+\n+  const doSetupSchedule = useCallback(\n+    (cronExpression: string, scheduleName: string) => {\n+      // Scheduling is not supported for webhook-triggered agents\n+      if (agent.has_external_trigger) return;\n+\n+      api\n+        .createGraphExecutionSchedule({\n+          graph_id: agent.graph_id,\n+          graph_version: agent.graph_version,\n+          name: scheduleName || agent.name,\n+          cron: cronExpression,\n+          inputs: inputValues,\n+          credentials: inputCredentials,\n+        })\n+        .then((schedule) => onCreateSchedule(schedule))\n+        .catch(toastOnFail(\"set up agent run schedule\"));\n+    },\n+    [api, agent, inputValues, inputCredentials, onCreateSchedule, toastOnFail],\n+  );\n+\n   const runActions: ButtonAction[] = useMemo(\n     () => [\n       // \"Regular\" agent: [run] + [save as preset] buttons\n@@ -328,6 +370,14 @@ export default function AgentRunDraftView({\n               variant: \"accent\",\n               callback: doRun,\n             },\n+            {\n+              label: (\n+                <>\n+                  <CalendarClockIcon className=\"mr-2 size-4\" /> Schedule\n+                </>\n+              ),\n+              callback: openScheduleDialog,\n+            },\n             // {\n             //   label: (\n             //     <>\n@@ -418,12 +468,12 @@ export default function AgentRunDraftView({\n     [\n       agent.has_external_trigger,\n       agentPreset,\n-      api,\n       doRun,\n       doSetupTrigger,\n       doCreatePreset,\n       doUpdatePreset,\n       doDeletePreset,\n+      openScheduleDialog,\n       changedPresetAttributes,\n       presetName,\n       allRequiredInputsAreSet,\n@@ -545,6 +595,12 @@ export default function AgentRunDraftView({\n             title={`${agent.has_external_trigger ? \"Trigger\" : agentPreset ? \"Preset\" : \"Run\"} actions`}\n             actions={runActions}\n           />\n+          <CronSchedulerDialog\n+            open={cronScheduleDialogOpen}\n+            setOpen={setCronScheduleDialogOpen}\n+            afterCronCreation={doSetupSchedule}\n+            defaultScheduleName={agent.name}\n+          />\n \n           <ActionButtonGroup title=\"Agent actions\" actions={agentActions} />\n         </div>\ndiff --git a/autogpt_platform/frontend/src/components/agents/agent-runs-selector-list.tsx b/autogpt_platform/frontend/src/components/agents/agent-runs-selector-list.tsx\nindex 634a809bb6c5..7d6dc483828f 100644\n--- a/autogpt_platform/frontend/src/components/agents/agent-runs-selector-list.tsx\n+++ b/autogpt_platform/frontend/src/components/agents/agent-runs-selector-list.tsx\n@@ -1,5 +1,5 @@\n \"use client\";\n-import React, { useState } from \"react\";\n+import React, { useEffect, useState } from \"react\";\n import { Plus } from \"lucide-react\";\n \n import { cn } from \"@/lib/utils\";\n@@ -30,11 +30,11 @@ interface AgentRunsSelectorListProps {\n   allowDraftNewRun?: boolean;\n   onSelectRun: (id: GraphExecutionID) => void;\n   onSelectPreset: (preset: LibraryAgentPresetID) => void;\n-  onSelectSchedule: (schedule: Schedule) => void;\n+  onSelectSchedule: (id: ScheduleID) => void;\n   onSelectDraftNewRun: () => void;\n-  onDeleteRun: (id: GraphExecutionMeta) => void;\n-  onDeletePreset: (id: LibraryAgentPresetID) => void;\n-  onDeleteSchedule: (id: ScheduleID) => void;\n+  doDeleteRun: (id: GraphExecutionMeta) => void;\n+  doDeletePreset: (id: LibraryAgentPresetID) => void;\n+  doDeleteSchedule: (id: ScheduleID) => void;\n   className?: string;\n }\n \n@@ -49,15 +49,23 @@ export default function AgentRunsSelectorList({\n   onSelectPreset,\n   onSelectSchedule,\n   onSelectDraftNewRun,\n-  onDeleteRun,\n-  onDeletePreset,\n-  onDeleteSchedule,\n+  doDeleteRun,\n+  doDeletePreset,\n+  doDeleteSchedule,\n   className,\n }: AgentRunsSelectorListProps): React.ReactElement {\n   const [activeListTab, setActiveListTab] = useState<\"runs\" | \"scheduled\">(\n     \"runs\",\n   );\n \n+  useEffect(() => {\n+    if (selectedView.type === \"schedule\") {\n+      setActiveListTab(\"scheduled\");\n+    } else {\n+      setActiveListTab(\"runs\");\n+    }\n+  }, [selectedView]);\n+\n   const listItemClasses = \"h-28 w-72 lg:h-32 xl:w-80\";\n \n   return (\n@@ -94,9 +102,7 @@ export default function AgentRunsSelectorList({\n           onClick={() => setActiveListTab(\"scheduled\")}\n         >\n           <span>Scheduled</span>\n-          <span className=\"text-neutral-600\">\n-            {schedules.filter((s) => s.graph_id === agent.graph_id).length}\n-          </span>\n+          <span className=\"text-neutral-600\">{schedules.length}</span>\n         </Badge>\n       </div>\n \n@@ -136,7 +142,7 @@ export default function AgentRunsSelectorList({\n                     // timestamp={preset.last_run_time} // TODO: implement this\n                     selected={selectedView.id === preset.id}\n                     onClick={() => onSelectPreset(preset.id)}\n-                    onDelete={() => onDeletePreset(preset.id)}\n+                    onDelete={() => doDeletePreset(preset.id)}\n                   />\n                 ))}\n               {agentPresets.length > 0 && <Separator className=\"my-1\" />}\n@@ -158,26 +164,24 @@ export default function AgentRunsSelectorList({\n                     timestamp={run.started_at}\n                     selected={selectedView.id === run.id}\n                     onClick={() => onSelectRun(run.id)}\n-                    onDelete={() => onDeleteRun(run)}\n+                    onDelete={() => doDeleteRun(run)}\n                   />\n                 ))}\n             </>\n           ) : (\n-            schedules\n-              .filter((schedule) => schedule.graph_id === agent.graph_id)\n-              .map((schedule) => (\n-                <AgentRunSummaryCard\n-                  className={listItemClasses}\n-                  key={schedule.id}\n-                  type=\"schedule\"\n-                  status=\"scheduled\" // TODO: implement active/inactive status for schedules\n-                  title={schedule.name}\n-                  timestamp={schedule.next_run_time}\n-                  selected={selectedView.id === schedule.id}\n-                  onClick={() => onSelectSchedule(schedule)}\n-                  onDelete={() => onDeleteSchedule(schedule.id)}\n-                />\n-              ))\n+            schedules.map((schedule) => (\n+              <AgentRunSummaryCard\n+                className={listItemClasses}\n+                key={schedule.id}\n+                type=\"schedule\"\n+                status=\"scheduled\" // TODO: implement active/inactive status for schedules\n+                title={schedule.name}\n+                timestamp={schedule.next_run_time}\n+                selected={selectedView.id === schedule.id}\n+                onClick={() => onSelectSchedule(schedule.id)}\n+                onDelete={() => doDeleteSchedule(schedule.id)}\n+              />\n+            ))\n           )}\n         </div>\n       </ScrollArea>\ndiff --git a/autogpt_platform/frontend/src/components/agents/agent-schedule-details-view.tsx b/autogpt_platform/frontend/src/components/agents/agent-schedule-details-view.tsx\nindex da03527a79b0..2ae2c2fde09e 100644\n--- a/autogpt_platform/frontend/src/components/agents/agent-schedule-details-view.tsx\n+++ b/autogpt_platform/frontend/src/components/agents/agent-schedule-details-view.tsx\n@@ -5,27 +5,33 @@ import {\n   GraphExecutionID,\n   GraphMeta,\n   Schedule,\n+  ScheduleID,\n } from \"@/lib/autogpt-server-api\";\n import { useBackendAPI } from \"@/lib/autogpt-server-api/context\";\n \n import type { ButtonAction } from \"@/components/agptui/types\";\n import { Card, CardContent, CardHeader, CardTitle } from \"@/components/ui/card\";\n+import { humanizeCronExpression } from \"@/lib/cron-expression-utils\";\n import { AgentRunStatus } from \"@/components/agents/agent-run-status-chip\";\n import { useToastOnFail } from \"@/components/ui/use-toast\";\n import ActionButtonGroup from \"@/components/agptui/action-button-group\";\n+import { IconCross } from \"@/components/ui/icons\";\n+import { PlayIcon } from \"lucide-react\";\n import LoadingBox from \"@/components/ui/loading\";\n import { Input } from \"@/components/ui/input\";\n \n export default function AgentScheduleDetailsView({\n   graph,\n   schedule,\n-  onForcedRun,\n   agentActions,\n+  onForcedRun,\n+  doDeleteSchedule,\n }: {\n   graph: GraphMeta;\n   schedule: Schedule;\n-  onForcedRun: (runID: GraphExecutionID) => void;\n   agentActions: ButtonAction[];\n+  onForcedRun: (runID: GraphExecutionID) => void;\n+  doDeleteSchedule: (scheduleID: ScheduleID) => void;\n }): React.ReactNode {\n   const api = useBackendAPI();\n \n@@ -42,7 +48,11 @@ export default function AgentScheduleDetailsView({\n           selectedRunStatus.slice(1),\n       },\n       {\n-        label: \"Scheduled for\",\n+        label: \"Schedule\",\n+        value: humanizeCronExpression(schedule.cron),\n+      },\n+      {\n+        label: \"Next run\",\n         value: schedule.next_run_time.toLocaleString(),\n       },\n     ];\n@@ -70,14 +80,39 @@ export default function AgentScheduleDetailsView({\n   const runNow = useCallback(\n     () =>\n       api\n-        .executeGraph(graph.id, graph.version, schedule.input_data)\n+        .executeGraph(\n+          graph.id,\n+          graph.version,\n+          schedule.input_data,\n+          schedule.input_credentials,\n+        )\n         .then((run) => onForcedRun(run.graph_exec_id))\n         .catch(toastOnFail(\"execute agent\")),\n     [api, graph, schedule, onForcedRun, toastOnFail],\n   );\n \n   const runActions: ButtonAction[] = useMemo(\n-    () => [{ label: \"Run now\", callback: () => runNow() }],\n+    () => [\n+      {\n+        label: (\n+          <>\n+            <PlayIcon className=\"mr-2 size-4\" />\n+            Run now\n+          </>\n+        ),\n+        callback: runNow,\n+      },\n+      {\n+        label: (\n+          <>\n+            <IconCross className=\"mr-2 size-4 px-0.5\" />\n+            Delete schedule\n+          </>\n+        ),\n+        callback: () => doDeleteSchedule(schedule.id),\n+        variant: \"destructive\",\n+      },\n+    ],\n     [runNow],\n   );\n \ndiff --git a/autogpt_platform/frontend/src/components/cron-scheduler-dialog.tsx b/autogpt_platform/frontend/src/components/cron-scheduler-dialog.tsx\nnew file mode 100644\nindex 000000000000..f7ea368fe2db\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/cron-scheduler-dialog.tsx\n@@ -0,0 +1,76 @@\n+import { useEffect, useState } from \"react\";\n+import { Input } from \"@/components/ui/input\";\n+import { Button } from \"@/components/ui/button\";\n+import { useToast } from \"@/components/ui/use-toast\";\n+import { Separator } from \"@/components/ui/separator\";\n+import { CronScheduler } from \"@/components/cron-scheduler\";\n+import { Dialog, DialogContent, DialogTitle } from \"@/components/ui/dialog\";\n+\n+type CronSchedulerDialogProps = {\n+  open: boolean;\n+  setOpen: (open: boolean) => void;\n+  afterCronCreation: (cronExpression: string, scheduleName: string) => void;\n+  defaultScheduleName?: string;\n+};\n+\n+export function CronSchedulerDialog({\n+  open,\n+  setOpen,\n+  afterCronCreation,\n+  defaultScheduleName = \"\",\n+}: CronSchedulerDialogProps) {\n+  const { toast } = useToast();\n+  const [cronExpression, setCronExpression] = useState<string>(\"\");\n+  const [scheduleName, setScheduleName] = useState<string>(defaultScheduleName);\n+\n+  // Reset state when dialog opens\n+  useEffect(() => {\n+    if (open) {\n+      setScheduleName(defaultScheduleName);\n+      setCronExpression(\"\");\n+    }\n+  }, [open]);\n+\n+  const handleDone = () => {\n+    if (!scheduleName.trim()) {\n+      toast({\n+        title: \"Please enter a schedule name\",\n+        variant: \"destructive\",\n+      });\n+      return;\n+    }\n+    afterCronCreation(cronExpression, scheduleName);\n+    setOpen(false);\n+  };\n+\n+  return (\n+    <Dialog open={open} onOpenChange={setOpen}>\n+      <DialogContent>\n+        <DialogTitle>Schedule Task</DialogTitle>\n+        <div className=\"p-2\">\n+          <div className=\"flex flex-col gap-4\">\n+            <div className=\"flex flex-col space-y-2\">\n+              <label className=\"text-sm font-medium\">Schedule Name</label>\n+              <Input\n+                value={scheduleName}\n+                onChange={(e) => setScheduleName(e.target.value)}\n+                placeholder=\"Enter a name for this schedule\"\n+              />\n+            </div>\n+\n+            <CronScheduler onCronExpressionChange={setCronExpression} />\n+          </div>\n+\n+          <Separator className=\"my-4\" />\n+\n+          <div className=\"flex justify-end space-x-2\">\n+            <Button variant=\"outline\" onClick={() => setOpen(false)}>\n+              Cancel\n+            </Button>\n+            <Button onClick={handleDone}>Done</Button>\n+          </div>\n+        </div>\n+      </DialogContent>\n+    </Dialog>\n+  );\n+}\ndiff --git a/autogpt_platform/frontend/src/components/cron-scheduler.tsx b/autogpt_platform/frontend/src/components/cron-scheduler.tsx\nnew file mode 100644\nindex 000000000000..2ffc8a0857e3\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/cron-scheduler.tsx\n@@ -0,0 +1,338 @@\n+import React, { useEffect, useState } from \"react\";\n+import {\n+  Select,\n+  SelectContent,\n+  SelectItem,\n+  SelectTrigger,\n+  SelectValue,\n+} from \"@/components/ui/select\";\n+import { Label } from \"@/components/ui/label\";\n+import { Input } from \"@/components/ui/input\";\n+import { Button } from \"@/components/ui/button\";\n+import { CronFrequency, makeCronExpression } from \"@/lib/cron-expression-utils\";\n+\n+const weekDays = [\n+  { label: \"Su\", value: 0 },\n+  { label: \"Mo\", value: 1 },\n+  { label: \"Tu\", value: 2 },\n+  { label: \"We\", value: 3 },\n+  { label: \"Th\", value: 4 },\n+  { label: \"Fr\", value: 5 },\n+  { label: \"Sa\", value: 6 },\n+];\n+\n+const months = [\n+  { label: \"Jan\", value: \"January\" },\n+  { label: \"Feb\", value: \"February\" },\n+  { label: \"Mar\", value: \"March\" },\n+  { label: \"Apr\", value: \"April\" },\n+  { label: \"May\", value: \"May\" },\n+  { label: \"Jun\", value: \"June\" },\n+  { label: \"Jul\", value: \"July\" },\n+  { label: \"Aug\", value: \"August\" },\n+  { label: \"Sep\", value: \"September\" },\n+  { label: \"Oct\", value: \"October\" },\n+  { label: \"Nov\", value: \"November\" },\n+  { label: \"Dec\", value: \"December\" },\n+];\n+\n+type CronSchedulerProps = {\n+  onCronExpressionChange: (cronExpression: string) => void;\n+};\n+\n+export function CronScheduler({\n+  onCronExpressionChange,\n+}: CronSchedulerProps): React.ReactElement {\n+  const [frequency, setFrequency] = useState<CronFrequency>(\"daily\");\n+  const [selectedMinute, setSelectedMinute] = useState<string>(\"0\");\n+  const [selectedTime, setSelectedTime] = useState<string>(\"09:00\");\n+  const [selectedWeekDays, setSelectedWeekDays] = useState<number[]>([]);\n+  const [selectedMonthDays, setSelectedMonthDays] = useState<number[]>([]);\n+  const [selectedMonths, setSelectedMonths] = useState<number[]>([]);\n+  const [customInterval, setCustomInterval] = useState<{\n+    value: number;\n+    unit: \"minutes\" | \"hours\" | \"days\";\n+  }>({ value: 1, unit: \"minutes\" });\n+  const [showCustomDays, setShowCustomDays] = useState<boolean>(false);\n+\n+  useEffect(() => {\n+    const cronExpr = makeCronExpression({\n+      frequency,\n+      minute:\n+        frequency === \"hourly\"\n+          ? parseInt(selectedMinute)\n+          : parseInt(selectedTime.split(\":\")[1]),\n+      hour: parseInt(selectedTime.split(\":\")[0]),\n+      days:\n+        frequency === \"weekly\"\n+          ? selectedWeekDays\n+          : frequency === \"monthly\"\n+            ? selectedMonthDays\n+            : [],\n+      months: frequency === \"yearly\" ? selectedMonths : [],\n+      customInterval:\n+        frequency === \"custom\" ? customInterval : { unit: \"minutes\", value: 1 },\n+    });\n+    onCronExpressionChange(cronExpr);\n+  }, [\n+    frequency,\n+    selectedMinute,\n+    selectedTime,\n+    selectedWeekDays,\n+    selectedMonthDays,\n+    selectedMonths,\n+    customInterval,\n+    onCronExpressionChange,\n+  ]);\n+\n+  return (\n+    <div className=\"max-w-md space-y-6\">\n+      <div className=\"space-y-4\">\n+        <Label className=\"text-base font-medium\">Repeat</Label>\n+\n+        <Select\n+          value={frequency}\n+          onValueChange={(value: CronFrequency) => setFrequency(value)}\n+        >\n+          <SelectTrigger>\n+            <SelectValue placeholder=\"Select frequency\" />\n+          </SelectTrigger>\n+          <SelectContent>\n+            <SelectItem value=\"every minute\">Every Minute</SelectItem>\n+            <SelectItem value=\"hourly\">Every Hour</SelectItem>\n+            <SelectItem value=\"daily\">Daily</SelectItem>\n+            <SelectItem value=\"weekly\">Weekly</SelectItem>\n+            <SelectItem value=\"monthly\">Monthly</SelectItem>\n+            <SelectItem value=\"yearly\">Yearly</SelectItem>\n+            <SelectItem value=\"custom\">Custom</SelectItem>\n+          </SelectContent>\n+        </Select>\n+\n+        {frequency === \"hourly\" && (\n+          <div className=\"flex items-center gap-2\">\n+            <Label>At minute</Label>\n+            <Select value={selectedMinute} onValueChange={setSelectedMinute}>\n+              <SelectTrigger className=\"w-24\">\n+                <SelectValue placeholder=\"Select minute\" />\n+              </SelectTrigger>\n+              <SelectContent>\n+                {[0, 15, 30, 45].map((min) => (\n+                  <SelectItem key={min} value={min.toString()}>\n+                    {min}\n+                  </SelectItem>\n+                ))}\n+              </SelectContent>\n+            </Select>\n+          </div>\n+        )}\n+\n+        {frequency === \"custom\" && (\n+          <div className=\"flex items-center gap-2\">\n+            <Label>Every</Label>\n+            <Input\n+              type=\"number\"\n+              min=\"1\"\n+              className=\"w-20\"\n+              value={customInterval.value}\n+              onChange={(e) =>\n+                setCustomInterval({\n+                  ...customInterval,\n+                  value: parseInt(e.target.value),\n+                })\n+              }\n+            />\n+            <Select\n+              value={customInterval.unit}\n+              onValueChange={(value: any) =>\n+                setCustomInterval({ ...customInterval, unit: value })\n+              }\n+            >\n+              <SelectTrigger className=\"w-32\">\n+                <SelectValue />\n+              </SelectTrigger>\n+              <SelectContent>\n+                <SelectItem value=\"minutes\">Minutes</SelectItem>\n+                <SelectItem value=\"hours\">Hours</SelectItem>\n+                <SelectItem value=\"days\">Days</SelectItem>\n+              </SelectContent>\n+            </Select>\n+          </div>\n+        )}\n+      </div>\n+\n+      {frequency === \"weekly\" && (\n+        <div className=\"space-y-2\">\n+          <div className=\"flex items-center gap-2\">\n+            <Label>On</Label>\n+            <Button\n+              variant=\"outline\"\n+              className=\"h-8 px-2 py-1 text-xs\"\n+              onClick={() => {\n+                if (selectedWeekDays.length === weekDays.length) {\n+                  setSelectedWeekDays([]);\n+                } else {\n+                  setSelectedWeekDays(weekDays.map((day) => day.value));\n+                }\n+              }}\n+            >\n+              {selectedWeekDays.length === weekDays.length\n+                ? \"Deselect All\"\n+                : \"Select All\"}\n+            </Button>\n+            <Button\n+              variant=\"outline\"\n+              className=\"h-8 px-2 py-1 text-xs\"\n+              onClick={() => setSelectedWeekDays([1, 2, 3, 4, 5])}\n+            >\n+              Weekdays\n+            </Button>\n+            <Button\n+              variant=\"outline\"\n+              className=\"h-8 px-2 py-1 text-xs\"\n+              onClick={() => setSelectedWeekDays([0, 6])}\n+            >\n+              Weekends\n+            </Button>\n+          </div>\n+          <div className=\"flex flex-wrap gap-2\">\n+            {weekDays.map((day) => (\n+              <Button\n+                key={day.value}\n+                variant={\n+                  selectedWeekDays.includes(day.value) ? \"default\" : \"outline\"\n+                }\n+                className=\"h-10 w-10 p-0\"\n+                onClick={() => {\n+                  setSelectedWeekDays((prev) =>\n+                    prev.includes(day.value)\n+                      ? prev.filter((d) => d !== day.value)\n+                      : [...prev, day.value],\n+                  );\n+                }}\n+              >\n+                {day.label}\n+              </Button>\n+            ))}\n+          </div>\n+        </div>\n+      )}\n+      {frequency === \"monthly\" && (\n+        <div className=\"space-y-4\">\n+          <Label>Days of Month</Label>\n+          <div className=\"flex gap-2\">\n+            <Button\n+              variant={!showCustomDays ? \"default\" : \"outline\"}\n+              onClick={() => {\n+                setShowCustomDays(false);\n+                setSelectedMonthDays(\n+                  Array.from({ length: 31 }, (_, i) => i + 1),\n+                );\n+              }}\n+            >\n+              All Days\n+            </Button>\n+            <Button\n+              variant={showCustomDays ? \"default\" : \"outline\"}\n+              onClick={() => {\n+                setShowCustomDays(true);\n+                setSelectedMonthDays([]);\n+              }}\n+            >\n+              Customize\n+            </Button>\n+            <Button\n+              variant=\"outline\"\n+              onClick={() => setSelectedMonthDays([15])}\n+            >\n+              15th\n+            </Button>\n+            <Button\n+              variant=\"outline\"\n+              onClick={() => setSelectedMonthDays([31])}\n+            >\n+              Last Day\n+            </Button>\n+          </div>\n+          {showCustomDays && (\n+            <div className=\"flex flex-wrap gap-2\">\n+              {Array.from({ length: 31 }, (_, i) => (\n+                <Button\n+                  key={i + 1}\n+                  variant={\n+                    selectedMonthDays.includes(i + 1) ? \"default\" : \"outline\"\n+                  }\n+                  className=\"h-10 w-10 p-0\"\n+                  onClick={() => {\n+                    setSelectedMonthDays((prev) =>\n+                      prev.includes(i + 1)\n+                        ? prev.filter((d) => d !== i + 1)\n+                        : [...prev, i + 1],\n+                    );\n+                  }}\n+                >\n+                  {i + 1}\n+                </Button>\n+              ))}\n+            </div>\n+          )}\n+        </div>\n+      )}\n+      {frequency === \"yearly\" && (\n+        <div className=\"space-y-4\">\n+          <Label>Months</Label>\n+          <div className=\"flex gap-2\">\n+            <Button\n+              variant=\"outline\"\n+              className=\"h-8 px-2 py-1 text-xs\"\n+              onClick={() => {\n+                if (selectedMonths.length === months.length) {\n+                  setSelectedMonths([]);\n+                } else {\n+                  setSelectedMonths(Array.from({ length: 12 }, (_, i) => i));\n+                }\n+              }}\n+            >\n+              {selectedMonths.length === months.length\n+                ? \"Deselect All\"\n+                : \"Select All\"}\n+            </Button>\n+          </div>\n+          <div className=\"flex flex-wrap gap-2\">\n+            {months.map((month, i) => {\n+              const monthNumber = i + 1;\n+              return (\n+                <Button\n+                  key={i}\n+                  variant={\n+                    selectedMonths.includes(monthNumber) ? \"default\" : \"outline\"\n+                  }\n+                  className=\"px-2 py-1\"\n+                  onClick={() => {\n+                    setSelectedMonths((prev) =>\n+                      prev.includes(monthNumber)\n+                        ? prev.filter((m) => m !== monthNumber)\n+                        : [...prev, monthNumber],\n+                    );\n+                  }}\n+                >\n+                  {month.label}\n+                </Button>\n+              );\n+            })}\n+          </div>\n+        </div>\n+      )}\n+\n+      {frequency !== \"every minute\" && frequency !== \"hourly\" && (\n+        <div className=\"flex items-center gap-4 space-y-2\">\n+          <Label className=\"pt-2\">At</Label>\n+          <Input\n+            type=\"time\"\n+            value={selectedTime}\n+            onChange={(e) => setSelectedTime(e.target.value)}\n+          />\n+        </div>\n+      )}\n+    </div>\n+  );\n+}\ndiff --git a/autogpt_platform/frontend/src/components/cronScheduler.tsx b/autogpt_platform/frontend/src/components/cronScheduler.tsx\ndeleted file mode 100644\nindex 37022fb171e2..000000000000\n--- a/autogpt_platform/frontend/src/components/cronScheduler.tsx\n+++ /dev/null\n@@ -1,417 +0,0 @@\n-import { useState } from \"react\";\n-import {\n-  Select,\n-  SelectContent,\n-  SelectItem,\n-  SelectTrigger,\n-  SelectValue,\n-} from \"@/components/ui/select\";\n-import { Label } from \"@/components/ui/label\";\n-import { Input } from \"@/components/ui/input\";\n-import { Button } from \"@/components/ui/button\";\n-import { Dialog, DialogContent, DialogTitle } from \"@/components/ui/dialog\";\n-import { Separator } from \"./ui/separator\";\n-import { CronExpressionManager } from \"@/lib/monitor/cronExpressionManager\";\n-\n-interface CronSchedulerProps {\n-  setOpen: React.Dispatch<React.SetStateAction<boolean>>;\n-  open: boolean;\n-  afterCronCreation: (cronExpression: string) => void;\n-}\n-\n-export function CronScheduler({\n-  setOpen,\n-  open,\n-  afterCronCreation,\n-}: CronSchedulerProps) {\n-  const [frequency, setFrequency] = useState<\n-    \"minute\" | \"hour\" | \"daily\" | \"weekly\" | \"monthly\" | \"yearly\" | \"custom\"\n-  >(\"daily\");\n-  const [selectedDays, setSelectedDays] = useState<number[]>([]);\n-  const [selectedTime, setSelectedTime] = useState<string>(\"09:00\");\n-  const [showCustomDays, setShowCustomDays] = useState<boolean>(false);\n-  const [selectedMinute, setSelectedMinute] = useState<string>(\"0\");\n-  const [customInterval, setCustomInterval] = useState<{\n-    value: number;\n-    unit: \"minutes\" | \"hours\" | \"days\";\n-  }>({ value: 1, unit: \"minutes\" });\n-\n-  // const [endType, setEndType] = useState<\"never\" | \"on\" | \"after\">(\"never\");\n-  // const [endDate, setEndDate] = useState<Date | undefined>();\n-  // const [occurrences, setOccurrences] = useState<number>(1);\n-\n-  const weekDays = [\n-    { label: \"Su\", value: 0 },\n-    { label: \"Mo\", value: 1 },\n-    { label: \"Tu\", value: 2 },\n-    { label: \"We\", value: 3 },\n-    { label: \"Th\", value: 4 },\n-    { label: \"Fr\", value: 5 },\n-    { label: \"Sa\", value: 6 },\n-  ];\n-\n-  const months = [\n-    { label: \"Jan\", value: \"January\" },\n-    { label: \"Feb\", value: \"February\" },\n-    { label: \"Mar\", value: \"March\" },\n-    { label: \"Apr\", value: \"April\" },\n-    { label: \"May\", value: \"May\" },\n-    { label: \"Jun\", value: \"June\" },\n-    { label: \"Jul\", value: \"July\" },\n-    { label: \"Aug\", value: \"August\" },\n-    { label: \"Sep\", value: \"September\" },\n-    { label: \"Oct\", value: \"October\" },\n-    { label: \"Nov\", value: \"November\" },\n-    { label: \"Dec\", value: \"December\" },\n-  ];\n-\n-  const cron_manager = new CronExpressionManager();\n-\n-  return (\n-    <Dialog open={open} onOpenChange={setOpen}>\n-      <DialogContent>\n-        <DialogTitle>Schedule Task</DialogTitle>\n-        <div className=\"max-w-md space-y-6 p-2\">\n-          <div className=\"space-y-4\">\n-            <Label className=\"text-base font-medium\">Repeat</Label>\n-\n-            <Select\n-              onValueChange={(value: any) => setFrequency(value)}\n-              defaultValue=\"daily\"\n-            >\n-              <SelectTrigger>\n-                <SelectValue placeholder=\"Select frequency\" />\n-              </SelectTrigger>\n-              <SelectContent>\n-                <SelectItem value=\"minute\">Every Minute</SelectItem>\n-                <SelectItem value=\"hour\">Every Hour</SelectItem>\n-                <SelectItem value=\"daily\">Daily</SelectItem>\n-                <SelectItem value=\"weekly\">Weekly</SelectItem>\n-                <SelectItem value=\"monthly\">Monthly</SelectItem>\n-                <SelectItem value=\"yearly\">Yearly</SelectItem>\n-                <SelectItem value=\"custom\">Custom</SelectItem>\n-              </SelectContent>\n-            </Select>\n-\n-            {frequency === \"hour\" && (\n-              <div className=\"flex items-center gap-2\">\n-                <Label>At minute</Label>\n-                <Select\n-                  value={selectedMinute}\n-                  onValueChange={setSelectedMinute}\n-                >\n-                  <SelectTrigger className=\"w-24\">\n-                    <SelectValue placeholder=\"Select minute\" />\n-                  </SelectTrigger>\n-                  <SelectContent>\n-                    {[0, 15, 30, 45].map((min) => (\n-                      <SelectItem key={min} value={min.toString()}>\n-                        {min}\n-                      </SelectItem>\n-                    ))}\n-                  </SelectContent>\n-                </Select>\n-              </div>\n-            )}\n-\n-            {frequency === \"custom\" && (\n-              <div className=\"flex items-center gap-2\">\n-                <Label>Every</Label>\n-                <Input\n-                  type=\"number\"\n-                  min=\"1\"\n-                  className=\"w-20\"\n-                  value={customInterval.value}\n-                  onChange={(e) =>\n-                    setCustomInterval({\n-                      ...customInterval,\n-                      value: parseInt(e.target.value),\n-                    })\n-                  }\n-                />\n-                <Select\n-                  value={customInterval.unit}\n-                  onValueChange={(value: any) =>\n-                    setCustomInterval({ ...customInterval, unit: value })\n-                  }\n-                >\n-                  <SelectTrigger className=\"w-32\">\n-                    <SelectValue />\n-                  </SelectTrigger>\n-                  <SelectContent>\n-                    <SelectItem value=\"minutes\">Minutes</SelectItem>\n-                    <SelectItem value=\"hours\">Hours</SelectItem>\n-                    <SelectItem value=\"days\">Days</SelectItem>\n-                  </SelectContent>\n-                </Select>\n-              </div>\n-            )}\n-          </div>\n-\n-          {frequency === \"weekly\" && (\n-            <div className=\"space-y-2\">\n-              <div className=\"flex items-center gap-2\">\n-                <Label>On</Label>\n-                <Button\n-                  variant=\"outline\"\n-                  className=\"h-8 px-2 py-1 text-xs\"\n-                  onClick={() => {\n-                    if (selectedDays.length === weekDays.length) {\n-                      setSelectedDays([]);\n-                    } else {\n-                      setSelectedDays(weekDays.map((day) => day.value));\n-                    }\n-                  }}\n-                >\n-                  {selectedDays.length === weekDays.length\n-                    ? \"Deselect All\"\n-                    : \"Select All\"}\n-                </Button>\n-                <Button\n-                  variant=\"outline\"\n-                  className=\"h-8 px-2 py-1 text-xs\"\n-                  onClick={() => setSelectedDays([1, 2, 3, 4, 5])}\n-                >\n-                  Weekdays\n-                </Button>\n-                <Button\n-                  variant=\"outline\"\n-                  className=\"h-8 px-2 py-1 text-xs\"\n-                  onClick={() => setSelectedDays([0, 6])}\n-                >\n-                  Weekends\n-                </Button>\n-              </div>\n-              <div className=\"flex flex-wrap gap-2\">\n-                {weekDays.map((day) => (\n-                  <Button\n-                    key={day.value}\n-                    variant={\n-                      selectedDays.includes(day.value) ? \"default\" : \"outline\"\n-                    }\n-                    className=\"h-10 w-10 p-0\"\n-                    onClick={() => {\n-                      setSelectedDays((prev) =>\n-                        prev.includes(day.value)\n-                          ? prev.filter((d) => d !== day.value)\n-                          : [...prev, day.value],\n-                      );\n-                    }}\n-                  >\n-                    {day.label}\n-                  </Button>\n-                ))}\n-              </div>\n-            </div>\n-          )}\n-          {frequency === \"monthly\" && (\n-            <div className=\"space-y-4\">\n-              <Label>Days of Month</Label>\n-              <div className=\"flex gap-2\">\n-                <Button\n-                  variant={!showCustomDays ? \"default\" : \"outline\"}\n-                  onClick={() => {\n-                    setShowCustomDays(false);\n-                    setSelectedDays(\n-                      Array.from({ length: 31 }, (_, i) => i + 1),\n-                    );\n-                  }}\n-                >\n-                  All Days\n-                </Button>\n-                <Button\n-                  variant={showCustomDays ? \"default\" : \"outline\"}\n-                  onClick={() => {\n-                    setShowCustomDays(true);\n-                    setSelectedDays([]);\n-                  }}\n-                >\n-                  Customize\n-                </Button>\n-                <Button variant=\"outline\" onClick={() => setSelectedDays([15])}>\n-                  15th\n-                </Button>\n-                <Button variant=\"outline\" onClick={() => setSelectedDays([31])}>\n-                  Last Day\n-                </Button>\n-              </div>\n-              {showCustomDays && (\n-                <div className=\"flex flex-wrap gap-2\">\n-                  {Array.from({ length: 31 }, (_, i) => (\n-                    <Button\n-                      key={i + 1}\n-                      variant={\n-                        selectedDays.includes(i + 1) ? \"default\" : \"outline\"\n-                      }\n-                      className=\"h-10 w-10 p-0\"\n-                      onClick={() => {\n-                        setSelectedDays((prev) =>\n-                          prev.includes(i + 1)\n-                            ? prev.filter((d) => d !== i + 1)\n-                            : [...prev, i + 1],\n-                        );\n-                      }}\n-                    >\n-                      {i + 1}\n-                    </Button>\n-                  ))}\n-                </div>\n-              )}\n-            </div>\n-          )}\n-          {frequency === \"yearly\" && (\n-            <div className=\"space-y-4\">\n-              <Label>Months</Label>\n-              <div className=\"flex gap-2\">\n-                <Button\n-                  variant=\"outline\"\n-                  className=\"h-8 px-2 py-1 text-xs\"\n-                  onClick={() => {\n-                    if (selectedDays.length === months.length) {\n-                      setSelectedDays([]);\n-                    } else {\n-                      setSelectedDays(Array.from({ length: 12 }, (_, i) => i));\n-                    }\n-                  }}\n-                >\n-                  {selectedDays.length === months.length\n-                    ? \"Deselect All\"\n-                    : \"Select All\"}\n-                </Button>\n-              </div>\n-              <div className=\"flex flex-wrap gap-2\">\n-                {months.map((month, index) => (\n-                  <Button\n-                    key={index}\n-                    variant={\n-                      selectedDays.includes(index) ? \"default\" : \"outline\"\n-                    }\n-                    className=\"px-2 py-1\"\n-                    onClick={() => {\n-                      setSelectedDays((prev) =>\n-                        prev.includes(index)\n-                          ? prev.filter((m) => m !== index)\n-                          : [...prev, index],\n-                      );\n-                    }}\n-                  >\n-                    {month.label}\n-                  </Button>\n-                ))}\n-              </div>\n-            </div>\n-          )}\n-\n-          {frequency !== \"minute\" && frequency !== \"hour\" && (\n-            <div className=\"flex items-center gap-4 space-y-2\">\n-              <Label className=\"pt-2\">At</Label>\n-              <Input\n-                type=\"time\"\n-                value={selectedTime}\n-                onChange={(e) => setSelectedTime(e.target.value)}\n-              />\n-            </div>\n-          )}\n-\n-          <Separator />\n-          {/*\n-\n-            On the backend, we are using standard cron expressions,\n-            which makes it challenging to add an end date or stop execution\n-            after a certain time using only cron expressions.\n-            (since standard cron expressions have limitations, like the lack of a year field or more...).\n-\n-            We could also use ranges in cron expression for end dates but It doesm't cover all cases (sometimes break)\n-\n-            To automatically end the scheduler, we need to store the end date and time occurrence in the database\n-            and modify scheduler.add_job. Currently, we can only stop the scheduler manually from the monitor tab.\n-\n-            */}\n-\n-          {/* <div className=\"space-y-6\">\n-            <Label className=\"text-lg font-medium\">Ends</Label>\n-            <RadioGroup\n-              value={endType}\n-              onValueChange={(value: \"never\" | \"on\" | \"after\") =>\n-                setEndType(value)\n-              }\n-            >\n-              <div className=\"flex items-center space-x-2\">\n-                <RadioGroupItem value=\"never\" id=\"never\" />\n-                <Label htmlFor=\"never\">Never</Label>\n-              </div>\n-\n-              <div className=\"flex items-center space-x-2\">\n-                <RadioGroupItem value=\"on\" id=\"on\" />\n-                <Label htmlFor=\"on\" className=\"w-[50px]\">\n-                  On\n-                </Label>\n-                <Popover>\n-                  <PopoverTrigger asChild>\n-                    <Button\n-                      variant=\"outline\"\n-                      className=\"w-full\"\n-                      disabled={endType !== \"on\"}\n-                    >\n-                      <CalendarIcon className=\"mr-2 h-4 w-4\" />\n-                      {endDate ? format(endDate, \"PPP\") : \"Pick a date\"}\n-                    </Button>\n-                  </PopoverTrigger>\n-                  <PopoverContent className=\"w-auto p-0\">\n-                    <Calendar\n-                      mode=\"single\"\n-                      selected={endDate}\n-                      onSelect={setEndDate}\n-                      disabled={(date) => date < new Date()}\n-                      fromDate={new Date()}\n-                    />\n-                  </PopoverContent>\n-                </Popover>\n-              </div>\n-              <div className=\"flex items-center space-x-2\">\n-                <RadioGroupItem value=\"after\" id=\"after\" />\n-                <Label htmlFor=\"after\" className=\"ml-2 w-[50px]\">\n-                  After\n-                </Label>\n-                <Input\n-                  type=\"number\"\n-                  className=\"ml-2 w-[100px]\"\n-                  value={occurrences}\n-                  onChange={(e) => setOccurrences(Number(e.target.value))}\n-                />\n-                <span>times</span>\n-              </div>\n-            </RadioGroup>\n-          </div> */}\n-\n-          <div className=\"flex justify-end space-x-2\">\n-            <Button variant=\"outline\" onClick={() => setOpen(false)}>\n-              Cancel\n-            </Button>\n-            <Button\n-              onClick={() => {\n-                const cronExpr = cron_manager.generateCronExpression(\n-                  frequency,\n-                  selectedTime,\n-                  selectedDays,\n-                  selectedMinute,\n-                  customInterval,\n-                );\n-                setFrequency(\"minute\");\n-                setSelectedDays([]);\n-                setSelectedTime(\"00:00\");\n-                setShowCustomDays(false);\n-                setSelectedMinute(\"0\");\n-                setCustomInterval({ value: 1, unit: \"minutes\" });\n-                setOpen(false);\n-                afterCronCreation(cronExpr);\n-              }}\n-            >\n-              Done\n-            </Button>\n-          </div>\n-        </div>\n-      </DialogContent>\n-    </Dialog>\n-  );\n-}\ndiff --git a/autogpt_platform/frontend/src/components/monitor/scheduleTable.tsx b/autogpt_platform/frontend/src/components/monitor/scheduleTable.tsx\nindex 4c3982099b0d..b395080e6151 100644\n--- a/autogpt_platform/frontend/src/components/monitor/scheduleTable.tsx\n+++ b/autogpt_platform/frontend/src/components/monitor/scheduleTable.tsx\n@@ -13,7 +13,7 @@ import { Badge } from \"@/components/ui/badge\";\n import { ScrollArea } from \"@/components/ui/scroll-area\";\n import { ClockIcon, Loader2 } from \"lucide-react\";\n import { useToast } from \"@/components/ui/use-toast\";\n-import { CronExpressionManager } from \"@/lib/monitor/cronExpressionManager\";\n+import { humanizeCronExpression } from \"@/lib/cron-expression-utils\";\n import {\n   Select,\n   SelectContent,\n@@ -52,7 +52,6 @@ export const SchedulesTable = ({\n }: SchedulesTableProps) => {\n   const { toast } = useToast();\n   const router = useRouter();\n-  const cron_manager = new CronExpressionManager();\n   const [selectedAgent, setSelectedAgent] = useState<string>(\"\"); // Library Agent ID\n   const [selectedVersion, setSelectedVersion] = useState<number>(0); // Graph version\n   const [maxVersion, setMaxVersion] = useState<number>(0);\n@@ -246,7 +245,7 @@ export const SchedulesTable = ({\n                   </TableCell>\n                   <TableCell>\n                     <Badge variant=\"secondary\">\n-                      {cron_manager.generateDescription(schedule.cron || \"\")}\n+                      {humanizeCronExpression(schedule.cron)}\n                     </Badge>\n                   </TableCell>\n \ndiff --git a/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx b/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx\nindex 55b7c4d2bdcf..1cde0ccd58fa 100644\n--- a/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx\n+++ b/autogpt_platform/frontend/src/hooks/useAgentGraph.tsx\n@@ -1073,16 +1073,21 @@ export default function useAgentGraph(\n \n   // runs after saving cron expression and inputs (if exists)\n   const scheduleRunner = useCallback(\n-    async (cronExpression: string, inputs: InputItem[]) => {\n+    async (\n+      cronExpression: string,\n+      inputs: InputItem[],\n+      scheduleName: string,\n+    ) => {\n       await saveAgent();\n       try {\n         if (flowID) {\n-          await api.createSchedule({\n+          await api.createGraphExecutionSchedule({\n             graph_id: flowID,\n             // flowVersion is always defined here because scheduling is opened for a specific version\n             graph_version: flowVersion!,\n+            name: scheduleName,\n             cron: cronExpression,\n-            input_data: inputs.reduce(\n+            inputs: inputs.reduce(\n               (acc, input) => ({\n                 ...acc,\n                 [input.hardcodedValues.name]: input.hardcodedValues.value,\ndiff --git a/autogpt_platform/frontend/src/lib/autogpt-server-api/client.ts b/autogpt_platform/frontend/src/lib/autogpt-server-api/client.ts\nindex 3a6ffeb3aa95..80537ac1416a 100644\n--- a/autogpt_platform/frontend/src/lib/autogpt-server-api/client.ts\n+++ b/autogpt_platform/frontend/src/lib/autogpt-server-api/client.ts\n@@ -742,22 +742,35 @@ export default class BackendAPI {\n   /////////// SCHEDULES ////////////\n   //////////////////////////////////\n \n-  async createSchedule(schedule: ScheduleCreatable): Promise<Schedule> {\n-    return this._request(\"POST\", `/schedules`, schedule).then(\n-      parseScheduleTimestamp,\n-    );\n+  async createGraphExecutionSchedule(\n+    params: ScheduleCreatable,\n+  ): Promise<Schedule> {\n+    return this._request(\n+      \"POST\",\n+      `/graphs/${params.graph_id}/schedules`,\n+      params,\n+    ).then(parseScheduleTimestamp);\n   }\n \n-  async deleteSchedule(scheduleId: ScheduleID): Promise<{ id: string }> {\n-    return this._request(\"DELETE\", `/schedules/${scheduleId}`);\n+  async listGraphExecutionSchedules(graphID: GraphID): Promise<Schedule[]> {\n+    return this._get(`/graphs/${graphID}/schedules`).then((schedules) =>\n+      schedules.map(parseScheduleTimestamp),\n+    );\n   }\n \n-  async listSchedules(): Promise<Schedule[]> {\n+  /** @deprecated only used in legacy `Monitor` */\n+  async listAllGraphsExecutionSchedules(): Promise<Schedule[]> {\n     return this._get(`/schedules`).then((schedules) =>\n       schedules.map(parseScheduleTimestamp),\n     );\n   }\n \n+  async deleteGraphExecutionSchedule(\n+    scheduleID: ScheduleID,\n+  ): Promise<{ id: ScheduleID }> {\n+    return this._request(\"DELETE\", `/schedules/${scheduleID}`);\n+  }\n+\n   //////////////////////////////////\n   ////////////// OTTO //////////////\n   //////////////////////////////////\ndiff --git a/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts b/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\nindex 5800140ee621..e85e326c3b65 100644\n--- a/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\n+++ b/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\n@@ -771,6 +771,7 @@ export type ProfileDetails = {\n   avatar_url: string;\n };\n \n+/* Mirror of backend/executor/scheduler.py:GraphExecutionJobInfo */\n export type Schedule = {\n   id: ScheduleID;\n   name: string;\n@@ -778,17 +779,21 @@ export type Schedule = {\n   user_id: UserID;\n   graph_id: GraphID;\n   graph_version: number;\n-  input_data: { [key: string]: any };\n+  input_data: Record<string, any>;\n+  input_credentials: Record<string, CredentialsMetaInput>;\n   next_run_time: Date;\n };\n \n export type ScheduleID = Brand<string, \"ScheduleID\">;\n \n+/* Mirror of backend/server/routers/v1.py:ScheduleCreationRequest */\n export type ScheduleCreatable = {\n-  cron: string;\n   graph_id: GraphID;\n   graph_version: number;\n-  input_data: { [key: string]: any };\n+  name: string;\n+  cron: string;\n+  inputs: Record<string, any>;\n+  credentials?: Record<string, CredentialsMetaInput>;\n };\n \n export type MyAgent = {\ndiff --git a/autogpt_platform/frontend/src/lib/cron-expression-utils.ts b/autogpt_platform/frontend/src/lib/cron-expression-utils.ts\nnew file mode 100644\nindex 000000000000..7a85082cc4bf\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/lib/cron-expression-utils.ts\n@@ -0,0 +1,265 @@\n+export type CronFrequency =\n+  | \"every minute\"\n+  | \"hourly\"\n+  | \"daily\"\n+  | \"weekly\"\n+  | \"monthly\"\n+  | \"yearly\"\n+  | \"custom\";\n+\n+export type CronExpressionParams =\n+  | { frequency: \"every minute\" }\n+  | {\n+      frequency: \"hourly\";\n+      minute: number;\n+    }\n+  | ((\n+      | {\n+          frequency: \"daily\";\n+        }\n+      | {\n+          frequency: \"weekly\";\n+          /** 0-based list of weekdays: 0 = Monday ... 6 = Sunday */\n+          days: number[];\n+        }\n+      | {\n+          frequency: \"monthly\";\n+          /** 1-based list of month days */\n+          days: number[];\n+        }\n+      | {\n+          frequency: \"yearly\";\n+          /** 1-based list of months (1-12) */\n+          months: number[];\n+        }\n+      | {\n+          frequency: \"custom\";\n+          customInterval: { unit: string; value: number };\n+        }\n+    ) & {\n+      minute: number;\n+      hour: number;\n+    });\n+\n+export function makeCronExpression(params: CronExpressionParams): string {\n+  const frequency = params.frequency;\n+\n+  if (frequency === \"every minute\") return \"* * * * *\";\n+  if (frequency === \"hourly\") return `${params.minute} * * * *`;\n+  if (frequency === \"daily\") return `${params.minute} ${params.hour} * * *`;\n+  if (frequency === \"weekly\") {\n+    const { minute, hour, days } = params;\n+    const weekDaysExpr = days.sort((a, b) => a - b).join(\",\");\n+    return `${minute} ${hour} * * ${weekDaysExpr}`;\n+  }\n+  if (frequency === \"monthly\") {\n+    const { minute, hour, days } = params;\n+    const monthDaysExpr = days.sort((a, b) => a - b).join(\",\");\n+    return `${minute} ${hour} ${monthDaysExpr} * *`;\n+  }\n+  if (frequency === \"yearly\") {\n+    const { minute, hour, months } = params;\n+    const monthList = months.sort((a, b) => a - b).join(\",\");\n+    return `${minute} ${hour} 1 ${monthList} *`;\n+  }\n+  if (frequency === \"custom\") {\n+    const { minute, hour, customInterval } = params;\n+    if (customInterval.unit === \"minutes\") {\n+      return `*/${customInterval.value} * * * *`;\n+    } else if (customInterval.unit === \"hours\") {\n+      return `0 */${customInterval.value} * * *`;\n+    } else {\n+      return `${minute} ${hour} */${customInterval.value} * *`;\n+    }\n+  }\n+\n+  return \"\";\n+}\n+\n+export function humanizeCronExpression(cronExpression: string): string {\n+  const parts = cronExpression.trim().split(/\\s+/);\n+  if (parts.length !== 5) {\n+    throw new Error(\"Invalid cron expression format.\");\n+  }\n+\n+  const [minute, hour, dayOfMonth, month, dayOfWeek] = parts;\n+\n+  // Handle every minute\n+  if (cronExpression === \"* * * * *\") {\n+    return \"Every minute\";\n+  }\n+\n+  // Handle minute intervals (e.g., */5 * * * *)\n+  if (\n+    minute.startsWith(\"*/\") &&\n+    hour === \"*\" &&\n+    dayOfMonth === \"*\" &&\n+    month === \"*\" &&\n+    dayOfWeek === \"*\"\n+  ) {\n+    const interval = minute.substring(2);\n+    return `Every ${interval} minutes`;\n+  }\n+\n+  // Handle hour intervals (e.g., 30 * * * *)\n+  if (\n+    hour === \"*\" &&\n+    !minute.includes(\"/\") &&\n+    dayOfMonth === \"*\" &&\n+    month === \"*\" &&\n+    dayOfWeek === \"*\"\n+  ) {\n+    return `Every hour at minute ${minute}`;\n+  }\n+\n+  // Handle every N hours (e.g., 0 */2 * * *)\n+  if (\n+    hour.startsWith(\"*/\") &&\n+    minute === \"0\" &&\n+    dayOfMonth === \"*\" &&\n+    month === \"*\" &&\n+    dayOfWeek === \"*\"\n+  ) {\n+    const interval = hour.substring(2);\n+    return `Every ${interval} hours`;\n+  }\n+\n+  // Handle daily (e.g., 30 14 * * *)\n+  if (\n+    dayOfMonth === \"*\" &&\n+    month === \"*\" &&\n+    dayOfWeek === \"*\" &&\n+    !minute.includes(\"/\") &&\n+    !hour.includes(\"/\")\n+  ) {\n+    return `Every day at ${formatTime(hour, minute)}`;\n+  }\n+\n+  // Handle weekly (e.g., 30 14 * * 1,3,5)\n+  if (\n+    dayOfWeek !== \"*\" &&\n+    dayOfMonth === \"*\" &&\n+    month === \"*\" &&\n+    !minute.includes(\"/\") &&\n+    !hour.includes(\"/\")\n+  ) {\n+    const days = getDayNames(dayOfWeek);\n+    return `Every ${days} at ${formatTime(hour, minute)}`;\n+  }\n+\n+  // Handle monthly (e.g., 30 14 1,15 * *)\n+  if (\n+    dayOfMonth !== \"*\" &&\n+    month === \"*\" &&\n+    dayOfWeek === \"*\" &&\n+    !minute.includes(\"/\") &&\n+    !hour.includes(\"/\")\n+  ) {\n+    const days = dayOfMonth.split(\",\").map(Number);\n+    const dayList = days.join(\", \");\n+    return `On day ${dayList} of every month at ${formatTime(hour, minute)}`;\n+  }\n+\n+  // Handle yearly (e.g., 30 14 1 1,6,12 *)\n+  if (\n+    dayOfMonth !== \"*\" &&\n+    month !== \"*\" &&\n+    dayOfWeek === \"*\" &&\n+    !minute.includes(\"/\") &&\n+    !hour.includes(\"/\")\n+  ) {\n+    const months = getMonthNames(month);\n+    return `Every year on the 1st day of ${months} at ${formatTime(hour, minute)}`;\n+  }\n+\n+  // Handle custom minute intervals with other fields as * (e.g., every N minutes)\n+  if (\n+    minute.includes(\"/\") &&\n+    hour === \"*\" &&\n+    dayOfMonth === \"*\" &&\n+    month === \"*\" &&\n+    dayOfWeek === \"*\"\n+  ) {\n+    const interval = minute.split(\"/\")[1];\n+    return `Every ${interval} minutes`;\n+  }\n+\n+  // Handle custom hour intervals with other fields as * (e.g., every N hours)\n+  if (\n+    hour.includes(\"/\") &&\n+    minute === \"0\" &&\n+    dayOfMonth === \"*\" &&\n+    month === \"*\" &&\n+    dayOfWeek === \"*\"\n+  ) {\n+    const interval = hour.split(\"/\")[1];\n+    return `Every ${interval} hours`;\n+  }\n+\n+  // Handle specific days with custom intervals (e.g., every N days)\n+  if (\n+    dayOfMonth.startsWith(\"*/\") &&\n+    month === \"*\" &&\n+    dayOfWeek === \"*\" &&\n+    !minute.includes(\"/\") &&\n+    !hour.includes(\"/\")\n+  ) {\n+    const interval = dayOfMonth.substring(2);\n+    return `Every ${interval} days at ${formatTime(hour, minute)}`;\n+  }\n+\n+  return `Cron Expression: ${cronExpression}`;\n+}\n+\n+function formatTime(hour: string, minute: string): string {\n+  const formattedHour = padZero(hour);\n+  const formattedMinute = padZero(minute);\n+  return `${formattedHour}:${formattedMinute}`;\n+}\n+\n+function padZero(value: string): string {\n+  return value.padStart(2, \"0\");\n+}\n+\n+function getDayNames(dayOfWeek: string): string {\n+  const days = dayOfWeek.split(\",\").map(Number);\n+  const dayNames = days\n+    .map((d) => {\n+      const names = [\n+        \"Sunday\",\n+        \"Monday\",\n+        \"Tuesday\",\n+        \"Wednesday\",\n+        \"Thursday\",\n+        \"Friday\",\n+        \"Saturday\",\n+      ];\n+      return names[d] || `Unknown(${d})`;\n+    })\n+    .join(\", \");\n+  return dayNames;\n+}\n+\n+function getMonthNames(month: string): string {\n+  const months = month.split(\",\").map(Number);\n+  const monthNames = months\n+    .map((m) => {\n+      const names = [\n+        \"January\",\n+        \"February\",\n+        \"March\",\n+        \"April\",\n+        \"May\",\n+        \"June\",\n+        \"July\",\n+        \"August\",\n+        \"September\",\n+        \"October\",\n+        \"November\",\n+        \"December\",\n+      ];\n+      return names[m - 1] || `Unknown(${m})`;\n+    })\n+    .join(\", \");\n+  return monthNames;\n+}\ndiff --git a/autogpt_platform/frontend/src/lib/monitor/cronExpressionManager.ts b/autogpt_platform/frontend/src/lib/monitor/cronExpressionManager.ts\ndeleted file mode 100644\nindex fb9fbe8eee1d..000000000000\n--- a/autogpt_platform/frontend/src/lib/monitor/cronExpressionManager.ts\n+++ /dev/null\n@@ -1,239 +0,0 @@\n-export class CronExpressionManager {\n-  generateCronExpression(\n-    frequency: string,\n-    selectedTime: string,\n-    selectedDays: number[],\n-    selectedMinute: string,\n-    customInterval: { unit: string; value: number },\n-  ): string {\n-    const [hours, minutes] = selectedTime.split(\":\").map(Number);\n-    let expression = \"\";\n-\n-    switch (frequency) {\n-      case \"minute\":\n-        expression = \"* * * * *\";\n-        break;\n-      case \"hour\":\n-        expression = `${selectedMinute} * * * *`;\n-        break;\n-      case \"daily\":\n-        expression = `${minutes} ${hours} * * *`;\n-        break;\n-      case \"weekly\":\n-        const days = selectedDays.join(\",\");\n-        expression = `${minutes} ${hours} * * ${days}`;\n-        break;\n-      case \"monthly\":\n-        const monthDays = selectedDays.sort((a, b) => a - b).join(\",\");\n-        expression = `${minutes} ${hours} ${monthDays} * *`;\n-        break;\n-      case \"yearly\":\n-        const monthList = selectedDays\n-          .map((d) => d + 1)\n-          .sort((a, b) => a - b)\n-          .join(\",\");\n-        expression = `${minutes} ${hours} 1 ${monthList} *`;\n-        break;\n-      case \"custom\":\n-        if (customInterval.unit === \"minutes\") {\n-          expression = `*/${customInterval.value} * * * *`;\n-        } else if (customInterval.unit === \"hours\") {\n-          expression = `0 */${customInterval.value} * * *`;\n-        } else {\n-          expression = `${minutes} ${hours} */${customInterval.value} * *`;\n-        }\n-        break;\n-      default:\n-        expression = \"\";\n-    }\n-    return expression;\n-  }\n-\n-  generateDescription(cronExpression: string): string {\n-    const parts = cronExpression.trim().split(/\\s+/);\n-    if (parts.length !== 5) {\n-      throw new Error(\"Invalid cron expression format.\");\n-    }\n-\n-    const [minute, hour, dayOfMonth, month, dayOfWeek] = parts;\n-\n-    // Handle every minute\n-    if (cronExpression === \"* * * * *\") {\n-      return \"Every minute\";\n-    }\n-\n-    // Handle minute intervals (e.g., */5 * * * *)\n-    if (\n-      minute.startsWith(\"*/\") &&\n-      hour === \"*\" &&\n-      dayOfMonth === \"*\" &&\n-      month === \"*\" &&\n-      dayOfWeek === \"*\"\n-    ) {\n-      const interval = minute.substring(2);\n-      return `Every ${interval} minutes`;\n-    }\n-\n-    // Handle hour intervals (e.g., 30 * * * *)\n-    if (\n-      hour === \"*\" &&\n-      !minute.includes(\"/\") &&\n-      dayOfMonth === \"*\" &&\n-      month === \"*\" &&\n-      dayOfWeek === \"*\"\n-    ) {\n-      return `Every hour at minute ${minute}`;\n-    }\n-\n-    // Handle every N hours (e.g., 0 */2 * * *)\n-    if (\n-      hour.startsWith(\"*/\") &&\n-      minute === \"0\" &&\n-      dayOfMonth === \"*\" &&\n-      month === \"*\" &&\n-      dayOfWeek === \"*\"\n-    ) {\n-      const interval = hour.substring(2);\n-      return `Every ${interval} hours`;\n-    }\n-\n-    // Handle daily (e.g., 30 14 * * *)\n-    if (\n-      dayOfMonth === \"*\" &&\n-      month === \"*\" &&\n-      dayOfWeek === \"*\" &&\n-      !minute.includes(\"/\") &&\n-      !hour.includes(\"/\")\n-    ) {\n-      return `Every day at ${this.formatTime(hour, minute)}`;\n-    }\n-\n-    // Handle weekly (e.g., 30 14 * * 1,3,5)\n-    if (\n-      dayOfWeek !== \"*\" &&\n-      dayOfMonth === \"*\" &&\n-      month === \"*\" &&\n-      !minute.includes(\"/\") &&\n-      !hour.includes(\"/\")\n-    ) {\n-      const days = this.getDayNames(dayOfWeek);\n-      return `Every ${days} at ${this.formatTime(hour, minute)}`;\n-    }\n-\n-    // Handle monthly (e.g., 30 14 1,15 * *)\n-    if (\n-      dayOfMonth !== \"*\" &&\n-      month === \"*\" &&\n-      dayOfWeek === \"*\" &&\n-      !minute.includes(\"/\") &&\n-      !hour.includes(\"/\")\n-    ) {\n-      const days = dayOfMonth.split(\",\").map(Number);\n-      const dayList = days.join(\", \");\n-      return `On day ${dayList} of every month at ${this.formatTime(hour, minute)}`;\n-    }\n-\n-    // Handle yearly (e.g., 30 14 1 1,6,12 *)\n-    if (\n-      dayOfMonth !== \"*\" &&\n-      month !== \"*\" &&\n-      dayOfWeek === \"*\" &&\n-      !minute.includes(\"/\") &&\n-      !hour.includes(\"/\")\n-    ) {\n-      const months = this.getMonthNames(month);\n-      return `Every year on the 1st day of ${months} at ${this.formatTime(hour, minute)}`;\n-    }\n-\n-    // Handle custom minute intervals with other fields as * (e.g., every N minutes)\n-    if (\n-      minute.includes(\"/\") &&\n-      hour === \"*\" &&\n-      dayOfMonth === \"*\" &&\n-      month === \"*\" &&\n-      dayOfWeek === \"*\"\n-    ) {\n-      const interval = minute.split(\"/\")[1];\n-      return `Every ${interval} minutes`;\n-    }\n-\n-    // Handle custom hour intervals with other fields as * (e.g., every N hours)\n-    if (\n-      hour.includes(\"/\") &&\n-      minute === \"0\" &&\n-      dayOfMonth === \"*\" &&\n-      month === \"*\" &&\n-      dayOfWeek === \"*\"\n-    ) {\n-      const interval = hour.split(\"/\")[1];\n-      return `Every ${interval} hours`;\n-    }\n-\n-    // Handle specific days with custom intervals (e.g., every N days)\n-    if (\n-      dayOfMonth.startsWith(\"*/\") &&\n-      month === \"*\" &&\n-      dayOfWeek === \"*\" &&\n-      !minute.includes(\"/\") &&\n-      !hour.includes(\"/\")\n-    ) {\n-      const interval = dayOfMonth.substring(2);\n-      return `Every ${interval} days at ${this.formatTime(hour, minute)}`;\n-    }\n-\n-    return `Cron Expression: ${cronExpression}`;\n-  }\n-\n-  private formatTime(hour: string, minute: string): string {\n-    const formattedHour = this.padZero(hour);\n-    const formattedMinute = this.padZero(minute);\n-    return `${formattedHour}:${formattedMinute}`;\n-  }\n-\n-  private padZero(value: string): string {\n-    return value.padStart(2, \"0\");\n-  }\n-\n-  private getDayNames(dayOfWeek: string): string {\n-    const days = dayOfWeek.split(\",\").map(Number);\n-    const dayNames = days\n-      .map((d) => {\n-        const names = [\n-          \"Sunday\",\n-          \"Monday\",\n-          \"Tuesday\",\n-          \"Wednesday\",\n-          \"Thursday\",\n-          \"Friday\",\n-          \"Saturday\",\n-        ];\n-        return names[d] || `Unknown(${d})`;\n-      })\n-      .join(\", \");\n-    return dayNames;\n-  }\n-\n-  private getMonthNames(month: string): string {\n-    const months = month.split(\",\").map(Number);\n-    const monthNames = months\n-      .map((m) => {\n-        const names = [\n-          \"January\",\n-          \"February\",\n-          \"March\",\n-          \"April\",\n-          \"May\",\n-          \"June\",\n-          \"July\",\n-          \"August\",\n-          \"September\",\n-          \"October\",\n-          \"November\",\n-          \"December\",\n-        ];\n-        return names[m - 1] || `Unknown(${m})`;\n-      })\n-      .join(\", \");\n-    return monthNames;\n-  }\n-}\n"
  },
  {
    "index": 4,
    "filtered_comments": [
      "Convert this to an issue plz?\r\n> Nice-to-have: make a button on webhook blocks to trigger a ping and check its result. The API endpoints for this is already implemented.",
      "![image](https://github.com/user-attachments/assets/f1d2b2a2-7550-456e-af11-1754fe3d1a5a)\r\ncredentials seems non compatible with #8516 \r\n\r\nAlso hit this issue \r\n```\r\nINFO:     127.0.0.1:64414 - \"POST /api/graphs HTTP/1.1\" 400 Bad Request\r\n2024-11-12 19:01:02,046 ERROR  POST /api/graphs failed: Failed to create GitHub webhook: Validation Failed\r\n* url is missing a scheme\r\nTraceback (most recent call last):\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/sentry_sdk/integrations/fastapi.py\", line 143, in _sentry_app\r\n    return await old_app(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/server/routers/v1.py\", line 186, in create_new_graph\r\n    return await do_create_graph(create_graph, is_template=False, user_id=user_id)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/server/routers/v1.py\", line 221, in do_create_graph\r\n    graph = await on_graph_activate(\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\", line 43, in on_graph_activate\r\n    updated_node = await on_node_activate(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\", line 140, in on_node_activate\r\n    new_webhook = await webhooks_manager.get_suitable_webhook(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/base.py\", line 40, in get_suitable_webhook\r\n    return await self._create_webhook(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/base.py\", line 138, in _create_webhook\r\n    provider_webhook_id, config = await self._register_webhook(\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/github.py\", line 122, in _register_webhook\r\n    raise ValueError(f\"Failed to create GitHub webhook: {error_msg}\")\r\nValueError: Failed to create GitHub webhook: Validation Failed\r\n* url is missing a scheme\r\nINFO:     127.0.0.1:64466 - \"POST /api/graphs HTTP/1.1\" 400 Bad Request\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/ebd92341-5b65-4221-8c79-837e8eb934ef)\r\n\r\n\r\nWe should probably have a better error message than that for saying \"set your .env correctly\"",
      "> Good catch. I'm not sure how to deal with this. The graph can't be run manually because it relies on a webhook to trigger it. Should we hide the \"Run\" button if a graph can't be run manually? Should it just do nothing?\r\n\r\nwhat's toran and john think",
      "> The graph can't be run manually because it relies on a webhook to trigger it. Should we hide the \"Run\" button if a graph can't be run manually? Should it just do nothing?\r\n\r\nSo there needs to be a way of saying that you want your Agent to be \"Running\" - in this case listening for a webhook - or \"Stopped\" - i.e not listening on a webhook. I was thinking the run button would do that here.\r\n\r\nWhat's the current UX for this? Let's sync on this one @Pwuts as it's a lot for a comment.\r\n",
      "> The advanced button here does nothing?\r\n\r\nThat's because the block has no \"advanced\" inputs, and the toggle doesn't hide when there is nothing to toggle.\r\n\r\n> We probably want these to be able to trivially link\r\n\r\nYes, there are a few ways to do that but most of those are out of scope for this PR and the rest not a sustainable fix imo. We should do a QOL improvement on all of the GitHub blocks to address stuff like this.\r\n\r\n> How do I pass a variable to this block [picture of GitHub webhook trigger block]\r\n\r\nYou don't. Due to the system's architecture, the webhook trigger block can't accept input links and must be the starting node.\r\n\r\n> Why is the output not number type for number\r\n\r\nBecause `NodeHandle` doesn't know what an `integer` is apparently:\r\nhttps://github.com/Significant-Gravitas/AutoGPT/blob/86535b5811f8d1cc0bdde2232693919c4b1115e3/autogpt_platform/frontend/src/components/NodeHandle.tsx#L22-L29\r\n\r\n- [ ] Add `integer` type to `NodeHandle` type list\r\n\r\n> we should probably allow the block to output the repo, URL, etc too for the trigger if its not taking in inputs\r\n\r\nMy idea for a sustainable and scalable fix for that is: allow directly connecting links to nested properties of object outputs. That's way out of scope for this PR.\r\n\r\nDue to the block layout, I don't want to add a large number of outputs because that just fills up the screen very quickly.\r\n\r\n> We need to clarify Payload, Sender and Pull request for normal people\r\n\r\nIf you don't know what a pull request is, why would you be using this block?\r\n\r\n- [x] Improve description of `payload` output\r\n\r\n> I'm not sure the diff in pull request and Payload\r\n\r\ncan't parse, come again?\r\n\r\n> I assume sender is creator?\r\n\r\nSender already has the description *\"Object representing the user who triggered the event\"*. Do you think that output also needs a better name, and if so what?\r\n\r\n> I have no idea (as a dev, not even user) how to debug this via UI. As a dev, I checked the raw output of the block in the logs. It just \"didn't work\" but succeeded from the UI perspective\r\n\r\nI also just debug by looking at the backend logs. Suggestions welcome.\r\n\r\nWe could store all incoming webhook payloads and add a view for that, but that's a significant feature addition. WDYT?\r\n\r\n> If a user does a bad design (ex: leaving out a value on a comparison) the webhook rejects but they should probably know that when saving because it will be an issue they won't be able to diagnose.\r\n\r\nYeah the node needs an indicator for whether a webhook is attached or not. Determining why can usually be done client-side, because it depends directly on whether the user filled out all the required inputs on the node.\r\n\r\n- [x] Create issue for webhook status indicator on webhook-triggered nodes\r\n\r\n> The Run button throws an error when you run (this is better than crashing tho)\r\n\r\nWould you rather hide the button? I'm not sure how to properly fix this.\r\n\r\n> We may want to do something to require platform base URL to be set if someone uses a trigger because currently it just doesn't do anything.\r\n\r\n- [x] Disable webhook-triggered blocks if `PLATFORM_BASE_URL` is not set\r\n- [x] Raise error in `BaseWebhooksManager` on attempt to create webhook if `PLATFORM_BASE_URL` is not set\r\n\r\n> we currently don't actually check platform base URL on inbound webhooks so we just execute from anything lol.\r\n> \r\n> > Replicate by running ngrok and disabling the line in your .env\r\n\r\nWhy would we need to check it on inbound webhook payloads? If it arrives, that's a job done. The `PLATFORM_BASE_URL` is only necessary to configure the webhook in the first place."
    ],
    "code_diff": "diff --git a/autogpt_platform/backend/.env.example b/autogpt_platform/backend/.env.example\nindex b6d41c25d449..0dd10e838501 100644\n--- a/autogpt_platform/backend/.env.example\n+++ b/autogpt_platform/backend/.env.example\n@@ -28,8 +28,15 @@ SUPABASE_URL=http://localhost:8000\n SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q\n SUPABASE_JWT_SECRET=your-super-secret-jwt-token-with-at-least-32-characters-long\n \n-# For local development, you may need to set FRONTEND_BASE_URL for the OAuth flow for integrations to work.\n-FRONTEND_BASE_URL=http://localhost:3000\n+## For local development, you may need to set FRONTEND_BASE_URL for the OAuth flow\n+## for integrations to work. Defaults to the value of PLATFORM_BASE_URL if not set.\n+# FRONTEND_BASE_URL=http://localhost:3000\n+\n+## PLATFORM_BASE_URL must be set to a *publicly accessible* URL pointing to your backend\n+## to use the platform's webhook-related functionality.\n+## If you are developing locally, you can use something like ngrok to get a publc URL\n+## and tunnel it to your locally running backend.\n+PLATFORM_BASE_URL=https://your-public-url-here\n \n ## == INTEGRATION CREDENTIALS == ##\n # Each set of server side credentials is required for the corresponding 3rd party\ndiff --git a/autogpt_platform/backend/backend/blocks/__init__.py b/autogpt_platform/backend/backend/blocks/__init__.py\nindex 4fb89e3957ff..03b4a9701760 100644\n--- a/autogpt_platform/backend/backend/blocks/__init__.py\n+++ b/autogpt_platform/backend/backend/blocks/__init__.py\n@@ -60,13 +60,6 @@ def all_subclasses(cls: Type[T]) -> list[Type[T]]:\n     input_schema = block.input_schema.model_fields\n     output_schema = block.output_schema.model_fields\n \n-    # Prevent duplicate field name in input_schema and output_schema\n-    duplicate_field_names = set(input_schema.keys()) & set(output_schema.keys())\n-    if duplicate_field_names:\n-        raise ValueError(\n-            f\"{block.name} has duplicate field names in input_schema and output_schema: {duplicate_field_names}\"\n-        )\n-\n     # Make sure `error` field is a string in the output schema\n     if \"error\" in output_schema and output_schema[\"error\"].annotation is not str:\n         raise ValueError(\ndiff --git a/autogpt_platform/backend/backend/blocks/agent.py b/autogpt_platform/backend/backend/blocks/agent.py\nindex ec5c2efd6e31..afbe410e4291 100644\n--- a/autogpt_platform/backend/backend/blocks/agent.py\n+++ b/autogpt_platform/backend/backend/blocks/agent.py\n@@ -27,7 +27,7 @@ def get_executor_manager_client():\n \n @thread_cached\n def get_event_bus():\n-    from backend.data.queue import RedisExecutionEventBus\n+    from backend.data.execution import RedisExecutionEventBus\n \n     return RedisExecutionEventBus()\n \ndiff --git a/autogpt_platform/backend/backend/blocks/github/example_payloads/pull_request.synchronize.json b/autogpt_platform/backend/backend/blocks/github/example_payloads/pull_request.synchronize.json\nnew file mode 100644\nindex 000000000000..7d8f8efbe054\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/github/example_payloads/pull_request.synchronize.json\n@@ -0,0 +1,700 @@\n+{\n+  \"action\": \"synchronize\",\n+  \"number\": 8358,\n+  \"pull_request\": {\n+    \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls/8358\",\n+    \"id\": 2128918491,\n+    \"node_id\": \"PR_kwDOJKSTjM5-5Lfb\",\n+    \"html_url\": \"https://github.com/Significant-Gravitas/AutoGPT/pull/8358\",\n+    \"diff_url\": \"https://github.com/Significant-Gravitas/AutoGPT/pull/8358.diff\",\n+    \"patch_url\": \"https://github.com/Significant-Gravitas/AutoGPT/pull/8358.patch\",\n+    \"issue_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/8358\",\n+    \"number\": 8358,\n+    \"state\": \"open\",\n+    \"locked\": false,\n+    \"title\": \"feat(platform, blocks): Webhook-triggered blocks\",\n+    \"user\": {\n+      \"login\": \"Pwuts\",\n+      \"id\": 12185583,\n+      \"node_id\": \"MDQ6VXNlcjEyMTg1NTgz\",\n+      \"avatar_url\": \"https://avatars.githubusercontent.com/u/12185583?v=4\",\n+      \"gravatar_id\": \"\",\n+      \"url\": \"https://api.github.com/users/Pwuts\",\n+      \"html_url\": \"https://github.com/Pwuts\",\n+      \"followers_url\": \"https://api.github.com/users/Pwuts/followers\",\n+      \"following_url\": \"https://api.github.com/users/Pwuts/following{/other_user}\",\n+      \"gists_url\": \"https://api.github.com/users/Pwuts/gists{/gist_id}\",\n+      \"starred_url\": \"https://api.github.com/users/Pwuts/starred{/owner}{/repo}\",\n+      \"subscriptions_url\": \"https://api.github.com/users/Pwuts/subscriptions\",\n+      \"organizations_url\": \"https://api.github.com/users/Pwuts/orgs\",\n+      \"repos_url\": \"https://api.github.com/users/Pwuts/repos\",\n+      \"events_url\": \"https://api.github.com/users/Pwuts/events{/privacy}\",\n+      \"received_events_url\": \"https://api.github.com/users/Pwuts/received_events\",\n+      \"type\": \"User\",\n+      \"user_view_type\": \"public\",\n+      \"site_admin\": false\n+    },\n+    \"body\": \"- Resolves #8352\\r\\n\\r\\n## Changes \\r\\n\\r\\n- feat(blocks): Add GitHub Pull Request Trigger block\\r\\n\\r\\n### feat(platform): Add support for Webhook-triggered blocks\\r\\n-  Add `PLATFORM_BASE_URL` setting\\r\\n\\r\\n- Add webhook config option and `BlockType.WEBHOOK` to `Block`\\r\\n  - Add check to `Block.__init__` to enforce type and shape of webhook event filter\\r\\n  - Add check to `Block.__init__` to enforce `payload` input on webhook blocks\\r\\n\\r\\n- Add `Webhook` model + CRUD functions in `backend.data.integrations` to represent webhooks created by our system\\r\\n  - Add `IntegrationWebhook` to DB schema + reference `AgentGraphNode.webhook_id`\\r\\n    - Add `set_node_webhook(..)` in `backend.data.graph`\\r\\n\\r\\n- Add webhook-related endpoints:\\r\\n  - `POST /integrations/{provider}/webhooks/{webhook_id}/ingress` endpoint, to receive webhook payloads, and for all associated nodes create graph executions\\r\\n    - Add `Node.is_triggered_by_event_type(..)` helper method\\r\\n  - `POST /integrations/{provider}/webhooks/{webhook_id}/ping` endpoint, to allow testing a webhook\\r\\n  - Add `WebhookEvent` + pub/sub functions in `backend.data.integrations`\\r\\n\\r\\n- Add `backend.integrations.webhooks` module, including:\\r\\n  - `graph_lifecycle_hooks`, e.g. `on_graph_activate(..)`, to handle corresponding webhook creation etc.\\r\\n    - Add calls to these hooks in the graph create/update endpoints\\r\\n  - `BaseWebhooksManager` + `GithubWebhooksManager` to handle creating + registering, removing + deregistering, and retrieving existing webhooks, and validating incoming payloads\\r\\n\\r\\n### Other improvements\\r\\n- fix(blocks): Allow having an input and output pin with the same name\\r\\n- feat(blocks): Allow hiding inputs (e.g. `payload`) with `SchemaField(hidden=True)`\\r\\n- feat(backend/data): Add `graph_id`, `graph_version` to `Node`; `user_id` to `GraphMeta`\\r\\n  - Add `Creatable` versions of `Node`, `GraphMeta` and `Graph` without these properties\\r\\n  - Add `graph_from_creatable(..)` helper function in `backend.data.graph`\\r\\n- refactor(backend/data): Make `RedisEventQueue` generic\\r\\n- refactor(frontend): Deduplicate & clean up code for different block types in `generateInputHandles(..)` in `CustomNode`\\r\\n- refactor(backend): Remove unused subgraph functionality\\r\\n\\r\\n## How it works\\r\\n- When a graph is created, the `on_graph_activate` and `on_node_activate` hooks are called on the graph and its nodes\\r\\n- If a webhook-triggered node has presets for all the relevant inputs, `on_node_activate` will get/create a suitable webhook and link it by setting `AgentGraphNode.webhook_id`\\r\\n  - `on_node_activate` uses `webhook_manager.get_suitable_webhook(..)`, which tries to find a suitable webhook (with matching requirements) or creates it if none exists yet\\r\\n- When a graph is deactivated (in favor of a newer/other version) or deleted, `on_graph_deactivate` and `on_node_deactivate` are called on the graph and its nodes to clean up webhooks that are no longer in use\\r\\n- When a valid webhook payload is received, two things happen:\\r\\n  1. It is broadcast on the Redis channel `webhooks/{webhook_id}/{event_type}`\\r\\n  2. Graph executions are initiated for all nodes triggered by this webhook\\r\\n\\r\\n## TODO\\r\\n- [ ] #8537\\r\\n- [x] #8538\\r\\n- [ ] #8357\\r\\n- [ ] ~~#8554~~ can be done in a follow-up PR\\r\\n- [ ] Test test test!\\r\\n- [ ] Add note on `repo` input of webhook blocks that the credentials used must have the right permissions for the given organization/repo\\r\\n- [x] Implement proper detection and graceful handling of webhook creation failing due to insufficient permissions. This should give a clear message to the user to e.g. \\\"give the app access to this organization in your settings\\\".\\r\\n- [ ] Nice-to-have: make a button on webhook blocks to trigger a ping and check its result. The API endpoints for this is already implemented.\",\n+    \"created_at\": \"2024-10-16T22:13:47Z\",\n+    \"updated_at\": \"2024-11-11T18:34:54Z\",\n+    \"closed_at\": null,\n+    \"merged_at\": null,\n+    \"merge_commit_sha\": \"cbfd0cdd8db52cdd5a3b7ce088fc0ab4617a652e\",\n+    \"assignee\": {\n+      \"login\": \"Pwuts\",\n+      \"id\": 12185583,\n+      \"node_id\": \"MDQ6VXNlcjEyMTg1NTgz\",\n+      \"avatar_url\": \"https://avatars.githubusercontent.com/u/12185583?v=4\",\n+      \"gravatar_id\": \"\",\n+      \"url\": \"https://api.github.com/users/Pwuts\",\n+      \"html_url\": \"https://github.com/Pwuts\",\n+      \"followers_url\": \"https://api.github.com/users/Pwuts/followers\",\n+      \"following_url\": \"https://api.github.com/users/Pwuts/following{/other_user}\",\n+      \"gists_url\": \"https://api.github.com/users/Pwuts/gists{/gist_id}\",\n+      \"starred_url\": \"https://api.github.com/users/Pwuts/starred{/owner}{/repo}\",\n+      \"subscriptions_url\": \"https://api.github.com/users/Pwuts/subscriptions\",\n+      \"organizations_url\": \"https://api.github.com/users/Pwuts/orgs\",\n+      \"repos_url\": \"https://api.github.com/users/Pwuts/repos\",\n+      \"events_url\": \"https://api.github.com/users/Pwuts/events{/privacy}\",\n+      \"received_events_url\": \"https://api.github.com/users/Pwuts/received_events\",\n+      \"type\": \"User\",\n+      \"user_view_type\": \"public\",\n+      \"site_admin\": false\n+    },\n+    \"assignees\": [\n+      {\n+        \"login\": \"Pwuts\",\n+        \"id\": 12185583,\n+        \"node_id\": \"MDQ6VXNlcjEyMTg1NTgz\",\n+        \"avatar_url\": \"https://avatars.githubusercontent.com/u/12185583?v=4\",\n+        \"gravatar_id\": \"\",\n+        \"url\": \"https://api.github.com/users/Pwuts\",\n+        \"html_url\": \"https://github.com/Pwuts\",\n+        \"followers_url\": \"https://api.github.com/users/Pwuts/followers\",\n+        \"following_url\": \"https://api.github.com/users/Pwuts/following{/other_user}\",\n+        \"gists_url\": \"https://api.github.com/users/Pwuts/gists{/gist_id}\",\n+        \"starred_url\": \"https://api.github.com/users/Pwuts/starred{/owner}{/repo}\",\n+        \"subscriptions_url\": \"https://api.github.com/users/Pwuts/subscriptions\",\n+        \"organizations_url\": \"https://api.github.com/users/Pwuts/orgs\",\n+        \"repos_url\": \"https://api.github.com/users/Pwuts/repos\",\n+        \"events_url\": \"https://api.github.com/users/Pwuts/events{/privacy}\",\n+        \"received_events_url\": \"https://api.github.com/users/Pwuts/received_events\",\n+        \"type\": \"User\",\n+        \"user_view_type\": \"public\",\n+        \"site_admin\": false\n+      }\n+    ],\n+    \"requested_reviewers\": [\n+      {\n+        \"login\": \"kcze\",\n+        \"id\": 34861343,\n+        \"node_id\": \"MDQ6VXNlcjM0ODYxMzQz\",\n+        \"avatar_url\": \"https://avatars.githubusercontent.com/u/34861343?v=4\",\n+        \"gravatar_id\": \"\",\n+        \"url\": \"https://api.github.com/users/kcze\",\n+        \"html_url\": \"https://github.com/kcze\",\n+        \"followers_url\": \"https://api.github.com/users/kcze/followers\",\n+        \"following_url\": \"https://api.github.com/users/kcze/following{/other_user}\",\n+        \"gists_url\": \"https://api.github.com/users/kcze/gists{/gist_id}\",\n+        \"starred_url\": \"https://api.github.com/users/kcze/starred{/owner}{/repo}\",\n+        \"subscriptions_url\": \"https://api.github.com/users/kcze/subscriptions\",\n+        \"organizations_url\": \"https://api.github.com/users/kcze/orgs\",\n+        \"repos_url\": \"https://api.github.com/users/kcze/repos\",\n+        \"events_url\": \"https://api.github.com/users/kcze/events{/privacy}\",\n+        \"received_events_url\": \"https://api.github.com/users/kcze/received_events\",\n+        \"type\": \"User\",\n+        \"user_view_type\": \"public\",\n+        \"site_admin\": false\n+      }\n+    ],\n+    \"requested_teams\": [\n+      {\n+        \"name\": \"DevOps\",\n+        \"id\": 9547361,\n+        \"node_id\": \"T_kwDOB8roIc4Aka5h\",\n+        \"slug\": \"devops\",\n+        \"description\": \"\",\n+        \"privacy\": \"closed\",\n+        \"notification_setting\": \"notifications_enabled\",\n+        \"url\": \"https://api.github.com/organizations/130738209/team/9547361\",\n+        \"html_url\": \"https://github.com/orgs/Significant-Gravitas/teams/devops\",\n+        \"members_url\": \"https://api.github.com/organizations/130738209/team/9547361/members{/member}\",\n+        \"repositories_url\": \"https://api.github.com/organizations/130738209/team/9547361/repos\",\n+        \"permission\": \"pull\",\n+        \"parent\": null\n+      }\n+    ],\n+    \"labels\": [\n+      {\n+        \"id\": 5272676214,\n+        \"node_id\": \"LA_kwDOJKSTjM8AAAABOkandg\",\n+        \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/labels/documentation\",\n+        \"name\": \"documentation\",\n+        \"color\": \"0075ca\",\n+        \"default\": true,\n+        \"description\": \"Improvements or additions to documentation\"\n+      },\n+      {\n+        \"id\": 5410633769,\n+        \"node_id\": \"LA_kwDOJKSTjM8AAAABQn-4KQ\",\n+        \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/labels/size/xl\",\n+        \"name\": \"size/xl\",\n+        \"color\": \"E751DD\",\n+        \"default\": false,\n+        \"description\": \"\"\n+      },\n+      {\n+        \"id\": 6892322271,\n+        \"node_id\": \"LA_kwDOJKSTjM8AAAABmtB93w\",\n+        \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/labels/Review%20effort%20[1-5]:%204\",\n+        \"name\": \"Review effort [1-5]: 4\",\n+        \"color\": \"d1bcf9\",\n+        \"default\": false,\n+        \"description\": null\n+      },\n+      {\n+        \"id\": 7218433025,\n+        \"node_id\": \"LA_kwDOJKSTjM8AAAABrkCMAQ\",\n+        \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/labels/platform/frontend\",\n+        \"name\": \"platform/frontend\",\n+        \"color\": \"033C07\",\n+        \"default\": false,\n+        \"description\": \"AutoGPT Platform - Front end\"\n+      },\n+      {\n+        \"id\": 7219356193,\n+        \"node_id\": \"LA_kwDOJKSTjM8AAAABrk6iIQ\",\n+        \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/labels/platform/backend\",\n+        \"name\": \"platform/backend\",\n+        \"color\": \"ededed\",\n+        \"default\": false,\n+        \"description\": \"AutoGPT Platform - Back end\"\n+      },\n+      {\n+        \"id\": 7515330106,\n+        \"node_id\": \"LA_kwDOJKSTjM8AAAABv_LWOg\",\n+        \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/labels/platform/blocks\",\n+        \"name\": \"platform/blocks\",\n+        \"color\": \"eb5757\",\n+        \"default\": false,\n+        \"description\": null\n+      }\n+    ],\n+    \"milestone\": null,\n+    \"draft\": false,\n+    \"commits_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls/8358/commits\",\n+    \"review_comments_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls/8358/comments\",\n+    \"review_comment_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls/comments{/number}\",\n+    \"comments_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/8358/comments\",\n+    \"statuses_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/statuses/8f708a2b60463eec10747d8f45dead35b5a45bd0\",\n+    \"head\": {\n+      \"label\": \"Significant-Gravitas:reinier/open-1961-implement-github-on-pull-request-block\",\n+      \"ref\": \"reinier/open-1961-implement-github-on-pull-request-block\",\n+      \"sha\": \"8f708a2b60463eec10747d8f45dead35b5a45bd0\",\n+      \"user\": {\n+        \"login\": \"Significant-Gravitas\",\n+        \"id\": 130738209,\n+        \"node_id\": \"O_kgDOB8roIQ\",\n+        \"avatar_url\": \"https://avatars.githubusercontent.com/u/130738209?v=4\",\n+        \"gravatar_id\": \"\",\n+        \"url\": \"https://api.github.com/users/Significant-Gravitas\",\n+        \"html_url\": \"https://github.com/Significant-Gravitas\",\n+        \"followers_url\": \"https://api.github.com/users/Significant-Gravitas/followers\",\n+        \"following_url\": \"https://api.github.com/users/Significant-Gravitas/following{/other_user}\",\n+        \"gists_url\": \"https://api.github.com/users/Significant-Gravitas/gists{/gist_id}\",\n+        \"starred_url\": \"https://api.github.com/users/Significant-Gravitas/starred{/owner}{/repo}\",\n+        \"subscriptions_url\": \"https://api.github.com/users/Significant-Gravitas/subscriptions\",\n+        \"organizations_url\": \"https://api.github.com/users/Significant-Gravitas/orgs\",\n+        \"repos_url\": \"https://api.github.com/users/Significant-Gravitas/repos\",\n+        \"events_url\": \"https://api.github.com/users/Significant-Gravitas/events{/privacy}\",\n+        \"received_events_url\": \"https://api.github.com/users/Significant-Gravitas/received_events\",\n+        \"type\": \"Organization\",\n+        \"user_view_type\": \"public\",\n+        \"site_admin\": false\n+      },\n+      \"repo\": {\n+        \"id\": 614765452,\n+        \"node_id\": \"R_kgDOJKSTjA\",\n+        \"name\": \"AutoGPT\",\n+        \"full_name\": \"Significant-Gravitas/AutoGPT\",\n+        \"private\": false,\n+        \"owner\": {\n+          \"login\": \"Significant-Gravitas\",\n+          \"id\": 130738209,\n+          \"node_id\": \"O_kgDOB8roIQ\",\n+          \"avatar_url\": \"https://avatars.githubusercontent.com/u/130738209?v=4\",\n+          \"gravatar_id\": \"\",\n+          \"url\": \"https://api.github.com/users/Significant-Gravitas\",\n+          \"html_url\": \"https://github.com/Significant-Gravitas\",\n+          \"followers_url\": \"https://api.github.com/users/Significant-Gravitas/followers\",\n+          \"following_url\": \"https://api.github.com/users/Significant-Gravitas/following{/other_user}\",\n+          \"gists_url\": \"https://api.github.com/users/Significant-Gravitas/gists{/gist_id}\",\n+          \"starred_url\": \"https://api.github.com/users/Significant-Gravitas/starred{/owner}{/repo}\",\n+          \"subscriptions_url\": \"https://api.github.com/users/Significant-Gravitas/subscriptions\",\n+          \"organizations_url\": \"https://api.github.com/users/Significant-Gravitas/orgs\",\n+          \"repos_url\": \"https://api.github.com/users/Significant-Gravitas/repos\",\n+          \"events_url\": \"https://api.github.com/users/Significant-Gravitas/events{/privacy}\",\n+          \"received_events_url\": \"https://api.github.com/users/Significant-Gravitas/received_events\",\n+          \"type\": \"Organization\",\n+          \"user_view_type\": \"public\",\n+          \"site_admin\": false\n+        },\n+        \"html_url\": \"https://github.com/Significant-Gravitas/AutoGPT\",\n+        \"description\": \"AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.\",\n+        \"fork\": false,\n+        \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT\",\n+        \"forks_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/forks\",\n+        \"keys_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/keys{/key_id}\",\n+        \"collaborators_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/collaborators{/collaborator}\",\n+        \"teams_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/teams\",\n+        \"hooks_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/hooks\",\n+        \"issue_events_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/events{/number}\",\n+        \"events_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/events\",\n+        \"assignees_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/assignees{/user}\",\n+        \"branches_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/branches{/branch}\",\n+        \"tags_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/tags\",\n+        \"blobs_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/blobs{/sha}\",\n+        \"git_tags_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/tags{/sha}\",\n+        \"git_refs_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/refs{/sha}\",\n+        \"trees_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/trees{/sha}\",\n+        \"statuses_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/statuses/{sha}\",\n+        \"languages_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/languages\",\n+        \"stargazers_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/stargazers\",\n+        \"contributors_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/contributors\",\n+        \"subscribers_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/subscribers\",\n+        \"subscription_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/subscription\",\n+        \"commits_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/commits{/sha}\",\n+        \"git_commits_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/commits{/sha}\",\n+        \"comments_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/comments{/number}\",\n+        \"issue_comment_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/comments{/number}\",\n+        \"contents_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/contents/{+path}\",\n+        \"compare_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/compare/{base}...{head}\",\n+        \"merges_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/merges\",\n+        \"archive_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/{archive_format}{/ref}\",\n+        \"downloads_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/downloads\",\n+        \"issues_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues{/number}\",\n+        \"pulls_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls{/number}\",\n+        \"milestones_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/milestones{/number}\",\n+        \"notifications_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/notifications{?since,all,participating}\",\n+        \"labels_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/labels{/name}\",\n+        \"releases_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/releases{/id}\",\n+        \"deployments_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/deployments\",\n+        \"created_at\": \"2023-03-16T09:21:07Z\",\n+        \"updated_at\": \"2024-11-11T18:16:29Z\",\n+        \"pushed_at\": \"2024-11-11T18:34:52Z\",\n+        \"git_url\": \"git://github.com/Significant-Gravitas/AutoGPT.git\",\n+        \"ssh_url\": \"git@github.com:Significant-Gravitas/AutoGPT.git\",\n+        \"clone_url\": \"https://github.com/Significant-Gravitas/AutoGPT.git\",\n+        \"svn_url\": \"https://github.com/Significant-Gravitas/AutoGPT\",\n+        \"homepage\": \"https://agpt.co\",\n+        \"size\": 181894,\n+        \"stargazers_count\": 168203,\n+        \"watchers_count\": 168203,\n+        \"language\": \"Python\",\n+        \"has_issues\": true,\n+        \"has_projects\": true,\n+        \"has_downloads\": true,\n+        \"has_wiki\": true,\n+        \"has_pages\": false,\n+        \"has_discussions\": true,\n+        \"forks_count\": 44376,\n+        \"mirror_url\": null,\n+        \"archived\": false,\n+        \"disabled\": false,\n+        \"open_issues_count\": 189,\n+        \"license\": {\n+          \"key\": \"other\",\n+          \"name\": \"Other\",\n+          \"spdx_id\": \"NOASSERTION\",\n+          \"url\": null,\n+          \"node_id\": \"MDc6TGljZW5zZTA=\"\n+        },\n+        \"allow_forking\": true,\n+        \"is_template\": false,\n+        \"web_commit_signoff_required\": false,\n+        \"topics\": [\n+          \"ai\",\n+          \"artificial-intelligence\",\n+          \"autonomous-agents\",\n+          \"gpt-4\",\n+          \"openai\",\n+          \"python\"\n+        ],\n+        \"visibility\": \"public\",\n+        \"forks\": 44376,\n+        \"open_issues\": 189,\n+        \"watchers\": 168203,\n+        \"default_branch\": \"master\",\n+        \"allow_squash_merge\": true,\n+        \"allow_merge_commit\": false,\n+        \"allow_rebase_merge\": false,\n+        \"allow_auto_merge\": true,\n+        \"delete_branch_on_merge\": true,\n+        \"allow_update_branch\": true,\n+        \"use_squash_pr_title_as_default\": true,\n+        \"squash_merge_commit_message\": \"COMMIT_MESSAGES\",\n+        \"squash_merge_commit_title\": \"PR_TITLE\",\n+        \"merge_commit_message\": \"BLANK\",\n+        \"merge_commit_title\": \"PR_TITLE\"\n+      }\n+    },\n+    \"base\": {\n+      \"label\": \"Significant-Gravitas:dev\",\n+      \"ref\": \"dev\",\n+      \"sha\": \"0b5b95eff5e18c1e162d2b30b66a7be2bed1cbc2\",\n+      \"user\": {\n+        \"login\": \"Significant-Gravitas\",\n+        \"id\": 130738209,\n+        \"node_id\": \"O_kgDOB8roIQ\",\n+        \"avatar_url\": \"https://avatars.githubusercontent.com/u/130738209?v=4\",\n+        \"gravatar_id\": \"\",\n+        \"url\": \"https://api.github.com/users/Significant-Gravitas\",\n+        \"html_url\": \"https://github.com/Significant-Gravitas\",\n+        \"followers_url\": \"https://api.github.com/users/Significant-Gravitas/followers\",\n+        \"following_url\": \"https://api.github.com/users/Significant-Gravitas/following{/other_user}\",\n+        \"gists_url\": \"https://api.github.com/users/Significant-Gravitas/gists{/gist_id}\",\n+        \"starred_url\": \"https://api.github.com/users/Significant-Gravitas/starred{/owner}{/repo}\",\n+        \"subscriptions_url\": \"https://api.github.com/users/Significant-Gravitas/subscriptions\",\n+        \"organizations_url\": \"https://api.github.com/users/Significant-Gravitas/orgs\",\n+        \"repos_url\": \"https://api.github.com/users/Significant-Gravitas/repos\",\n+        \"events_url\": \"https://api.github.com/users/Significant-Gravitas/events{/privacy}\",\n+        \"received_events_url\": \"https://api.github.com/users/Significant-Gravitas/received_events\",\n+        \"type\": \"Organization\",\n+        \"user_view_type\": \"public\",\n+        \"site_admin\": false\n+      },\n+      \"repo\": {\n+        \"id\": 614765452,\n+        \"node_id\": \"R_kgDOJKSTjA\",\n+        \"name\": \"AutoGPT\",\n+        \"full_name\": \"Significant-Gravitas/AutoGPT\",\n+        \"private\": false,\n+        \"owner\": {\n+          \"login\": \"Significant-Gravitas\",\n+          \"id\": 130738209,\n+          \"node_id\": \"O_kgDOB8roIQ\",\n+          \"avatar_url\": \"https://avatars.githubusercontent.com/u/130738209?v=4\",\n+          \"gravatar_id\": \"\",\n+          \"url\": \"https://api.github.com/users/Significant-Gravitas\",\n+          \"html_url\": \"https://github.com/Significant-Gravitas\",\n+          \"followers_url\": \"https://api.github.com/users/Significant-Gravitas/followers\",\n+          \"following_url\": \"https://api.github.com/users/Significant-Gravitas/following{/other_user}\",\n+          \"gists_url\": \"https://api.github.com/users/Significant-Gravitas/gists{/gist_id}\",\n+          \"starred_url\": \"https://api.github.com/users/Significant-Gravitas/starred{/owner}{/repo}\",\n+          \"subscriptions_url\": \"https://api.github.com/users/Significant-Gravitas/subscriptions\",\n+          \"organizations_url\": \"https://api.github.com/users/Significant-Gravitas/orgs\",\n+          \"repos_url\": \"https://api.github.com/users/Significant-Gravitas/repos\",\n+          \"events_url\": \"https://api.github.com/users/Significant-Gravitas/events{/privacy}\",\n+          \"received_events_url\": \"https://api.github.com/users/Significant-Gravitas/received_events\",\n+          \"type\": \"Organization\",\n+          \"user_view_type\": \"public\",\n+          \"site_admin\": false\n+        },\n+        \"html_url\": \"https://github.com/Significant-Gravitas/AutoGPT\",\n+        \"description\": \"AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.\",\n+        \"fork\": false,\n+        \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT\",\n+        \"forks_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/forks\",\n+        \"keys_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/keys{/key_id}\",\n+        \"collaborators_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/collaborators{/collaborator}\",\n+        \"teams_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/teams\",\n+        \"hooks_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/hooks\",\n+        \"issue_events_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/events{/number}\",\n+        \"events_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/events\",\n+        \"assignees_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/assignees{/user}\",\n+        \"branches_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/branches{/branch}\",\n+        \"tags_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/tags\",\n+        \"blobs_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/blobs{/sha}\",\n+        \"git_tags_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/tags{/sha}\",\n+        \"git_refs_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/refs{/sha}\",\n+        \"trees_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/trees{/sha}\",\n+        \"statuses_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/statuses/{sha}\",\n+        \"languages_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/languages\",\n+        \"stargazers_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/stargazers\",\n+        \"contributors_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/contributors\",\n+        \"subscribers_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/subscribers\",\n+        \"subscription_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/subscription\",\n+        \"commits_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/commits{/sha}\",\n+        \"git_commits_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/commits{/sha}\",\n+        \"comments_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/comments{/number}\",\n+        \"issue_comment_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/comments{/number}\",\n+        \"contents_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/contents/{+path}\",\n+        \"compare_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/compare/{base}...{head}\",\n+        \"merges_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/merges\",\n+        \"archive_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/{archive_format}{/ref}\",\n+        \"downloads_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/downloads\",\n+        \"issues_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues{/number}\",\n+        \"pulls_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls{/number}\",\n+        \"milestones_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/milestones{/number}\",\n+        \"notifications_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/notifications{?since,all,participating}\",\n+        \"labels_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/labels{/name}\",\n+        \"releases_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/releases{/id}\",\n+        \"deployments_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/deployments\",\n+        \"created_at\": \"2023-03-16T09:21:07Z\",\n+        \"updated_at\": \"2024-11-11T18:16:29Z\",\n+        \"pushed_at\": \"2024-11-11T18:34:52Z\",\n+        \"git_url\": \"git://github.com/Significant-Gravitas/AutoGPT.git\",\n+        \"ssh_url\": \"git@github.com:Significant-Gravitas/AutoGPT.git\",\n+        \"clone_url\": \"https://github.com/Significant-Gravitas/AutoGPT.git\",\n+        \"svn_url\": \"https://github.com/Significant-Gravitas/AutoGPT\",\n+        \"homepage\": \"https://agpt.co\",\n+        \"size\": 181894,\n+        \"stargazers_count\": 168203,\n+        \"watchers_count\": 168203,\n+        \"language\": \"Python\",\n+        \"has_issues\": true,\n+        \"has_projects\": true,\n+        \"has_downloads\": true,\n+        \"has_wiki\": true,\n+        \"has_pages\": false,\n+        \"has_discussions\": true,\n+        \"forks_count\": 44376,\n+        \"mirror_url\": null,\n+        \"archived\": false,\n+        \"disabled\": false,\n+        \"open_issues_count\": 189,\n+        \"license\": {\n+          \"key\": \"other\",\n+          \"name\": \"Other\",\n+          \"spdx_id\": \"NOASSERTION\",\n+          \"url\": null,\n+          \"node_id\": \"MDc6TGljZW5zZTA=\"\n+        },\n+        \"allow_forking\": true,\n+        \"is_template\": false,\n+        \"web_commit_signoff_required\": false,\n+        \"topics\": [\n+          \"ai\",\n+          \"artificial-intelligence\",\n+          \"autonomous-agents\",\n+          \"gpt-4\",\n+          \"openai\",\n+          \"python\"\n+        ],\n+        \"visibility\": \"public\",\n+        \"forks\": 44376,\n+        \"open_issues\": 189,\n+        \"watchers\": 168203,\n+        \"default_branch\": \"master\",\n+        \"allow_squash_merge\": true,\n+        \"allow_merge_commit\": false,\n+        \"allow_rebase_merge\": false,\n+        \"allow_auto_merge\": true,\n+        \"delete_branch_on_merge\": true,\n+        \"allow_update_branch\": true,\n+        \"use_squash_pr_title_as_default\": true,\n+        \"squash_merge_commit_message\": \"COMMIT_MESSAGES\",\n+        \"squash_merge_commit_title\": \"PR_TITLE\",\n+        \"merge_commit_message\": \"BLANK\",\n+        \"merge_commit_title\": \"PR_TITLE\"\n+      }\n+    },\n+    \"_links\": {\n+      \"self\": {\n+        \"href\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls/8358\"\n+      },\n+      \"html\": {\n+        \"href\": \"https://github.com/Significant-Gravitas/AutoGPT/pull/8358\"\n+      },\n+      \"issue\": {\n+        \"href\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/8358\"\n+      },\n+      \"comments\": {\n+        \"href\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/8358/comments\"\n+      },\n+      \"review_comments\": {\n+        \"href\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls/8358/comments\"\n+      },\n+      \"review_comment\": {\n+        \"href\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls/comments{/number}\"\n+      },\n+      \"commits\": {\n+        \"href\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls/8358/commits\"\n+      },\n+      \"statuses\": {\n+        \"href\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/statuses/8f708a2b60463eec10747d8f45dead35b5a45bd0\"\n+      }\n+    },\n+    \"author_association\": \"MEMBER\",\n+    \"auto_merge\": null,\n+    \"active_lock_reason\": null,\n+    \"merged\": false,\n+    \"mergeable\": null,\n+    \"rebaseable\": null,\n+    \"mergeable_state\": \"unknown\",\n+    \"merged_by\": null,\n+    \"comments\": 12,\n+    \"review_comments\": 29,\n+    \"maintainer_can_modify\": false,\n+    \"commits\": 62,\n+    \"additions\": 1674,\n+    \"deletions\": 331,\n+    \"changed_files\": 36\n+  },\n+  \"before\": \"f40aef87672203f47bbbd53f83fae0964c5624da\",\n+  \"after\": \"8f708a2b60463eec10747d8f45dead35b5a45bd0\",\n+  \"repository\": {\n+    \"id\": 614765452,\n+    \"node_id\": \"R_kgDOJKSTjA\",\n+    \"name\": \"AutoGPT\",\n+    \"full_name\": \"Significant-Gravitas/AutoGPT\",\n+    \"private\": false,\n+    \"owner\": {\n+      \"login\": \"Significant-Gravitas\",\n+      \"id\": 130738209,\n+      \"node_id\": \"O_kgDOB8roIQ\",\n+      \"avatar_url\": \"https://avatars.githubusercontent.com/u/130738209?v=4\",\n+      \"gravatar_id\": \"\",\n+      \"url\": \"https://api.github.com/users/Significant-Gravitas\",\n+      \"html_url\": \"https://github.com/Significant-Gravitas\",\n+      \"followers_url\": \"https://api.github.com/users/Significant-Gravitas/followers\",\n+      \"following_url\": \"https://api.github.com/users/Significant-Gravitas/following{/other_user}\",\n+      \"gists_url\": \"https://api.github.com/users/Significant-Gravitas/gists{/gist_id}\",\n+      \"starred_url\": \"https://api.github.com/users/Significant-Gravitas/starred{/owner}{/repo}\",\n+      \"subscriptions_url\": \"https://api.github.com/users/Significant-Gravitas/subscriptions\",\n+      \"organizations_url\": \"https://api.github.com/users/Significant-Gravitas/orgs\",\n+      \"repos_url\": \"https://api.github.com/users/Significant-Gravitas/repos\",\n+      \"events_url\": \"https://api.github.com/users/Significant-Gravitas/events{/privacy}\",\n+      \"received_events_url\": \"https://api.github.com/users/Significant-Gravitas/received_events\",\n+      \"type\": \"Organization\",\n+      \"user_view_type\": \"public\",\n+      \"site_admin\": false\n+    },\n+    \"html_url\": \"https://github.com/Significant-Gravitas/AutoGPT\",\n+    \"description\": \"AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.\",\n+    \"fork\": false,\n+    \"url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT\",\n+    \"forks_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/forks\",\n+    \"keys_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/keys{/key_id}\",\n+    \"collaborators_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/collaborators{/collaborator}\",\n+    \"teams_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/teams\",\n+    \"hooks_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/hooks\",\n+    \"issue_events_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/events{/number}\",\n+    \"events_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/events\",\n+    \"assignees_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/assignees{/user}\",\n+    \"branches_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/branches{/branch}\",\n+    \"tags_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/tags\",\n+    \"blobs_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/blobs{/sha}\",\n+    \"git_tags_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/tags{/sha}\",\n+    \"git_refs_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/refs{/sha}\",\n+    \"trees_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/trees{/sha}\",\n+    \"statuses_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/statuses/{sha}\",\n+    \"languages_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/languages\",\n+    \"stargazers_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/stargazers\",\n+    \"contributors_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/contributors\",\n+    \"subscribers_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/subscribers\",\n+    \"subscription_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/subscription\",\n+    \"commits_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/commits{/sha}\",\n+    \"git_commits_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/git/commits{/sha}\",\n+    \"comments_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/comments{/number}\",\n+    \"issue_comment_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues/comments{/number}\",\n+    \"contents_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/contents/{+path}\",\n+    \"compare_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/compare/{base}...{head}\",\n+    \"merges_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/merges\",\n+    \"archive_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/{archive_format}{/ref}\",\n+    \"downloads_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/downloads\",\n+    \"issues_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/issues{/number}\",\n+    \"pulls_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/pulls{/number}\",\n+    \"milestones_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/milestones{/number}\",\n+    \"notifications_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/notifications{?since,all,participating}\",\n+    \"labels_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/labels{/name}\",\n+    \"releases_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/releases{/id}\",\n+    \"deployments_url\": \"https://api.github.com/repos/Significant-Gravitas/AutoGPT/deployments\",\n+    \"created_at\": \"2023-03-16T09:21:07Z\",\n+    \"updated_at\": \"2024-11-11T18:16:29Z\",\n+    \"pushed_at\": \"2024-11-11T18:34:52Z\",\n+    \"git_url\": \"git://github.com/Significant-Gravitas/AutoGPT.git\",\n+    \"ssh_url\": \"git@github.com:Significant-Gravitas/AutoGPT.git\",\n+    \"clone_url\": \"https://github.com/Significant-Gravitas/AutoGPT.git\",\n+    \"svn_url\": \"https://github.com/Significant-Gravitas/AutoGPT\",\n+    \"homepage\": \"https://agpt.co\",\n+    \"size\": 181894,\n+    \"stargazers_count\": 168203,\n+    \"watchers_count\": 168203,\n+    \"language\": \"Python\",\n+    \"has_issues\": true,\n+    \"has_projects\": true,\n+    \"has_downloads\": true,\n+    \"has_wiki\": true,\n+    \"has_pages\": false,\n+    \"has_discussions\": true,\n+    \"forks_count\": 44376,\n+    \"mirror_url\": null,\n+    \"archived\": false,\n+    \"disabled\": false,\n+    \"open_issues_count\": 189,\n+    \"license\": {\n+      \"key\": \"other\",\n+      \"name\": \"Other\",\n+      \"spdx_id\": \"NOASSERTION\",\n+      \"url\": null,\n+      \"node_id\": \"MDc6TGljZW5zZTA=\"\n+    },\n+    \"allow_forking\": true,\n+    \"is_template\": false,\n+    \"web_commit_signoff_required\": false,\n+    \"topics\": [\n+      \"ai\",\n+      \"artificial-intelligence\",\n+      \"autonomous-agents\",\n+      \"gpt-4\",\n+      \"openai\",\n+      \"python\"\n+    ],\n+    \"visibility\": \"public\",\n+    \"forks\": 44376,\n+    \"open_issues\": 189,\n+    \"watchers\": 168203,\n+    \"default_branch\": \"master\",\n+    \"custom_properties\": {\n+\n+    }\n+  },\n+  \"organization\": {\n+    \"login\": \"Significant-Gravitas\",\n+    \"id\": 130738209,\n+    \"node_id\": \"O_kgDOB8roIQ\",\n+    \"url\": \"https://api.github.com/orgs/Significant-Gravitas\",\n+    \"repos_url\": \"https://api.github.com/orgs/Significant-Gravitas/repos\",\n+    \"events_url\": \"https://api.github.com/orgs/Significant-Gravitas/events\",\n+    \"hooks_url\": \"https://api.github.com/orgs/Significant-Gravitas/hooks\",\n+    \"issues_url\": \"https://api.github.com/orgs/Significant-Gravitas/issues\",\n+    \"members_url\": \"https://api.github.com/orgs/Significant-Gravitas/members{/member}\",\n+    \"public_members_url\": \"https://api.github.com/orgs/Significant-Gravitas/public_members{/member}\",\n+    \"avatar_url\": \"https://avatars.githubusercontent.com/u/130738209?v=4\",\n+    \"description\": \"\"\n+  },\n+  \"enterprise\": {\n+    \"id\": 149607,\n+    \"slug\": \"significant-gravitas\",\n+    \"name\": \"Significant Gravitas\",\n+    \"node_id\": \"E_kgDOAAJIZw\",\n+    \"avatar_url\": \"https://avatars.githubusercontent.com/b/149607?v=4\",\n+    \"description\": \"The creators of AutoGPT\",\n+    \"website_url\": \"discord.gg/autogpt\",\n+    \"html_url\": \"https://github.com/enterprises/significant-gravitas\",\n+    \"created_at\": \"2024-04-18T17:43:53Z\",\n+    \"updated_at\": \"2024-10-23T16:59:55Z\"\n+  },\n+  \"sender\": {\n+    \"login\": \"Pwuts\",\n+    \"id\": 12185583,\n+    \"node_id\": \"MDQ6VXNlcjEyMTg1NTgz\",\n+    \"avatar_url\": \"https://avatars.githubusercontent.com/u/12185583?v=4\",\n+    \"gravatar_id\": \"\",\n+    \"url\": \"https://api.github.com/users/Pwuts\",\n+    \"html_url\": \"https://github.com/Pwuts\",\n+    \"followers_url\": \"https://api.github.com/users/Pwuts/followers\",\n+    \"following_url\": \"https://api.github.com/users/Pwuts/following{/other_user}\",\n+    \"gists_url\": \"https://api.github.com/users/Pwuts/gists{/gist_id}\",\n+    \"starred_url\": \"https://api.github.com/users/Pwuts/starred{/owner}{/repo}\",\n+    \"subscriptions_url\": \"https://api.github.com/users/Pwuts/subscriptions\",\n+    \"organizations_url\": \"https://api.github.com/users/Pwuts/orgs\",\n+    \"repos_url\": \"https://api.github.com/users/Pwuts/repos\",\n+    \"events_url\": \"https://api.github.com/users/Pwuts/events{/privacy}\",\n+    \"received_events_url\": \"https://api.github.com/users/Pwuts/received_events\",\n+    \"type\": \"User\",\n+    \"user_view_type\": \"public\",\n+    \"site_admin\": false\n+  }\n+}\n\\ No newline at end of file\ndiff --git a/autogpt_platform/backend/backend/blocks/github/triggers.py b/autogpt_platform/backend/backend/blocks/github/triggers.py\nnew file mode 100644\nindex 000000000000..ce24a649fe2f\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/blocks/github/triggers.py\n@@ -0,0 +1,156 @@\n+import json\n+import logging\n+from pathlib import Path\n+\n+from pydantic import BaseModel\n+\n+from backend.data.block import (\n+    Block,\n+    BlockCategory,\n+    BlockOutput,\n+    BlockSchema,\n+    BlockWebhookConfig,\n+)\n+from backend.data.model import SchemaField\n+\n+from ._auth import (\n+    TEST_CREDENTIALS,\n+    TEST_CREDENTIALS_INPUT,\n+    GithubCredentialsField,\n+    GithubCredentialsInput,\n+)\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+# --8<-- [start:GithubTriggerExample]\n+class GitHubTriggerBase:\n+    class Input(BlockSchema):\n+        credentials: GithubCredentialsInput = GithubCredentialsField(\"repo\")\n+        repo: str = SchemaField(\n+            description=(\n+                \"Repository to subscribe to.\\n\\n\"\n+                \"**Note:** Make sure your GitHub credentials have permissions \"\n+                \"to create webhooks on this repo.\"\n+            ),\n+            placeholder=\"{owner}/{repo}\",\n+        )\n+        # --8<-- [start:example-payload-field]\n+        payload: dict = SchemaField(hidden=True, default={})\n+        # --8<-- [end:example-payload-field]\n+\n+    class Output(BlockSchema):\n+        payload: dict = SchemaField(\n+            description=\"The complete webhook payload that was received from GitHub. \"\n+            \"Includes information about the affected resource (e.g. pull request), \"\n+            \"the event, and the user who triggered the event.\"\n+        )\n+        triggered_by_user: dict = SchemaField(\n+            description=\"Object representing the GitHub user who triggered the event\"\n+        )\n+        error: str = SchemaField(\n+            description=\"Error message if the payload could not be processed\"\n+        )\n+\n+    def run(self, input_data: Input, **kwargs) -> BlockOutput:\n+        yield \"payload\", input_data.payload\n+        yield \"triggered_by_user\", input_data.payload[\"sender\"]\n+\n+\n+class GithubPullRequestTriggerBlock(GitHubTriggerBase, Block):\n+    EXAMPLE_PAYLOAD_FILE = (\n+        Path(__file__).parent / \"example_payloads\" / \"pull_request.synchronize.json\"\n+    )\n+\n+    # --8<-- [start:example-event-filter]\n+    class Input(GitHubTriggerBase.Input):\n+        class EventsFilter(BaseModel):\n+            \"\"\"\n+            https://docs.github.com/en/webhooks/webhook-events-and-payloads#pull_request\n+            \"\"\"\n+\n+            opened: bool = False\n+            edited: bool = False\n+            closed: bool = False\n+            reopened: bool = False\n+            synchronize: bool = False\n+            assigned: bool = False\n+            unassigned: bool = False\n+            labeled: bool = False\n+            unlabeled: bool = False\n+            converted_to_draft: bool = False\n+            locked: bool = False\n+            unlocked: bool = False\n+            enqueued: bool = False\n+            dequeued: bool = False\n+            milestoned: bool = False\n+            demilestoned: bool = False\n+            ready_for_review: bool = False\n+            review_requested: bool = False\n+            review_request_removed: bool = False\n+            auto_merge_enabled: bool = False\n+            auto_merge_disabled: bool = False\n+\n+        events: EventsFilter = SchemaField(\n+            title=\"Events\", description=\"The events to subscribe to\"\n+        )\n+        # --8<-- [end:example-event-filter]\n+\n+    class Output(GitHubTriggerBase.Output):\n+        event: str = SchemaField(\n+            description=\"The PR event that triggered the webhook (e.g. 'opened')\"\n+        )\n+        number: int = SchemaField(description=\"The number of the affected pull request\")\n+        pull_request: dict = SchemaField(\n+            description=\"Object representing the affected pull request\"\n+        )\n+        pull_request_url: str = SchemaField(\n+            description=\"The URL of the affected pull request\"\n+        )\n+\n+    def __init__(self):\n+        from backend.integrations.webhooks.github import GithubWebhookType\n+\n+        example_payload = json.loads(self.EXAMPLE_PAYLOAD_FILE.read_text())\n+\n+        super().__init__(\n+            id=\"6c60ec01-8128-419e-988f-96a063ee2fea\",\n+            description=\"This block triggers on pull request events and outputs the event type and payload.\",\n+            categories={BlockCategory.DEVELOPER_TOOLS, BlockCategory.INPUT},\n+            input_schema=GithubPullRequestTriggerBlock.Input,\n+            output_schema=GithubPullRequestTriggerBlock.Output,\n+            # --8<-- [start:example-webhook_config]\n+            webhook_config=BlockWebhookConfig(\n+                provider=\"github\",\n+                webhook_type=GithubWebhookType.REPO,\n+                resource_format=\"{repo}\",\n+                event_filter_input=\"events\",\n+                event_format=\"pull_request.{event}\",\n+            ),\n+            # --8<-- [end:example-webhook_config]\n+            test_input={\n+                \"repo\": \"Significant-Gravitas/AutoGPT\",\n+                \"events\": {\"opened\": True, \"synchronize\": True},\n+                \"credentials\": TEST_CREDENTIALS_INPUT,\n+                \"payload\": example_payload,\n+            },\n+            test_credentials=TEST_CREDENTIALS,\n+            test_output=[\n+                (\"payload\", example_payload),\n+                (\"triggered_by_user\", example_payload[\"sender\"]),\n+                (\"event\", example_payload[\"action\"]),\n+                (\"number\", example_payload[\"number\"]),\n+                (\"pull_request\", example_payload[\"pull_request\"]),\n+                (\"pull_request_url\", example_payload[\"pull_request\"][\"html_url\"]),\n+            ],\n+        )\n+\n+    def run(self, input_data: Input, **kwargs) -> BlockOutput:  # type: ignore\n+        yield from super().run(input_data, **kwargs)\n+        yield \"event\", input_data.payload[\"action\"]\n+        yield \"number\", input_data.payload[\"number\"]\n+        yield \"pull_request\", input_data.payload[\"pull_request\"]\n+        yield \"pull_request_url\", input_data.payload[\"pull_request\"][\"html_url\"]\n+\n+\n+# --8<-- [end:GithubTriggerExample]\ndiff --git a/autogpt_platform/backend/backend/data/block.py b/autogpt_platform/backend/backend/data/block.py\nindex f86eee08429f..14108d71bf1c 100644\n--- a/autogpt_platform/backend/backend/data/block.py\n+++ b/autogpt_platform/backend/backend/data/block.py\n@@ -20,9 +20,12 @@\n from pydantic import BaseModel\n \n from backend.util import json\n+from backend.util.settings import Config\n \n from .model import CREDENTIALS_FIELD_NAME, ContributorDetails, CredentialsMetaInput\n \n+app_config = Config()\n+\n BlockData = tuple[str, Any]  # Input & Output data should be a tuple of (name, data).\n BlockInput = dict[str, Any]  # Input: 1 input pin consumes 1 data.\n BlockOutput = Generator[BlockData, None, None]  # Output: 1 output pin produces n data.\n@@ -34,6 +37,7 @@ class BlockType(Enum):\n     INPUT = \"Input\"\n     OUTPUT = \"Output\"\n     NOTE = \"Note\"\n+    WEBHOOK = \"Webhook\"\n     AGENT = \"Agent\"\n \n \n@@ -177,6 +181,41 @@ class EmptySchema(BlockSchema):\n     pass\n \n \n+# --8<-- [start:BlockWebhookConfig]\n+class BlockWebhookConfig(BaseModel):\n+    provider: str\n+    \"\"\"The service provider that the webhook connects to\"\"\"\n+\n+    webhook_type: str\n+    \"\"\"\n+    Identifier for the webhook type. E.g. GitHub has repo and organization level hooks.\n+\n+    Only for use in the corresponding `WebhooksManager`.\n+    \"\"\"\n+\n+    resource_format: str\n+    \"\"\"\n+    Template string for the resource that a block instance subscribes to.\n+    Fields will be filled from the block's inputs (except `payload`).\n+\n+    Example: `f\"{repo}/pull_requests\"` (note: not how it's actually implemented)\n+\n+    Only for use in the corresponding `WebhooksManager`.\n+    \"\"\"\n+\n+    event_filter_input: str\n+    \"\"\"Name of the block's event filter input.\"\"\"\n+\n+    event_format: str = \"{event}\"\n+    \"\"\"\n+    Template string for the event(s) that a block instance subscribes to.\n+    Applied individually to each event selected in the event filter input.\n+\n+    Example: `\"pull_request.{event}\"` -> `\"pull_request.opened\"`\n+    \"\"\"\n+    # --8<-- [end:BlockWebhookConfig]\n+\n+\n class Block(ABC, Generic[BlockSchemaInputType, BlockSchemaOutputType]):\n     def __init__(\n         self,\n@@ -193,6 +232,7 @@ def __init__(\n         disabled: bool = False,\n         static_output: bool = False,\n         block_type: BlockType = BlockType.STANDARD,\n+        webhook_config: Optional[BlockWebhookConfig] = None,\n     ):\n         \"\"\"\n         Initialize the block with the given schema.\n@@ -223,9 +263,38 @@ def __init__(\n         self.contributors = contributors or set()\n         self.disabled = disabled\n         self.static_output = static_output\n-        self.block_type = block_type\n+        self.block_type = block_type if not webhook_config else BlockType.WEBHOOK\n+        self.webhook_config = webhook_config\n         self.execution_stats = {}\n \n+        if self.webhook_config:\n+            # Enforce shape of webhook event filter\n+            event_filter_field = self.input_schema.model_fields[\n+                self.webhook_config.event_filter_input\n+            ]\n+            if not (\n+                isinstance(event_filter_field.annotation, type)\n+                and issubclass(event_filter_field.annotation, BaseModel)\n+                and all(\n+                    field.annotation is bool\n+                    for field in event_filter_field.annotation.model_fields.values()\n+                )\n+            ):\n+                raise NotImplementedError(\n+                    f\"{self.name} has an invalid webhook event selector: \"\n+                    \"field must be a BaseModel and all its fields must be boolean\"\n+                )\n+\n+            # Enforce presence of 'payload' input\n+            if \"payload\" not in self.input_schema.model_fields:\n+                raise TypeError(\n+                    f\"{self.name} is webhook-triggered but has no 'payload' input\"\n+                )\n+\n+            # Disable webhook-triggered block if webhook functionality not available\n+            if not app_config.platform_base_url:\n+                self.disabled = True\n+\n     @classmethod\n     def create(cls: Type[\"Block\"]) -> \"Block\":\n         return cls()\ndiff --git a/autogpt_platform/backend/backend/data/execution.py b/autogpt_platform/backend/backend/data/execution.py\nindex 4fa1f567fef5..dc28b48837f7 100644\n--- a/autogpt_platform/backend/backend/data/execution.py\n+++ b/autogpt_platform/backend/backend/data/execution.py\n@@ -1,7 +1,7 @@\n from collections import defaultdict\n from datetime import datetime, timezone\n from multiprocessing import Manager\n-from typing import Any, Generic, TypeVar\n+from typing import Any, AsyncGenerator, Generator, Generic, TypeVar\n \n from prisma.enums import AgentExecutionStatus\n from prisma.models import (\n@@ -14,7 +14,9 @@\n \n from backend.data.block import BlockData, BlockInput, CompletedBlockOutput\n from backend.data.includes import EXECUTION_RESULT_INCLUDE, GRAPH_EXECUTION_INCLUDE\n+from backend.data.queue import AsyncRedisEventBus, RedisEventBus\n from backend.util import json, mock\n+from backend.util.settings import Config\n \n \n class GraphExecution(BaseModel):\n@@ -271,7 +273,6 @@ async def update_graph_execution_stats(\n     graph_exec_id: str,\n     stats: dict[str, Any],\n ) -> ExecutionResult:\n-\n     status = ExecutionStatus.FAILED if stats.get(\"error\") else ExecutionStatus.COMPLETED\n     res = await AgentGraphExecution.prisma().update(\n         where={\"id\": graph_exec_id},\n@@ -471,3 +472,42 @@ async def get_incomplete_executions(\n         include=EXECUTION_RESULT_INCLUDE,\n     )\n     return [ExecutionResult.from_db(execution) for execution in executions]\n+\n+\n+# --------------------- Event Bus --------------------- #\n+\n+config = Config()\n+\n+\n+class RedisExecutionEventBus(RedisEventBus[ExecutionResult]):\n+    Model = ExecutionResult\n+\n+    @property\n+    def event_bus_name(self) -> str:\n+        return config.execution_event_bus_name\n+\n+    def publish(self, res: ExecutionResult):\n+        self.publish_event(res, f\"{res.graph_id}/{res.graph_exec_id}\")\n+\n+    def listen(\n+        self, graph_id: str = \"*\", graph_exec_id: str = \"*\"\n+    ) -> Generator[ExecutionResult, None, None]:\n+        for execution_result in self.listen_events(f\"{graph_id}/{graph_exec_id}\"):\n+            yield execution_result\n+\n+\n+class AsyncRedisExecutionEventBus(AsyncRedisEventBus[ExecutionResult]):\n+    Model = ExecutionResult\n+\n+    @property\n+    def event_bus_name(self) -> str:\n+        return config.execution_event_bus_name\n+\n+    async def publish(self, res: ExecutionResult):\n+        await self.publish_event(res, f\"{res.graph_id}/{res.graph_exec_id}\")\n+\n+    async def listen(\n+        self, graph_id: str = \"*\", graph_exec_id: str = \"*\"\n+    ) -> AsyncGenerator[ExecutionResult, None]:\n+        async for execution_result in self.listen_events(f\"{graph_id}/{graph_exec_id}\"):\n+            yield execution_result\ndiff --git a/autogpt_platform/backend/backend/data/graph.py b/autogpt_platform/backend/backend/data/graph.py\nindex cc253af59fd2..1081d6b90e7c 100644\n--- a/autogpt_platform/backend/backend/data/graph.py\n+++ b/autogpt_platform/backend/backend/data/graph.py\n@@ -3,7 +3,7 @@\n import uuid\n from collections import defaultdict\n from datetime import datetime, timezone\n-from typing import Any, Literal, Type\n+from typing import Any, Literal, Optional, Type\n \n import prisma\n from prisma.models import AgentGraph, AgentGraphExecution, AgentNode, AgentNodeLink\n@@ -12,12 +12,14 @@\n \n from backend.blocks.agent import AgentExecutorBlock\n from backend.blocks.basic import AgentInputBlock, AgentOutputBlock\n-from backend.data.block import BlockInput, BlockType, get_block, get_blocks\n-from backend.data.db import BaseDbModel, transaction\n-from backend.data.execution import ExecutionStatus\n-from backend.data.includes import AGENT_GRAPH_INCLUDE, AGENT_NODE_INCLUDE\n from backend.util import json\n \n+from .block import BlockInput, BlockType, get_block, get_blocks\n+from .db import BaseDbModel, transaction\n+from .execution import ExecutionStatus\n+from .includes import AGENT_GRAPH_INCLUDE, AGENT_NODE_INCLUDE\n+from .integrations import Webhook\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -50,20 +52,51 @@ class Node(BaseDbModel):\n     input_links: list[Link] = []\n     output_links: list[Link] = []\n \n+    webhook_id: Optional[str] = None\n+\n+\n+class NodeModel(Node):\n+    graph_id: str\n+    graph_version: int\n+\n+    webhook: Optional[Webhook] = None\n+\n     @staticmethod\n     def from_db(node: AgentNode):\n         if not node.AgentBlock:\n             raise ValueError(f\"Invalid node {node.id}, invalid AgentBlock.\")\n-        obj = Node(\n+        obj = NodeModel(\n             id=node.id,\n             block_id=node.AgentBlock.id,\n             input_default=json.loads(node.constantInput, target_type=dict[str, Any]),\n             metadata=json.loads(node.metadata, target_type=dict[str, Any]),\n+            graph_id=node.agentGraphId,\n+            graph_version=node.agentGraphVersion,\n+            webhook_id=node.webhookId,\n+            webhook=Webhook.from_db(node.Webhook) if node.Webhook else None,\n         )\n         obj.input_links = [Link.from_db(link) for link in node.Input or []]\n         obj.output_links = [Link.from_db(link) for link in node.Output or []]\n         return obj\n \n+    def is_triggered_by_event_type(self, event_type: str) -> bool:\n+        if not (block := get_block(self.block_id)):\n+            raise ValueError(f\"Block #{self.block_id} not found for node #{self.id}\")\n+        if not block.webhook_config:\n+            raise TypeError(\"This method can't be used on non-webhook blocks\")\n+        event_filter = self.input_default.get(block.webhook_config.event_filter_input)\n+        if not event_filter:\n+            raise ValueError(f\"Event filter is not configured on node #{self.id}\")\n+        return event_type in [\n+            block.webhook_config.event_format.format(event=k)\n+            for k in event_filter\n+            if event_filter[k] is True\n+        ]\n+\n+\n+# Fix 2-way reference Node <-> Webhook\n+Webhook.model_rebuild()\n+\n \n class GraphExecution(BaseDbModel):\n     execution_id: str\n@@ -110,6 +143,34 @@ class Graph(BaseDbModel):\n     nodes: list[Node] = []\n     links: list[Link] = []\n \n+    @computed_field\n+    @property\n+    def input_schema(self) -> dict[str, Any]:\n+        return self._generate_schema(\n+            AgentInputBlock.Input,\n+            [\n+                node.input_default\n+                for node in self.nodes\n+                if (b := get_block(node.block_id))\n+                and b.block_type == BlockType.INPUT\n+                and \"name\" in node.input_default\n+            ],\n+        )\n+\n+    @computed_field\n+    @property\n+    def output_schema(self) -> dict[str, Any]:\n+        return self._generate_schema(\n+            AgentOutputBlock.Input,\n+            [\n+                node.input_default\n+                for node in self.nodes\n+                if (b := get_block(node.block_id))\n+                and b.block_type == BlockType.OUTPUT\n+                and \"name\" in node.input_default\n+            ],\n+        )\n+\n     @staticmethod\n     def _generate_schema(\n         type_class: Type[AgentInputBlock.Input] | Type[AgentOutputBlock.Input],\n@@ -137,33 +198,10 @@ def _generate_schema(\n             \"required\": [p.name for p in props if p.value is None],\n         }\n \n-    @computed_field\n-    @property\n-    def input_schema(self) -> dict[str, Any]:\n-        return self._generate_schema(\n-            AgentInputBlock.Input,\n-            [\n-                node.input_default\n-                for node in self.nodes\n-                if (b := get_block(node.block_id))\n-                and b.block_type == BlockType.INPUT\n-                and \"name\" in node.input_default\n-            ],\n-        )\n \n-    @computed_field\n-    @property\n-    def output_schema(self) -> dict[str, Any]:\n-        return self._generate_schema(\n-            AgentOutputBlock.Input,\n-            [\n-                node.input_default\n-                for node in self.nodes\n-                if (b := get_block(node.block_id))\n-                and b.block_type == BlockType.OUTPUT\n-                and \"name\" in node.input_default\n-            ],\n-        )\n+class GraphModel(Graph):\n+    user_id: str\n+    nodes: list[NodeModel] = []  # type: ignore\n \n     @property\n     def starting_nodes(self) -> list[Node]:\n@@ -291,36 +329,39 @@ def from_db(graph: AgentGraph, hide_credentials: bool = False):\n             GraphExecution.from_db(execution)\n             for execution in graph.AgentGraphExecution or []\n         ]\n-        nodes = graph.AgentNodes or []\n \n-        return Graph(\n+        return GraphModel(\n             id=graph.id,\n+            user_id=graph.userId,\n             version=graph.version,\n             is_active=graph.isActive,\n             is_template=graph.isTemplate,\n             name=graph.name or \"\",\n             description=graph.description or \"\",\n             executions=executions,\n-            nodes=[Graph._process_node(node, hide_credentials) for node in nodes],\n+            nodes=[\n+                GraphModel._process_node(node, hide_credentials)\n+                for node in graph.AgentNodes or []\n+            ],\n             links=list(\n                 {\n                     Link.from_db(link)\n-                    for node in nodes\n+                    for node in graph.AgentNodes or []\n                     for link in (node.Input or []) + (node.Output or [])\n                 }\n             ),\n         )\n \n     @staticmethod\n-    def _process_node(node: AgentNode, hide_credentials: bool) -> Node:\n-        node_dict = node.model_dump()\n+    def _process_node(node: AgentNode, hide_credentials: bool) -> NodeModel:\n+        node_dict = {field: getattr(node, field) for field in node.model_fields}\n         if hide_credentials and \"constantInput\" in node_dict:\n             constant_input = json.loads(\n                 node_dict[\"constantInput\"], target_type=dict[str, Any]\n             )\n-            constant_input = Graph._hide_credentials_in_input(constant_input)\n+            constant_input = GraphModel._hide_credentials_in_input(constant_input)\n             node_dict[\"constantInput\"] = json.dumps(constant_input)\n-        return Node.from_db(AgentNode(**node_dict))\n+        return NodeModel.from_db(AgentNode(**node_dict))\n \n     @staticmethod\n     def _hide_credentials_in_input(input_data: dict[str, Any]) -> dict[str, Any]:\n@@ -328,7 +369,7 @@ def _hide_credentials_in_input(input_data: dict[str, Any]) -> dict[str, Any]:\n         result = {}\n         for key, value in input_data.items():\n             if isinstance(value, dict):\n-                result[key] = Graph._hide_credentials_in_input(value)\n+                result[key] = GraphModel._hide_credentials_in_input(value)\n             elif isinstance(value, str) and any(\n                 sensitive_key in key.lower() for sensitive_key in sensitive_keys\n             ):\n@@ -339,22 +380,37 @@ def _hide_credentials_in_input(input_data: dict[str, Any]) -> dict[str, Any]:\n         return result\n \n \n-# --------------------- Model functions --------------------- #\n+# --------------------- CRUD functions --------------------- #\n \n \n-async def get_node(node_id: str) -> Node:\n+async def get_node(node_id: str) -> NodeModel:\n     node = await AgentNode.prisma().find_unique_or_raise(\n         where={\"id\": node_id},\n         include=AGENT_NODE_INCLUDE,\n     )\n-    return Node.from_db(node)\n+    return NodeModel.from_db(node)\n+\n+\n+async def set_node_webhook(node_id: str, webhook_id: str | None) -> NodeModel:\n+    node = await AgentNode.prisma().update(\n+        where={\"id\": node_id},\n+        data=(\n+            {\"Webhook\": {\"connect\": {\"id\": webhook_id}}}\n+            if webhook_id\n+            else {\"Webhook\": {\"disconnect\": True}}\n+        ),\n+        include=AGENT_NODE_INCLUDE,\n+    )\n+    if not node:\n+        raise ValueError(f\"Node #{node_id} not found\")\n+    return NodeModel.from_db(node)\n \n \n async def get_graphs(\n     user_id: str,\n     include_executions: bool = False,\n     filter_by: Literal[\"active\", \"template\"] | None = \"active\",\n-) -> list[Graph]:\n+) -> list[GraphModel]:\n     \"\"\"\n     Retrieves graph metadata objects.\n     Default behaviour is to get all currently active graphs.\n@@ -365,7 +421,7 @@ async def get_graphs(\n         user_id: The ID of the user that owns the graph.\n \n     Returns:\n-        list[Graph]: A list of objects representing the retrieved graph metadata.\n+        list[GraphModel]: A list of objects representing the retrieved graphs.\n     \"\"\"\n     where_clause: AgentGraphWhereInput = {}\n \n@@ -386,7 +442,7 @@ async def get_graphs(\n         include=graph_include,\n     )\n \n-    return [Graph.from_db(graph) for graph in graphs]\n+    return [GraphModel.from_db(graph) for graph in graphs]\n \n \n async def get_graph(\n@@ -395,7 +451,7 @@ async def get_graph(\n     template: bool = False,\n     user_id: str | None = None,\n     hide_credentials: bool = False,\n-) -> Graph | None:\n+) -> GraphModel | None:\n     \"\"\"\n     Retrieves a graph from the DB.\n     Defaults to the version with `is_active` if `version` is not passed,\n@@ -420,38 +476,35 @@ async def get_graph(\n         include=AGENT_GRAPH_INCLUDE,\n         order={\"version\": \"desc\"},\n     )\n-    return Graph.from_db(graph, hide_credentials) if graph else None\n+    return GraphModel.from_db(graph, hide_credentials) if graph else None\n \n \n async def set_graph_active_version(graph_id: str, version: int, user_id: str) -> None:\n-    # Check if the graph belongs to the user\n-    graph = await AgentGraph.prisma().find_first(\n+    # Activate the requested version if it exists and is owned by the user.\n+    updated_count = await AgentGraph.prisma().update_many(\n+        data={\"isActive\": True},\n         where={\n             \"id\": graph_id,\n             \"version\": version,\n             \"userId\": user_id,\n-        }\n-    )\n-    if not graph:\n-        raise Exception(f\"Graph #{graph_id} v{version} not found or not owned by user\")\n-\n-    updated_graph = await AgentGraph.prisma().update(\n-        data={\"isActive\": True},\n-        where={\n-            \"graphVersionId\": {\"id\": graph_id, \"version\": version},\n         },\n     )\n-    if not updated_graph:\n-        raise Exception(f\"Graph #{graph_id} v{version} not found\")\n+    if updated_count == 0:\n+        raise Exception(f\"Graph #{graph_id} v{version} not found or not owned by user\")\n \n-    # Deactivate all other versions\n+    # Deactivate all other versions.\n     await AgentGraph.prisma().update_many(\n         data={\"isActive\": False},\n-        where={\"id\": graph_id, \"version\": {\"not\": version}, \"userId\": user_id},\n+        where={\n+            \"id\": graph_id,\n+            \"version\": {\"not\": version},\n+            \"userId\": user_id,\n+            \"isActive\": True,\n+        },\n     )\n \n \n-async def get_graph_all_versions(graph_id: str, user_id: str) -> list[Graph]:\n+async def get_graph_all_versions(graph_id: str, user_id: str) -> list[GraphModel]:\n     graph_versions = await AgentGraph.prisma().find_many(\n         where={\"id\": graph_id, \"userId\": user_id},\n         order={\"version\": \"desc\"},\n@@ -461,7 +514,7 @@ async def get_graph_all_versions(graph_id: str, user_id: str) -> list[Graph]:\n     if not graph_versions:\n         return []\n \n-    return [Graph.from_db(graph) for graph in graph_versions]\n+    return [GraphModel.from_db(graph) for graph in graph_versions]\n \n \n async def delete_graph(graph_id: str, user_id: str) -> int:\n@@ -473,7 +526,7 @@ async def delete_graph(graph_id: str, user_id: str) -> int:\n     return entries_count\n \n \n-async def create_graph(graph: Graph, user_id: str) -> Graph:\n+async def create_graph(graph: Graph, user_id: str) -> GraphModel:\n     async with transaction() as tx:\n         await __create_graph(tx, graph, user_id)\n \n@@ -534,6 +587,32 @@ async def __create_graph(tx, graph: Graph, user_id: str):\n # ------------------------ UTILITIES ------------------------ #\n \n \n+def make_graph_model(creatable_graph: Graph, user_id: str) -> GraphModel:\n+    \"\"\"\n+    Convert a Graph to a GraphModel, setting graph_id and graph_version on all nodes.\n+\n+    Args:\n+        creatable_graph (Graph): The creatable graph to convert.\n+        user_id (str): The ID of the user creating the graph.\n+\n+    Returns:\n+        GraphModel: The converted Graph object.\n+    \"\"\"\n+    # Create a new Graph object, inheriting properties from CreatableGraph\n+    return GraphModel(\n+        **creatable_graph.model_dump(exclude={\"nodes\"}),\n+        user_id=user_id,\n+        nodes=[\n+            NodeModel(\n+                **creatable_node.model_dump(),\n+                graph_id=creatable_graph.id,\n+                graph_version=creatable_graph.version,\n+            )\n+            for creatable_node in creatable_graph.nodes\n+        ],\n+    )\n+\n+\n async def fix_llm_provider_credentials():\n     \"\"\"Fix node credentials with provider `llm`\"\"\"\n     from autogpt_libs.supabase_integration_credentials_store import (\ndiff --git a/autogpt_platform/backend/backend/data/includes.py b/autogpt_platform/backend/backend/data/includes.py\nindex 371d87ec5d24..0b791f502a1e 100644\n--- a/autogpt_platform/backend/backend/data/includes.py\n+++ b/autogpt_platform/backend/backend/data/includes.py\n@@ -3,6 +3,7 @@\n AGENT_NODE_INCLUDE: prisma.types.AgentNodeInclude = {\n     \"Input\": True,\n     \"Output\": True,\n+    \"Webhook\": True,\n     \"AgentBlock\": True,\n }\n \n@@ -27,3 +28,7 @@\n         }\n     }\n }\n+\n+INTEGRATION_WEBHOOK_INCLUDE: prisma.types.IntegrationWebhookInclude = {\n+    \"AgentNodes\": {\"include\": AGENT_NODE_INCLUDE}  # type: ignore\n+}\ndiff --git a/autogpt_platform/backend/backend/data/integrations.py b/autogpt_platform/backend/backend/data/integrations.py\nnew file mode 100644\nindex 000000000000..f86ecd3a4e49\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/data/integrations.py\n@@ -0,0 +1,168 @@\n+import logging\n+from typing import TYPE_CHECKING, AsyncGenerator, Optional\n+\n+from prisma import Json\n+from prisma.models import IntegrationWebhook\n+from pydantic import Field\n+\n+from backend.data.includes import INTEGRATION_WEBHOOK_INCLUDE\n+from backend.data.queue import AsyncRedisEventBus\n+\n+from .db import BaseDbModel\n+\n+if TYPE_CHECKING:\n+    from .graph import NodeModel\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class Webhook(BaseDbModel):\n+    user_id: str\n+    provider: str\n+    credentials_id: str\n+    webhook_type: str\n+    resource: str\n+    events: list[str]\n+    config: dict = Field(default_factory=dict)\n+    secret: str\n+\n+    provider_webhook_id: str\n+\n+    attached_nodes: Optional[list[\"NodeModel\"]] = None\n+\n+    @staticmethod\n+    def from_db(webhook: IntegrationWebhook):\n+        from .graph import NodeModel\n+\n+        return Webhook(\n+            id=webhook.id,\n+            user_id=webhook.userId,\n+            provider=webhook.provider,\n+            credentials_id=webhook.credentialsId,\n+            webhook_type=webhook.webhookType,\n+            resource=webhook.resource,\n+            events=webhook.events,\n+            config=dict(webhook.config),\n+            secret=webhook.secret,\n+            provider_webhook_id=webhook.providerWebhookId,\n+            attached_nodes=(\n+                [NodeModel.from_db(node) for node in webhook.AgentNodes]\n+                if webhook.AgentNodes is not None\n+                else None\n+            ),\n+        )\n+\n+\n+# --------------------- CRUD functions --------------------- #\n+\n+\n+async def create_webhook(webhook: Webhook) -> Webhook:\n+    created_webhook = await IntegrationWebhook.prisma().create(\n+        data={\n+            \"id\": webhook.id,\n+            \"userId\": webhook.user_id,\n+            \"provider\": webhook.provider,\n+            \"credentialsId\": webhook.credentials_id,\n+            \"webhookType\": webhook.webhook_type,\n+            \"resource\": webhook.resource,\n+            \"events\": webhook.events,\n+            \"config\": Json(webhook.config),\n+            \"secret\": webhook.secret,\n+            \"providerWebhookId\": webhook.provider_webhook_id,\n+        }\n+    )\n+    return Webhook.from_db(created_webhook)\n+\n+\n+async def get_webhook(webhook_id: str) -> Webhook:\n+    \"\"\" No `user_id` check: DO NOT USE without check in user-facing endpoints.\"\"\"\n+    webhook = await IntegrationWebhook.prisma().find_unique_or_raise(\n+        where={\"id\": webhook_id},\n+        include=INTEGRATION_WEBHOOK_INCLUDE,\n+    )\n+    return Webhook.from_db(webhook)\n+\n+\n+async def get_all_webhooks(credentials_id: str) -> list[Webhook]:\n+    \"\"\" No `user_id` check: DO NOT USE without check in user-facing endpoints.\"\"\"\n+    webhooks = await IntegrationWebhook.prisma().find_many(\n+        where={\"credentialsId\": credentials_id},\n+        include=INTEGRATION_WEBHOOK_INCLUDE,\n+    )\n+    return [Webhook.from_db(webhook) for webhook in webhooks]\n+\n+\n+async def find_webhook(\n+    credentials_id: str, webhook_type: str, resource: str, events: list[str]\n+) -> Webhook | None:\n+    \"\"\" No `user_id` check: DO NOT USE without check in user-facing endpoints.\"\"\"\n+    webhook = await IntegrationWebhook.prisma().find_first(\n+        where={\n+            \"credentialsId\": credentials_id,\n+            \"webhookType\": webhook_type,\n+            \"resource\": resource,\n+            \"events\": {\"has_every\": events},\n+        },\n+        include=INTEGRATION_WEBHOOK_INCLUDE,\n+    )\n+    return Webhook.from_db(webhook) if webhook else None\n+\n+\n+async def update_webhook_config(webhook_id: str, updated_config: dict) -> Webhook:\n+    \"\"\" No `user_id` check: DO NOT USE without check in user-facing endpoints.\"\"\"\n+    _updated_webhook = await IntegrationWebhook.prisma().update(\n+        where={\"id\": webhook_id},\n+        data={\"config\": Json(updated_config)},\n+        include=INTEGRATION_WEBHOOK_INCLUDE,\n+    )\n+    if _updated_webhook is None:\n+        raise ValueError(f\"Webhook #{webhook_id} not found\")\n+    return Webhook.from_db(_updated_webhook)\n+\n+\n+async def delete_webhook(webhook_id: str) -> None:\n+    \"\"\" No `user_id` check: DO NOT USE without check in user-facing endpoints.\"\"\"\n+    deleted = await IntegrationWebhook.prisma().delete(where={\"id\": webhook_id})\n+    if not deleted:\n+        raise ValueError(f\"Webhook #{webhook_id} not found\")\n+\n+\n+# --------------------- WEBHOOK EVENTS --------------------- #\n+\n+\n+class WebhookEvent(BaseDbModel):\n+    provider: str\n+    webhook_id: str\n+    event_type: str\n+    payload: dict\n+\n+\n+class WebhookEventBus(AsyncRedisEventBus[WebhookEvent]):\n+    Model = WebhookEvent\n+\n+    @property\n+    def event_bus_name(self) -> str:\n+        return \"webhooks\"\n+\n+    async def publish(self, event: WebhookEvent):\n+        await self.publish_event(event, f\"{event.webhook_id}/{event.event_type}\")\n+\n+    async def listen(\n+        self, webhook_id: str, event_type: Optional[str] = None\n+    ) -> AsyncGenerator[WebhookEvent, None]:\n+        async for event in self.listen_events(f\"{webhook_id}/{event_type or '*'}\"):\n+            yield event\n+\n+\n+event_bus = WebhookEventBus()\n+\n+\n+async def publish_webhook_event(event: WebhookEvent):\n+    await event_bus.publish(event)\n+\n+\n+async def listen_for_webhook_event(\n+    webhook_id: str, event_type: Optional[str] = None\n+) -> WebhookEvent | None:\n+    async for event in event_bus.listen(webhook_id, event_type):\n+        return event  # Only one event is expected\ndiff --git a/autogpt_platform/backend/backend/data/model.py b/autogpt_platform/backend/backend/data/model.py\nindex 4068bdcfa236..9a988133eb59 100644\n--- a/autogpt_platform/backend/backend/data/model.py\n+++ b/autogpt_platform/backend/backend/data/model.py\n@@ -113,6 +113,7 @@ def SchemaField(\n     advanced: Optional[bool] = None,\n     secret: bool = False,\n     exclude: bool = False,\n+    hidden: Optional[bool] = None,\n     **kwargs,\n ) -> T:\n     json_extra = {\n@@ -121,6 +122,7 @@ def SchemaField(\n             \"placeholder\": placeholder,\n             \"secret\": secret,\n             \"advanced\": advanced,\n+            \"hidden\": hidden,\n         }.items()\n         if v is not None\n     }\ndiff --git a/autogpt_platform/backend/backend/data/queue.py b/autogpt_platform/backend/backend/data/queue.py\nindex 3b3db57ecd66..b6fa72d53046 100644\n--- a/autogpt_platform/backend/backend/data/queue.py\n+++ b/autogpt_platform/backend/backend/data/queue.py\n@@ -9,11 +9,8 @@\n from redis.client import PubSub\n \n from backend.data import redis\n-from backend.data.execution import ExecutionResult\n-from backend.util.settings import Config\n \n logger = logging.getLogger(__name__)\n-config = Config()\n \n \n class DateTimeEncoder(json.JSONEncoder):\n@@ -36,7 +33,7 @@ def event_bus_name(self) -> str:\n \n     def _serialize_message(self, item: M, channel_key: str) -> tuple[str, str]:\n         message = json.dumps(item.model_dump(), cls=DateTimeEncoder)\n-        channel_name = f\"{self.event_bus_name}-{channel_key}\"\n+        channel_name = f\"{self.event_bus_name}/{channel_key}\"\n         logger.info(f\"[{channel_name}] Publishing an event to Redis {message}\")\n         return message, channel_name\n \n@@ -54,7 +51,7 @@ def _deserialize_message(self, msg: Any, channel_key: str) -> M | None:\n     def _subscribe(\n         self, connection: redis.Redis | redis.AsyncRedis, channel_key: str\n     ) -> tuple[PubSub | AsyncPubSub, str]:\n-        channel_name = f\"{self.event_bus_name}-{channel_key}\"\n+        channel_name = f\"{self.event_bus_name}/{channel_key}\"\n         pubsub = connection.pubsub()\n         return pubsub, channel_name\n \n@@ -108,37 +105,3 @@ async def listen_events(self, channel_key: str) -> AsyncGenerator[M, None]:\n         async for message in pubsub.listen():\n             if event := self._deserialize_message(message, channel_key):\n                 yield event\n-\n-\n-class RedisExecutionEventBus(RedisEventBus[ExecutionResult]):\n-    Model = ExecutionResult\n-\n-    @property\n-    def event_bus_name(self) -> str:\n-        return config.execution_event_bus_name\n-\n-    def publish(self, res: ExecutionResult):\n-        self.publish_event(res, f\"{res.graph_id}-{res.graph_exec_id}\")\n-\n-    def listen(\n-        self, graph_id: str = \"*\", graph_exec_id: str = \"*\"\n-    ) -> Generator[ExecutionResult, None, None]:\n-        for execution_result in self.listen_events(f\"{graph_id}-{graph_exec_id}\"):\n-            yield execution_result\n-\n-\n-class AsyncRedisExecutionEventBus(AsyncRedisEventBus[ExecutionResult]):\n-    Model = ExecutionResult\n-\n-    @property\n-    def event_bus_name(self) -> str:\n-        return config.execution_event_bus_name\n-\n-    async def publish(self, res: ExecutionResult):\n-        await self.publish_event(res, f\"{res.graph_id}-{res.graph_exec_id}\")\n-\n-    async def listen(\n-        self, graph_id: str = \"*\", graph_exec_id: str = \"*\"\n-    ) -> AsyncGenerator[ExecutionResult, None]:\n-        async for execution_result in self.listen_events(f\"{graph_id}-{graph_exec_id}\"):\n-            yield execution_result\ndiff --git a/autogpt_platform/backend/backend/executor/database.py b/autogpt_platform/backend/backend/executor/database.py\nindex 2597429b3ab6..4016363c1ab8 100644\n--- a/autogpt_platform/backend/backend/executor/database.py\n+++ b/autogpt_platform/backend/backend/executor/database.py\n@@ -4,6 +4,7 @@\n from backend.data.credit import get_user_credit_model\n from backend.data.execution import (\n     ExecutionResult,\n+    RedisExecutionEventBus,\n     create_graph_execution,\n     get_execution_results,\n     get_incomplete_executions,\n@@ -15,14 +16,13 @@\n     upsert_execution_output,\n )\n from backend.data.graph import get_graph, get_node\n-from backend.data.queue import RedisExecutionEventBus\n from backend.data.user import (\n     get_user_integrations,\n     get_user_metadata,\n     update_user_integrations,\n     update_user_metadata,\n )\n-from backend.util.service import AppService, expose\n+from backend.util.service import AppService, expose, register_pydantic_serializers\n from backend.util.settings import Config\n \n P = ParamSpec(\"P\")\n@@ -56,6 +56,9 @@ def wrapper(self, *args: P.args, **kwargs: P.kwargs) -> R:\n             res = self.run_and_wait(coroutine)\n             return res\n \n+        # Register serializers for annotations on bare function\n+        register_pydantic_serializers(f)\n+\n         return wrapper\n \n     # Executions\ndiff --git a/autogpt_platform/backend/backend/executor/manager.py b/autogpt_platform/backend/backend/executor/manager.py\nindex 2d1e2d223f59..46cb554db8ab 100644\n--- a/autogpt_platform/backend/backend/executor/manager.py\n+++ b/autogpt_platform/backend/backend/executor/manager.py\n@@ -30,7 +30,7 @@\n     merge_execution_input,\n     parse_execution_output,\n )\n-from backend.data.graph import Graph, Link, Node\n+from backend.data.graph import GraphModel, Link, Node\n from backend.data.model import CREDENTIALS_FIELD_NAME, CredentialsMetaInput\n from backend.integrations.creds_manager import IntegrationCredentialsManager\n from backend.util import json\n@@ -186,7 +186,7 @@ def update_execution(status: ExecutionStatus) -> ExecutionResult:\n             input_data, **extra_exec_kwargs\n         ):\n             output_size += len(json.dumps(output_data))\n-            log_metadata.info(\"Node produced output\", output_name=output_data)\n+            log_metadata.info(\"Node produced output\", **{output_name: output_data})\n             db_client.upsert_execution_output(node_exec_id, output_name, output_data)\n \n             for execution in _enqueue_next_nodes(\n@@ -253,7 +253,6 @@ def _enqueue_next_nodes(\n     graph_id: str,\n     log_metadata: LogMetadata,\n ) -> list[NodeExecution]:\n-\n     def add_enqueued_execution(\n         node_exec_id: str, node_id: str, data: BlockInput\n     ) -> NodeExecution:\n@@ -713,7 +712,6 @@ def callback(result: object):\n \n \n class ExecutionManager(AppService):\n-\n     def __init__(self):\n         super().__init__()\n         self.use_redis = True\n@@ -775,7 +773,7 @@ def add_execution(\n         user_id: str,\n         graph_version: int | None = None,\n     ) -> GraphExecution:\n-        graph: Graph | None = self.db_client.get_graph(\n+        graph: GraphModel | None = self.db_client.get_graph(\n             graph_id=graph_id, user_id=user_id, version=graph_version\n         )\n         if not graph:\n@@ -799,6 +797,15 @@ def add_execution(\n                 if name and name in data:\n                     input_data = {\"value\": data[name]}\n \n+            # Extract webhook payload, and assign it to the input pin\n+            webhook_payload_key = f\"webhook_{node.webhook_id}_payload\"\n+            if (\n+                block.block_type == BlockType.WEBHOOK\n+                and node.webhook_id\n+                and webhook_payload_key in data\n+            ):\n+                input_data = {\"payload\": data[webhook_payload_key]}\n+\n             input_data, error = validate_exec(node, input_data)\n             if input_data is None:\n                 raise ValueError(error)\n@@ -876,7 +883,7 @@ def cancel_execution(self, graph_exec_id: str) -> None:\n                 )\n                 self.db_client.send_execution_update(exec_update)\n \n-    def _validate_node_input_credentials(self, graph: Graph, user_id: str):\n+    def _validate_node_input_credentials(self, graph: GraphModel, user_id: str):\n         \"\"\"Checks all credentials for all nodes of the graph\"\"\"\n \n         for node in graph.nodes:\ndiff --git a/autogpt_platform/backend/backend/integrations/creds_manager.py b/autogpt_platform/backend/backend/integrations/creds_manager.py\nindex 96f9d1a3c56d..0fee2e3a8104 100644\n--- a/autogpt_platform/backend/backend/integrations/creds_manager.py\n+++ b/autogpt_platform/backend/backend/integrations/creds_manager.py\n@@ -11,6 +11,7 @@\n \n from backend.data import redis\n from backend.integrations.oauth import HANDLERS_BY_NAME, BaseOAuthHandler\n+from backend.util.exceptions import MissingConfigError\n from backend.util.settings import Settings\n \n logger = logging.getLogger(__name__)\n@@ -157,12 +158,14 @@ def _get_provider_oauth_handler(provider_name: str) -> BaseOAuthHandler:\n     client_id = getattr(settings.secrets, f\"{provider_name}_client_id\")\n     client_secret = getattr(settings.secrets, f\"{provider_name}_client_secret\")\n     if not (client_id and client_secret):\n-        raise Exception(  # TODO: ConfigError\n+        raise MissingConfigError(\n             f\"Integration with provider '{provider_name}' is not configured\",\n         )\n \n     handler_class = HANDLERS_BY_NAME[provider_name]\n-    frontend_base_url = settings.config.frontend_base_url\n+    frontend_base_url = (\n+        settings.config.frontend_base_url or settings.config.platform_base_url\n+    )\n     return handler_class(\n         client_id=client_id,\n         client_secret=client_secret,\ndiff --git a/autogpt_platform/backend/backend/integrations/providers.py b/autogpt_platform/backend/backend/integrations/providers.py\nnew file mode 100644\nindex 000000000000..b38becf5ca4e\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/integrations/providers.py\n@@ -0,0 +1,7 @@\n+from enum import Enum\n+\n+\n+class ProviderName(str, Enum):\n+    GITHUB = \"github\"\n+    GOOGLE = \"google\"\n+    NOTION = \"notion\"\ndiff --git a/autogpt_platform/backend/backend/integrations/webhooks/__init__.py b/autogpt_platform/backend/backend/integrations/webhooks/__init__.py\nnew file mode 100644\nindex 000000000000..14d1f7216567\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/integrations/webhooks/__init__.py\n@@ -0,0 +1,17 @@\n+from typing import TYPE_CHECKING\n+\n+from .github import GithubWebhooksManager\n+\n+if TYPE_CHECKING:\n+    from .base import BaseWebhooksManager\n+\n+# --8<-- [start:WEBHOOK_MANAGERS_BY_NAME]\n+WEBHOOK_MANAGERS_BY_NAME: dict[str, type[\"BaseWebhooksManager\"]] = {\n+    handler.PROVIDER_NAME: handler\n+    for handler in [\n+        GithubWebhooksManager,\n+    ]\n+}\n+# --8<-- [end:WEBHOOK_MANAGERS_BY_NAME]\n+\n+__all__ = [\"WEBHOOK_MANAGERS_BY_NAME\"]\ndiff --git a/autogpt_platform/backend/backend/integrations/webhooks/base.py b/autogpt_platform/backend/backend/integrations/webhooks/base.py\nnew file mode 100644\nindex 000000000000..b30f419a03e8\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/integrations/webhooks/base.py\n@@ -0,0 +1,163 @@\n+import logging\n+import secrets\n+from abc import ABC, abstractmethod\n+from typing import ClassVar, Generic, TypeVar\n+from uuid import uuid4\n+\n+from autogpt_libs.supabase_integration_credentials_store import Credentials\n+from fastapi import Request\n+from strenum import StrEnum\n+\n+from backend.data import integrations\n+from backend.util.exceptions import MissingConfigError\n+from backend.util.settings import Config\n+\n+logger = logging.getLogger(__name__)\n+app_config = Config()\n+\n+WT = TypeVar(\"WT\", bound=StrEnum)\n+\n+\n+class BaseWebhooksManager(ABC, Generic[WT]):\n+    # --8<-- [start:BaseWebhooksManager1]\n+    PROVIDER_NAME: ClassVar[str]\n+    # --8<-- [end:BaseWebhooksManager1]\n+\n+    WebhookType: WT\n+\n+    async def get_suitable_webhook(\n+        self,\n+        user_id: str,\n+        credentials: Credentials,\n+        webhook_type: WT,\n+        resource: str,\n+        events: list[str],\n+    ) -> integrations.Webhook:\n+        if not app_config.platform_base_url:\n+            raise MissingConfigError(\n+                \"PLATFORM_BASE_URL must be set to use Webhook functionality\"\n+            )\n+\n+        if webhook := await integrations.find_webhook(\n+            credentials.id, webhook_type, resource, events\n+        ):\n+            return webhook\n+        return await self._create_webhook(\n+            user_id, credentials, webhook_type, resource, events\n+        )\n+\n+    async def prune_webhook_if_dangling(\n+        self, webhook_id: str, credentials: Credentials\n+    ) -> bool:\n+        webhook = await integrations.get_webhook(webhook_id)\n+        if webhook.attached_nodes is None:\n+            raise ValueError(\"Error retrieving webhook including attached nodes\")\n+        if webhook.attached_nodes:\n+            # Don't prune webhook if in use\n+            return False\n+\n+        await self._deregister_webhook(webhook, credentials)\n+        await integrations.delete_webhook(webhook.id)\n+        return True\n+\n+    # --8<-- [start:BaseWebhooksManager3]\n+    @classmethod\n+    @abstractmethod\n+    async def validate_payload(\n+        cls, webhook: integrations.Webhook, request: Request\n+    ) -> tuple[dict, str]:\n+        \"\"\"\n+        Validates an incoming webhook request and returns its payload and type.\n+\n+        Params:\n+            webhook: Object representing the configured webhook and its properties in our system.\n+            request: Incoming FastAPI `Request`\n+\n+        Returns:\n+            dict: The validated payload\n+            str: The event type associated with the payload\n+        \"\"\"\n+\n+    # --8<-- [end:BaseWebhooksManager3]\n+\n+    # --8<-- [start:BaseWebhooksManager5]\n+    async def trigger_ping(self, webhook: integrations.Webhook) -> None:\n+        \"\"\"\n+        Triggers a ping to the given webhook.\n+\n+        Raises:\n+            NotImplementedError: if the provider doesn't support pinging\n+        \"\"\"\n+        # --8<-- [end:BaseWebhooksManager5]\n+        raise NotImplementedError(f\"{self.__class__.__name__} doesn't support pinging\")\n+\n+    # --8<-- [start:BaseWebhooksManager2]\n+    @abstractmethod\n+    async def _register_webhook(\n+        self,\n+        credentials: Credentials,\n+        webhook_type: WT,\n+        resource: str,\n+        events: list[str],\n+        ingress_url: str,\n+        secret: str,\n+    ) -> tuple[str, dict]:\n+        \"\"\"\n+        Registers a new webhook with the provider.\n+\n+        Params:\n+            credentials: The credentials with which to create the webhook\n+            webhook_type: The provider-specific webhook type to create\n+            resource: The resource to receive events for\n+            events: The events to subscribe to\n+            ingress_url: The ingress URL for webhook payloads\n+            secret: Secret used to verify webhook payloads\n+\n+        Returns:\n+            str: Webhook ID assigned by the provider\n+            config: Provider-specific configuration for the webhook\n+        \"\"\"\n+        ...\n+\n+    # --8<-- [end:BaseWebhooksManager2]\n+\n+    # --8<-- [start:BaseWebhooksManager4]\n+    @abstractmethod\n+    async def _deregister_webhook(\n+        self, webhook: integrations.Webhook, credentials: Credentials\n+    ) -> None: ...\n+\n+    # --8<-- [end:BaseWebhooksManager4]\n+\n+    async def _create_webhook(\n+        self,\n+        user_id: str,\n+        credentials: Credentials,\n+        webhook_type: WT,\n+        resource: str,\n+        events: list[str],\n+    ) -> integrations.Webhook:\n+        id = str(uuid4())\n+        secret = secrets.token_hex(32)\n+        provider_name = self.PROVIDER_NAME\n+        ingress_url = (\n+            f\"{app_config.platform_base_url}/api/integrations/{provider_name}\"\n+            f\"/webhooks/{id}/ingress\"\n+        )\n+        provider_webhook_id, config = await self._register_webhook(\n+            credentials, webhook_type, resource, events, ingress_url, secret\n+        )\n+        return await integrations.create_webhook(\n+            integrations.Webhook(\n+                id=id,\n+                user_id=user_id,\n+                provider=provider_name,\n+                credentials_id=credentials.id,\n+                webhook_type=webhook_type,\n+                resource=resource,\n+                events=events,\n+                provider_webhook_id=provider_webhook_id,\n+                config=config,\n+                secret=secret,\n+            )\n+        )\ndiff --git a/autogpt_platform/backend/backend/integrations/webhooks/github.py b/autogpt_platform/backend/backend/integrations/webhooks/github.py\nnew file mode 100644\nindex 000000000000..25152caff438\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/integrations/webhooks/github.py\n@@ -0,0 +1,175 @@\n+import hashlib\n+import hmac\n+import logging\n+\n+import requests\n+from autogpt_libs.supabase_integration_credentials_store import Credentials\n+from fastapi import HTTPException, Request\n+from strenum import StrEnum\n+\n+from backend.data import integrations\n+\n+from .base import BaseWebhooksManager\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+# --8<-- [start:GithubWebhooksManager]\n+class GithubWebhookType(StrEnum):\n+    REPO = \"repo\"\n+\n+\n+class GithubWebhooksManager(BaseWebhooksManager):\n+    PROVIDER_NAME = \"github\"\n+\n+    WebhookType = GithubWebhookType\n+\n+    GITHUB_API_URL = \"https://api.github.com\"\n+    GITHUB_API_DEFAULT_HEADERS = {\"Accept\": \"application/vnd.github.v3+json\"}\n+\n+    @classmethod\n+    async def validate_payload(\n+        cls, webhook: integrations.Webhook, request: Request\n+    ) -> tuple[dict, str]:\n+        if not (event_type := request.headers.get(\"X-GitHub-Event\")):\n+            raise HTTPException(\n+                status_code=400, detail=\"X-GitHub-Event header is missing!\"\n+            )\n+\n+        if not (signature_header := request.headers.get(\"X-Hub-Signature-256\")):\n+            raise HTTPException(\n+                status_code=403, detail=\"X-Hub-Signature-256 header is missing!\"\n+            )\n+\n+        payload_body = await request.body()\n+        hash_object = hmac.new(\n+            webhook.secret.encode(\"utf-8\"), msg=payload_body, digestmod=hashlib.sha256\n+        )\n+        expected_signature = \"sha256=\" + hash_object.hexdigest()\n+\n+        if not hmac.compare_digest(expected_signature, signature_header):\n+            raise HTTPException(\n+                status_code=403, detail=\"Request signatures didn't match!\"\n+            )\n+\n+        payload = await request.json()\n+        if action := payload.get(\"action\"):\n+            event_type += f\".{action}\"\n+\n+        return payload, event_type\n+\n+    async def trigger_ping(self, webhook: integrations.Webhook) -> None:\n+        headers = {\n+            **self.GITHUB_API_DEFAULT_HEADERS,\n+            \"Authorization\": f\"Bearer {webhook.config.get('access_token')}\",\n+        }\n+\n+        repo, github_hook_id = webhook.resource, webhook.provider_webhook_id\n+        ping_url = f\"{self.GITHUB_API_URL}/repos/{repo}/hooks/{github_hook_id}/pings\"\n+\n+        response = requests.post(ping_url, headers=headers)\n+\n+        if response.status_code != 204:\n+            error_msg = extract_github_error_msg(response)\n+            raise ValueError(f\"Failed to ping GitHub webhook: {error_msg}\")\n+\n+    async def _register_webhook(\n+        self,\n+        credentials: Credentials,\n+        webhook_type: GithubWebhookType,\n+        resource: str,\n+        events: list[str],\n+        ingress_url: str,\n+        secret: str,\n+    ) -> tuple[str, dict]:\n+        if webhook_type == self.WebhookType.REPO and resource.count(\"/\") > 1:\n+            raise ValueError(\"Invalid repo format: expected 'owner/repo'\")\n+\n+        # Extract main event, e.g. `pull_request.opened` -> `pull_request`\n+        github_events = list({event.split(\".\")[0] for event in events})\n+\n+        headers = {\n+            **self.GITHUB_API_DEFAULT_HEADERS,\n+            \"Authorization\": credentials.bearer(),\n+        }\n+        webhook_data = {\n+            \"name\": \"web\",\n+            \"active\": True,\n+            \"events\": github_events,\n+            \"config\": {\n+                \"url\": ingress_url,\n+                \"content_type\": \"json\",\n+                \"insecure_ssl\": \"0\",\n+                \"secret\": secret,\n+            },\n+        }\n+\n+        response = requests.post(\n+            f\"{self.GITHUB_API_URL}/repos/{resource}/hooks\",\n+            headers=headers,\n+            json=webhook_data,\n+        )\n+\n+        if response.status_code != 201:\n+            error_msg = extract_github_error_msg(response)\n+            if \"not found\" in error_msg.lower():\n+                error_msg = (\n+                    f\"{error_msg} \"\n+                    \"(Make sure the GitHub account or API key has 'repo' or \"\n+                    f\"webhook create permissions to '{resource}')\"\n+                )\n+            raise ValueError(f\"Failed to create GitHub webhook: {error_msg}\")\n+\n+        webhook_id = response.json()[\"id\"]\n+        config = response.json()[\"config\"]\n+\n+        return str(webhook_id), config\n+\n+    async def _deregister_webhook(\n+        self, webhook: integrations.Webhook, credentials: Credentials\n+    ) -> None:\n+        webhook_type = self.WebhookType(webhook.webhook_type)\n+        if webhook.credentials_id != credentials.id:\n+            raise ValueError(\n+                f\"Webhook #{webhook.id} does not belong to credentials {credentials.id}\"\n+            )\n+\n+        headers = {\n+            **self.GITHUB_API_DEFAULT_HEADERS,\n+            \"Authorization\": credentials.bearer(),\n+        }\n+\n+        if webhook_type == self.WebhookType.REPO:\n+            repo = webhook.resource\n+            delete_url = f\"{self.GITHUB_API_URL}/repos/{repo}/hooks/{webhook.provider_webhook_id}\"  # noqa\n+        else:\n+            raise NotImplementedError(\n+                f\"Unsupported webhook type '{webhook.webhook_type}'\"\n+            )\n+\n+        response = requests.delete(delete_url, headers=headers)\n+\n+        if response.status_code not in [204, 404]:\n+            # 204 means successful deletion, 404 means the webhook was already deleted\n+            error_msg = extract_github_error_msg(response)\n+            raise ValueError(f\"Failed to delete GitHub webhook: {error_msg}\")\n+\n+        # If we reach here, the webhook was successfully deleted or didn't exist\n+\n+\n+# --8<-- [end:GithubWebhooksManager]\n+\n+\n+def extract_github_error_msg(response: requests.Response) -> str:\n+    error_msgs = []\n+    resp = response.json()\n+    if resp.get(\"message\"):\n+        error_msgs.append(resp[\"message\"])\n+    if resp.get(\"errors\"):\n+        error_msgs.extend(f\"* {err.get('message', err)}\" for err in resp[\"errors\"])\n+    if resp.get(\"error\"):\n+        if isinstance(resp[\"error\"], dict):\n+            error_msgs.append(resp[\"error\"].get(\"message\", resp[\"error\"]))\n+        else:\n+            error_msgs.append(resp[\"error\"])\n+    return \"\\n\".join(error_msgs)\ndiff --git a/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py b/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\nnew file mode 100644\nindex 000000000000..1f6351d5fab2\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\n@@ -0,0 +1,198 @@\n+import logging\n+from typing import TYPE_CHECKING, Callable, Optional, cast\n+\n+from backend.data.block import get_block\n+from backend.data.graph import set_node_webhook\n+from backend.data.model import CREDENTIALS_FIELD_NAME\n+from backend.integrations.webhooks import WEBHOOK_MANAGERS_BY_NAME\n+\n+if TYPE_CHECKING:\n+    from autogpt_libs.supabase_integration_credentials_store.types import Credentials\n+\n+    from backend.data.graph import GraphModel, NodeModel\n+\n+    from .base import BaseWebhooksManager\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+async def on_graph_activate(\n+    graph: \"GraphModel\", get_credentials: Callable[[str], \"Credentials | None\"]\n+):\n+    \"\"\"\n+    Hook to be called when a graph is activated/created.\n+\n+     Assuming node entities are not re-used between graph versions, \n+    this hook calls `on_node_activate` on all nodes in this graph.\n+\n+    Params:\n+        get_credentials: `credentials_id` -> Credentials\n+    \"\"\"\n+    # Compare nodes in new_graph_version with previous_graph_version\n+    updated_nodes = []\n+    for new_node in graph.nodes:\n+        node_credentials = None\n+        if creds_meta := new_node.input_default.get(CREDENTIALS_FIELD_NAME):\n+            node_credentials = get_credentials(creds_meta[\"id\"])\n+            if not node_credentials:\n+                raise ValueError(\n+                    f\"Node #{new_node.id} updated with non-existent \"\n+                    f\"credentials #{node_credentials}\"\n+                )\n+\n+        updated_node = await on_node_activate(\n+            graph.user_id, new_node, credentials=node_credentials\n+        )\n+        updated_nodes.append(updated_node)\n+\n+    graph.nodes = updated_nodes\n+    return graph\n+\n+\n+async def on_graph_deactivate(\n+    graph: \"GraphModel\", get_credentials: Callable[[str], \"Credentials | None\"]\n+):\n+    \"\"\"\n+    Hook to be called when a graph is deactivated/deleted.\n+\n+     Assuming node entities are not re-used between graph versions, \n+    this hook calls `on_node_deactivate` on all nodes in `graph`.\n+\n+    Params:\n+        get_credentials: `credentials_id` -> Credentials\n+    \"\"\"\n+    updated_nodes = []\n+    for node in graph.nodes:\n+        node_credentials = None\n+        if creds_meta := node.input_default.get(CREDENTIALS_FIELD_NAME):\n+            node_credentials = get_credentials(creds_meta[\"id\"])\n+            if not node_credentials:\n+                logger.error(\n+                    f\"Node #{node.id} referenced non-existent \"\n+                    f\"credentials #{creds_meta['id']}\"\n+                )\n+\n+        updated_node = await on_node_deactivate(node, credentials=node_credentials)\n+        updated_nodes.append(updated_node)\n+\n+    graph.nodes = updated_nodes\n+    return graph\n+\n+\n+async def on_node_activate(\n+    user_id: str,\n+    node: \"NodeModel\",\n+    *,\n+    credentials: Optional[\"Credentials\"] = None,\n+) -> \"NodeModel\":\n+    \"\"\"Hook to be called when the node is activated/created\"\"\"\n+\n+    block = get_block(node.block_id)\n+    if not block:\n+        raise ValueError(\n+            f\"Node #{node.id} is instance of unknown block #{node.block_id}\"\n+        )\n+\n+    if not block.webhook_config:\n+        return node\n+\n+    logger.debug(\n+        f\"Activating webhook node #{node.id} with config {block.webhook_config}\"\n+    )\n+\n+    webhooks_manager = WEBHOOK_MANAGERS_BY_NAME[block.webhook_config.provider]()\n+\n+    try:\n+        resource = block.webhook_config.resource_format.format(**node.input_default)\n+    except KeyError:\n+        resource = None\n+    logger.debug(\n+        f\"Constructed resource string {resource} from input {node.input_default}\"\n+    )\n+\n+    event_filter_input_name = block.webhook_config.event_filter_input\n+    has_everything_for_webhook = (\n+        resource is not None\n+        and CREDENTIALS_FIELD_NAME in node.input_default\n+        and event_filter_input_name in node.input_default\n+        and any(is_on for is_on in node.input_default[event_filter_input_name].values())\n+    )\n+\n+    if has_everything_for_webhook and resource:\n+        logger.debug(f\"Node #{node} has everything for a webhook!\")\n+        if not credentials:\n+            credentials_meta = node.input_default[CREDENTIALS_FIELD_NAME]\n+            raise ValueError(\n+                f\"Cannot set up webhook for node #{node.id}: \"\n+                f\"credentials #{credentials_meta['id']} not available\"\n+            )\n+\n+        # Shape of the event filter is enforced in Block.__init__\n+        event_filter = cast(dict, node.input_default[event_filter_input_name])\n+        events = [\n+            block.webhook_config.event_format.format(event=event)\n+            for event, enabled in event_filter.items()\n+            if enabled is True\n+        ]\n+        logger.debug(f\"Webhook events to subscribe to: {', '.join(events)}\")\n+\n+        # Find/make and attach a suitable webhook to the node\n+        new_webhook = await webhooks_manager.get_suitable_webhook(\n+            user_id,\n+            credentials,\n+            block.webhook_config.webhook_type,\n+            resource,\n+            events,\n+        )\n+        logger.debug(f\"Acquired webhook: {new_webhook}\")\n+        return await set_node_webhook(node.id, new_webhook.id)\n+\n+    return node\n+\n+\n+async def on_node_deactivate(\n+    node: \"NodeModel\",\n+    *,\n+    credentials: Optional[\"Credentials\"] = None,\n+    webhooks_manager: Optional[\"BaseWebhooksManager\"] = None,\n+) -> \"NodeModel\":\n+    \"\"\"Hook to be called when node is deactivated/deleted\"\"\"\n+\n+    logger.debug(f\"Deactivating node #{node.id}\")\n+    block = get_block(node.block_id)\n+    if not block:\n+        raise ValueError(\n+            f\"Node #{node.id} is instance of unknown block #{node.block_id}\"\n+        )\n+\n+    if not block.webhook_config:\n+        return node\n+\n+    webhooks_manager = WEBHOOK_MANAGERS_BY_NAME[block.webhook_config.provider]()\n+\n+    if node.webhook_id:\n+        logger.debug(f\"Node #{node.id} has webhook_id {node.webhook_id}\")\n+        if not node.webhook:\n+            logger.error(f\"Node #{node.id} has webhook_id but no webhook object\")\n+            raise ValueError(\"node.webhook not included\")\n+\n+        # Detach webhook from node\n+        logger.debug(f\"Detaching webhook from node #{node.id}\")\n+        updated_node = await set_node_webhook(node.id, None)\n+\n+        # Prune and deregister the webhook if it is no longer used anywhere\n+        logger.debug(\"Pruning and deregistering webhook if dangling\")\n+        webhook = node.webhook\n+        if credentials:\n+            logger.debug(f\"Pruning webhook #{webhook.id} with credentials\")\n+            await webhooks_manager.prune_webhook_if_dangling(webhook.id, credentials)\n+        else:\n+            logger.warning(\n+                f\"Cannot deregister webhook #{webhook.id}: credentials \"\n+                f\"#{webhook.credentials_id} not available \"\n+                f\"({webhook.provider} webhook ID: {webhook.provider_webhook_id})\"\n+            )\n+        return updated_node\n+\n+    logger.debug(f\"Node #{node.id} has no webhook_id, returning\")\n+    return node\ndiff --git a/autogpt_platform/backend/backend/server/integrations/router.py b/autogpt_platform/backend/backend/server/integrations/router.py\nindex 1e3d01e0bfc0..ecf28cedde97 100644\n--- a/autogpt_platform/backend/backend/server/integrations/router.py\n+++ b/autogpt_platform/backend/backend/server/integrations/router.py\n@@ -10,8 +10,20 @@\n from fastapi import APIRouter, Body, Depends, HTTPException, Path, Query, Request\n from pydantic import BaseModel, Field, SecretStr\n \n+from backend.data.graph import set_node_webhook\n+from backend.data.integrations import (\n+    WebhookEvent,\n+    get_all_webhooks,\n+    get_webhook,\n+    listen_for_webhook_event,\n+    publish_webhook_event,\n+)\n+from backend.executor.manager import ExecutionManager\n from backend.integrations.creds_manager import IntegrationCredentialsManager\n from backend.integrations.oauth import HANDLERS_BY_NAME, BaseOAuthHandler\n+from backend.integrations.webhooks import WEBHOOK_MANAGERS_BY_NAME\n+from backend.util.exceptions import NeedConfirmation\n+from backend.util.service import get_service_client\n from backend.util.settings import Settings\n \n from ..utils import get_user_id\n@@ -183,13 +195,22 @@ class CredentialsDeletionResponse(BaseModel):\n     )\n \n \n+class CredentialsDeletionNeedsConfirmationResponse(BaseModel):\n+    deleted: Literal[False] = False\n+    need_confirmation: Literal[True] = True\n+    message: str\n+\n+\n @router.delete(\"/{provider}/credentials/{cred_id}\")\n-def delete_credentials(\n+async def delete_credentials(\n     request: Request,\n     provider: Annotated[str, Path(title=\"The provider to delete credentials for\")],\n     cred_id: Annotated[str, Path(title=\"The ID of the credentials to delete\")],\n     user_id: Annotated[str, Depends(get_user_id)],\n-) -> CredentialsDeletionResponse:\n+    force: Annotated[\n+        bool, Query(title=\"Whether to proceed if any linked webhooks are still in use\")\n+    ] = False,\n+) -> CredentialsDeletionResponse | CredentialsDeletionNeedsConfirmationResponse:\n     creds = creds_manager.store.get_creds_by_id(user_id, cred_id)\n     if not creds:\n         raise HTTPException(status_code=404, detail=\"Credentials not found\")\n@@ -198,6 +219,11 @@ def delete_credentials(\n             status_code=404, detail=\"Credentials do not match the specified provider\"\n         )\n \n+    try:\n+        await remove_all_webhooks_for_credentials(creds, force)\n+    except NeedConfirmation as e:\n+        return CredentialsDeletionNeedsConfirmationResponse(message=str(e))\n+\n     creds_manager.delete(user_id, cred_id)\n \n     tokens_revoked = None\n@@ -208,7 +234,98 @@ def delete_credentials(\n     return CredentialsDeletionResponse(revoked=tokens_revoked)\n \n \n-# -------- UTILITIES --------- #\n+# ------------------------- WEBHOOK STUFF -------------------------- #\n+\n+\n+#  Note\n+# No user auth check because this endpoint is for webhook ingress and relies on\n+# validation by the provider-specific `WebhooksManager`.\n+@router.post(\"/{provider}/webhooks/{webhook_id}/ingress\")\n+async def webhook_ingress_generic(\n+    request: Request,\n+    provider: Annotated[str, Path(title=\"Provider where the webhook was registered\")],\n+    webhook_id: Annotated[str, Path(title=\"Our ID for the webhook\")],\n+):\n+    logger.debug(f\"Received {provider} webhook ingress for ID {webhook_id}\")\n+    webhook_manager = WEBHOOK_MANAGERS_BY_NAME[provider]()\n+    webhook = await get_webhook(webhook_id)\n+    logger.debug(f\"Webhook #{webhook_id}: {webhook}\")\n+    payload, event_type = await webhook_manager.validate_payload(webhook, request)\n+    logger.debug(f\"Validated {provider} {event_type} event with payload {payload}\")\n+\n+    webhook_event = WebhookEvent(\n+        provider=provider,\n+        webhook_id=webhook_id,\n+        event_type=event_type,\n+        payload=payload,\n+    )\n+    await publish_webhook_event(webhook_event)\n+    logger.debug(f\"Webhook event published: {webhook_event}\")\n+\n+    if not webhook.attached_nodes:\n+        return\n+\n+    executor = get_service_client(ExecutionManager)\n+    for node in webhook.attached_nodes:\n+        logger.debug(f\"Webhook-attached node: {node}\")\n+        if not node.is_triggered_by_event_type(event_type):\n+            logger.debug(f\"Node #{node.id} doesn't trigger on event {event_type}\")\n+            continue\n+        logger.debug(f\"Executing graph #{node.graph_id} node #{node.id}\")\n+        executor.add_execution(\n+            node.graph_id,\n+            data={f\"webhook_{webhook_id}_payload\": payload},\n+            user_id=webhook.user_id,\n+        )\n+\n+\n+@router.post(\"/{provider}/webhooks/{webhook_id}/ping\")\n+async def webhook_ping(\n+    provider: Annotated[str, Path(title=\"Provider where the webhook was registered\")],\n+    webhook_id: Annotated[str, Path(title=\"Our ID for the webhook\")],\n+    user_id: Annotated[str, Depends(get_user_id)],  # require auth\n+):\n+    webhook_manager = WEBHOOK_MANAGERS_BY_NAME[provider]()\n+    webhook = await get_webhook(webhook_id)\n+\n+    await webhook_manager.trigger_ping(webhook)\n+    if not await listen_for_webhook_event(webhook_id, event_type=\"ping\"):\n+        raise HTTPException(status_code=500, detail=\"Webhook ping event not received\")\n+\n+\n+# --------------------------- UTILITIES ---------------------------- #\n+\n+\n+async def remove_all_webhooks_for_credentials(\n+    credentials: Credentials, force: bool = False\n+) -> None:\n+    \"\"\"\n+    Remove and deregister all webhooks that were registered using the given credentials.\n+\n+    Params:\n+        credentials: The credentials for which to remove the associated webhooks.\n+        force: Whether to proceed if any of the webhooks are still in use.\n+\n+    Raises:\n+        NeedConfirmation: If any of the webhooks are still in use and `force` is `False`\n+    \"\"\"\n+    webhooks = await get_all_webhooks(credentials.id)\n+    if any(w.attached_nodes for w in webhooks) and not force:\n+        raise NeedConfirmation(\n+            \"Some webhooks linked to these credentials are still in use by an agent\"\n+        )\n+    for webhook in webhooks:\n+        # Unlink all nodes\n+        for node in webhook.attached_nodes or []:\n+            await set_node_webhook(node.id, None)\n+\n+        # Prune the webhook\n+        webhook_manager = WEBHOOK_MANAGERS_BY_NAME[credentials.provider]()\n+        success = await webhook_manager.prune_webhook_if_dangling(\n+            webhook.id, credentials\n+        )\n+        if not success:\n+            logger.warning(f\"Webhook #{webhook.id} failed to prune\")\n \n \n def _get_provider_oauth_handler(req: Request, provider_name: str) -> BaseOAuthHandler:\n@@ -226,7 +343,11 @@ def _get_provider_oauth_handler(req: Request, provider_name: str) -> BaseOAuthHa\n         )\n \n     handler_class = HANDLERS_BY_NAME[provider_name]\n-    frontend_base_url = settings.config.frontend_base_url or str(req.base_url)\n+    frontend_base_url = (\n+        settings.config.frontend_base_url\n+        or settings.config.platform_base_url\n+        or str(req.base_url)\n+    )\n     return handler_class(\n         client_id=client_id,\n         client_secret=client_secret,\ndiff --git a/autogpt_platform/backend/backend/server/rest_api.py b/autogpt_platform/backend/backend/server/rest_api.py\nindex 59b7f04af154..06e7dc64a844 100644\n--- a/autogpt_platform/backend/backend/server/rest_api.py\n+++ b/autogpt_platform/backend/backend/server/rest_api.py\n@@ -29,21 +29,6 @@ async def lifespan_context(app: fastapi.FastAPI):\n     await backend.data.db.disconnect()\n \n \n-def handle_internal_http_error(status_code: int = 500, log_error: bool = True):\n-    def handler(request: fastapi.Request, exc: Exception):\n-        if log_error:\n-            logger.exception(f\"{request.method} {request.url.path} failed: {exc}\")\n-        return fastapi.responses.JSONResponse(\n-            content={\n-                \"message\": f\"{request.method} {request.url.path} failed\",\n-                \"detail\": str(exc),\n-            },\n-            status_code=status_code,\n-        )\n-\n-    return handler\n-\n-\n docs_url = (\n     \"/docs\"\n     if settings.config.app_env == backend.util.settings.AppEnvironment.LOCAL\n@@ -62,8 +47,24 @@ def handler(request: fastapi.Request, exc: Exception):\n     docs_url=docs_url,\n )\n \n+\n+def handle_internal_http_error(status_code: int = 500, log_error: bool = True):\n+    def handler(request: fastapi.Request, exc: Exception):\n+        if log_error:\n+            logger.exception(f\"{request.method} {request.url.path} failed: {exc}\")\n+        return fastapi.responses.JSONResponse(\n+            content={\n+                \"message\": f\"{request.method} {request.url.path} failed\",\n+                \"detail\": str(exc),\n+            },\n+            status_code=status_code,\n+        )\n+\n+    return handler\n+\n+\n app.add_exception_handler(ValueError, handle_internal_http_error(400))\n-app.add_exception_handler(500, handle_internal_http_error(500))\n+app.add_exception_handler(Exception, handle_internal_http_error(500))\n app.include_router(backend.server.routers.v1.v1_router, tags=[\"v1\"])\n \n \ndiff --git a/autogpt_platform/backend/backend/server/routers/v1.py b/autogpt_platform/backend/backend/server/routers/v1.py\nindex a44f25c1ae28..f1f1fda385bd 100644\n--- a/autogpt_platform/backend/backend/server/routers/v1.py\n+++ b/autogpt_platform/backend/backend/server/routers/v1.py\n@@ -1,7 +1,7 @@\n import asyncio\n import logging\n from collections import defaultdict\n-from typing import Annotated, Any, List\n+from typing import TYPE_CHECKING, Annotated, Any, Sequence\n \n import pydantic\n from autogpt_libs.auth.middleware import auth_middleware\n@@ -30,6 +30,11 @@\n from backend.data.credit import get_block_costs, get_user_credit_model\n from backend.data.user import get_or_create_user\n from backend.executor import ExecutionManager, ExecutionScheduler, scheduler\n+from backend.integrations.creds_manager import IntegrationCredentialsManager\n+from backend.integrations.webhooks.graph_lifecycle_hooks import (\n+    on_graph_activate,\n+    on_graph_deactivate,\n+)\n from backend.server.model import (\n     CreateAPIKeyRequest,\n     CreateAPIKeyResponse,\n@@ -41,6 +46,9 @@\n from backend.util.service import get_service_client\n from backend.util.settings import Settings\n \n+if TYPE_CHECKING:\n+    from autogpt_libs.supabase_integration_credentials_store.types import Credentials\n+\n \n @thread_cached\n def execution_manager_client() -> ExecutionManager:\n@@ -54,6 +62,7 @@ def execution_scheduler_client() -> ExecutionScheduler:\n \n settings = Settings()\n logger = logging.getLogger(__name__)\n+integration_creds_manager = IntegrationCredentialsManager()\n \n \n _user_credit_model = get_user_credit_model()\n@@ -62,14 +71,10 @@ def execution_scheduler_client() -> ExecutionScheduler:\n v1_router = APIRouter(prefix=\"/api\")\n \n \n-v1_router.dependencies.append(Depends(auth_middleware))\n-\n-\n v1_router.include_router(\n     backend.server.integrations.router.router,\n     prefix=\"/integrations\",\n     tags=[\"integrations\"],\n-    dependencies=[Depends(auth_middleware)],\n )\n \n v1_router.include_router(\n@@ -97,13 +102,17 @@ async def get_or_create_user_route(user_data: dict = Depends(auth_middleware)):\n \n \n @v1_router.get(path=\"/blocks\", tags=[\"blocks\"], dependencies=[Depends(auth_middleware)])\n-def get_graph_blocks() -> list[dict[Any, Any]]:\n+def get_graph_blocks() -> Sequence[dict[Any, Any]]:\n     blocks = [block() for block in backend.data.block.get_blocks().values()]\n     costs = get_block_costs()\n     return [{**b.to_dict(), \"costs\": costs.get(b.id, [])} for b in blocks]\n \n \n-@v1_router.post(path=\"/blocks/{block_id}/execute\", tags=[\"blocks\"])\n+@v1_router.post(\n+    path=\"/blocks/{block_id}/execute\",\n+    tags=[\"blocks\"],\n+    dependencies=[Depends(auth_middleware)],\n+)\n def execute_graph_block(block_id: str, data: BlockInput) -> CompletedBlockOutput:\n     obj = backend.data.block.get_block(block_id)\n     if not obj:\n@@ -141,7 +150,7 @@ class DeleteGraphResponse(TypedDict):\n async def get_graphs(\n     user_id: Annotated[str, Depends(get_user_id)],\n     with_runs: bool = False,\n-) -> list[graph_db.Graph]:\n+) -> Sequence[graph_db.Graph]:\n     return await graph_db.get_graphs(\n         include_executions=with_runs, filter_by=\"active\", user_id=user_id\n     )\n@@ -181,13 +190,61 @@ async def get_graph(\n )\n async def get_graph_all_versions(\n     graph_id: str, user_id: Annotated[str, Depends(get_user_id)]\n-) -> list[graph_db.Graph]:\n+) -> Sequence[graph_db.Graph]:\n     graphs = await graph_db.get_graph_all_versions(graph_id, user_id=user_id)\n     if not graphs:\n         raise HTTPException(status_code=404, detail=f\"Graph #{graph_id} not found.\")\n     return graphs\n \n \n+@v1_router.post(\n+    path=\"/graphs\", tags=[\"graphs\"], dependencies=[Depends(auth_middleware)]\n+)\n+async def create_new_graph(\n+    create_graph: CreateGraph, user_id: Annotated[str, Depends(get_user_id)]\n+) -> graph_db.Graph:\n+    return await do_create_graph(create_graph, is_template=False, user_id=user_id)\n+\n+\n+async def do_create_graph(\n+    create_graph: CreateGraph,\n+    is_template: bool,\n+    # user_id doesn't have to be annotated like on other endpoints,\n+    # because create_graph isn't used directly as an endpoint\n+    user_id: str,\n+) -> graph_db.Graph:\n+    if create_graph.graph:\n+        graph = graph_db.make_graph_model(create_graph.graph, user_id)\n+    elif create_graph.template_id:\n+        # Create a new graph from a template\n+        graph = await graph_db.get_graph(\n+            create_graph.template_id,\n+            create_graph.template_version,\n+            template=True,\n+            user_id=user_id,\n+        )\n+        if not graph:\n+            raise HTTPException(\n+                400, detail=f\"Template #{create_graph.template_id} not found\"\n+            )\n+        graph.version = 1\n+    else:\n+        raise HTTPException(\n+            status_code=400, detail=\"Either graph or template_id must be provided.\"\n+        )\n+\n+    graph.is_template = is_template\n+    graph.is_active = not is_template\n+    graph.reassign_ids(user_id=user_id, reassign_graph_id=True)\n+\n+    graph = await graph_db.create_graph(graph, user_id=user_id)\n+    graph = await on_graph_activate(\n+        graph,\n+        get_credentials=lambda id: integration_creds_manager.get(user_id, id),\n+    )\n+    return graph\n+\n+\n @v1_router.delete(\n     path=\"/graphs/{graph_id}\", tags=[\"graphs\"], dependencies=[Depends(auth_middleware)]\n )\n@@ -224,33 +281,41 @@ async def update_graph(\n     latest_version_graph = next(\n         v for v in existing_versions if v.version == latest_version_number\n     )\n+    current_active_version = next((v for v in existing_versions if v.is_active), None)\n     if latest_version_graph.is_template != graph.is_template:\n         raise HTTPException(\n             400, detail=\"Changing is_template on an existing graph is forbidden\"\n         )\n     graph.is_active = not graph.is_template\n+    graph = graph_db.make_graph_model(graph, user_id)\n     graph.reassign_ids(user_id=user_id)\n \n     new_graph_version = await graph_db.create_graph(graph, user_id=user_id)\n \n     if new_graph_version.is_active:\n+\n+        def get_credentials(credentials_id: str) -> \"Credentials | None\":\n+            return integration_creds_manager.get(user_id, credentials_id)\n+\n+        # Handle activation of the new graph first to ensure continuity\n+        new_graph_version = await on_graph_activate(\n+            new_graph_version,\n+            get_credentials=get_credentials,\n+        )\n         # Ensure new version is the only active version\n         await graph_db.set_graph_active_version(\n             graph_id=graph_id, version=new_graph_version.version, user_id=user_id\n         )\n+        if current_active_version:\n+            # Handle deactivation of the previously active version\n+            await on_graph_deactivate(\n+                current_active_version,\n+                get_credentials=get_credentials,\n+            )\n \n     return new_graph_version\n \n \n-@v1_router.post(\n-    path=\"/graphs\", tags=[\"graphs\"], dependencies=[Depends(auth_middleware)]\n-)\n-async def create_new_graph(\n-    create_graph: CreateGraph, user_id: Annotated[str, Depends(get_user_id)]\n-) -> graph_db.Graph:\n-    return await do_create_graph(create_graph, is_template=False, user_id=user_id)\n-\n-\n @v1_router.put(\n     path=\"/graphs/{graph_id}/versions/active\",\n     tags=[\"graphs\"],\n@@ -262,13 +327,34 @@ async def set_graph_active_version(\n     user_id: Annotated[str, Depends(get_user_id)],\n ):\n     new_active_version = request_body.active_graph_version\n-    if not await graph_db.get_graph(graph_id, new_active_version, user_id=user_id):\n+    new_active_graph = await graph_db.get_graph(\n+        graph_id, new_active_version, user_id=user_id\n+    )\n+    if not new_active_graph:\n         raise HTTPException(404, f\"Graph #{graph_id} v{new_active_version} not found\")\n+\n+    current_active_graph = await graph_db.get_graph(graph_id, user_id=user_id)\n+\n+    def get_credentials(credentials_id: str) -> \"Credentials | None\":\n+        return integration_creds_manager.get(user_id, credentials_id)\n+\n+    # Handle activation of the new graph first to ensure continuity\n+    await on_graph_activate(\n+        new_active_graph,\n+        get_credentials=get_credentials,\n+    )\n+    # Ensure new version is the only active version\n     await graph_db.set_graph_active_version(\n         graph_id=graph_id,\n-        version=request_body.active_graph_version,\n+        version=new_active_version,\n         user_id=user_id,\n     )\n+    if current_active_graph and current_active_graph.version != new_active_version:\n+        # Handle deactivation of the previously active version\n+        await on_graph_deactivate(\n+            current_active_graph,\n+            get_credentials=get_credentials,\n+        )\n \n \n @v1_router.post(\n@@ -298,7 +384,7 @@ def execute_graph(\n )\n async def stop_graph_run(\n     graph_exec_id: str, user_id: Annotated[str, Depends(get_user_id)]\n-) -> list[execution_db.ExecutionResult]:\n+) -> Sequence[execution_db.ExecutionResult]:\n     if not await execution_db.get_graph_execution(graph_exec_id, user_id):\n         raise HTTPException(404, detail=f\"Agent execution #{graph_exec_id} not found\")\n \n@@ -319,7 +405,7 @@ async def list_graph_runs(\n     graph_id: str,\n     user_id: Annotated[str, Depends(get_user_id)],\n     graph_version: int | None = None,\n-) -> list[str]:\n+) -> Sequence[str]:\n     graph = await graph_db.get_graph(graph_id, graph_version, user_id=user_id)\n     if not graph:\n         rev = \"\" if graph_version is None else f\" v{graph_version}\"\n@@ -339,7 +425,7 @@ async def get_graph_run_node_execution_results(\n     graph_id: str,\n     graph_exec_id: str,\n     user_id: Annotated[str, Depends(get_user_id)],\n-) -> list[execution_db.ExecutionResult]:\n+) -> Sequence[execution_db.ExecutionResult]:\n     graph = await graph_db.get_graph(graph_id, user_id=user_id)\n     if not graph:\n         raise HTTPException(status_code=404, detail=f\"Graph #{graph_id} not found.\")\n@@ -378,7 +464,7 @@ async def get_graph_run_status(\n )\n async def get_templates(\n     user_id: Annotated[str, Depends(get_user_id)]\n-) -> list[graph_db.Graph]:\n+) -> Sequence[graph_db.Graph]:\n     return await graph_db.get_graphs(filter_by=\"template\", user_id=user_id)\n \n \n@@ -394,40 +480,6 @@ async def get_template(graph_id: str, version: int | None = None) -> graph_db.Gr\n     return graph\n \n \n-async def do_create_graph(\n-    create_graph: CreateGraph,\n-    is_template: bool,\n-    # user_id doesn't have to be annotated like on other endpoints,\n-    # because create_graph isn't used directly as an endpoint\n-    user_id: str,\n-) -> graph_db.Graph:\n-    if create_graph.graph:\n-        graph = create_graph.graph\n-    elif create_graph.template_id:\n-        # Create a new graph from a template\n-        graph = await graph_db.get_graph(\n-            create_graph.template_id,\n-            create_graph.template_version,\n-            template=True,\n-            user_id=user_id,\n-        )\n-        if not graph:\n-            raise HTTPException(\n-                400, detail=f\"Template #{create_graph.template_id} not found\"\n-            )\n-        graph.version = 1\n-    else:\n-        raise HTTPException(\n-            status_code=400, detail=\"Either graph or template_id must be provided.\"\n-        )\n-\n-    graph.is_template = is_template\n-    graph.is_active = not is_template\n-    graph.reassign_ids(user_id=user_id, reassign_graph_id=True)\n-\n-    return await graph_db.create_graph(graph, user_id=user_id)\n-\n-\n @v1_router.post(\n     path=\"/templates\",\n     tags=[\"templates\", \"graphs\"],\n@@ -534,13 +586,13 @@ async def create_api_key(\n \n @v1_router.get(\n     \"/api-keys\",\n-    response_model=List[APIKeyWithoutHash],\n+    response_model=list[APIKeyWithoutHash],\n     tags=[\"api-keys\"],\n     dependencies=[Depends(auth_middleware)],\n )\n async def get_api_keys(\n     user_id: Annotated[str, Depends(get_user_id)]\n-) -> List[APIKeyWithoutHash]:\n+) -> list[APIKeyWithoutHash]:\n     \"\"\"List all API keys for the user\"\"\"\n     try:\n         return await list_user_api_keys(user_id)\ndiff --git a/autogpt_platform/backend/backend/server/ws_api.py b/autogpt_platform/backend/backend/server/ws_api.py\nindex 421a911abdaf..a6da64b8e53c 100644\n--- a/autogpt_platform/backend/backend/server/ws_api.py\n+++ b/autogpt_platform/backend/backend/server/ws_api.py\n@@ -8,7 +8,7 @@\n from starlette.middleware.cors import CORSMiddleware\n \n from backend.data import redis\n-from backend.data.queue import AsyncRedisExecutionEventBus\n+from backend.data.execution import AsyncRedisExecutionEventBus\n from backend.data.user import DEFAULT_USER_ID\n from backend.server.conn_manager import ConnectionManager\n from backend.server.model import ExecutionSubscription, Methods, WsMessage\ndiff --git a/autogpt_platform/backend/backend/util/exceptions.py b/autogpt_platform/backend/backend/util/exceptions.py\nnew file mode 100644\nindex 000000000000..4bb3a08d9548\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/util/exceptions.py\n@@ -0,0 +1,6 @@\n+class MissingConfigError(Exception):\n+    \"\"\"The attempted operation requires configuration which is not available\"\"\"\n+\n+\n+class NeedConfirmation(Exception):\n+    \"\"\"The user must explicitly confirm that they want to proceed\"\"\"\ndiff --git a/autogpt_platform/backend/backend/util/service.py b/autogpt_platform/backend/backend/util/service.py\nindex a0b6bde40b6f..dffd8e37a8a0 100644\n--- a/autogpt_platform/backend/backend/util/service.py\n+++ b/autogpt_platform/backend/backend/util/service.py\n@@ -11,6 +11,7 @@\n from typing import (\n     Annotated,\n     Any,\n+    Awaitable,\n     Callable,\n     Coroutine,\n     Dict,\n@@ -64,7 +65,13 @@ def wrapper(*args, **kwargs):\n             logger.exception(msg)\n             raise\n \n-    # Register custom serializers and deserializers for annotated Pydantic models\n+    register_pydantic_serializers(func)\n+\n+    return pyro.expose(wrapper)  # type: ignore\n+\n+\n+def register_pydantic_serializers(func: Callable):\n+    \"\"\"Register custom serializers and deserializers for annotated Pydantic models\"\"\"\n     for name, annotation in func.__annotations__.items():\n         try:\n             pydantic_types = _pydantic_models_from_type_annotation(annotation)\n@@ -81,8 +88,6 @@ def wrapper(*args, **kwargs):\n                 model.__qualname__, _make_custom_deserializer(model)\n             )\n \n-    return pyro.expose(wrapper)  # type: ignore\n-\n \n def _make_custom_serializer(model: Type[BaseModel]):\n     def custom_class_to_dict(obj):\n@@ -252,6 +257,10 @@ def _pydantic_models_from_type_annotation(annotation) -> Iterator[type[BaseModel\n         key_type, value_type = args\n         yield from _pydantic_models_from_type_annotation(key_type)\n         yield from _pydantic_models_from_type_annotation(value_type)\n+    elif origin in (Awaitable, Coroutine):\n+        # For coroutines and awaitables, check the return type\n+        return_type = args[-1]\n+        yield from _pydantic_models_from_type_annotation(return_type)\n     else:\n         annotype = annotation if origin is None else origin\n \ndiff --git a/autogpt_platform/backend/backend/util/settings.py b/autogpt_platform/backend/backend/util/settings.py\nindex 98b1b66c82a1..34ca9336d45d 100644\n--- a/autogpt_platform/backend/backend/util/settings.py\n+++ b/autogpt_platform/backend/backend/util/settings.py\n@@ -3,7 +3,7 @@\n from enum import Enum\n from typing import Any, Dict, Generic, List, Set, Tuple, Type, TypeVar\n \n-from pydantic import BaseModel, Field, PrivateAttr, field_validator\n+from pydantic import BaseModel, Field, PrivateAttr, ValidationInfo, field_validator\n from pydantic_settings import (\n     BaseSettings,\n     JsonConfigSettingsSource,\n@@ -136,12 +136,32 @@ class Config(UpdateTrackingModel[\"Config\"], BaseSettings):\n         description=\"The port for agent server API to run on\",\n     )\n \n+    platform_base_url: str = Field(\n+        default=\"\",\n+        description=\"Must be set so the application knows where it's hosted at. \"\n+        \"This is necessary to make sure webhooks find their way.\",\n+    )\n+\n     frontend_base_url: str = Field(\n-        default=\"http://localhost:3000\",\n+        default=\"\",\n         description=\"Can be used to explicitly set the base URL for the frontend. \"\n         \"This value is then used to generate redirect URLs for OAuth flows.\",\n     )\n \n+    @field_validator(\"platform_base_url\", \"frontend_base_url\")\n+    @classmethod\n+    def validate_platform_base_url(cls, v: str, info: ValidationInfo) -> str:\n+        if not v:\n+            return v\n+        if not v.startswith((\"http://\", \"https://\")):\n+            raise ValueError(\n+                f\"{info.field_name} must be a full URL \"\n+                \"including a http:// or https:// schema\"\n+            )\n+        if v.endswith(\"/\"):\n+            return v.rstrip(\"/\")  # Remove trailing slash\n+        return v\n+\n     app_env: AppEnvironment = Field(\n         default=AppEnvironment.LOCAL,\n         description=\"The name of the app environment: local or dev or prod\",\ndiff --git a/autogpt_platform/backend/backend/util/test.py b/autogpt_platform/backend/backend/util/test.py\nindex b9204ec214b3..37ab2302da54 100644\n--- a/autogpt_platform/backend/backend/util/test.py\n+++ b/autogpt_platform/backend/backend/util/test.py\n@@ -1,9 +1,10 @@\n import logging\n import time\n+from typing import Sequence\n \n from backend.data import db\n from backend.data.block import Block, initialize_blocks\n-from backend.data.execution import ExecutionStatus\n+from backend.data.execution import ExecutionResult, ExecutionStatus\n from backend.data.model import CREDENTIALS_FIELD_NAME\n from backend.data.user import create_default_user\n from backend.executor import DatabaseManager, ExecutionManager, ExecutionScheduler\n@@ -57,7 +58,7 @@ async def wait_execution(\n     graph_id: str,\n     graph_exec_id: str,\n     timeout: int = 20,\n-) -> list:\n+) -> Sequence[ExecutionResult]:\n     async def is_execution_completed():\n         status = await AgentServer().test_get_graph_run_status(\n             graph_id, graph_exec_id, user_id\ndiff --git a/autogpt_platform/backend/migrations/20241017180251_add_webhooks_and_their_relation_to_nodes/migration.sql b/autogpt_platform/backend/migrations/20241017180251_add_webhooks_and_their_relation_to_nodes/migration.sql\nnew file mode 100644\nindex 000000000000..011a017c8f20\n--- /dev/null\n+++ b/autogpt_platform/backend/migrations/20241017180251_add_webhooks_and_their_relation_to_nodes/migration.sql\n@@ -0,0 +1,26 @@\n+-- AlterTable\n+ALTER TABLE \"AgentNode\" ADD COLUMN     \"webhookId\" TEXT;\n+\n+-- CreateTable\n+CREATE TABLE \"IntegrationWebhook\" (\n+    \"id\" TEXT NOT NULL,\n+    \"createdAt\" TIMESTAMP(3) NOT NULL DEFAULT CURRENT_TIMESTAMP,\n+    \"updatedAt\" TIMESTAMP(3),\n+    \"userId\" TEXT NOT NULL,\n+    \"provider\" TEXT NOT NULL,\n+    \"credentialsId\" TEXT NOT NULL,\n+    \"webhookType\" TEXT NOT NULL,\n+    \"resource\" TEXT NOT NULL,\n+    \"events\" TEXT[],\n+    \"config\" JSONB NOT NULL,\n+    \"secret\" TEXT NOT NULL,\n+    \"providerWebhookId\" TEXT NOT NULL,\n+\n+    CONSTRAINT \"IntegrationWebhook_pkey\" PRIMARY KEY (\"id\")\n+);\n+\n+-- AddForeignKey\n+ALTER TABLE \"AgentNode\" ADD CONSTRAINT \"AgentNode_webhookId_fkey\" FOREIGN KEY (\"webhookId\") REFERENCES \"IntegrationWebhook\"(\"id\") ON DELETE SET NULL ON UPDATE CASCADE;\n+\n+-- AddForeignKey\n+ALTER TABLE \"IntegrationWebhook\" ADD CONSTRAINT \"IntegrationWebhook_userId_fkey\" FOREIGN KEY (\"userId\") REFERENCES \"User\"(\"id\") ON DELETE RESTRICT ON UPDATE CASCADE;\ndiff --git a/autogpt_platform/backend/schema.prisma b/autogpt_platform/backend/schema.prisma\nindex 39829b8a2b62..df3198870591 100644\n--- a/autogpt_platform/backend/schema.prisma\n+++ b/autogpt_platform/backend/schema.prisma\n@@ -23,6 +23,7 @@ model User {\n   // Relations\n   AgentGraphs                  AgentGraph[]\n   AgentGraphExecutions         AgentGraphExecution[]\n+  IntegrationWebhooks          IntegrationWebhook[]\n   AnalyticsDetails             AnalyticsDetails[]\n   AnalyticsMetrics             AnalyticsMetrics[]\n   UserBlockCredit              UserBlockCredit[]\n@@ -74,6 +75,10 @@ model AgentNode {\n   // JSON serialized dict[str, str] containing predefined input values.\n   constantInput String @default(\"{}\")\n \n+  // For webhook-triggered blocks: reference to the webhook that triggers the node\n+  webhookId String?\n+  Webhook   IntegrationWebhook? @relation(fields: [webhookId], references: [id])\n+\n   // JSON serialized dict[str, str] containing the node metadata.\n   metadata String @default(\"{}\")\n \n@@ -186,6 +191,28 @@ model AgentNodeExecutionInputOutput {\n   @@unique([referencedByInputExecId, referencedByOutputExecId, name])\n }\n \n+// Webhook that is registered with a provider and propagates to one or more nodes\n+model IntegrationWebhook {\n+  id        String    @id @default(uuid())\n+  createdAt DateTime  @default(now())\n+  updatedAt DateTime? @updatedAt\n+\n+  userId String\n+  user   User   @relation(fields: [userId], references: [id], onDelete: Restrict) // Webhooks must be deregistered before deleting\n+\n+  provider      String // e.g. 'github'\n+  credentialsId String // relation to the credentials that the webhook was created with\n+  webhookType   String // e.g. 'repo'\n+  resource      String // e.g. 'Significant-Gravitas/AutoGPT'\n+  events        String[] // e.g. ['created', 'updated']\n+  config        Json\n+  secret        String // crypto string, used to verify payload authenticity\n+\n+  providerWebhookId String // Webhook ID assigned by the provider\n+\n+  AgentNodes AgentNode[]\n+}\n+\n model AnalyticsDetails {\n   // PK uses gen_random_uuid() to allow the db inserts to happen outside of prisma\n   // typical uuid() inserts are handled by prisma\ndiff --git a/autogpt_platform/frontend/package.json b/autogpt_platform/frontend/package.json\nindex 53aeb99eda0f..84dbfaf35da0 100644\n--- a/autogpt_platform/frontend/package.json\n+++ b/autogpt_platform/frontend/package.json\n@@ -26,6 +26,7 @@\n     \"@faker-js/faker\": \"^9.2.0\",\n     \"@hookform/resolvers\": \"^3.9.1\",\n     \"@next/third-parties\": \"^15.0.3\",\n+    \"@radix-ui/react-alert-dialog\": \"^1.1.2\",\n     \"@radix-ui/react-avatar\": \"^1.1.1\",\n     \"@radix-ui/react-checkbox\": \"^1.1.2\",\n     \"@radix-ui/react-collapsible\": \"^1.1.1\",\ndiff --git a/autogpt_platform/frontend/src/app/globals.css b/autogpt_platform/frontend/src/app/globals.css\nindex 7930a00b3c88..7be802f39b41 100644\n--- a/autogpt_platform/frontend/src/app/globals.css\n+++ b/autogpt_platform/frontend/src/app/globals.css\n@@ -74,7 +74,7 @@\n   }\n \n   .agpt-border-input {\n-    @apply border-input focus-visible:border-gray-400 focus-visible:outline-none;\n+    @apply border border-input focus-visible:border-gray-400 focus-visible:outline-none;\n   }\n \n   .agpt-shadow-input {\ndiff --git a/autogpt_platform/frontend/src/app/profile/page.tsx b/autogpt_platform/frontend/src/app/profile/page.tsx\nindex e01c4b1b8ecd..93d9b77085cf 100644\n--- a/autogpt_platform/frontend/src/app/profile/page.tsx\n+++ b/autogpt_platform/frontend/src/app/profile/page.tsx\n@@ -4,7 +4,7 @@ import { useSupabase } from \"@/components/SupabaseProvider\";\n import { Button } from \"@/components/ui/button\";\n import useUser from \"@/hooks/useUser\";\n import { useRouter } from \"next/navigation\";\n-import { useCallback, useContext, useMemo } from \"react\";\n+import { useCallback, useContext, useMemo, useState } from \"react\";\n import { FaSpinner } from \"react-icons/fa\";\n import { Separator } from \"@/components/ui/separator\";\n import { useToast } from \"@/components/ui/use-toast\";\n@@ -21,6 +21,16 @@ import {\n   TableRow,\n } from \"@/components/ui/table\";\n import { CredentialsProviderName } from \"@/lib/autogpt-server-api\";\n+import {\n+  AlertDialog,\n+  AlertDialogAction,\n+  AlertDialogCancel,\n+  AlertDialogContent,\n+  AlertDialogDescription,\n+  AlertDialogFooter,\n+  AlertDialogHeader,\n+  AlertDialogTitle,\n+} from \"@/components/ui/alert-dialog\";\n \n export default function PrivatePage() {\n   const { user, isLoading, error } = useUser();\n@@ -29,15 +39,40 @@ export default function PrivatePage() {\n   const providers = useContext(CredentialsProvidersContext);\n   const { toast } = useToast();\n \n+  const [confirmationDialogState, setConfirmationDialogState] = useState<\n+    | {\n+        open: true;\n+        message: string;\n+        onConfirm: () => void;\n+        onReject: () => void;\n+      }\n+    | { open: false }\n+  >({ open: false });\n+\n   const removeCredentials = useCallback(\n-    async (provider: CredentialsProviderName, id: string) => {\n+    async (\n+      provider: CredentialsProviderName,\n+      id: string,\n+      force: boolean = false,\n+    ) => {\n       if (!providers || !providers[provider]) {\n         return;\n       }\n \n+      let result;\n       try {\n-        const { revoked } = await providers[provider].deleteCredentials(id);\n-        if (revoked !== false) {\n+        result = await providers[provider].deleteCredentials(id, force);\n+      } catch (error: any) {\n+        toast({\n+          title: \"Something went wrong when deleting credentials: \" + error,\n+          variant: \"destructive\",\n+          duration: 2000,\n+        });\n+        setConfirmationDialogState({ open: false });\n+        return;\n+      }\n+      if (result.deleted) {\n+        if (result.revoked) {\n           toast({\n             title: \"Credentials deleted\",\n             duration: 2000,\n@@ -49,11 +84,13 @@ export default function PrivatePage() {\n             duration: 3000,\n           });\n         }\n-      } catch (error: any) {\n-        toast({\n-          title: \"Something went wrong when deleting credentials: \" + error,\n-          variant: \"destructive\",\n-          duration: 2000,\n+        setConfirmationDialogState({ open: false });\n+      } else if (result.need_confirmation) {\n+        setConfirmationDialogState({\n+          open: true,\n+          message: result.message,\n+          onConfirm: () => removeCredentials(provider, id, true),\n+          onReject: () => setConfirmationDialogState({ open: false }),\n         });\n       }\n     },\n@@ -158,6 +195,36 @@ export default function PrivatePage() {\n           ))}\n         </TableBody>\n       </Table>\n+\n+      <AlertDialog open={confirmationDialogState.open}>\n+        <AlertDialogContent>\n+          <AlertDialogHeader>\n+            <AlertDialogTitle>Are you sure?</AlertDialogTitle>\n+            <AlertDialogDescription>\n+              {confirmationDialogState.open && confirmationDialogState.message}\n+            </AlertDialogDescription>\n+          </AlertDialogHeader>\n+          <AlertDialogFooter>\n+            <AlertDialogCancel\n+              onClick={() =>\n+                confirmationDialogState.open &&\n+                confirmationDialogState.onReject()\n+              }\n+            >\n+              Cancel\n+            </AlertDialogCancel>\n+            <AlertDialogAction\n+              variant=\"destructive\"\n+              onClick={() =>\n+                confirmationDialogState.open &&\n+                confirmationDialogState.onConfirm()\n+              }\n+            >\n+              Continue\n+            </AlertDialogAction>\n+          </AlertDialogFooter>\n+        </AlertDialogContent>\n+      </AlertDialog>\n     </div>\n   );\n }\ndiff --git a/autogpt_platform/frontend/src/components/CustomNode.tsx b/autogpt_platform/frontend/src/components/CustomNode.tsx\nindex 4157a6599016..710aa1932bb0 100644\n--- a/autogpt_platform/frontend/src/components/CustomNode.tsx\n+++ b/autogpt_platform/frontend/src/components/CustomNode.tsx\n@@ -38,6 +38,7 @@ import { getPrimaryCategoryColor } from \"@/lib/utils\";\n import { FlowContext } from \"./Flow\";\n import { Badge } from \"./ui/badge\";\n import NodeOutputs from \"./NodeOutputs\";\n+import SchemaTooltip from \"./SchemaTooltip\";\n import { IconCoin } from \"./ui/icons\";\n import * as Separator from \"@radix-ui/react-separator\";\n import * as ContextMenu from \"@radix-ui/react-context-menu\";\n@@ -166,7 +167,7 @@ export function CustomNode({\n       <div key={key}>\n         <NodeHandle\n           keyName={key}\n-          isConnected={isHandleConnected(key)}\n+          isConnected={isOutputHandleConnected(key)}\n           schema={schema.properties[key]}\n           side=\"right\"\n         />\n@@ -205,16 +206,18 @@ export function CustomNode({\n \n         return keys.map(([propKey, propSchema]) => {\n           const isRequired = data.inputSchema.required?.includes(propKey);\n-          const isConnected = isHandleConnected(propKey);\n           const isAdvanced = propSchema.advanced;\n+          const isHidden = propSchema.hidden;\n           const isConnectable =\n+            // No input connection handles on INPUT and WEBHOOK blocks\n+            ![BlockUIType.INPUT, BlockUIType.WEBHOOK].includes(nodeType) &&\n             // No input connection handles for credentials\n             propKey !== \"credentials\" &&\n-            // No input connection handles on INPUT blocks\n-            nodeType !== BlockUIType.INPUT &&\n             // For OUTPUT blocks, only show the 'value' (hides 'name') input connection handle\n             !(nodeType == BlockUIType.OUTPUT && propKey == \"name\");\n+          const isConnected = isInputHandleConnected(propKey);\n           return (\n+            !isHidden &&\n             (isRequired || isAdvancedOpen || isConnected || !isAdvanced) && (\n               <div key={propKey} data-id={`input-handle-${propKey}`}>\n                 {isConnectable ? (\n@@ -227,15 +230,15 @@ export function CustomNode({\n                   />\n                 ) : (\n                   propKey != \"credentials\" && (\n-                    <span\n-                      className=\"text-m green mb-0 text-gray-900\"\n-                      title={propSchema.description}\n-                    >\n-                      {propSchema.title || beautifyString(propKey)}\n-                    </span>\n+                    <div className=\"flex gap-1\">\n+                      <span className=\"text-m green mb-0 text-gray-900\">\n+                        {propSchema.title || beautifyString(propKey)}\n+                      </span>\n+                      <SchemaTooltip description={propSchema.description} />\n+                    </div>\n                   )\n                 )}\n-                {!isConnected && (\n+                {isConnected || (\n                   <NodeGenericInputField\n                     nodeId={id}\n                     propKey={getInputPropKey(propKey)}\n@@ -298,21 +301,28 @@ export function CustomNode({\n     setErrors({ ...errors });\n   };\n \n-  const isHandleConnected = (key: string) => {\n+  const isInputHandleConnected = (key: string) => {\n     return (\n       data.connections &&\n       data.connections.some((conn: any) => {\n         if (typeof conn === \"string\") {\n-          const [source, target] = conn.split(\" -> \");\n-          return (\n-            (target.includes(key) && target.includes(data.title)) ||\n-            (source.includes(key) && source.includes(data.title))\n-          );\n+          const [_source, target] = conn.split(\" -> \");\n+          return target.includes(key) && target.includes(data.title);\n         }\n-        return (\n-          (conn.target === id && conn.targetHandle === key) ||\n-          (conn.source === id && conn.sourceHandle === key)\n-        );\n+        return conn.target === id && conn.targetHandle === key;\n+      })\n+    );\n+  };\n+\n+  const isOutputHandleConnected = (key: string) => {\n+    return (\n+      data.connections &&\n+      data.connections.some((conn: any) => {\n+        if (typeof conn === \"string\") {\n+          const [source, _target] = conn.split(\" -> \");\n+          return source.includes(key) && source.includes(data.title);\n+        }\n+        return conn.source === id && conn.sourceHandle === key;\n       })\n     );\n   };\ndiff --git a/autogpt_platform/frontend/src/components/flow.css b/autogpt_platform/frontend/src/components/flow.css\nindex 30ef76b68f25..abbee903b42c 100644\n--- a/autogpt_platform/frontend/src/components/flow.css\n+++ b/autogpt_platform/frontend/src/components/flow.css\n@@ -11,22 +11,6 @@ code {\n     monospace;\n }\n \n-input,\n-textarea {\n-  background-color: #ffffff;\n-  color: #000000;\n-  border: 1px solid #555;\n-  padding: 8px;\n-  border-radius: 4px;\n-  width: calc(100% - 18px);\n-  box-sizing: border-box;\n-}\n-\n-input::placeholder,\n-textarea::placeholder {\n-  color: #aaa;\n-}\n-\n .modal {\n   position: absolute;\n   top: 50%;\ndiff --git a/autogpt_platform/frontend/src/components/integrations/credentials-input.tsx b/autogpt_platform/frontend/src/components/integrations/credentials-input.tsx\nindex df08a2ffe070..d48a581bdf35 100644\n--- a/autogpt_platform/frontend/src/components/integrations/credentials-input.tsx\n+++ b/autogpt_platform/frontend/src/components/integrations/credentials-input.tsx\n@@ -3,6 +3,7 @@ import { cn } from \"@/lib/utils\";\n import { useForm } from \"react-hook-form\";\n import { Input } from \"@/components/ui/input\";\n import { Button } from \"@/components/ui/button\";\n+import SchemaTooltip from \"@/components/SchemaTooltip\";\n import useCredentials from \"@/hooks/useCredentials\";\n import { zodResolver } from \"@hookform/resolvers/zod\";\n import AutoGPTServerAPI from \"@/lib/autogpt-server-api\";\n@@ -235,12 +236,10 @@ export const CredentialsInput: FC<{\n   if (savedApiKeys.length === 0 && savedOAuthCredentials.length === 0) {\n     return (\n       <>\n-        <span\n-          className=\"text-m green mb-0 text-gray-900\"\n-          title={schema.description}\n-        >\n-          Credentials\n-        </span>\n+        <div className=\"mb-2 flex gap-1\">\n+          <span className=\"text-m green text-gray-900\">Credentials</span>\n+          <SchemaTooltip description={schema.description} />\n+        </div>\n         <div className={cn(\"flex flex-row space-x-2\", className)}>\n           {supportsOAuth2 && (\n             <Button onClick={handleOAuthLogin}>\ndiff --git a/autogpt_platform/frontend/src/components/integrations/credentials-provider.tsx b/autogpt_platform/frontend/src/components/integrations/credentials-provider.tsx\nindex 1cc28f29c991..8307b81e40d3 100644\n--- a/autogpt_platform/frontend/src/components/integrations/credentials-provider.tsx\n+++ b/autogpt_platform/frontend/src/components/integrations/credentials-provider.tsx\n@@ -1,5 +1,6 @@\n import AutoGPTServerAPI, {\n   APIKeyCredentials,\n+  CredentialsDeleteNeedConfirmationResponse,\n   CredentialsDeleteResponse,\n   CredentialsMetaResponse,\n   CredentialsProviderName,\n@@ -59,7 +60,12 @@ export type CredentialsProviderData = {\n   createAPIKeyCredentials: (\n     credentials: APIKeyCredentialsCreatable,\n   ) => Promise<CredentialsMetaResponse>;\n-  deleteCredentials: (id: string) => Promise<CredentialsDeleteResponse>;\n+  deleteCredentials: (\n+    id: string,\n+    force?: boolean,\n+  ) => Promise<\n+    CredentialsDeleteResponse | CredentialsDeleteNeedConfirmationResponse\n+  >;\n };\n \n export type CredentialsProvidersContextType = {\n@@ -144,8 +150,14 @@ export default function CredentialsProvider({\n     async (\n       provider: CredentialsProviderName,\n       id: string,\n-    ): Promise<CredentialsDeleteResponse> => {\n-      const result = await api.deleteCredentials(provider, id);\n+      force: boolean = false,\n+    ): Promise<\n+      CredentialsDeleteResponse | CredentialsDeleteNeedConfirmationResponse\n+    > => {\n+      const result = await api.deleteCredentials(provider, id, force);\n+      if (!result.deleted) {\n+        return result;\n+      }\n       setProviders((prev) => {\n         if (!prev || !prev[provider]) return prev;\n \n@@ -202,8 +214,8 @@ export default function CredentialsProvider({\n                 createAPIKeyCredentials: (\n                   credentials: APIKeyCredentialsCreatable,\n                 ) => createAPIKeyCredentials(provider, credentials),\n-                deleteCredentials: (id: string) =>\n-                  deleteCredentials(provider, id),\n+                deleteCredentials: (id: string, force: boolean = false) =>\n+                  deleteCredentials(provider, id, force),\n               },\n             }));\n           });\ndiff --git a/autogpt_platform/frontend/src/components/node-input-components.tsx b/autogpt_platform/frontend/src/components/node-input-components.tsx\nindex 9eeb90bbfb52..b93a894b9e92 100644\n--- a/autogpt_platform/frontend/src/components/node-input-components.tsx\n+++ b/autogpt_platform/frontend/src/components/node-input-components.tsx\n@@ -20,6 +20,14 @@ import {\n   SelectTrigger,\n   SelectValue,\n } from \"./ui/select\";\n+import {\n+  MultiSelector,\n+  MultiSelectorContent,\n+  MultiSelectorInput,\n+  MultiSelectorItem,\n+  MultiSelectorList,\n+  MultiSelectorTrigger,\n+} from \"./ui/multiselect\";\n import { LocalValuedInput } from \"./ui/input\";\n import NodeHandle from \"./NodeHandle\";\n import { ConnectionData } from \"./CustomNode\";\n@@ -133,6 +141,37 @@ export const NodeGenericInputField: FC<{\n   }\n \n   if (\"properties\" in propSchema) {\n+    // Render a multi-select for all-boolean sub-schemas with more than 3 properties\n+    if (\n+      Object.values(propSchema.properties).every(\n+        (subSchema) => \"type\" in subSchema && subSchema.type == \"boolean\",\n+      ) &&\n+      Object.keys(propSchema.properties).length >= 3\n+    ) {\n+      const options = Object.keys(propSchema.properties);\n+      const selectedKeys = Object.entries(currentValue || {})\n+        .filter(([_, v]) => v)\n+        .map(([k, _]) => k);\n+      return (\n+        <NodeMultiSelectInput\n+          selfKey={propKey}\n+          schema={propSchema}\n+          selection={selectedKeys}\n+          error={errors[propKey]}\n+          className={className}\n+          displayName={displayName}\n+          handleInputChange={(key, selection) => {\n+            handleInputChange(\n+              key,\n+              Object.fromEntries(\n+                options.map((option) => [option, selection.includes(option)]),\n+              ),\n+            );\n+          }}\n+        />\n+      );\n+    }\n+\n     return (\n       <NodeObjectInputTree\n         nodeId={nodeId}\n@@ -595,6 +634,56 @@ const NodeArrayInput: FC<{\n   );\n };\n \n+const NodeMultiSelectInput: FC<{\n+  selfKey: string;\n+  schema: BlockIOObjectSubSchema; // TODO: Support BlockIOArraySubSchema\n+  selection?: string[];\n+  error?: string;\n+  className?: string;\n+  displayName?: string;\n+  handleInputChange: NodeObjectInputTreeProps[\"handleInputChange\"];\n+}> = ({\n+  selfKey,\n+  schema,\n+  selection = [],\n+  error,\n+  className,\n+  displayName,\n+  handleInputChange,\n+}) => {\n+  const options = Object.keys(schema.properties);\n+\n+  return (\n+    <div className={cn(\"flex flex-col\", className)}>\n+      <MultiSelector\n+        className=\"nodrag\"\n+        values={selection}\n+        onValuesChange={(v) => handleInputChange(selfKey, v)}\n+      >\n+        <MultiSelectorTrigger>\n+          <MultiSelectorInput\n+            placeholder={\n+              schema.placeholder ?? `Select ${displayName || schema.title}...`\n+            }\n+          />\n+        </MultiSelectorTrigger>\n+        <MultiSelectorContent className=\"nowheel\">\n+          <MultiSelectorList>\n+            {options\n+              .map((key) => ({ ...schema.properties[key], key }))\n+              .map(({ key, title, description }) => (\n+                <MultiSelectorItem key={key} value={key} title={description}>\n+                  {title ?? key}\n+                </MultiSelectorItem>\n+              ))}\n+          </MultiSelectorList>\n+        </MultiSelectorContent>\n+      </MultiSelector>\n+      {error && <span className=\"error-message\">{error}</span>}\n+    </div>\n+  );\n+};\n+\n const NodeStringInput: FC<{\n   selfKey: string;\n   schema: BlockIOStringSubSchema;\n@@ -783,7 +872,7 @@ const NodeBooleanInput: FC<{\n           defaultChecked={value}\n           onCheckedChange={(v) => handleInputChange(selfKey, v)}\n         />\n-        <span className=\"ml-3\">{displayName}</span>\n+        {displayName && <span className=\"ml-3\">{displayName}</span>}\n       </div>\n       {error && <span className=\"error-message\">{error}</span>}\n     </div>\ndiff --git a/autogpt_platform/frontend/src/components/ui/alert-dialog.tsx b/autogpt_platform/frontend/src/components/ui/alert-dialog.tsx\nnew file mode 100644\nindex 000000000000..41adf9a224f3\n--- /dev/null\n+++ b/autogpt_platform/frontend/src/components/ui/alert-dialog.tsx\n@@ -0,0 +1,143 @@\n+\"use client\";\n+\n+import * as React from \"react\";\n+import * as AlertDialogPrimitive from \"@radix-ui/react-alert-dialog\";\n+\n+import { cn } from \"@/lib/utils\";\n+import { buttonVariants } from \"@/components/ui/button\";\n+import { VariantProps } from \"class-variance-authority\";\n+\n+const AlertDialog = AlertDialogPrimitive.Root;\n+\n+const AlertDialogTrigger = AlertDialogPrimitive.Trigger;\n+\n+const AlertDialogPortal = AlertDialogPrimitive.Portal;\n+\n+const AlertDialogOverlay = React.forwardRef<\n+  React.ElementRef<typeof AlertDialogPrimitive.Overlay>,\n+  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Overlay>\n+>(({ className, ...props }, ref) => (\n+  <AlertDialogPrimitive.Overlay\n+    className={cn(\n+      \"fixed inset-0 z-50 bg-black/80 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0\",\n+      className,\n+    )}\n+    {...props}\n+    ref={ref}\n+  />\n+));\n+AlertDialogOverlay.displayName = AlertDialogPrimitive.Overlay.displayName;\n+\n+const AlertDialogContent = React.forwardRef<\n+  React.ElementRef<typeof AlertDialogPrimitive.Content>,\n+  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Content>\n+>(({ className, ...props }, ref) => (\n+  <AlertDialogPortal>\n+    <AlertDialogOverlay />\n+    <AlertDialogPrimitive.Content\n+      ref={ref}\n+      className={cn(\n+        \"fixed left-[50%] top-[50%] z-50 grid w-full max-w-lg translate-x-[-50%] translate-y-[-50%] gap-4 border border-neutral-200 bg-white p-6 shadow-lg duration-200 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[state=closed]:slide-out-to-left-1/2 data-[state=closed]:slide-out-to-top-[48%] data-[state=open]:slide-in-from-left-1/2 data-[state=open]:slide-in-from-top-[48%] dark:border-neutral-800 dark:bg-neutral-950 sm:rounded-lg\",\n+        className,\n+      )}\n+      {...props}\n+    />\n+  </AlertDialogPortal>\n+));\n+AlertDialogContent.displayName = AlertDialogPrimitive.Content.displayName;\n+\n+const AlertDialogHeader = ({\n+  className,\n+  ...props\n+}: React.HTMLAttributes<HTMLDivElement>) => (\n+  <div\n+    className={cn(\n+      \"flex flex-col space-y-2 text-center sm:text-left\",\n+      className,\n+    )}\n+    {...props}\n+  />\n+);\n+AlertDialogHeader.displayName = \"AlertDialogHeader\";\n+\n+const AlertDialogFooter = ({\n+  className,\n+  ...props\n+}: React.HTMLAttributes<HTMLDivElement>) => (\n+  <div\n+    className={cn(\n+      \"flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2\",\n+      className,\n+    )}\n+    {...props}\n+  />\n+);\n+AlertDialogFooter.displayName = \"AlertDialogFooter\";\n+\n+const AlertDialogTitle = React.forwardRef<\n+  React.ElementRef<typeof AlertDialogPrimitive.Title>,\n+  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Title>\n+>(({ className, ...props }, ref) => (\n+  <AlertDialogPrimitive.Title\n+    ref={ref}\n+    className={cn(\"text-lg font-semibold\", className)}\n+    {...props}\n+  />\n+));\n+AlertDialogTitle.displayName = AlertDialogPrimitive.Title.displayName;\n+\n+const AlertDialogDescription = React.forwardRef<\n+  React.ElementRef<typeof AlertDialogPrimitive.Description>,\n+  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Description>\n+>(({ className, ...props }, ref) => (\n+  <AlertDialogPrimitive.Description\n+    ref={ref}\n+    className={cn(\"text-sm text-neutral-500 dark:text-neutral-400\", className)}\n+    {...props}\n+  />\n+));\n+AlertDialogDescription.displayName =\n+  AlertDialogPrimitive.Description.displayName;\n+\n+const AlertDialogAction = React.forwardRef<\n+  React.ElementRef<typeof AlertDialogPrimitive.Action>,\n+  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Action> &\n+    VariantProps<typeof buttonVariants>\n+>(({ className, variant, ...props }, ref) => (\n+  <AlertDialogPrimitive.Action\n+    ref={ref}\n+    className={cn(buttonVariants({ variant: variant }), className)}\n+    {...props}\n+  />\n+));\n+AlertDialogAction.displayName = AlertDialogPrimitive.Action.displayName;\n+\n+const AlertDialogCancel = React.forwardRef<\n+  React.ElementRef<typeof AlertDialogPrimitive.Cancel>,\n+  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Cancel>\n+>(({ className, ...props }, ref) => (\n+  <AlertDialogPrimitive.Cancel\n+    ref={ref}\n+    className={cn(\n+      buttonVariants({ variant: \"outline\" }),\n+      \"mt-2 sm:mt-0\",\n+      className,\n+    )}\n+    {...props}\n+  />\n+));\n+AlertDialogCancel.displayName = AlertDialogPrimitive.Cancel.displayName;\n+\n+export {\n+  AlertDialog,\n+  AlertDialogPortal,\n+  AlertDialogOverlay,\n+  AlertDialogTrigger,\n+  AlertDialogContent,\n+  AlertDialogHeader,\n+  AlertDialogFooter,\n+  AlertDialogTitle,\n+  AlertDialogDescription,\n+  AlertDialogAction,\n+  AlertDialogCancel,\n+};\ndiff --git a/autogpt_platform/frontend/src/components/ui/multiselect.tsx b/autogpt_platform/frontend/src/components/ui/multiselect.tsx\nindex 5400688b05b8..876444262194 100644\n--- a/autogpt_platform/frontend/src/components/ui/multiselect.tsx\n+++ b/autogpt_platform/frontend/src/components/ui/multiselect.tsx\n@@ -144,7 +144,7 @@ const MultiSelector = forwardRef<HTMLDivElement, MultiSelectorProps>(\n           ref={ref}\n           onKeyDown={handleKeyDown}\n           className={cn(\n-            \"flex flex-col space-y-2 overflow-visible bg-transparent\",\n+            \"flex flex-col overflow-visible bg-transparent\",\n             className,\n           )}\n           dir={dir}\n@@ -174,7 +174,7 @@ const MultiSelectorTrigger = forwardRef<\n     <div\n       ref={ref}\n       className={cn(\n-        \"flex flex-wrap gap-1 rounded-lg border border-muted bg-background p-1 py-2\",\n+        \"agpt-border-input agpt-shadow-input flex flex-wrap gap-1 rounded-lg bg-background px-3 py-2 pl-1 text-sm\",\n         className,\n       )}\n       {...props}\n@@ -183,7 +183,7 @@ const MultiSelectorTrigger = forwardRef<\n         <Badge\n           key={item}\n           className={cn(\n-            \"flex items-center gap-1 rounded-xl px-1\",\n+            \"flex items-center gap-1 rounded-xl px-1 pl-2\",\n             activeIndex === index && \"ring-2 ring-muted-foreground\",\n           )}\n           variant={\"secondary\"}\n@@ -237,10 +237,10 @@ MultiSelectorInput.displayName = \"MultiSelectorInput\";\n const MultiSelectorContent = forwardRef<\n   HTMLDivElement,\n   React.HTMLAttributes<HTMLDivElement>\n->(({ children }, ref) => {\n+>(({ children, className }, ref) => {\n   const { open } = useMultiSelect();\n   return (\n-    <div ref={ref} className=\"relative\">\n+    <div ref={ref} className={cn(\"relative mt-2\", className)}>\n       {open && children}\n     </div>\n   );\ndiff --git a/autogpt_platform/frontend/src/lib/autogpt-server-api/baseClient.ts b/autogpt_platform/frontend/src/lib/autogpt-server-api/baseClient.ts\nindex da3e77f1c41f..eac9939abdc7 100644\n--- a/autogpt_platform/frontend/src/lib/autogpt-server-api/baseClient.ts\n+++ b/autogpt_platform/frontend/src/lib/autogpt-server-api/baseClient.ts\n@@ -1,24 +1,24 @@\n import { SupabaseClient } from \"@supabase/supabase-js\";\n import {\n-  AnalyticsMetrics,\n   AnalyticsDetails,\n+  AnalyticsMetrics,\n   APIKeyCredentials,\n   Block,\n+  CredentialsDeleteNeedConfirmationResponse,\n   CredentialsDeleteResponse,\n   CredentialsMetaResponse,\n+  ExecutionMeta,\n   Graph,\n   GraphCreatable,\n-  GraphUpdateable,\n+  GraphExecuteResponse,\n   GraphMeta,\n   GraphMetaWithRuns,\n-  GraphExecuteResponse,\n-  ExecutionMeta,\n+  GraphUpdateable,\n   NodeExecutionResult,\n   OAuth2Credentials,\n-  User,\n-  ScheduleCreatable,\n-  ScheduleUpdateable,\n   Schedule,\n+  ScheduleCreatable,\n+  User,\n } from \"./types\";\n \n export default class BaseAutoGPTServerAPI {\n@@ -226,10 +226,14 @@ export default class BaseAutoGPTServerAPI {\n   deleteCredentials(\n     provider: string,\n     id: string,\n-  ): Promise<CredentialsDeleteResponse> {\n+    force: boolean = true,\n+  ): Promise<\n+    CredentialsDeleteResponse | CredentialsDeleteNeedConfirmationResponse\n+  > {\n     return this._request(\n       \"DELETE\",\n       `/integrations/${provider}/credentials/${id}`,\n+      force ? { force: true } : undefined,\n     );\n   }\n \n@@ -271,13 +275,14 @@ export default class BaseAutoGPTServerAPI {\n         ?.access_token || \"\";\n \n     let url = this.baseUrl + path;\n-    if (method === \"GET\" && payload) {\n+    const payloadAsQuery = [\"GET\", \"DELETE\"].includes(method);\n+    if (payloadAsQuery && payload) {\n       // For GET requests, use payload as query\n       const queryParams = new URLSearchParams(payload);\n       url += `?${queryParams.toString()}`;\n     }\n \n-    const hasRequestBody = method !== \"GET\" && payload !== undefined;\n+    const hasRequestBody = !payloadAsQuery && payload !== undefined;\n     const response = await fetch(url, {\n       method,\n       headers: {\ndiff --git a/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts b/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\nindex 0794cc8611c6..c63c192b7218 100644\n--- a/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\n+++ b/autogpt_platform/frontend/src/lib/autogpt-server-api/types.ts\n@@ -56,6 +56,7 @@ export type BlockIOSubSchemaMeta = {\n   description?: string;\n   placeholder?: string;\n   advanced?: boolean;\n+  hidden?: boolean;\n };\n \n export type BlockIOObjectSubSchema = BlockIOSubSchemaMeta & {\n@@ -271,6 +272,13 @@ export type CredentialsDeleteResponse = {\n   revoked: boolean | null;\n };\n \n+/* Mirror of backend/server/integrations/router.py:CredentialsDeletionNeedsConfirmationResponse */\n+export type CredentialsDeleteNeedConfirmationResponse = {\n+  deleted: false;\n+  need_confirmation: true;\n+  message: string;\n+};\n+\n /* Mirror of backend/data/model.py:CredentialsMetaInput */\n export type CredentialsMetaInput = {\n   id: string;\n@@ -317,6 +325,7 @@ export enum BlockUIType {\n   INPUT = \"Input\",\n   OUTPUT = \"Output\",\n   NOTE = \"Note\",\n+  WEBHOOK = \"Webhook\",\n   AGENT = \"Agent\",\n }\n \ndiff --git a/autogpt_platform/frontend/yarn.lock b/autogpt_platform/frontend/yarn.lock\nindex 2445587c6912..755dd45af421 100644\n--- a/autogpt_platform/frontend/yarn.lock\n+++ b/autogpt_platform/frontend/yarn.lock\n@@ -2100,6 +2100,18 @@\n   resolved \"https://registry.npmjs.org/@radix-ui/primitive/-/primitive-1.1.0.tgz\"\n   integrity sha512-4Z8dn6Upk0qk4P74xBhZ6Hd/w0mPEzOOLxy4xiPXOXqjF7jZS0VAKk7/x/H6FyY2zCkYJqePf1G5KmkmNJ4RBA==\n \n+\"@radix-ui/react-alert-dialog@^1.1.2\":\n+  version \"1.1.2\"\n+  resolved \"https://registry.yarnpkg.com/@radix-ui/react-alert-dialog/-/react-alert-dialog-1.1.2.tgz#ac3bb7f71f5cbb595d3d0949bb12b598c2a99981\"\n+  integrity sha512-eGSlLzPhKO+TErxkiGcCZGuvbVMnLA1MTnyBksGOeGRGkxHiiJUujsjmNTdWTm4iHVSRaUao9/4Ur671auMghQ==\n+  dependencies:\n+    \"@radix-ui/primitive\" \"1.1.0\"\n+    \"@radix-ui/react-compose-refs\" \"1.1.0\"\n+    \"@radix-ui/react-context\" \"1.1.1\"\n+    \"@radix-ui/react-dialog\" \"1.1.2\"\n+    \"@radix-ui/react-primitive\" \"2.0.0\"\n+    \"@radix-ui/react-slot\" \"1.1.0\"\n+\n \"@radix-ui/react-arrow@1.1.0\":\n   version \"1.1.0\"\n   resolved \"https://registry.npmjs.org/@radix-ui/react-arrow/-/react-arrow-1.1.0.tgz\"\n@@ -2182,7 +2194,7 @@\n   resolved \"https://registry.npmjs.org/@radix-ui/react-context/-/react-context-1.1.1.tgz\"\n   integrity sha512-UASk9zi+crv9WteK/NU4PLvOoL3OuE6BWVKNF6hPRBtYBDXQ2u5iu3O59zUlJiTVvkyuycnqrztsHVJwcK9K+Q==\n \n-\"@radix-ui/react-dialog@^1.1.2\":\n+\"@radix-ui/react-dialog@1.1.2\", \"@radix-ui/react-dialog@^1.1.2\":\n   version \"1.1.2\"\n   resolved \"https://registry.yarnpkg.com/@radix-ui/react-dialog/-/react-dialog-1.1.2.tgz#d9345575211d6f2d13e209e84aec9a8584b54d6c\"\n   integrity sha512-Yj4dZtqa2o+kG61fzB0H2qUvmwBA2oyQroGLyNtBj1beo1khoQ3q1a2AO8rrQYjd8256CO9+N8L9tvsS+bnIyA==\n@@ -2428,13 +2440,20 @@\n   dependencies:\n     \"@radix-ui/react-primitive\" \"2.0.0\"\n \n-\"@radix-ui/react-slot@1.1.0\", \"@radix-ui/react-slot@^1.1.0\":\n+\"@radix-ui/react-slot@1.1.0\":\n   version \"1.1.0\"\n   resolved \"https://registry.npmjs.org/@radix-ui/react-slot/-/react-slot-1.1.0.tgz\"\n   integrity sha512-FUCf5XMfmW4dtYl69pdS4DbxKy8nj4M7SafBgPllysxmdachynNflAdp/gCsnYWNDnge6tI9onzMp5ARYc1KNw==\n   dependencies:\n     \"@radix-ui/react-compose-refs\" \"1.1.0\"\n \n+\"@radix-ui/react-slot@^1.1.0\":\n+  version \"1.1.0\"\n+  resolved \"https://registry.yarnpkg.com/@radix-ui/react-slot/-/react-slot-1.1.0.tgz#7c5e48c36ef5496d97b08f1357bb26ed7c714b84\"\n+  integrity sha512-FUCf5XMfmW4dtYl69pdS4DbxKy8nj4M7SafBgPllysxmdachynNflAdp/gCsnYWNDnge6tI9onzMp5ARYc1KNw==\n+  dependencies:\n+    \"@radix-ui/react-compose-refs\" \"1.1.0\"\n+\n \"@radix-ui/react-switch@^1.1.1\":\n   version \"1.1.1\"\n   resolved \"https://registry.yarnpkg.com/@radix-ui/react-switch/-/react-switch-1.1.1.tgz#1401658c24d66a18610f18793afbaa7fedf5429a\"\ndiff --git a/docs/content/platform/new_blocks.md b/docs/content/platform/new_blocks.md\nindex 4ef48ea738bd..faad5e8a9d78 100644\n--- a/docs/content/platform/new_blocks.md\n+++ b/docs/content/platform/new_blocks.md\n@@ -83,7 +83,7 @@ Follow these steps to create and test a new block:\n \n      In this case, we're mocking the `get_request` method to always return a dictionary with an 'extract' key, simulating a successful API response. This allows us to test the block's logic without making actual network requests, which could be slow, unreliable, or rate-limited.\n \n-5. **Implement the `run` method with error handling:**, this should contain the main logic of the block:\n+5. **Implement the `run` method with error handling.** This should contain the main logic of the block:\n \n    ```python\n    def run(self, input_data: Input, **kwargs) -> BlockOutput:\n@@ -234,7 +234,7 @@ All our existing handlers and the base class can be found [here][OAuth2 handlers\n \n Every handler must implement the following parts of the [`BaseOAuthHandler`] interface:\n \n-```python title=\"autogpt_platform/backend/backend/integrations/oauth/base.py\"\n+```python title=\"backend/integrations/oauth/base.py\"\n --8<-- \"autogpt_platform/backend/backend/integrations/oauth/base.py:BaseOAuthHandler1\"\n --8<-- \"autogpt_platform/backend/backend/integrations/oauth/base.py:BaseOAuthHandler2\"\n --8<-- \"autogpt_platform/backend/backend/integrations/oauth/base.py:BaseOAuthHandler3\"\n@@ -249,13 +249,13 @@ Aside from implementing the `OAuthHandler` itself, adding a handler into the sys\n \n - Adding the handler class to `HANDLERS_BY_NAME` under [`integrations/oauth/__init__.py`](https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpt_platform/backend/backend/integrations/oauth/__init__.py)\n \n-```python title=\"autogpt_platform/backend/backend/integrations/oauth/__init__.py\"\n+```python title=\"backend/integrations/oauth/__init__.py\"\n --8<-- \"autogpt_platform/backend/backend/integrations/oauth/__init__.py:HANDLERS_BY_NAMEExample\"\n ```\n \n - Adding `{provider}_client_id` and `{provider}_client_secret` to the application's `Secrets` under [`util/settings.py`](https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpt_platform/backend/backend/util/settings.py)\n \n-```python title=\"autogpt_platform/backend/backend/util/settings.py\"\n+```python title=\"backend/util/settings.py\"\n --8<-- \"autogpt_platform/backend/backend/util/settings.py:OAuthServerCredentialsExample\"\n ```\n \n@@ -286,13 +286,13 @@ Finally you will need to add the provider to the `CredentialsType` enum in [`fro\n \n - GitHub blocks with API key + OAuth2 support: [`blocks/github`](https://github.com/Significant-Gravitas/AutoGPT/tree/master/autogpt_platform/backend/backend/blocks/github/)\n \n-```python title=\"blocks/github/issues.py\"\n+```python title=\"backend/blocks/github/issues.py\"\n --8<-- \"autogpt_platform/backend/backend/blocks/github/issues.py:GithubCommentBlockExample\"\n ```\n \n - GitHub OAuth2 handler: [`integrations/oauth/github.py`](https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpt_platform/backend/backend/integrations/oauth/github.py)\n \n-```python title=\"blocks/github/github.py\"\n+```python title=\"backend/integrations/oauth/github.py\"\n --8<-- \"autogpt_platform/backend/backend/integrations/oauth/github.py:GithubOAuthHandlerExample\"\n ```\n \n@@ -300,18 +300,148 @@ Finally you will need to add the provider to the `CredentialsType` enum in [`fro\n \n - Google OAuth2 handler: [`integrations/oauth/google.py`](https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpt_platform/backend/backend/integrations/oauth/google.py)\n \n-```python title=\"integrations/oauth/google.py\"\n+```python title=\"backend/integrations/oauth/google.py\"\n --8<-- \"autogpt_platform/backend/backend/integrations/oauth/google.py:GoogleOAuthHandlerExample\"\n ```\n \n You can see that google has defined a `DEFAULT_SCOPES` variable, this is used to set the scopes that are requested no matter what the user asks for.\n \n-```python title=\"blocks/google/_auth.py\"\n+```python title=\"backend/blocks/google/_auth.py\"\n --8<-- \"autogpt_platform/backend/backend/blocks/google/_auth.py:GoogleOAuthIsConfigured\"\n ```\n \n You can also see that `GOOGLE_OAUTH_IS_CONFIGURED` is used to disable the blocks that require OAuth if the oauth is not configured. This is in the `__init__` method of each block. This is because there is no api key fallback for google blocks so we need to make sure that the oauth is configured before we allow the user to use the blocks.\n \n+### Webhook-triggered Blocks\n+\n+Webhook-triggered blocks allow your agent to respond to external events in real-time.\n+These blocks are triggered by incoming webhooks from third-party services\n+rather than being executed manually.\n+\n+Creating and running a webhook-triggered block involves three main components:\n+\n+- The block itself, which specifies:\n+    - Inputs for the user to select a resource and events to subscribe to\n+    - A `credentials` input with the scopes needed to manage webhooks\n+    - Logic to turn the webhook payload into outputs for the webhook block\n+- The `WebhooksManager` for the corresponding webhook service provider, which handles:\n+    - (De)registering webhooks with the provider\n+    - Parsing and validating incoming webhook payloads\n+- The credentials system for the corresponding service provider, which may include an `OAuthHandler`\n+\n+There is more going on under the hood, e.g. to store and retrieve webhooks and their\n+links to nodes, but to add a webhook-triggered block you shouldn't need to make changes\n+to those parts of the system.\n+\n+#### Creating a Webhook-triggered Block\n+\n+To create a webhook-triggered block, follow these additional steps on top of the basic block creation process:\n+\n+1. **Define `webhook_config`** in your block's `__init__` method.\n+\n+    <details>\n+    <summary>Example: <code>GitHubPullRequestTriggerBlock</code></summary>\n+\n+    ```python title=\"backend/blocks/github/triggers.py\"\n+    --8<-- \"autogpt_platform/backend/backend/blocks/github/triggers.py:example-webhook_config\"\n+    ```\n+    </details>\n+\n+    <details>\n+    <summary><code>BlockWebhookConfig</code> definition</summary>\n+\n+    ```python title=\"backend/data/block.py\"\n+    --8<-- \"autogpt_platform/backend/backend/data/block.py:BlockWebhookConfig\"\n+    ```\n+    </details>\n+\n+2. **Define event filter input** in your block's Input schema.\n+    This allows the user to select which specific types of events will trigger the block in their agent.\n+\n+    <details>\n+    <summary>Example: <code>GitHubPullRequestTriggerBlock</code></summary>\n+\n+    ```python title=\"backend/blocks/github/triggers.py\"\n+    --8<-- \"autogpt_platform/backend/backend/blocks/github/triggers.py:example-event-filter\"\n+    ```\n+    </details>\n+\n+    - The name of the input field (`events` in this case) must match `webhook_config.event_filter_input`.\n+    - The event filter itself must be a Pydantic model with only boolean fields.\n+\n+4. **Include payload field** in your block's Input schema.\n+\n+    <details>\n+    <summary>Example: <code>GitHubTriggerBase</code></summary>\n+\n+    ```python title=\"backend/blocks/github/triggers.py\"\n+    --8<-- \"autogpt_platform/backend/backend/blocks/github/triggers.py:example-payload-field\"\n+    ```\n+    </details>\n+\n+5. **Define `credentials` input** in your block's Input schema.\n+    - Its scopes must be sufficient to manage a user's webhooks through the provider's API\n+    - See [Blocks with authentication](#blocks-with-authentication) for further details\n+\n+6. **Process webhook payload** and output relevant parts of it in your block's `run` method.\n+\n+    <details>\n+    <summary>Example: <code>GitHubPullRequestTriggerBlock</code></summary>\n+\n+    ```python\n+    def run(self, input_data: Input, **kwargs) -> BlockOutput:\n+        yield \"payload\", input_data.payload\n+        yield \"sender\", input_data.payload[\"sender\"]\n+        yield \"event\", input_data.payload[\"action\"]\n+        yield \"number\", input_data.payload[\"number\"]\n+        yield \"pull_request\", input_data.payload[\"pull_request\"]\n+    ```\n+\n+    Note that the `credentials` parameter can be omitted if the credentials\n+    aren't used at block runtime, like in the example.\n+    </details>\n+\n+#### Adding a Webhooks Manager\n+\n+To add support for a new webhook provider, you'll need to create a WebhooksManager that implements the `BaseWebhooksManager` interface:\n+\n+```python title=\"backend/integrations/webhooks/base.py\"\n+--8<-- \"autogpt_platform/backend/backend/integrations/webhooks/base.py:BaseWebhooksManager1\"\n+\n+--8<-- \"autogpt_platform/backend/backend/integrations/webhooks/base.py:BaseWebhooksManager2\"\n+--8<-- \"autogpt_platform/backend/backend/integrations/webhooks/base.py:BaseWebhooksManager3\"\n+--8<-- \"autogpt_platform/backend/backend/integrations/webhooks/base.py:BaseWebhooksManager4\"\n+--8<-- \"autogpt_platform/backend/backend/integrations/webhooks/base.py:BaseWebhooksManager5\"\n+```\n+\n+And add a reference to your `WebhooksManager` class in `WEBHOOK_MANAGERS_BY_NAME`:\n+\n+```python title=\"backend/integrations/webhooks/__init__.py\"\n+--8<-- \"autogpt_platform/backend/backend/integrations/webhooks/__init__.py:WEBHOOK_MANAGERS_BY_NAME\"\n+```\n+\n+#### Example: GitHub Webhook Integration\n+\n+<details>\n+<summary>\n+GitHub Webhook triggers: <a href=\"https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpt_platform/backend/backend/blocks/github/triggers.py\"><code>blocks/github/triggers.py</code></a>\n+</summary>\n+\n+```python title=\"backend/blocks/github/triggers.py\"\n+--8<-- \"autogpt_platform/backend/backend/blocks/github/triggers.py:GithubTriggerExample\"\n+```\n+</details>\n+\n+<details>\n+<summary>\n+GitHub Webhooks Manager: <a href=\"https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpt_platform/backend/backend/integrations/webhooks/github.py\"><code>integrations/webhooks/github.py</code></a>\n+</summary>\n+\n+```python title=\"backend/integrations/webhooks/github.py\"\n+--8<-- \"autogpt_platform/backend/backend/integrations/webhooks/github.py:GithubWebhooksManager\"\n+```\n+</details>\n+\n ## Key Points to Remember\n \n - **Unique ID**: Give your block a unique ID in the **init** method.\n"
  },
  {
    "index": 5,
    "filtered_comments": [
      "Awesome, @josephfrazier!\n\nWell, I feel I can push this even further   As you're in the process of fixing these issues and changes haven't yet made into master, what would you say if I suggested you to remove a couple of commits and squash those which are logically similar/related?\n\nCommit 85a3e34 is a revert of 4d9f177, both could be removed. Also, all EXYZ-related commits could be joined together, leaving one single commit per violation E123, E225, E231, E265, E302, E402, E711 and E731.\n\nWhat do you think about getting a cleaner history?\n",
      "@nvbn, do you have any thoughts on this? Is there anything else that needs to be done to land it (other than fixing files that have been changed on the `master` branch)?",
      "@josephfrazier, ooops, sorry, I just forgot about this pr.\r\n\r\nI'll check it in the nearest time and will merge it.\r\n\r\nThansk.",
      "Great, thanks! I just pushed some changes that fix the files that had been updated on the master branch, but haven't rebased them yet. If everything looks good to you, let me know and I can `git rebase -i --autosquash master` to combine the `fixup!` commits with their referents and get rid of that `Merge branch 'master' into flake8` commit.",
      "Cheers, hopefully this will make it easier to ensure future contributions adhere to the code style."
    ],
    "code_diff": "diff --git a/.travis.yml b/.travis.yml\nindex 2798eed99..990a7f004 100644\n--- a/.travis.yml\n+++ b/.travis.yml\n@@ -42,6 +42,7 @@ install:\n   - python setup.py develop\n   - rm -rf build\n script:\n+  - flake8\n   - export COVERAGE_PYTHON_VERSION=python-${TRAVIS_PYTHON_VERSION:0:1}\n   - export RUN_TESTS=\"coverage run --source=thefuck,tests -m py.test -v --capture=sys tests\"\n   - if [[ $TRAVIS_PYTHON_VERSION == 3.6 && $TRAVIS_OS_NAME != \"osx\" ]]; then $RUN_TESTS --enable-functional; fi\ndiff --git a/README.md b/README.md\nindex 3e84a35a3..530e78627 100644\n--- a/README.md\n+++ b/README.md\n@@ -387,6 +387,12 @@ pip install -r requirements.txt\n python setup.py develop\n ```\n \n+Run code style checks:\n+\n+```bash\n+flake8\n+```\n+\n Run unit tests:\n \n ```bash\ndiff --git a/appveyor.yml b/appveyor.yml\nindex 005e0d1aa..96da08c93 100644\n--- a/appveyor.yml\n+++ b/appveyor.yml\n@@ -20,4 +20,5 @@ install:\n   - \"%PYTHON%/Scripts/pip.exe install -U -r requirements.txt\"\n \n test_script:\n+  - \"%PYTHON%/python.exe -m flake8\"\n   - \"%PYTHON%/Scripts/py.test.exe -sv\"\ndiff --git a/requirements.txt b/requirements.txt\nindex ca9a8cc80..c450e4376 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,4 +1,5 @@\n pip\n+flake8\n pytest\n mock\n pytest-mock\ndiff --git a/tests/rules/test_brew_link.py b/tests/rules/test_brew_link.py\nindex 14b13c7ab..0586574cd 100644\n--- a/tests/rules/test_brew_link.py\n+++ b/tests/rules/test_brew_link.py\n@@ -6,15 +6,15 @@\n @pytest.fixture\n def stderr():\n     return (\"Error: Could not symlink bin/gcp\\n\"\n-\t\t\t\"Target /usr/local/bin/gcp\\n\"\n-\t\t\t\"already exists. You may want to remove it:\\n\"\n-\t\t\t  \"rm '/usr/local/bin/gcp'\\n\"\n-\t\t\t\"\\n\"\n-\t\t\t\"To force the link and overwrite all conflicting files:\\n\"\n-\t\t\t  \"brew link --overwrite coreutils\\n\"\n-\t\t\t\"\\n\"\n-\t\t\t\"To list all files that would be deleted:\\n\"\n-\t\t\t  \"brew link --overwrite --dry-run coreutils\\n\")\n+            \"Target /usr/local/bin/gcp\\n\"\n+            \"already exists. You may want to remove it:\\n\"\n+            \"  rm '/usr/local/bin/gcp'\\n\"\n+            \"\\n\"\n+            \"To force the link and overwrite all conflicting files:\\n\"\n+            \"  brew link --overwrite coreutils\\n\"\n+            \"\\n\"\n+            \"To list all files that would be deleted:\\n\"\n+            \"  brew link --overwrite --dry-run coreutils\\n\")\n \n \n @pytest.fixture\n@@ -29,7 +29,7 @@ def test_match(stderr, script):\n \n @pytest.mark.parametrize('script', ['brew link coreutils'])\n def test_not_match(script):\n-    stderr=''\n+    stderr = ''\n     assert not match(Command(script=script, stderr=stderr))\n \n \ndiff --git a/tests/rules/test_brew_uninstall.py b/tests/rules/test_brew_uninstall.py\nindex e01ffc1e0..cba0d424d 100644\n--- a/tests/rules/test_brew_uninstall.py\n+++ b/tests/rules/test_brew_uninstall.py\n@@ -22,7 +22,7 @@ def test_match(stdout, script):\n \n @pytest.mark.parametrize('script', ['brew remove gnuplot'])\n def test_not_match(script):\n-    stdout='Uninstalling /usr/local/Cellar/gnuplot/5.0.4_1... (44 files, 2.3M)\\n'\n+    stdout = 'Uninstalling /usr/local/Cellar/gnuplot/5.0.4_1... (44 files, 2.3M)\\n'\n     assert not match(Command(script=script, stdout=stdout))\n \n \ndiff --git a/tests/rules/test_brew_unknown_command.py b/tests/rules/test_brew_unknown_command.py\nindex 45e02f67d..2185b867d 100644\n--- a/tests/rules/test_brew_unknown_command.py\n+++ b/tests/rules/test_brew_unknown_command.py\n@@ -21,8 +21,8 @@ def test_match(brew_unknown_cmd):\n \n \n def test_get_new_command(brew_unknown_cmd, brew_unknown_cmd2):\n-    assert get_new_command(Command('brew inst', stderr=brew_unknown_cmd)) \\\n-           == ['brew list', 'brew install', 'brew uninstall']\n+    assert (get_new_command(Command('brew inst', stderr=brew_unknown_cmd))\n+            == ['brew list', 'brew install', 'brew uninstall'])\n \n     cmds = get_new_command(Command('brew instaa', stderr=brew_unknown_cmd2))\n     assert 'brew install' in cmds\ndiff --git a/tests/rules/test_composer_not_command.py b/tests/rules/test_composer_not_command.py\nindex c4e4a5a4d..717ee3ab4 100644\n--- a/tests/rules/test_composer_not_command.py\n+++ b/tests/rules/test_composer_not_command.py\n@@ -48,9 +48,9 @@ def test_match(composer_not_command, composer_not_command_one_of_this):\n \n \n def test_get_new_command(composer_not_command, composer_not_command_one_of_this):\n-    assert get_new_command(Command('composer udpate',\n-                                   stderr=composer_not_command)) \\\n-           == 'composer update'\n-    assert get_new_command(\n-        Command('composer pdate', stderr=composer_not_command_one_of_this)) \\\n-           == 'composer selfupdate'\n+    assert (get_new_command(Command('composer udpate',\n+                                    stderr=composer_not_command))\n+            == 'composer update')\n+    assert (get_new_command(Command('composer pdate',\n+                                    stderr=composer_not_command_one_of_this))\n+            == 'composer selfupdate')\ndiff --git a/tests/rules/test_dirty_untar.py b/tests/rules/test_dirty_untar.py\nindex b33a8900d..5308a4e5e 100644\n--- a/tests/rules/test_dirty_untar.py\n+++ b/tests/rules/test_dirty_untar.py\n@@ -2,7 +2,7 @@\n import pytest\n import tarfile\n from thefuck.rules.dirty_untar import match, get_new_command, side_effect, \\\n-                                      tar_extensions\n+                                      tar_extensions  # noqa: E126\n from tests.utils import Command\n \n \n@@ -33,6 +33,7 @@ def reset(path):\n \n     return fixture\n \n+\n parametrize_extensions = pytest.mark.parametrize('ext', tar_extensions)\n \n # (filename as typed by the user, unquoted filename, quoted filename as per shells.quote)\ndiff --git a/tests/rules/test_django_south_ghost.py b/tests/rules/test_django_south_ghost.py\nindex 836564673..bbc83fe51 100644\n--- a/tests/rules/test_django_south_ghost.py\n+++ b/tests/rules/test_django_south_ghost.py\n@@ -37,7 +37,7 @@ def stderr():\n  ! I'm not trusting myself; either fix this yourself by fiddling\n  ! with the south_migrationhistory table, or pass --delete-ghost-migrations\n  ! to South to have it delete ALL of these records (this may not be good).\n-'''\n+'''  # noqa\n \n \n def test_match(stderr):\ndiff --git a/tests/rules/test_django_south_merge.py b/tests/rules/test_django_south_merge.py\nindex 13f7dbafd..581a1cf3a 100644\n--- a/tests/rules/test_django_south_merge.py\n+++ b/tests/rules/test_django_south_merge.py\n@@ -39,5 +39,5 @@ def test_match(stderr):\n \n \n def test_get_new_command():\n-    assert get_new_command(Command('./manage.py migrate auth')) \\\n-           == './manage.py migrate auth --merge'\n+    assert (get_new_command(Command('./manage.py migrate auth'))\n+            == './manage.py migrate auth --merge')\ndiff --git a/tests/rules/test_fab_command_not_found.py b/tests/rules/test_fab_command_not_found.py\nindex 361c37420..da596b6b4 100644\n--- a/tests/rules/test_fab_command_not_found.py\n+++ b/tests/rules/test_fab_command_not_found.py\n@@ -45,5 +45,5 @@ def test_not_match(command):\n      'fab prepare_extension:version=2016 deploy:beta=true -H the.fuck'),\n ])\n def test_get_new_command(script, result):\n-    command = Command(script, stdout,stderr)\n+    command = Command(script, stdout, stderr)\n     assert get_new_command(command) == result\ndiff --git a/tests/rules/test_fix_alt_space.py b/tests/rules/test_fix_alt_space.py\nindex c5b03c509..abc0ed1f4 100644\n--- a/tests/rules/test_fix_alt_space.py\n+++ b/tests/rules/test_fix_alt_space.py\n@@ -18,5 +18,5 @@ def test_match():\n \n def test_get_new_command():\n     \"\"\" Replace the Alt+Space character by a simple space \"\"\"\n-    assert get_new_command(Command(u'ps -ef |grep foo'))\\\n-           == 'ps -ef | grep foo'\n+    assert (get_new_command(Command(u'ps -ef |grep foo'))\n+            == 'ps -ef | grep foo')\ndiff --git a/tests/rules/test_fix_file.py b/tests/rules/test_fix_file.py\nindex 30f5fa1dc..70921cb87 100644\n--- a/tests/rules/test_fix_file.py\n+++ b/tests/rules/test_fix_file.py\n@@ -191,7 +191,7 @@ def test_get_new_command(monkeypatch, test):\n \n /home/thefuck/tests/rules/test_fix_file.py:218: NameError\n \"\"\", ''),\n-)\n+)  # noqa\n \n \n @pytest.mark.parametrize('test', tests)\n@@ -227,10 +227,6 @@ def test_get_new_command(mocker, monkeypatch, test):\n     mocker.patch('os.path.isfile', return_value=True)\n     monkeypatch.setenv('EDITOR', 'dummy_editor')\n \n-    cmd = Command(script=test[0], stdout=test[4], stderr=test[5])\n-    #assert (get_new_command(cmd, Settings({})) ==\n-    #    'dummy_editor {} +{} && {}'.format(test[1], test[2], test[0]))\n-\n \n @pytest.mark.parametrize('test', tests)\n @pytest.mark.usefixtures('no_memoize')\n@@ -243,7 +239,7 @@ def test_get_new_command_with_settings(mocker, monkeypatch, test, settings):\n \n     if test[3]:\n         assert (get_new_command(cmd) ==\n-            u'dummy_editor {} +{}:{} && {}'.format(test[1], test[2], test[3], test[0]))\n+                u'dummy_editor {} +{}:{} && {}'.format(test[1], test[2], test[3], test[0]))\n     else:\n         assert (get_new_command(cmd) ==\n-            u'dummy_editor {} +{} && {}'.format(test[1], test[2], test[0]))\n+                u'dummy_editor {} +{} && {}'.format(test[1], test[2], test[0]))\ndiff --git a/tests/rules/test_git_add_force.py b/tests/rules/test_git_add_force.py\nindex abe2bd79b..d6d3dd7c1 100644\n--- a/tests/rules/test_git_add_force.py\n+++ b/tests/rules/test_git_add_force.py\n@@ -18,5 +18,5 @@ def test_match(stderr):\n \n \n def test_get_new_command(stderr):\n-    assert get_new_command(Command('git add dist/*.js', stderr=stderr)) \\\n-           == \"git add --force dist/*.js\"\n+    assert (get_new_command(Command('git add dist/*.js', stderr=stderr))\n+            == \"git add --force dist/*.js\")\ndiff --git a/tests/rules/test_git_not_command.py b/tests/rules/test_git_not_command.py\nindex 6257767e9..0c3c06f7a 100644\n--- a/tests/rules/test_git_not_command.py\n+++ b/tests/rules/test_git_not_command.py\n@@ -49,9 +49,9 @@ def test_match(git_not_command, git_command, git_not_command_one_of_this):\n \n def test_get_new_command(git_not_command, git_not_command_one_of_this,\n                          git_not_command_closest):\n-    assert get_new_command(Command('git brnch', stderr=git_not_command)) \\\n-           == ['git branch']\n-    assert get_new_command(Command('git st', stderr=git_not_command_one_of_this)) \\\n-           == ['git stats', 'git stash', 'git stage']\n-    assert get_new_command(Command('git tags', stderr=git_not_command_closest)) \\\n-           == ['git tag', 'git stage']\n+    assert (get_new_command(Command('git brnch', stderr=git_not_command))\n+            == ['git branch'])\n+    assert (get_new_command(Command('git st', stderr=git_not_command_one_of_this))\n+            == ['git stats', 'git stash', 'git stage'])\n+    assert (get_new_command(Command('git tags', stderr=git_not_command_closest))\n+            == ['git tag', 'git stage'])\ndiff --git a/tests/rules/test_git_pull.py b/tests/rules/test_git_pull.py\nindex 11c50171d..d24e400c2 100644\n--- a/tests/rules/test_git_pull.py\n+++ b/tests/rules/test_git_pull.py\n@@ -25,5 +25,5 @@ def test_match(stderr):\n \n \n def test_get_new_command(stderr):\n-    assert get_new_command(Command('git pull', stderr=stderr)) \\\n-           == \"git branch --set-upstream-to=origin/master master && git pull\"\n+    assert (get_new_command(Command('git pull', stderr=stderr))\n+            == \"git branch --set-upstream-to=origin/master master && git pull\")\ndiff --git a/tests/rules/test_git_pull_uncommitted_changes.py b/tests/rules/test_git_pull_uncommitted_changes.py\nindex 480b31d8b..e32a73704 100644\n--- a/tests/rules/test_git_pull_uncommitted_changes.py\n+++ b/tests/rules/test_git_pull_uncommitted_changes.py\n@@ -15,5 +15,5 @@ def test_match(stderr):\n \n \n def test_get_new_command(stderr):\n-    assert get_new_command(Command('git pull', stderr=stderr)) \\\n-           == \"git stash && git pull && git stash pop\"\n+    assert (get_new_command(Command('git pull', stderr=stderr))\n+            == \"git stash && git pull && git stash pop\")\ndiff --git a/tests/rules/test_git_pull_unstaged_changes.py b/tests/rules/test_git_pull_unstaged_changes.py\nindex a8cbb4c37..ef3d94eab 100644\n--- a/tests/rules/test_git_pull_unstaged_changes.py\n+++ b/tests/rules/test_git_pull_unstaged_changes.py\n@@ -15,5 +15,5 @@ def test_match(stderr):\n \n \n def test_get_new_command(stderr):\n-    assert get_new_command(Command('git pull', stderr=stderr)) \\\n-           == \"git stash && git pull && git stash pop\"\n+    assert (get_new_command(Command('git pull', stderr=stderr))\n+            == \"git stash && git pull && git stash pop\")\ndiff --git a/tests/rules/test_git_push_pull.py b/tests/rules/test_git_push_pull.py\nindex afa74748e..93e41bc2b 100644\n--- a/tests/rules/test_git_push_pull.py\n+++ b/tests/rules/test_git_push_pull.py\n@@ -39,12 +39,7 @@\n @pytest.mark.parametrize('command', [\n     Command(script='git push', stderr=git_err),\n     Command(script='git push nvbn', stderr=git_err),\n-    Command(script='git push nvbn master', stderr=git_err)])\n-def test_match(command):\n-    assert match(command)\n-\n-\n-@pytest.mark.parametrize('command', [\n+    Command(script='git push nvbn master', stderr=git_err),\n     Command(script='git push', stderr=git_err2),\n     Command(script='git push nvbn', stderr=git_err2),\n     Command(script='git push nvbn master', stderr=git_err2)])\n@@ -68,12 +63,7 @@ def test_not_match(command):\n     (Command(script='git push nvbn', stderr=git_err),\n      'git pull nvbn && git push nvbn'),\n     (Command(script='git push nvbn master', stderr=git_err),\n-     'git pull nvbn master && git push nvbn master')])\n-def test_get_new_command(command, output):\n-    assert get_new_command(command) == output\n-\n-\n-@pytest.mark.parametrize('command, output', [\n+     'git pull nvbn master && git push nvbn master'),\n     (Command(script='git push', stderr=git_err2), 'git pull && git push'),\n     (Command(script='git push nvbn', stderr=git_err2),\n      'git pull nvbn && git push nvbn'),\ndiff --git a/tests/rules/test_git_remote_seturl_add.py b/tests/rules/test_git_remote_seturl_add.py\nindex 8b8667e56..011d62076 100644\n--- a/tests/rules/test_git_remote_seturl_add.py\n+++ b/tests/rules/test_git_remote_seturl_add.py\n@@ -14,11 +14,11 @@ def test_match(command):\n     Command('git remote add origin url'),\n     Command('git remote remove origin'),\n     Command('git remote prune origin'),\n-    Command('git remote set-branches origin branch')\n-    ])\n+    Command('git remote set-branches origin branch')])\n def test_not_match(command):\n     assert not match(command)\n \n+\n @pytest.mark.parametrize('command, new_command', [\n     (Command('git remote set-url origin git@github.com:nvbn/thefuck.git'),\n      'git remote add origin git@github.com:nvbn/thefuck.git')])\ndiff --git a/tests/rules/test_git_stash.py b/tests/rules/test_git_stash.py\nindex da9aded7d..53cdce42b 100644\n--- a/tests/rules/test_git_stash.py\n+++ b/tests/rules/test_git_stash.py\n@@ -4,14 +4,14 @@\n \n \n cherry_pick_error = (\n-        'error: Your local changes would be overwritten by cherry-pick.\\n'\n-        'hint: Commit your changes or stash them to proceed.\\n'\n-        'fatal: cherry-pick failed')\n+    'error: Your local changes would be overwritten by cherry-pick.\\n'\n+    'hint: Commit your changes or stash them to proceed.\\n'\n+    'fatal: cherry-pick failed')\n \n \n rebase_error = (\n-        'Cannot rebase: Your index contains uncommitted changes.\\n'\n-        'Please commit or stash them.')\n+    'Cannot rebase: Your index contains uncommitted changes.\\n'\n+    'Please commit or stash them.')\n \n \n @pytest.mark.parametrize('command', [\ndiff --git a/tests/rules/test_git_stash_pop.py b/tests/rules/test_git_stash_pop.py\nindex 1ff346860..2e0578e4d 100644\n--- a/tests/rules/test_git_stash_pop.py\n+++ b/tests/rules/test_git_stash_pop.py\n@@ -14,5 +14,5 @@ def test_match(stderr):\n \n \n def test_get_new_command(stderr):\n-    assert get_new_command(Command('git stash pop', stderr=stderr)) \\\n-           == \"git add . && git stash pop && git reset .\"\n+    assert (get_new_command(Command('git stash pop', stderr=stderr))\n+            == \"git add . && git stash pop && git reset .\")\ndiff --git a/tests/rules/test_git_tag_force.py b/tests/rules/test_git_tag_force.py\nindex 46c96fc6a..3a2e40d92 100644\n--- a/tests/rules/test_git_tag_force.py\n+++ b/tests/rules/test_git_tag_force.py\n@@ -14,5 +14,5 @@ def test_match(stderr):\n \n \n def test_get_new_command(stderr):\n-    assert get_new_command(Command('git tag alert', stderr=stderr)) \\\n-           == \"git tag --force alert\"\n+    assert (get_new_command(Command('git tag alert', stderr=stderr))\n+            == \"git tag --force alert\")\ndiff --git a/tests/rules/test_has_exists_script.py b/tests/rules/test_has_exists_script.py\nindex c6b4b1ed2..7ddb7b894 100644\n--- a/tests/rules/test_has_exists_script.py\n+++ b/tests/rules/test_has_exists_script.py\n@@ -7,7 +7,7 @@ def test_match():\n     with patch('os.path.exists', return_value=True):\n         assert match(Command(script='main', stderr='main: command not found'))\n         assert match(Command(script='main --help',\n-                          stderr='main: command not found'))\n+                             stderr='main: command not found'))\n         assert not match(Command(script='main', stderr=''))\n \n     with patch('os.path.exists', return_value=False):\ndiff --git a/tests/rules/test_lein_not_task.py b/tests/rules/test_lein_not_task.py\nindex b4fc4498f..8b3420c28 100644\n--- a/tests/rules/test_lein_not_task.py\n+++ b/tests/rules/test_lein_not_task.py\n@@ -19,5 +19,5 @@ def test_match(is_not_task):\n \n \n def test_get_new_command(is_not_task):\n-    assert get_new_command(Command(script='lein rpl --help', stderr=is_not_task)) \\\n-           == ['lein repl --help', 'lein jar --help']\n+    assert (get_new_command(Command(script='lein rpl --help', stderr=is_not_task))\n+            == ['lein repl --help', 'lein jar --help'])\ndiff --git a/tests/rules/test_ln_s_order.py b/tests/rules/test_ln_s_order.py\nindex d4119f40e..8501b6649 100644\n--- a/tests/rules/test_ln_s_order.py\n+++ b/tests/rules/test_ln_s_order.py\n@@ -11,16 +11,6 @@ def file_exists(mocker):\n get_stderr = \"ln: failed to create symbolic link '{}': File exists\".format\n \n \n-@pytest.mark.usefixtures('file_exists')\n-@pytest.mark.parametrize('script', [\n-    'ln -s dest source',\n-    'ln dest -s source',\n-    'ln dest source -s'])\n-def test_match(script):\n-    stderr = get_stderr('source')\n-    assert match(Command(script, stderr=stderr))\n-\n-\n @pytest.mark.parametrize('script, stderr, exists', [\n     ('ln dest source', get_stderr('source'), True),\n     ('ls -s dest source', get_stderr('source'), True),\n@@ -38,4 +28,5 @@ def test_not_match(file_exists, script, stderr, exists):\n     ('ln dest source -s', 'ln source -s dest')])\n def test_match(script, result):\n     stderr = get_stderr('source')\n+    assert match(Command(script, stderr=stderr))\n     assert get_new_command(Command(script, stderr=stderr)) == result\ndiff --git a/tests/rules/test_mvn_no_command.py b/tests/rules/test_mvn_no_command.py\nindex 1871716cd..7e7294577 100644\n--- a/tests/rules/test_mvn_no_command.py\n+++ b/tests/rules/test_mvn_no_command.py\n@@ -25,7 +25,7 @@ def test_match(command):\n [INFO] Finished at: Wed Aug 26 13:05:47 BST 2015\n [INFO] Final Memory: 6M/240M\n [INFO] ------------------------------------------------------------------------\n-\"\"\"),\n+\"\"\"),  # noqa\n     Command(script='mvn --help'),\n     Command(script='mvn -v')\n ])\ndiff --git a/tests/rules/test_mvn_unknown_lifecycle_phase.py b/tests/rules/test_mvn_unknown_lifecycle_phase.py\nindex c4b4b7a52..b880fd208 100644\n--- a/tests/rules/test_mvn_unknown_lifecycle_phase.py\n+++ b/tests/rules/test_mvn_unknown_lifecycle_phase.py\n@@ -25,7 +25,7 @@ def test_match(command):\n [INFO] Finished at: Wed Aug 26 13:05:47 BST 2015\n [INFO] Final Memory: 6M/240M\n [INFO] ------------------------------------------------------------------------\n-\"\"\"),\n+\"\"\"),  # noqa\n     Command(script='mvn --help'),\n     Command(script='mvn -v')\n ])\ndiff --git a/tests/rules/test_python_command.py b/tests/rules/test_python_command.py\nindex 053457da3..234d2e541 100644\n--- a/tests/rules/test_python_command.py\n+++ b/tests/rules/test_python_command.py\n@@ -8,5 +8,5 @@ def test_match():\n \n \n def test_get_new_command():\n-    assert get_new_command(Command('./test_sudo.py'))\\\n-           == 'python ./test_sudo.py'\n+    assert (get_new_command(Command('./test_sudo.py'))\n+            == 'python ./test_sudo.py')\ndiff --git a/tests/rules/test_rm_root.py b/tests/rules/test_rm_root.py\nindex 47c243f8a..fe4199049 100644\n--- a/tests/rules/test_rm_root.py\n+++ b/tests/rules/test_rm_root.py\n@@ -17,5 +17,5 @@ def test_not_match(command):\n \n \n def test_get_new_command():\n-    assert get_new_command(Command(script='rm -rf /')) \\\n-           == 'rm -rf / --no-preserve-root'\n+    assert (get_new_command(Command(script='rm -rf /'))\n+            == 'rm -rf / --no-preserve-root')\ndiff --git a/tests/rules/test_sed_unterminated_s.py b/tests/rules/test_sed_unterminated_s.py\nindex 0c890efd1..7b2bf78d5 100644\n--- a/tests/rules/test_sed_unterminated_s.py\n+++ b/tests/rules/test_sed_unterminated_s.py\n@@ -18,11 +18,11 @@ def test_match(sed_unterminated_s):\n \n \n def test_get_new_command(sed_unterminated_s):\n-    assert get_new_command(Command('sed -e s/foo/bar', stderr=sed_unterminated_s)) \\\n-            == 'sed -e s/foo/bar/'\n-    assert get_new_command(Command('sed -es/foo/bar', stderr=sed_unterminated_s)) \\\n-            == 'sed -es/foo/bar/'\n-    assert get_new_command(Command(r\"sed -e 's/\\/foo/bar'\", stderr=sed_unterminated_s)) \\\n-            == r\"sed -e 's/\\/foo/bar/'\"\n-    assert get_new_command(Command(r\"sed -e s/foo/bar -es/baz/quz\", stderr=sed_unterminated_s)) \\\n-            == r\"sed -e s/foo/bar/ -es/baz/quz/\"\n+    assert (get_new_command(Command('sed -e s/foo/bar', stderr=sed_unterminated_s))\n+            == 'sed -e s/foo/bar/')\n+    assert (get_new_command(Command('sed -es/foo/bar', stderr=sed_unterminated_s))\n+            == 'sed -es/foo/bar/')\n+    assert (get_new_command(Command(r\"sed -e 's/\\/foo/bar'\", stderr=sed_unterminated_s))\n+            == r\"sed -e 's/\\/foo/bar/'\")\n+    assert (get_new_command(Command(r\"sed -e s/foo/bar -es/baz/quz\", stderr=sed_unterminated_s))\n+            == r\"sed -e s/foo/bar/ -es/baz/quz/\")\ndiff --git a/tests/rules/test_unknown_command.py b/tests/rules/test_unknown_command.py\nindex 9a2d37e75..7e7f9aa72 100644\n--- a/tests/rules/test_unknown_command.py\n+++ b/tests/rules/test_unknown_command.py\n@@ -23,12 +23,12 @@ def test_not_match(command):\n \n @pytest.mark.parametrize('command, new_command', [\n     (Command('hdfs dfs ls',\n-        stderr='ls: Unknown command\\nDid you mean -ls?  This command begins with a dash.'), ['hdfs dfs -ls']),\n+             stderr='ls: Unknown command\\nDid you mean -ls?  This command begins with a dash.'), ['hdfs dfs -ls']),\n     (Command('hdfs dfs rm /foo/bar',\n-        stderr='rm: Unknown command\\nDid you mean -rm?  This command begins with a dash.'), ['hdfs dfs -rm /foo/bar']),\n+             stderr='rm: Unknown command\\nDid you mean -rm?  This command begins with a dash.'), ['hdfs dfs -rm /foo/bar']),\n     (Command('./bin/hdfs dfs ls -R /foo/bar',\n-        stderr='ls: Unknown command\\nDid you mean -ls?  This command begins with a dash.'), ['./bin/hdfs dfs -ls -R /foo/bar']),\n+             stderr='ls: Unknown command\\nDid you mean -ls?  This command begins with a dash.'), ['./bin/hdfs dfs -ls -R /foo/bar']),\n     (Command('./bin/hdfs dfs -Dtest=fred ls -R /foo/bar',\n-        stderr='ls: Unknown command\\nDid you mean -ls?  This command begins with a dash.'), ['./bin/hdfs dfs -Dtest=fred -ls -R /foo/bar'])])\n+             stderr='ls: Unknown command\\nDid you mean -ls?  This command begins with a dash.'), ['./bin/hdfs dfs -Dtest=fred -ls -R /foo/bar'])])\n def test_get_new_command(command, new_command):\n     assert get_new_command(command) == new_command\ndiff --git a/tests/rules/test_vagrant_up.py b/tests/rules/test_vagrant_up.py\nindex 19d7b3354..4319eed6f 100644\n--- a/tests/rules/test_vagrant_up.py\n+++ b/tests/rules/test_vagrant_up.py\n@@ -27,8 +27,8 @@ def test_not_match(command):\n     (Command(script='vagrant ssh', stderr='VM must be running to open SSH connection. Run `vagrant up`\\nto start the virtual machine.'), 'vagrant up && vagrant ssh'),\n     (Command(script='vagrant ssh devbox', stderr='VM must be running to open SSH connection. Run `vagrant up`\\nto start the virtual machine.'), ['vagrant up devbox && vagrant ssh devbox', 'vagrant up && vagrant ssh devbox']),\n     (Command(script='vagrant rdp',\n-            stderr='VM must be created before running this command. Run `vagrant up` first.'), 'vagrant up && vagrant rdp'),\n+             stderr='VM must be created before running this command. Run `vagrant up` first.'), 'vagrant up && vagrant rdp'),\n     (Command(script='vagrant rdp devbox',\n-            stderr='VM must be created before running this command. Run `vagrant up` first.'), ['vagrant up devbox && vagrant rdp devbox', 'vagrant up && vagrant rdp devbox'])])\n+             stderr='VM must be created before running this command. Run `vagrant up` first.'), ['vagrant up devbox && vagrant rdp devbox', 'vagrant up && vagrant rdp devbox'])])\n def test_get_new_command(command, new_command):\n     assert get_new_command(command) == new_command\ndiff --git a/tests/rules/test_yarn_command_not_found.py b/tests/rules/test_yarn_command_not_found.py\nindex 67b7823b6..d8a116186 100644\n--- a/tests/rules/test_yarn_command_not_found.py\n+++ b/tests/rules/test_yarn_command_not_found.py\n@@ -82,7 +82,7 @@\n \n   Run `yarn help COMMAND` for more information on specific commands.\n   Visit https://yarnpkg.com/en/docs/cli/ to learn more about Yarn.\n-'''\n+''' # noqa\n \n \n @pytest.fixture(autouse=True)\ndiff --git a/tests/test_corrector.py b/tests/test_corrector.py\nindex 08e4160e5..39b3c1084 100644\n--- a/tests/test_corrector.py\n+++ b/tests/test_corrector.py\n@@ -47,8 +47,8 @@ def test_get_corrected_commands(mocker):\n                   get_new_command=lambda x: [x.script + '@', x.script + ';'],\n                   priority=60)]\n     mocker.patch('thefuck.corrector.get_rules', return_value=rules)\n-    assert [cmd.script for cmd in get_corrected_commands(command)] \\\n-           == ['test!', 'test@', 'test;']\n+    assert ([cmd.script for cmd in get_corrected_commands(command)]\n+            == ['test!', 'test@', 'test;'])\n \n \n def test_organize_commands():\ndiff --git a/tests/test_types.py b/tests/test_types.py\nindex 9cb951ba4..4faf8edfd 100644\n--- a/tests/test_types.py\n+++ b/tests/test_types.py\n@@ -13,10 +13,10 @@\n class TestCorrectedCommand(object):\n \n     def test_equality(self):\n-        assert CorrectedCommand('ls', None, 100) == \\\n-               CorrectedCommand('ls', None, 200)\n-        assert CorrectedCommand('ls', None, 100) != \\\n-               CorrectedCommand('ls', lambda *_: _, 100)\n+        assert (CorrectedCommand('ls', None, 100) ==\n+                CorrectedCommand('ls', None, 200))\n+        assert (CorrectedCommand('ls', None, 100) !=\n+                CorrectedCommand('ls', lambda *_: _, 100))\n \n     def test_hashable(self):\n         assert {CorrectedCommand('ls', None, 100),\n@@ -41,8 +41,8 @@ def test_from_path(self, mocker):\n                               priority=900,\n                               requires_output=True))\n         rule_path = os.path.join(os.sep, 'rules', 'bash.py')\n-        assert Rule.from_path(Path(rule_path)) \\\n-               == Rule('bash', match, get_new_command, priority=900)\n+        assert (Rule.from_path(Path(rule_path))\n+                == Rule('bash', match, get_new_command, priority=900))\n         load_source.assert_called_once_with('bash', rule_path)\n \n     @pytest.mark.parametrize('rules, exclude_rules, rule, is_enabled', [\n@@ -79,15 +79,15 @@ def test_isnt_match_when_rule_failed(self, capsys):\n     def test_get_corrected_commands_with_rule_returns_list(self):\n         rule = Rule(get_new_command=lambda x: [x.script + '!', x.script + '@'],\n                     priority=100)\n-        assert list(rule.get_corrected_commands(Command(script='test'))) \\\n-               == [CorrectedCommand(script='test!', priority=100),\n-                   CorrectedCommand(script='test@', priority=200)]\n+        assert (list(rule.get_corrected_commands(Command(script='test')))\n+                == [CorrectedCommand(script='test!', priority=100),\n+                    CorrectedCommand(script='test@', priority=200)])\n \n     def test_get_corrected_commands_with_rule_returns_command(self):\n         rule = Rule(get_new_command=lambda x: x.script + '!',\n                     priority=100)\n-        assert list(rule.get_corrected_commands(Command(script='test'))) \\\n-               == [CorrectedCommand(script='test!', priority=100)]\n+        assert (list(rule.get_corrected_commands(Command(script='test')))\n+                == [CorrectedCommand(script='test!', priority=100)])\n \n \n class TestCommand(object):\ndiff --git a/tests/test_ui.py b/tests/test_ui.py\nindex 90efbac25..1c2a9933c 100644\n--- a/tests/test_ui.py\n+++ b/tests/test_ui.py\n@@ -30,11 +30,11 @@ def test_read_actions(patch_get_key):\n         const.KEY_DOWN, 'j',\n         # Ctrl+C:\n         const.KEY_CTRL_C, 'q'])\n-    assert list(islice(ui.read_actions(), 8)) \\\n-           == [const.ACTION_SELECT, const.ACTION_SELECT,\n-               const.ACTION_PREVIOUS, const.ACTION_PREVIOUS,\n-               const.ACTION_NEXT, const.ACTION_NEXT,\n-               const.ACTION_ABORT, const.ACTION_ABORT]\n+    assert (list(islice(ui.read_actions(), 8))\n+            == [const.ACTION_SELECT, const.ACTION_SELECT,\n+                const.ACTION_PREVIOUS, const.ACTION_PREVIOUS,\n+                const.ACTION_NEXT, const.ACTION_NEXT,\n+                const.ACTION_ABORT, const.ACTION_ABORT])\n \n \n def test_command_selector():\n@@ -74,8 +74,8 @@ def test_without_confirmation(self, capsys, commands, settings):\n     def test_without_confirmation_with_side_effects(\n             self, capsys, commands_with_side_effect, settings):\n         settings.require_confirmation = False\n-        assert ui.select_command(iter(commands_with_side_effect)) \\\n-               == commands_with_side_effect[0]\n+        assert (ui.select_command(iter(commands_with_side_effect))\n+                == commands_with_side_effect[0])\n         assert capsys.readouterr() == ('', 'ls (+side effect)\\n')\n \n     def test_with_confirmation(self, capsys, patch_get_key, commands):\n@@ -91,8 +91,8 @@ def test_with_confirmation_abort(self, capsys, patch_get_key, commands):\n     def test_with_confirmation_with_side_effct(self, capsys, patch_get_key,\n                                                commands_with_side_effect):\n         patch_get_key(['\\n'])\n-        assert ui.select_command(iter(commands_with_side_effect)) \\\n-               == commands_with_side_effect[0]\n+        assert (ui.select_command(iter(commands_with_side_effect))\n+                == commands_with_side_effect[0])\n         assert capsys.readouterr() == ('', u'\\x1b[1K\\rls (+side effect) [enter///ctrl+c]\\n')\n \n     def test_with_confirmation_select_second(self, capsys, patch_get_key, commands):\ndiff --git a/tests/test_utils.py b/tests/test_utils.py\nindex c9290894b..ac45a7b15 100644\n--- a/tests/test_utils.py\n+++ b/tests/test_utils.py\n@@ -18,8 +18,7 @@\n def test_default_settings(settings, override, old, new):\n     settings.clear()\n     settings.update(old)\n-    fn = lambda _: _\n-    default_settings(override)(fn)(None)\n+    default_settings(override)(lambda _: _)(None)\n     assert settings == new\n \n \ndiff --git a/thefuck/main.py b/thefuck/main.py\nindex 6ca89ac29..5fa9e7c0b 100644\n--- a/thefuck/main.py\n+++ b/thefuck/main.py\n@@ -3,16 +3,16 @@\n \n init_output()\n \n-from argparse import ArgumentParser\n-from pprint import pformat\n-import sys\n-from . import logs, types\n-from .shells import shell\n-from .conf import settings\n-from .corrector import get_corrected_commands\n-from .exceptions import EmptyCommand\n-from .utils import get_installation_info, get_alias\n-from .ui import select_command\n+from argparse import ArgumentParser  # noqa: E402\n+from pprint import pformat  # noqa: E402\n+import sys  # noqa: E402\n+from . import logs, types  # noqa: E402\n+from .shells import shell  # noqa: E402\n+from .conf import settings  # noqa: E402\n+from .corrector import get_corrected_commands  # noqa: E402\n+from .exceptions import EmptyCommand  # noqa: E402\n+from .utils import get_installation_info, get_alias  # noqa: E402\n+from .ui import select_command  # noqa: E402\n \n \n def fix_command():\n@@ -59,11 +59,10 @@ def how_to_configure_alias():\n def main():\n     parser = ArgumentParser(prog='thefuck')\n     version = get_installation_info().version\n-    parser.add_argument(\n-            '-v', '--version',\n-            action='version',\n-            version='The Fuck {} using Python {}'.format(\n-                    version, sys.version.split()[0]))\n+    parser.add_argument('-v', '--version',\n+                        action='version',\n+                        version='The Fuck {} using Python {}'.format(\n+                            version, sys.version.split()[0]))\n     parser.add_argument('-a', '--alias',\n                         action='store_true',\n                         help='[custom-alias-name] prints alias for current shell')\ndiff --git a/thefuck/rules/brew_unknown_command.py b/thefuck/rules/brew_unknown_command.py\nindex 35f6e9377..1d8186e6b 100644\n--- a/thefuck/rules/brew_unknown_command.py\n+++ b/thefuck/rules/brew_unknown_command.py\n@@ -54,8 +54,8 @@ def _brew_commands():\n     brew_path_prefix = get_brew_path_prefix()\n     if brew_path_prefix:\n         try:\n-            return _get_brew_commands(brew_path_prefix) \\\n-                   + _get_brew_tap_specific_commands(brew_path_prefix)\n+            return (_get_brew_commands(brew_path_prefix)\n+                    + _get_brew_tap_specific_commands(brew_path_prefix))\n         except OSError:\n             pass\n \ndiff --git a/thefuck/rules/dirty_unzip.py b/thefuck/rules/dirty_unzip.py\nindex 23878f5f2..3f74ef142 100644\n--- a/thefuck/rules/dirty_unzip.py\n+++ b/thefuck/rules/dirty_unzip.py\n@@ -39,7 +39,7 @@ def match(command):\n \n def get_new_command(command):\n     return u'{} -d {}'.format(\n-            command.script, shell.quote(_zip_file(command)[:-4]))\n+        command.script, shell.quote(_zip_file(command)[:-4]))\n \n \n def side_effect(old_cmd, command):\ndiff --git a/thefuck/rules/dry.py b/thefuck/rules/dry.py\nindex cfc91da84..dce9b3d78 100644\n--- a/thefuck/rules/dry.py\n+++ b/thefuck/rules/dry.py\n@@ -9,6 +9,7 @@ def match(command):\n def get_new_command(command):\n     return ' '.join(command.script_parts[1:])\n \n+\n # it should be rare enough to actually have to type twice the same word, so\n # this rule can have a higher priority to come before things like \"cd cd foo\"\n priority = 900\ndiff --git a/thefuck/rules/fix_file.py b/thefuck/rules/fix_file.py\nindex a83034d42..b10cf0471 100644\n--- a/thefuck/rules/fix_file.py\n+++ b/thefuck/rules/fix_file.py\n@@ -40,6 +40,8 @@ def _make_pattern(pattern):\n                      .replace('{line}', '(?P<line>[0-9]+)') \\\n                      .replace('{col}', '(?P<col>[0-9]+)')\n     return re.compile(pattern, re.MULTILINE)\n+\n+\n patterns = [_make_pattern(p).search for p in patterns]\n \n \ndiff --git a/thefuck/rules/git_add_force.py b/thefuck/rules/git_add_force.py\nindex 9adc07255..bf7c28b8c 100644\n--- a/thefuck/rules/git_add_force.py\n+++ b/thefuck/rules/git_add_force.py\n@@ -10,4 +10,4 @@ def match(command):\n \n @git_support\n def get_new_command(command):\n-\treturn replace_argument(command.script, 'add', 'add --force')\n+    return replace_argument(command.script, 'add', 'add --force')\ndiff --git a/thefuck/rules/git_fix_stash.py b/thefuck/rules/git_fix_stash.py\nindex 19cef4c35..0ff01b929 100644\n--- a/thefuck/rules/git_fix_stash.py\n+++ b/thefuck/rules/git_fix_stash.py\n@@ -11,6 +11,7 @@ def match(command):\n     else:\n         return False\n \n+\n # git's output here is too complicated to be parsed (see the test file)\n stash_commands = (\n     'apply',\ndiff --git a/thefuck/rules/git_pull_uncommitted_changes.py b/thefuck/rules/git_pull_uncommitted_changes.py\nindex 8e9a640d7..3a8be31ad 100644\n--- a/thefuck/rules/git_pull_uncommitted_changes.py\n+++ b/thefuck/rules/git_pull_uncommitted_changes.py\n@@ -6,7 +6,7 @@\n def match(command):\n     return ('pull' in command.script\n             and ('You have unstaged changes' in command.stderr\n-            or 'contains uncommitted changes' in command.stderr))\n+                 or 'contains uncommitted changes' in command.stderr))\n \n \n @git_support\ndiff --git a/thefuck/rules/ifconfig_device_not_found.py b/thefuck/rules/ifconfig_device_not_found.py\nindex f8692236f..a7e5026fb 100644\n--- a/thefuck/rules/ifconfig_device_not_found.py\n+++ b/thefuck/rules/ifconfig_device_not_found.py\n@@ -1,6 +1,6 @@\n import subprocess\n from thefuck.utils import for_app, replace_command, eager\n-import sys\n+\n \n @for_app('ifconfig')\n def match(command):\n@@ -21,5 +21,3 @@ def get_new_command(command):\n     interface = command.stderr.split(' ')[0][:-1]\n     possible_interfaces = _get_possible_interfaces()\n     return replace_command(command, interface, possible_interfaces)\n-\n-\ndiff --git a/thefuck/rules/man_no_space.py b/thefuck/rules/man_no_space.py\nindex c5869345c..2d5d74e60 100644\n--- a/thefuck/rules/man_no_space.py\n+++ b/thefuck/rules/man_no_space.py\n@@ -6,4 +6,5 @@ def match(command):\n def get_new_command(command):\n     return u'man {}'.format(command.script[3:])\n \n+\n priority = 2000\ndiff --git a/thefuck/rules/pacman.py b/thefuck/rules/pacman.py\nindex 13a08d65b..779ed20f9 100644\n--- a/thefuck/rules/pacman.py\n+++ b/thefuck/rules/pacman.py\n@@ -13,4 +13,5 @@ def get_new_command(command):\n     return [formatme.format(pacman, package, command.script)\n             for package in packages]\n \n+\n enabled_by_default, pacman = archlinux_env()\ndiff --git a/thefuck/rules/switch_lang.py b/thefuck/rules/switch_lang.py\nindex b1abb63d6..60e5780c3 100644\n--- a/thefuck/rules/switch_lang.py\n+++ b/thefuck/rules/switch_lang.py\n@@ -33,8 +33,8 @@ def match(command):\n     if 'not found' not in command.stderr:\n         return False\n     matched_layout = _get_matched_layout(command)\n-    return matched_layout and \\\n-           _switch_command(command, matched_layout) != get_alias()\n+    return (matched_layout and\n+            _switch_command(command, matched_layout) != get_alias())\n \n \n def get_new_command(command):\ndiff --git a/thefuck/rules/unknown_command.py b/thefuck/rules/unknown_command.py\nindex ee7139e29..30a90bc87 100644\n--- a/thefuck/rules/unknown_command.py\n+++ b/thefuck/rules/unknown_command.py\n@@ -3,8 +3,8 @@\n \n \n def match(command):\n-    return (re.search(r\"([^:]*): Unknown command.*\", command.stderr) != None\n-            and re.search(r\"Did you mean ([^?]*)?\", command.stderr) != None)\n+    return (re.search(r\"([^:]*): Unknown command.*\", command.stderr) is not None\n+            and re.search(r\"Did you mean ([^?]*)?\", command.stderr) is not None)\n \n \n def get_new_command(command):\ndiff --git a/thefuck/rules/workon_doesnt_exists.py b/thefuck/rules/workon_doesnt_exists.py\nindex 6c97ce493..41d17c07b 100644\n--- a/thefuck/rules/workon_doesnt_exists.py\n+++ b/thefuck/rules/workon_doesnt_exists.py\n@@ -26,7 +26,7 @@ def get_new_command(command):\n \n     available = _get_all_environments()\n     if available:\n-        return replace_command(command, misspelled_env, available) \\\n-               + [create_new]\n+        return (replace_command(command, misspelled_env, available)\n+                + [create_new])\n     else:\n         return create_new\ndiff --git a/thefuck/shells/tcsh.py b/thefuck/shells/tcsh.py\nindex 057dc5603..9c9ee0d4b 100644\n--- a/thefuck/shells/tcsh.py\n+++ b/thefuck/shells/tcsh.py\n@@ -19,9 +19,9 @@ def _parse_alias(self, alias):\n     def get_aliases(self):\n         proc = Popen(['tcsh', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n         return dict(\n-                self._parse_alias(alias)\n-                for alias in proc.stdout.read().decode('utf-8').split('\\n')\n-                if alias and '\\t' in alias)\n+            self._parse_alias(alias)\n+            for alias in proc.stdout.read().decode('utf-8').split('\\n')\n+            if alias and '\\t' in alias)\n \n     def _get_history_file_name(self):\n         return os.environ.get(\"HISTFILE\",\ndiff --git a/thefuck/system/__init__.py b/thefuck/system/__init__.py\nindex cc4698ac7..bb13c30b4 100644\n--- a/thefuck/system/__init__.py\n+++ b/thefuck/system/__init__.py\n@@ -2,6 +2,6 @@\n \n \n if sys.platform == 'win32':\n-    from .win32 import *\n+    from .win32 import *  # noqa: F401,F403\n else:\n-    from .unix import *\n+    from .unix import *  # noqa: F401,F403\ndiff --git a/thefuck/system/unix.py b/thefuck/system/unix.py\nindex ca878033b..0d4565bf9 100644\n--- a/thefuck/system/unix.py\n+++ b/thefuck/system/unix.py\n@@ -35,6 +35,7 @@ def get_key():\n \n     return ch\n \n+\n try:\n     from pathlib import Path\n except ImportError:\n@@ -44,5 +45,6 @@ def get_key():\n def _expanduser(self):\n     return self.__class__(os.path.expanduser(str(self)))\n \n+\n if not hasattr(Path, 'expanduser'):\n     Path.expanduser = _expanduser\ndiff --git a/thefuck/system/win32.py b/thefuck/system/win32.py\nindex 5e49ff6ed..2bf3e2e77 100644\n--- a/thefuck/system/win32.py\n+++ b/thefuck/system/win32.py\n@@ -26,6 +26,7 @@ def get_key():\n     encoding = sys.stdout.encoding or os.environ.get('PYTHONIOENCODING', 'utf-8')\n     return ch.decode(encoding)\n \n+\n try:\n     from pathlib import Path\n except ImportError:\n@@ -35,5 +36,6 @@ def get_key():\n def _expanduser(self):\n     return self.__class__(os.path.expanduser(str(self)))\n \n+\n # pathlib's expanduser fails on windows, see http://bugs.python.org/issue19776\n Path.expanduser = _expanduser\ndiff --git a/thefuck/types.py b/thefuck/types.py\nindex c03cb9320..91f69e9cd 100644\n--- a/thefuck/types.py\n+++ b/thefuck/types.py\n@@ -40,8 +40,8 @@ def script_parts(self):\n \n     def __eq__(self, other):\n         if isinstance(other, Command):\n-            return (self.script, self.stdout, self.stderr) \\\n-                   == (other.script, other.stdout, other.stderr)\n+            return ((self.script, self.stdout, self.stderr)\n+                    == (other.script, other.stdout, other.stderr))\n         else:\n             return False\n \n@@ -159,12 +159,12 @@ def __init__(self, name, match, get_new_command,\n \n     def __eq__(self, other):\n         if isinstance(other, Rule):\n-            return (self.name, self.match, self.get_new_command,\n-                    self.enabled_by_default, self.side_effect,\n-                    self.priority, self.requires_output) \\\n-                   == (other.name, other.match, other.get_new_command,\n-                       other.enabled_by_default, other.side_effect,\n-                       other.priority, other.requires_output)\n+            return ((self.name, self.match, self.get_new_command,\n+                     self.enabled_by_default, self.side_effect,\n+                     self.priority, self.requires_output)\n+                    == (other.name, other.match, other.get_new_command,\n+                        other.enabled_by_default, other.side_effect,\n+                        other.priority, other.requires_output))\n         else:\n             return False\n \n@@ -172,9 +172,9 @@ def __repr__(self):\n         return 'Rule(name={}, match={}, get_new_command={}, ' \\\n                'enabled_by_default={}, side_effect={}, ' \\\n                'priority={}, requires_output)'.format(\n-            self.name, self.match, self.get_new_command,\n-            self.enabled_by_default, self.side_effect,\n-            self.priority, self.requires_output)\n+                   self.name, self.match, self.get_new_command,\n+                   self.enabled_by_default, self.side_effect,\n+                   self.priority, self.requires_output)\n \n     @classmethod\n     def from_path(cls, path):\ndiff --git a/thefuck/utils.py b/thefuck/utils.py\nindex 2bb20a222..29ea24868 100644\n--- a/thefuck/utils.py\n+++ b/thefuck/utils.py\n@@ -40,6 +40,8 @@ def wrapper(*args, **kwargs):\n         return value\n \n     return wrapper\n+\n+\n memoize.disabled = False\n \n \n@@ -238,6 +240,8 @@ def _cache(fn, *args, **kwargs):\n                 return value\n \n     return _cache\n+\n+\n cache.disabled = False\n \n \ndiff --git a/tox.ini b/tox.ini\nindex 2b0b03e46..71deba950 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -4,3 +4,7 @@ envlist = py27,py33,py34,py35,py36\n [testenv]\n deps = -rrequirements.txt\n commands = py.test -v --capture=sys\n+\n+[flake8]\n+ignore = E501,W503\n+exclude = venv\n"
  },
  {
    "index": 6,
    "filtered_comments": [
      "Also, the fix for Python 2.7 tests is already on current `master`, would you please rebase on top of that?  ",
      "Howdy @storey247! Thanks for contributing. I'd really love to read your comments on my suggestions/questions above. I understand that you're probably busy with many stuff. But I thought I'd ping you  ",
      "@scorphus I have now rebased my work from latest `master` and cleaned up the comments as discussed.\r\n\r\nI also added some notes into the `Contribute.md` to help people get up and running with the devcontainer setup if required.\r\n\r\nHopefully now this work can be merged in too. Let me know if there is anything else you need me to look at",
      "> Thats super, @storey247! Thanks so much for hanging in! Please consider my suggestion below.\r\n\r\nFeedback actioned, just waiting on the builds and then should be good to merge  "
    ],
    "code_diff": "diff --git a/.devcontainer/Dockerfile b/.devcontainer/Dockerfile\nnew file mode 100644\nindex 000000000..c0682570f\n--- /dev/null\n+++ b/.devcontainer/Dockerfile\n@@ -0,0 +1,10 @@\n+# See here for image contents: https://github.com/microsoft/vscode-dev-containers/tree/v0.163.1/containers/python-3/.devcontainer/base.Dockerfile\n+\n+# [Choice] Python version: 3, 3.9, 3.8, 3.7, 3.6\n+ARG VARIANT=\"3\"\n+FROM mcr.microsoft.com/vscode/devcontainers/python:0-${VARIANT}\n+\n+# [Optional] If your pip requirements rarely change, uncomment this section to add them to the image.\n+COPY requirements.txt /tmp/pip-tmp/\n+RUN pip3 --disable-pip-version-check --no-cache-dir install -r /tmp/pip-tmp/requirements.txt \\\n+   && rm -rf /tmp/pip-tmp\ndiff --git a/.devcontainer/devcontainer.json b/.devcontainer/devcontainer.json\nnew file mode 100644\nindex 000000000..b59f56261\n--- /dev/null\n+++ b/.devcontainer/devcontainer.json\n@@ -0,0 +1,34 @@\n+// For format details, see https://aka.ms/devcontainer.json. For config options, see the README at:\n+// https://github.com/microsoft/vscode-dev-containers/tree/v0.163.1/containers/python-3\n+{\n+\t\"name\": \"Python 3\",\n+\t\"build\": {\n+\t\t\"dockerfile\": \"Dockerfile\",\n+\t\t\"context\": \"..\"\n+\t},\n+\n+\t// Set *default* container specific settings.json values on container create.\n+\t\"settings\": {\n+\t\t\"terminal.integrated.shell.linux\": \"/bin/bash\",\n+\t\t\"python.pythonPath\": \"/usr/local/bin/python\",\n+\t\t\"python.linting.enabled\": true,\n+\t\t\"python.linting.pylintEnabled\": true,\n+\t\t\"python.formatting.autopep8Path\": \"/usr/local/py-utils/bin/autopep8\",\n+\t\t\"python.formatting.blackPath\": \"/usr/local/py-utils/bin/black\",\n+\t\t\"python.formatting.yapfPath\": \"/usr/local/py-utils/bin/yapf\",\n+\t\t\"python.linting.banditPath\": \"/usr/local/py-utils/bin/bandit\",\n+\t\t\"python.linting.flake8Path\": \"/usr/local/py-utils/bin/flake8\",\n+\t\t\"python.linting.mypyPath\": \"/usr/local/py-utils/bin/mypy\",\n+\t\t\"python.linting.pycodestylePath\": \"/usr/local/py-utils/bin/pycodestyle\",\n+\t\t\"python.linting.pydocstylePath\": \"/usr/local/py-utils/bin/pydocstyle\",\n+\t\t\"python.linting.pylintPath\": \"/usr/local/py-utils/bin/pylint\"\n+\t},\n+\n+\t// Add the IDs of extensions you want installed when the container is created.\n+\t\"extensions\": [\n+\t\t\"ms-python.python\"\n+\t],\n+\n+\t// Use 'postCreateCommand' to run commands after the container is created.\n+\t\"postCreateCommand\": \"pip3 install -r requirements.txt && python3 setup.py develop\"\n+}\ndiff --git a/CONTRIBUTING.md b/CONTRIBUTING.md\nindex 33b193dad..0e6525301 100644\n--- a/CONTRIBUTING.md\n+++ b/CONTRIBUTING.md\n@@ -26,6 +26,13 @@ fixes, etc.\n \n # Developing\n \n+In order to develop locally, there are two options:\n+\n+- Develop using a local installation of Python 3 and setting up a virtual environment\n+- Develop using an automated VSCode Dev Container.\n+\n+## Develop using local Python installation\n+\n [Create and activate a Python 3 virtual environment.](https://docs.python.org/3/tutorial/venv.html)\n \n Install `The Fuck` for development:\n@@ -59,3 +66,27 @@ For sending package to pypi:\n sudo apt-get install pandoc\n ./release.py\n ```\n+\n+## Develop using Dev Container\n+\n+To make local development easier a [VSCode Devcontainer](https://code.visualstudio.com/docs/remote/remote-overview) is included with this repository. This will allows you to spin up a Docker container with all the necessary prerequisites for this project pre-installed ready to go, no local Python install/setup required.\n+\n+### Prerequisites\n+\n+To use the container you require:\n+- [Docker](https://www.docker.com/products/docker-desktop)\n+- [VSCode](https://code.visualstudio.com/)\n+- [VSCode Remote Development Extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack)\n+- [Windows Users Only]: [Installation of WSL2 and configuration of Docker to use it](https://docs.docker.com/docker-for-windows/wsl/)\n+\n+Full notes about [installation are here](https://code.visualstudio.com/docs/remote/containers#_installation)\n+\n+### Running the container\n+\n+Assuming you have the prerequisites:\n+\n+1. Open VSCode\n+1. Open command palette (CMD+SHIFT+P (mac) or CTRL+SHIFT+P (windows))\n+1. Select `Remote-Containers: Reopen in Container`.\n+1. Container will be built, install all pip requirements and your VSCode will mount into it automagically.\n+1. Your VSCode and container now essentially become a throw away environment.\n\\ No newline at end of file\n"
  },
  {
    "index": 7,
    "filtered_comments": [
      "Yes I totally agree with all your points. My only thoughts lie on the matter of the global rule. Due to the almost identical commands of the env packages, my solution seemed really convenient. Though, if you think is better to make separate files for each command, I will work on it for sure. Probably the testing process will be more efficient in this way as well. ",
      "The identical parts of the rules could be kept in a single, separate `devenv` submodule imported by all of the rules. Such submodule would reside under `thefuck/specific` along with other specific ones. This way theres less repetition. What do you think?",
      "Yes, that' s a great idea! I will work on it, as well as the requested changes and come up with a new pr.",
      "Need to check why the tests fail, otherwise I think I managed to fullfill the requested changes and they seem really great!",
      "@scorphus Hello again, everything seems to work pretty great! Unfortunately, your idea of integrating some of the common code of the rules to specific/devenv.py was making the tests to fail, so I decided to simplify it. Hope I find some extra time and manage to integrade more of the common code of the rules in a couple of weeks.\r\n\r\nThank you for your help!"
    ],
    "code_diff": "diff --git a/README.md b/README.md\nindex 57ca6234e..3d965d064 100644\n--- a/README.md\n+++ b/README.md\n@@ -269,13 +269,13 @@ following rules are enabled by default:\n * `npm_wrong_command` &ndash; fixes wrong npm commands like `npm urgrade`;\n * `no_command` &ndash; fixes wrong console commands, for example `vom/vim`;\n * `no_such_file` &ndash; creates missing directories with `mv` and `cp` commands;\n+* `omnienv_no_such_command` &ndash; fixes wrong commands for `goenv`, `nodenv`, `pyenv` and `rbenv` (eg.: `pyenv isntall` or `goenv list`);\n * `open` &ndash; either prepends `http://` to address passed to `open` or create a new file or directory and passes it to `open`;\n * `pip_install` &ndash; fixes permission issues with `pip install` commands by adding `--user` or prepending `sudo` if necessary;\n * `pip_unknown_command` &ndash; fixes wrong `pip` commands, for example `pip instatl/pip install`;\n * `php_s` &ndash; replaces `-s` by `-S` when trying to run a local php server;\n * `port_already_in_use` &ndash; kills process that bound port;\n * `prove_recursively` &ndash; adds `-r` when called with directory;\n-* `pyenv_no_such_command` &ndash; fixes wrong pyenv commands like `pyenv isntall` or `pyenv list`;\n * `python_command` &ndash; prepends `python` when you try to run non-executable/without `./` python script;\n * `python_execute` &ndash; appends missing `.py` when executing Python files;\n * `quotation_marks` &ndash; fixes uneven usage of `'` and `\"` when containing args';\ndiff --git a/tests/rules/test_pyenv_no_such_command.py b/tests/rules/test_omnienv_no_such_command.py\nsimilarity index 84%\nrename from tests/rules/test_pyenv_no_such_command.py\nrename to tests/rules/test_omnienv_no_such_command.py\nindex 298620f74..45f84a6c0 100644\n--- a/tests/rules/test_pyenv_no_such_command.py\n+++ b/tests/rules/test_omnienv_no_such_command.py\n@@ -1,6 +1,6 @@\n import pytest\n \n-from thefuck.rules.pyenv_no_such_command import get_new_command, match\n+from thefuck.rules.omnienv_no_such_command import get_new_command, match\n from thefuck.types import Command\n \n \n@@ -11,7 +11,7 @@ def output(pyenv_cmd):\n \n @pytest.fixture(autouse=True)\n def Popen(mocker):\n-    mock = mocker.patch('thefuck.rules.pyenv_no_such_command.Popen')\n+    mock = mocker.patch('thefuck.rules.omnienv_no_such_command.Popen')\n     mock.return_value.stdout.readlines.return_value = (\n         b'--version\\nactivate\\ncommands\\ncompletions\\ndeactivate\\nexec_\\n'\n         b'global\\nhelp\\nhooks\\ninit\\ninstall\\nlocal\\nprefix_\\n'\n@@ -33,6 +33,11 @@ def test_match(script, pyenv_cmd, output):\n     assert match(Command(script, output=output))\n \n \n+def test_match_goenv_output_quote():\n+    \"\"\"test goenv's specific output with quotes (')\"\"\"\n+    assert match(Command('goenv list', output=\"goenv: no such command 'list'\"))\n+\n+\n @pytest.mark.parametrize('script, output', [\n     ('pyenv global', 'system'),\n     ('pyenv versions', '  3.7.0\\n  3.7.1\\n* 3.7.2\\n'),\ndiff --git a/thefuck/rules/pyenv_no_such_command.py b/thefuck/rules/omnienv_no_such_command.py\nsimilarity index 50%\nrename from thefuck/rules/pyenv_no_such_command.py\nrename to thefuck/rules/omnienv_no_such_command.py\nindex cc9b609e3..ca8cc36d6 100644\n--- a/thefuck/rules/pyenv_no_such_command.py\n+++ b/thefuck/rules/omnienv_no_such_command.py\n@@ -1,8 +1,12 @@\n import re\n-from subprocess import PIPE, Popen\n-\n from thefuck.utils import (cache, for_app, replace_argument, replace_command,\n                            which)\n+from subprocess import PIPE, Popen\n+\n+\n+supported_apps = 'goenv', 'nodenv', 'pyenv', 'rbenv'\n+enabled_by_default = any(which(a) for a in supported_apps)\n+\n \n COMMON_TYPOS = {\n     'list': ['versions', 'install --list'],\n@@ -10,24 +14,22 @@\n }\n \n \n-@for_app('pyenv')\n+@for_app(*supported_apps, at_least=1)\n def match(command):\n-    return 'pyenv: no such command' in command.output\n+    return 'env: no such command ' in command.output\n \n \n-def get_pyenv_commands():\n-    proc = Popen(['pyenv', 'commands'], stdout=PIPE)\n+def get_app_commands(app):\n+    proc = Popen([app, 'commands'], stdout=PIPE)\n     return [line.decode('utf-8').strip() for line in proc.stdout.readlines()]\n \n \n-if which('pyenv'):\n-    get_pyenv_commands = cache(which('pyenv'))(get_pyenv_commands)\n-\n-\n-@for_app('pyenv')\n def get_new_command(command):\n-    broken = re.findall(r\"pyenv: no such command `([^']*)'\", command.output)[0]\n+    broken = re.findall(r\"env: no such command ['`]([^']*)'\", command.output)[0]\n     matched = [replace_argument(command.script, broken, common_typo)\n                for common_typo in COMMON_TYPOS.get(broken, [])]\n-    matched.extend(replace_command(command, broken, get_pyenv_commands()))\n+\n+    app = command.script_parts[0]\n+    app_commands = cache(which(app))(get_app_commands)(app)\n+    matched.extend(replace_command(command, broken, app_commands))\n     return matched\n"
  },
  {
    "index": 8,
    "filtered_comments": [
      "Ouch, no... too bad. `mktemp`'s implementations differ amongst Linux and OS X\n",
      "So, how about this:\n\n``` fish\nfunction __thefuck_repl --description 'Replace operators into fish-compatible'\n    set -l tmp (echo $argv | sed 's/ && / ; and /g')\n    echo $tmp | sed 's/ || / ; or /g'\nend\n\nfunction fuck --description 'Correct your previous console command'\n    set -l eval_script (mktemp 2>/dev/null ; or mktemp -t 'thefuck')\n    thefuck $history[1] > $eval_script\n    eval (__thefuck_repl (cat $eval_script))\n    rm $eval_script\nend\n```\n",
      "Ya, I think this should work. Needs a rebase though since Github thinks it has a merge conflict.\n",
      "Thanks for the input, @daenney!\n\nThe part of the README.md file regarding shell aliases and/or functions was move over to the wiki, that's why this is conflicting with master. I'll move the solution to the wiki too.\n",
      "One thing to remind ourselves of, though, is that from now on, every single rule that involves logical operators (`&&` and `||` for instance) should enclose them in blank spaces, like the following: `cmd_x && cmd_y` or `cmd_x || cmd_z`.\n"
    ],
    "code_diff": "diff --git a/README.md b/README.md\nindex 9f55b5337..61bec38d2 100644\n--- a/README.md\n+++ b/README.md\n@@ -117,8 +117,16 @@ alias FUCK='fuck'\n Or in `config.fish`:\n \n ```fish\n-function fuck\n-    eval (thefuck $history[1])\n+function __thefuck_repl --description 'Replace operators into fish-compatible'\n+    set -l tmp (echo $argv | sed 's/ && / ; and /g')\n+    echo $tmp | sed 's/ || / ; or /g'\n+end\n+\n+function fuck --description 'Correct your previous console command'\n+    set -l eval_script (mktemp 2>/dev/null ; or mktemp -t 'thefuck')\n+    thefuck $history[1] > $eval_script\n+    eval (__thefuck_repl (cat $eval_script))\n+    rm $eval_script\n end\n ```\n \n"
  },
  {
    "index": 9,
    "filtered_comments": [
      "Nice idea, maybe it will be a good thing to make that more generic? To just replace any argument that starts with `0` if it appears in `stderr`.",
      "@nvbn When I quickly read your comment on my phone, I thought making this fix more generic sounded like a good idea, but upon reflection, I'm not so sure.\r\n\r\nIn the specific case I coded up, the argument needs to turn into a flag. Would making that assumption more generic (i.e. convert all arguments starting with `0` to `-`) make sense?\r\n\r\nFor that matter, what would the \"undo\" action look like, or would there even need to be an \"undo\" action? (In the specific example here, I need to delete a branch that was just accidentally created...what should the proper response be if `git branch 0v` wasn't the command?",
      "@ProfessorTom you're right, initially I thought that `git branch 0v` prints something, but it just creates a branch.\r\n\r\nI guess the only way to generalize this rule is to also support cases like `git branch 0l`, `git branch 0a` and etc, so just check that the argument after `branch` starts with `0`.",
      "Do you still want me to generalize this feature turning the `0` into a `-` and deleting the branch just created in all cases?\r\n\r\nIs there a case where this would cause more harm than good?"
    ],
    "code_diff": "diff --git a/README.md b/README.md\nindex 838300762..070ee9759 100644\n--- a/README.md\n+++ b/README.md\n@@ -203,6 +203,7 @@ following rules are enabled by default:\n * `git_branch_delete` &ndash; changes `git branch -d` to `git branch -D`;\n * `git_branch_exists` &ndash; offers `git branch -d foo`, `git branch -D foo` or `git checkout foo` when creating a branch that already exists;\n * `git_branch_list` &ndash; catches `git branch list` in place of `git branch` and removes created branch;\n+* `git_branch_flag_0_to_flag_dash_v` &ndash; undoes `git branch 0v` and runs `git branch -v` in its place;\n * `git_checkout` &ndash; fixes branch name or creates new branch;\n * `git_commit_amend` &ndash; offers `git commit --amend` after previous commit;\n * `git_commit_reset` &ndash; offers `git reset HEAD~` after previous commit;\ndiff --git a/tests/rules/test_git_branch_flag_0_to_flag_dash_v.py b/tests/rules/test_git_branch_flag_0_to_flag_dash_v.py\nnew file mode 100644\nindex 000000000..21c1e021d\n--- /dev/null\n+++ b/tests/rules/test_git_branch_flag_0_to_flag_dash_v.py\n@@ -0,0 +1,22 @@\n+import pytest\n+from thefuck.rules.git_branch_flag_0_to_flag_dash_v import match, get_new_command\n+from thefuck.types import Command\n+\n+\n+@pytest.fixture\n+def output():\n+    return \"\"\n+\n+\n+def test_match_git_branch_0v(output):\n+    assert match(Command('git branch 0v', output))\n+\n+\n+def test_matches_no__git_branch_0_anything(output):\n+    assert not match(Command('git branch -v', ''))\n+    assert not match(Command('ls', output))\n+\n+\n+def test_get_new_command(output):\n+    assert get_new_command(Command('git branch 0v', output))\\\n+        == 'git branch -D 0v && git branch -v'\ndiff --git a/thefuck/rules/git_branch_flag_0_to_flag_dash_v.py b/thefuck/rules/git_branch_flag_0_to_flag_dash_v.py\nnew file mode 100644\nindex 000000000..205f9db17\n--- /dev/null\n+++ b/thefuck/rules/git_branch_flag_0_to_flag_dash_v.py\n@@ -0,0 +1,36 @@\n+from thefuck.shells import shell\n+from thefuck.specific.git import git_support\n+from thefuck.utils import memoize\n+\n+'''\n+keys are fatfingered entry, values are two-element tuples\n+where the first element is \"the fix\" and the second element\n+is \"what you meant to do\n+ '''\n+# clunky when there's only one key, but as others get added, I _think_\n+# this will be cleaner\n+flags_and_their_fixes = dict()\n+flags_and_their_fixes[\"v\"] = ('git branch -D 0v', 'git branch -v')\n+\n+\n+@memoize\n+def _supported_flag_fix(command):\n+    flag = command.script_parts[2:][0]\n+\n+    if len(flag) == 2 and flag.startswith(\"0\"):\n+        return flags_and_their_fixes[flag[1]]\n+    else:\n+        return None\n+\n+\n+@git_support\n+def match(command):\n+    return (command.script_parts\n+            and command.script_parts[1] == 'branch'\n+            and _supported_flag_fix(command) is not None)\n+\n+\n+@git_support\n+def get_new_command(command):\n+    fix_parts = _supported_flag_fix(command)\n+    return shell.and_(fix_parts[0], fix_parts[1])\n"
  },
  {
    "index": 10,
    "filtered_comments": [
      "looks like you [can use this already](https://github.com/NixOS/nixpkgs/compare/master...KiaraGrouwstra:nixpkgs:thefuck-nix-shell) using e.g. an overlay, altho i had a bit of trouble getting it to work out of the box.\r\nspecifically, without adding `doCheck = false;`, i would run into this error:\r\n\r\n```\r\nerror: builder for '/nix/store/rl44gb6qd4x2myclj9i8cpkfrvw6ysqa-thefuck-3.32.drv' failed with exit code 2;\r\n       last 10 log lines:\r\n       > thefuck/system/unix.py:6\r\n       >   /build/source/thefuck/system/unix.py:6: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\r\n       >     from distutils.spawn import find_executable\r\n       >\r\n       > -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\r\n       > =========================== short test summary info ============================\r\n       > ERROR  - ModuleNotFoundError: No module named 'pytest_docker_pexpect'\r\n       > !!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\r\n       > ========================= 1 warning, 1 error in 0.09s ==========================\r\n       > /nix/store/bknngadwym46j65qs14ic2w79rpav888-stdenv-linux/setup: line 1582: pop_var_context: head of shell_variables not a function context\r\n```\r\n\r\ni had tried removing the added test, altho that appeared not to resolve the issue.\r\n",
      "it would seem cool to similarly get an approach using `nix run`, i.e. go from suggesting `nix-shell -p ponysay --run \"ponysay moo\"` to `nix run nixpkgs#ponysay -- moo` - this might eventually help extend beyond just `nixpkgs`.\r\n\r\nedit: https://github.com/KiaraGrouwstra/thefuck/commit/81d6786c80b86f2cc80b3ea90adc214df8266643\r\n",
      "I've been using a custom rule that supports the new [unified CLI](https://zero-to-nix.com/concepts/nix#unified-cli) for a while, and was planning on opening a PR once this one has been merged (I hesitate to update this current PR as it's already tested and ready to be merged). I don't know if that will happen soon, so in the meantime I've pushed the changes to [this](https://github.com/thenbe/thefuck/tree/nix-shell-new) new branch instead, which [builds](https://github.com/thenbe/thefuck/compare/nix-shell...thenbe:thefuck:nix-shell-new) on this here PR. You can use the updated rule by adding it as a [custom rule](https://github.com/nvbn/thefuck?tab=readme-ov-file#creating-your-own-rules) to your config.\r\n\r\nIn the new rule, three variants are suggested. Assuming I run `cowsay hello world`, I am presented with the following:\r\n\r\n1. `nix run nixpkgs#cowsay -- hello world`: This runs my command in a non-interactive shell. Uses the nix unified CLI.\r\n1. `nix shell nixpkgs#cowsay`: This enters an interactive shell with `cowsay` available, but does not run any command. This is useful if you'd rather run the command yourself after entering the shell because your command requires delicate massaging (e.g. running it with `sudo`, prefixing it with environment variable, juggling quote variants, etc).\r\n1. `nix-shell -p cowsay --run \"cowsay hello world\"`. This runs my command in a non-interactive shell. Uses the nix original CLI.\r\n1. `nix shell nixpkgs#cowsay --command cowsay hello world`: Very similar to the first one so I've personally disabled this one.\r\n\r\n### Thoughts on future updates:\r\n\r\n\r\n\r\n- It'd be nice if there was a variant that runs my command and then _keeps me_ in the shell.\r\n  - For the original CLI, we [can](https://nix.dev/manual/nix/2.19/command-ref/nix-shell#options) add a `--command \"echo hello; return\"` to our `nix-shell` invocation.\r\n  - For the unified CLI: not sure yet, we might need to do something like this example from the [docs](https://nix.dev/manual/nix/2.19/command-ref/new-cli/nix3-shell): ` nix shell nixpkgs#gnumake --command sh -c \"cd src && make\"`\r\n- We should expose a couple of flags for users to configure this.\r\n  - `disable_unified_cli` (boolean)\r\n  - `disable_original_cli` (boolean)\r\n- As far as I can tell, the `command-not-found` db doesn't really play nice if you use flakes to configure your system and might return stale results (unless you update it manually?). [`nix-index`](https://github.com/nix-community/nix-index) seems to be the go-to alternative. It'd be great if we could optionally use that instead (perhaps behind a flag `enable_nix_index` for users who have installed `nix-index` (`programs.nix-index.enable = true;` in home manager).",
      "@thenbe i agree integrating with `nix-index`'s `command-not-found` replacement seems cool, as a flake user.\r\ni kinda wish we could have `command-not-found` (and this `thefuck` integration) [extend to flake inputs beyond nixpkgs](https://github.com/nix-community/nix-index/issues/244) as well, such as to packages from NUR for example. preferably this should be dynamic based on your inputs rather than hardcoded to specific ones like nixpkgs, or NUR for that matter.\r\ni'll admit i haven't really figured out how that might work tho.",
      "just tried these with a command like `program_i_have | program_i_dont_have`, seems that may complicate the suggestions a bit",
      "@thenbe hm, i'm not sure.\r\n\r\n```\r\nfortune | cowsay\r\nThe program 'cowsay' is not in your PATH. It is provided by several packages.\r\nYou can make it available in an ephemeral shell by typing one of the following:\r\n  nix-shell -p cowsay\r\n  nix-shell -p neo-cowsay\r\n$ fuck\r\nnix run nixpkgs#fortune | cowsay\r\n```\r\n\r\nfeels like it knows about the whole command given it's reproducing it?\r\n",
      "another common nix thing we might be able to address from `thefuck` would be errors about packages being unfree\r\n\r\nedit: https://github.com/KiaraGrouwstra/thefuck/commit/16d838bf6f63117b161a2f1e6572e06108b007eb\r\n",
      "If I'm only looking to execute a program (and don't need to be dropped into a shell) then I prefer `nix run` over `nix shell` as the [documentation](https://nix.dev/manual/nix/2.19/command-ref/new-cli/nix3-run) suggests `nix run` specifically for this use case.\r\n\r\nI also recall `nix run` being more performant (perhaps because we forego the overhead of launching a shell?). This last point is not derived from benchmarks, only anecdotal evidence.\r\n\r\n> i guess the latter seems a bit more generic in case of handling non-standard binaries at least\r\n\r\nI've added this variant (the 4th one in my [previous post](https://github.com/nvbn/thefuck/pull/1393#issuecomment-1961487094)), but disabled it after a while when I realized that I never reach for it. Do you find that you still need it over `nix run` (the 1st variant in my previous post)?",
      "> another common nix thing we might be able to address from `thefuck` would be errors about packages being unfree\r\n> \r\n> edit: [KiaraGrouwstra@16d838b](https://github.com/KiaraGrouwstra/thefuck/commit/16d838bf6f63117b161a2f1e6572e06108b007eb)\r\n\r\nThis would be useful. Does it still complain about the `--impure` flag? Or do you use a workaround for that?",
      "I just have it aliased to `f` for extra convenience.\r\n\r\nI opted not to package it for nix separately since `fuck` already exposes a method for easily adding custom rules. Instead, I placed the rule in `~/mydotfiles/thefuck/rules/nix-shell.py` then told home-manager to symlink it to the appropriate place in `.config`:\r\n\r\n```nix\r\n# home.nix\r\nhome.file.\".config/thefuck/rules/nix-shell.py\".source = config.lib.file.mkOutOfStoreSymlink \"${config.home.homeDirectory}/mydotfiles/thefuck/rules/nix-shell.py\";\r\n```\r\n\r\nThis way I don't need to rebuild every time I tweak the rule.\r\n\r\n> what was the --impure error?\r\n\r\nThe unified CLI commands (`nix shell`, `nix run`, etc) will not acknowledge environment variables unless the `--impure` flag is used.\r\n\r\n<details>\r\n  <summary> output </summary>\r\n\r\n```\r\n$ NIXPKGS_ALLOW_UNFREE=1 nix shell nixpkgs#github-copilot-cli\r\n\r\nerror:\r\n        in the condition of the assert statement\r\n\r\n         at /nix/store/xwc3zfc544jg6zhr0wi6k8253s7mwlhi-source/lib/customisation.nix:267:17:\r\n\r\n          266|     in commonAttrs // {\r\n          267|       drvPath = assert condition; drv.drvPath;\r\n             |                 ^\r\n          268|       outPath = assert condition; drv.outPath;\r\n\r\n        while evaluating the attribute 'handled'\r\n\r\n         at /nix/store/xwc3zfc544jg6zhr0wi6k8253s7mwlhi-source/pkgs/stdenv/generic/check-meta.nix:490:7:\r\n\r\n          489|       # or, alternatively, just output a warning message.\r\n          490|       handled =\r\n             |       ^\r\n          491|         (\r\n\r\n       (stack trace truncated; use '--show-trace' to show the full trace)\r\n\r\n       error: Package github-copilot-cli-0.1.36 in /nix/store/xwc3zfc544jg6zhr0wi6k8253s7mwlhi-source/pkgs/tools/misc/github-copilot-cli/default.nix:21 has\r\n an unfree license (unfree), refusing to evaluate.\r\n\r\n       a) To temporarily allow unfree packages, you can use an environment variable\r\n          for a single invocation of the nix tools.\r\n\r\n            $ export NIXPKGS_ALLOW_UNFREE=1\r\n\r\n          Note: When using `nix shell`, `nix build`, `nix develop`, etc with a flake,\r\n                then pass `--impure` in order to allow use of environment variables.\r\n\r\n       b) For `nixos-rebuild` you can set\r\n         { nixpkgs.config.allowUnfree = true; }\r\n       in configuration.nix to override this.\r\n\r\n       Alternatively you can configure a predicate to allow specific packages:\r\n         { nixpkgs.config.allowUnfreePredicate = pkg: builtins.elem (lib.getName pkg) [\r\n             \"github-copilot-cli-0.1.36\"\r\n           ];\r\n         }\r\n\r\n       c) For `nix-env`, `nix-build`, `nix-shell` or any other Nix command you can add\r\n         { allowUnfree = true; }\r\n       to ~/.config/nixpkgs/config.nix.\r\n\r\n\r\n```\r\n```bash\r\n# it wants this instead:\r\n$ NIXPKGS_ALLOW_UNFREE=1 nix shell nixpkgs#github-copilot-cli --impure\r\n```\r\n\r\n</details>\r\n"
    ],
    "code_diff": "diff --git a/README.md b/README.md\nindex 48b4b0fb3..724dc3569 100644\n--- a/README.md\n+++ b/README.md\n@@ -367,6 +367,7 @@ The following rules are enabled by default on specific platforms only:\n * `brew_update_formula` &ndash; turns `brew update <formula>` into `brew upgrade <formula>`;\n * `dnf_no_such_command` &ndash; fixes mistyped DNF commands;\n * `nixos_cmd_not_found` &ndash; installs apps on NixOS;\n+* `nix_shell` &ndash; re-runs your command in a `nix-shell`;\n * `pacman` &ndash; installs app with `pacman` if it is not installed (uses `yay`, `pikaur` or `yaourt` if available);\n * `pacman_invalid_option` &ndash; replaces lowercase `pacman` options with uppercase.\n * `pacman_not_found` &ndash; fixes package name with `pacman`, `yay`, `pikaur` or `yaourt`.\ndiff --git a/tests/rules/test_nix_shell.py b/tests/rules/test_nix_shell.py\nnew file mode 100644\nindex 000000000..69ba14872\n--- /dev/null\n+++ b/tests/rules/test_nix_shell.py\n@@ -0,0 +1,90 @@\n+import pytest\n+from thefuck.rules.nix_shell import get_nixpkgs_names, match, get_new_command\n+from thefuck.types import Command\n+from unittest.mock import patch, MagicMock\n+\n+\n+@pytest.mark.parametrize(\n+    \"script,output,nixpkgs_names\",\n+    [\n+        # output can be retrived by running `THEFUCK_DEBUG=true thefuck lsof`\n+        (\n+            \"lsof\",\n+            \"/nix/store/p6dlr3skfhxpyphipg2bqnj52999banh-bash-5.2-p15/bin/sh: line 1: lsof: command not found\",\n+            [\"lsof\"],\n+        ),\n+    ],\n+)\n+def test_match(script, output, nixpkgs_names):\n+    with patch(\"thefuck.rules.nix_shell.get_nixpkgs_names\") as mocked_get_nixpkgs_names:\n+        mocked_get_nixpkgs_names.return_value = nixpkgs_names\n+        command = Command(script, output)\n+        assert match(command)\n+\n+\n+@pytest.mark.parametrize(\n+    \"script,output,nixpkgs_names\",\n+    [\n+        # output can be retrived by running `THEFUCK_DEBUG=true thefuck foo`\n+        (\n+            \"foo\",\n+            \"/nix/store/p6dlr3skfhxpyphipg2bqnj52999banh-bash-5.2-p15/bin/sh: line 1: foo: command not found\",\n+            [],\n+        ),\n+    ],\n+)\n+def test_not_match(script, output, nixpkgs_names):\n+    with patch(\"thefuck.rules.nix_shell.get_nixpkgs_names\") as mocked_get_nixpkgs_names:\n+        mocked_get_nixpkgs_names.return_value = nixpkgs_names\n+        command = Command(script, output)\n+        assert not match(command)\n+\n+\n+@pytest.mark.parametrize(\n+    \"script,nixpkgs_names,new_command\",\n+    [\n+        (\n+            \"lsof -i :3000\",\n+            [\"busybox\", \"lsof\"],\n+            [\n+                'nix-shell -p busybox --run \"lsof -i :3000\"',\n+                'nix-shell -p lsof --run \"lsof -i :3000\"',\n+            ],\n+        ),\n+        (\"xev\", [\"xorg.xev\"], ['nix-shell -p xorg.xev --run \"xev\"']),\n+    ],\n+)\n+def test_get_new_command(script, nixpkgs_names, new_command):\n+    \"\"\"Check that flags and params are preserved in the new command\"\"\"\n+\n+    command = Command(script, \"\")\n+    with patch(\"thefuck.rules.nix_shell.get_nixpkgs_names\") as mocked_get_nixpkgs_names:\n+        mocked_get_nixpkgs_names.return_value = nixpkgs_names\n+        assert get_new_command(command) == new_command\n+\n+\n+# Mocks the stderr of `command-not-found QUERY`. Mock values are retrieved by\n+# running `THEFUCK_DEBUG=true thefuck command-not-found lsof`.\n+mocked_cnf_stderr = {\n+    \"lsof\": \"The program 'lsof' is not in your PATH. It is provided by several packages.\\nYou can make it available in an ephemeral shell by typing one of the following:\\n  nix-shell -p busybox\\n  nix-shell -p lsof\",\n+    \"xev\": \"The program 'xev' is not in your PATH. You can make it available in an ephemeral shell by typing:\\n  nix-shell -p xorg.xev\",\n+    \"foo\": \"foo: command not found\",\n+}\n+\n+\n+@pytest.mark.parametrize(\n+    \"bin,expected_nixpkgs_names,cnf_stderr\",\n+    [\n+        (\"lsof\", [\"busybox\", \"lsof\"], mocked_cnf_stderr[\"lsof\"]),\n+        (\"xev\", [\"xorg.xev\"], mocked_cnf_stderr[\"xev\"]),\n+        (\"foo\", [], mocked_cnf_stderr[\"foo\"]),\n+    ],\n+)\n+def test_get_nixpkgs_names(bin, expected_nixpkgs_names, cnf_stderr):\n+    \"\"\"Check that `get_nixpkgs_names` returns the correct names\"\"\"\n+\n+    with patch(\"subprocess.run\") as mocked_run:\n+        result = MagicMock()\n+        result.stderr = cnf_stderr\n+        mocked_run.return_value = result\n+        assert get_nixpkgs_names(bin) == expected_nixpkgs_names\ndiff --git a/thefuck/rules/nix_shell.py b/thefuck/rules/nix_shell.py\nnew file mode 100644\nindex 000000000..7bfba7eac\n--- /dev/null\n+++ b/thefuck/rules/nix_shell.py\n@@ -0,0 +1,51 @@\n+from thefuck.specific.nix import nix_available\n+import subprocess\n+\n+enabled_by_default = nix_available\n+\n+# Set the priority just ahead of `fix_file` rule, which can generate low quality matches due\n+# to the sheer amount of paths in the nix store.\n+priority = 999\n+\n+\n+def get_nixpkgs_names(bin):\n+    \"\"\"\n+    Returns the name of the Nix package that provides the given binary. It uses the\n+    `command-not-found` binary to do so, which is how nix-shell generates it's own suggestions.\n+    \"\"\"\n+\n+    result = subprocess.run(\n+        [\"command-not-found\", bin], stderr=subprocess.PIPE, universal_newlines=True\n+    )\n+\n+    # The suggestion, if any, will be found in stderr. Upstream definition: https://github.com/NixOS/nixpkgs/blob/b6fbd87328f8eabd82d65cc8f75dfb74341b0ace/nixos/modules/programs/command-not-found/command-not-found.nix#L48-L90\n+    text = result.stderr\n+\n+    # return early if binary is not available through nix\n+    if \"nix-shell\" not in text:\n+        return []\n+\n+    nixpkgs_names = [\n+        line.split()[-1] for line in text.splitlines() if \"nix-shell -p\" in line\n+    ]\n+    return nixpkgs_names\n+\n+\n+def match(command):\n+    bin = command.script_parts[0]\n+    return (\n+        \"command not found\" in command.output  # only match commands which had exit code: 127                   # noqa: E501\n+        and get_nixpkgs_names(bin)             # only match commands which could be made available through nix  # noqa: E501\n+    )\n+\n+\n+def get_new_command(command):\n+    bin = command.script_parts[0]\n+    nixpkgs_names = get_nixpkgs_names(bin)\n+\n+    # Construct a command for each package name\n+    commands = [\n+        'nix-shell -p {} --run \"{}\"'.format(name, command.script)\n+        for name in nixpkgs_names\n+    ]\n+    return commands\n"
  },
  {
    "index": 11,
    "filtered_comments": [
      "I'm getting this error when try to use MPS\r\n\r\n/Users/diego/.pyenv/versions/3.10.6/lib/python3.10/site-packages/whisper-1.0-py3.10.egg/whisper/decoding.py:629: UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/diego/Projects/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/AppleInternal/Library/BuildRoots/2d9b4df9-4b93-11ed-b0fc-2e32217d8374/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:794: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 23200 bytes\r\n'\r\nAbort trap: 6\r\n/Users/diego/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n\r\nany clues?",
      "@DiegoGiovany Not an expert on this but It looks like PyTorch itself is missing some operators for MPS. See for example\r\nhttps://github.com/pytorch/pytorch/issues/77764#issuecomment-1254352628\r\n(which refers to repeat_interleave)\r\n\r\nand\r\nhttps://github.com/pytorch/pytorch/issues/87219\r\n",
      "Thanks for your work. I just tried this. Unfortunately, it didn't work for me on my m1 max with 32GB.\r\nHere is what I did:\r\npip install git+https://github.com/openai/whisper.git@refs/pull/382/head\r\n\r\nNo errors on install and it works fine when run without mps: whisper audiofile_name --model medium \r\n\r\nWhen I run: whisper audiofile_name --model medium --device mps\r\n\r\nHere is the error I get:\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/810eba08-405a-11ed-86e9-6af958a02716/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x1024x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s). \r\n\r\nWhen I run:  whisper audiofile_name --model medium --device mps --fp16 False\r\n\r\nHere is the error I get:\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: English\r\n/anaconda3/lib/python3.9/site-packages/whisper/decoding.py:633: UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/AppleInternal/Library/BuildRoots/f0468ab4-4115-11ed-8edc-7ef33c48bc85/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:794: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 1007280 bytes\r\n\r\nBasically, same error as @DiegoGiovany.\r\n\r\nAny ideas on how to fix?",
      "@dwarkeshsp \r\n\r\nnot workwith mbp2015 pytorch 1.3 stableegpu RX580, MacOS 12.3.\r\n\r\nchanged the code as the same as yours.\r\n\r\nchanged  to use --device mps but show error, maybe there is still somewhere to change or modify.\r\n\r\nuse --device cpu, it works.\r\n\r\nwith other pytorch-metal project, MPS works.",
      "I also see the same errors as others mentioned above, on an M1 Mac running arm64 Python. ",
      "On an M1 16\" MBP with 16GB running MacOS 13.0.1, I'm seeing the following with `openai-whisper-20230117`:\r\n\r\nUsing this command:\r\n```(venv) whisper_ai_playground % whisper './test_file.mp3' --model tiny.en --output_dir ./output --device mps```\r\n\r\nI'm encountering the following errors:\r\n\r\n```loc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/810eba08-405a-11ed-86e9-6af958a02716/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x384x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible```\r\n\r\n```LLVM ERROR: Failed to infer result type(s).```\r\n\r\n```zsh: abort      whisper  --model tiny.en --output_dir ./output --device mps```\r\n\r\n```/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '```",
      "Same problem with osx 13.2 in MacBook Pro M2 max:\r\n\r\n```\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x1280x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\nzsh: abort      whisper audio.wav --language en --model large\r\nm2@Render ~ % /opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```",
      "I'm getting the same error as @renderpci using the M1 Base Model\r\n```bash\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x512x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\n[1]    3746 abort      python3 test.py\r\n```\r\n**test.py:**\r\n```py\r\nimport whisper\r\n\r\nmodel = whisper.load_model(\"base\")\r\nresult = model.transcribe(\"audio.mp3\")\r\nprint(result[\"text\"])\r\n```",
      "FWIW I switched to the C++ port https://github.com/ggerganov/whisper.cpp/ and got a ~15x speedup compared to CPU pytorch on my M1 Pro. (But note that it doesn't have all the features/flags from the official whisper repo.)",
      "> FWIW I switched to the C++ port https://github.com/ggerganov/whisper.cpp/ \r\n\r\nFor us whisper.cpp is not an option:\r\n\r\n> **Should I use whisper.cpp in my project?**\r\n> \r\n> whisper.cpp is a hobby project. It does not strive to provide a production ready implementation. The main goals of the implementation is to be educational, minimalistic, portable, hackable and performant. There are no guarantees that the implementation is correct and bug-free and stuff can break at any point in the future. Support and updates will depend mostly on contributions, since with time I will move on and won't dedicate too much time on the project.\r\n> \r\n> If you plan to use whisper.cpp in your own project, keep in mind the above.\r\n> My advice is to not put all your eggs into the whisper.cpp basket.",
      "The same error as @renderpci using the M2\r\n\r\n\r\nwhisper interview.mp4 --language en --model large --device mps\r\n\r\n```\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x1280x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\nzsh: abort      whisper interview.mp4 --language en --model large --device mps\r\npac@dd ~ % /opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```",
      "Hey @devpacdd  - this should be fixed in latest pytorch nightly (pip3 install --pre --force-reinstall torch --index-url https://download.pytorch.org/whl/nightly/cpu). Let me know if you still see any issues. Thanks",
      "Still have the same error after updating\r\n\r\nEdit: After adding `--fp16 False` to the command, I now get a new error, as well as the old one:\r\n```\r\n/opt/homebrew/lib/python3.10/site-packages/whisper/decoding.py:633: UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/AppleInternal/Library/BuildRoots/5b8a32f9-5db2-11ed-8aeb-7ef33c48bc85/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:794: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 1007280 bytes\r\n'\r\nzsh: abort      whisper --model large --language de --task transcribe  --device mps --fp16\r\n/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```",
      "i was able to get it to kinda work: https://github.com/davabase/whisper_real_time/issues/5#issue-1596258783",
      "> The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n>   audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n\r\n@manuthebyte could you please make sure you are on a recent nightly? `repeat_interleave` should be natively supported. If you could try grabbing today's nightly and give a try that would be awesome! (You can get today's nightly with `pip3 install --pre --force-reinstall torch==2.0.0.dev20230224 --index-url https://download.pytorch.org/whl/nightly/cpu`)\r\n\r\n",
      "Wow! \r\n\r\nwhen running:\r\n`Python3 transcribe_demo.py --model medium` (from https://github.com/davabase/whisper_real_time)\r\n\r\nwith the following packages in my pipenv's requirements.txt\r\n```\r\ncertifi==2022.12.7\r\ncharset-normalizer==3.0.1\r\nffmpeg-python==0.2.0\r\nfilelock==3.9.0\r\nfuture==0.18.3\r\nhuggingface-hub==0.12.1\r\nidna==3.4\r\nmore-itertools==9.0.0\r\nmpmath==1.2.1\r\nnetworkx==3.0rc1\r\nnumpy==1.24.2\r\nopenai-whisper @ git+https://github.com/openai/whisper.git@51c785f7c91b8c032a1fa79c0e8f862dea81b860\r\npackaging==23.0\r\nPillow==9.4.0\r\nPyAudio==0.2.13\r\nPyYAML==6.0\r\nregex==2022.10.31\r\nrequests==2.28.2\r\nSpeechRecognition==3.9.0\r\nsympy==1.11.1\r\ntokenizers==0.13.2\r\ntorch==2.0.0.dev20230224\r\ntorchaudio==0.13.1\r\ntorchvision==0.14.1\r\ntqdm==4.64.1\r\ntransformers==4.26.1\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.14\r\n```\r\n\r\nit gets every word! while i was singing! in realtime, with maybe 50%~ gpu usage on the apple M2 Pro Max.",
      "> > The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n> > audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n> \r\n> @manuthebyte could you please make sure you are on a recent nightly? `repeat_interleave` should be natively supported. If you could try grabbing today's nightly and give a try that would be awesome! (You can get today's nightly with `pip3 install --pre --force-reinstall torch==2.0.0.dev20230224 --index-url https://download.pytorch.org/whl/nightly/cpu`)\r\n\r\nWith my pip3 freeze being:\r\n```\r\nbeautifulsoup4==4.11.2\r\ncertifi==2022.12.7\r\ncharset-normalizer==3.0.1\r\ncolorama==0.4.6\r\ndnspython==2.3.0\r\nffmpeg-python==0.2.0\r\nfilelock==3.9.0\r\nfuture==0.18.3\r\nhuggingface-hub==0.12.1\r\nidna==3.4\r\nmore-itertools==9.0.0\r\nmpmath==1.2.1\r\nnetworkx==3.0rc1\r\nnumpy==1.24.2\r\nopenai-whisper @ git+https://github.com/openai/whisper.git@7858aa9c08d98f75575035ecd6481f462d66ca27\r\npackaging==23.0\r\nprotobuf==4.21.12\r\nPyYAML==6.0\r\nregex==2022.10.31\r\nrequests==2.28.2\r\nsix==1.16.0\r\nsoupsieve==2.4\r\nsympy==1.11.1\r\ntokenizers==0.13.2\r\ntorch==2.0.0.dev20230224\r\ntqdm==4.64.1\r\ntransformers==4.26.1\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.14\r\n```\r\n\r\nIt now seems to use the GPU but I now get these errors:\r\n```\r\n/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py:636: UserWarning: 0MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py:443: UserWarning: 1MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:143.)\r\n  timestamp_logprob = logprobs[k, self.tokenizer.timestamp_begin :].logsumexp(dim=-1)\r\n/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py:444: UserWarning: 1MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1269.)\r\n  max_text_token_logprob = logprobs[k, : self.tokenizer.timestamp_begin].max()\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/bin/whisper\", line 8, in <module>\r\n    sys.exit(cli())\r\n             ^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py\", line 314, in cli\r\n    result = transcribe(model, audio_path, temperature=temperature, **args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py\", line 183, in transcribe\r\n    result: DecodingResult = decode_with_fallback(segment)\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py\", line 118, in decode_with_fallback\r\n    decode_result = model.decode(segment, options)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 707, in decode\r\n    result = DecodingTask(model, options).run(mel)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 640, in run\r\n    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\r\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 609, in _main_loop\r\n    tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 258, in update\r\n    next_tokens = Categorical(logits=logits / self.temperature).sample()\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/distributions/categorical.py\", line 66, in __init__\r\n    super().__init__(batch_shape, validate_args=validate_args)\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/distributions/distribution.py\", line 62, in __init__\r\n    raise ValueError(\r\nValueError: Expected parameter logits (Tensor of shape (5, 51865)) of distribution Categorical(logits: torch.Size([5, 51865])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\r\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan]], device='mps:0')\r\n```\r\n\r\nWhen running the command `whisper --model small --language en --task transcribe ***.wav --device mps`",
      "> Hey @devpacdd - this should be fixed in latest pytorch nightly (pip3 install --pre --force-reinstall torch --index-url https://download.pytorch.org/whl/nightly/cpu). Let me know if you still see any issues. Thanks\r\n\r\nGeart! it works!\r\nBut.. In my test the GPU is slow than CPU... ??? \r\n\r\nAudio to transcribe: 1 minute with model large, language catalan\r\n\r\nCPU  : 2m : 33 s\r\nGPU (--device mps): 4m : 54 s\r\n\r\nI tried with different files and the result was the same; +/- double time with GPU enable.\r\n\r\nIt's normal? I expected less time for GPU than CPU.\r\n\r\nBest",
      "I get this error while trying to use MPS\r\n\r\nHere is the command I am running: `whisper --model large --language en --task transcribe test.mp3 --device mps`\r\n\r\n```\r\n$ whisper --model large --language en --task transcribe test.mp3 --device mps\r\nTraceback (most recent call last):\r\n  File \"/Users/mukul/miniconda3/envs/ml/bin/whisper\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/whisper/transcribe.py\", line 433, in cli\r\n    model = load_model(model_name, device=device, download_root=model_dir)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/whisper/__init__.py\", line 159, in load_model\r\n    return model.to(device)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1170, in to\r\n    return self._apply(convert)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 869, in _apply\r\n    self._buffers[key] = fn(buf)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1168, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\nNotImplementedError: Could not run 'aten::empty.memory_format' with arguments from the 'SparseMPS' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, MPS, Meta, QuantizedCPU, QuantizedMeta, MkldnnCPU, SparseCPU, SparseMeta, SparseCsrCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\r\n\r\nCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31085 [kernel]\r\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMPS.cpp:24065 [kernel]\r\nMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26824 [kernel]\r\nQuantizedCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\r\nQuantizedMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterQuantizedMeta.cpp:105 [kernel]\r\nMkldnnCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMkldnnCPU.cpp:507 [kernel]\r\nSparseCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1379 [kernel]\r\nSparseMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseMeta.cpp:249 [kernel]\r\nSparseCsrCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1128 [kernel]\r\nBackendSelect: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:734 [kernel]\r\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\r\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\r\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\r\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\r\nConjugate: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\r\nNegative: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:23 [kernel]\r\nZeroTensor: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:90 [kernel]\r\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\r\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16872 [kernel]\r\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\r\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\r\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\r\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\r\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\r\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\r\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\r\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\r\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\r\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\r\n```",
      "@mukulpatnaik \r\nMy device is M1 MacBook Pro, I got the same error with the latest version of whisper([v20230314](https://github.com/openai/whisper/releases/tag/v20230314)), then I switch to [v20230124](https://github.com/openai/whisper/releases/tag/v20230124), every thing works fine. (torch nightly version)\r\n\r\nBut, seems like mps is slower than cpu like @renderpci reported, for my task\r\n* cpu 3.26 s\r\n* mps 5.25 s\r\n* cpu+torch2 compile 3.31 s\r\n* mps+torch2 compile 4.94 s\r\n\r\n"
    ],
    "code_diff": "diff --git a/whisper/__init__.py b/whisper/__init__.py\nindex 2a1fb4ec6..4f45f9969 100644\n--- a/whisper/__init__.py\n+++ b/whisper/__init__.py\n@@ -92,7 +92,12 @@ def load_model(name: str, device: Optional[Union[str, torch.device]] = None, dow\n     \"\"\"\n \n     if device is None:\n-        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+        if torch.cuda.is_available():\n+            device = \"cuda\"\n+        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n+            device = \"mps\"\n+        else:\n+            device = \"cpu\"\n     if download_root is None:\n         download_root = os.getenv(\n             \"XDG_CACHE_HOME\", \ndiff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex d95d3336d..0d89a5d77 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -75,6 +75,8 @@ def transcribe(\n     if model.device == torch.device(\"cpu\"):\n         if torch.cuda.is_available():\n             warnings.warn(\"Performing inference on CPU when CUDA is available\")\n+        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n+            warnings.warn(\"Performing inference on CPU when MPS is available\")\n         if dtype == torch.float16:\n             warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n             dtype = torch.float32\n"
  },
  {
    "index": 12,
    "filtered_comments": [
      "Hi!\r\nI tried out this branch with ```kwargs['word_level_timestamps'] = True``` but the model performed very slowly. In addition (or rather because of) it started to hallucinate like mad. \r\nIm using chunks of short (couple of seconds) audio data in german produced by a VAD for live transcription.\r\n\r\nMaybe its a problem on my side, maybe anyone can try to reproduce?",
      "I found an interesting edge case with the `small` model where enabling the word-level timestamps option causes it to repeat the prompt at the end of the audio while also failing to infer the last word.\r\n\r\n```bash\r\n$ ffmpeg -t 29 -i https://audio2.redcircle.com/episodes/6b196013-8672-43d9-be52-4332b3207d93/stream.mp3 test.mp3\r\n\r\n$ whisper --model small test.mp3\r\n.../whisper/transcribe.py:98: UserWarning: FP16 is not supported on CPU; using FP32 instead\r\n  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: English\r\n[00:00.000 --> 00:15.920]  Military veteran Eric Weinstein began 69 Whiskey as a college radio show on 107.7 The\r\n[00:15.920 --> 00:21.720]  Bronx, located on the campus of Ryder University in Lawrenceville, New Jersey.\r\n[00:21.720 --> 00:27.560]  A show once restrained by rules and boundaries now comes straight to you raw, uncensored and\r\n[00:27.560 --> 00:28.960]  unapologetic.\r\n\r\n$ whisper --model small --output_format json --word_timestamps True test.mp3\r\n.../whisper/transcribe.py:98: UserWarning: FP16 is not supported on CPU; using FP32 instead\r\n  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: English\r\n[00:08.040 --> 00:15.940]  Military veteran Eric Weinstein began 69 Whiskey as a college radio show on 107.7 The\r\n[00:15.940 --> 00:21.320]  Bronx, located on the campus of Ryder University in Lawrenceville, New Jersey.\r\n[00:21.720 --> 00:28.980]  A show once restrained by rules and boundaries now comes straight to you raw, uncensored and\r\n[00:28.960 --> 00:28.960]  Military veteran Eric Weinstein began 69 Whiskey as a college radio show on 107.7 The\r\n[00:28.960 --> 00:28.960]  Bronx, located on the campus of Ryder University in Lawrenceville, New Jersey.\r\n[00:28.960 --> 00:28.960]  A show once restrained by rules and boundaries now comes straight to you raw, uncensored and\r\n[00:28.960 --> 00:28.960]  Military veteran Eric Weinstein began 69 Whiskey as a college radio show on 107.7 The\r\n[00:28.960 --> 00:28.960]  Bronx, located on the campus of Ryder University in Lawrenceville, New Jersey.\r\n```",
      "Hi @jongwook ,\r\nSince you first release the notebook to obtain word-level timestamps I've been working on this to add to whisper process. And I've tried to test other alingment methods than DTW. Have you tried something else and found out that it works better?\r\n\r\nAlso, I've been struggling a lot with alucinations, specially for spanish content. I've create a cleaner function at segmet level, is there any smarter way?",
      "Hi @IgnacioSan22, the custom DTW implementation in this PR was for the license issue as noted by others and also for the speed. An alternative is to use the timestamp predictions from the model, but we found that it's less reliable than using the attention patterns like in this PR. If you have solutions using any other algorithms for alignment, please let me know!\r\n\r\nThe community had some success handling hallucinations by preprocessing the inputs with VAD, like:\r\n\r\n- #679\r\n- #397\r\n- https://github.com/m-bain/whisperX\r\n\r\n---\r\n\r\nHi @ioskevinshah, this feature is still experimental but we do plan to add it to the API as an option, once we're sure that it's reliable enough.",
      "> Hi @IgnacioSan22, the custom DTW implementation in this PR was for the license issue as noted by others and also for the speed. An alternative is to use the timestamp predictions from the model, but we found that it's less reliable than using the attention patterns like in this PR. If you have solutions using any other algorithms for alignment, please let me know!\r\n> \r\n> The community had some success handling hallucinations by preprocessing the inputs with VAD, like:\r\n> \r\n> * [A possible solution to Whisper hallucination#679](https://github.com/openai/whisper/discussions/679)\r\n> * [Whisper WebUI with a VAD for more accurate non-English transcripts (Japanese)#397](https://github.com/openai/whisper/discussions/397)\r\n> * https://github.com/m-bain/whisperX\r\n> \r\n> Hi @ioskevinshah, this feature is still experimental but we do plan to add it to the API as an option, once we're sure that it's reliable enough.\r\n\r\nHi @jongwook, I've tried the hungarian algorithm and in some cases the results are better, however due to the lack of resources I'm not capable to perform a proper study to find the best alingment algorithm. For hallucinations I've developed a postprocess functions that cleans the segments. It improves quite a lot, but I'll check those references. \r\n\r\nThanks",
      "> Hi @IgnacioSan22, the custom DTW implementation in this PR was for the license issue as noted by others and also for the speed. An alternative is to use the timestamp predictions from the model, but we found that it's less reliable than using the attention patterns like in this PR. If you have solutions using any other algorithms for alignment, please let me know!\r\n> \r\n> The community had some success handling hallucinations by preprocessing the inputs with VAD, like:\r\n> \r\n> * [A possible solution to Whisper hallucination#679](https://github.com/openai/whisper/discussions/679)\r\n> * [Whisper WebUI with a VAD for more accurate non-English transcripts (Japanese)#397](https://github.com/openai/whisper/discussions/397)\r\n> * https://github.com/m-bain/whisperX\r\n> \r\n> Hi @ioskevinshah, this feature is still experimental but we do plan to add it to the API as an option, once we're sure that it's reliable enough.\r\n\r\nany workaround or logic after the API response?"
    ],
    "code_diff": "diff --git a/.github/workflows/test.yml b/.github/workflows/test.yml\nindex c5b4eeccd..f06bff79a 100644\n--- a/.github/workflows/test.yml\n+++ b/.github/workflows/test.yml\n@@ -21,6 +21,5 @@ jobs:\n       - run: conda install -n test ffmpeg python=${{ matrix.python-version }} pytorch=${{ matrix.pytorch-version }} cpuonly -c pytorch\n       - uses: actions/checkout@v2\n       - run: echo \"$CONDA/envs/test/bin\" >> $GITHUB_PATH\n-      - run: pip install pytest\n-      - run: pip install .\n-      - run: pytest --durations=0 -vv -k 'not test_transcribe or test_transcribe[tiny] or test_transcribe[tiny.en]'\n+      - run: pip install .[\"dev\"]\n+      - run: pytest --durations=0 -vv -k 'not test_transcribe or test_transcribe[tiny] or test_transcribe[tiny.en]' -m 'not requires_cuda'\ndiff --git a/requirements.txt b/requirements.txt\nindex a46140353..45ecbe769 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,3 +1,4 @@\n+numba\n numpy\n torch\n tqdm\ndiff --git a/setup.py b/setup.py\nindex 0e822ab9e..a548c8d35 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -1,4 +1,5 @@\n import os\n+import sys\n \n import pkg_resources\n from setuptools import setup, find_packages\n@@ -9,6 +10,21 @@ def read_version(fname=\"whisper/version.py\"):\n     return locals()[\"__version__\"]\n \n \n+requirements = []\n+if sys.platform.startswith(\"linux\"):\n+    triton_requirement = \"triton>=2.0.0.dev20221202\"\n+    try:\n+        import re\n+        import subprocess\n+        version_line = subprocess.check_output([\"nvcc\", \"--version\"]).strip().split(b\"\\n\")[-1]\n+        major, minor = re.findall(rb\"([\\d]+)\\.([\\d]+)\", version_line)[0]\n+        if (int(major), int(minor)) < (11, 4):\n+            # the last version supporting CUDA < 11.4\n+            triton_requirement = \"triton==2.0.0.dev20221011\"\n+    except (IndexError, OSError, subprocess.SubprocessError):\n+        pass\n+    requirements.append(triton_requirement)\n+\n setup(\n     name=\"openai-whisper\",\n     py_modules=[\"whisper\"],\n@@ -22,7 +38,7 @@ def read_version(fname=\"whisper/version.py\"):\n     url=\"https://github.com/openai/whisper\",\n     license=\"MIT\",\n     packages=find_packages(exclude=[\"tests*\"]),\n-    install_requires=[\n+    install_requires=requirements + [\n         str(r)\n         for r in pkg_resources.parse_requirements(\n             open(os.path.join(os.path.dirname(__file__), \"requirements.txt\"))\n@@ -32,5 +48,5 @@ def read_version(fname=\"whisper/version.py\"):\n         \"console_scripts\": [\"whisper=whisper.transcribe:cli\"],\n     },\n     include_package_data=True,\n-    extras_require={\"dev\": [\"pytest\"]},\n+    extras_require={\"dev\": [\"pytest\", \"scipy\"]},\n )\ndiff --git a/tests/conftest.py b/tests/conftest.py\nnew file mode 100644\nindex 000000000..31f1d6b48\n--- /dev/null\n+++ b/tests/conftest.py\n@@ -0,0 +1,14 @@\n+import random as rand\n+\n+import numpy\n+import pytest\n+\n+\n+def pytest_configure(config):\n+    config.addinivalue_line(\"markers\", \"requires_cuda\")\n+\n+\n+@pytest.fixture\n+def random():\n+    rand.seed(42)\n+    numpy.random.seed(42)\ndiff --git a/tests/test_timing.py b/tests/test_timing.py\nnew file mode 100644\nindex 000000000..50a2583f6\n--- /dev/null\n+++ b/tests/test_timing.py\n@@ -0,0 +1,87 @@\n+import pytest\n+import numpy as np\n+import scipy.ndimage\n+import torch\n+\n+from whisper.timing import dtw_cpu, dtw_cuda, median_filter\n+\n+\n+sizes = [\n+    (10, 20), (32, 16), (123, 1500), (234, 189),\n+]\n+shapes = [\n+    (10,), (1, 15),  (4, 5, 345), (6, 12, 240, 512),\n+]\n+\n+\n+@pytest.mark.parametrize(\"N, M\", sizes)\n+def test_dtw(N: int, M: int):\n+    steps = np.concatenate([np.zeros(N - 1), np.ones(M - 1)])\n+    np.random.shuffle(steps)\n+    x = np.random.random((N, M)).astype(np.float32)\n+\n+    i, j, k = 0, 0, 0\n+    trace = []\n+    while True:\n+        x[i, j] -= 1\n+        trace.append((i, j))\n+\n+        if k == len(steps):\n+            break\n+\n+        if k + 1 < len(steps) and steps[k] != steps[k + 1]:\n+            i += 1\n+            j += 1\n+            k += 2\n+            continue\n+\n+        if steps[k] == 0:\n+            i += 1\n+        if steps[k] == 1:\n+            j += 1\n+        k += 1\n+\n+    trace = np.array(trace).T\n+    dtw_trace = dtw_cpu(x)\n+\n+    assert np.allclose(trace, dtw_trace)\n+\n+\n+@pytest.mark.requires_cuda\n+@pytest.mark.parametrize(\"N, M\", sizes)\n+def test_dtw_cuda_equivalence(N: int, M: int):\n+    x_numpy = np.random.randn(N, M).astype(np.float32)\n+    x_cuda = torch.from_numpy(x_numpy).cuda()\n+\n+    trace_cpu = dtw_cpu(x_numpy)\n+    trace_cuda = dtw_cuda(x_cuda)\n+\n+    assert np.allclose(trace_cpu, trace_cuda)\n+\n+\n+@pytest.mark.parametrize(\"shape\", shapes)\n+def test_median_filter(shape):\n+    x = torch.randn(*shape)\n+\n+    for filter_width in [3, 5, 7, 13]:\n+        filtered = median_filter(x, filter_width)\n+\n+        # using np.pad to reflect-pad, because Scipy's behavior is different near the edges.\n+        pad_width = filter_width // 2\n+        padded_x = np.pad(x, [(0, 0)] * (x.ndim - 1) + [(pad_width, pad_width)], mode=\"reflect\")\n+        scipy_filtered = scipy.ndimage.median_filter(padded_x, [1] * (x.ndim - 1) + [filter_width])\n+        scipy_filtered = scipy_filtered[..., pad_width:-pad_width]\n+\n+        assert np.allclose(filtered, scipy_filtered)\n+\n+\n+@pytest.mark.requires_cuda\n+@pytest.mark.parametrize(\"shape\", shapes)\n+def test_median_filter_equivalence(shape):\n+    x = torch.randn(*shape)\n+\n+    for filter_width in [3, 5, 7, 13]:\n+        filtered_cpu = median_filter(x, filter_width)\n+        filtered_gpu = median_filter(x.cuda(), filter_width).cpu()\n+\n+        assert np.allclose(filtered_cpu, filtered_gpu)\ndiff --git a/tests/test_transcribe.py b/tests/test_transcribe.py\nindex f5d66c37f..9802f734b 100644\n--- a/tests/test_transcribe.py\n+++ b/tests/test_transcribe.py\n@@ -13,10 +13,22 @@ def test_transcribe(model_name: str):\n     audio_path = os.path.join(os.path.dirname(__file__), \"jfk.flac\")\n \n     language = \"en\" if model_name.endswith(\".en\") else None\n-    result = model.transcribe(audio_path, language=language, temperature=0.0)\n+    result = model.transcribe(audio_path, language=language, temperature=0.0, word_timestamps=True)\n     assert result[\"language\"] == \"en\"\n \n     transcription = result[\"text\"].lower()\n     assert \"my fellow americans\" in transcription\n     assert \"your country\" in transcription\n     assert \"do for you\" in transcription\n+\n+    timing_checked = False\n+    for segment in result[\"segments\"]:\n+        for timing in segment[\"words\"]:\n+            assert timing[\"start\"] < timing[\"end\"]\n+            if timing[\"word\"].strip(\" ,\") == \"Americans\":\n+                assert timing[\"start\"] <= 1.8\n+                assert timing[\"end\"] >= 1.8\n+                print(timing)\n+                timing_checked = True\n+\n+    assert timing_checked\ndiff --git a/whisper/__init__.py b/whisper/__init__.py\nindex cb334065e..26d1e0eaf 100644\n--- a/whisper/__init__.py\n+++ b/whisper/__init__.py\n@@ -29,6 +29,23 @@\n     \"large\": \"https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt\",\n }\n \n+# base85-encoded (n_layers, n_heads) boolean arrays indicating the cross-attention heads that are\n+# highly correlated to the word-level timing, i.e. the alignment between audio and text tokens.\n+_ALIGNMENT_HEADS = {\n+    \"tiny.en\": b\"ABzY8J1N>@0{>%R00Bk>$p{7v037`oCl~+#00\",\n+    \"tiny\": b\"ABzY8bu8Lr0{>%RKn9Fp%m@SkK7Kt=7ytkO\",\n+    \"base.en\": b\"ABzY8;40c<0{>%RzzG;p*o+Vo09|#PsxSZm00\",\n+    \"base\": b\"ABzY8KQ!870{>%RzyTQH3`Q^yNP!>##QT-<FaQ7m\",\n+    \"small.en\": b\"ABzY8>?_)10{>%RpeA61k&I|OI3I$65C{;;pbCHh0B{qLQ;+}v00\",\n+    \"small\": b\"ABzY8DmU6=0{>%Rpa?J`kvJ6qF(V^F86#Xh7JUGMK}P<N0000\",\n+    \"medium.en\": b\"ABzY8usPae0{>%R7<zz_OvQ{)4kMa0BMw6u5rT}kRKX;$NfYBv00*Hl@qhsU00\",\n+    \"medium\": b\"ABzY8B0Jh+0{>%R7}kK1fFL7w6%<-Pf*t^=N)Qr&0RR9\",\n+    \"large-v1\": b\"ABzY8r9j$a0{>%R7#4sLmoOs{s)o3~84-RPdcFk!JR<kSfC2yj\",\n+    \"large-v2\": b'ABzY8zd+h!0{>%R7=D0pU<_bnWW*tkYAhobTNnu$jnkEkXqp)j;w1Tzk)UH3X%SZd&fFZ2fC2yj',\n+    \"large\": b'ABzY8zd+h!0{>%R7=D0pU<_bnWW*tkYAhobTNnu$jnkEkXqp)j;w1Tzk)UH3X%SZd&fFZ2fC2yj',\n+}\n+\n+\n \n def _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:\n     os.makedirs(root, exist_ok=True)\n@@ -106,8 +123,10 @@ def load_model(name: str, device: Optional[Union[str, torch.device]] = None, dow\n \n     if name in _MODELS:\n         checkpoint_file = _download(_MODELS[name], download_root, in_memory)\n+        alignment_heads = _ALIGNMENT_HEADS[name]\n     elif os.path.isfile(name):\n         checkpoint_file = open(name, \"rb\").read() if in_memory else name\n+        alignment_heads = None\n     else:\n         raise RuntimeError(f\"Model {name} not found; available models = {available_models()}\")\n \n@@ -119,4 +138,7 @@ def load_model(name: str, device: Optional[Union[str, torch.device]] = None, dow\n     model = Whisper(dims)\n     model.load_state_dict(checkpoint[\"model_state_dict\"])\n \n+    if alignment_heads is not None:\n+        model.set_alignment_heads(alignment_heads)\n+\n     return model.to(device)\ndiff --git a/whisper/audio.py b/whisper/audio.py\nindex de8a19514..964d41579 100644\n--- a/whisper/audio.py\n+++ b/whisper/audio.py\n@@ -18,6 +18,10 @@\n N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000: number of samples in a chunk\n N_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000: number of frames in a mel spectrogram input\n \n+N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n+FRAMES_PER_SECOND = exact_div(SAMPLE_RATE, HOP_LENGTH)  # 100 mel frames in 1s (10ms each)\n+TOKENS_PER_SECOND = exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 50 audio tokens in 1s (20ms each)\n+\n \n def load_audio(file: str, sr: int = SAMPLE_RATE):\n     \"\"\"\ndiff --git a/whisper/model.py b/whisper/model.py\nindex be73a4a87..a1ab2e349 100644\n--- a/whisper/model.py\n+++ b/whisper/model.py\n@@ -1,3 +1,5 @@\n+import base64\n+import gzip\n from dataclasses import dataclass\n from typing import Dict\n from typing import Iterable, Optional\n@@ -8,8 +10,8 @@\n from torch import Tensor\n from torch import nn\n \n-from .transcribe import transcribe as transcribe_function\n from .decoding import detect_language as detect_language_function, decode as decode_function\n+from .transcribe import transcribe as transcribe_function\n \n \n @dataclass\n@@ -213,6 +215,15 @@ def __init__(self, dims: ModelDimensions):\n             self.dims.n_text_head,\n             self.dims.n_text_layer,\n         )\n+        # use the last half layers for alignment by default; see `set_alignment_heads()` below\n+        all_heads = torch.zeros(self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool)\n+        all_heads[self.dims.n_text_layer // 2:] = True\n+        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n+\n+    def set_alignment_heads(self, dump: bytes):\n+        array = np.frombuffer(gzip.decompress(base64.b85decode(dump)), dtype=bool).copy()\n+        mask = torch.from_numpy(array).reshape(self.dims.n_text_layer, self.dims.n_text_head)\n+        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n \n     def embed_audio(self, mel: torch.Tensor):\n         return self.encoder(mel)\ndiff --git a/whisper/timing.py b/whisper/timing.py\nnew file mode 100644\nindex 000000000..98927aa04\n--- /dev/null\n+++ b/whisper/timing.py\n@@ -0,0 +1,305 @@\n+import subprocess\n+import warnings\n+from dataclasses import dataclass\n+from typing import List, TYPE_CHECKING\n+\n+import numba\n+import numpy as np\n+import torch\n+import torch.nn.functional as F\n+\n+from .audio import HOP_LENGTH, SAMPLE_RATE, TOKENS_PER_SECOND\n+from .tokenizer import Tokenizer\n+\n+if TYPE_CHECKING:\n+    from .model import Whisper\n+\n+\n+def median_filter(x: torch.Tensor, filter_width: int):\n+    \"\"\"Apply a median filter of width `filter_width` along the last dimension of `x`\"\"\"\n+    pad_width = filter_width // 2\n+    if x.shape[-1] <= pad_width:\n+        # F.pad requires the padding width to be smaller than the input dimension\n+        return x\n+\n+    if (ndim := x.ndim) <= 2:\n+        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D\n+        x = x[None, None, :]\n+\n+    assert filter_width > 0 and filter_width % 2 == 1, \"`filter_width` should be an odd number\"\n+\n+    result = None\n+    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=\"reflect\")\n+    if x.is_cuda:\n+        try:\n+            from .triton_ops import median_filter_cuda\n+            result = median_filter_cuda(x, filter_width)\n+        except (RuntimeError, subprocess.CalledProcessError):\n+            warnings.warn(\n+                \"Failed to launch Triton kernels, likely due to missing CUDA toolkit; \"\n+                \"falling back to a slower median kernel implementation...\"\n+            )\n+\n+    if result is None:\n+        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)\n+        result = x.unfold(-1, filter_width, 1).sort()[0][..., filter_width // 2]\n+\n+    if ndim <= 2:\n+        result = result[0, 0]\n+\n+    return result\n+\n+@numba.jit\n+def backtrace(trace: np.ndarray):\n+    i = trace.shape[0] - 1\n+    j = trace.shape[1] - 1\n+    trace[0, :] = 2\n+    trace[:, 0] = 1\n+\n+    result = []\n+    while i > 0 or j > 0:\n+        result.append((i - 1, j - 1))\n+\n+        if trace[i, j] == 0:\n+            i -= 1\n+            j -= 1\n+        elif trace[i, j] == 1:\n+            i -= 1\n+        elif trace[i, j] == 2:\n+            j -= 1\n+        else:\n+            raise ValueError(\"Unexpected trace[i, j]\")\n+\n+    result = np.array(result)\n+    return result[::-1, :].T\n+\n+\n+@numba.jit(nopython=True, parallel=True)\n+def dtw_cpu(x: np.ndarray):\n+    N, M = x.shape\n+    cost = np.ones((N + 1, M + 1), dtype=np.float32) * np.inf\n+    trace = -np.ones((N + 1, M + 1), dtype=np.float32)\n+\n+    cost[0, 0] = 0\n+    for j in range(1, M + 1):\n+        for i in range(1, N + 1):\n+            c0 = cost[i - 1, j - 1]\n+            c1 = cost[i - 1, j]\n+            c2 = cost[i, j - 1]\n+\n+            if c0 < c1 and c0 < c2:\n+                c, t = c0, 0\n+            elif c1 < c0 and c1 < c2:\n+                c, t = c1, 1\n+            else:\n+                c, t = c2, 2\n+\n+            cost[i, j] = x[i - 1, j - 1] + c\n+            trace[i, j] = t\n+\n+    return backtrace(trace)\n+\n+\n+def dtw_cuda(x, BLOCK_SIZE=1024):\n+    from .triton_ops import dtw_kernel\n+\n+    M, N = x.shape\n+    assert M < BLOCK_SIZE, f\"M should be smaller than {BLOCK_SIZE=}\"\n+\n+    x_skew = F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M)\n+    x_skew = x_skew.T.contiguous()\n+    cost = torch.ones(N + M + 2, M + 2) * np.inf\n+    cost[0, 0] = 0\n+    cost = cost.cuda()\n+    trace = torch.zeros_like(cost, dtype=torch.int32)\n+\n+    dtw_kernel[(1,)](\n+        cost,\n+        trace,\n+        x_skew,\n+        x_skew.stride(0),\n+        cost.stride(0),\n+        trace.stride(0),\n+        N,\n+        M,\n+        BLOCK_SIZE=BLOCK_SIZE\n+    )\n+\n+    trace = trace.T.flatten()[:(M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[:, :N + 1]\n+    return backtrace(trace.cpu().numpy())\n+\n+\n+def dtw(x: torch.Tensor) -> np.ndarray:\n+    if x.is_cuda:\n+        try:\n+            return dtw_cuda(x)\n+        except (RuntimeError, subprocess.CalledProcessError):\n+            warnings.warn(\n+                \"Failed to launch Triton kernels, likely due to missing CUDA toolkit; \"\n+                \"falling back to a slower DTW implementation...\"\n+            )\n+\n+    return dtw_cpu(x.double().cpu().numpy())\n+\n+\n+@dataclass\n+class WordTiming:\n+    word: str\n+    tokens: List[int]\n+    start: float\n+    end: float\n+    probability: float\n+\n+\n+def find_alignment(\n+    model: \"Whisper\",\n+    tokenizer: Tokenizer,\n+    text_tokens: List[int],\n+    mel: torch.Tensor,\n+    num_frames: int,\n+    *,\n+    medfilt_width: int = 7,\n+    qk_scale: float = 1.0,\n+) -> List[WordTiming]:\n+    tokens = torch.tensor(\n+        [\n+            *tokenizer.sot_sequence,\n+            tokenizer.no_timestamps,\n+            *text_tokens,\n+            tokenizer.eot,\n+        ]\n+    ).to(model.device)\n+\n+    # install hooks on the cross attention layers to retrieve the attention weights\n+    QKs = [None] * model.dims.n_text_layer\n+    hooks = [\n+        block.cross_attn.register_forward_hook(\n+            lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1][0])\n+        )\n+        for i, block in enumerate(model.decoder.blocks)\n+    ]\n+\n+    with torch.no_grad():\n+        logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))[0]\n+        token_probs = logits[len(tokenizer.sot_sequence):, :tokenizer.eot].softmax(dim=-1)\n+        text_token_probs = token_probs[np.arange(len(text_tokens)), text_tokens].tolist()\n+\n+    for hook in hooks:\n+        hook.remove()\n+\n+    # heads * tokens * frames\n+    weights = torch.stack([QKs[l][h] for l, h in model.alignment_heads.indices().T])\n+    weights = weights[:, :, : num_frames // 2]\n+    weights = (weights * qk_scale).softmax(dim=-1)\n+    std, mean = torch.std_mean(weights, dim=-2, keepdim=True, unbiased=False)\n+    weights = (weights - mean) / std\n+    weights = median_filter(weights, medfilt_width)\n+\n+    matrix = weights.mean(axis=0)\n+    matrix = matrix[len(tokenizer.sot_sequence):-1]\n+    text_indices, time_indices = dtw(-matrix)\n+\n+    words, word_tokens = tokenizer.split_to_word_tokens(text_tokens + [tokenizer.eot])\n+    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n+\n+    jumps = np.pad(np.diff(text_indices), (1, 0), constant_values=1).astype(bool)\n+    jump_times = time_indices[jumps] / TOKENS_PER_SECOND\n+    start_times = jump_times[word_boundaries[:-1]]\n+    end_times = jump_times[word_boundaries[1:]]\n+    word_probabilities = [\n+        np.mean(text_token_probs[i:j]) for i, j in zip(word_boundaries[:-1], word_boundaries[1:])\n+    ]\n+\n+    # hack: ensure the first and second word is not longer than twice the median word duration.\n+    # a better segmentation algorithm based on VAD should be able to replace this.\n+    word_durations = end_times - start_times\n+    word_durations = word_durations[word_durations.nonzero()]\n+    if len(word_durations) > 0:\n+        median_duration = np.median(word_durations)\n+        max_duration = median_duration * 2\n+        if len(word_durations) >= 2 and word_durations[1] > max_duration:\n+            end_times[0] = start_times[1] = max(end_times[2] / 2, end_times[2] - max_duration)\n+        if len(word_durations) >= 1 and end_times[0] - start_times[0] > max_duration:\n+            start_times[0] = max(0, end_times[0] - max_duration)\n+\n+    return [\n+        WordTiming(word, tokens, start, end, probability)\n+        for word, tokens, start, end, probability in zip(\n+            words, word_tokens, start_times, end_times, word_probabilities\n+        )\n+    ]\n+\n+\n+def merge_punctuations(alignment: List[WordTiming], prepended: str, appended: str):\n+    # merge prepended punctuations\n+    i = len(alignment) - 2\n+    j = len(alignment) - 1\n+    while i >= 0:\n+        previous = alignment[i]\n+        following = alignment[j]\n+        if previous.word.startswith(\" \") and previous.word.strip() in prepended:\n+            # prepend it to the following word\n+            following.word = previous.word + following.word\n+            following.tokens = previous.tokens + following.tokens\n+            previous.word = \"\"\n+            previous.tokens = []\n+        else:\n+            j = i\n+        i -= 1\n+\n+    # merge appended punctuations\n+    i = 0\n+    j = 1\n+    while j < len(alignment):\n+        previous = alignment[i]\n+        following = alignment[j]\n+        if not previous.word.endswith(\" \") and following.word in appended:\n+            # append it to the previous word\n+            previous.word = previous.word + following.word\n+            previous.tokens = previous.tokens + following.tokens\n+            following.word = \"\"\n+            following.tokens = []\n+        else:\n+            i = j\n+        j += 1\n+\n+\n+def add_word_timestamps(\n+    *,\n+    segments: List[dict],\n+    model: \"Whisper\",\n+    tokenizer: Tokenizer,\n+    mel: torch.Tensor,\n+    num_frames: int,\n+    prepend_punctuations: str = \"\\\"\\'([{-\",\n+    append_punctuations: str = \"\\\"\\'.,!?:)]}\",\n+    **hyperparams,\n+):\n+    if len(segments) == 0:\n+        return\n+\n+    text_tokens = [t for segment in segments for t in segment[\"tokens\"]]\n+    alignment = find_alignment(model, tokenizer, text_tokens, mel, num_frames, **hyperparams)\n+    merge_punctuations(alignment, prepend_punctuations, append_punctuations)\n+\n+    time_offset = segments[0][\"seek\"] * HOP_LENGTH / SAMPLE_RATE\n+    token_sources = np.repeat(np.arange(len(segments)), [len(s[\"tokens\"]) for s in segments])\n+\n+    for segment in segments:\n+        segment[\"words\"] = []\n+\n+    word_boundaries = np.pad(np.cumsum([len(w.tokens) for w in alignment]), (1, 0))\n+    for i, timing in enumerate(alignment):\n+        if timing.word:\n+            segment = segments[token_sources[word_boundaries[i]]]\n+            start = round(time_offset + timing.start, 2)\n+            end = round(time_offset + timing.end, 2)\n+            segment[\"words\"].append(\n+                dict(word=timing.word, start=start, end=end, probability=timing.probability)\n+            )\n+\n+    for segment in segments:\n+        if len(words := segment[\"words\"]) > 0:\n+            # adjust the segment-level timestamps based on the word-level timestamps\n+            segment[\"start\"] = words[0][\"start\"]\n+            segment[\"end\"] = words[-1][\"end\"]\ndiff --git a/whisper/tokenizer.py b/whisper/tokenizer.py\nindex 7efa2d4af..ea1117f76 100644\n--- a/whisper/tokenizer.py\n+++ b/whisper/tokenizer.py\n@@ -1,4 +1,5 @@\n import os\n+import string\n from dataclasses import dataclass\n from functools import lru_cache, cached_property\n from typing import List, Optional, Tuple, Union\n@@ -265,6 +266,48 @@ def _get_single_token_id(self, text) -> int:\n         assert len(tokens) == 1, f\"{text} is not encoded as a single token\"\n         return tokens[0]\n \n+    def split_to_word_tokens(self, tokens: List[int]):\n+        if self.language in {\"zh\", \"ja\", \"th\", \"lo\", \"my\"}:\n+            # These languages don't typically use spaces, so it is difficult to split words\n+            # without morpheme analysis. Here, we instead split words at any\n+            # position where the tokens are decoded as valid unicode points\n+            return self.split_tokens_on_unicode(tokens)\n+\n+        return self.split_tokens_on_spaces(tokens)\n+\n+    def split_tokens_on_unicode(self, tokens: List[int]):\n+        words = []\n+        word_tokens = []\n+        current_tokens = []\n+\n+        for token in tokens:\n+            current_tokens.append(token)\n+            decoded = self.decode_with_timestamps(current_tokens)\n+            if \"\\ufffd\" not in decoded:\n+                words.append(decoded)\n+                word_tokens.append(current_tokens)\n+                current_tokens = []\n+\n+        return words, word_tokens\n+\n+    def split_tokens_on_spaces(self, tokens: List[int]):\n+        subwords, subword_tokens_list = self.split_tokens_on_unicode(tokens)\n+        words = []\n+        word_tokens = []\n+\n+        for subword, subword_tokens in zip(subwords, subword_tokens_list):\n+            special = subword_tokens[0] >= self.eot\n+            with_space = subword.startswith(\" \")\n+            punctuation = subword.strip() in string.punctuation\n+            if special or with_space or punctuation or len(words) == 0:\n+                words.append(subword)\n+                word_tokens.append(subword_tokens)\n+            else:\n+                words[-1] = words[-1] + subword\n+                word_tokens[-1].extend(subword_tokens)\n+\n+        return words, word_tokens\n+\n \n @lru_cache(maxsize=None)\n def build_tokenizer(name: str = \"gpt2\"):\ndiff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex d155b20df..d7c048749 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -7,8 +7,9 @@\n import torch\n import tqdm\n \n-from .audio import SAMPLE_RATE, N_FRAMES, HOP_LENGTH, pad_or_trim, log_mel_spectrogram\n+from .audio import HOP_LENGTH, N_FRAMES, SAMPLE_RATE, FRAMES_PER_SECOND, log_mel_spectrogram, pad_or_trim\n from .decoding import DecodingOptions, DecodingResult\n+from .timing import add_word_timestamps\n from .tokenizer import LANGUAGES, TO_LANGUAGE_CODE, get_tokenizer\n from .utils import exact_div, format_timestamp, make_safe, optional_int, optional_float, str2bool, get_writer\n \n@@ -27,6 +28,9 @@ def transcribe(\n     no_speech_threshold: Optional[float] = 0.6,\n     condition_on_previous_text: bool = True,\n     initial_prompt: Optional[str] = None,\n+    word_timestamps: bool = False,\n+    prepend_punctuations: str = \"\\\"\\'([{-\",\n+    append_punctuations: str = \"\\\"\\'.,!?:)]}\",\n     **decode_options,\n ):\n     \"\"\"\n@@ -63,6 +67,21 @@ def transcribe(\n         disabling may make the text inconsistent across windows, but the model becomes less prone to\n         getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n \n+    word_timestamps: bool\n+        Extract word-level timestamps using the cross-attention pattern and dynamic time warping,\n+        and include the timestamps for each word in each segment.\n+\n+    prepend_punctuations: str\n+        If word_timestamps is True, merge these punctuation symbols with the next word\n+\n+    append_punctuations: str\n+        If word_timestamps is True, merge these punctuation symbols with the previous word\n+\n+    initial_prompt: Optional[str]\n+        Optional text to provide as a prompt for the first window. This can be used to provide, or\n+        \"prompt-engineer\" a context for transcription, e.g. custom vocabularies or proper nouns\n+        to make it more likely to predict those word correctly.\n+\n     decode_options: dict\n         Keyword arguments to construct `DecodingOptions` instances\n \n@@ -90,16 +109,19 @@ def transcribe(\n         else:\n             if verbose:\n                 print(\"Detecting language using up to the first 30 seconds. Use `--language` to specify the language\")\n-            segment = pad_or_trim(mel, N_FRAMES).to(model.device).to(dtype)\n-            _, probs = model.detect_language(segment)\n+            mel_segment = pad_or_trim(mel, N_FRAMES).to(model.device).to(dtype)\n+            _, probs = model.detect_language(mel_segment)\n             decode_options[\"language\"] = max(probs, key=probs.get)\n             if verbose is not None:\n                 print(f\"Detected language: {LANGUAGES[decode_options['language']].title()}\")\n \n-    language = decode_options[\"language\"]\n-    task = decode_options.get(\"task\", \"transcribe\")\n+    language: str = decode_options[\"language\"]\n+    task: str = decode_options.get(\"task\", \"transcribe\")\n     tokenizer = get_tokenizer(model.is_multilingual, language=language, task=task)\n \n+    if word_timestamps and task == \"translate\":\n+        warnings.warn(\"Word-level timestamps on translations may not be reliable.\")\n+\n     def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n         temperatures = [temperature] if isinstance(temperature, (int, float)) else temperature\n         decode_result = None\n@@ -145,42 +167,35 @@ def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n     else:\n         initial_prompt_tokens = []\n \n-    def add_segment(\n-        *, start: float, end: float, text_tokens: torch.Tensor, result: DecodingResult\n+    def new_segment(\n+        *, start: float, end: float, tokens: torch.Tensor, result: DecodingResult\n     ):\n-        text = tokenizer.decode([token for token in text_tokens if token < tokenizer.eot])\n-        if len(text.strip()) == 0:  # skip empty text output\n-            return\n-\n-        all_segments.append(\n-            {\n-                \"id\": len(all_segments),\n-                \"seek\": seek,\n-                \"start\": start,\n-                \"end\": end,\n-                \"text\": text,\n-                \"tokens\": text_tokens.tolist(),\n-                \"temperature\": result.temperature,\n-                \"avg_logprob\": result.avg_logprob,\n-                \"compression_ratio\": result.compression_ratio,\n-                \"no_speech_prob\": result.no_speech_prob,\n-            }\n-        )\n-        if verbose:\n-            print(make_safe(f\"[{format_timestamp(start)} --> {format_timestamp(end)}] {text}\"))\n-\n-    # show the progress bar when verbose is False (otherwise the transcribed text will be printed)\n+        text_tokens = [token for token in tokens.tolist() if token < tokenizer.eot]\n+        return {\n+            \"id\": len(all_segments),\n+            \"seek\": seek,\n+            \"start\": start,\n+            \"end\": end,\n+            \"text\": tokenizer.decode(text_tokens),\n+            \"tokens\": text_tokens,\n+            \"temperature\": result.temperature,\n+            \"avg_logprob\": result.avg_logprob,\n+            \"compression_ratio\": result.compression_ratio,\n+            \"no_speech_prob\": result.no_speech_prob,\n+        }\n+\n+    # show the progress bar when verbose is False (if True, transcribed text will be printed)\n     num_frames = mel.shape[-1]\n-    previous_seek_value = seek\n-\n     with tqdm.tqdm(total=num_frames, unit='frames', disable=verbose is not False) as pbar:\n         while seek < num_frames:\n-            timestamp_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n-            segment = pad_or_trim(mel[:, seek:], N_FRAMES).to(model.device).to(dtype)\n-            segment_duration = segment.shape[-1] * HOP_LENGTH / SAMPLE_RATE\n+            time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n+            mel_segment = mel[:, seek:]\n+            segment_size = min(mel_segment.shape[-1], N_FRAMES)\n+            segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n+            mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n \n             decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n-            result: DecodingResult = decode_with_fallback(segment)\n+            result: DecodingResult = decode_with_fallback(mel_segment)\n             tokens = torch.tensor(result.tokens)\n \n             if no_speech_threshold is not None:\n@@ -191,29 +206,36 @@ def add_segment(\n                     should_skip = False\n \n                 if should_skip:\n-                    seek += segment.shape[-1]  # fast-forward to the next segment boundary\n+                    seek += segment_size  # fast-forward to the next segment boundary\n                     continue\n \n+            previous_seek = seek\n+            current_segments = []\n+            current_tokens = []\n+\n             timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n             consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0].add_(1)\n             if len(consecutive) > 0:  # if the output contains two consecutive timestamp tokens\n                 if ended_with_single_timestamp := timestamp_tokens[-2:].tolist() == [False, True]:\n                     consecutive = consecutive.tolist() + [len(tokens)]\n+\n                 last_slice = 0\n                 for current_slice in consecutive:\n                     sliced_tokens = tokens[last_slice:current_slice]\n                     start_timestamp_pos = sliced_tokens[0].item() - tokenizer.timestamp_begin\n                     end_timestamp_pos = sliced_tokens[-1].item() - tokenizer.timestamp_begin\n-                    add_segment(\n-                        start=timestamp_offset + start_timestamp_pos * time_precision,\n-                        end=timestamp_offset + end_timestamp_pos * time_precision,\n-                        text_tokens=sliced_tokens[1:-1],\n+                    current_segments.append(new_segment(\n+                        start=time_offset + start_timestamp_pos * time_precision,\n+                        end=time_offset + end_timestamp_pos * time_precision,\n+                        tokens=sliced_tokens,\n                         result=result,\n-                    )\n+                    ))\n+                    current_tokens.append(sliced_tokens.tolist())\n                     last_slice = current_slice\n+\n                 if ended_with_single_timestamp:\n                     # single timestamp at the end means no speech after the last timestamp.\n-                    seek += segment.shape[-1]\n+                    seek += segment_size\n                 else:\n                     # otherwise, ignore the unfinished segment and seek to the last timestamp\n                     last_timestamp_pos = tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n@@ -227,23 +249,54 @@ def add_segment(\n                     last_timestamp_pos = timestamps[-1].item() - tokenizer.timestamp_begin\n                     duration = last_timestamp_pos * time_precision\n \n-                add_segment(\n-                    start=timestamp_offset,\n-                    end=timestamp_offset + duration,\n-                    text_tokens=tokens,\n+                current_segments.append(new_segment(\n+                    start=time_offset,\n+                    end=time_offset + duration,\n+                    tokens=tokens,\n                     result=result,\n-                )\n-\n-                seek += segment.shape[-1]\n-                all_tokens.extend(tokens.tolist())\n+                ))\n+                current_tokens.append(tokens.tolist())\n+                seek += segment_size\n \n             if not condition_on_previous_text or result.temperature > 0.5:\n                 # do not feed the prompt tokens if a high temperature was used\n                 prompt_reset_since = len(all_tokens)\n \n+            if word_timestamps:\n+                add_word_timestamps(\n+                    segments=current_segments,\n+                    model=model,\n+                    tokenizer=tokenizer,\n+                    mel=mel_segment,\n+                    num_frames=segment_size,\n+                    prepend_punctuations=prepend_punctuations,\n+                    append_punctuations=append_punctuations,\n+                )\n+                word_end_timestamps = [w[\"end\"] for s in current_segments for w in s[\"words\"]]\n+                if len(consecutive) > 0 and len(word_end_timestamps) > 0:\n+                    seek_shift = round((word_end_timestamps[-1] - time_offset) * FRAMES_PER_SECOND)\n+                    if seek_shift > 0:\n+                        seek = previous_seek + seek_shift\n+\n+            if verbose:\n+                for segment in current_segments:\n+                    start, end, text = segment[\"start\"], segment[\"end\"], segment[\"text\"]\n+                    line = f\"[{format_timestamp(start)} --> {format_timestamp(end)}] {text}\"\n+                    print(make_safe(line))\n+\n+            # if a segment is instantaneous or does not contain text, clear it\n+            for i, segment in enumerate(current_segments):\n+                if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":\n+                    segment[\"text\"] = \"\"\n+                    segment[\"tokens\"] = []\n+                    segment[\"words\"] = []\n+                    current_tokens[i] = []\n+\n+            all_segments.extend(current_segments)\n+            all_tokens.extend([token for segment in current_tokens for token in segment])\n+\n             # update progress bar\n-            pbar.update(min(num_frames, seek) - previous_seek_value)\n-            previous_seek_value = seek\n+            pbar.update(min(num_frames, seek) - previous_seek)\n \n     return dict(\n         text=tokenizer.decode(all_tokens[len(initial_prompt_tokens):]),\n@@ -282,6 +335,9 @@ def cli():\n     parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=2.4, help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n     parser.add_argument(\"--logprob_threshold\", type=optional_float, default=-1.0, help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n     parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=0.6, help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n+    parser.add_argument(\"--word_timestamps\", type=str2bool, default=False, help=\"(experimental) extract word-level timestamps and refine the results based on them\")\n+    parser.add_argument(\"--prepend_punctuations\", type=str, default=\"\\\"\\'([{-\", help=\"if word_timestamps is True, merge these punctuation symbols with the next word\")\n+    parser.add_argument(\"--append_punctuations\", type=str, default=\"\\\"\\'.,!?:)]}\", help=\"if word_timestamps is True, merge these punctuation symbols with the previous word\")\n     parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n \n     args = parser.parse_args().__dict__\ndiff --git a/whisper/triton_ops.py b/whisper/triton_ops.py\nnew file mode 100644\nindex 000000000..d829e204a\n--- /dev/null\n+++ b/whisper/triton_ops.py\n@@ -0,0 +1,92 @@\n+import math\n+\n+import numpy as np\n+import torch\n+from functools import lru_cache\n+\n+try:\n+    import triton\n+    import triton.language as tl\n+except ImportError:\n+    raise RuntimeError(\"triton import failed; try `pip install --pre triton`\")\n+\n+\n+@triton.jit\n+def dtw_kernel(cost, trace, x, x_stride, cost_stride, trace_stride, N, M, BLOCK_SIZE: tl.constexpr):\n+    offsets = tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < M\n+\n+    for k in range(1, N + M + 1):  # k = i + j\n+        tl.debug_barrier()\n+\n+        p0 = cost + (k - 1) * cost_stride\n+        p1 = cost + k * cost_stride\n+        p2 = cost + k * cost_stride + 1\n+\n+        c0 = tl.load(p0 + offsets, mask=mask)\n+        c1 = tl.load(p1 + offsets, mask=mask)\n+        c2 = tl.load(p2 + offsets, mask=mask)\n+\n+        x_row = tl.load(x + (k - 1) * x_stride + offsets, mask=mask, other=0)\n+        cost_row = x_row + tl.minimum(tl.minimum(c0, c1), c2)\n+\n+        cost_ptr = cost + (k + 1) * cost_stride + 1\n+        tl.store(cost_ptr + offsets, cost_row, mask=mask)\n+\n+        trace_ptr = trace + (k + 1) * trace_stride + 1\n+        tl.store(trace_ptr + offsets, 2, mask=mask & (c2 <= c0) & (c2 <= c1))\n+        tl.store(trace_ptr + offsets, 1, mask=mask & (c1 <= c0) & (c1 <= c2))\n+        tl.store(trace_ptr + offsets, 0, mask=mask & (c0 <= c1) & (c0 <= c2))\n+\n+\n+@lru_cache(maxsize=None)\n+def median_kernel(filter_width: int):\n+    @triton.jit\n+    def kernel(y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr):  # x.shape[-1] == filter_width\n+        row_idx = tl.program_id(0)\n+        offsets = tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < y_stride\n+\n+        x_ptr = x + row_idx * x_stride\n+        y_ptr = y + row_idx * y_stride\n+\n+        LOAD_ALL_ROWS_HERE\n+\n+        BUBBLESORT_HERE\n+\n+        tl.store(y_ptr + offsets, MIDDLE_ROW_HERE, mask=mask)\n+\n+    kernel = triton.JITFunction(kernel.fn)\n+    kernel.src = kernel.src.replace(\"    LOAD_ALL_ROWS_HERE\", \"\\n\".join([\n+        f\"    row{i} = tl.load(x_ptr + offsets + {i}, mask=mask)\"\n+        for i in range(filter_width)\n+    ]))\n+    kernel.src = kernel.src.replace(\"    BUBBLESORT_HERE\", \"\\n\\n\".join([\n+        \"\\n\\n\".join([\n+            \"\\n\".join([\n+                f\"    smaller = tl.where(row{j} < row{j + 1}, row{j}, row{j + 1})\",\n+                f\"    larger = tl.where(row{j} > row{j + 1}, row{j}, row{j + 1})\",\n+                f\"    row{j} = smaller\",\n+                f\"    row{j + 1} = larger\",\n+            ])\n+            for j in range(filter_width - i - 1)\n+        ])\n+        for i in range(filter_width // 2 + 1)\n+    ]))\n+    kernel.src = kernel.src.replace(\"MIDDLE_ROW_HERE\", f\"row{filter_width // 2}\")\n+\n+    return kernel\n+\n+\n+def median_filter_cuda(x: torch.Tensor, filter_width: int):\n+    \"\"\"Apply a median filter of given width along the last dimension of x\"\"\"\n+    slices = x.contiguous().unfold(-1, filter_width, 1)\n+    grid = np.prod(slices.shape[:-2])\n+\n+    kernel = median_kernel(filter_width)\n+    y = torch.empty_like(slices[..., 0])\n+\n+    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()\n+    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)\n+\n+    return y\ndiff --git a/whisper/utils.py b/whisper/utils.py\nindex 5dacc173c..8ee912932 100644\n--- a/whisper/utils.py\n+++ b/whisper/utils.py\n@@ -85,34 +85,63 @@ def write_result(self, result: dict, file: TextIO):\n             print(segment['text'].strip(), file=file, flush=True)\n \n \n-class WriteVTT(ResultWriter):\n+class SubtitlesWriter(ResultWriter):\n+    always_include_hours: bool\n+    decimal_marker: str\n+\n+    def iterate_result(self, result: dict):\n+        for segment in result[\"segments\"]:\n+            segment_start = self.format_timestamp(segment[\"start\"])\n+            segment_end = self.format_timestamp(segment[\"end\"])\n+            segment_text = segment['text'].strip().replace('-->', '->')\n+\n+            if word_timings := segment.get(\"words\", None):\n+                all_words = [timing[\"word\"] for timing in word_timings]\n+                all_words[0] = all_words[0].strip()  # remove the leading space, if any\n+                last = segment_start\n+                for i, this_word in enumerate(word_timings):\n+                    start = self.format_timestamp(this_word[\"start\"])\n+                    end = self.format_timestamp(this_word[\"end\"])\n+                    if last != start:\n+                        yield last, start, segment_text\n+\n+                    yield start, end, \"\".join(\n+                        [f\"<u>{word}</u>\" if j == i else word for j, word in enumerate(all_words)]\n+                    )\n+                    last = end\n+\n+                if last != segment_end:\n+                    yield last, segment_end, segment_text\n+            else:\n+                yield segment_start, segment_end, segment_text\n+\n+    def format_timestamp(self, seconds: float):\n+        return format_timestamp(\n+            seconds=seconds,\n+            always_include_hours=self.always_include_hours,\n+            decimal_marker=self.decimal_marker,\n+        )\n+\n+\n+class WriteVTT(SubtitlesWriter):\n     extension: str = \"vtt\"\n+    always_include_hours: bool = False\n+    decimal_marker: str = '.'\n \n     def write_result(self, result: dict, file: TextIO):\n         print(\"WEBVTT\\n\", file=file)\n-        for segment in result[\"segments\"]:\n-            print(\n-                f\"{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}\\n\"\n-                f\"{segment['text'].strip().replace('-->', '->')}\\n\",\n-                file=file,\n-                flush=True,\n-            )\n+        for start, end, text in self.iterate_result(result):\n+            print(f\"{start} --> {end}\\n{text}\\n\", file=file, flush=True)\n \n \n-class WriteSRT(ResultWriter):\n+class WriteSRT(SubtitlesWriter):\n     extension: str = \"srt\"\n+    always_include_hours: bool = True\n+    decimal_marker: str = ','\n \n     def write_result(self, result: dict, file: TextIO):\n-        for i, segment in enumerate(result[\"segments\"], start=1):\n-            # write srt lines\n-            print(\n-                f\"{i}\\n\"\n-                f\"{format_timestamp(segment['start'], always_include_hours=True, decimal_marker=',')} --> \"\n-                f\"{format_timestamp(segment['end'], always_include_hours=True, decimal_marker=',')}\\n\"\n-                f\"{segment['text'].strip().replace('-->', '->')}\\n\",\n-                file=file,\n-                flush=True,\n-            )\n+        for i, (start, end, text) in enumerate(self.iterate_result(result), start=1):\n+            print(f\"{i}\\n{start} --> {end}\\n{text}\\n\", file=file, flush=True)\n \n \n class WriteTSV(ResultWriter):\n"
  },
  {
    "index": 13,
    "filtered_comments": [
      "Testing on another example from https://github.com/openai/whisper/discussions/679#discussioncomment-7649183\r\n\r\n<details>\r\n<summary>Output</summary>\r\n\r\n```\r\nv2 runs:\r\n\r\n[00:00.000 --> 00:05.660]  spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo\r\nDETECTED HALLUCINATION:  non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho\r\nDETECTED HALLUCINATION:  no\r\nDETECTED HALLUCINATION:  no\r\n\r\n\r\n\r\n[00:00.000 --> 00:05.660]  spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo\r\nDETECTED HALLUCINATION:  non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho\r\nDETECTED HALLUCINATION:  .....\r\nDETECTED HALLUCINATION:  .....\r\n\r\n\r\n\r\n[00:00.000 --> 00:05.660]  spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo\r\nDETECTED HALLUCINATION:  non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho\r\nDETECTED HALLUCINATION:  uh\r\nDETECTED HALLUCINATION:  uh\r\n\r\n\r\n\r\nv3 run:\r\n\r\n[00:00.000 --> 00:04.240]  Spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo.\r\nDETECTED HALLUCINATION:  Grazie a tutti.\r\nDETECTED HALLUCINATION:  E' un attimo che non dovevo.\r\n[00:54.440 --> 00:55.700]  Ehm, ehm.\r\n```",
      "I don't have the exact code anymore, but you could try temporarily inserting these two lines:\r\n\r\n```python\r\n                if score >= 3 or score + 0.01 >= len(words):\r\n                    print(f\"DETECTED HALLUCINATION: {segment['text']}\")\r\n```\r\n\r\nbefore the return in this function:\r\n\r\n```python\r\n            def is_segment_anomaly(segment: Optional[dict]) -> bool:\r\n                if segment is None or not segment[\"words\"]:\r\n                    return False\r\n                words = [w for w in segment[\"words\"] if w[\"word\"] not in punctuation]\r\n                words = words[:8]\r\n                score = sum(word_anomaly_score(w) for w in words)\r\n                return score >= 3 or score + 0.01 >= len(words)\r\n```",
      "@ryanheise\r\nSometimes `--hallucination_silence_threshold` makes whole non-hallucinating segments or part of segments disappear.\r\n\r\nBelow is example with disappeared `orange pigmentation.` segment.\r\n\r\nI'm using faster-whisper, but you should be able to reproduce it with whisper too as implementation is same.\r\nAudio file -> https://we.tl/t-U5a6Al5bRs\r\n\r\n\r\n`--language en --model=base --beam_size=5 --word_timestamps=True --hallucination_silence_threshold=None`:\r\n\r\n```\r\n[02:06.620 --> 02:11.120]  White tigers carry a mutated version of this gene, which prevents them from producing\r\n  Processing segment at 02:11.120\r\n[02:11.120 --> 02:12.460]  orange pigmentation.\r\n[02:15.360 --> 02:18.340]  Fewer than 4,000 tigers remain in the wild.\r\n```\r\n\r\n`--language en --model=base --beam_size=5 --word_timestamps=True --hallucination_silence_threshold=2`:\r\n\r\n```\r\n[02:06.620 --> 02:11.120]  White tigers carry a mutated version of this gene, which prevents them from producing\r\n  Processing segment at 02:12.380\r\n* HST_1: Skipping silence before possible hallucinations.\r\n* HST_3: DETECTED HALLUCINATION:  oxygen.\r\n  Processing segment at 02:13.380\r\n* HST_1: Skipping silence before possible hallucinations.\r\n[02:14.680 --> 02:18.360]  fewer than 4,000 tigers remain in the wild.\r\n```\r\n\r\nEDIT:\r\nfloat32 was in use",
      "I think, I've noticed a pattern, it happens when `if remaining_duration > threshold:` is not triggered, in there:\r\n`seek = previous_seek + segment_size`\r\n\r\nThen chunk go exactly by 30 secs cutting off the word.\r\n\r\nChunking when `--hallucination_silence_threshold=None`:\r\n\r\n```\r\n  Processing segment at 00:00.000\r\n  Processing segment at 00:26.040\r\n  Processing segment at 00:48.280\r\n  Processing segment at 01:14.400\r\n  Processing segment at 01:42.380\r\n  Processing segment at 02:11.120\r\n  Processing segment at 02:35.400\r\n  Processing segment at 03:05.400\r\n```\r\nChunking by setting high threshold `--hallucination_silence_threshold=40`:\r\n```\r\n  Processing segment at 00:00.000\r\n  Processing segment at 00:30.000\r\n  Processing segment at 01:00.000\r\n  Processing segment at 01:30.000\r\n  Processing segment at 02:00.000\r\n  Processing segment at 02:30.000\r\n  Processing segment at 03:00.000\r\n```",
      "Another thing, this PR affects transcription even if both new parameters are not enabled, I meant comparing vs without this PR.\r\n\r\nThis happens sometimes, but when it happens the discrepancy is always in the last chunk.\r\n\r\nAnd sometimes when discrepancy happens it tries to process additional micro chunk after it which produces some hallucination or fails because no-speech threshold is met, not sure if this is related to PR or to a discrepancy.\r\n\r\nExample of such discrepancy [audio is `05:05.877` long]:\r\n\r\nWithout this PR [perfect transcription]:\r\n```\r\nProcessing segment at 04:48.000\r\n[04:58.120 --> 05:05.260]  I just...\r\n[05:05.260 --> 05:05.760]  I...\r\n```\r\n\r\nWith this PR [all goes exactly same till the last chunk]:\r\n\r\n```\r\n  Processing segment at 04:48.000\r\n* Compression ratio threshold is not met with temperature 0.0 (3.523810 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.2 (3.523810 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.4 (8.038462 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.6 (3.523810 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.8 (2.423077 > 2.400000)\r\n[05:01.940 --> 05:02.900]  Okay.\r\n[05:02.900 --> 05:04.000]  I just-\r\n[05:04.940 --> 05:05.740]  I-\r\n[05:05.740 --> 05:05.840]  I-\r\n* Reset prompt. prompt_reset_on_temperature threshold is met 1.000000 > 0.500000\r\n  Processing segment at 05:05.840\r\n* Log probability threshold is not met with temperature 0.0 (-1.105777 < -1.000000)\r\n* No speech threshold is met (0.772002 > 0.600000)\r\n```",
      "> Sometimes `--hallucination_silence_threshold` makes whole non-hallucinating segments or part of segments disappear.\r\n\r\nThis logic is part of the original Whisper strategy of advancing by the full 30 seconds to the next window whenever the current segment is unfinished. So basically, if the segment finishes before the end of the 30 second window, then Whisper will crop the window to the exact end timestamp of the last word in that segment. But if the segment does not finish by the end of the 30 second window, the window is not cropped, the speech is assumed to run all the way to the end of the window.\r\n\r\nThis logic exists whether or not the `hallucination_silence_threshold` is enabled, and I have seen it cause problems in both cases, however the larger models tend to be better at picking up the words across the window boundary.\r\n\r\nIn your case, the sentence in question is:\r\n\r\n> White tigers carry a mutated version of this gene, which prevents them from producing orange pigmentation.\r\n\r\nThis sentence does not fit within the 30 second window, and the word \"orange\" is right on the boundary. In fact, the word \"orange\" is slightly before the boundary and the human ear can pick it up (as can the larger models) but the smaller models fail to pick it up.\r\n\r\nAnd given Whisper's logic in this case, it will assume the speech went right up to the end of the 30 second window and will resume the next window from there.\r\n\r\nSo although yes the large models would probably resolve this, I think it would still be better to change Whisper's strategy and crop the window to the end timestamp of the last word even in this case where we have an unfinished segment.",
      "> This logic is part of the original Whisper strategy of advancing by the full 30 seconds to the next window whenever the current segment is unfinished.\r\n\r\nI can't connect the dots...\r\nThen why it's \"unfinished\" when using `hallucination_silence_threshold` and it's \"finished\" without it?\r\n\r\nHow `remaining_duration <= hallucination_silence_threshold` means an \"unfinished\" segment? The option doesn't read as \"finished/unfinished segment threshold\"....\r\n",
      "Apologies, my explanation of that was around the wrong way. The original Whisper behaviour was that if the last segment in the window is \"complete\", THEN it skips to the end of the full 30 second window. If the last segment is incomplete, then it crops the window to end timestamp of the last word.\r\n\r\nBut when `hallucination_silence_threshold` is set, it still applies this logic in most cases except that it also includes a misfired heuristic that skips to the end of the full 30 second window if the end of the speech is close enough to the end of the window:\r\n\r\n```python\r\n                # skip silence before possible hallucinations\r\n                if hallucination_silence_threshold is not None:\r\n                    threshold = hallucination_silence_threshold\r\n                    if not single_timestamp_ending:\r\n                        last_word_end = get_end(current_segments)\r\n                        if last_word_end is not None and last_word_end > time_offset:\r\n                            remaining_duration = window_end_time - last_word_end\r\n                            if remaining_duration > threshold:  # <--- misfired heuristic\r\n                                seek = round(last_word_end * FRAMES_PER_SECOND)\r\n                            else:\r\n                                seek = previous_seek + segment_size\r\n````\r\n\r\nThe goal was to skip over as much silence as safely possible.\r\n\r\nHowever, in hindsight, this was a bit opportunistic, since after all `single_timestamp_ending` was `False` for good reason. You should find your example will work if you remove that heuristic. i.e. Delete this entire section:\r\n\r\n```python\r\n                    if not single_timestamp_ending:\r\n                        last_word_end = get_end(current_segments)\r\n                        if last_word_end is not None and last_word_end > time_offset:\r\n                            remaining_duration = window_end_time - last_word_end\r\n                            if remaining_duration > threshold:  # <--- misfired heuristic\r\n                                seek = round(last_word_end * FRAMES_PER_SECOND)\r\n                            else:\r\n                                seek = previous_seek + segment_size\r\n```\r\n\r\n(It's OK, the other parts of this code block are already handled elsewhere.)",
      "Thanks for explanation, now this part of code makes sense.\r\nDo you have idea why seek in the last window can be affected by PR? -> https://github.com/openai/whisper/pull/1838#issuecomment-1960581637\r\n\r\n> The goal was to skip over as much silence as safely possible.\r\n\r\nImho, skipping to full 30s window is pretty unsafe.  \r\nAnd it contradicted the description: \"skip silent periods longer than this threshold (in seconds) **when a possible hallucination is detected**\"",
      "> Do you have an audio file to reproduce?\r\n\r\nThis file has discrepancy in the last window/chunk:\r\nt-001.mka -> https://we.tl/t-ecd6U1QaZp\r\n`--language en --model=base --beam_size 1 --word_timestamps=True`\r\n\r\nWhisper without this PR:\r\n```\r\n[01:53.920 --> 01:54.500]  I'll give you some advice.\r\n[01:59.500 --> 02:00.080]  I'll give you some advice.\r\n[02:00.080 --> 02:00.080]  I'll give you some advice.\r\n[02:00.080 --> 02:00.980]  Say the word, General.\r\n[02:02.300 --> 02:03.320]  Let him go.\r\n```\r\nWhisper with this PR:\r\n```\r\n[01:53.920 --> 01:55.200]  I'll give you some advice.\r\n[01:59.500 --> 02:00.980]  Say the word, General.\r\n[02:02.280 --> 02:03.320]  Let him go.\r\n```\r\n",
      "> I'll test tomorrow, but does this also happen on PR #2043 ?\r\n\r\nYes, because `hallucination_silence_threshold` option is not relevant for the issue.\r\n\r\nCulprit affecting only the last window is found. it happens because of this:\r\n```python\r\n            mel_segment = mel[:, seek : seek + segment_size]\r\n```\r\n\r\nThis is the fix [that's how it was before this PR]:\r\n```python\r\n            mel_segment = mel[:, seek : seek + N_FRAMES]\r\n```\r\n\r\nNot sure why you changed it, on my observation it makes more hallucinations [probably it's random].\r\nAnyway, the fix brings back the previous behavior.",
      "I've confirmed the discrepancy, which seems to be a consequence of slightly different mel spectrograms. Although in the two examples you gave (only the latter of which I have tested with the supplied audio file), the PR actually removed a hallucination on one example and introduced a hallucination on the other example. So on balance, it's hard to say whether this discrepancy it better or worse or about the same.\r\n\r\nSo if it's not clear whether it's better or worse, do you see anything incorrect in the clipping logic? I think the difference is that I am always clipping exactly to the stretch of audio being examined, and then padding it. But originally, there was padding on the end that was added immediately when the mel spectrogram was first generated, and then (in the original code), it is also possible that due to the dynamic shifting of the window starts, it could end up padding the last part of the audio twice, because there is no guarantee that that initial padding Whisper added at the start of the process was enough to reflect where this last window ended up actually starting.\r\n\r\nBut it's possible I've done something wrong which I can't see, so let me know if you do spot something incorrect in the logic.",
      "After plotting the mel spectrograms, I noticed the padding when the audio is first loaded (as a whole) contains all -1.0's, while the padding in the main loop for each 30 second window contains all 0.0's. Not sure why that is, but there are two different padding algorithms in the code, and weirdly they are producing different padding results.\r\n\r\nSo in your example, the PR ends up always using the padding algorithm that pads to 0.0's whereas originally the end of file padding had -1.0's. ",
      "There's still a chance that a hallucination will be produced.\r\nFor me it was:\r\n```\r\n[02:15:58.100 --> 02:16:05.380]    .  ,  .\r\n[02:16:28.100 --> 02:16:30.100]    .\r\n```\r\ni. e.\r\n```\r\n....\r\n[02:16:28.100 --> 02:16:30.100] Subtitle Editor I. Boykova\r\n```\r\nNotably, this timestamp belongs to the end of the audio.\r\n\r\nModel size: small. Also there are some results in google, if you search for this phrase. One of them:\r\n```\r\n[24:26.800 --> 24:30.160]      .\r\n[24:30.160 --> 24:32.160]    .\r\n[24:32.160 --> 24:39.160]   .\r\n```\r\nfrom https://storage.googleapis.com/data.gdeltproject.org/blog/2022-tv-news-whisperasr/BELARUSTV_20221005_161500.small.transcribe.run1.txt",
      "That's certainly possible, and unfortunately there is no single choice of parameters that will be perfect in all scenarios. You can tweak the silence threshold, which is exposed on the command line. You can also try tweaking the other thresholds that were built into the code (like how long a word must be before it is flagged as an abnormality). If we can gather a large enough dataset of audio samples that produce hallucinations, we should be able to come up with better default settings that work well across a variety of scenarios and languages.",
      "@ryanheise \r\nI was using a bit tweaked segment anomaly heuristics to reduce false-positives, didn't noticed increase of false-negatives:\r\n\r\nchanged\r\n `if duration < 0.133:`\r\nto:\r\n`if duration < 0.133 and probability < 0.8:`\r\n\r\nchanged\r\n`return score >= 3 or score + 0.01 >= len(words)`\r\nto:\r\n`return score >= 3 or score + 0.001 >= len(words)`\r\n\r\nWhat you think about this tweak?",
      "Unfortunately I'm between computers right now (my old computer died 2 weeks ago, and I'm just in the process of installing everything on the recently arrived replacement...)\r\n\r\n>  return score >= 3 or score + 0.001 >= len(words)\r\n\r\nI don't see any problem with that change.\r\n\r\n>  `if duration < 0.133 and probability < 0.8:`\r\n\r\nDo you have an example audio for this one? I'd be interested to analyse this correlation between duration < 0.133 and probability < 0.8.\r\n\r\nThe alternative is to take into account more observations (like your audio example) and try to fit a new curve to the data. I initially fitted a simple linear curve, and maybe exponential could help because it could model a slower initial gradient."
    ],
    "code_diff": "diff --git a/whisper/timing.py b/whisper/timing.py\nindex befcf464e..b695ead0a 100644\n--- a/whisper/timing.py\n+++ b/whisper/timing.py\n@@ -299,6 +299,7 @@ def add_word_timestamps(\n     word_durations = np.array([t.end - t.start for t in alignment])\n     word_durations = word_durations[word_durations.nonzero()]\n     median_duration = np.median(word_durations) if len(word_durations) > 0 else 0.0\n+    median_duration = min(0.7, float(median_duration))\n     max_duration = median_duration * 2\n \n     # hack: truncate long words at sentence boundaries.\ndiff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex e80bede1d..1c075a201 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -2,7 +2,7 @@\n import os\n import traceback\n import warnings\n-from typing import TYPE_CHECKING, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -23,6 +23,7 @@\n from .utils import (\n     exact_div,\n     format_timestamp,\n+    get_end,\n     get_writer,\n     make_safe,\n     optional_float,\n@@ -48,6 +49,8 @@ def transcribe(\n     word_timestamps: bool = False,\n     prepend_punctuations: str = \"\\\"'([{-\",\n     append_punctuations: str = \"\\\"'.,!?:)]}\",\n+    clip_timestamps: Union[str, List[float]] = \"0\",\n+    hallucination_silence_threshold: Optional[float] = None,\n     **decode_options,\n ):\n     \"\"\"\n@@ -102,6 +105,14 @@ def transcribe(\n     decode_options: dict\n         Keyword arguments to construct `DecodingOptions` instances\n \n+    clip_timestamps: Union[str, List[float]]\n+        Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process.\n+        The last end timestamp defaults to the end of the file.\n+\n+    hallucination_silence_threshold: Optional[float]\n+        When word_timestamps is True, skip silent periods longer than this threshold (in seconds)\n+        when a possible hallucination is detected\n+\n     Returns\n     -------\n     A dictionary containing the resulting text (\"text\") and segment-level details (\"segments\"), and\n@@ -121,6 +132,7 @@ def transcribe(\n     # Pad 30-seconds of silence to the input audio, for slicing\n     mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n     content_frames = mel.shape[-1] - N_FRAMES\n+    content_duration = float(content_frames * HOP_LENGTH / SAMPLE_RATE)\n \n     if decode_options.get(\"language\", None) is None:\n         if not model.is_multilingual:\n@@ -147,6 +159,19 @@ def transcribe(\n         task=task,\n     )\n \n+    if isinstance(clip_timestamps, str):\n+        clip_timestamps = [\n+            float(ts) for ts in (clip_timestamps.split(\",\") if clip_timestamps else [])\n+        ]\n+    seek_points: List[int] = [round(ts * FRAMES_PER_SECOND) for ts in clip_timestamps]\n+    if len(seek_points) == 0:\n+        seek_points.append(0)\n+    if len(seek_points) % 2 == 1:\n+        seek_points.append(content_frames)\n+    seek_clips: List[Tuple[int, int]] = list(zip(seek_points[::2], seek_points[1::2]))\n+\n+    punctuation = \"\\\"'([{-\\\"'.,!?:)]}\"\n+\n     if word_timestamps and task == \"translate\":\n         warnings.warn(\"Word-level timestamps on translations may not be reliable.\")\n \n@@ -190,7 +215,8 @@ def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n \n         return decode_result\n \n-    seek = 0\n+    clip_idx = 0\n+    seek = seek_clips[clip_idx][0]\n     input_stride = exact_div(\n         N_FRAMES, model.dims.n_audio_ctx\n     )  # mel frames per output token: 2\n@@ -229,10 +255,23 @@ def new_segment(\n         total=content_frames, unit=\"frames\", disable=verbose is not False\n     ) as pbar:\n         last_speech_timestamp = 0.0\n-        while seek < content_frames:\n+        # NOTE: This loop is obscurely flattened to make the diff readable.\n+        # A later commit should turn this into a simpler nested loop.\n+        # for seek_clip_start, seek_clip_end in seek_clips:\n+        #     while seek < seek_clip_end\n+        while clip_idx < len(seek_clips):\n+            seek_clip_start, seek_clip_end = seek_clips[clip_idx]\n+            if seek < seek_clip_start:\n+                seek = seek_clip_start\n+            if seek >= seek_clip_end:\n+                clip_idx += 1\n+                if clip_idx < len(seek_clips):\n+                    seek = seek_clips[clip_idx][0]\n+                continue\n             time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n-            mel_segment = mel[:, seek : seek + N_FRAMES]\n-            segment_size = min(N_FRAMES, content_frames - seek)\n+            window_end_time = float((seek + N_FRAMES) * HOP_LENGTH / SAMPLE_RATE)\n+            segment_size = min(N_FRAMES, content_frames - seek, seek_clip_end - seek)\n+            mel_segment = mel[:, seek : seek + segment_size]\n             segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n             mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n \n@@ -257,6 +296,30 @@ def new_segment(\n             previous_seek = seek\n             current_segments = []\n \n+            # anomalous words are very long/short/improbable\n+            def word_anomaly_score(word: dict) -> float:\n+                probability = word.get(\"probability\", 0.0)\n+                duration = word[\"end\"] - word[\"start\"]\n+                score = 0.0\n+                if probability < 0.15:\n+                    score += 1.0\n+                if duration < 0.133:\n+                    score += (0.133 - duration) * 15\n+                if duration > 2.0:\n+                    score += duration - 2.0\n+                return score\n+\n+            def is_segment_anomaly(segment: Optional[dict]) -> bool:\n+                if segment is None or not segment[\"words\"]:\n+                    return False\n+                words = [w for w in segment[\"words\"] if w[\"word\"] not in punctuation]\n+                words = words[:8]\n+                score = sum(word_anomaly_score(w) for w in words)\n+                return score >= 3 or score + 0.01 >= len(words)\n+\n+            def next_words_segment(segments: List[dict]) -> Optional[dict]:\n+                return next((s for s in segments if s[\"words\"]), None)\n+\n             timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n             single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n \n@@ -330,17 +393,71 @@ def new_segment(\n                     append_punctuations=append_punctuations,\n                     last_speech_timestamp=last_speech_timestamp,\n                 )\n-                word_end_timestamps = [\n-                    w[\"end\"] for s in current_segments for w in s[\"words\"]\n-                ]\n-                if len(word_end_timestamps) > 0:\n-                    last_speech_timestamp = word_end_timestamps[-1]\n-                if not single_timestamp_ending and len(word_end_timestamps) > 0:\n-                    seek_shift = round(\n-                        (word_end_timestamps[-1] - time_offset) * FRAMES_PER_SECOND\n-                    )\n-                    if seek_shift > 0:\n-                        seek = previous_seek + seek_shift\n+\n+                if not single_timestamp_ending:\n+                    last_word_end = get_end(current_segments)\n+                    if last_word_end is not None and last_word_end > time_offset:\n+                        seek = round(last_word_end * FRAMES_PER_SECOND)\n+\n+                # skip silence before possible hallucinations\n+                if hallucination_silence_threshold is not None:\n+                    threshold = hallucination_silence_threshold\n+                    if not single_timestamp_ending:\n+                        last_word_end = get_end(current_segments)\n+                        if last_word_end is not None and last_word_end > time_offset:\n+                            remaining_duration = window_end_time - last_word_end\n+                            if remaining_duration > threshold:\n+                                seek = round(last_word_end * FRAMES_PER_SECOND)\n+                            else:\n+                                seek = previous_seek + segment_size\n+\n+                    # if first segment might be a hallucination, skip leading silence\n+                    first_segment = next_words_segment(current_segments)\n+                    if first_segment is not None and is_segment_anomaly(first_segment):\n+                        gap = first_segment[\"start\"] - time_offset\n+                        if gap > threshold:\n+                            seek = previous_seek + round(gap * FRAMES_PER_SECOND)\n+                            continue\n+\n+                    # skip silence before any possible hallucination that is surrounded\n+                    # by silence or more hallucinations\n+                    hal_last_end = last_speech_timestamp\n+                    for si in range(len(current_segments)):\n+                        segment = current_segments[si]\n+                        if not segment[\"words\"]:\n+                            continue\n+                        if is_segment_anomaly(segment):\n+                            next_segment = next_words_segment(\n+                                current_segments[si + 1 :]\n+                            )\n+                            if next_segment is not None:\n+                                hal_next_start = next_segment[\"words\"][0][\"start\"]\n+                            else:\n+                                hal_next_start = time_offset + segment_duration\n+                            silence_before = (\n+                                segment[\"start\"] - hal_last_end > threshold\n+                                or segment[\"start\"] < threshold\n+                                or segment[\"start\"] - time_offset < 2.0\n+                            )\n+                            silence_after = (\n+                                hal_next_start - segment[\"end\"] > threshold\n+                                or is_segment_anomaly(next_segment)\n+                                or window_end_time - segment[\"end\"] < 2.0\n+                            )\n+                            if silence_before and silence_after:\n+                                seek = round(\n+                                    max(time_offset + 1, segment[\"start\"])\n+                                    * FRAMES_PER_SECOND\n+                                )\n+                                if content_duration - segment[\"end\"] < threshold:\n+                                    seek = content_frames\n+                                current_segments[si:] = []\n+                                break\n+                        hal_last_end = segment[\"end\"]\n+\n+                last_word_end = get_end(current_segments)\n+                if last_word_end is not None:\n+                    last_speech_timestamp = last_word_end\n \n             if verbose:\n                 for segment in current_segments:\n@@ -427,6 +544,8 @@ def valid_model_name(name):\n     parser.add_argument(\"--max_line_count\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of lines in a segment\")\n     parser.add_argument(\"--max_words_per_line\", type=optional_int, default=None, help=\"(requires --word_timestamps True, no effect with --max_line_width) the maximum number of words in a segment\")\n     parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n+    parser.add_argument(\"--clip_timestamps\", type=str, default=\"0\", help=\"comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process, where the last end timestamp defaults to the end of the file\")\n+    parser.add_argument(\"--hallucination_silence_threshold\", type=optional_float, help=\"(requires --word_timestamps True) skip silent periods longer than this threshold (in seconds) when a possible hallucination is detected\")\n     # fmt: on\n \n     args = parser.parse_args().__dict__\ndiff --git a/whisper/utils.py b/whisper/utils.py\nindex 7a172c401..9b9b13862 100644\n--- a/whisper/utils.py\n+++ b/whisper/utils.py\n@@ -3,7 +3,7 @@\n import re\n import sys\n import zlib\n-from typing import Callable, Optional, TextIO\n+from typing import Callable, List, Optional, TextIO\n \n system_encoding = sys.getdefaultencoding()\n \n@@ -68,6 +68,20 @@ def format_timestamp(\n     )\n \n \n+def get_start(segments: List[dict]) -> Optional[float]:\n+    return next(\n+        (w[\"start\"] for s in segments for w in s[\"words\"]),\n+        segments[0][\"start\"] if segments else None,\n+    )\n+\n+\n+def get_end(segments: List[dict]) -> Optional[float]:\n+    return next(\n+        (w[\"end\"] for s in reversed(segments) for w in reversed(s[\"words\"])),\n+        segments[-1][\"end\"] if segments else None,\n+    )\n+\n+\n class ResultWriter:\n     extension: str\n \n@@ -129,8 +143,8 @@ def iterate_subtitles():\n             line_len = 0\n             line_count = 1\n             # the next subtitle to yield (a list of word timings with whitespace)\n-            subtitle: list[dict] = []\n-            last = result[\"segments\"][0][\"words\"][0][\"start\"]\n+            subtitle: List[dict] = []\n+            last: float = get_start(result[\"segments\"]) or 0.0\n             for segment in result[\"segments\"]:\n                 chunk_index = 0\n                 words_count = max_words_per_line\n"
  },
  {
    "index": 14,
    "filtered_comments": [
      "The problem triggered by the test data from @ryanheise is model sensitive. I see the problem with `small` but using either `small.en` or `medium.en` looks ok although the timing of the last few words is off. Below is the mp3 fragment converted to video to show the English subtitles.\r\n\r\nhttps://user-images.githubusercontent.com/3035114/223597998-74a8ec7f-da0b-4948-9f6a-75712820eb15.mp4\r\n\r\n\r\n",
      "Thanks all! The incorrect zero-padding of Mel spectrograms as identified in #730 and #838 was contributing to this error. The fix in 477f0be appears to fix the repetition issue.",
      "Btw have you guys tried with longer audio, e.g. 5 mins long? I am still getting a lot of repetition even with this fix.\r\nE.g. on the TEDLIUM test set \"AimeeMullins_2009P.wav\"\r\n>[02:10.440 --> 02:14.720]  and needless to say, thank God, I wasn't using a thesaurus back then.\r\n[02:14.720 --> 02:14.720]  and needless to say, thank God, I wasn't using a thesaurus back then.\r\n[02:15.460 --> 02:18.580]  I mean from this entry, it would seem that\r\n[02:18.580 --> 02:22.800]  I was born into a world that perceived someone like me\r\n[02:22.800 --> 02:23.340]  I was born into a world that perceived someone like me\r\n[02:23.340 --> 02:27.540]  to have nothing positive, whatsoever, going for them\r\n[02:27.540 --> 02:27.540]  to have nothing positive, whatsoever, going for them\r\n[02:27.540 --> 02:35.340]  When in fact today, I'm celebrated for the opportunities and adventures my life has procured\r\n[02:35.340 --> 02:35.960]  When in fact today, I'm celebrated for the opportunities and adventures my life has procured\r\n[02:35.960 --> 02:42.140]  So I immediately went to look up the 2009 online edition\r\n[02:42.140 --> 02:42.160]  So I immediately went to look up the 2009 online edition\r\n[02:42.160 --> 02:42.160]  So I immediately went to look up the 2009 online edition\r\n\r\nI was hoping to update word segmentation results for whisper-only word timestamps in our paper https://arxiv.org/abs/2303.00747\r\n\r\nBut currently i am getting better results with our implementation which is similar to https://github.com/linto-ai/whisper-timestamped\r\n",
      "@jongwook Note from @m-bain example above the repetition occurring with verbose print. The repetitions in this example are all \"instantaneous\" ; eg same start and end time\r\n> [02:14.720 --> 02:14.720] and needless to say, thank God, I wasn't using a thesaurus back then.\r\n\r\nthey are printed but then immediately cleared by this code, which looks like a bug unique to `--verbose True`\r\n\r\nhttps://github.com/openai/whisper/blob/aac47c98349b98cec5ca7b1be53960fb59f4436b/whisper/transcribe.py#L345",
      "This is not a verbose error, and the start times and end times of repetition are not always instantaneous, see output for the .srt file without verbose:\r\n\r\n271\r\n00:02:14,440 --> 00:02:14,720\r\nand needless to say, thank God, I wasn't using a thesaurus back<u> then.</u>\r\n\r\n272\r\n00:02:14,720 --> 00:02:14,720\r\n\r\n\r\n273\r\n00:02:15,460 --> 00:02:16,180\r\n<u>I</u> mean from this entry, it would seem that\r\n\r\n274\r\n00:02:16,180 --> 00:02:16,360\r\nI<u> mean</u> from this entry, it would seem that\r\n\r\n275\r\n00:02:16,360 --> 00:02:16,960\r\nI mean<u> from</u> this entry, it would seem that\r\n\r\n276\r\n00:02:16,960 --> 00:02:17,220\r\nI mean from<u> this</u> entry, it would seem that\r\n\r\n277\r\n00:02:17,220 --> 00:02:17,620\r\nI mean from this<u> entry,</u> it would seem that\r\n\r\n278\r\n00:02:17,620 --> 00:02:17,800\r\nI mean from this entry, it would seem that\r\n>",
      "So there are at least two problems then\r\n* verbose mode can print cleared segments\r\n* something else triggered by word_timestamps\r\n\r\nGiven how close the start/end times are it feels like something related to `seek_shift` is still off\r\nhttps://github.com/openai/whisper/blob/aac47c98349b98cec5ca7b1be53960fb59f4436b/whisper/transcribe.py#L337\r\n\r\n@m-bain Do the same repetitions happen with `word_timestamps False` or no? ",
      "Update, I realise there is some specific underline formatting in the word_timestamps, was able to get it working in the end. See here for comparison on word-level timestamp accuracy\r\n\r\n![image](https://user-images.githubusercontent.com/36994049/224011580-4782f2ad-a178-4b2d-80c3-4baa8ca54ab9.png)\r\n\r\n@jongwook could you share the evaluation for long-form transcription WER? I am unable to reproduce whisper results, right now I report in the vanilla setting -- greedy/beam5 decoding without the heuristic tricks\r\n"
    ],
    "code_diff": "diff --git a/whisper/audio.py b/whisper/audio.py\nindex a19b7ab0d..513ab7c9d 100644\n--- a/whisper/audio.py\n+++ b/whisper/audio.py\n@@ -1,6 +1,6 @@\n import os\n from functools import lru_cache\n-from typing import Union\n+from typing import Optional, Union\n \n import ffmpeg\n import numpy as np\n@@ -15,10 +15,8 @@\n N_MELS = 80\n HOP_LENGTH = 160\n CHUNK_LENGTH = 30\n-N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000: number of samples in a chunk\n-N_FRAMES = exact_div(\n-    N_SAMPLES, HOP_LENGTH\n-)  # 3000: number of frames in a mel spectrogram input\n+N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n+N_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n \n N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n FRAMES_PER_SECOND = exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n@@ -100,7 +98,10 @@ def mel_filters(device, n_mels: int = N_MELS) -> torch.Tensor:\n \n \n def log_mel_spectrogram(\n-    audio: Union[str, np.ndarray, torch.Tensor], n_mels: int = N_MELS\n+    audio: Union[str, np.ndarray, torch.Tensor],\n+    n_mels: int = N_MELS,\n+    padding: int = 0,\n+    device: Optional[Union[str, torch.device]] = None,\n ):\n     \"\"\"\n     Compute the log-Mel spectrogram of\n@@ -113,6 +114,12 @@ def log_mel_spectrogram(\n     n_mels: int\n         The number of Mel-frequency filters, only 80 is supported\n \n+    padding: int\n+        Number of zero samples to pad to the right\n+\n+    device: Optional[Union[str, torch.device]]\n+        If given, the audio tensor is moved to this device before STFT\n+\n     Returns\n     -------\n     torch.Tensor, shape = (80, n_frames)\n@@ -123,6 +130,10 @@ def log_mel_spectrogram(\n             audio = load_audio(audio)\n         audio = torch.from_numpy(audio)\n \n+    if device is not None:\n+        audio = audio.to(device)\n+    if padding > 0:\n+        audio = F.pad(audio, (0, padding))\n     window = torch.hann_window(N_FFT).to(audio.device)\n     stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)\n     magnitudes = stft[..., :-1].abs() ** 2\ndiff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex 20f01477e..773e6365e 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -11,6 +11,7 @@\n     FRAMES_PER_SECOND,\n     HOP_LENGTH,\n     N_FRAMES,\n+    N_SAMPLES,\n     SAMPLE_RATE,\n     log_mel_spectrogram,\n     pad_or_trim,\n@@ -116,7 +117,9 @@ def transcribe(\n     if dtype == torch.float32:\n         decode_options[\"fp16\"] = False\n \n-    mel = log_mel_spectrogram(audio)\n+    # Pad 30-seconds of silence to the input audio, for slicing\n+    mel = log_mel_spectrogram(audio, padding=N_SAMPLES)\n+    content_frames = mel.shape[-1] - N_FRAMES\n \n     if decode_options.get(\"language\", None) is None:\n         if not model.is_multilingual:\n@@ -212,14 +215,13 @@ def new_segment(\n         }\n \n     # show the progress bar when verbose is False (if True, transcribed text will be printed)\n-    num_frames = mel.shape[-1]\n     with tqdm.tqdm(\n-        total=num_frames, unit=\"frames\", disable=verbose is not False\n+        total=content_frames, unit=\"frames\", disable=verbose is not False\n     ) as pbar:\n-        while seek < num_frames:\n+        while seek < content_frames:\n             time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n-            mel_segment = mel[:, seek:]\n-            segment_size = min(mel_segment.shape[-1], N_FRAMES)\n+            mel_segment = mel[:, seek : seek + N_FRAMES]\n+            segment_size = min(N_FRAMES, content_frames - seek)\n             segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n             mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n \n@@ -246,20 +248,18 @@ def new_segment(\n             current_tokens = []\n \n             timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n-            consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[\n-                0\n-            ].add_(1)\n-            if (\n-                len(consecutive) > 0\n-            ):  # if the output contains two consecutive timestamp tokens\n-                if ended_with_single_timestamp := timestamp_tokens[-2:].tolist() == [\n-                    False,\n-                    True,\n-                ]:\n-                    consecutive = consecutive.tolist() + [len(tokens)]\n+            single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n+\n+            consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n+            consecutive.add_(1)\n+            if len(consecutive) > 0:\n+                # if the output contains two consecutive timestamp tokens\n+                slices = consecutive.tolist()\n+                if single_timestamp_ending:\n+                    slices.append(len(tokens))\n \n                 last_slice = 0\n-                for current_slice in consecutive:\n+                for current_slice in slices:\n                     sliced_tokens = tokens[last_slice:current_slice]\n                     start_timestamp_pos = (\n                         sliced_tokens[0].item() - tokenizer.timestamp_begin\n@@ -278,7 +278,7 @@ def new_segment(\n                     current_tokens.append(sliced_tokens.tolist())\n                     last_slice = current_slice\n \n-                if ended_with_single_timestamp:\n+                if single_timestamp_ending:\n                     # single timestamp at the end means no speech after the last timestamp.\n                     seek += segment_size\n                 else:\n@@ -329,7 +329,7 @@ def new_segment(\n                 word_end_timestamps = [\n                     w[\"end\"] for s in current_segments for w in s[\"words\"]\n                 ]\n-                if len(consecutive) > 0 and len(word_end_timestamps) > 0:\n+                if not single_timestamp_ending and len(word_end_timestamps) > 0:\n                     seek_shift = round(\n                         (word_end_timestamps[-1] - time_offset) * FRAMES_PER_SECOND\n                     )\n@@ -356,7 +356,7 @@ def new_segment(\n             )\n \n             # update progress bar\n-            pbar.update(min(num_frames, seek) - previous_seek)\n+            pbar.update(min(content_frames, seek) - previous_seek)\n \n     return dict(\n         text=tokenizer.decode(all_tokens[len(initial_prompt_tokens) :]),\n"
  },
  {
    "index": 15,
    "filtered_comments": [
      "This can be really useful for proofing the output via something like Subtitle Edit.  \r\n\r\nWould really need an command line option to output an additional subtitle though, right?  \r\n\r\nI get the impression @jongwook doesn't want to stuff too many features in though, so how does such a useful feature get added without having a fork?",
      "Hello!\r\n\r\n> Although the colour terminal stuff might be questionable\r\n\r\nI implemented the per-token confidence as is and implemented the colorful CLI output only in an example.\r\nThe main whisper code does not contain anything with color\r\n\r\n@jongwook is there anything I should modify or change for you to accept the PR? ",
      "I'm hesitant to add this because the incremental utility of this compared to the probabilities returned by `word_timestamps=True` is quite niche, versus the added complexity & latency due to the additional GPU operations. The decoding logic is already taking as much as the forward pass, and I'm hoping to reduce this overhead. The subword token probabilities are not very useful anyway, because it's usually influenced more by language modeling than from speech recognition.\r\n\r\nFor the case you need per-token probs, you can add another forward pass without modifying decoding.py (similar to how it's done in [timing.py](https://github.com/openai/whisper/blob/76c901ab8d4558992c44138479c4d69eb52fadcb/whisper/timing.py#L197)) without incurring too much additional latency. It may even be faster than adding GPU operations for every autoregressive step.\r\n\r\nThe example script looks nifty, but i'd prefer it in the [show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) section.",
      "@SinanAkkoyun thanks for your contribution. Not sure, but seems it works incorrect, \r\nI made distorted speech example https://drive.google.com/file/d/12zGWllJg6edftcnwuHX_ZHMuwk7PlVjg/view?usp=sharing .\r\nIf I don't set the language of decoding i.e. `options = whisper.DecodingOptions() `,  the output is correct in terms of locating mispronounce (I can read this slavic) though it translates it to random language.\r\n![Screenshot from 2023-08-09 10-58-49](https://github.com/openai/whisper/assets/54935496/cf45eaab-1799-45d0-a386-2fc2e3076b1a)\r\n\r\nBut if I set 'en' for decoding  `options = whisper.DecodingOptions(language=\"en\")` the picture is wrong.\r\n![Screenshot from 2023-08-09 10-58-53](https://github.com/openai/whisper/assets/54935496/8c009a46-ec4d-4a47-abe3-786408580857)\r\n The rest of the code is the same as in your PR except I used \"small\" model.\r\n",
      "@Rtut654 Hi, I don't quite understand the issue you are having, the \"I like to play badminton and football.\" seems to be correct, the football especially sounds vague in the audio you provided. Could you please tell me more about your issue?\r\n\r\nDespite that, the PR is not going to get merged, so I stopped working on it and use that modification in my own work which does not include translation\r\n\r\nIf the random translation is the problem you are referring to, I believe that my PR did not modify nor change the output prediction by any means, it just grabbed the logits and displays them as confidence",
      "@SinanAkkoyun \r\nThe issue is in the accuracy of token_probs. The first version (with translation to Ukrainian) gives very accurate result since \"like\" was also mispronounced very much. Also the word \"football \" was mispronounced in the last part which is correctly shown in the first picture. \r\n\r\nI did the same test with other audio, setting language of decoding to English.  The picture was same. Somehow it is lowering the prob of the last word even when it is pronounced correctly. At the same time probs of mispronounciations were high which is strange. So something is wrong in the way it predicts probs when language is set to English.",
      "In case it's of interest, I created a small web component to view the Whisper JSON file when `--word_timestamps` has been used. Ideas for improving it would be welcome!\r\n\r\nhttps://edsu.github.io/whisper-transcript/"
    ],
    "code_diff": "diff --git a/.gitignore b/.gitignore\nindex 7ae8fabc6..9f5ed862b 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -9,3 +9,6 @@ thumbs.db\n .DS_Store\n .idea\n \n+.venv/\n+\n+samples/\ndiff --git a/README.md b/README.md\nindex eba82ce22..9ea3a38e5 100644\n--- a/README.md\n+++ b/README.md\n@@ -144,4 +144,4 @@ Please use the [ Show and tell](https://github.com/openai/whisper/discussion\n \n ## License\n \n-Whisper's code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.\n+Whisper's code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.\n\\ No newline at end of file\ndiff --git a/examples/confidence_per_token.py b/examples/confidence_per_token.py\nnew file mode 100644\nindex 000000000..0bf2a30d7\n--- /dev/null\n+++ b/examples/confidence_per_token.py\n@@ -0,0 +1,59 @@\n+# IMPORTANT: This is just for using the local whisper dir as the package directly. Delete until next comment when just installing whisper normally.\n+import sys\n+from pathlib import Path\n+sys.path.insert(0, str(Path(__file__).resolve().parents[1]))\n+# end of dev import\n+import whisper\n+\n+import colorsys\n+from typing import List\n+from whisper.tokenizer import get_tokenizer\n+from colorama import init, Style\n+\n+\n+print('Loading model')\n+model = whisper.load_model(\"large\")\n+\n+\n+print('Loading audio') # load audio and pad/trim it to fit 30 seconds\n+audio = whisper.load_audio(\"samples/your_audio.wav\")\n+audio = whisper.pad_or_trim(audio)\n+\n+\n+mel = whisper.log_mel_spectrogram(audio).to(model.device) # make log-Mel spectrogram and move to the same device as the model\n+\n+\n+detect_lang = False\n+language = \"en\"\n+if detect_lang: # detect the spoken language\n+    print('Detecting language')\n+    _, probs = model.detect_language(mel)\n+    print(f\"Detected language: {max(probs, key=probs.get)}\")\n+    language=max(probs, key=probs.get)\n+\n+\n+print('Decoding audio') # decode the audio\n+options = whisper.DecodingOptions()\n+result = whisper.decode(model, mel, options)\n+\n+\n+def get_colored_text(tokens: List[int], token_probs: List[float], tokenizer, prompt: str=\"\"):\n+    init(autoreset=False)  # Initialize colorama\n+    text_tokens = [tokenizer.decode([t]) for t in tokens]\n+\n+    output_text = \"\"\n+    for i, (token, prob) in enumerate(zip(text_tokens, token_probs)):\n+        # Interpolate between red and green in the HSV color space\n+        r, g, b = colorsys.hsv_to_rgb(prob * (1/3), 1, 1)\n+        r, g, b = int(r * 255), int(g * 255), int(b * 255)\n+        color_code = f\"\\033[38;2;{r};{g};{b}m\"\n+\n+        colored_token = f\"{color_code}{Style.BRIGHT}{token}{Style.RESET_ALL}\"\n+        output_text += colored_token\n+\n+    return output_text\n+\n+\n+tokenizer = get_tokenizer(multilingual=model.is_multilingual, language=language, task=options.task)\n+print(get_colored_text(result.tokens, result.token_probs, tokenizer))  # print text with fancy confidence colors\n+# HINT: when using a prompt, you must provide it in the get_colored_text as well\ndiff --git a/whisper/decoding.py b/whisper/decoding.py\nindex 81cd8452b..4ded71bd7 100644\n--- a/whisper/decoding.py\n+++ b/whisper/decoding.py\n@@ -118,6 +118,7 @@ class DecodingResult:\n     language: str\n     language_probs: Optional[Dict[str, float]] = None\n     tokens: List[int] = field(default_factory=list)\n+    token_probs: List[float] = field(default_factory=list)\n     text: str = \"\"\n     avg_logprob: float = np.nan\n     no_speech_prob: float = np.nan\n@@ -211,7 +212,7 @@ def reset(self):\n         \"\"\"Initialize any stateful variables for decoding a new sequence\"\"\"\n \n     def update(\n-        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n+        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor, token_probs: Tensor\n     ) -> Tuple[Tensor, bool]:\n         \"\"\"Specify how to select the next token, based on the current trace and logits\n \n@@ -238,7 +239,7 @@ def update(\n         raise NotImplementedError\n \n     def finalize(\n-        self, tokens: Tensor, sum_logprobs: Tensor\n+        self, tokens: Tensor, sum_logprobs: Tensor, token_probs: Tensor\n     ) -> Tuple[Sequence[Sequence[Tensor]], List[List[float]]]:\n         \"\"\"Finalize search and return the final candidate sequences\n \n@@ -268,7 +269,7 @@ def __init__(self, temperature: float, eot: int):\n         self.eot = eot\n \n     def update(\n-        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n+        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor, token_probs: Tensor\n     ) -> Tuple[Tensor, bool]:\n         if self.temperature == 0:\n             next_tokens = logits.argmax(dim=-1)\n@@ -276,19 +277,28 @@ def update(\n             next_tokens = Categorical(logits=logits / self.temperature).sample()\n \n         logprobs = F.log_softmax(logits.float(), dim=-1)\n+        probs = torch.exp(logprobs)\n         current_logprobs = logprobs[torch.arange(logprobs.shape[0]), next_tokens]\n         sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)\n \n         next_tokens[tokens[:, -1] == self.eot] = self.eot\n         tokens = torch.cat([tokens, next_tokens[:, None]], dim=-1)\n \n+        current_token_probs = probs[torch.arange(probs.shape[0]), next_tokens]\n+        token_probs = torch.cat([token_probs, current_token_probs[:, None]], dim=-1)\n+\n+        # token_logits = torch.stack([logits[k, next_tokens[k]] for k in range(next_tokens .shape[0])], dim=0)\n+        # or use logprobs, the log softmax of the logits\n+        # return it along with tokens and completed\n+\n         completed = (tokens[:, -1] == self.eot).all()\n-        return tokens, completed\n+        return tokens, completed, token_probs\n \n-    def finalize(self, tokens: Tensor, sum_logprobs: Tensor):\n+    def finalize(self, tokens: Tensor, sum_logprobs: Tensor, token_probs: Tensor):\n         # make sure each sequence has at least one EOT token at the end\n         tokens = F.pad(tokens, (0, 1), value=self.eot)\n-        return tokens, sum_logprobs.tolist()\n+        token_probs = F.pad(token_probs, (0, 1), value=0) # 0 ok?\n+        return tokens, sum_logprobs.tolist(), token_probs.tolist()\n \n \n class BeamSearchDecoder(TokenDecoder):\n@@ -374,7 +384,7 @@ def update(\n         )\n         return tokens, completed\n \n-    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor):\n+    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor, token_probs: Tensor):\n         # collect all finished sequences, including patience, and add unfinished ones if not enough\n         sum_logprobs = sum_logprobs.cpu()\n         for i, sequences in enumerate(self.finished_sequences):\n@@ -668,6 +678,8 @@ def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n         sum_logprobs: Tensor = torch.zeros(n_batch, device=audio_features.device)\n         no_speech_probs = [np.nan] * n_batch\n \n+        token_probs = torch.zeros_like(tokens).float()\n+\n         try:\n             for i in range(self.sample_len):\n                 logits = self.inference.logits(tokens, audio_features)\n@@ -686,14 +698,14 @@ def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n                     logit_filter.apply(logits, tokens)\n \n                 # expand the tokens tensor with the selected next tokens\n-                tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\n+                tokens, completed, token_probs = self.decoder.update(tokens, logits, sum_logprobs, token_probs)\n \n                 if completed or tokens.shape[-1] > self.n_ctx:\n                     break\n         finally:\n             self.inference.cleanup_caching()\n \n-        return tokens, sum_logprobs, no_speech_probs\n+        return tokens, sum_logprobs, no_speech_probs, token_probs\n \n     @torch.no_grad()\n     def run(self, mel: Tensor) -> List[DecodingResult]:\n@@ -721,7 +733,7 @@ def run(self, mel: Tensor) -> List[DecodingResult]:\n         tokens = tokens.repeat_interleave(self.n_group, dim=0).to(audio_features.device)\n \n         # call the main sampling loop\n-        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n+        tokens, sum_logprobs, no_speech_probs, token_probs = self._main_loop(audio_features, tokens)\n \n         # reshape the tensors to have (n_audio, n_group) as the first two dimensions\n         audio_features = audio_features[:: self.n_group]\n@@ -732,7 +744,7 @@ def run(self, mel: Tensor) -> List[DecodingResult]:\n         sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)\n \n         # get the final candidates for each group, and slice between the first sampled token and EOT\n-        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)\n+        tokens, sum_logprobs, token_probs = self.decoder.finalize(tokens, sum_logprobs, token_probs)\n         tokens: List[List[Tensor]] = [\n             [t[self.sample_begin : (t == tokenizer.eot).nonzero()[0, 0]] for t in s]\n             for s in tokens\n@@ -755,6 +767,7 @@ def run(self, mel: Tensor) -> List[DecodingResult]:\n             audio_features,\n             avg_logprobs,\n             no_speech_probs,\n+            token_probs\n         )\n         if len(set(map(len, fields))) != 1:\n             raise RuntimeError(f\"inconsistent result lengths: {list(map(len, fields))}\")\n@@ -769,8 +782,9 @@ def run(self, mel: Tensor) -> List[DecodingResult]:\n                 no_speech_prob=no_speech_prob,\n                 temperature=self.options.temperature,\n                 compression_ratio=compression_ratio(text),\n+                token_probs=token_probs[-len(tokens):]\n             )\n-            for text, language, tokens, features, avg_logprob, no_speech_prob in zip(\n+            for text, language, tokens, features, avg_logprob, no_speech_prob, token_probs in zip(\n                 *fields\n             )\n         ]\n"
  },
  {
    "index": 16,
    "filtered_comments": [
      "I think some variation on this idea might help it to remember your prompting in long audio, but when a window boundary occurs mid sentence, I think it's also important to have the previous text as the prompt.\r\n\r\nAs a compromise, have you thought about truncating the previous text at a sentence boundary and then prepending the initial prompt before that? It might be the best of both worlds.",
      "A really cheap modification might be to add a check here:\r\n\r\n```python\r\n            if not condition_on_previous_text or result.temperature > 0.5:\r\n                # do not feed the prompt tokens if a high temperature was used\r\n                prompt_reset_since = len(all_tokens)\r\n```\r\n\r\nso that you also check if your option is enabled and if the latest token ends with one of these characters `\".!?\"`, effectively resetting the prompt after every sentence boundary. Then when feeding the prompt:\r\n\r\n```python\r\n            decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\r\n```\r\n\r\nIf your option is enabled you could prepend the initial prompt here.\r\n\r\nBUT, I think it might be more useful to parameterise how many previous sentences back to include in the prompt. For that the code would be a bit more complicated. But you could keep a FIFO buffer, e.g. to remember the last 3 sentences, you have a FIFO of size 3 containing the last 3 sentence boundary positions, which you put into the FIFO under the same condition as that first block of code above. The oldest sentence boundary gets popped out so you never have more than the last 3 in there.",
      "@mercury233 it is hallucinating significantly after this change. anyway to prevent it? other than that it works great. you found a solution for hallucination ? I can use very big beam size and best of but they didnt help. ",
      "> ```python\r\n>  not condition_on_previous_text or result.temperature > 0.5\r\n> ```\r\n\r\ncan you share modified file like this? i would like to test. currently it is having problems ",
      "yes with this way it is skipping 30 second blocks sometimes. we need optimization.  @mercury233 @ryanheise @radurevutchi ",
      "I have used the same basic idea of applying the initial prompt to every window to supply a dictionary of obscure words that might be in the transcript. It's very effective at boosting recognition of some words. However, I don't see it as in opposition to condition_on_previous text; the basic idea of using context from the end of the previous window to influence understanding of the beginning of the next window is still valuable."
    ],
    "code_diff": "diff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex 8e1240bd6..674e450de 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -46,6 +46,7 @@ def transcribe(\n     no_speech_threshold: Optional[float] = 0.6,\n     condition_on_previous_text: bool = True,\n     initial_prompt: Optional[str] = None,\n+    always_use_initial_prompt: bool = False,\n     word_timestamps: bool = False,\n     prepend_punctuations: str = \"\\\"'([{-\",\n     append_punctuations: str = \"\\\"'.,!?:)]}\",\n@@ -102,6 +103,11 @@ def transcribe(\n         \"prompt-engineer\" a context for transcription, e.g. custom vocabularies or proper nouns\n         to make it more likely to predict those word correctly.\n \n+    always_use_initial_prompt: bool\n+        if True, the initial_prompt will be used to all windows, and condition_on_previous_text\n+        will be ignored. Enabling this may make the text more consistent if the audio is long\n+        and you set the initial_prompt properly.\n+\n     decode_options: dict\n         Keyword arguments to construct `DecodingOptions` instances\n \n@@ -275,7 +281,11 @@ def new_segment(\n             segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n             mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n \n-            decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n+            if always_use_initial_prompt:\n+                decode_options[\"prompt\"] = initial_prompt_tokens\n+            else:\n+                decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n+\n             result: DecodingResult = decode_with_fallback(mel_segment)\n             tokens = torch.tensor(result.tokens)\n \n@@ -530,6 +540,7 @@ def valid_model_name(name):\n     parser.add_argument(\"--suppress_tokens\", type=str, default=\"-1\", help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n     parser.add_argument(\"--initial_prompt\", type=str, default=None, help=\"optional text to provide as a prompt for the first window.\")\n     parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=True, help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n+    parser.add_argument(\"--always_use_initial_prompt\", type=str2bool, default=False, help=\"if True, the initial_prompt will be used to all windows, and condition_on_previous_text will be ignored. Enabling this may make the text more consistent if the audio is long and you set the initial_prompt properly.\")\n     parser.add_argument(\"--fp16\", type=str2bool, default=True, help=\"whether to perform inference in fp16; True by default\")\n \n     parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=0.2, help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n"
  },
  {
    "index": 17,
    "filtered_comments": [
      "Instead of using environment variable, i suggest to use `extras_require` as could be see in [here](https://setuptools.pypa.io/en/latest/userguide/dependency_management.html#optional-dependencies). Another option is to automatically detect if it is using ROCm platform.",
      "Based on the suggestion, remove environmental variable and add function to detect ROCm Platform automatically.",
      "Will this work with generic AMD gpus, ie newer integrated gpus?",
      "> Will this work with generic AMD gpus, ie newer integrated gpus?\r\n\r\n@x86Gr This is the list of supported GPUs\r\n\r\nhttps://rocm.docs.amd.com/en/latest/release/gpu_os_support.html",
      "Any particular reason the PR only selected a subset of supported AMD GPUs? \r\n\r\ngfx1030 or gfx1100 appear to be missing",
      "running `pip install .` on that branch shows me \r\n```\r\n% pip install .            \r\nProcessing <snip>whisper/whisper\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nINFO: pip is looking at multiple versions of openai-whisper to determine which version is compatible with other requirements. This could take a while.\r\nERROR: Could not find a version that satisfies the requirement pytorch-triton-rocm>=2.0.1 (from openai-whisper) (from versions: 0.0.1)\r\nERROR: No matching distribution found for pytorch-triton-rocm>=2.0.1\r\n```\r\nwhat am i missing here?\r\n\r\n--\r\nEdit:\r\nNevermind, `pip install --upgrade --no-deps torch pytorch-triton-rocm --index-url https://download.pytorch.org/whl/rocm6.2` solved it. Maybe we should add that to the readme?"
    ],
    "code_diff": "diff --git a/.github/workflows/test.yml b/.github/workflows/test.yml\nindex 3796a3973..dffc17c61 100644\n--- a/.github/workflows/test.yml\n+++ b/.github/workflows/test.yml\n@@ -6,8 +6,38 @@ on:\n   pull_request:\n     branches:\n       - main\n+\n jobs:\n+  pre-commit:\n+    runs-on: ubuntu-latest\n+    steps:\n+      - uses: actions/checkout@v3\n+      - name: Fetch base branch\n+        run: git fetch origin ${{ github.base_ref }}\n+      - uses: actions/setup-python@v4\n+        with:\n+          python-version: \"3.8\"\n+          architecture: x64\n+      - name: Get pip cache dir\n+        id: pip-cache\n+        run: |\n+          echo \"dir=$(pip cache dir)\" >> $GITHUB_OUTPUT\n+      - name: pip/pre-commit cache\n+        uses: actions/cache@v3\n+        with:\n+          path: |\n+            ${{ steps.pip-cache.outputs.dir }}\n+            ~/.cache/pre-commit\n+          key: ${{ runner.os }}-pip-pre-commit-${{ hashFiles('**/.pre-commit-config.yaml') }}\n+          restore-keys: |\n+            ${{ runner.os }}-pip-pre-commit\n+      - name: pre-commit\n+        run: |\n+          pip install -U pre-commit\n+          pre-commit install --install-hooks\n+          pre-commit run --all-files\n   whisper-test:\n+    needs: pre-commit\n     runs-on: ubuntu-latest\n     strategy:\n       matrix:\n@@ -23,7 +53,4 @@ jobs:\n       - uses: actions/checkout@v3\n       - run: echo \"$CONDA/envs/test/bin\" >> $GITHUB_PATH\n       - run: pip install .[\"dev\"]\n-      - run: black --check --diff -t py38 --include '(\\.pyi?)$' .\n-      - run: isort --check --diff .\n-      - run: flake8 --ignore E203,W503,W504,E501,E731,E741 .\n       - run: pytest --durations=0 -vv -k 'not test_transcribe or test_transcribe[tiny] or test_transcribe[tiny.en]' -m 'not requires_cuda'\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nnew file mode 100644\nindex 000000000..3f5a74b6d\n--- /dev/null\n+++ b/.pre-commit-config.yaml\n@@ -0,0 +1,28 @@\n+repos:\n+  - repo: https://github.com/pre-commit/pre-commit-hooks\n+    rev: v4.0.1\n+    hooks:\n+      - id: check-json\n+      - id: end-of-file-fixer\n+        types: [file, python]\n+      - id: trailing-whitespace\n+        types: [file, python]\n+      - id: mixed-line-ending\n+      - id: check-added-large-files\n+        args: [--maxkb=4096]\n+  - repo: https://github.com/psf/black\n+    rev: 23.7.0\n+    hooks:\n+      - id: black\n+  - repo: https://github.com/pycqa/isort\n+    rev: 5.12.0\n+    hooks:\n+      - id: isort\n+        name: isort (python)\n+        args: [\"--profile\", \"black\", \"-l\", \"88\", \"--trailing-comma\", \"--multi-line\", \"3\"]\n+  - repo: https://github.com/pycqa/flake8.git\n+    rev: 6.0.0\n+    hooks:\n+      - id: flake8\n+        types: [python]\n+        args: [\"--max-line-length\", \"88\", \"--ignore\", \"E203,E501,W503,W504\"]\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex a77d966f9..58955410f 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -1,5 +1,45 @@\n # CHANGELOG\n \n+## [v20231117](https://github.com/openai/whisper/releases/tag/v20231117)\n+\n+* Relax triton requirements for compatibility with pytorch 2.1 and newer ([#1802](https://github.com/openai/whisper/pull/1802))\n+\n+## [v20231106](https://github.com/openai/whisper/releases/tag/v20231106)\n+\n+* large-v3 ([#1761](https://github.com/openai/whisper/pull/1761))\n+\n+## [v20231105](https://github.com/openai/whisper/releases/tag/v20231105)\n+\n+* remove tiktoken pin ([#1759](https://github.com/openai/whisper/pull/1759))\n+* docs: Disambiguation of the term \"relative speed\" in the README ([#1751](https://github.com/openai/whisper/pull/1751))\n+* allow_pickle=False while loading of mel matrix IN audio.py ([#1511](https://github.com/openai/whisper/pull/1511))\n+* handling transcribe exceptions. ([#1682](https://github.com/openai/whisper/pull/1682))\n+* Add new option to generate subtitles by a specific number of words ([#1729](https://github.com/openai/whisper/pull/1729))\n+* Fix exception when an audio file with no speech is provided ([#1396](https://github.com/openai/whisper/pull/1396))\n+\n+## [v20230918](https://github.com/openai/whisper/releases/tag/v20230918)\n+\n+* Add .pre-commit-config.yaml ([#1528](https://github.com/openai/whisper/pull/1528))\n+* fix doc of TextDecoder ([#1526](https://github.com/openai/whisper/pull/1526))\n+* Update model-card.md ([#1643](https://github.com/openai/whisper/pull/1643))\n+* word timing tweaks ([#1559](https://github.com/openai/whisper/pull/1559))\n+* Avoid rearranging all caches ([#1483](https://github.com/openai/whisper/pull/1483))\n+* Improve timestamp heuristics. ([#1461](https://github.com/openai/whisper/pull/1461))\n+* fix condition_on_previous_text ([#1224](https://github.com/openai/whisper/pull/1224))\n+* Fix numba depreceation notice ([#1233](https://github.com/openai/whisper/pull/1233))\n+* Updated README.md to provide more insight on BLEU and specific appendices ([#1236](https://github.com/openai/whisper/pull/1236))\n+* Avoid computing higher temperatures on no_speech segments ([#1279](https://github.com/openai/whisper/pull/1279))\n+* Dropped unused execute bit from mel_filters.npz. ([#1254](https://github.com/openai/whisper/pull/1254))\n+* Drop ffmpeg-python dependency and call ffmpeg directly. ([#1242](https://github.com/openai/whisper/pull/1242))\n+* Python 3.11 ([#1171](https://github.com/openai/whisper/pull/1171))\n+* Update decoding.py ([#1219](https://github.com/openai/whisper/pull/1219))\n+* Update decoding.py ([#1155](https://github.com/openai/whisper/pull/1155))\n+* Update README.md to reference tiktoken ([#1105](https://github.com/openai/whisper/pull/1105))\n+* Implement max line width and max line count, and make word highlighting optional ([#1184](https://github.com/openai/whisper/pull/1184))\n+* Squash long words at window and sentence boundaries. ([#1114](https://github.com/openai/whisper/pull/1114))\n+* python-publish.yml: bump actions version to fix node warning ([#1211](https://github.com/openai/whisper/pull/1211))\n+* Update tokenizer.py ([#1163](https://github.com/openai/whisper/pull/1163))\n+\n ## [v20230314](https://github.com/openai/whisper/releases/tag/v20230314)\n \n * abort find_alignment on empty input ([#1090](https://github.com/openai/whisper/pull/1090))\ndiff --git a/README.md b/README.md\nindex 20532574c..607c08b6e 100644\n--- a/README.md\n+++ b/README.md\n@@ -29,6 +29,14 @@ To update the package to the latest version of this repository, please run:\n \n     pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n \n+For AMD GPU (ROCm Platform), you need to install pytorch for ROCm at the first using wheels from https://pytorch.org/ or https://repo.radeon.com/rocm/manylinux/, then install whisper from source:\n+\n+    pip install --no-build-isolation  git+https://github.com/openai/whisper.git\n+\n+To update the package to the latest version of this repository for AMD GPU (ROCm Platform), please run:\n+\n+    pip install --upgrade --no-deps --force-reinstall --no-build-isolation git+https://github.com/openai/whisper.git\n+\n It also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system, which is available from most package managers:\n \n ```bash\n@@ -57,8 +65,7 @@ pip install setuptools-rust\n \n ## Available models and languages\n \n-There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and relative speed. \n-\n+There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and inference speed relative to the large model; actual speed may vary depending on many factors including the available hardware.\n \n |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n@@ -70,9 +77,9 @@ There are five model sizes, four with English-only versions, offering speed and\n \n The `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.\n \n-Whisper's performance varies widely depending on the language. The figure below shows a WER (Word Error Rate) breakdown by languages of the Fleurs dataset using the `large-v2` model (The smaller the numbers, the better the performance). Additional WER scores corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4. Meanwhile, more BLEU (Bilingual Evaluation Understudy) scores can be found in Appendix D.3. Both are found in [the paper](https://arxiv.org/abs/2212.04356). \n+Whisper's performance varies widely depending on the language. The figure below shows a performance breakdown of `large-v3` and `large-v2` models by language, using WERs (word error rates) or CER (character error rates, shown in *Italic*) evaluated on the Common Voice 15 and Fleurs datasets. Additional WER/CER metrics corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4 of [the paper](https://arxiv.org/abs/2212.04356), as well as the BLEU (Bilingual Evaluation Understudy) scores for translation in Appendix D.3.\n \n-![WER breakdown by language](https://raw.githubusercontent.com/openai/whisper/main/language-breakdown.svg)\n+![WER breakdown by language](https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62)\n \n \n \ndiff --git a/check_rocm_platform.py b/check_rocm_platform.py\nnew file mode 100644\nindex 000000000..d968d6cae\n--- /dev/null\n+++ b/check_rocm_platform.py\n@@ -0,0 +1,147 @@\n+import os\n+import sys\n+import subprocess\n+import re\n+from shutil import which\n+def is_command(cmd):\n+    return which(cmd) is not None\n+    \n+def check_amd_gpu_lspci():\n+    check_cmd =\"lspci\"\n+    try:\n+        ps1 = subprocess.run(check_cmd.split(),  stdout=subprocess.PIPE,\n+                  stderr=subprocess.STDOUT, check=True)\n+        for line in str.splitlines(ps1.stdout.decode('utf-8')):\n+            if re.search(r'ATI] Device 740f', line):   # MI210\n+                return True\n+            elif re.search(r'ATI] Device 740c', line): # MI250 \n+                return True\n+            elif re.search(r'ATI] Aldebaran', line): # MI200\n+                return True\n+            elif re.search(r'ATI] Device 7408', line): # MI250x\n+                return True\n+            elif re.search(r'ATI] Device 738c', line): # MI100\n+                return True  \n+            elif re.search(r'ATI] Device Arcturus', line): # MI200\n+                return True\n+            elif re.search(r'ATI] Device 66af', line): # MI50\n+                return True                              \n+        return False\n+    except (FileNotFoundError, subprocess.CalledProcessError) as err:\n+        return False\n+def check_amd_gpu_rocminfo():\n+    check_cmd =\"rocminfo\"\n+    try:\n+        ps1 = subprocess.run(check_cmd.split(),  stdout=subprocess.PIPE,\n+                  stderr=subprocess.STDOUT, check=True)\n+        for line in str.splitlines(ps1.stdout.decode('utf-8')):\n+            if re.search(r'gfx906', line): # MI50/MI60\n+                return True\n+            elif re.search(r'gfx908', line): # MI100\n+                return True\n+            elif re.search(r'gfx90a', line): # MI200\n+                return True\n+        return False\n+    except (FileNotFoundError, subprocess.CalledProcessError) as err:\n+        return False\n+def check_rocm_packages( ):\n+    UBUNTU_TYPE = \"ubuntu\"\t\n+    DEBIAN_TYPE = \"debian\"\t\n+    RHEL_TYPE = \"rhel\"\t    \n+    CENTOS_TYPE = \"centos\"\t\n+    SLES_TYPE = \"sles\"\t    \n+    PKGTYPE_RPM = \"rpm\"\n+    PKGTYPE_DEB = \"deb\"\n+    RPM_CMD = \"/usr/bin/rpm\"\n+    DPKG_CMD = \"/usr/bin/dpkg\"\n+    try:\n+        import distro\n+        linux_id =  distro.id()\n+    except ModuleNotFoundError as err:\n+        ETC_OS_RELEASE = \"/etc/os-release\"\n+        with open(ETC_OS_RELEASE, 'r') as f:\n+            for line in f:\n+                if CENTOS_TYPE.lower() in line.lower():\n+                    linux_id = CENTOS_TYPE\n+                    break\n+                if DEBIAN_TYPE.lower() in line.lower():\n+                    linux_id = UBUNTU_TYPE\n+                    break\n+                if UBUNTU_TYPE.lower() in line.lower():\n+                    linux_id = UBUNTU_TYPE\n+                    break\n+                if SLES_TYPE.lower() in line.lower():\n+                    linux_id= SLES_TYPE\n+                    break\n+                if RHEL_TYPE.lower() in line.lower():\n+                    linux_id = RHEL_TYPE\n+                    break       \n+    pkgtype = {\n+        CENTOS_TYPE : PKGTYPE_RPM,\n+        RHEL_TYPE : PKGTYPE_RPM,\n+        CENTOS_TYPE : PKGTYPE_RPM,\n+        UBUNTU_TYPE : PKGTYPE_DEB,\n+        SLES_TYPE : PKGTYPE_RPM\n+    }[linux_id]\n+    if pkgtype is PKGTYPE_RPM:\n+        check_cmd = RPM_CMD + \" -q rock-dkms\"\n+        try:\n+            ps1 = subprocess.run(check_cmd.split(), stdout=subprocess.PIPE,\n+                stderr=subprocess.STDOUT, check=True)\n+            return True\n+        except subprocess.CalledProcessError as err:\n+            pass\n+        check_cmd = RPM_CMD + \" -q amdgpu-dkms\"\n+        try:\n+            ps1 = subprocess.run(check_cmd.split(), stdout=subprocess.PIPE,\n+                stderr=subprocess.STDOUT, check=True)\n+            return True\n+        except subprocess.CalledProcessError as err:\n+            pass\n+        check_cmd = RPM_CMD + \" -q hip-devel\"\n+        try:\n+            ps1 = subprocess.run(check_cmd.split(), stdout=subprocess.PIPE,\n+                stderr=subprocess.STDOUT, check=True)\n+            return True\n+        except subprocess.CalledProcessError as err:\n+            return False\n+    elif pkgtype is PKGTYPE_DEB:\n+        check_cmd = DPKG_CMD + \" -l rock-dkms\"\n+        try:\n+            ps1 = subprocess.run(check_cmd.split(), stdout=subprocess.PIPE,\n+                stderr=subprocess.STDOUT, check=True)\n+            for line in str.splitlines(ps1.stdout.decode('utf-8')):\n+                if re.search(r'^i.*rock-dkms.*all', line): # 'i' for installed\n+                    return True\n+        except subprocess.CalledProcessError as err:\n+            pass\n+        check_cmd = DPKG_CMD + \" -l amdgpu-dkms\"\n+        try:\n+            ps1 = subprocess.run(check_cmd.split(), stdout=subprocess.PIPE,\n+                stderr=subprocess.STDOUT, check=True)\n+            for line in str.splitlines(ps1.stdout.decode('utf-8')):\n+                if re.search(r'^i.*amdgpu-dkms.*all', line): # 'i' for installed\n+                    return True\n+        except subprocess.CalledProcessError as err:\n+            pass\n+        check_cmd = DPKG_CMD + \" -l miopen-hip\"\n+        try:\n+            ps1 = subprocess.run(check_cmd.split(), stdout=subprocess.PIPE,\n+                stderr=subprocess.STDOUT, check=True)\n+            for line in str.splitlines(ps1.stdout.decode('utf-8')):\n+                if re.search(r'^i.*miopen-hip.*', line): # 'i' for installed\n+                    return True\n+        except subprocess.CalledProcessError as err:\n+            pass\n+        check_cmd = DPKG_CMD + \" -l hip-dev\"\n+        try:\n+            ps1 = subprocess.run(check_cmd.split(), stdout=subprocess.PIPE,\n+                stderr=subprocess.STDOUT, check=True)\n+            for line in str.splitlines(ps1.stdout.decode('utf-8')):\n+                if re.search(r'^i.*hip-dev.*', line): # 'i' for installed\n+                    return True\n+        except subprocess.CalledProcessError as err:\n+            return False\n+    else:\n+        print(\"Unknown package type {}. Cannot detect rock-dkms amdgpu-dkms status.\".format(pkgtype))\n+        return False    \ndiff --git a/language-breakdown.svg b/language-breakdown.svg\nindex 49a0653eb..616fd57ea 100644\n--- a/language-breakdown.svg\n+++ b/language-breakdown.svg\n@@ -1,16 +1,16 @@\n <?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n <!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n   \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n-<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"653.534375pt\" height=\"876.35625pt\" viewBox=\"0 0 653.534375 876.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n+<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"658.3pt\" height=\"946.914375pt\" viewBox=\"0 0 658.3 946.914375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n  <metadata>\n   <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n    <cc:Work>\n     <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n-    <dc:date>2022-12-03T03:56:51.812586</dc:date>\n+    <dc:date>2023-11-06T12:34:22.337927</dc:date>\n     <dc:format>image/svg+xml</dc:format>\n     <dc:creator>\n      <cc:Agent>\n-      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n+      <dc:title>Matplotlib v3.7.3, https://matplotlib.org/</dc:title>\n      </cc:Agent>\n     </dc:creator>\n    </cc:Work>\n@@ -21,498 +21,42 @@\n  </defs>\n  <g id=\"figure_1\">\n   <g id=\"patch_1\">\n-   <path d=\"M 0 876.35625 \n-L 653.534375 876.35625 \n-L 653.534375 0 \n+   <path d=\"M 0 946.914375 \n+L 658.3 946.914375 \n+L 658.3 0 \n L 0 0 \n z\n \" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"axes_1\">\n    <g id=\"patch_2\">\n-    <path d=\"M 88.334375 838.8 \n-L 646.334375 838.8 \n-L 646.334375 7.2 \n-L 88.334375 7.2 \n+    <path d=\"M 93.1 909.358125 \n+L 325.6 909.358125 \n+L 325.6 22.318125 \n+L 93.1 22.318125 \n z\n \" style=\"fill: #ffffff\"/>\n-   </g>\n-   <g id=\"patch_3\">\n-    <path d=\"M -404358.487716 8.658947 \n-L 123.184246 8.658947 \n-L 123.184246 20.330526 \n-L -404358.487716 20.330526 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_4\">\n-    <path d=\"M -404358.487716 23.248421 \n-L 171.30291 23.248421 \n-L 171.30291 34.92 \n-L -404358.487716 34.92 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_5\">\n-    <path d=\"M -404358.487716 37.837895 \n-L 178.734895 37.837895 \n-L 178.734895 49.509474 \n-L -404358.487716 49.509474 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_6\">\n-    <path d=\"M -404358.487716 52.427368 \n-L 185.202663 52.427368 \n-L 185.202663 64.098947 \n-L -404358.487716 64.098947 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_7\">\n-    <path d=\"M -404358.487716 67.016842 \n-L 191.500972 67.016842 \n-L 191.500972 78.688421 \n-L -404358.487716 78.688421 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_8\">\n-    <path d=\"M -404358.487716 81.606316 \n-L 219.316664 81.606316 \n-L 219.316664 93.277895 \n-L -404358.487716 93.277895 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_9\">\n-    <path d=\"M -404358.487716 96.195789 \n-L 224.698608 96.195789 \n-L 224.698608 107.867368 \n-L -404358.487716 107.867368 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_10\">\n-    <path d=\"M -404358.487716 110.785263 \n-L 230.6044 110.785263 \n-L 230.6044 122.456842 \n-L -404358.487716 122.456842 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_11\">\n-    <path d=\"M -404358.487716 125.374737 \n-L 261.548355 125.374737 \n-L 261.548355 137.046316 \n-L -404358.487716 137.046316 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_12\">\n-    <path d=\"M -404358.487716 139.964211 \n-L 270.550997 139.964211 \n-L 270.550997 151.635789 \n-L -404358.487716 151.635789 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_13\">\n-    <path d=\"M -404358.487716 154.553684 \n-L 276.715851 154.553684 \n-L 276.715851 166.225263 \n-L -404358.487716 166.225263 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_14\">\n-    <path d=\"M -404358.487716 169.143158 \n-L 299.071954 169.143158 \n-L 299.071954 180.814737 \n-L -404358.487716 180.814737 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_15\">\n-    <path d=\"M -404358.487716 183.732632 \n-L 300.832666 183.732632 \n-L 300.832666 195.404211 \n-L -404358.487716 195.404211 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_16\">\n-    <path d=\"M -404358.487716 198.322105 \n-L 303.486189 198.322105 \n-L 303.486189 209.993684 \n-L -404358.487716 209.993684 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_17\">\n-    <path d=\"M -404358.487716 212.911579 \n-L 304.285774 212.911579 \n-L 304.285774 224.583158 \n-L -404358.487716 224.583158 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_18\">\n-    <path d=\"M -404358.487716 227.501053 \n-L 306.543645 227.501053 \n-L 306.543645 239.172632 \n-L -404358.487716 239.172632 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_19\">\n-    <path d=\"M -404358.487716 242.090526 \n-L 322.050979 242.090526 \n-L 322.050979 253.762105 \n-L -404358.487716 253.762105 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_20\">\n-    <path d=\"M -404358.487716 256.68 \n-L 325.898458 256.68 \n-L 325.898458 268.351579 \n-L -404358.487716 268.351579 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_21\">\n-    <path d=\"M -404358.487716 271.269474 \n-L 336.531215 271.269474 \n-L 336.531215 282.941053 \n-L -404358.487716 282.941053 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_22\">\n-    <path d=\"M -404358.487716 285.858947 \n-L 357.022664 285.858947 \n-L 357.022664 297.530526 \n-L -404358.487716 297.530526 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_23\">\n-    <path d=\"M -404358.487716 300.448421 \n-L 359.535051 300.448421 \n-L 359.535051 312.12 \n-L -404358.487716 312.12 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_24\">\n-    <path d=\"M -404358.487716 315.037895 \n-L 371.137848 315.037895 \n-L 371.137848 326.709474 \n-L -404358.487716 326.709474 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_25\">\n-    <path d=\"M -404358.487716 329.627368 \n-L 382.371511 329.627368 \n-L 382.371511 341.298947 \n-L -404358.487716 341.298947 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_26\">\n-    <path d=\"M -404358.487716 344.216842 \n-L 383.594727 344.216842 \n-L 383.594727 355.888421 \n-L -404358.487716 355.888421 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_27\">\n-    <path d=\"M -404358.487716 358.806316 \n-L 387.914549 358.806316 \n-L 387.914549 370.477895 \n-L -404358.487716 370.477895 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_28\">\n-    <path d=\"M -404358.487716 373.395789 \n-L 388.310994 373.395789 \n-L 388.310994 385.067368 \n-L -404358.487716 385.067368 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_29\">\n-    <path d=\"M -404358.487716 387.985263 \n-L 394.084973 387.985263 \n-L 394.084973 399.656842 \n-L -404358.487716 399.656842 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_30\">\n-    <path d=\"M -404358.487716 402.574737 \n-L 396.219137 402.574737 \n-L 396.219137 414.246316 \n-L -404358.487716 414.246316 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_31\">\n-    <path d=\"M -404358.487716 417.164211 \n-L 397.847423 417.164211 \n-L 397.847423 428.835789 \n-L -404358.487716 428.835789 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_32\">\n-    <path d=\"M -404358.487716 431.753684 \n-L 399.339759 431.753684 \n-L 399.339759 443.425263 \n-L -404358.487716 443.425263 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_33\">\n-    <path d=\"M -404358.487716 446.343158 \n-L 408.062556 446.343158 \n-L 408.062556 458.014737 \n-L -404358.487716 458.014737 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_34\">\n-    <path d=\"M -404358.487716 460.932632 \n-L 410.403084 460.932632 \n-L 410.403084 472.604211 \n-L -404358.487716 472.604211 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_35\">\n-    <path d=\"M -404358.487716 475.522105 \n-L 414.005627 475.522105 \n-L 414.005627 487.193684 \n-L -404358.487716 487.193684 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_36\">\n-    <path d=\"M -404358.487716 490.111579 \n-L 419.982408 490.111579 \n-L 419.982408 501.783158 \n-L -404358.487716 501.783158 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_37\">\n-    <path d=\"M -404358.487716 504.701053 \n-L 424.547595 504.701053 \n-L 424.547595 516.372632 \n-L -404358.487716 516.372632 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_38\">\n-    <path d=\"M -404358.487716 519.290526 \n-L 429.536815 519.290526 \n-L 429.536815 530.962105 \n-L -404358.487716 530.962105 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_39\">\n-    <path d=\"M -404358.487716 533.88 \n-L 466.298908 533.88 \n-L 466.298908 545.551579 \n-L -404358.487716 545.551579 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_40\">\n-    <path d=\"M -404358.487716 548.469474 \n-L 469.282836 548.469474 \n-L 469.282836 560.141053 \n-L -404358.487716 560.141053 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_41\">\n-    <path d=\"M -404358.487716 563.058947 \n-L 474.575165 563.058947 \n-L 474.575165 574.730526 \n-L -404358.487716 574.730526 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_42\">\n-    <path d=\"M -404358.487716 577.648421 \n-L 478.743587 577.648421 \n-L 478.743587 589.32 \n-L -404358.487716 589.32 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_43\">\n-    <path d=\"M -404358.487716 592.237895 \n-L 479.082047 592.237895 \n-L 479.082047 603.909474 \n-L -404358.487716 603.909474 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_44\">\n-    <path d=\"M -404358.487716 606.827368 \n-L 481.281513 606.827368 \n-L 481.281513 618.498947 \n-L -404358.487716 618.498947 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_45\">\n-    <path d=\"M -404358.487716 621.416842 \n-L 506.539444 621.416842 \n-L 506.539444 633.088421 \n-L -404358.487716 633.088421 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_46\">\n-    <path d=\"M -404358.487716 636.006316 \n-L 513.385941 636.006316 \n-L 513.385941 647.677895 \n-L -404358.487716 647.677895 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_47\">\n-    <path d=\"M -404358.487716 650.595789 \n-L 540.809897 650.595789 \n-L 540.809897 662.267368 \n-L -404358.487716 662.267368 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_48\">\n-    <path d=\"M -404358.487716 665.185263 \n-L 541.346557 665.185263 \n-L 541.346557 676.856842 \n-L -404358.487716 676.856842 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_49\">\n-    <path d=\"M -404358.487716 679.774737 \n-L 546.060086 679.774737 \n-L 546.060086 691.446316 \n-L -404358.487716 691.446316 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_50\">\n-    <path d=\"M -404358.487716 694.364211 \n-L 559.805754 694.364211 \n-L 559.805754 706.035789 \n-L -404358.487716 706.035789 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_51\">\n-    <path d=\"M -404358.487716 708.953684 \n-L 561.410241 708.953684 \n-L 561.410241 720.625263 \n-L -404358.487716 720.625263 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_52\">\n-    <path d=\"M -404358.487716 723.543158 \n-L 564.694128 723.543158 \n-L 564.694128 735.214737 \n-L -404358.487716 735.214737 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_53\">\n-    <path d=\"M -404358.487716 738.132632 \n-L 567.234021 738.132632 \n-L 567.234021 749.804211 \n-L -404358.487716 749.804211 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_54\">\n-    <path d=\"M -404358.487716 752.722105 \n-L 567.381428 752.722105 \n-L 567.381428 764.393684 \n-L -404358.487716 764.393684 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_55\">\n-    <path d=\"M -404358.487716 767.311579 \n-L 568.631096 767.311579 \n-L 568.631096 778.983158 \n-L -404358.487716 778.983158 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_56\">\n-    <path d=\"M -404358.487716 781.901053 \n-L 572.140669 781.901053 \n-L 572.140669 793.572632 \n-L -404358.487716 793.572632 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_57\">\n-    <path d=\"M -404358.487716 796.490526 \n-L 594.098456 796.490526 \n-L 594.098456 808.162105 \n-L -404358.487716 808.162105 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_58\">\n-    <path d=\"M -404358.487716 811.08 \n-L 597.544184 811.08 \n-L 597.544184 822.751579 \n-L -404358.487716 822.751579 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n-   </g>\n-   <g id=\"patch_59\">\n-    <path d=\"M -404358.487716 825.669474 \n-L 603.770213 825.669474 \n-L 603.770213 837.341053 \n-L -404358.487716 837.341053 \n-z\n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: #1f77b4\"/>\n    </g>\n    <g id=\"matplotlib.axis_1\">\n     <g id=\"xtick_1\">\n      <g id=\"line2d_1\">\n-      <path d=\"M 88.334375 838.8 \n-L 88.334375 7.2 \n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+      <path d=\"M 93.1 909.358125 \n+L 93.1 22.318125 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n      </g>\n      <g id=\"line2d_2\">\n       <defs>\n-       <path id=\"me1c2435a4d\" d=\"M 0 0 \n+       <path id=\"m40feb81c4c\" d=\"M 0 0 \n L 0 3.5 \n \" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </defs>\n       <g>\n-       <use xlink:href=\"#me1c2435a4d\" x=\"88.334375\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"93.1\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_1\">\n       <!-- 2.5 -->\n-      <g transform=\"translate(80.382812 853.398438)scale(0.1 -0.1)\">\n+      <g transform=\"translate(85.148438 923.956562) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n L 3431 531 \n@@ -579,36 +123,36 @@ z\n     </g>\n     <g id=\"xtick_2\">\n      <g id=\"line2d_3\">\n-      <path d=\"M 210.03657 838.8 \n-L 210.03657 7.2 \n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+      <path d=\"M 141.463394 909.358125 \n+L 141.463394 22.318125 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n      </g>\n      <g id=\"line2d_4\">\n       <g>\n-       <use xlink:href=\"#me1c2435a4d\" x=\"210.03657\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"141.463394\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_2\">\n       <!-- 5 -->\n-      <g transform=\"translate(206.85532 853.398438)scale(0.1 -0.1)\">\n+      <g transform=\"translate(138.282144 923.956562) scale(0.1 -0.1)\">\n        <use xlink:href=\"#DejaVuSans-35\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"xtick_3\">\n      <g id=\"line2d_5\">\n-      <path d=\"M 331.738765 838.8 \n-L 331.738765 7.2 \n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+      <path d=\"M 189.826788 909.358125 \n+L 189.826788 22.318125 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n      </g>\n      <g id=\"line2d_6\">\n       <g>\n-       <use xlink:href=\"#me1c2435a4d\" x=\"331.738765\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"189.826788\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_3\">\n       <!-- 10 -->\n-      <g transform=\"translate(325.376265 853.398438)scale(0.1 -0.1)\">\n+      <g transform=\"translate(183.464288 923.956562) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-31\" d=\"M 794 531 \n L 1825 531 \n@@ -653,18 +197,18 @@ z\n     </g>\n     <g id=\"xtick_4\">\n      <g id=\"line2d_7\">\n-      <path d=\"M 453.44096 838.8 \n-L 453.44096 7.2 \n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+      <path d=\"M 238.190182 909.358125 \n+L 238.190182 22.318125 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n      </g>\n      <g id=\"line2d_8\">\n       <g>\n-       <use xlink:href=\"#me1c2435a4d\" x=\"453.44096\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"238.190182\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_4\">\n       <!-- 20 -->\n-      <g transform=\"translate(447.07846 853.398438)scale(0.1 -0.1)\">\n+      <g transform=\"translate(231.827682 923.956562) scale(0.1 -0.1)\">\n        <use xlink:href=\"#DejaVuSans-32\"/>\n        <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       </g>\n@@ -672,18 +216,18 @@ L 453.44096 7.2\n     </g>\n     <g id=\"xtick_5\">\n      <g id=\"line2d_9\">\n-      <path d=\"M 575.143155 838.8 \n-L 575.143155 7.2 \n-\" clip-path=\"url(#pd97573e82b)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+      <path d=\"M 286.553576 909.358125 \n+L 286.553576 22.318125 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n      </g>\n      <g id=\"line2d_10\">\n       <g>\n-       <use xlink:href=\"#me1c2435a4d\" x=\"575.143155\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"286.553576\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_5\">\n       <!-- 40 -->\n-      <g transform=\"translate(568.780655 853.398438)scale(0.1 -0.1)\">\n+      <g transform=\"translate(280.191076 923.956562) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n L 825 1625 \n@@ -713,74 +257,81 @@ z\n     <g id=\"xtick_6\">\n      <g id=\"line2d_11\">\n       <defs>\n-       <path id=\"m2616ef86b4\" d=\"M 0 0 \n+       <path id=\"mc7c5720e90\" d=\"M 0 0 \n L 0 2 \n \" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </defs>\n       <g>\n-       <use xlink:href=\"#m2616ef86b4\" x=\"120.34624\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"105.821237\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"xtick_7\">\n      <g id=\"line2d_12\">\n       <g>\n-       <use xlink:href=\"#m2616ef86b4\" x=\"170.857214\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"125.893859\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"xtick_8\">\n      <g id=\"line2d_13\">\n       <g>\n-       <use xlink:href=\"#m2616ef86b4\" x=\"242.048434\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"154.184631\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"xtick_9\">\n      <g id=\"line2d_14\">\n       <g>\n-       <use xlink:href=\"#m2616ef86b4\" x=\"269.11408\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"164.940283\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"xtick_10\">\n      <g id=\"line2d_15\">\n       <g>\n-       <use xlink:href=\"#m2616ef86b4\" x=\"292.559409\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"174.257253\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"xtick_11\">\n      <g id=\"line2d_16\">\n       <g>\n-       <use xlink:href=\"#m2616ef86b4\" x=\"313.239655\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"182.475402\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"xtick_12\">\n      <g id=\"line2d_17\">\n       <g>\n-       <use xlink:href=\"#m2616ef86b4\" x=\"524.63218\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"266.480954\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"xtick_13\">\n      <g id=\"line2d_18\">\n       <g>\n-       <use xlink:href=\"#m2616ef86b4\" x=\"614.32251\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"302.123111\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"xtick_14\">\n      <g id=\"line2d_19\">\n       <g>\n-       <use xlink:href=\"#m2616ef86b4\" x=\"646.334375\" y=\"838.8\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"314.844348\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_15\">\n+     <g id=\"line2d_20\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"325.6\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"text_6\">\n-     <!-- WER (%) on Fleurs -->\n-     <g transform=\"translate(320.939063 867.076562)scale(0.1 -0.1)\">\n+     <!-- WER or $CER$ (%) -->\n+     <g transform=\"translate(169.1 937.634687) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-57\" d=\"M 213 4666 \n L 850 4666 \n@@ -842,6 +393,112 @@ L 1259 4147\n z\n \" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n+       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n+Q 1497 3097 1228 2736 \n+Q 959 2375 959 1747 \n+Q 959 1119 1226 758 \n+Q 1494 397 1959 397 \n+Q 2419 397 2687 759 \n+Q 2956 1122 2956 1747 \n+Q 2956 2369 2687 2733 \n+Q 2419 3097 1959 3097 \n+z\n+M 1959 3584 \n+Q 2709 3584 3137 3096 \n+Q 3566 2609 3566 1747 \n+Q 3566 888 3137 398 \n+Q 2709 -91 1959 -91 \n+Q 1206 -91 779 398 \n+Q 353 888 353 1747 \n+Q 353 2609 779 3096 \n+Q 1206 3584 1959 3584 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n+Q 2534 3019 2420 3045 \n+Q 2306 3072 2169 3072 \n+Q 1681 3072 1420 2755 \n+Q 1159 2438 1159 1844 \n+L 1159 0 \n+L 581 0 \n+L 581 3500 \n+L 1159 3500 \n+L 1159 2956 \n+Q 1341 3275 1631 3429 \n+Q 1922 3584 2338 3584 \n+Q 2397 3584 2469 3576 \n+Q 2541 3569 2628 3553 \n+L 2631 2963 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       <path id=\"DejaVuSans-Oblique-43\" d=\"M 4447 4306 \n+L 4319 3641 \n+Q 4019 3944 3683 4091 \n+Q 3347 4238 2956 4238 \n+Q 2422 4238 2017 3981 \n+Q 1613 3725 1319 3200 \n+Q 1131 2863 1032 2486 \n+Q 934 2109 934 1728 \n+Q 934 1091 1264 756 \n+Q 1594 422 2222 422 \n+Q 2656 422 3056 561 \n+Q 3456 700 3834 978 \n+L 3688 231 \n+Q 3316 72 2936 -9 \n+Q 2556 -91 2175 -91 \n+Q 1278 -91 773 396 \n+Q 269 884 269 1753 \n+Q 269 2309 461 2846 \n+Q 653 3384 1013 3828 \n+Q 1394 4300 1883 4525 \n+Q 2372 4750 3022 4750 \n+Q 3422 4750 3780 4639 \n+Q 4138 4528 4447 4306 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       <path id=\"DejaVuSans-Oblique-45\" d=\"M 1081 4666 \n+L 4031 4666 \n+L 3928 4134 \n+L 1606 4134 \n+L 1338 2753 \n+L 3566 2753 \n+L 3463 2222 \n+L 1234 2222 \n+L 909 531 \n+L 3284 531 \n+L 3181 0 \n+L 172 0 \n+L 1081 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       <path id=\"DejaVuSans-Oblique-52\" d=\"M 1613 4147 \n+L 1294 2491 \n+L 2106 2491 \n+Q 2584 2491 2879 2755 \n+Q 3175 3019 3175 3444 \n+Q 3175 3784 2976 3965 \n+Q 2778 4147 2406 4147 \n+L 1613 4147 \n+z\n+M 2772 2241 \n+Q 2972 2194 3105 2009 \n+Q 3238 1825 3413 1275 \n+L 3809 0 \n+L 3144 0 \n+L 2778 1197 \n+Q 2638 1659 2453 1815 \n+Q 2269 1972 1888 1972 \n+L 1191 1972 \n+L 806 0 \n+L 172 0 \n+L 1081 4666 \n+L 2503 4666 \n+Q 3150 4666 3495 4373 \n+Q 3841 4081 3841 3531 \n+Q 3841 3044 3547 2687 \n+Q 3253 2331 2772 2241 \n+z\n+\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n Q 1566 4138 1362 3434 \n Q 1159 2731 1159 2009 \n@@ -915,92 +572,60 @@ Q 1338 2731 1133 3434\n Q 928 4138 513 4856 \n z\n \" transform=\"scale(0.015625)\"/>\n-       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n-Q 1497 3097 1228 2736 \n-Q 959 2375 959 1747 \n-Q 959 1119 1226 758 \n-Q 1494 397 1959 397 \n-Q 2419 397 2687 759 \n-Q 2956 1122 2956 1747 \n-Q 2956 2369 2687 2733 \n-Q 2419 3097 1959 3097 \n+      </defs>\n+      <use xlink:href=\"#DejaVuSans-57\" transform=\"translate(0 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-45\" transform=\"translate(98.876953 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-52\" transform=\"translate(162.060547 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(231.542969 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(263.330078 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(324.511719 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(365.625 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-Oblique-43\" transform=\"translate(397.412109 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-Oblique-45\" transform=\"translate(467.236328 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-Oblique-52\" transform=\"translate(530.419922 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(599.902344 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(631.689453 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-25\" transform=\"translate(670.703125 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(765.722656 0.125)\"/>\n+     </g>\n+    </g>\n+   </g>\n+   <g id=\"matplotlib.axis_2\">\n+    <g id=\"ytick_1\">\n+     <g id=\"line2d_21\">\n+      <defs>\n+       <path id=\"m4a569c92d6\" d=\"M 0 0 \n+L -3.5 0 \n+\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </defs>\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"30.099178\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_7\">\n+      <!-- Dutch -->\n+      <g transform=\"translate(56.30625 33.898396) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-44\" d=\"M 1259 4147 \n+L 1259 519 \n+L 2022 519 \n+Q 2988 519 3436 956 \n+Q 3884 1394 3884 2338 \n+Q 3884 3275 3436 3711 \n+Q 2988 4147 2022 4147 \n+L 1259 4147 \n z\n-M 1959 3584 \n-Q 2709 3584 3137 3096 \n-Q 3566 2609 3566 1747 \n-Q 3566 888 3137 398 \n-Q 2709 -91 1959 -91 \n-Q 1206 -91 779 398 \n-Q 353 888 353 1747 \n-Q 353 2609 779 3096 \n-Q 1206 3584 1959 3584 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n-L 3513 0 \n-L 2938 0 \n-L 2938 2094 \n-Q 2938 2591 2744 2837 \n-Q 2550 3084 2163 3084 \n-Q 1697 3084 1428 2787 \n-Q 1159 2491 1159 1978 \n-L 1159 0 \n-L 581 0 \n-L 581 3500 \n-L 1159 3500 \n-L 1159 2956 \n-Q 1366 3272 1645 3428 \n-Q 1925 3584 2291 3584 \n-Q 2894 3584 3203 3211 \n-Q 3513 2838 3513 2113 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n-L 3309 4666 \n-L 3309 4134 \n-L 1259 4134 \n-L 1259 2759 \n-L 3109 2759 \n-L 3109 2228 \n-L 1259 2228 \n-L 1259 0 \n+M 628 4666 \n+L 1925 4666 \n+Q 3281 4666 3915 4102 \n+Q 4550 3538 4550 2338 \n+Q 4550 1131 3912 565 \n+Q 3275 0 1925 0 \n L 628 0 \n L 628 4666 \n z\n \" transform=\"scale(0.015625)\"/>\n-       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n-L 1178 4863 \n-L 1178 0 \n-L 603 0 \n-L 603 4863 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n-L 3597 1613 \n-L 953 1613 \n-Q 991 1019 1311 708 \n-Q 1631 397 2203 397 \n-Q 2534 397 2845 478 \n-Q 3156 559 3463 722 \n-L 3463 178 \n-Q 3153 47 2828 -22 \n-Q 2503 -91 2169 -91 \n-Q 1331 -91 842 396 \n-Q 353 884 353 1716 \n-Q 353 2575 817 3079 \n-Q 1281 3584 2069 3584 \n-Q 2775 3584 3186 3129 \n-Q 3597 2675 3597 1894 \n-z\n-M 3022 2063 \n-Q 3016 2534 2758 2815 \n-Q 2500 3097 2075 3097 \n-Q 1594 3097 1305 2825 \n-Q 1016 2553 972 2059 \n-L 3022 2063 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n+        <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n L 544 3500 \n L 1119 3500 \n L 1119 1403 \n@@ -1022,90 +647,85 @@ M 1991 3584\n L 1991 3584 \n z\n \" transform=\"scale(0.015625)\"/>\n-       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n-Q 2534 3019 2420 3045 \n-Q 2306 3072 2169 3072 \n-Q 1681 3072 1420 2755 \n-Q 1159 2438 1159 1844 \n+        <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n+L 1172 3500 \n+L 2356 3500 \n+L 2356 3053 \n+L 1172 3053 \n+L 1172 1153 \n+Q 1172 725 1289 603 \n+Q 1406 481 1766 481 \n+L 2356 481 \n+L 2356 0 \n+L 1766 0 \n+Q 1100 0 847 248 \n+Q 594 497 594 1153 \n+L 594 3053 \n+L 172 3053 \n+L 172 3500 \n+L 594 3500 \n+L 594 4494 \n+L 1172 4494 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n+L 3122 2828 \n+Q 2878 2963 2633 3030 \n+Q 2388 3097 2138 3097 \n+Q 1578 3097 1268 2742 \n+Q 959 2388 959 1747 \n+Q 959 1106 1268 751 \n+Q 1578 397 2138 397 \n+Q 2388 397 2633 464 \n+Q 2878 531 3122 666 \n+L 3122 134 \n+Q 2881 22 2623 -34 \n+Q 2366 -91 2075 -91 \n+Q 1284 -91 818 406 \n+Q 353 903 353 1747 \n+Q 353 2603 823 3093 \n+Q 1294 3584 2113 3584 \n+Q 2378 3584 2631 3529 \n+Q 2884 3475 3122 3366 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n+L 3513 0 \n+L 2938 0 \n+L 2938 2094 \n+Q 2938 2591 2744 2837 \n+Q 2550 3084 2163 3084 \n+Q 1697 3084 1428 2787 \n+Q 1159 2491 1159 1978 \n L 1159 0 \n L 581 0 \n-L 581 3500 \n-L 1159 3500 \n+L 581 4863 \n+L 1159 4863 \n L 1159 2956 \n-Q 1341 3275 1631 3429 \n-Q 1922 3584 2338 3584 \n-Q 2397 3584 2469 3576 \n-Q 2541 3569 2628 3553 \n-L 2631 2963 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n-L 2834 2853 \n-Q 2591 2978 2328 3040 \n-Q 2066 3103 1784 3103 \n-Q 1356 3103 1142 2972 \n-Q 928 2841 928 2578 \n-Q 928 2378 1081 2264 \n-Q 1234 2150 1697 2047 \n-L 1894 2003 \n-Q 2506 1872 2764 1633 \n-Q 3022 1394 3022 966 \n-Q 3022 478 2636 193 \n-Q 2250 -91 1575 -91 \n-Q 1294 -91 989 -36 \n-Q 684 19 347 128 \n-L 347 722 \n-Q 666 556 975 473 \n-Q 1284 391 1588 391 \n-Q 1994 391 2212 530 \n-Q 2431 669 2431 922 \n-Q 2431 1156 2273 1281 \n-Q 2116 1406 1581 1522 \n-L 1381 1569 \n-Q 847 1681 609 1914 \n-Q 372 2147 372 2553 \n-Q 372 3047 722 3315 \n-Q 1072 3584 1716 3584 \n-Q 2034 3584 2315 3537 \n-Q 2597 3491 2834 3397 \n+Q 1366 3272 1645 3428 \n+Q 1925 3584 2291 3584 \n+Q 2894 3584 3203 3211 \n+Q 3513 2838 3513 2113 \n z\n \" transform=\"scale(0.015625)\"/>\n-      </defs>\n-      <use xlink:href=\"#DejaVuSans-57\"/>\n-      <use xlink:href=\"#DejaVuSans-45\" x=\"98.876953\"/>\n-      <use xlink:href=\"#DejaVuSans-52\" x=\"162.060547\"/>\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"231.542969\"/>\n-      <use xlink:href=\"#DejaVuSans-28\" x=\"263.330078\"/>\n-      <use xlink:href=\"#DejaVuSans-25\" x=\"302.34375\"/>\n-      <use xlink:href=\"#DejaVuSans-29\" x=\"397.363281\"/>\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"436.376953\"/>\n-      <use xlink:href=\"#DejaVuSans-6f\" x=\"468.164062\"/>\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"529.345703\"/>\n-      <use xlink:href=\"#DejaVuSans-20\" x=\"592.724609\"/>\n-      <use xlink:href=\"#DejaVuSans-46\" x=\"624.511719\"/>\n-      <use xlink:href=\"#DejaVuSans-6c\" x=\"682.03125\"/>\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"709.814453\"/>\n-      <use xlink:href=\"#DejaVuSans-75\" x=\"771.337891\"/>\n-      <use xlink:href=\"#DejaVuSans-72\" x=\"834.716797\"/>\n-      <use xlink:href=\"#DejaVuSans-73\" x=\"875.830078\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-44\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"77.001953\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"140.380859\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"179.589844\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"234.570312\"/>\n+      </g>\n      </g>\n     </g>\n-   </g>\n-   <g id=\"matplotlib.axis_2\">\n-    <g id=\"ytick_1\">\n-     <g id=\"line2d_20\">\n-      <defs>\n-       <path id=\"m1fe7c4892d\" d=\"M 0 0 \n-L -3.5 0 \n-\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </defs>\n+    <g id=\"ytick_2\">\n+     <g id=\"line2d_22\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"14.494737\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"45.661283\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_7\">\n+     <g id=\"text_8\">\n       <!-- Spanish -->\n-      <g transform=\"translate(41.846875 18.293956)scale(0.1 -0.1)\">\n+      <g transform=\"translate(46.6125 49.460502) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \n L 3425 3897 \n@@ -1197,20 +817,7 @@ Q 2591 3584 2966 3190\n Q 3341 2797 3341 1997 \n z\n \" transform=\"scale(0.015625)\"/>\n-        <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n-L 1178 3500 \n-L 1178 0 \n-L 603 0 \n-L 603 3500 \n-z\n-M 603 4863 \n-L 1178 4863 \n-L 1178 4134 \n-L 603 4134 \n-L 603 4863 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n+        <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n L 3513 0 \n L 2938 0 \n L 2938 2094 \n@@ -1220,14 +827,58 @@ Q 1697 3084 1428 2787\n Q 1159 2491 1159 1978 \n L 1159 0 \n L 581 0 \n-L 581 4863 \n-L 1159 4863 \n+L 581 3500 \n+L 1159 3500 \n L 1159 2956 \n Q 1366 3272 1645 3428 \n Q 1925 3584 2291 3584 \n Q 2894 3584 3203 3211 \n Q 3513 2838 3513 2113 \n z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n+L 1178 3500 \n+L 1178 0 \n+L 603 0 \n+L 603 3500 \n+z\n+M 603 4863 \n+L 1178 4863 \n+L 1178 4134 \n+L 603 4134 \n+L 603 4863 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n+L 2834 2853 \n+Q 2591 2978 2328 3040 \n+Q 2066 3103 1784 3103 \n+Q 1356 3103 1142 2972 \n+Q 928 2841 928 2578 \n+Q 928 2378 1081 2264 \n+Q 1234 2150 1697 2047 \n+L 1894 2003 \n+Q 2506 1872 2764 1633 \n+Q 3022 1394 3022 966 \n+Q 3022 478 2636 193 \n+Q 2250 -91 1575 -91 \n+Q 1294 -91 989 -36 \n+Q 684 19 347 128 \n+L 347 722 \n+Q 666 556 975 473 \n+Q 1284 391 1588 391 \n+Q 1994 391 2212 530 \n+Q 2431 669 2431 922 \n+Q 2431 1156 2273 1281 \n+Q 2116 1406 1581 1522 \n+L 1381 1569 \n+Q 847 1681 609 1914 \n+Q 372 2147 372 2553 \n+Q 372 3047 722 3315 \n+Q 1072 3584 1716 3584 \n+Q 2034 3584 2315 3537 \n+Q 2597 3491 2834 3397 \n+z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n        <use xlink:href=\"#DejaVuSans-53\"/>\n@@ -1240,164 +891,220 @@ z\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_2\">\n-     <g id=\"line2d_21\">\n+    <g id=\"ytick_3\">\n+     <g id=\"line2d_23\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"29.084211\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"61.223388\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_8\">\n-      <!-- Italian -->\n-      <g transform=\"translate(50.314063 32.883429)scale(0.1 -0.1)\">\n+     <g id=\"text_9\">\n+      <!-- $Korean$ -->\n+      <g transform=\"translate(50.6 65.022607) scale(0.1 -0.1)\">\n        <defs>\n-        <path id=\"DejaVuSans-49\" d=\"M 628 4666 \n-L 1259 4666 \n-L 1259 0 \n-L 628 0 \n-L 628 4666 \n+        <path id=\"DejaVuSans-Oblique-4b\" d=\"M 1081 4666 \n+L 1716 4666 \n+L 1331 2700 \n+L 3781 4666 \n+L 4622 4666 \n+L 1850 2438 \n+L 3878 0 \n+L 3109 0 \n+L 1247 2272 \n+L 806 0 \n+L 172 0 \n+L 1081 4666 \n z\n \" transform=\"scale(0.015625)\"/>\n-        <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n-L 1172 3500 \n-L 2356 3500 \n-L 2356 3053 \n-L 1172 3053 \n-L 1172 1153 \n-Q 1172 725 1289 603 \n-Q 1406 481 1766 481 \n-L 2356 481 \n-L 2356 0 \n-L 1766 0 \n-Q 1100 0 847 248 \n-Q 594 497 594 1153 \n-L 594 3053 \n-L 172 3053 \n-L 172 3500 \n-L 594 3500 \n-L 594 4494 \n-L 1172 4494 \n+        <path id=\"DejaVuSans-Oblique-6f\" d=\"M 1625 -91 \n+Q 1009 -91 651 289 \n+Q 294 669 294 1325 \n+Q 294 1706 417 2101 \n+Q 541 2497 738 2766 \n+Q 1047 3184 1428 3384 \n+Q 1809 3584 2291 3584 \n+Q 2888 3584 3255 3212 \n+Q 3622 2841 3622 2241 \n+Q 3622 1825 3500 1412 \n+Q 3378 1000 3181 728 \n+Q 2875 309 2494 109 \n+Q 2113 -91 1625 -91 \n+z\n+M 891 1344 \n+Q 891 869 1089 633 \n+Q 1288 397 1691 397 \n+Q 2269 397 2648 901 \n+Q 3028 1406 3028 2181 \n+Q 3028 2634 2825 2865 \n+Q 2622 3097 2228 3097 \n+Q 1903 3097 1650 2945 \n+Q 1397 2794 1197 2484 \n+Q 1050 2253 970 1956 \n+Q 891 1659 891 1344 \n z\n \" transform=\"scale(0.015625)\"/>\n-       </defs>\n-       <use xlink:href=\"#DejaVuSans-49\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"29.492188\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"68.701172\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"129.980469\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"157.763672\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"185.546875\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"246.826172\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_3\">\n-     <g id=\"line2d_22\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"43.673684\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_9\">\n-      <!-- English -->\n-      <g transform=\"translate(45.226563 47.472903)scale(0.1 -0.1)\">\n-       <defs>\n-        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n-Q 2906 2416 2648 2759 \n-Q 2391 3103 1925 3103 \n-Q 1463 3103 1205 2759 \n-Q 947 2416 947 1791 \n-Q 947 1169 1205 825 \n-Q 1463 481 1925 481 \n-Q 2391 481 2648 825 \n-Q 2906 1169 2906 1791 \n+        <path id=\"DejaVuSans-Oblique-72\" d=\"M 2853 2969 \n+Q 2766 3016 2653 3041 \n+Q 2541 3066 2413 3066 \n+Q 1953 3066 1609 2717 \n+Q 1266 2369 1153 1784 \n+L 800 0 \n+L 225 0 \n+L 909 3500 \n+L 1484 3500 \n+L 1375 2956 \n+Q 1603 3259 1920 3421 \n+Q 2238 3584 2597 3584 \n+Q 2691 3584 2781 3573 \n+Q 2872 3563 2963 3538 \n+L 2853 2969 \n z\n-M 3481 434 \n-Q 3481 -459 3084 -895 \n-Q 2688 -1331 1869 -1331 \n-Q 1566 -1331 1297 -1286 \n-Q 1028 -1241 775 -1147 \n-L 775 -588 \n-Q 1028 -725 1275 -790 \n-Q 1522 -856 1778 -856 \n-Q 2344 -856 2625 -561 \n-Q 2906 -266 2906 331 \n-L 2906 616 \n-Q 2728 306 2450 153 \n-Q 2172 0 1784 0 \n-Q 1141 0 747 490 \n-Q 353 981 353 1791 \n-Q 353 2603 747 3093 \n-Q 1141 3584 1784 3584 \n-Q 2172 3584 2450 3431 \n-Q 2728 3278 2906 2969 \n-L 2906 3500 \n-L 3481 3500 \n-L 3481 434 \n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-65\" d=\"M 3078 2063 \n+Q 3088 2113 3092 2166 \n+Q 3097 2219 3097 2272 \n+Q 3097 2653 2873 2875 \n+Q 2650 3097 2266 3097 \n+Q 1838 3097 1509 2826 \n+Q 1181 2556 1013 2059 \n+L 3078 2063 \n+z\n+M 3578 1613 \n+L 903 1613 \n+Q 884 1494 878 1425 \n+Q 872 1356 872 1306 \n+Q 872 872 1139 634 \n+Q 1406 397 1894 397 \n+Q 2269 397 2603 481 \n+Q 2938 566 3225 728 \n+L 3116 159 \n+Q 2806 34 2476 -28 \n+Q 2147 -91 1806 -91 \n+Q 1078 -91 686 257 \n+Q 294 606 294 1247 \n+Q 294 1794 489 2264 \n+Q 684 2734 1063 3103 \n+Q 1306 3334 1642 3459 \n+Q 1978 3584 2356 3584 \n+Q 2950 3584 3301 3228 \n+Q 3653 2872 3653 2272 \n+Q 3653 2128 3634 1964 \n+Q 3616 1800 3578 1613 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-61\" d=\"M 3438 1997 \n+L 3047 0 \n+L 2472 0 \n+L 2578 531 \n+Q 2325 219 2001 64 \n+Q 1678 -91 1281 -91 \n+Q 834 -91 548 182 \n+Q 263 456 263 884 \n+Q 263 1497 752 1853 \n+Q 1241 2209 2100 2209 \n+L 2900 2209 \n+L 2931 2363 \n+Q 2938 2388 2941 2417 \n+Q 2944 2447 2944 2509 \n+Q 2944 2788 2717 2942 \n+Q 2491 3097 2081 3097 \n+Q 1800 3097 1504 3025 \n+Q 1209 2953 897 2809 \n+L 997 3341 \n+Q 1322 3463 1633 3523 \n+Q 1944 3584 2234 3584 \n+Q 2853 3584 3176 3315 \n+Q 3500 3047 3500 2534 \n+Q 3500 2431 3484 2292 \n+Q 3469 2153 3438 1997 \n+z\n+M 2816 1759 \n+L 2241 1759 \n+Q 1534 1759 1195 1570 \n+Q 856 1381 856 984 \n+Q 856 709 1029 553 \n+Q 1203 397 1509 397 \n+Q 1978 397 2328 733 \n+Q 2678 1069 2791 1631 \n+L 2816 1759 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-6e\" d=\"M 3566 2113 \n+L 3156 0 \n+L 2578 0 \n+L 2988 2091 \n+Q 3016 2238 3031 2350 \n+Q 3047 2463 3047 2528 \n+Q 3047 2791 2881 2937 \n+Q 2716 3084 2419 3084 \n+Q 1956 3084 1622 2776 \n+Q 1288 2469 1184 1941 \n+L 800 0 \n+L 225 0 \n+L 903 3500 \n+L 1478 3500 \n+L 1363 2950 \n+Q 1603 3253 1940 3418 \n+Q 2278 3584 2650 3584 \n+Q 3113 3584 3367 3334 \n+Q 3622 3084 3622 2631 \n+Q 3622 2519 3608 2391 \n+Q 3594 2263 3566 2113 \n z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n-       <use xlink:href=\"#DejaVuSans-45\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"126.5625\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"190.039062\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"217.822266\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"245.605469\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"297.705078\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-4b\" transform=\"translate(0 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6f\" transform=\"translate(65.576172 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-72\" transform=\"translate(126.757812 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(167.871094 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(229.394531 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(290.673828 0.09375)\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"ytick_4\">\n-     <g id=\"line2d_23\">\n+     <g id=\"line2d_24\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"58.263158\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"76.785493\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_10\">\n-      <!-- Portuguese -->\n-      <g transform=\"translate(24.978125 62.062377)scale(0.1 -0.1)\">\n+      <!-- Italian -->\n+      <g transform=\"translate(55.079688 80.584712) scale(0.1 -0.1)\">\n        <defs>\n-        <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n-L 1259 2394 \n-L 2053 2394 \n-Q 2494 2394 2734 2622 \n-Q 2975 2850 2975 3272 \n-Q 2975 3691 2734 3919 \n-Q 2494 4147 2053 4147 \n-L 1259 4147 \n-z\n-M 628 4666 \n-L 2053 4666 \n-Q 2838 4666 3239 4311 \n-Q 3641 3956 3641 3272 \n-Q 3641 2581 3239 2228 \n-Q 2838 1875 2053 1875 \n-L 1259 1875 \n+        <path id=\"DejaVuSans-49\" d=\"M 628 4666 \n+L 1259 4666 \n L 1259 0 \n L 628 0 \n L 628 4666 \n z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n+L 1178 4863 \n+L 1178 0 \n+L 603 0 \n+L 603 4863 \n+z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n-       <use xlink:href=\"#DejaVuSans-50\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"117.859375\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"158.972656\"/>\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"198.181641\"/>\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"261.560547\"/>\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"325.037109\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"388.416016\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"449.939453\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"502.039062\"/>\n+       <use xlink:href=\"#DejaVuSans-49\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"29.492188\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"68.701172\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"129.980469\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"157.763672\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"185.546875\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"246.826172\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"ytick_5\">\n-     <g id=\"line2d_24\">\n+     <g id=\"line2d_25\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"72.852632\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"92.347599\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_11\">\n       <!-- German -->\n-      <g transform=\"translate(41.290625 76.65185)scale(0.1 -0.1)\">\n+      <g transform=\"translate(46.05625 96.146817) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-47\" d=\"M 3809 666 \n L 3809 1919 \n@@ -1423,6 +1130,31 @@ Q 1884 428 2741 428\n Q 3075 428 3337 486 \n Q 3600 544 3809 666 \n z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n+L 3597 1613 \n+L 953 1613 \n+Q 991 1019 1311 708 \n+Q 1631 397 2203 397 \n+Q 2534 397 2845 478 \n+Q 3156 559 3463 722 \n+L 3463 178 \n+Q 3153 47 2828 -22 \n+Q 2503 -91 2169 -91 \n+Q 1331 -91 842 396 \n+Q 353 884 353 1716 \n+Q 353 2575 817 3079 \n+Q 1281 3584 2069 3584 \n+Q 2775 3584 3186 3129 \n+Q 3597 2675 3597 1894 \n+z\n+M 3022 2063 \n+Q 3016 2534 2758 2815 \n+Q 2500 3097 2075 3097 \n+Q 1594 3097 1305 2825 \n+Q 1016 2553 972 2059 \n+L 3022 2063 \n+z\n \" transform=\"scale(0.015625)\"/>\n         <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n Q 3544 3216 3844 3400 \n@@ -1465,67 +1197,79 @@ z\n      </g>\n     </g>\n     <g id=\"ytick_6\">\n-     <g id=\"line2d_25\">\n+     <g id=\"line2d_26\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"87.442105\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"107.909704\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_12\">\n-      <!-- Japanese -->\n-      <g transform=\"translate(35.926563 91.241324)scale(0.1 -0.1)\">\n+      <!-- $Thai$ -->\n+      <g transform=\"translate(64.7 111.708923) scale(0.1 -0.1)\">\n        <defs>\n-        <path id=\"DejaVuSans-4a\" d=\"M 628 4666 \n-L 1259 4666 \n-L 1259 325 \n-Q 1259 -519 939 -900 \n-Q 619 -1281 -91 -1281 \n-L -331 -1281 \n-L -331 -750 \n-L -134 -750 \n-Q 284 -750 456 -515 \n-Q 628 -281 628 325 \n-L 628 4666 \n+        <path id=\"DejaVuSans-Oblique-54\" d=\"M 378 4666 \n+L 4325 4666 \n+L 4225 4134 \n+L 2559 4134 \n+L 1759 0 \n+L 1125 0 \n+L 1925 4134 \n+L 275 4134 \n+L 378 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-68\" d=\"M 3566 2113 \n+L 3156 0 \n+L 2578 0 \n+L 2988 2091 \n+Q 3016 2238 3031 2350 \n+Q 3047 2463 3047 2528 \n+Q 3047 2791 2881 2937 \n+Q 2716 3084 2419 3084 \n+Q 1956 3084 1617 2771 \n+Q 1278 2459 1178 1941 \n+L 800 0 \n+L 225 0 \n+L 1172 4863 \n+L 1747 4863 \n+L 1375 2950 \n+Q 1594 3244 1934 3414 \n+Q 2275 3584 2650 3584 \n+Q 3113 3584 3367 3334 \n+Q 3622 3084 3622 2631 \n+Q 3622 2519 3608 2391 \n+Q 3594 2263 3566 2113 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-69\" d=\"M 1172 4863 \n+L 1747 4863 \n+L 1606 4134 \n+L 1031 4134 \n+L 1172 4863 \n+z\n+M 909 3500 \n+L 1484 3500 \n+L 800 0 \n+L 225 0 \n+L 909 3500 \n z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n-       <use xlink:href=\"#DejaVuSans-4a\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"29.492188\"/>\n-       <use xlink:href=\"#DejaVuSans-70\" x=\"90.771484\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"154.248047\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"215.527344\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"278.90625\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"340.429688\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"392.529297\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-54\" transform=\"translate(0 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-68\" transform=\"translate(61.083984 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(124.462891 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-69\" transform=\"translate(185.742188 0.015625)\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"ytick_7\">\n-     <g id=\"line2d_26\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"102.031579\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_13\">\n-      <!-- Polish -->\n-      <g transform=\"translate(52.445313 105.830798)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-50\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"117.859375\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"145.642578\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"173.425781\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"225.525391\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_8\">\n      <g id=\"line2d_27\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"116.621053\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"123.471809\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_14\">\n+     <g id=\"text_13\">\n       <!-- Russian -->\n-      <g transform=\"translate(42.835938 120.420271)scale(0.1 -0.1)\">\n+      <g transform=\"translate(47.601563 127.271028) scale(0.1 -0.1)\">\n        <use xlink:href=\"#DejaVuSans-52\"/>\n        <use xlink:href=\"#DejaVuSans-75\" x=\"64.982422\"/>\n        <use xlink:href=\"#DejaVuSans-73\" x=\"128.361328\"/>\n@@ -1536,74 +1280,112 @@ z\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_9\">\n+    <g id=\"ytick_8\">\n      <g id=\"line2d_28\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"131.210526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"139.033914\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_15\">\n-      <!-- Dutch -->\n-      <g transform=\"translate(51.540625 135.009745)scale(0.1 -0.1)\">\n+     <g id=\"text_14\">\n+      <!-- Portuguese -->\n+      <g transform=\"translate(29.74375 142.833133) scale(0.1 -0.1)\">\n        <defs>\n-        <path id=\"DejaVuSans-44\" d=\"M 1259 4147 \n-L 1259 519 \n-L 2022 519 \n-Q 2988 519 3436 956 \n-Q 3884 1394 3884 2338 \n-Q 3884 3275 3436 3711 \n-Q 2988 4147 2022 4147 \n+        <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n+L 1259 2394 \n+L 2053 2394 \n+Q 2494 2394 2734 2622 \n+Q 2975 2850 2975 3272 \n+Q 2975 3691 2734 3919 \n+Q 2494 4147 2053 4147 \n L 1259 4147 \n z\n M 628 4666 \n-L 1925 4666 \n-Q 3281 4666 3915 4102 \n-Q 4550 3538 4550 2338 \n-Q 4550 1131 3912 565 \n-Q 3275 0 1925 0 \n+L 2053 4666 \n+Q 2838 4666 3239 4311 \n+Q 3641 3956 3641 3272 \n+Q 3641 2581 3239 2228 \n+Q 2838 1875 2053 1875 \n+L 1259 1875 \n+L 1259 0 \n L 628 0 \n L 628 4666 \n z\n \" transform=\"scale(0.015625)\"/>\n-        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n-L 3122 2828 \n-Q 2878 2963 2633 3030 \n-Q 2388 3097 2138 3097 \n-Q 1578 3097 1268 2742 \n-Q 959 2388 959 1747 \n-Q 959 1106 1268 751 \n-Q 1578 397 2138 397 \n-Q 2388 397 2633 464 \n-Q 2878 531 3122 666 \n-L 3122 134 \n-Q 2881 22 2623 -34 \n-Q 2366 -91 2075 -91 \n-Q 1284 -91 818 406 \n-Q 353 903 353 1747 \n-Q 353 2603 823 3093 \n-Q 1294 3584 2113 3584 \n-Q 2378 3584 2631 3529 \n-Q 2884 3475 3122 3366 \n+        <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n+Q 2906 2416 2648 2759 \n+Q 2391 3103 1925 3103 \n+Q 1463 3103 1205 2759 \n+Q 947 2416 947 1791 \n+Q 947 1169 1205 825 \n+Q 1463 481 1925 481 \n+Q 2391 481 2648 825 \n+Q 2906 1169 2906 1791 \n z\n-\" transform=\"scale(0.015625)\"/>\n-       </defs>\n-       <use xlink:href=\"#DejaVuSans-44\"/>\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"77.001953\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"140.380859\"/>\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"179.589844\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"234.570312\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_10\">\n-     <g id=\"line2d_29\">\n+M 3481 434 \n+Q 3481 -459 3084 -895 \n+Q 2688 -1331 1869 -1331 \n+Q 1566 -1331 1297 -1286 \n+Q 1028 -1241 775 -1147 \n+L 775 -588 \n+Q 1028 -725 1275 -790 \n+Q 1522 -856 1778 -856 \n+Q 2344 -856 2625 -561 \n+Q 2906 -266 2906 331 \n+L 2906 616 \n+Q 2728 306 2450 153 \n+Q 2172 0 1784 0 \n+Q 1141 0 747 490 \n+Q 353 981 353 1791 \n+Q 353 2603 747 3093 \n+Q 1141 3584 1784 3584 \n+Q 2172 3584 2450 3431 \n+Q 2728 3278 2906 2969 \n+L 2906 3500 \n+L 3481 3500 \n+L 3481 434 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-50\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"117.859375\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"158.972656\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"198.181641\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"261.560547\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"325.037109\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"388.416016\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"449.939453\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"502.039062\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_9\">\n+     <g id=\"line2d_29\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"154.59602\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_15\">\n+      <!-- Polish -->\n+      <g transform=\"translate(57.210938 158.395238) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-50\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"117.859375\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"145.642578\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"173.425781\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"225.525391\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_10\">\n+     <g id=\"line2d_30\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"170.158125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_16\">\n       <!-- Indonesian -->\n-      <g transform=\"translate(26.635938 149.599219)scale(0.1 -0.1)\">\n+      <g transform=\"translate(31.401563 173.957344) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n L 2906 4863 \n@@ -1646,14 +1428,141 @@ z\n      </g>\n     </g>\n     <g id=\"ytick_11\">\n-     <g id=\"line2d_30\">\n+     <g id=\"line2d_31\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"160.389474\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"185.72023\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_17\">\n-      <!-- Catalan -->\n-      <g transform=\"translate(42.93125 164.188692)scale(0.1 -0.1)\">\n+      <!-- $Mandarin~(TW)$ -->\n+      <g transform=\"translate(12.2 189.519449) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-Oblique-4d\" d=\"M 1081 4666 \n+L 2028 4666 \n+L 2572 1522 \n+L 4378 4666 \n+L 5350 4666 \n+L 4441 0 \n+L 3828 0 \n+L 4622 4091 \n+L 2791 897 \n+L 2175 897 \n+L 1581 4103 \n+L 788 0 \n+L 172 0 \n+L 1081 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-64\" d=\"M 2675 525 \n+Q 2444 222 2128 65 \n+Q 1813 -91 1428 -91 \n+Q 903 -91 598 267 \n+Q 294 625 294 1247 \n+Q 294 1766 478 2236 \n+Q 663 2706 1013 3078 \n+Q 1244 3325 1534 3454 \n+Q 1825 3584 2144 3584 \n+Q 2481 3584 2739 3421 \n+Q 2997 3259 3138 2956 \n+L 3513 4863 \n+L 4091 4863 \n+L 3144 0 \n+L 2566 0 \n+L 2675 525 \n+z\n+M 891 1350 \n+Q 891 897 1095 644 \n+Q 1300 391 1663 391 \n+Q 1931 391 2161 520 \n+Q 2391 650 2566 903 \n+Q 2750 1166 2856 1509 \n+Q 2963 1853 2963 2188 \n+Q 2963 2622 2758 2865 \n+Q 2553 3109 2194 3109 \n+Q 1922 3109 1687 2981 \n+Q 1453 2853 1288 2613 \n+Q 1106 2353 998 2009 \n+Q 891 1666 891 1350 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-57\" d=\"M 616 4666 \n+L 1228 4666 \n+L 1453 697 \n+L 3213 4666 \n+L 3916 4666 \n+L 4147 697 \n+L 5888 4666 \n+L 6528 4666 \n+L 4453 0 \n+L 3659 0 \n+L 3444 3891 \n+L 1697 0 \n+L 903 0 \n+L 616 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-Oblique-4d\" transform=\"translate(0 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(86.279297 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(147.558594 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-64\" transform=\"translate(210.9375 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(274.414062 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-72\" transform=\"translate(335.693359 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-69\" transform=\"translate(376.806641 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(404.589844 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(500.439128 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-54\" transform=\"translate(539.4528 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-57\" transform=\"translate(600.536785 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(699.413738 0.015625)\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_12\">\n+     <g id=\"line2d_32\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"201.282336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_18\">\n+      <!-- Swedish -->\n+      <g transform=\"translate(44.746875 205.081554) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-77\" d=\"M 269 3500 \n+L 844 3500 \n+L 1563 769 \n+L 2278 3500 \n+L 2956 3500 \n+L 3675 769 \n+L 4391 3500 \n+L 4966 3500 \n+L 4050 0 \n+L 3372 0 \n+L 2619 2869 \n+L 1863 0 \n+L 1184 0 \n+L 269 3500 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-77\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"145.263672\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"206.787109\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"270.263672\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"298.046875\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"350.146484\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_13\">\n+     <g id=\"line2d_33\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"216.844441\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_19\">\n+      <!-- Czech -->\n+      <g transform=\"translate(55.879688 220.64366) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \n L 4122 3641 \n@@ -1675,27 +1584,170 @@ Q 1578 4750 2638 4750\n Q 3056 4750 3426 4639 \n Q 3797 4528 4122 4306 \n z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-7a\" d=\"M 353 3500 \n+L 3084 3500 \n+L 3084 2975 \n+L 922 459 \n+L 3084 459 \n+L 3084 0 \n+L 275 0 \n+L 275 525 \n+L 2438 3041 \n+L 353 3041 \n+L 353 3500 \n+z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n        <use xlink:href=\"#DejaVuSans-43\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"69.824219\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"131.103516\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"170.3125\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"231.591797\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"259.375\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"320.654297\"/>\n+       <use xlink:href=\"#DejaVuSans-7a\" x=\"69.824219\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"122.314453\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"183.837891\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"238.818359\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_12\">\n-     <g id=\"line2d_31\">\n+    <g id=\"ytick_14\">\n+     <g id=\"line2d_34\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"174.978947\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"232.406546\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_18\">\n+     <g id=\"text_20\">\n+      <!-- English -->\n+      <g transform=\"translate(49.992188 236.205765) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-45\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"126.5625\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"190.039062\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"217.822266\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"245.605469\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"297.705078\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_15\">\n+     <g id=\"line2d_35\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"247.968651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_21\">\n+      <!-- $Japanese$ -->\n+      <g transform=\"translate(40.6 251.757714) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-Oblique-4a\" d=\"M 1069 4666 \n+L 1703 4666 \n+L 856 325 \n+Q 691 -522 298 -901 \n+Q -94 -1281 -800 -1281 \n+L -1050 -1281 \n+L -947 -750 \n+L -750 -750 \n+Q -328 -750 -109 -509 \n+Q 109 -269 225 325 \n+L 1069 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-70\" d=\"M 3175 2156 \n+Q 3175 2616 2975 2859 \n+Q 2775 3103 2400 3103 \n+Q 2144 3103 1911 2972 \n+Q 1678 2841 1497 2591 \n+Q 1319 2344 1212 1994 \n+Q 1106 1644 1106 1300 \n+Q 1106 863 1306 627 \n+Q 1506 391 1875 391 \n+Q 2147 391 2380 519 \n+Q 2613 647 2778 891 \n+Q 2956 1147 3065 1494 \n+Q 3175 1841 3175 2156 \n+z\n+M 1394 2969 \n+Q 1625 3272 1939 3428 \n+Q 2253 3584 2638 3584 \n+Q 3175 3584 3472 3232 \n+Q 3769 2881 3769 2247 \n+Q 3769 1728 3584 1258 \n+Q 3400 788 3053 416 \n+Q 2822 169 2531 39 \n+Q 2241 -91 1919 -91 \n+Q 1547 -91 1294 64 \n+Q 1041 219 916 525 \n+L 556 -1331 \n+L -19 -1331 \n+L 922 3500 \n+L 1497 3500 \n+L 1394 2969 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-73\" d=\"M 3200 3397 \n+L 3091 2853 \n+Q 2863 2978 2609 3040 \n+Q 2356 3103 2088 3103 \n+Q 1634 3103 1373 2948 \n+Q 1113 2794 1113 2528 \n+Q 1113 2219 1719 2053 \n+Q 1766 2041 1788 2034 \n+L 1972 1978 \n+Q 2547 1819 2739 1644 \n+Q 2931 1469 2931 1166 \n+Q 2931 609 2489 259 \n+Q 2047 -91 1331 -91 \n+Q 1053 -91 747 -37 \n+Q 441 16 72 128 \n+L 184 722 \n+Q 500 559 806 475 \n+Q 1113 391 1394 391 \n+Q 1816 391 2080 572 \n+Q 2344 753 2344 1031 \n+Q 2344 1331 1650 1516 \n+L 1591 1531 \n+L 1394 1581 \n+Q 956 1697 753 1886 \n+Q 550 2075 550 2369 \n+Q 550 2928 970 3256 \n+Q 1391 3584 2113 3584 \n+Q 2397 3584 2667 3537 \n+Q 2938 3491 3200 3397 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-Oblique-4a\" transform=\"translate(0 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(29.492188 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-70\" transform=\"translate(90.771484 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(154.248047 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(215.527344 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(278.90625 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-73\" transform=\"translate(340.429688 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(392.529297 0.09375)\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_16\">\n+     <g id=\"line2d_36\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"263.530757\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_22\">\n       <!-- French -->\n-      <g transform=\"translate(48.095313 178.778166)scale(0.1 -0.1)\">\n+      <g transform=\"translate(52.860938 267.329975) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n+L 3309 4666 \n+L 3309 4134 \n+L 1259 4134 \n+L 1259 2759 \n+L 3109 2759 \n+L 3109 2228 \n+L 1259 2228 \n+L 1259 0 \n+L 628 0 \n+L 628 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n        <use xlink:href=\"#DejaVuSans-46\"/>\n        <use xlink:href=\"#DejaVuSans-72\" x=\"50.269531\"/>\n        <use xlink:href=\"#DejaVuSans-65\" x=\"89.132812\"/>\n@@ -1705,15 +1757,100 @@ z\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_13\">\n-     <g id=\"line2d_32\">\n+    <g id=\"ytick_17\">\n+     <g id=\"line2d_37\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"189.568421\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"279.092862\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_19\">\n+     <g id=\"text_23\">\n+      <!-- Romanian -->\n+      <g transform=\"translate(36.032813 282.892081) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-52\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"64.982422\"/>\n+       <use xlink:href=\"#DejaVuSans-6d\" x=\"126.164062\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"223.576172\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"284.855469\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"348.234375\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"376.017578\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"437.296875\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_18\">\n+     <g id=\"line2d_38\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"294.654967\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_24\">\n+      <!-- $Cantonese~(CN)$ -->\n+      <g transform=\"translate(7.2 298.454186) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-Oblique-74\" d=\"M 2706 3500 \n+L 2619 3053 \n+L 1472 3053 \n+L 1100 1153 \n+Q 1081 1047 1072 975 \n+Q 1063 903 1063 863 \n+Q 1063 663 1183 572 \n+Q 1303 481 1569 481 \n+L 2150 481 \n+L 2053 0 \n+L 1503 0 \n+Q 991 0 739 200 \n+Q 488 400 488 806 \n+Q 488 878 497 964 \n+Q 506 1050 525 1153 \n+L 897 3053 \n+L 409 3053 \n+L 500 3500 \n+L 978 3500 \n+L 1172 4494 \n+L 1747 4494 \n+L 1556 3500 \n+L 2706 3500 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-Oblique-4e\" d=\"M 1081 4666 \n+L 1931 4666 \n+L 3219 666 \n+L 4000 4666 \n+L 4616 4666 \n+L 3706 0 \n+L 2853 0 \n+L 1569 4025 \n+L 788 0 \n+L 172 0 \n+L 1081 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-Oblique-43\" transform=\"translate(0 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(69.824219 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(131.103516 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-74\" transform=\"translate(194.482422 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6f\" transform=\"translate(233.691406 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(294.873047 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(358.251953 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-73\" transform=\"translate(419.775391 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(471.875 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(565.868816 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-43\" transform=\"translate(604.882488 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-4e\" transform=\"translate(674.706707 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(749.511394 0.125)\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_19\">\n+     <g id=\"line2d_39\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"310.217072\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_25\">\n       <!-- Turkish -->\n-      <g transform=\"translate(46.175 193.36764)scale(0.1 -0.1)\">\n+      <g transform=\"translate(50.940625 314.016291) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n L 3928 4666 \n@@ -1751,52 +1888,96 @@ z\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_14\">\n-     <g id=\"line2d_33\">\n+    <g id=\"ytick_20\">\n+     <g id=\"line2d_40\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"204.157895\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"325.779178\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_20\">\n-      <!-- Swedish -->\n-      <g transform=\"translate(39.98125 207.957113)scale(0.1 -0.1)\">\n-       <defs>\n-        <path id=\"DejaVuSans-77\" d=\"M 269 3500 \n-L 844 3500 \n-L 1563 769 \n-L 2278 3500 \n-L 2956 3500 \n-L 3675 769 \n-L 4391 3500 \n-L 4966 3500 \n-L 4050 0 \n-L 3372 0 \n-L 2619 2869 \n-L 1863 0 \n-L 1184 0 \n-L 269 3500 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       </defs>\n-       <use xlink:href=\"#DejaVuSans-53\"/>\n-       <use xlink:href=\"#DejaVuSans-77\" x=\"63.476562\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"145.263672\"/>\n-       <use xlink:href=\"#DejaVuSans-64\" x=\"206.787109\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"270.263672\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"298.046875\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"350.146484\"/>\n+     <g id=\"text_26\">\n+      <!-- $Mandarin~(CN)$ -->\n+      <g transform=\"translate(13.7 329.578396) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-Oblique-4d\" transform=\"translate(0 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(86.279297 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(147.558594 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-64\" transform=\"translate(210.9375 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(274.414062 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-72\" transform=\"translate(335.693359 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-69\" transform=\"translate(376.806641 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(404.589844 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(500.439128 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-43\" transform=\"translate(539.4528 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-4e\" transform=\"translate(609.277019 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(684.081707 0.015625)\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_15\">\n-     <g id=\"line2d_34\">\n+    <g id=\"ytick_21\">\n+     <g id=\"line2d_41\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"218.747368\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"341.341283\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_21\">\n+     <g id=\"text_27\">\n+      <!-- Catalan -->\n+      <g transform=\"translate(47.696875 345.140502) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-43\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"69.824219\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"131.103516\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"170.3125\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"231.591797\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"259.375\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"320.654297\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_22\">\n+     <g id=\"line2d_42\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"356.903388\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_28\">\n+      <!-- Hungarian -->\n+      <g transform=\"translate(34.073438 360.702607) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-48\" d=\"M 628 4666 \n+L 1259 4666 \n+L 1259 2753 \n+L 3553 2753 \n+L 3553 4666 \n+L 4184 4666 \n+L 4184 0 \n+L 3553 0 \n+L 3553 2222 \n+L 1259 2222 \n+L 1259 0 \n+L 628 0 \n+L 628 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-48\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"75.195312\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"138.574219\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"201.953125\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"265.429688\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"326.708984\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"367.822266\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"395.605469\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"456.884766\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_23\">\n+     <g id=\"line2d_43\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"372.465493\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_29\">\n       <!-- Ukrainian -->\n-      <g transform=\"translate(33.626563 222.546587)scale(0.1 -0.1)\">\n+      <g transform=\"translate(38.392188 376.264712) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-55\" d=\"M 556 4666 \n L 1191 4666 \n@@ -1828,15 +2009,168 @@ z\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_16\">\n-     <g id=\"line2d_35\">\n+    <g id=\"ytick_24\">\n+     <g id=\"line2d_44\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"233.336842\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"388.027599\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_22\">\n-      <!-- Malay -->\n-      <g transform=\"translate(51.753125 237.136061)scale(0.1 -0.1)\">\n+     <g id=\"text_30\">\n+      <!-- Greek -->\n+      <g transform=\"translate(56.36875 391.826817) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-47\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"77.490234\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"116.353516\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"177.876953\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"239.400391\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_25\">\n+     <g id=\"line2d_45\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"403.589704\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_31\">\n+      <!-- Bulgarian -->\n+      <g transform=\"translate(38.292188 407.388923) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-42\" d=\"M 1259 2228 \n+L 1259 519 \n+L 2272 519 \n+Q 2781 519 3026 730 \n+Q 3272 941 3272 1375 \n+Q 3272 1813 3026 2020 \n+Q 2781 2228 2272 2228 \n+L 1259 2228 \n+z\n+M 1259 4147 \n+L 1259 2741 \n+L 2194 2741 \n+Q 2656 2741 2882 2914 \n+Q 3109 3088 3109 3444 \n+Q 3109 3797 2882 3972 \n+Q 2656 4147 2194 4147 \n+L 1259 4147 \n+z\n+M 628 4666 \n+L 2241 4666 \n+Q 2963 4666 3353 4366 \n+Q 3744 4066 3744 3513 \n+Q 3744 3084 3544 2831 \n+Q 3344 2578 2956 2516 \n+Q 3422 2416 3680 2098 \n+Q 3938 1781 3938 1306 \n+Q 3938 681 3513 340 \n+Q 3088 0 2303 0 \n+L 628 0 \n+L 628 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-42\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"68.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"131.982422\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"159.765625\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"223.242188\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"284.521484\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"325.634766\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"353.417969\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"414.697266\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_26\">\n+     <g id=\"line2d_46\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"419.151809\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_32\">\n+      <!-- Arabic -->\n+      <g transform=\"translate(54.395313 422.951028) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-41\" d=\"M 2188 4044 \n+L 1331 1722 \n+L 3047 1722 \n+L 2188 4044 \n+z\n+M 1831 4666 \n+L 2547 4666 \n+L 4325 0 \n+L 3669 0 \n+L 3244 1197 \n+L 1141 1197 \n+L 716 0 \n+L 50 0 \n+L 1831 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+        <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n+Q 3116 2381 2855 2742 \n+Q 2594 3103 2138 3103 \n+Q 1681 3103 1420 2742 \n+Q 1159 2381 1159 1747 \n+Q 1159 1113 1420 752 \n+Q 1681 391 2138 391 \n+Q 2594 391 2855 752 \n+Q 3116 1113 3116 1747 \n+z\n+M 1159 2969 \n+Q 1341 3281 1617 3432 \n+Q 1894 3584 2278 3584 \n+Q 2916 3584 3314 3078 \n+Q 3713 2572 3713 1747 \n+Q 3713 922 3314 415 \n+Q 2916 -91 2278 -91 \n+Q 1894 -91 1617 61 \n+Q 1341 213 1159 525 \n+L 1159 0 \n+L 581 0 \n+L 581 4863 \n+L 1159 4863 \n+L 1159 2969 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-41\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"68.408203\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"109.521484\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"170.800781\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"234.277344\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"262.060547\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_27\">\n+     <g id=\"line2d_47\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"434.713914\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_33\">\n+      <!-- Serbian -->\n+      <g transform=\"translate(47.895313 438.513133) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"125\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"166.113281\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"229.589844\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"257.373047\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"318.652344\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_28\">\n+     <g id=\"line2d_48\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"450.27602\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_34\">\n+      <!-- Macedonian -->\n+      <g transform=\"translate(25.64375 454.075238) scale(0.1 -0.1)\">\n        <defs>\n         <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \n L 1569 4666 \n@@ -1853,148 +2187,82 @@ L 1241 0\n L 628 0 \n L 628 4666 \n z\n-\" transform=\"scale(0.015625)\"/>\n-        <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n-Q 1816 -950 1584 -1140 \n-Q 1353 -1331 966 -1331 \n-L 506 -1331 \n-L 506 -850 \n-L 844 -850 \n-Q 1081 -850 1212 -737 \n-Q 1344 -625 1503 -206 \n-L 1606 56 \n-L 191 3500 \n-L 800 3500 \n-L 1894 763 \n-L 2988 3500 \n-L 3597 3500 \n-L 2059 -325 \n-z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n        <use xlink:href=\"#DejaVuSans-4d\"/>\n        <use xlink:href=\"#DejaVuSans-61\" x=\"86.279297\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"147.558594\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"175.341797\"/>\n-       <use xlink:href=\"#DejaVuSans-79\" x=\"236.621094\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"147.558594\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"202.539062\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"264.0625\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"327.539062\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"388.720703\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"452.099609\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"479.882812\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"541.162109\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_17\">\n-     <g id=\"line2d_36\">\n+    <g id=\"ytick_29\">\n+     <g id=\"line2d_49\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"247.926316\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"465.838125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_23\">\n-      <!-- Norwegian -->\n-      <g transform=\"translate(27.7 251.725535)scale(0.1 -0.1)\">\n+     <g id=\"text_35\">\n+      <!-- $Cantonese~(HK)$ -->\n+      <g transform=\"translate(7.6 469.637344) scale(0.1 -0.1)\">\n        <defs>\n-        <path id=\"DejaVuSans-4e\" d=\"M 628 4666 \n-L 1478 4666 \n-L 3547 763 \n-L 3547 4666 \n-L 4159 4666 \n-L 4159 0 \n-L 3309 0 \n-L 1241 3903 \n-L 1241 0 \n-L 628 0 \n-L 628 4666 \n+        <path id=\"DejaVuSans-Oblique-48\" d=\"M 1081 4666 \n+L 1716 4666 \n+L 1344 2753 \n+L 3634 2753 \n+L 4006 4666 \n+L 4641 4666 \n+L 3731 0 \n+L 3097 0 \n+L 3531 2222 \n+L 1241 2222 \n+L 806 0 \n+L 172 0 \n+L 1081 4666 \n z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n-       <use xlink:href=\"#DejaVuSans-4e\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"74.804688\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"135.986328\"/>\n-       <use xlink:href=\"#DejaVuSans-77\" x=\"177.099609\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"258.886719\"/>\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"320.410156\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"383.886719\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"411.669922\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"472.949219\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_18\">\n-     <g id=\"line2d_37\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"262.515789\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_24\">\n-      <!-- Finnish -->\n-      <g transform=\"translate(46.529688 266.315008)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-46\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"50.269531\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"78.052734\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"141.431641\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"204.810547\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"232.59375\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"284.693359\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-43\" transform=\"translate(0 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(69.824219 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(131.103516 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-74\" transform=\"translate(194.482422 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6f\" transform=\"translate(233.691406 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(294.873047 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(358.251953 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-73\" transform=\"translate(419.775391 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(471.875 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(565.868816 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-48\" transform=\"translate(604.882488 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-4b\" transform=\"translate(680.0778 0.125)\"/>\n+       <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(745.653972 0.125)\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_19\">\n-     <g id=\"line2d_38\">\n+    <g id=\"ytick_30\">\n+     <g id=\"line2d_50\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"277.105263\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"481.40023\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_25\">\n-      <!-- Vietnamese -->\n-      <g transform=\"translate(22.145313 280.904482)scale(0.1 -0.1)\">\n+     <g id=\"text_36\">\n+      <!-- Latvian -->\n+      <g transform=\"translate(49.317188 485.199449) scale(0.1 -0.1)\">\n        <defs>\n-        <path id=\"DejaVuSans-56\" d=\"M 1831 0 \n-L 50 4666 \n-L 709 4666 \n-L 2188 738 \n-L 3669 4666 \n-L 4325 4666 \n-L 2547 0 \n-L 1831 0 \n+        <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \n+L 1259 4666 \n+L 1259 531 \n+L 3531 531 \n+L 3531 0 \n+L 628 0 \n+L 628 4666 \n z\n \" transform=\"scale(0.015625)\"/>\n-       </defs>\n-       <use xlink:href=\"#DejaVuSans-56\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"66.158203\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"93.941406\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"155.464844\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"194.673828\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"258.052734\"/>\n-       <use xlink:href=\"#DejaVuSans-6d\" x=\"319.332031\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"416.744141\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"478.267578\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"530.367188\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_20\">\n-     <g id=\"line2d_39\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"291.694737\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_26\">\n-      <!-- Thai -->\n-      <g transform=\"translate(59.982813 295.493956)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-54\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"61.083984\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"124.462891\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"185.742188\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_21\">\n-     <g id=\"line2d_40\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"306.284211\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_27\">\n-      <!-- Slovak -->\n-      <g transform=\"translate(48.251563 310.083429)scale(0.1 -0.1)\">\n-       <defs>\n         <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n L 800 3500 \n L 1894 563 \n@@ -2006,112 +2274,83 @@ L 191 3500\n z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n-       <use xlink:href=\"#DejaVuSans-53\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"63.476562\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"91.259766\"/>\n-       <use xlink:href=\"#DejaVuSans-76\" x=\"152.441406\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"211.621094\"/>\n-       <use xlink:href=\"#DejaVuSans-6b\" x=\"272.900391\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_22\">\n-     <g id=\"line2d_41\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"320.873684\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_28\">\n-      <!-- Greek -->\n-      <g transform=\"translate(51.603125 324.672903)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-47\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"77.490234\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"116.353516\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"177.876953\"/>\n-       <use xlink:href=\"#DejaVuSans-6b\" x=\"239.400391\"/>\n+       <use xlink:href=\"#DejaVuSans-4c\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"55.712891\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"116.992188\"/>\n+       <use xlink:href=\"#DejaVuSans-76\" x=\"156.201172\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"215.380859\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"243.164062\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"304.443359\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_23\">\n-     <g id=\"line2d_42\">\n+    <g id=\"ytick_31\">\n+     <g id=\"line2d_51\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"335.463158\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"496.962336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_29\">\n-      <!-- Czech -->\n-      <g transform=\"translate(51.114063 339.262377)scale(0.1 -0.1)\">\n-       <defs>\n-        <path id=\"DejaVuSans-7a\" d=\"M 353 3500 \n-L 3084 3500 \n-L 3084 2975 \n-L 922 459 \n-L 3084 459 \n-L 3084 0 \n-L 275 0 \n-L 275 525 \n-L 2438 3041 \n-L 353 3041 \n-L 353 3500 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       </defs>\n-       <use xlink:href=\"#DejaVuSans-43\"/>\n-       <use xlink:href=\"#DejaVuSans-7a\" x=\"69.824219\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"122.314453\"/>\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"183.837891\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"238.818359\"/>\n+     <g id=\"text_37\">\n+      <!-- Slovenian -->\n+      <g transform=\"translate(37.201563 500.761554) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"91.259766\"/>\n+       <use xlink:href=\"#DejaVuSans-76\" x=\"152.441406\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"211.621094\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"273.144531\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"336.523438\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"364.306641\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"425.585938\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_24\">\n-     <g id=\"line2d_43\">\n+    <g id=\"ytick_32\">\n+     <g id=\"line2d_52\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"350.052632\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"512.524441\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_30\">\n-      <!-- Croatian -->\n-      <g transform=\"translate(39.054688 353.85185)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-43\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"69.824219\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"108.6875\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"169.869141\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"231.148438\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"270.357422\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"298.140625\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"359.419922\"/>\n+     <g id=\"text_38\">\n+      <!-- Hindi -->\n+      <g transform=\"translate(60.3375 516.32366) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-48\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"75.195312\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"102.978516\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"166.357422\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"229.833984\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_25\">\n-     <g id=\"line2d_44\">\n+    <g id=\"ytick_33\">\n+     <g id=\"line2d_53\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"364.642105\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"528.086546\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_31\">\n-      <!-- Tagalog -->\n-      <g transform=\"translate(43.026563 368.441324)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-54\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"105.863281\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"169.339844\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"230.619141\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"258.402344\"/>\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"319.583984\"/>\n+     <g id=\"text_39\">\n+      <!-- Galician -->\n+      <g transform=\"translate(45.925 531.885765) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-47\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"77.490234\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"138.769531\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"166.552734\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"194.335938\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"249.316406\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"277.099609\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"338.378906\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_26\">\n-     <g id=\"line2d_45\">\n+    <g id=\"ytick_34\">\n+     <g id=\"line2d_54\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"379.231579\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"543.648651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_32\">\n+     <g id=\"text_40\">\n       <!-- Danish -->\n-      <g transform=\"translate(46.84375 383.030798)scale(0.1 -0.1)\">\n+      <g transform=\"translate(51.609375 547.44787) scale(0.1 -0.1)\">\n        <use xlink:href=\"#DejaVuSans-44\"/>\n        <use xlink:href=\"#DejaVuSans-61\" x=\"77.001953\"/>\n        <use xlink:href=\"#DejaVuSans-6e\" x=\"138.28125\"/>\n@@ -2121,627 +2360,496 @@ z\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_27\">\n-     <g id=\"line2d_46\">\n+    <g id=\"ytick_35\">\n+     <g id=\"line2d_55\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"393.821053\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"559.210757\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_33\">\n-      <!-- Korean -->\n-      <g transform=\"translate(46.653125 397.620271)scale(0.1 -0.1)\">\n-       <defs>\n-        <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \n-L 1259 4666 \n-L 1259 2694 \n-L 3353 4666 \n-L 4166 4666 \n-L 1850 2491 \n-L 4331 0 \n-L 3500 0 \n-L 1259 2247 \n-L 1259 0 \n-L 628 0 \n-L 628 4666 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       </defs>\n-       <use xlink:href=\"#DejaVuSans-4b\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"60.576172\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"121.757812\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"160.621094\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"222.144531\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"283.423828\"/>\n+     <g id=\"text_41\">\n+      <!-- Urdu -->\n+      <g transform=\"translate(62.159375 563.009975) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-55\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"73.193359\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"112.556641\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"176.033203\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_28\">\n-     <g id=\"line2d_47\">\n+    <g id=\"ytick_36\">\n+     <g id=\"line2d_56\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"408.410526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"574.772862\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_34\">\n-      <!-- Romanian -->\n-      <g transform=\"translate(31.267188 412.209745)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-52\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"64.982422\"/>\n-       <use xlink:href=\"#DejaVuSans-6d\" x=\"126.164062\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"223.576172\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"284.855469\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"348.234375\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"376.017578\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"437.296875\"/>\n+     <g id=\"text_42\">\n+      <!-- Slovak -->\n+      <g transform=\"translate(53.017188 578.572081) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"91.259766\"/>\n+       <use xlink:href=\"#DejaVuSans-76\" x=\"152.441406\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"211.621094\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"272.900391\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_29\">\n-     <g id=\"line2d_48\">\n+    <g id=\"ytick_37\">\n+     <g id=\"line2d_57\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"423\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"590.334967\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_35\">\n-      <!-- Bulgarian -->\n-      <g transform=\"translate(33.526563 426.799219)scale(0.1 -0.1)\">\n-       <defs>\n-        <path id=\"DejaVuSans-42\" d=\"M 1259 2228 \n-L 1259 519 \n-L 2272 519 \n-Q 2781 519 3026 730 \n-Q 3272 941 3272 1375 \n-Q 3272 1813 3026 2020 \n-Q 2781 2228 2272 2228 \n-L 1259 2228 \n-z\n-M 1259 4147 \n-L 1259 2741 \n-L 2194 2741 \n-Q 2656 2741 2882 2914 \n-Q 3109 3088 3109 3444 \n-Q 3109 3797 2882 3972 \n-Q 2656 4147 2194 4147 \n-L 1259 4147 \n-z\n-M 628 4666 \n-L 2241 4666 \n-Q 2963 4666 3353 4366 \n-Q 3744 4066 3744 3513 \n-Q 3744 3084 3544 2831 \n-Q 3344 2578 2956 2516 \n-Q 3422 2416 3680 2098 \n-Q 3938 1781 3938 1306 \n-Q 3938 681 3513 340 \n-Q 3088 0 2303 0 \n-L 628 0 \n-L 628 4666 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       </defs>\n-       <use xlink:href=\"#DejaVuSans-42\"/>\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"68.603516\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"131.982422\"/>\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"159.765625\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"223.242188\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"284.521484\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"325.634766\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"353.417969\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"414.697266\"/>\n+     <g id=\"text_43\">\n+      <!-- Hebrew -->\n+      <g transform=\"translate(47.860938 594.134186) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-48\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"75.195312\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"136.71875\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"200.195312\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"239.058594\"/>\n+       <use xlink:href=\"#DejaVuSans-77\" x=\"300.582031\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_30\">\n-     <g id=\"line2d_49\">\n+    <g id=\"ytick_38\">\n+     <g id=\"line2d_58\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"437.589474\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"605.897072\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_36\">\n-      <!-- Chinese -->\n-      <g transform=\"translate(41.382813 441.388692)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-43\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"69.824219\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"133.203125\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"160.986328\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"224.365234\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"285.888672\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"337.988281\"/>\n+     <g id=\"text_44\">\n+      <!-- Finnish -->\n+      <g transform=\"translate(51.295313 609.696291) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-46\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"50.269531\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"78.052734\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"141.431641\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"204.810547\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"232.59375\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"284.693359\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_31\">\n-     <g id=\"line2d_50\">\n+    <g id=\"ytick_39\">\n+     <g id=\"line2d_59\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"452.178947\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"621.459178\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_37\">\n-      <!-- Galician -->\n-      <g transform=\"translate(41.159375 455.978166)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-47\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"77.490234\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"138.769531\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"166.552734\"/>\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"194.335938\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"249.316406\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"277.099609\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"338.378906\"/>\n+     <g id=\"text_45\">\n+      <!-- Azerbaijani -->\n+      <g transform=\"translate(30.470313 625.258396) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-6a\" d=\"M 603 3500 \n+L 1178 3500 \n+L 1178 -63 \n+Q 1178 -731 923 -1031 \n+Q 669 -1331 103 -1331 \n+L -116 -1331 \n+L -116 -844 \n+L 38 -844 \n+Q 366 -844 484 -692 \n+Q 603 -541 603 -63 \n+L 603 3500 \n+z\n+M 603 4863 \n+L 1178 4863 \n+L 1178 4134 \n+L 603 4134 \n+L 603 4863 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-41\"/>\n+       <use xlink:href=\"#DejaVuSans-7a\" x=\"68.408203\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"120.898438\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"182.421875\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"223.535156\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"287.011719\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"348.291016\"/>\n+       <use xlink:href=\"#DejaVuSans-6a\" x=\"376.074219\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"403.857422\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"465.136719\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"528.515625\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_32\">\n-     <g id=\"line2d_51\">\n+    <g id=\"ytick_40\">\n+     <g id=\"line2d_60\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"466.768421\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"637.021283\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_38\">\n-      <!-- Bosnian -->\n-      <g transform=\"translate(41.564063 470.56764)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-42\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"68.603516\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"129.785156\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"181.884766\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"245.263672\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"273.046875\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"334.326172\"/>\n+     <g id=\"text_46\">\n+      <!-- Lithuanian -->\n+      <g transform=\"translate(33.445313 640.820502) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4c\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"55.712891\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"83.496094\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"122.705078\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"186.083984\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"249.462891\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"310.742188\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"374.121094\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"401.904297\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"463.183594\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_33\">\n-     <g id=\"line2d_52\">\n+    <g id=\"ytick_41\">\n+     <g id=\"line2d_61\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"481.357895\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"652.583388\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_39\">\n-      <!-- Arabic -->\n-      <g transform=\"translate(49.629688 485.157113)scale(0.1 -0.1)\">\n+     <g id=\"text_47\">\n+      <!-- Estonian -->\n+      <g transform=\"translate(42.951563 656.382607) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-45\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"63.183594\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"115.283203\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"154.492188\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"215.673828\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"279.052734\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"306.835938\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"368.115234\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_42\">\n+     <g id=\"line2d_62\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"668.145493\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_48\">\n+      <!-- Nynorsk -->\n+      <g transform=\"translate(45.132813 671.944712) scale(0.1 -0.1)\">\n        <defs>\n-        <path id=\"DejaVuSans-41\" d=\"M 2188 4044 \n-L 1331 1722 \n-L 3047 1722 \n-L 2188 4044 \n-z\n-M 1831 4666 \n-L 2547 4666 \n-L 4325 0 \n-L 3669 0 \n-L 3244 1197 \n-L 1141 1197 \n-L 716 0 \n-L 50 0 \n-L 1831 4666 \n+        <path id=\"DejaVuSans-4e\" d=\"M 628 4666 \n+L 1478 4666 \n+L 3547 763 \n+L 3547 4666 \n+L 4159 4666 \n+L 4159 0 \n+L 3309 0 \n+L 1241 3903 \n+L 1241 0 \n+L 628 0 \n+L 628 4666 \n z\n \" transform=\"scale(0.015625)\"/>\n-        <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n-Q 3116 2381 2855 2742 \n-Q 2594 3103 2138 3103 \n-Q 1681 3103 1420 2742 \n-Q 1159 2381 1159 1747 \n-Q 1159 1113 1420 752 \n-Q 1681 391 2138 391 \n-Q 2594 391 2855 752 \n-Q 3116 1113 3116 1747 \n-z\n-M 1159 2969 \n-Q 1341 3281 1617 3432 \n-Q 1894 3584 2278 3584 \n-Q 2916 3584 3314 3078 \n-Q 3713 2572 3713 1747 \n-Q 3713 922 3314 415 \n-Q 2916 -91 2278 -91 \n-Q 1894 -91 1617 61 \n-Q 1341 213 1159 525 \n-L 1159 0 \n-L 581 0 \n-L 581 4863 \n-L 1159 4863 \n-L 1159 2969 \n+        <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n+Q 1816 -950 1584 -1140 \n+Q 1353 -1331 966 -1331 \n+L 506 -1331 \n+L 506 -850 \n+L 844 -850 \n+Q 1081 -850 1212 -737 \n+Q 1344 -625 1503 -206 \n+L 1606 56 \n+L 191 3500 \n+L 800 3500 \n+L 1894 763 \n+L 2988 3500 \n+L 3597 3500 \n+L 2059 -325 \n z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n-       <use xlink:href=\"#DejaVuSans-41\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"68.408203\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"109.521484\"/>\n-       <use xlink:href=\"#DejaVuSans-62\" x=\"170.800781\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"234.277344\"/>\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"262.060547\"/>\n+       <use xlink:href=\"#DejaVuSans-4e\"/>\n+       <use xlink:href=\"#DejaVuSans-79\" x=\"74.804688\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"133.984375\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"197.363281\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"258.544922\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"299.658203\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"351.757812\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_34\">\n-     <g id=\"line2d_53\">\n+    <g id=\"ytick_43\">\n+     <g id=\"line2d_63\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"495.947368\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"683.707599\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_40\">\n-      <!-- Macedonian -->\n-      <g transform=\"translate(20.878125 499.746587)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-4d\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"86.279297\"/>\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"147.558594\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"202.539062\"/>\n-       <use xlink:href=\"#DejaVuSans-64\" x=\"264.0625\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"327.539062\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"388.720703\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"452.099609\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"479.882812\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"541.162109\"/>\n+     <g id=\"text_49\">\n+      <!-- Welsh -->\n+      <g transform=\"translate(56.321875 687.506817) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-57\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"93.001953\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"154.525391\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"182.308594\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"234.408203\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_35\">\n-     <g id=\"line2d_54\">\n+    <g id=\"ytick_44\">\n+     <g id=\"line2d_64\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"510.536842\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"699.269704\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_41\">\n-      <!-- Hungarian -->\n-      <g transform=\"translate(29.307813 514.336061)scale(0.1 -0.1)\">\n+     <g id=\"text_50\">\n+      <!-- Punjabi -->\n+      <g transform=\"translate(49.5375 703.068923) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-50\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"58.552734\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"121.931641\"/>\n+       <use xlink:href=\"#DejaVuSans-6a\" x=\"185.310547\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"213.09375\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"274.373047\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"337.849609\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_45\">\n+     <g id=\"line2d_65\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"714.831809\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_51\">\n+      <!-- Afrikaans -->\n+      <g transform=\"translate(39.79375 718.631028) scale(0.1 -0.1)\">\n        <defs>\n-        <path id=\"DejaVuSans-48\" d=\"M 628 4666 \n-L 1259 4666 \n-L 1259 2753 \n-L 3553 2753 \n-L 3553 4666 \n-L 4184 4666 \n-L 4184 0 \n-L 3553 0 \n-L 3553 2222 \n-L 1259 2222 \n-L 1259 0 \n-L 628 0 \n-L 628 4666 \n+        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n+L 2375 4384 \n+L 1825 4384 \n+Q 1516 4384 1395 4259 \n+Q 1275 4134 1275 3809 \n+L 1275 3500 \n+L 2222 3500 \n+L 2222 3053 \n+L 1275 3053 \n+L 1275 0 \n+L 697 0 \n+L 697 3053 \n+L 147 3053 \n+L 147 3500 \n+L 697 3500 \n+L 697 3744 \n+Q 697 4328 969 4595 \n+Q 1241 4863 1831 4863 \n+L 2375 4863 \n z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n-       <use xlink:href=\"#DejaVuSans-48\"/>\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"75.195312\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"138.574219\"/>\n-       <use xlink:href=\"#DejaVuSans-67\" x=\"201.953125\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"265.429688\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"326.708984\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"367.822266\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"395.605469\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"456.884766\"/>\n+       <use xlink:href=\"#DejaVuSans-41\"/>\n+       <use xlink:href=\"#DejaVuSans-66\" x=\"64.783203\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"99.988281\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"141.101562\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"168.884766\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"225.044922\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"286.324219\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"347.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"410.982422\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_36\">\n-     <g id=\"line2d_55\">\n+    <g id=\"ytick_46\">\n+     <g id=\"line2d_66\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"525.126316\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"730.393914\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_42\">\n-      <!-- Tamil -->\n-      <g transform=\"translate(55.451563 528.925535)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-54\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n-       <use xlink:href=\"#DejaVuSans-6d\" x=\"105.863281\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"203.275391\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"231.058594\"/>\n+     <g id=\"text_52\">\n+      <!-- Persian -->\n+      <g transform=\"translate(49.715625 734.193133) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-50\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"56.677734\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"118.201172\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"159.314453\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"211.414062\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"239.197266\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"300.476562\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_37\">\n-     <g id=\"line2d_56\">\n+    <g id=\"ytick_47\">\n+     <g id=\"line2d_67\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"539.715789\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"745.95602\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_43\">\n-      <!-- Hindi -->\n-      <g transform=\"translate(55.571875 543.515008)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-48\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"75.195312\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"102.978516\"/>\n-       <use xlink:href=\"#DejaVuSans-64\" x=\"166.357422\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"229.833984\"/>\n+     <g id=\"text_53\">\n+      <!-- Basque -->\n+      <g transform=\"translate(49.0625 749.755238) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n+Q 947 1113 1208 752 \n+Q 1469 391 1925 391 \n+Q 2381 391 2643 752 \n+Q 2906 1113 2906 1747 \n+Q 2906 2381 2643 2742 \n+Q 2381 3103 1925 3103 \n+Q 1469 3103 1208 2742 \n+Q 947 2381 947 1747 \n+z\n+M 2906 525 \n+Q 2725 213 2448 61 \n+Q 2172 -91 1784 -91 \n+Q 1150 -91 751 415 \n+Q 353 922 353 1747 \n+Q 353 2572 751 3078 \n+Q 1150 3584 1784 3584 \n+Q 2172 3584 2448 3432 \n+Q 2725 3281 2906 2969 \n+L 2906 3500 \n+L 3481 3500 \n+L 3481 -1331 \n+L 2906 -1331 \n+L 2906 525 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-42\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"68.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"129.882812\"/>\n+       <use xlink:href=\"#DejaVuSans-71\" x=\"181.982422\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"245.458984\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"308.837891\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_38\">\n-     <g id=\"line2d_57\">\n+    <g id=\"ytick_48\">\n+     <g id=\"line2d_68\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"554.305263\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"761.518125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_44\">\n-      <!-- Estonian -->\n-      <g transform=\"translate(38.185938 558.104482)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-45\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"63.183594\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"115.283203\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"154.492188\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"215.673828\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"279.052734\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"306.835938\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"368.115234\"/>\n+     <g id=\"text_54\">\n+      <!-- Vietnamese -->\n+      <g transform=\"translate(26.910938 765.317344) scale(0.1 -0.1)\">\n+       <defs>\n+        <path id=\"DejaVuSans-56\" d=\"M 1831 0 \n+L 50 4666 \n+L 709 4666 \n+L 2188 738 \n+L 3669 4666 \n+L 4325 4666 \n+L 2547 0 \n+L 1831 0 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       </defs>\n+       <use xlink:href=\"#DejaVuSans-56\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"66.158203\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"93.941406\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"155.464844\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"194.673828\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"258.052734\"/>\n+       <use xlink:href=\"#DejaVuSans-6d\" x=\"319.332031\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"416.744141\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"478.267578\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"530.367188\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_39\">\n-     <g id=\"line2d_58\">\n+    <g id=\"ytick_49\">\n+     <g id=\"line2d_69\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"568.894737\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"777.08023\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_45\">\n-      <!-- Urdu -->\n-      <g transform=\"translate(57.39375 572.693956)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-55\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"73.193359\"/>\n-       <use xlink:href=\"#DejaVuSans-64\" x=\"112.556641\"/>\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"176.033203\"/>\n+     <g id=\"text_55\">\n+      <!-- Bengali -->\n+      <g transform=\"translate(48.715625 780.879449) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-42\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"68.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"130.126953\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"193.505859\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"256.982422\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"318.261719\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"346.044922\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_40\">\n-     <g id=\"line2d_59\">\n+    <g id=\"ytick_50\">\n+     <g id=\"line2d_70\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"583.484211\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"792.642336\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_46\">\n-      <!-- Slovenian -->\n-      <g transform=\"translate(32.435938 587.283429)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-53\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"63.476562\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"91.259766\"/>\n-       <use xlink:href=\"#DejaVuSans-76\" x=\"152.441406\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"211.621094\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"273.144531\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"336.523438\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"364.306641\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"425.585938\"/>\n+     <g id=\"text_56\">\n+      <!-- Nepali -->\n+      <g transform=\"translate(54.432813 796.441554) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4e\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"74.804688\"/>\n+       <use xlink:href=\"#DejaVuSans-70\" x=\"136.328125\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"199.804688\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"261.083984\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"288.867188\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_41\">\n-     <g id=\"line2d_60\">\n+    <g id=\"ytick_51\">\n+     <g id=\"line2d_71\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"598.073684\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"808.204441\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_47\">\n-      <!-- Latvian -->\n-      <g transform=\"translate(44.551563 601.872903)scale(0.1 -0.1)\">\n-       <defs>\n-        <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \n-L 1259 4666 \n-L 1259 531 \n-L 3531 531 \n-L 3531 0 \n-L 628 0 \n-L 628 4666 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       </defs>\n-       <use xlink:href=\"#DejaVuSans-4c\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"55.712891\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"116.992188\"/>\n-       <use xlink:href=\"#DejaVuSans-76\" x=\"156.201172\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"215.380859\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"243.164062\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"304.443359\"/>\n+     <g id=\"text_57\">\n+      <!-- Marathi -->\n+      <g transform=\"translate(48.06875 812.00366) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4d\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"86.279297\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"147.558594\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"188.671875\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"249.951172\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"289.160156\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"352.539062\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_42\">\n-     <g id=\"line2d_61\">\n+    <g id=\"ytick_52\">\n+     <g id=\"line2d_72\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"612.663158\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"823.766546\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_48\">\n-      <!-- Azerbaijani -->\n-      <g transform=\"translate(25.704688 616.462377)scale(0.1 -0.1)\">\n-       <defs>\n-        <path id=\"DejaVuSans-6a\" d=\"M 603 3500 \n-L 1178 3500 \n-L 1178 -63 \n-Q 1178 -731 923 -1031 \n-Q 669 -1331 103 -1331 \n-L -116 -1331 \n-L -116 -844 \n-L 38 -844 \n-Q 366 -844 484 -692 \n-Q 603 -541 603 -63 \n-L 603 3500 \n-z\n-M 603 4863 \n-L 1178 4863 \n-L 1178 4134 \n-L 603 4134 \n-L 603 4863 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-       </defs>\n-       <use xlink:href=\"#DejaVuSans-41\"/>\n-       <use xlink:href=\"#DejaVuSans-7a\" x=\"68.408203\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"120.898438\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"182.421875\"/>\n-       <use xlink:href=\"#DejaVuSans-62\" x=\"223.535156\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"287.011719\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"348.291016\"/>\n-       <use xlink:href=\"#DejaVuSans-6a\" x=\"376.074219\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"403.857422\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"465.136719\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"528.515625\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_43\">\n-     <g id=\"line2d_62\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"627.252632\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_49\">\n-      <!-- Hebrew -->\n-      <g transform=\"translate(43.095313 631.05185)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-48\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"75.195312\"/>\n-       <use xlink:href=\"#DejaVuSans-62\" x=\"136.71875\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"200.195312\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"239.058594\"/>\n-       <use xlink:href=\"#DejaVuSans-77\" x=\"300.582031\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_44\">\n-     <g id=\"line2d_63\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"641.842105\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_50\">\n-      <!-- Lithuanian -->\n-      <g transform=\"translate(28.679688 645.641324)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-4c\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"55.712891\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"83.496094\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"122.705078\"/>\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"186.083984\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"249.462891\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"310.742188\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"374.121094\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"401.904297\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"463.183594\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_45\">\n-     <g id=\"line2d_64\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"656.431579\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_51\">\n-      <!-- Persian -->\n-      <g transform=\"translate(44.95 660.230798)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-50\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"56.677734\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"118.201172\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"159.314453\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"211.414062\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"239.197266\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"300.476562\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_46\">\n-     <g id=\"line2d_65\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"671.021053\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_52\">\n-      <!-- Welsh -->\n-      <g transform=\"translate(51.55625 674.820271)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-57\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"93.001953\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"154.525391\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"182.308594\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"234.408203\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_47\">\n-     <g id=\"line2d_66\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"685.610526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_53\">\n-      <!-- Serbian -->\n-      <g transform=\"translate(43.129688 689.409745)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-53\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"125\"/>\n-       <use xlink:href=\"#DejaVuSans-62\" x=\"166.113281\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"229.589844\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"257.373047\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"318.652344\"/>\n+     <g id=\"text_58\">\n+      <!-- Belarusian -->\n+      <g transform=\"translate(33.278125 827.565765) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-42\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"68.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"130.126953\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"157.910156\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"219.189453\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"260.302734\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"323.681641\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"375.78125\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"403.564453\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"464.84375\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_48\">\n-     <g id=\"line2d_67\">\n+    <g id=\"ytick_53\">\n+     <g id=\"line2d_73\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"700.2\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"839.328651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_54\">\n-      <!-- Afrikaans -->\n-      <g transform=\"translate(35.028125 703.999219)scale(0.1 -0.1)\">\n+     <g id=\"text_59\">\n+      <!-- Kazakh -->\n+      <g transform=\"translate(50.084375 843.12787) scale(0.1 -0.1)\">\n        <defs>\n-        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n-L 2375 4384 \n-L 1825 4384 \n-Q 1516 4384 1395 4259 \n-Q 1275 4134 1275 3809 \n-L 1275 3500 \n-L 2222 3500 \n-L 2222 3053 \n-L 1275 3053 \n-L 1275 0 \n-L 697 0 \n-L 697 3053 \n-L 147 3053 \n-L 147 3500 \n-L 697 3500 \n-L 697 3744 \n-Q 697 4328 969 4595 \n-Q 1241 4863 1831 4863 \n-L 2375 4863 \n+        <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \n+L 1259 4666 \n+L 1259 2694 \n+L 3353 4666 \n+L 4166 4666 \n+L 1850 2491 \n+L 4331 0 \n+L 3500 0 \n+L 1259 2247 \n+L 1259 0 \n+L 628 0 \n+L 628 4666 \n z\n \" transform=\"scale(0.015625)\"/>\n        </defs>\n-       <use xlink:href=\"#DejaVuSans-41\"/>\n-       <use xlink:href=\"#DejaVuSans-66\" x=\"64.783203\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"99.988281\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"141.101562\"/>\n-       <use xlink:href=\"#DejaVuSans-6b\" x=\"168.884766\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"225.044922\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"286.324219\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"347.603516\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"410.982422\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_49\">\n-     <g id=\"line2d_68\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"714.789474\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_55\">\n-      <!-- Kannada -->\n-      <g transform=\"translate(37.54375 718.588692)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-4b\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"63.826172\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"125.105469\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"188.484375\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"251.863281\"/>\n-       <use xlink:href=\"#DejaVuSans-64\" x=\"313.142578\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"376.619141\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_50\">\n-     <g id=\"line2d_69\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"729.378947\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_56\">\n-      <!-- Kazakh -->\n-      <g transform=\"translate(45.31875 733.178166)scale(0.1 -0.1)\">\n        <use xlink:href=\"#DejaVuSans-4b\"/>\n        <use xlink:href=\"#DejaVuSans-61\" x=\"63.826172\"/>\n        <use xlink:href=\"#DejaVuSans-7a\" x=\"125.105469\"/>\n@@ -2751,72 +2859,35 @@ z\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_51\">\n-     <g id=\"line2d_70\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"743.968421\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_57\">\n-      <!-- Icelandic -->\n-      <g transform=\"translate(36.864063 747.76764)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-49\"/>\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"29.492188\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"84.472656\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"145.996094\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"173.779297\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"235.058594\"/>\n-       <use xlink:href=\"#DejaVuSans-64\" x=\"298.4375\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"361.914062\"/>\n-       <use xlink:href=\"#DejaVuSans-63\" x=\"389.697266\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_52\">\n-     <g id=\"line2d_71\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"758.557895\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_58\">\n-      <!-- Marathi -->\n-      <g transform=\"translate(43.303125 762.357113)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-4d\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"86.279297\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"147.558594\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"188.671875\"/>\n-       <use xlink:href=\"#DejaVuSans-74\" x=\"249.951172\"/>\n-       <use xlink:href=\"#DejaVuSans-68\" x=\"289.160156\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"352.539062\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_53\">\n-     <g id=\"line2d_72\">\n+    <g id=\"ytick_54\">\n+     <g id=\"line2d_74\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"773.147368\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"854.890757\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_59\">\n-      <!-- Maori -->\n-      <g transform=\"translate(53.570313 776.946587)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-4d\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"86.279297\"/>\n-       <use xlink:href=\"#DejaVuSans-6f\" x=\"147.558594\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"208.740234\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"249.853516\"/>\n+     <g id=\"text_60\">\n+      <!-- Armenian -->\n+      <g transform=\"translate(37.848438 858.689975) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-41\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"68.408203\"/>\n+       <use xlink:href=\"#DejaVuSans-6d\" x=\"107.771484\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"205.183594\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"266.707031\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"330.085938\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"357.869141\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"419.148438\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_54\">\n-     <g id=\"line2d_73\">\n+    <g id=\"ytick_55\">\n+     <g id=\"line2d_75\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"787.736842\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"870.452862\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n-     <g id=\"text_60\">\n+     <g id=\"text_61\">\n       <!-- Swahili -->\n-      <g transform=\"translate(46.007813 791.536061)scale(0.1 -0.1)\">\n+      <g transform=\"translate(50.773438 874.252081) scale(0.1 -0.1)\">\n        <use xlink:href=\"#DejaVuSans-53\"/>\n        <use xlink:href=\"#DejaVuSans-77\" x=\"63.476562\"/>\n        <use xlink:href=\"#DejaVuSans-61\" x=\"145.263672\"/>\n@@ -2827,751 +2898,5564 @@ z\n       </g>\n      </g>\n     </g>\n-    <g id=\"ytick_55\">\n-     <g id=\"line2d_74\">\n+    <g id=\"ytick_56\">\n+     <g id=\"line2d_76\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"802.326316\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n-      </g>\n-     </g>\n-     <g id=\"text_61\">\n-      <!-- Armenian -->\n-      <g transform=\"translate(33.082813 806.125535)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-41\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"68.408203\"/>\n-       <use xlink:href=\"#DejaVuSans-6d\" x=\"107.771484\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"205.183594\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"266.707031\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"330.085938\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"357.869141\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"419.148438\"/>\n-      </g>\n-     </g>\n-    </g>\n-    <g id=\"ytick_56\">\n-     <g id=\"line2d_75\">\n-      <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"816.915789\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"886.014967\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_62\">\n-      <!-- Belarusian -->\n-      <g transform=\"translate(28.5125 820.715008)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-42\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"68.603516\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"130.126953\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"157.910156\"/>\n-       <use xlink:href=\"#DejaVuSans-72\" x=\"219.189453\"/>\n-       <use xlink:href=\"#DejaVuSans-75\" x=\"260.302734\"/>\n-       <use xlink:href=\"#DejaVuSans-73\" x=\"323.681641\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"375.78125\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"403.564453\"/>\n-       <use xlink:href=\"#DejaVuSans-6e\" x=\"464.84375\"/>\n+      <!-- Tamil -->\n+      <g transform=\"translate(60.217188 889.814186) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-54\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n+       <use xlink:href=\"#DejaVuSans-6d\" x=\"105.863281\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"203.275391\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"231.058594\"/>\n       </g>\n      </g>\n     </g>\n     <g id=\"ytick_57\">\n-     <g id=\"line2d_76\">\n+     <g id=\"line2d_77\">\n       <g>\n-       <use xlink:href=\"#m1fe7c4892d\" x=\"88.334375\" y=\"831.505263\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"93.1\" y=\"901.577072\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n       </g>\n      </g>\n      <g id=\"text_63\">\n-      <!-- Nepali -->\n-      <g transform=\"translate(49.667188 835.304482)scale(0.1 -0.1)\">\n-       <use xlink:href=\"#DejaVuSans-4e\"/>\n-       <use xlink:href=\"#DejaVuSans-65\" x=\"74.804688\"/>\n-       <use xlink:href=\"#DejaVuSans-70\" x=\"136.328125\"/>\n-       <use xlink:href=\"#DejaVuSans-61\" x=\"199.804688\"/>\n-       <use xlink:href=\"#DejaVuSans-6c\" x=\"261.083984\"/>\n-       <use xlink:href=\"#DejaVuSans-69\" x=\"288.867188\"/>\n+      <!-- Albanian -->\n+      <g transform=\"translate(42.423438 905.376291) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-41\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"68.408203\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"96.191406\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"159.667969\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"220.947266\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"284.326172\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"312.109375\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"373.388672\"/>\n       </g>\n      </g>\n     </g>\n-    <g id=\"text_64\">\n-     <!-- Language -->\n-     <g transform=\"translate(14.798438 447.676562)rotate(-90)scale(0.1 -0.1)\">\n-      <use xlink:href=\"#DejaVuSans-4c\"/>\n-      <use xlink:href=\"#DejaVuSans-61\" x=\"55.712891\"/>\n-      <use xlink:href=\"#DejaVuSans-6e\" x=\"116.992188\"/>\n-      <use xlink:href=\"#DejaVuSans-67\" x=\"180.371094\"/>\n-      <use xlink:href=\"#DejaVuSans-75\" x=\"243.847656\"/>\n-      <use xlink:href=\"#DejaVuSans-61\" x=\"307.226562\"/>\n-      <use xlink:href=\"#DejaVuSans-67\" x=\"368.505859\"/>\n-      <use xlink:href=\"#DejaVuSans-65\" x=\"431.982422\"/>\n+   </g>\n+   <g id=\"line2d_78\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_79\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_80\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_81\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_82\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_83\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_84\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_85\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_86\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_87\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_88\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_89\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_90\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_91\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_92\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_93\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_94\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_95\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_96\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_97\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_98\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_99\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_100\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_101\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_102\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_103\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_104\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_105\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_106\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_107\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_108\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_109\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_110\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_111\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_112\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_113\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_114\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_115\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_116\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_117\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_118\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_119\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_120\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_121\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_122\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_123\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_124\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_125\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_126\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_127\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_128\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_129\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_130\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_131\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_132\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_133\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_134\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_135\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_136\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_137\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_138\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_139\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_140\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_141\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_142\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_143\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_144\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_145\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_146\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_147\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_148\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_149\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_150\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_151\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_152\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_153\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_154\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_155\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_156\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_157\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_158\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_159\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_160\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_161\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_162\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_163\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_164\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_165\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_166\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_167\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_168\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_169\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_170\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_171\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_172\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_173\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_174\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_175\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_176\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_177\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_178\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_179\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_180\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_181\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_182\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_183\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_184\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_185\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_186\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_187\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_188\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_189\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_190\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_191\">\n+    <path clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"patch_3\">\n+    <path d=\"M 93.1 909.358125 \n+L 93.1 22.318125 \n+\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"patch_4\">\n+    <path d=\"M 325.6 909.358125 \n+L 325.6 22.318125 \n+\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"patch_5\">\n+    <path d=\"M 93.1 909.358125 \n+L 325.6 909.358125 \n+\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"patch_6\">\n+    <path d=\"M 93.1 22.318125 \n+L 325.6 22.318125 \n+\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"text_64\">\n+    <!-- Common Voice 15 -->\n+    <g transform=\"translate(154.909375 16.318125) scale(0.12 -0.12)\">\n+     <use xlink:href=\"#DejaVuSans-43\"/>\n+     <use xlink:href=\"#DejaVuSans-6f\" x=\"69.824219\"/>\n+     <use xlink:href=\"#DejaVuSans-6d\" x=\"131.005859\"/>\n+     <use xlink:href=\"#DejaVuSans-6d\" x=\"228.417969\"/>\n+     <use xlink:href=\"#DejaVuSans-6f\" x=\"325.830078\"/>\n+     <use xlink:href=\"#DejaVuSans-6e\" x=\"387.011719\"/>\n+     <use xlink:href=\"#DejaVuSans-20\" x=\"450.390625\"/>\n+     <use xlink:href=\"#DejaVuSans-56\" x=\"482.177734\"/>\n+     <use xlink:href=\"#DejaVuSans-6f\" x=\"542.835938\"/>\n+     <use xlink:href=\"#DejaVuSans-69\" x=\"604.017578\"/>\n+     <use xlink:href=\"#DejaVuSans-63\" x=\"631.800781\"/>\n+     <use xlink:href=\"#DejaVuSans-65\" x=\"686.78125\"/>\n+     <use xlink:href=\"#DejaVuSans-20\" x=\"748.304688\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"780.091797\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"843.714844\"/>\n+    </g>\n+   </g>\n+   <g id=\"legend_1\">\n+    <g id=\"patch_7\">\n+     <path d=\"M 245.629687 74.3525 \n+L 318.6 74.3525 \n+Q 320.6 74.3525 320.6 72.3525 \n+L 320.6 29.318125 \n+Q 320.6 27.318125 318.6 27.318125 \n+L 245.629687 27.318125 \n+Q 243.629687 27.318125 243.629687 29.318125 \n+L 243.629687 72.3525 \n+Q 243.629687 74.3525 245.629687 74.3525 \n+z\n+\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n+    </g>\n+    <g id=\"text_65\">\n+     <!-- Model -->\n+     <g transform=\"translate(267.101562 38.916562) scale(0.1 -0.1)\">\n+      <use xlink:href=\"#DejaVuSans-4d\"/>\n+      <use xlink:href=\"#DejaVuSans-6f\" x=\"86.279297\"/>\n+      <use xlink:href=\"#DejaVuSans-64\" x=\"147.460938\"/>\n+      <use xlink:href=\"#DejaVuSans-65\" x=\"210.9375\"/>\n+      <use xlink:href=\"#DejaVuSans-6c\" x=\"272.460938\"/>\n+     </g>\n+    </g>\n+    <g id=\"patch_8\">\n+     <path d=\"M 247.629687 53.594687 \n+L 267.629687 53.594687 \n+L 267.629687 46.594687 \n+L 247.629687 46.594687 \n+z\n+\" style=\"fill: #334b4b\"/>\n+    </g>\n+    <g id=\"text_66\">\n+     <!-- large-v3 -->\n+     <g transform=\"translate(275.629687 53.594687) scale(0.1 -0.1)\">\n+      <defs>\n+       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \n+L 1997 2009 \n+L 1997 1497 \n+L 313 1497 \n+L 313 2009 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+       <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n+Q 3050 2419 3304 2112 \n+Q 3559 1806 3559 1356 \n+Q 3559 666 3084 287 \n+Q 2609 -91 1734 -91 \n+Q 1441 -91 1130 -33 \n+Q 819 25 488 141 \n+L 488 750 \n+Q 750 597 1062 519 \n+Q 1375 441 1716 441 \n+Q 2309 441 2620 675 \n+Q 2931 909 2931 1356 \n+Q 2931 1769 2642 2001 \n+Q 2353 2234 1838 2234 \n+L 1294 2234 \n+L 1294 2753 \n+L 1863 2753 \n+Q 2328 2753 2575 2939 \n+Q 2822 3125 2822 3475 \n+Q 2822 3834 2567 4026 \n+Q 2313 4219 1838 4219 \n+Q 1578 4219 1281 4162 \n+Q 984 4106 628 3988 \n+L 628 4550 \n+Q 988 4650 1302 4700 \n+Q 1616 4750 1894 4750 \n+Q 2613 4750 3031 4423 \n+Q 3450 4097 3450 3541 \n+Q 3450 3153 3228 2886 \n+Q 3006 2619 2597 2516 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+      </defs>\n+      <use xlink:href=\"#DejaVuSans-6c\"/>\n+      <use xlink:href=\"#DejaVuSans-61\" x=\"27.783203\"/>\n+      <use xlink:href=\"#DejaVuSans-72\" x=\"89.0625\"/>\n+      <use xlink:href=\"#DejaVuSans-67\" x=\"128.425781\"/>\n+      <use xlink:href=\"#DejaVuSans-65\" x=\"191.902344\"/>\n+      <use xlink:href=\"#DejaVuSans-2d\" x=\"253.425781\"/>\n+      <use xlink:href=\"#DejaVuSans-76\" x=\"286.884766\"/>\n+      <use xlink:href=\"#DejaVuSans-33\" x=\"346.064453\"/>\n+     </g>\n+    </g>\n+    <g id=\"patch_9\">\n+     <path d=\"M 247.629687 68.272812 \n+L 267.629687 68.272812 \n+L 267.629687 61.272812 \n+L 247.629687 61.272812 \n+z\n+\" style=\"fill: #ffffff; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+    </g>\n+    <g id=\"text_67\">\n+     <!-- large-v2 -->\n+     <g transform=\"translate(275.629687 68.272812) scale(0.1 -0.1)\">\n+      <use xlink:href=\"#DejaVuSans-6c\"/>\n+      <use xlink:href=\"#DejaVuSans-61\" x=\"27.783203\"/>\n+      <use xlink:href=\"#DejaVuSans-72\" x=\"89.0625\"/>\n+      <use xlink:href=\"#DejaVuSans-67\" x=\"128.425781\"/>\n+      <use xlink:href=\"#DejaVuSans-65\" x=\"191.902344\"/>\n+      <use xlink:href=\"#DejaVuSans-2d\" x=\"253.425781\"/>\n+      <use xlink:href=\"#DejaVuSans-76\" x=\"286.884766\"/>\n+      <use xlink:href=\"#DejaVuSans-32\" x=\"346.064453\"/>\n      </g>\n     </g>\n    </g>\n-   <g id=\"patch_60\">\n-    <path d=\"M 88.334375 838.8 \n-L 88.334375 7.2 \n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   <g id=\"patch_10\">\n+    <path d=\"M -160630.550088 23.874336 \n+L 131.037072 23.874336 \n+L 131.037072 36.32402 \n+L -160630.550088 36.32402 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_11\">\n+    <path d=\"M -160630.550088 39.436441 \n+L 137.858563 39.436441 \n+L 137.858563 51.886125 \n+L -160630.550088 51.886125 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_12\">\n+    <path d=\"M -160630.550088 54.998546 \n+L 144.300337 54.998546 \n+L 144.300337 67.44823 \n+L -160630.550088 67.44823 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_13\">\n+    <path d=\"M -160630.550088 70.560651 \n+L 147.663314 70.560651 \n+L 147.663314 83.010336 \n+L -160630.550088 83.010336 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_14\">\n+    <path d=\"M -160630.550088 86.122757 \n+L 150.872577 86.122757 \n+L 150.872577 98.572441 \n+L -160630.550088 98.572441 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_15\">\n+    <path d=\"M -160630.550088 101.684862 \n+L 151.709278 101.684862 \n+L 151.709278 114.134546 \n+L -160630.550088 114.134546 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_16\">\n+    <path d=\"M -160630.550088 117.246967 \n+L 151.749107 117.246967 \n+L 151.749107 129.696651 \n+L -160630.550088 129.696651 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_17\">\n+    <path d=\"M -160630.550088 132.809072 \n+L 153.081305 132.809072 \n+L 153.081305 145.258757 \n+L -160630.550088 145.258757 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_18\">\n+    <path d=\"M -160630.550088 148.371178 \n+L 153.634029 148.371178 \n+L 153.634029 160.820862 \n+L -160630.550088 160.820862 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_19\">\n+    <path d=\"M -160630.550088 163.933283 \n+L 167.157058 163.933283 \n+L 167.157058 176.382967 \n+L -160630.550088 176.382967 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_20\">\n+    <path d=\"M -160630.550088 179.495388 \n+L 176.257197 179.495388 \n+L 176.257197 191.945072 \n+L -160630.550088 191.945072 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_21\">\n+    <path d=\"M -160630.550088 195.057493 \n+L 176.913466 195.057493 \n+L 176.913466 207.507178 \n+L -160630.550088 207.507178 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_22\">\n+    <path d=\"M -160630.550088 210.619599 \n+L 182.740378 210.619599 \n+L 182.740378 223.069283 \n+L -160630.550088 223.069283 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_23\">\n+    <path d=\"M -160630.550088 226.181704 \n+L 184.603889 226.181704 \n+L 184.603889 238.631388 \n+L -160630.550088 238.631388 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_24\">\n+    <path d=\"M -160630.550088 241.743809 \n+L 191.915052 241.743809 \n+L 191.915052 254.193493 \n+L -160630.550088 254.193493 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_25\">\n+    <path d=\"M -160630.550088 257.305914 \n+L 195.286657 257.305914 \n+L 195.286657 269.755599 \n+L -160630.550088 269.755599 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_26\">\n+    <path d=\"M -160630.550088 272.86802 \n+L 195.50041 272.86802 \n+L 195.50041 285.317704 \n+L -160630.550088 285.317704 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_27\">\n+    <path d=\"M -160630.550088 288.430125 \n+L 195.574499 288.430125 \n+L 195.574499 300.879809 \n+L -160630.550088 300.879809 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_28\">\n+    <path d=\"M -160630.550088 303.99223 \n+L 202.731771 303.99223 \n+L 202.731771 316.441914 \n+L -160630.550088 316.441914 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_29\">\n+    <path d=\"M -160630.550088 319.554336 \n+L 206.923595 319.554336 \n+L 206.923595 332.00402 \n+L -160630.550088 332.00402 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_30\">\n+    <path d=\"M -160630.550088 335.116441 \n+L 209.524294 335.116441 \n+L 209.524294 347.566125 \n+L -160630.550088 347.566125 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_31\">\n+    <path d=\"M -160630.550088 350.678546 \n+L 210.424967 350.678546 \n+L 210.424967 363.12823 \n+L -160630.550088 363.12823 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_32\">\n+    <path d=\"M -160630.550088 366.240651 \n+L 211.152087 366.240651 \n+L 211.152087 378.690336 \n+L -160630.550088 378.690336 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_33\">\n+    <path d=\"M -160630.550088 381.802757 \n+L 211.907147 381.802757 \n+L 211.907147 394.252441 \n+L -160630.550088 394.252441 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_34\">\n+    <path d=\"M -160630.550088 397.364862 \n+L 214.655317 397.364862 \n+L 214.655317 409.814546 \n+L -160630.550088 409.814546 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_35\">\n+    <path d=\"M -160630.550088 412.926967 \n+L 218.59591 412.926967 \n+L 218.59591 425.376651 \n+L -160630.550088 425.376651 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_36\">\n+    <path d=\"M -160630.550088 428.489072 \n+L 221.39498 428.489072 \n+L 221.39498 440.938757 \n+L -160630.550088 440.938757 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_37\">\n+    <path d=\"M -160630.550088 444.051178 \n+L 221.87157 444.051178 \n+L 221.87157 456.500862 \n+L -160630.550088 456.500862 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_38\">\n+    <path d=\"M -160630.550088 459.613283 \n+L 222.380734 459.613283 \n+L 222.380734 472.062967 \n+L -160630.550088 472.062967 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_39\">\n+    <path d=\"M -160630.550088 475.175388 \n+L 225.516319 475.175388 \n+L 225.516319 487.625072 \n+L -160630.550088 487.625072 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_40\">\n+    <path d=\"M -160630.550088 490.737493 \n+L 225.995466 490.737493 \n+L 225.995466 503.187178 \n+L -160630.550088 503.187178 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_41\">\n+    <path d=\"M -160630.550088 506.299599 \n+L 226.358803 506.299599 \n+L 226.358803 518.749283 \n+L -160630.550088 518.749283 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_42\">\n+    <path d=\"M -160630.550088 521.861704 \n+L 230.889356 521.861704 \n+L 230.889356 534.311388 \n+L -160630.550088 534.311388 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_43\">\n+    <path d=\"M -160630.550088 537.423809 \n+L 231.298386 537.423809 \n+L 231.298386 549.873493 \n+L -160630.550088 549.873493 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_44\">\n+    <path d=\"M -160630.550088 552.985914 \n+L 239.730993 552.985914 \n+L 239.730993 565.435599 \n+L -160630.550088 565.435599 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_45\">\n+    <path d=\"M -160630.550088 568.54802 \n+L 249.254878 568.54802 \n+L 249.254878 580.997704 \n+L -160630.550088 580.997704 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_46\">\n+    <path d=\"M -160630.550088 584.110125 \n+L 249.351496 584.110125 \n+L 249.351496 596.559809 \n+L -160630.550088 596.559809 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_47\">\n+    <path d=\"M -160630.550088 599.67223 \n+L 252.692874 599.67223 \n+L 252.692874 612.121914 \n+L -160630.550088 612.121914 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_48\">\n+    <path d=\"M -160630.550088 615.234336 \n+L 253.160798 615.234336 \n+L 253.160798 627.68402 \n+L -160630.550088 627.68402 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_49\">\n+    <path d=\"M -160630.550088 630.796441 \n+L 260.629928 630.796441 \n+L 260.629928 643.246125 \n+L -160630.550088 643.246125 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_50\">\n+    <path d=\"M -160630.550088 646.358546 \n+L 266.290286 646.358546 \n+L 266.290286 658.80823 \n+L -160630.550088 658.80823 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_51\">\n+    <path d=\"M -160630.550088 661.920651 \n+L 268.077534 661.920651 \n+L 268.077534 674.370336 \n+L -160630.550088 674.370336 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_52\">\n+    <path d=\"M -160630.550088 677.482757 \n+L 270.998729 677.482757 \n+L 270.998729 689.932441 \n+L -160630.550088 689.932441 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_53\">\n+    <path d=\"M -160630.550088 693.044862 \n+L 276.704953 693.044862 \n+L 276.704953 705.494546 \n+L -160630.550088 705.494546 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_54\">\n+    <path d=\"M -160630.550088 708.606967 \n+L 279.259172 708.606967 \n+L 279.259172 721.056651 \n+L -160630.550088 721.056651 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_55\">\n+    <path d=\"M -160630.550088 724.169072 \n+L 281.813282 724.169072 \n+L 281.813282 736.618757 \n+L -160630.550088 736.618757 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_56\">\n+    <path d=\"M -160630.550088 739.731178 \n+L 284.599542 739.731178 \n+L 284.599542 752.180862 \n+L -160630.550088 752.180862 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_57\">\n+    <path d=\"M -160630.550088 755.293283 \n+L 286.199156 755.293283 \n+L 286.199156 767.742967 \n+L -160630.550088 767.742967 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_58\">\n+    <path d=\"M -160630.550088 770.855388 \n+L 286.996557 770.855388 \n+L 286.996557 783.305072 \n+L -160630.550088 783.305072 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_59\">\n+    <path d=\"M -160630.550088 786.417493 \n+L 287.195222 786.417493 \n+L 287.195222 798.867178 \n+L -160630.550088 798.867178 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_60\">\n+    <path d=\"M -160630.550088 801.979599 \n+L 288.684721 801.979599 \n+L 288.684721 814.429283 \n+L -160630.550088 814.429283 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_61\">\n+    <path d=\"M -160630.550088 817.541704 \n+L 292.67167 817.541704 \n+L 292.67167 829.991388 \n+L -160630.550088 829.991388 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_62\">\n+    <path d=\"M -160630.550088 833.103809 \n+L 298.677909 833.103809 \n+L 298.677909 845.553493 \n+L -160630.550088 845.553493 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_63\">\n+    <path d=\"M -160630.550088 848.665914 \n+L 299.36189 848.665914 \n+L 299.36189 861.115599 \n+L -160630.550088 861.115599 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_64\">\n+    <path d=\"M -160630.550088 864.22802 \n+L 303.768811 864.22802 \n+L 303.768811 876.677704 \n+L -160630.550088 876.677704 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_65\">\n+    <path d=\"M -160630.550088 879.790125 \n+L 304.16547 879.790125 \n+L 304.16547 892.239809 \n+L -160630.550088 892.239809 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_66\">\n+    <path d=\"M -160630.550088 895.35223 \n+L 309.645011 895.35223 \n+L 309.645011 907.801914 \n+L -160630.550088 907.801914 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_67\">\n+    <path d=\"M -160630.550088 30.099178 \n+L -160630.550088 30.099178 \n+L -160630.550088 30.099178 \n+L -160630.550088 30.099178 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_68\">\n+    <path d=\"M -1 23.874336 \n+L 148.771055 23.874336 \n+L 148.771055 36.32402 \n+L -1 36.32402 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_69\">\n+    <path d=\"M -1 39.436441 \n+L 152.476831 39.436441 \n+L 152.476831 51.886125 \n+L -1 51.886125 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_70\">\n+    <path d=\"M -1 54.998546 \n+L 160.087575 54.998546 \n+L 160.087575 67.44823 \n+L -1 67.44823 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_71\">\n+    <path d=\"M -1 70.560651 \n+L 165.552822 70.560651 \n+L 165.552822 83.010336 \n+L -1 83.010336 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_72\">\n+    <path d=\"M -1 86.122757 \n+L 161.395711 86.122757 \n+L 161.395711 98.572441 \n+L -1 98.572441 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_73\">\n+    <path d=\"M -1 101.684862 \n+L 182.352442 101.684862 \n+L 182.352442 114.134546 \n+L -1 114.134546 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_74\">\n+    <path d=\"M -1 117.246967 \n+L 170.409819 117.246967 \n+L 170.409819 129.696651 \n+L -1 129.696651 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_75\">\n+    <path d=\"M -1 132.809072 \n+L 169.08925 132.809072 \n+L 169.08925 145.258757 \n+L -1 145.258757 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_76\">\n+    <path d=\"M -1 148.371178 \n+L 168.616953 148.371178 \n+L 168.616953 160.820862 \n+L -1 160.820862 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_77\">\n+    <path d=\"M -1 163.933283 \n+L 181.05718 163.933283 \n+L 181.05718 176.382967 \n+L -1 176.382967 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_78\">\n+    <path d=\"M -1 179.495388 \n+L 176.752429 179.495388 \n+L 176.752429 191.945072 \n+L -1 191.945072 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_79\">\n+    <path d=\"M -1 195.057493 \n+L 194.331245 195.057493 \n+L 194.331245 207.507178 \n+L -1 207.507178 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_80\">\n+    <path d=\"M -1 210.619599 \n+L 208.653769 210.619599 \n+L 208.653769 223.069283 \n+L -1 223.069283 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_81\">\n+    <path d=\"M -1 226.181704 \n+L 189.355912 226.181704 \n+L 189.355912 238.631388 \n+L -1 238.631388 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_82\">\n+    <path d=\"M -1 241.743809 \n+L 202.636006 241.743809 \n+L 202.636006 254.193493 \n+L -1 254.193493 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_83\">\n+    <path d=\"M -1 257.305914 \n+L 205.075494 257.305914 \n+L 205.075494 269.755599 \n+L -1 269.755599 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_84\">\n+    <path d=\"M -1 272.86802 \n+L 211.973761 272.86802 \n+L 211.973761 285.317704 \n+L -1 285.317704 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_85\">\n+    <path d=\"M -1 288.430125 \n+L 265.765263 288.430125 \n+L 265.765263 300.879809 \n+L -1 300.879809 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_86\">\n+    <path d=\"M -1 303.99223 \n+L 214.645519 303.99223 \n+L 214.645519 316.441914 \n+L -1 316.441914 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_87\">\n+    <path d=\"M -1 319.554336 \n+L 208.439067 319.554336 \n+L 208.439067 332.00402 \n+L -1 332.00402 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_88\">\n+    <path d=\"M -1 335.116441 \n+L 218.027296 335.116441 \n+L 218.027296 347.566125 \n+L -1 347.566125 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_89\">\n+    <path d=\"M -1 350.678546 \n+L 226.176273 350.678546 \n+L 226.176273 363.12823 \n+L -1 363.12823 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_90\">\n+    <path d=\"M -1 366.240651 \n+L 223.531579 366.240651 \n+L 223.531579 378.690336 \n+L -1 378.690336 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_91\">\n+    <path d=\"M -1 381.802757 \n+L 222.8651 381.802757 \n+L 222.8651 394.252441 \n+L -1 394.252441 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_92\">\n+    <path d=\"M -1 397.364862 \n+L 232.652899 397.364862 \n+L 232.652899 409.814546 \n+L -1 409.814546 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_93\">\n+    <path d=\"M -1 412.926967 \n+L 234.59059 412.926967 \n+L 234.59059 425.376651 \n+L -1 425.376651 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_94\">\n+    <path d=\"M -1 428.489072 \n+L 249.449831 428.489072 \n+L 249.449831 440.938757 \n+L -1 440.938757 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_95\">\n+    <path d=\"M -1 444.051178 \n+L 239.406689 444.051178 \n+L 239.406689 456.500862 \n+L -1 456.500862 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_96\">\n+    <path d=\"M -1 459.613283 \n+L 258.69836 459.613283 \n+L 258.69836 472.062967 \n+L -1 472.062967 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_97\">\n+    <path d=\"M -1 475.175388 \n+L 250.840297 475.175388 \n+L 250.840297 487.625072 \n+L -1 487.625072 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_98\">\n+    <path d=\"M -1 490.737493 \n+L 242.894287 490.737493 \n+L 242.894287 503.187178 \n+L -1 503.187178 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_99\">\n+    <path d=\"M -1 506.299599 \n+L 244.29354 506.299599 \n+L 244.29354 518.749283 \n+L -1 518.749283 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_100\">\n+    <path d=\"M -1 521.861704 \n+L 241.333898 521.861704 \n+L 241.333898 534.311388 \n+L -1 534.311388 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_101\">\n+    <path d=\"M -1 537.423809 \n+L 241.677591 537.423809 \n+L 241.677591 549.873493 \n+L -1 549.873493 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_102\">\n+    <path d=\"M -1 552.985914 \n+L 250.078277 552.985914 \n+L 250.078277 565.435599 \n+L -1 565.435599 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_103\">\n+    <path d=\"M -1 568.54802 \n+L 270.955291 568.54802 \n+L 270.955291 580.997704 \n+L -1 580.997704 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_104\">\n+    <path d=\"M -1 584.110125 \n+L 232.248413 584.110125 \n+L 232.248413 596.559809 \n+L -1 596.559809 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_105\">\n+    <path d=\"M -1 599.67223 \n+L 259.031258 599.67223 \n+L 259.031258 612.121914 \n+L -1 612.121914 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_106\">\n+    <path d=\"M -1 615.234336 \n+L 262.17637 615.234336 \n+L 262.17637 627.68402 \n+L -1 627.68402 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_107\">\n+    <path d=\"M -1 630.796441 \n+L 278.339088 630.796441 \n+L 278.339088 643.246125 \n+L -1 643.246125 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_108\">\n+    <path d=\"M -1 646.358546 \n+L 279.191611 646.358546 \n+L 279.191611 658.80823 \n+L -1 658.80823 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_109\">\n+    <path d=\"M -1 661.920651 \n+L 301.409705 661.920651 \n+L 301.409705 674.370336 \n+L -1 674.370336 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_110\">\n+    <path d=\"M -1 677.482757 \n+L 269.103131 677.482757 \n+L 269.103131 689.932441 \n+L -1 689.932441 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_111\">\n+    <path d=\"M -1 693.044862 \n+L 340.877713 693.044862 \n+L 340.877713 705.494546 \n+L -1 705.494546 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_112\">\n+    <path d=\"M -1 708.606967 \n+L 296.794291 708.606967 \n+L 296.794291 721.056651 \n+L -1 721.056651 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_113\">\n+    <path d=\"M -1 724.169072 \n+L 291.868977 724.169072 \n+L 291.868977 736.618757 \n+L -1 736.618757 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_114\">\n+    <path d=\"M -1 739.731178 \n+L 291.718337 739.731178 \n+L 291.718337 752.180862 \n+L -1 752.180862 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_115\">\n+    <path d=\"M -1 755.293283 \n+L 293.351338 755.293283 \n+L 293.351338 767.742967 \n+L -1 767.742967 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_116\">\n+    <path d=\"M -1 770.855388 \n+L 341.921388 770.855388 \n+L 341.921388 783.305072 \n+L -1 783.305072 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_117\">\n+    <path d=\"M -1 786.417493 \n+L 299.900327 786.417493 \n+L 299.900327 798.867178 \n+L -1 798.867178 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_118\">\n+    <path d=\"M -1 801.979599 \n+L 295.955921 801.979599 \n+L 295.955921 814.429283 \n+L -1 814.429283 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_119\">\n+    <path d=\"M -1 817.541704 \n+L 296.524834 817.541704 \n+L 296.524834 829.991388 \n+L -1 829.991388 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_120\">\n+    <path d=\"M -1 833.103809 \n+L 311.593373 833.103809 \n+L 311.593373 845.553493 \n+L -1 845.553493 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_121\">\n+    <path d=\"M -1 848.665914 \n+L 304.201667 848.665914 \n+L 304.201667 861.115599 \n+L -1 861.115599 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_122\">\n+    <path d=\"M -1 864.22802 \n+L 323.521791 864.22802 \n+L 323.521791 876.677704 \n+L -1 876.677704 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_123\">\n+    <path d=\"M -1 879.790125 \n+L 303.868558 879.790125 \n+L 303.868558 892.239809 \n+L -1 892.239809 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_124\">\n+    <path d=\"M -1 895.35223 \n+L 323.773642 895.35223 \n+L 323.773642 907.801914 \n+L -1 907.801914 \n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_125\">\n+    <path d=\"M -160630.550088 30.099178 \n+L -160630.550088 30.099178 \n+L -160630.550088 30.099178 \n+L -160630.550088 30.099178 \n+z\n+\" clip-path=\"url(#p9f626abd74)\" style=\"fill: #ffffff; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"text_68\">\n+    <!-- 4.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(116.189323 32.929162) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_69\">\n+    <!-- 4.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(123.010813 48.491267) scale(0.08 -0.08)\">\n+     <defs>\n+      <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n+L 3525 4666 \n+L 3525 4397 \n+L 1831 0 \n+L 1172 0 \n+L 2766 4134 \n+L 525 4134 \n+L 525 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+     </defs>\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_70\">\n+    <!-- 5.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(129.452588 64.053372) scale(0.08 -0.08)\">\n+     <defs>\n+      <path id=\"DejaVuSans-Oblique-35\" d=\"M 3719 4666 \n+L 3622 4141 \n+L 1691 4141 \n+L 1466 2988 \n+Q 1609 3034 1757 3056 \n+Q 1906 3078 2059 3078 \n+Q 2706 3078 3081 2731 \n+Q 3456 2384 3456 1791 \n+Q 3456 1375 3264 994 \n+Q 3072 613 2719 325 \n+Q 2469 125 2108 17 \n+Q 1747 -91 1319 -91 \n+Q 1038 -91 744 -41 \n+Q 450 9 128 109 \n+L 244 697 \n+Q 531 556 820 486 \n+Q 1109 416 1403 416 \n+Q 2013 416 2411 781 \n+Q 2809 1147 2809 1697 \n+Q 2809 2103 2548 2328 \n+Q 2288 2553 1819 2553 \n+Q 1572 2553 1305 2495 \n+Q 1038 2438 756 2322 \n+L 1209 4666 \n+L 3719 4666 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+      <path id=\"DejaVuSans-Oblique-2e\" d=\"M 525 794 \n+L 1184 794 \n+L 1031 0 \n+L 372 0 \n+L 525 794 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+      <path id=\"DejaVuSans-Oblique-32\" d=\"M 2950 2253 \n+L 934 525 \n+L 3163 525 \n+L 3053 0 \n+L 25 0 \n+L 128 531 \n+L 2234 2338 \n+Q 2656 2703 2832 2979 \n+Q 3009 3256 3009 3547 \n+Q 3009 3850 2796 4040 \n+Q 2584 4231 2241 4231 \n+Q 1944 4231 1581 4125 \n+Q 1219 4019 806 3816 \n+L 922 4441 \n+Q 1309 4594 1662 4672 \n+Q 2016 4750 2316 4750 \n+Q 2928 4750 3301 4425 \n+Q 3675 4100 3675 3572 \n+Q 3675 3216 3497 2892 \n+Q 3319 2569 2950 2253 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+     </defs>\n+     <use xlink:href=\"#DejaVuSans-Oblique-35\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-32\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_71\">\n+    <!-- 5.5 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(132.815565 79.615478) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-35\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_72\">\n+    <!-- 5.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(136.024828 95.177583) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-35\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_73\">\n+    <!-- 5.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(136.861528 110.739688) scale(0.08 -0.08)\">\n+     <defs>\n+      <path id=\"DejaVuSans-Oblique-38\" d=\"M 2847 1416 \n+Q 2847 1769 2600 1992 \n+Q 2353 2216 1953 2216 \n+Q 1466 2216 1148 1923 \n+Q 831 1631 831 1184 \n+Q 831 828 1073 618 \n+Q 1316 409 1728 409 \n+Q 2219 409 2533 693 \n+Q 2847 978 2847 1416 \n+z\n+M 3169 3591 \n+Q 3169 3888 2950 4069 \n+Q 2731 4250 2369 4250 \n+Q 1941 4250 1664 4009 \n+Q 1388 3769 1388 3406 \n+Q 1388 3094 1605 2903 \n+Q 1822 2713 2181 2713 \n+Q 2613 2713 2891 2961 \n+Q 3169 3209 3169 3591 \n+z\n+M 2741 2456 \n+Q 3094 2322 3281 2050 \n+Q 3469 1778 3469 1394 \n+Q 3469 747 2969 328 \n+Q 2469 -91 1678 -91 \n+Q 1006 -91 609 239 \n+Q 213 569 213 1119 \n+Q 213 1619 553 2008 \n+Q 894 2397 1441 2503 \n+Q 1113 2616 941 2852 \n+Q 769 3088 769 3425 \n+Q 769 3991 1239 4370 \n+Q 1709 4750 2431 4750 \n+Q 3034 4750 3414 4442 \n+Q 3794 4134 3794 3659 \n+Q 3794 3247 3511 2923 \n+Q 3228 2600 2741 2456 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+     </defs>\n+     <use xlink:href=\"#DejaVuSans-Oblique-35\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-38\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_74\">\n+    <!-- 5.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(136.901357 126.301793) scale(0.08 -0.08)\">\n+     <defs>\n+      <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n+Q 1584 2216 1326 1975 \n+Q 1069 1734 1069 1313 \n+Q 1069 891 1326 650 \n+Q 1584 409 2034 409 \n+Q 2484 409 2743 651 \n+Q 3003 894 3003 1313 \n+Q 3003 1734 2745 1975 \n+Q 2488 2216 2034 2216 \n+z\n+M 1403 2484 \n+Q 997 2584 770 2862 \n+Q 544 3141 544 3541 \n+Q 544 4100 942 4425 \n+Q 1341 4750 2034 4750 \n+Q 2731 4750 3128 4425 \n+Q 3525 4100 3525 3541 \n+Q 3525 3141 3298 2862 \n+Q 3072 2584 2669 2484 \n+Q 3125 2378 3379 2068 \n+Q 3634 1759 3634 1313 \n+Q 3634 634 3220 271 \n+Q 2806 -91 2034 -91 \n+Q 1263 -91 848 271 \n+Q 434 634 434 1313 \n+Q 434 1759 690 2068 \n+Q 947 2378 1403 2484 \n+z\n+M 1172 3481 \n+Q 1172 3119 1398 2916 \n+Q 1625 2713 2034 2713 \n+Q 2441 2713 2670 2916 \n+Q 2900 3119 2900 3481 \n+Q 2900 3844 2670 4047 \n+Q 2441 4250 2034 4250 \n+Q 1625 4250 1398 4047 \n+Q 1172 3844 1172 3481 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+     </defs>\n+     <use xlink:href=\"#DejaVuSans-35\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_75\">\n+    <!-- 5.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(138.233556 141.863899) scale(0.08 -0.08)\">\n+     <defs>\n+      <path id=\"DejaVuSans-39\" d=\"M 703 97 \n+L 703 672 \n+Q 941 559 1184 500 \n+Q 1428 441 1663 441 \n+Q 2288 441 2617 861 \n+Q 2947 1281 2994 2138 \n+Q 2813 1869 2534 1725 \n+Q 2256 1581 1919 1581 \n+Q 1219 1581 811 2004 \n+Q 403 2428 403 3163 \n+Q 403 3881 828 4315 \n+Q 1253 4750 1959 4750 \n+Q 2769 4750 3195 4129 \n+Q 3622 3509 3622 2328 \n+Q 3622 1225 3098 567 \n+Q 2575 -91 1691 -91 \n+Q 1453 -91 1209 -44 \n+Q 966 3 703 97 \n+z\n+M 1959 2075 \n+Q 2384 2075 2632 2365 \n+Q 2881 2656 2881 3163 \n+Q 2881 3666 2632 3958 \n+Q 2384 4250 1959 4250 \n+Q 1534 4250 1286 3958 \n+Q 1038 3666 1038 3163 \n+Q 1038 2656 1286 2365 \n+Q 1534 2075 1959 2075 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+     </defs>\n+     <use xlink:href=\"#DejaVuSans-35\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_76\">\n+    <!-- 6.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(138.786279 157.426004) scale(0.08 -0.08)\">\n+     <defs>\n+      <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n+Q 1688 2584 1439 2293 \n+Q 1191 2003 1191 1497 \n+Q 1191 994 1439 701 \n+Q 1688 409 2113 409 \n+Q 2538 409 2786 701 \n+Q 3034 994 3034 1497 \n+Q 3034 2003 2786 2293 \n+Q 2538 2584 2113 2584 \n+z\n+M 3366 4563 \n+L 3366 3988 \n+Q 3128 4100 2886 4159 \n+Q 2644 4219 2406 4219 \n+Q 1781 4219 1451 3797 \n+Q 1122 3375 1075 2522 \n+Q 1259 2794 1537 2939 \n+Q 1816 3084 2150 3084 \n+Q 2853 3084 3261 2657 \n+Q 3669 2231 3669 1497 \n+Q 3669 778 3244 343 \n+Q 2819 -91 2113 -91 \n+Q 1303 -91 875 529 \n+Q 447 1150 447 2328 \n+Q 447 3434 972 4092 \n+Q 1497 4750 2381 4750 \n+Q 2619 4750 2861 4703 \n+Q 3103 4656 3366 4563 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+     </defs>\n+     <use xlink:href=\"#DejaVuSans-36\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_77\">\n+    <!-- 7.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(152.309309 172.988109) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-37\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_78\">\n+    <!-- 8.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(161.409447 188.550214) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-Oblique-38\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-32\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_79\">\n+    <!-- 8.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(162.065716 204.11232) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-38\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_80\">\n+    <!-- 9.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(167.892629 219.674425) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-39\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_81\">\n+    <!-- 9.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(169.75614 235.23653) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-39\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_82\">\n+    <!-- 10.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(171.977303 250.798636) scale(0.08 -0.08)\">\n+     <defs>\n+      <path id=\"DejaVuSans-Oblique-31\" d=\"M 416 531 \n+L 1447 531 \n+L 2144 4122 \n+L 978 3897 \n+L 1088 4441 \n+L 2247 4666 \n+L 2881 4666 \n+L 2075 531 \n+L 3103 531 \n+L 3003 0 \n+L 313 0 \n+L 416 531 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+      <path id=\"DejaVuSans-Oblique-30\" d=\"M 2444 4750 \n+Q 3078 4750 3414 4328 \n+Q 3750 3906 3750 3116 \n+Q 3750 2516 3592 1923 \n+Q 3434 1331 3150 863 \n+Q 2859 394 2467 151 \n+Q 2075 -91 1600 -91 \n+Q 988 -91 653 334 \n+Q 319 759 319 1544 \n+Q 319 2138 480 2733 \n+Q 641 3328 928 3794 \n+Q 1216 4263 1602 4506 \n+Q 1988 4750 2444 4750 \n+z\n+M 2406 4250 \n+Q 2169 4250 1955 4123 \n+Q 1741 3997 1569 3750 \n+Q 1281 3338 1111 2720 \n+Q 941 2103 941 1453 \n+Q 941 938 1123 673 \n+Q 1306 409 1663 409 \n+Q 1913 409 2125 536 \n+Q 2338 663 2509 909 \n+Q 2794 1319 2961 1936 \n+Q 3128 2553 3128 3206 \n+Q 3128 3722 2945 3986 \n+Q 2763 4250 2406 4250 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+      <path id=\"DejaVuSans-Oblique-33\" d=\"M 1013 4550 \n+Q 1356 4650 1679 4700 \n+Q 2003 4750 2316 4750 \n+Q 2934 4750 3296 4475 \n+Q 3659 4200 3659 3738 \n+Q 3659 3284 3373 2959 \n+Q 3088 2634 2584 2516 \n+Q 2978 2403 3162 2165 \n+Q 3347 1928 3347 1538 \n+Q 3347 1163 3167 834 \n+Q 2988 506 2650 269 \n+Q 2394 88 2037 -1 \n+Q 1681 -91 1216 -91 \n+Q 922 -91 622 -33 \n+Q 322 25 13 141 \n+L 128 738 \n+Q 422 575 720 495 \n+Q 1019 416 1331 416 \n+Q 1947 416 2330 733 \n+Q 2713 1050 2713 1556 \n+Q 2713 1881 2469 2057 \n+Q 2225 2234 1772 2234 \n+L 1228 2234 \n+L 1325 2747 \n+L 1900 2747 \n+Q 2419 2747 2717 2984 \n+Q 3016 3222 3016 3628 \n+Q 3016 3922 2797 4083 \n+Q 2578 4244 2181 4244 \n+Q 1881 4244 1567 4180 \n+Q 1253 4116 916 3988 \n+L 1013 4550 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+     </defs>\n+     <use xlink:href=\"#DejaVuSans-Oblique-31\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-30\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-33\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_83\">\n+    <!-- 10.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(175.348907 266.360741) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_84\">\n+    <!-- 10.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(175.56266 281.922846) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_85\">\n+    <!-- 10.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(175.63675 297.484951) scale(0.08 -0.08)\">\n+     <defs>\n+      <path id=\"DejaVuSans-Oblique-39\" d=\"M 281 103 \n+L 397 678 \n+Q 578 559 809 496 \n+Q 1041 434 1306 434 \n+Q 1953 434 2348 843 \n+Q 2744 1253 2938 2119 \n+Q 2706 1853 2401 1714 \n+Q 2097 1575 1747 1575 \n+Q 1178 1575 842 1904 \n+Q 506 2234 506 2791 \n+Q 506 3247 672 3639 \n+Q 838 4031 1159 4325 \n+Q 1378 4531 1665 4640 \n+Q 1953 4750 2278 4750 \n+Q 2925 4750 3300 4339 \n+Q 3675 3928 3675 3219 \n+Q 3675 2559 3501 1943 \n+Q 3328 1328 3022 878 \n+Q 2697 403 2240 156 \n+Q 1784 -91 1228 -91 \n+Q 988 -91 748 -42 \n+Q 509 6 281 103 \n+z\n+M 1131 2938 \n+Q 1131 2541 1337 2308 \n+Q 1544 2075 1894 2075 \n+Q 2378 2075 2706 2447 \n+Q 3034 2819 3034 3372 \n+Q 3034 3784 2831 4017 \n+Q 2628 4250 2272 4250 \n+Q 1788 4250 1459 3870 \n+Q 1131 3491 1131 2938 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+     </defs>\n+     <use xlink:href=\"#DejaVuSans-Oblique-31\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-30\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-39\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_86\">\n+    <!-- 12.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(182.794022 313.047057) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_87\">\n+    <!-- 12.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(186.985846 328.609162) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-Oblique-31\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-32\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-38\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_88\">\n+    <!-- 13.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(189.586545 344.171267) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_89\">\n+    <!-- 13.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(190.487218 359.733372) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_90\">\n+    <!-- 13.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(191.214338 375.295478) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_91\">\n+    <!-- 13.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(191.969397 390.857583) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_92\">\n+    <!-- 14.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(194.717568 406.419688) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_93\">\n+    <!-- 15.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(198.658161 421.981793) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_94\">\n+    <!-- 15.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(201.457231 437.543899) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_95\">\n+    <!-- 15.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(201.93382 453.106004) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_96\">\n+    <!-- 15.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(202.442985 468.668109) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-Oblique-31\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-35\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-39\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_97\">\n+    <!-- 16.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(205.57857 484.230214) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_98\">\n+    <!-- 16.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(206.057717 499.79232) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_99\">\n+    <!-- 16.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(206.421054 515.354425) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_100\">\n+    <!-- 18.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(210.951607 530.91653) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_101\">\n+    <!-- 18.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(211.360637 546.478636) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-31\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_102\">\n+    <!-- 20.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(219.793244 562.040741) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-32\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_103\">\n+    <!-- 23.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(229.317129 577.602846) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-32\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_104\">\n+    <!-- 23.5 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(229.413747 593.164951) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-32\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_105\">\n+    <!-- 24.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(232.755125 608.727057) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-32\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_106\">\n+    <!-- 24.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(233.223049 624.289162) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-32\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_107\">\n+    <!-- 27.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(240.692179 639.851267) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-32\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_108\">\n+    <!-- 29.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(246.352537 655.413372) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-32\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_109\">\n+    <!-- 30.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(248.139785 670.975478) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-33\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_110\">\n+    <!-- 32.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(251.06098 686.537583) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-33\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_111\">\n+    <!-- 34.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(256.767203 702.099688) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-33\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_112\">\n+    <!-- 36.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(259.321422 717.661793) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-33\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_113\">\n+    <!-- 37.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(261.875533 733.223899) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-33\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_114\">\n+    <!-- 38.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(264.661793 748.786004) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-33\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_115\">\n+    <!-- 39.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(266.261406 764.348109) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-33\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_116\">\n+    <!-- 40.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(267.058807 779.910214) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_117\">\n+    <!-- 40.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(267.257472 795.47232) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_118\">\n+    <!-- 41.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(268.746971 811.034425) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_119\">\n+    <!-- 43.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(272.733921 826.59653) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_120\">\n+    <!-- 47.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(278.74016 842.158636) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_121\">\n+    <!-- 48.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(279.42414 857.720741) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_122\">\n+    <!-- 51.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(283.831061 873.282846) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-35\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_123\">\n+    <!-- 51.5 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(284.227721 888.844951) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-35\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_124\">\n+    <!-- 55.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(289.707261 904.407057) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-35\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+    </g>\n+   </g>\n+  </g>\n+  <g id=\"axes_2\">\n+   <g id=\"patch_126\">\n+    <path d=\"M 418.6 909.358125 \n+L 651.1 909.358125 \n+L 651.1 22.318125 \n+L 418.6 22.318125 \n+z\n+\" style=\"fill: #ffffff\"/>\n+   </g>\n+   <g id=\"matplotlib.axis_3\">\n+    <g id=\"xtick_16\">\n+     <g id=\"line2d_192\">\n+      <path d=\"M 433.853709 909.358125 \n+L 433.853709 22.318125 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+     </g>\n+     <g id=\"line2d_193\">\n+      <g>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"433.853709\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_125\">\n+      <!-- 2.5 -->\n+      <g transform=\"translate(425.902147 923.956562) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-32\"/>\n+       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_17\">\n+     <g id=\"line2d_194\">\n+      <path d=\"M 481.236058 909.358125 \n+L 481.236058 22.318125 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+     </g>\n+     <g id=\"line2d_195\">\n+      <g>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"481.236058\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_126\">\n+      <!-- 5 -->\n+      <g transform=\"translate(478.054808 923.956562) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-35\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_18\">\n+     <g id=\"line2d_196\">\n+      <path d=\"M 528.618406 909.358125 \n+L 528.618406 22.318125 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+     </g>\n+     <g id=\"line2d_197\">\n+      <g>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"528.618406\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_127\">\n+      <!-- 10 -->\n+      <g transform=\"translate(522.255906 923.956562) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-31\"/>\n+       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_19\">\n+     <g id=\"line2d_198\">\n+      <path d=\"M 576.000755 909.358125 \n+L 576.000755 22.318125 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+     </g>\n+     <g id=\"line2d_199\">\n+      <g>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"576.000755\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_128\">\n+      <!-- 20 -->\n+      <g transform=\"translate(569.638255 923.956562) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-32\"/>\n+       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_20\">\n+     <g id=\"line2d_200\">\n+      <path d=\"M 623.383103 909.358125 \n+L 623.383103 22.318125 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n+     </g>\n+     <g id=\"line2d_201\">\n+      <g>\n+       <use xlink:href=\"#m40feb81c4c\" x=\"623.383103\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_129\">\n+      <!-- 40 -->\n+      <g transform=\"translate(617.020603 923.956562) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-34\"/>\n+       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_21\">\n+     <g id=\"line2d_202\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"418.6\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_22\">\n+     <g id=\"line2d_203\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"446.316897\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_23\">\n+     <g id=\"line2d_204\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"465.982348\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_24\">\n+     <g id=\"line2d_205\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"493.699245\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_25\">\n+     <g id=\"line2d_206\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"504.236721\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_26\">\n+     <g id=\"line2d_207\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"513.364697\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_27\">\n+     <g id=\"line2d_208\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"521.416143\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_28\">\n+     <g id=\"line2d_209\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"603.717652\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_29\">\n+     <g id=\"line2d_210\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"638.636812\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"xtick_30\">\n+     <g id=\"line2d_211\">\n+      <g>\n+       <use xlink:href=\"#mc7c5720e90\" x=\"651.1\" y=\"909.358125\" style=\"stroke: #000000; stroke-width: 0.6\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"text_130\">\n+     <!-- WER or $CER$ (%) -->\n+     <g transform=\"translate(494.6 937.634687) scale(0.1 -0.1)\">\n+      <use xlink:href=\"#DejaVuSans-57\" transform=\"translate(0 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-45\" transform=\"translate(98.876953 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-52\" transform=\"translate(162.060547 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(231.542969 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(263.330078 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(324.511719 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(365.625 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-Oblique-43\" transform=\"translate(397.412109 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-Oblique-45\" transform=\"translate(467.236328 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-Oblique-52\" transform=\"translate(530.419922 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(599.902344 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(631.689453 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-25\" transform=\"translate(670.703125 0.125)\"/>\n+      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(765.722656 0.125)\"/>\n+     </g>\n+    </g>\n+   </g>\n+   <g id=\"matplotlib.axis_4\">\n+    <g id=\"ytick_58\">\n+     <g id=\"line2d_212\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"29.588945\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_131\">\n+      <!-- Spanish -->\n+      <g transform=\"translate(372.1125 33.388163) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-70\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"126.953125\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"188.232422\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"251.611328\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"279.394531\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"331.494141\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_59\">\n+     <g id=\"line2d_213\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"44.130584\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_132\">\n+      <!-- Italian -->\n+      <g transform=\"translate(380.579688 47.929803) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-49\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"29.492188\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"68.701172\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"129.980469\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"157.763672\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"185.546875\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"246.826172\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_60\">\n+     <g id=\"line2d_214\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"58.672223\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_133\">\n+      <!-- $Korean$ -->\n+      <g transform=\"translate(376.1 62.471442) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-Oblique-4b\" transform=\"translate(0 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6f\" transform=\"translate(65.576172 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-72\" transform=\"translate(126.757812 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(167.871094 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(229.394531 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(290.673828 0.09375)\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_61\">\n+     <g id=\"line2d_215\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"73.213863\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_134\">\n+      <!-- Portuguese -->\n+      <g transform=\"translate(355.24375 77.013081) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-50\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"117.859375\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"158.972656\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"198.181641\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"261.560547\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"325.037109\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"388.416016\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"449.939453\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"502.039062\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_62\">\n+     <g id=\"line2d_216\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"87.755502\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_135\">\n+      <!-- English -->\n+      <g transform=\"translate(375.492188 91.554721) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-45\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"63.183594\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"126.5625\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"190.039062\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"217.822266\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"245.605469\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"297.705078\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_63\">\n+     <g id=\"line2d_217\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"102.297141\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_136\">\n+      <!-- Polish -->\n+      <g transform=\"translate(382.710938 106.09636) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-50\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"56.677734\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"117.859375\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"145.642578\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"173.425781\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"225.525391\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_64\">\n+     <g id=\"line2d_218\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"116.838781\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_137\">\n+      <!-- Catalan -->\n+      <g transform=\"translate(373.196875 120.637999) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-43\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"69.824219\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"131.103516\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"170.3125\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"231.591797\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"259.375\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"320.654297\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_65\">\n+     <g id=\"line2d_219\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"131.38042\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_138\">\n+      <!-- $Japanese$ -->\n+      <g transform=\"translate(366.1 135.169483) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-Oblique-4a\" transform=\"translate(0 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(29.492188 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-70\" transform=\"translate(90.771484 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(154.248047 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(215.527344 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(278.90625 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-73\" transform=\"translate(340.429688 0.09375)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-65\" transform=\"translate(392.529297 0.09375)\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_66\">\n+     <g id=\"line2d_220\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"145.922059\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_139\">\n+      <!-- German -->\n+      <g transform=\"translate(371.55625 149.721278) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-47\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"77.490234\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"139.013672\"/>\n+       <use xlink:href=\"#DejaVuSans-6d\" x=\"178.376953\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"275.789062\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"337.068359\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_67\">\n+     <g id=\"line2d_221\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"160.463699\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_140\">\n+      <!-- Russian -->\n+      <g transform=\"translate(373.101563 164.262918) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-52\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"64.982422\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"128.361328\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"180.460938\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"232.560547\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"260.34375\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"321.623047\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_68\">\n+     <g id=\"line2d_222\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"175.005338\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_141\">\n+      <!-- Dutch -->\n+      <g transform=\"translate(381.80625 178.804557) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-44\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"77.001953\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"140.380859\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"179.589844\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"234.570312\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_69\">\n+     <g id=\"line2d_223\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"189.546977\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_142\">\n+      <!-- French -->\n+      <g transform=\"translate(378.360938 193.346196) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-46\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"50.269531\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"89.132812\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"150.65625\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"214.035156\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"269.015625\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_70\">\n+     <g id=\"line2d_224\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"204.088617\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_143\">\n+      <!-- Indonesian -->\n+      <g transform=\"translate(356.901563 207.887836) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-49\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"29.492188\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"92.871094\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"156.347656\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"217.529297\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"280.908203\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"342.431641\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"394.53125\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"422.314453\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"483.59375\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_71\">\n+     <g id=\"line2d_225\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"218.630256\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_144\">\n+      <!-- Ukrainian -->\n+      <g transform=\"translate(363.892188 222.429475) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-55\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"73.193359\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"131.103516\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"172.216797\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"233.496094\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"261.279297\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"324.658203\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"352.441406\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"413.720703\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_72\">\n+     <g id=\"line2d_226\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"233.171895\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_145\">\n+      <!-- Turkish -->\n+      <g transform=\"translate(376.440625 236.971114) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-54\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"45.958984\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"109.337891\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"150.451172\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"208.361328\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"236.144531\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"288.244141\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_73\">\n+     <g id=\"line2d_227\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"247.713535\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_146\">\n+      <!-- Malay -->\n+      <g transform=\"translate(382.01875 251.512754) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4d\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"86.279297\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"147.558594\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"175.341797\"/>\n+       <use xlink:href=\"#DejaVuSans-79\" x=\"236.621094\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_74\">\n+     <g id=\"line2d_228\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"262.255174\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_147\">\n+      <!-- Swedish -->\n+      <g transform=\"translate(370.246875 266.054393) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-77\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"145.263672\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"206.787109\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"270.263672\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"298.046875\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"350.146484\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_75\">\n+     <g id=\"line2d_229\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"276.796814\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_148\">\n+      <!-- $Mandarin$ -->\n+      <g transform=\"translate(364.8 280.596032) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-Oblique-4d\" transform=\"translate(0 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(86.279297 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(147.558594 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-64\" transform=\"translate(210.9375 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(274.414062 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-72\" transform=\"translate(335.693359 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-69\" transform=\"translate(376.806641 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-6e\" transform=\"translate(404.589844 0.015625)\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_76\">\n+     <g id=\"line2d_230\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"291.338453\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_149\">\n+      <!-- Finnish -->\n+      <g transform=\"translate(376.795313 295.137672) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-46\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"50.269531\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"78.052734\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"141.431641\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"204.810547\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"232.59375\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"284.693359\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_77\">\n+     <g id=\"line2d_231\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"305.880092\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_150\">\n+      <!-- Norwegian -->\n+      <g transform=\"translate(357.965625 309.679311) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4e\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"74.804688\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"135.986328\"/>\n+       <use xlink:href=\"#DejaVuSans-77\" x=\"177.099609\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"258.886719\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"320.410156\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"383.886719\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"411.669922\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"472.949219\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_78\">\n+     <g id=\"line2d_232\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"320.421732\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_151\">\n+      <!-- Romanian -->\n+      <g transform=\"translate(361.532813 324.22095) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-52\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"64.982422\"/>\n+       <use xlink:href=\"#DejaVuSans-6d\" x=\"126.164062\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"223.576172\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"284.855469\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"348.234375\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"376.017578\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"437.296875\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_79\">\n+     <g id=\"line2d_233\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"334.963371\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_152\">\n+      <!-- $Thai$ -->\n+      <g transform=\"translate(390.2 338.76259) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-Oblique-54\" transform=\"translate(0 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-68\" transform=\"translate(61.083984 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-61\" transform=\"translate(124.462891 0.015625)\"/>\n+       <use xlink:href=\"#DejaVuSans-Oblique-69\" transform=\"translate(185.742188 0.015625)\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_80\">\n+     <g id=\"line2d_234\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"349.50501\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_153\">\n+      <!-- Vietnamese -->\n+      <g transform=\"translate(352.410938 353.304229) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-56\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"66.158203\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"93.941406\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"155.464844\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"194.673828\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"258.052734\"/>\n+       <use xlink:href=\"#DejaVuSans-6d\" x=\"319.332031\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"416.744141\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"478.267578\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"530.367188\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_81\">\n+     <g id=\"line2d_235\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"364.04665\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_154\">\n+      <!-- Slovak -->\n+      <g transform=\"translate(378.517188 367.845868) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"91.259766\"/>\n+       <use xlink:href=\"#DejaVuSans-76\" x=\"152.441406\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"211.621094\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"272.900391\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_82\">\n+     <g id=\"line2d_236\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"378.588289\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_155\">\n+      <!-- Arabic -->\n+      <g transform=\"translate(379.895313 382.387508) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-41\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"68.408203\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"109.521484\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"170.800781\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"234.277344\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"262.060547\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_83\">\n+     <g id=\"line2d_237\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"393.129928\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_156\">\n+      <!-- Czech -->\n+      <g transform=\"translate(381.379688 396.929147) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-43\"/>\n+       <use xlink:href=\"#DejaVuSans-7a\" x=\"69.824219\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"122.314453\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"183.837891\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"238.818359\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_84\">\n+     <g id=\"line2d_238\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"407.671568\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_157\">\n+      <!-- Croatian -->\n+      <g transform=\"translate(369.320313 411.470786) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-43\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"69.824219\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"108.6875\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"169.869141\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"231.148438\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"270.357422\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"298.140625\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"359.419922\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_85\">\n+     <g id=\"line2d_239\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"422.213207\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_158\">\n+      <!-- Greek -->\n+      <g transform=\"translate(381.86875 426.012426) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-47\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"77.490234\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"116.353516\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"177.876953\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"239.400391\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_86\">\n+     <g id=\"line2d_240\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"436.754846\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_159\">\n+      <!-- Serbian -->\n+      <g transform=\"translate(373.395313 440.554065) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"125\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"166.113281\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"229.589844\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"257.373047\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"318.652344\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_87\">\n+     <g id=\"line2d_241\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"451.296486\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_160\">\n+      <!-- Danish -->\n+      <g transform=\"translate(377.109375 455.095704) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-44\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"77.001953\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"138.28125\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"201.660156\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"229.443359\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"281.542969\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_88\">\n+     <g id=\"line2d_242\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"465.838125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_161\">\n+      <!-- Bulgarian -->\n+      <g transform=\"translate(363.792188 469.637344) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-42\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"68.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"131.982422\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"159.765625\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"223.242188\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"284.521484\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"325.634766\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"353.417969\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"414.697266\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_89\">\n+     <g id=\"line2d_243\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"480.379764\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_162\">\n+      <!-- Hungarian -->\n+      <g transform=\"translate(359.573438 484.178983) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-48\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"75.195312\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"138.574219\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"201.953125\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"265.429688\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"326.708984\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"367.822266\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"395.605469\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"456.884766\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_90\">\n+     <g id=\"line2d_244\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"494.921404\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_163\">\n+      <!-- Filipino -->\n+      <g transform=\"translate(376.65625 498.720622) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-46\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"50.269531\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"78.052734\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"105.835938\"/>\n+       <use xlink:href=\"#DejaVuSans-70\" x=\"133.619141\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"197.095703\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"224.878906\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"288.257812\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_91\">\n+     <g id=\"line2d_245\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"509.463043\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_164\">\n+      <!-- Bosnian -->\n+      <g transform=\"translate(371.829688 513.262262) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-42\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"68.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"129.785156\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"181.884766\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"245.263672\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"273.046875\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"334.326172\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_92\">\n+     <g id=\"line2d_246\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"524.004682\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_165\">\n+      <!-- Galician -->\n+      <g transform=\"translate(371.425 527.803901) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-47\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"77.490234\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"138.769531\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"166.552734\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"194.335938\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"249.316406\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"277.099609\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"338.378906\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_93\">\n+     <g id=\"line2d_247\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"538.546322\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_166\">\n+      <!-- Macedonian -->\n+      <g transform=\"translate(351.14375 542.34554) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4d\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"86.279297\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"147.558594\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"202.539062\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"264.0625\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"327.539062\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"388.720703\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"452.099609\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"479.882812\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"541.162109\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_94\">\n+     <g id=\"line2d_248\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"553.087961\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_167\">\n+      <!-- Hindi -->\n+      <g transform=\"translate(385.8375 556.88718) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-48\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"75.195312\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"102.978516\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"166.357422\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"229.833984\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_95\">\n+     <g id=\"line2d_249\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"567.6296\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_168\">\n+      <!-- Estonian -->\n+      <g transform=\"translate(368.451563 571.428819) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-45\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"63.183594\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"115.283203\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"154.492188\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"215.673828\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"279.052734\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"306.835938\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"368.115234\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_96\">\n+     <g id=\"line2d_250\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"582.17124\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_169\">\n+      <!-- Slovenian -->\n+      <g transform=\"translate(362.701563 585.970459) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"91.259766\"/>\n+       <use xlink:href=\"#DejaVuSans-76\" x=\"152.441406\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"211.621094\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"273.144531\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"336.523438\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"364.306641\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"425.585938\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_97\">\n+     <g id=\"line2d_251\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"596.712879\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_170\">\n+      <!-- Tamil -->\n+      <g transform=\"translate(385.717188 600.512098) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-54\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n+       <use xlink:href=\"#DejaVuSans-6d\" x=\"105.863281\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"203.275391\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"231.058594\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_98\">\n+     <g id=\"line2d_252\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"611.254518\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_171\">\n+      <!-- Latvian -->\n+      <g transform=\"translate(374.817188 615.053737) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4c\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"55.712891\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"116.992188\"/>\n+       <use xlink:href=\"#DejaVuSans-76\" x=\"156.201172\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"215.380859\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"243.164062\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"304.443359\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_99\">\n+     <g id=\"line2d_253\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"625.796158\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_172\">\n+      <!-- Azerbaijani -->\n+      <g transform=\"translate(355.970313 629.595377) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-41\"/>\n+       <use xlink:href=\"#DejaVuSans-7a\" x=\"68.408203\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"120.898438\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"182.421875\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"223.535156\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"287.011719\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"348.291016\"/>\n+       <use xlink:href=\"#DejaVuSans-6a\" x=\"376.074219\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"403.857422\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"465.136719\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"528.515625\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_100\">\n+     <g id=\"line2d_254\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"640.337797\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_173\">\n+      <!-- Urdu -->\n+      <g transform=\"translate(387.659375 644.137016) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-55\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"73.193359\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"112.556641\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"176.033203\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_101\">\n+     <g id=\"line2d_255\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"654.879436\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_174\">\n+      <!-- Lithuanian -->\n+      <g transform=\"translate(358.945313 658.678655) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4c\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"55.712891\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"83.496094\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"122.705078\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"186.083984\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"249.462891\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"310.742188\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"374.121094\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"401.904297\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"463.183594\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_102\">\n+     <g id=\"line2d_256\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"669.421076\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_175\">\n+      <!-- Hebrew -->\n+      <g transform=\"translate(373.360938 673.220295) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-48\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"75.195312\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"136.71875\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"200.195312\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"239.058594\"/>\n+       <use xlink:href=\"#DejaVuSans-77\" x=\"300.582031\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_103\">\n+     <g id=\"line2d_257\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"683.962715\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_176\">\n+      <!-- Welsh -->\n+      <g transform=\"translate(381.821875 687.761934) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-57\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"93.001953\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"154.525391\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"182.308594\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"234.408203\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_104\">\n+     <g id=\"line2d_258\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"698.504355\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_177\">\n+      <!-- Persian -->\n+      <g transform=\"translate(375.215625 702.303573) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-50\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"56.677734\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"118.201172\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"159.314453\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"211.414062\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"239.197266\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"300.476562\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_105\">\n+     <g id=\"line2d_259\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"713.045994\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_178\">\n+      <!-- Icelandic -->\n+      <g transform=\"translate(367.129688 716.845213) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-49\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"29.492188\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"84.472656\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"145.996094\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"173.779297\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"235.058594\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"298.4375\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"361.914062\"/>\n+       <use xlink:href=\"#DejaVuSans-63\" x=\"389.697266\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_106\">\n+     <g id=\"line2d_260\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"727.587633\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_179\">\n+      <!-- Kazakh -->\n+      <g transform=\"translate(375.584375 731.386852) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4b\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"63.826172\"/>\n+       <use xlink:href=\"#DejaVuSans-7a\" x=\"125.105469\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"177.595703\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"238.875\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"296.785156\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_107\">\n+     <g id=\"line2d_261\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"742.129273\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_180\">\n+      <!-- Afrikaans -->\n+      <g transform=\"translate(365.29375 745.928491) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-41\"/>\n+       <use xlink:href=\"#DejaVuSans-66\" x=\"64.783203\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"99.988281\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"141.101562\"/>\n+       <use xlink:href=\"#DejaVuSans-6b\" x=\"168.884766\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"225.044922\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"286.324219\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"347.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"410.982422\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_108\">\n+     <g id=\"line2d_262\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"756.670912\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_181\">\n+      <!-- Kannada -->\n+      <g transform=\"translate(367.809375 760.470131) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4b\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"63.826172\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"125.105469\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"188.484375\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"251.863281\"/>\n+       <use xlink:href=\"#DejaVuSans-64\" x=\"313.142578\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"376.619141\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_109\">\n+     <g id=\"line2d_263\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"771.212551\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_182\">\n+      <!-- Marathi -->\n+      <g transform=\"translate(373.56875 775.01177) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4d\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"86.279297\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"147.558594\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"188.671875\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"249.951172\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"289.160156\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"352.539062\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_110\">\n+     <g id=\"line2d_264\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"785.754191\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_183\">\n+      <!-- Swahili -->\n+      <g transform=\"translate(376.273438 789.553409) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-53\"/>\n+       <use xlink:href=\"#DejaVuSans-77\" x=\"63.476562\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"145.263672\"/>\n+       <use xlink:href=\"#DejaVuSans-68\" x=\"206.542969\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"269.921875\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"297.705078\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"325.488281\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_111\">\n+     <g id=\"line2d_265\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"800.29583\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_184\">\n+      <!-- Telugu -->\n+      <g transform=\"translate(379.2375 804.095049) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-54\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"44.083984\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"105.607422\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"133.390625\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"196.769531\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"260.246094\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_112\">\n+     <g id=\"line2d_266\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"814.837469\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_185\">\n+      <!-- Maori -->\n+      <g transform=\"translate(383.835938 818.636688) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4d\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"86.279297\"/>\n+       <use xlink:href=\"#DejaVuSans-6f\" x=\"147.558594\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"208.740234\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"249.853516\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_113\">\n+     <g id=\"line2d_267\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"829.379109\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_186\">\n+      <!-- Nepali -->\n+      <g transform=\"translate(379.932813 833.178327) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-4e\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"74.804688\"/>\n+       <use xlink:href=\"#DejaVuSans-70\" x=\"136.328125\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"199.804688\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"261.083984\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"288.867188\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_114\">\n+     <g id=\"line2d_268\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"843.920748\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_187\">\n+      <!-- Armenian -->\n+      <g transform=\"translate(363.348438 847.719967) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-41\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"68.408203\"/>\n+       <use xlink:href=\"#DejaVuSans-6d\" x=\"107.771484\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"205.183594\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"266.707031\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"330.085938\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"357.869141\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"419.148438\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_115\">\n+     <g id=\"line2d_269\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"858.462387\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_188\">\n+      <!-- Belarusian -->\n+      <g transform=\"translate(358.778125 862.261606) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-42\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"68.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"130.126953\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"157.910156\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"219.189453\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"260.302734\"/>\n+       <use xlink:href=\"#DejaVuSans-73\" x=\"323.681641\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"375.78125\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"403.564453\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"464.84375\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_116\">\n+     <g id=\"line2d_270\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"873.004027\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_189\">\n+      <!-- Gujarati -->\n+      <g transform=\"translate(371.670313 876.803245) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-47\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"77.490234\"/>\n+       <use xlink:href=\"#DejaVuSans-6a\" x=\"140.869141\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"168.652344\"/>\n+       <use xlink:href=\"#DejaVuSans-72\" x=\"229.931641\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"271.044922\"/>\n+       <use xlink:href=\"#DejaVuSans-74\" x=\"332.324219\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"371.533203\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_117\">\n+     <g id=\"line2d_271\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"887.545666\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_190\">\n+      <!-- Punjabi -->\n+      <g transform=\"translate(375.0375 891.344885) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-50\"/>\n+       <use xlink:href=\"#DejaVuSans-75\" x=\"58.552734\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"121.931641\"/>\n+       <use xlink:href=\"#DejaVuSans-6a\" x=\"185.310547\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"213.09375\"/>\n+       <use xlink:href=\"#DejaVuSans-62\" x=\"274.373047\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"337.849609\"/>\n+      </g>\n+     </g>\n+    </g>\n+    <g id=\"ytick_118\">\n+     <g id=\"line2d_272\">\n+      <g>\n+       <use xlink:href=\"#m4a569c92d6\" x=\"418.6\" y=\"902.087305\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n+      </g>\n+     </g>\n+     <g id=\"text_191\">\n+      <!-- Bengali -->\n+      <g transform=\"translate(374.215625 905.886524) scale(0.1 -0.1)\">\n+       <use xlink:href=\"#DejaVuSans-42\"/>\n+       <use xlink:href=\"#DejaVuSans-65\" x=\"68.603516\"/>\n+       <use xlink:href=\"#DejaVuSans-6e\" x=\"130.126953\"/>\n+       <use xlink:href=\"#DejaVuSans-67\" x=\"193.505859\"/>\n+       <use xlink:href=\"#DejaVuSans-61\" x=\"256.982422\"/>\n+       <use xlink:href=\"#DejaVuSans-6c\" x=\"318.261719\"/>\n+       <use xlink:href=\"#DejaVuSans-69\" x=\"346.044922\"/>\n+      </g>\n+     </g>\n+    </g>\n+   </g>\n+   <g id=\"line2d_273\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_274\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_275\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_276\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_277\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_278\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_279\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_280\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_281\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_282\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_283\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_284\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_285\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_286\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_287\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_288\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_289\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_290\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_291\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_292\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_293\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_294\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_295\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_296\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_297\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_298\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_299\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_300\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_301\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_302\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_303\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_304\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_305\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_306\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_307\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_308\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_309\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_310\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_311\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_312\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_313\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_314\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_315\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_316\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_317\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_318\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_319\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_320\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_321\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_322\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_323\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_324\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_325\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_326\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_327\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_328\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_329\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_330\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_331\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_332\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_333\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_334\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_335\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_336\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_337\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_338\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_339\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_340\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_341\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_342\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_343\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_344\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_345\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_346\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_347\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_348\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_349\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_350\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_351\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_352\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_353\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_354\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_355\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_356\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_357\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_358\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_359\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_360\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_361\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_362\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_363\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_364\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_365\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_366\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_367\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_368\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_369\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_370\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_371\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_372\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_373\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_374\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_375\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_376\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_377\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_378\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_379\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_380\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_381\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_382\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_383\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_384\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_385\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_386\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_387\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_388\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_389\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_390\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_391\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_392\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_393\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"line2d_394\">\n+    <path clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #424242; stroke-width: 2.25; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"patch_127\">\n+    <path d=\"M 418.6 909.358125 \n+L 418.6 22.318125 \n+\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"patch_128\">\n+    <path d=\"M 651.1 909.358125 \n+L 651.1 22.318125 \n+\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"patch_129\">\n+    <path d=\"M 418.6 909.358125 \n+L 651.1 909.358125 \n+\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"patch_130\">\n+    <path d=\"M 418.6 22.318125 \n+L 651.1 22.318125 \n+\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   </g>\n+   <g id=\"text_192\">\n+    <!-- FLEURS -->\n+    <g transform=\"translate(511.895313 16.318125) scale(0.12 -0.12)\">\n+     <use xlink:href=\"#DejaVuSans-46\"/>\n+     <use xlink:href=\"#DejaVuSans-4c\" x=\"57.519531\"/>\n+     <use xlink:href=\"#DejaVuSans-45\" x=\"113.232422\"/>\n+     <use xlink:href=\"#DejaVuSans-55\" x=\"176.416016\"/>\n+     <use xlink:href=\"#DejaVuSans-52\" x=\"249.609375\"/>\n+     <use xlink:href=\"#DejaVuSans-53\" x=\"319.091797\"/>\n+    </g>\n+   </g>\n+   <g id=\"legend_2\">\n+    <g id=\"patch_131\">\n+     <path d=\"M 571.129688 74.3525 \n+L 644.1 74.3525 \n+Q 646.1 74.3525 646.1 72.3525 \n+L 646.1 29.318125 \n+Q 646.1 27.318125 644.1 27.318125 \n+L 571.129688 27.318125 \n+Q 569.129688 27.318125 569.129688 29.318125 \n+L 569.129688 72.3525 \n+Q 569.129688 74.3525 571.129688 74.3525 \n+z\n+\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n+    </g>\n+    <g id=\"text_193\">\n+     <!-- Model -->\n+     <g transform=\"translate(592.601563 38.916562) scale(0.1 -0.1)\">\n+      <use xlink:href=\"#DejaVuSans-4d\"/>\n+      <use xlink:href=\"#DejaVuSans-6f\" x=\"86.279297\"/>\n+      <use xlink:href=\"#DejaVuSans-64\" x=\"147.460938\"/>\n+      <use xlink:href=\"#DejaVuSans-65\" x=\"210.9375\"/>\n+      <use xlink:href=\"#DejaVuSans-6c\" x=\"272.460938\"/>\n+     </g>\n+    </g>\n+    <g id=\"patch_132\">\n+     <path d=\"M 573.129688 53.594687 \n+L 593.129688 53.594687 \n+L 593.129688 46.594687 \n+L 573.129688 46.594687 \n+z\n+\" style=\"fill: #334b4b\"/>\n+    </g>\n+    <g id=\"text_194\">\n+     <!-- large-v3 -->\n+     <g transform=\"translate(601.129688 53.594687) scale(0.1 -0.1)\">\n+      <use xlink:href=\"#DejaVuSans-6c\"/>\n+      <use xlink:href=\"#DejaVuSans-61\" x=\"27.783203\"/>\n+      <use xlink:href=\"#DejaVuSans-72\" x=\"89.0625\"/>\n+      <use xlink:href=\"#DejaVuSans-67\" x=\"128.425781\"/>\n+      <use xlink:href=\"#DejaVuSans-65\" x=\"191.902344\"/>\n+      <use xlink:href=\"#DejaVuSans-2d\" x=\"253.425781\"/>\n+      <use xlink:href=\"#DejaVuSans-76\" x=\"286.884766\"/>\n+      <use xlink:href=\"#DejaVuSans-33\" x=\"346.064453\"/>\n+     </g>\n+    </g>\n+    <g id=\"patch_133\">\n+     <path d=\"M 573.129688 68.272812 \n+L 593.129688 68.272812 \n+L 593.129688 61.272812 \n+L 573.129688 61.272812 \n+z\n+\" style=\"fill: #ffffff; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+    </g>\n+    <g id=\"text_195\">\n+     <!-- large-v2 -->\n+     <g transform=\"translate(601.129688 68.272812) scale(0.1 -0.1)\">\n+      <use xlink:href=\"#DejaVuSans-6c\"/>\n+      <use xlink:href=\"#DejaVuSans-61\" x=\"27.783203\"/>\n+      <use xlink:href=\"#DejaVuSans-72\" x=\"89.0625\"/>\n+      <use xlink:href=\"#DejaVuSans-67\" x=\"128.425781\"/>\n+      <use xlink:href=\"#DejaVuSans-65\" x=\"191.902344\"/>\n+      <use xlink:href=\"#DejaVuSans-2d\" x=\"253.425781\"/>\n+      <use xlink:href=\"#DejaVuSans-76\" x=\"286.884766\"/>\n+      <use xlink:href=\"#DejaVuSans-32\" x=\"346.064453\"/>\n+     </g>\n+    </g>\n+   </g>\n+   <g id=\"patch_134\">\n+    <path d=\"M -157029.536861 23.772289 \n+L 442.230083 23.772289 \n+L 442.230083 35.4056 \n+L -157029.536861 35.4056 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_135\">\n+    <path d=\"M -157029.536861 38.313928 \n+L 445.845526 38.313928 \n+L 445.845526 49.94724 \n+L -157029.536861 49.94724 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_136\">\n+    <path d=\"M -157029.536861 52.855568 \n+L 449.247575 52.855568 \n+L 449.247575 64.488879 \n+L -157029.536861 64.488879 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_137\">\n+    <path d=\"M -157029.536861 67.397207 \n+L 466.940237 67.397207 \n+L 466.940237 79.030518 \n+L -157029.536861 79.030518 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_138\">\n+    <path d=\"M -157029.536861 81.938846 \n+L 467.803367 81.938846 \n+L 467.803367 93.572158 \n+L -157029.536861 93.572158 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_139\">\n+    <path d=\"M -157029.536861 96.480486 \n+L 474.830013 96.480486 \n+L 474.830013 108.113797 \n+L -157029.536861 108.113797 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_140\">\n+    <path d=\"M -157029.536861 111.022125 \n+L 478.566282 111.022125 \n+L 478.566282 122.655436 \n+L -157029.536861 122.655436 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_141\">\n+    <path d=\"M -157029.536861 125.563764 \n+L 479.264865 125.563764 \n+L 479.264865 137.197076 \n+L -157029.536861 137.197076 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_142\">\n+    <path d=\"M -157029.536861 140.105404 \n+L 480.001355 140.105404 \n+L 480.001355 151.738715 \n+L -157029.536861 151.738715 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_143\">\n+    <path d=\"M -157029.536861 154.647043 \n+L 480.768268 154.647043 \n+L 480.768268 166.280355 \n+L -157029.536861 166.280355 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_144\">\n+    <path d=\"M -157029.536861 169.188682 \n+L 484.440504 169.188682 \n+L 484.440504 180.821994 \n+L -157029.536861 180.821994 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_145\">\n+    <path d=\"M -157029.536861 183.730322 \n+L 484.878997 183.730322 \n+L 484.878997 195.363633 \n+L -157029.536861 195.363633 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_146\">\n+    <path d=\"M -157029.536861 198.271961 \n+L 494.318179 198.271961 \n+L 494.318179 209.905273 \n+L -157029.536861 209.905273 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_147\">\n+    <path d=\"M -157029.536861 212.8136 \n+L 497.762243 212.8136 \n+L 497.762243 224.446912 \n+L -157029.536861 224.446912 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_148\">\n+    <path d=\"M -157029.536861 227.35524 \n+L 501.485115 227.35524 \n+L 501.485115 238.988551 \n+L -157029.536861 238.988551 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_149\">\n+    <path d=\"M -157029.536861 241.896879 \n+L 506.857566 241.896879 \n+L 506.857566 253.530191 \n+L -157029.536861 253.530191 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_150\">\n+    <path d=\"M -157029.536861 256.438518 \n+L 509.892739 256.438518 \n+L 509.892739 268.07183 \n+L -157029.536861 268.07183 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_151\">\n+    <path d=\"M -157029.536861 270.980158 \n+L 510.341518 270.980158 \n+L 510.341518 282.613469 \n+L -157029.536861 282.613469 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_152\">\n+    <path d=\"M -157029.536861 285.521797 \n+L 510.983322 285.521797 \n+L 510.983322 297.155109 \n+L -157029.536861 297.155109 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_153\">\n+    <path d=\"M -157029.536861 300.063436 \n+L 511.831346 300.063436 \n+L 511.831346 311.696748 \n+L -157029.536861 311.696748 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_154\">\n+    <path d=\"M -157029.536861 314.605076 \n+L 515.268725 314.605076 \n+L 515.268725 326.238387 \n+L -157029.536861 326.238387 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_155\">\n+    <path d=\"M -157029.536861 329.146715 \n+L 517.005127 329.146715 \n+L 517.005127 340.780027 \n+L -157029.536861 340.780027 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_156\">\n+    <path d=\"M -157029.536861 343.688355 \n+L 518.652131 343.688355 \n+L 518.652131 355.321666 \n+L -157029.536861 355.321666 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_157\">\n+    <path d=\"M -157029.536861 358.229994 \n+L 522.696949 358.229994 \n+L 522.696949 369.863305 \n+L -157029.536861 369.863305 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_158\">\n+    <path d=\"M -157029.536861 372.771633 \n+L 525.74249 372.771633 \n+L 525.74249 384.404945 \n+L -157029.536861 384.404945 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_159\">\n+    <path d=\"M -157029.536861 387.313273 \n+L 529.249494 387.313273 \n+L 529.249494 398.946584 \n+L -157029.536861 398.946584 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_160\">\n+    <path d=\"M -157029.536861 401.854912 \n+L 533.732694 401.854912 \n+L 533.732694 413.488223 \n+L -157029.536861 413.488223 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_161\">\n+    <path d=\"M -157029.536861 416.396551 \n+L 534.762801 416.396551 \n+L 534.762801 428.029863 \n+L -157029.536861 428.029863 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_162\">\n+    <path d=\"M -157029.536861 430.938191 \n+L 538.876054 430.938191 \n+L 538.876054 442.571502 \n+L -157029.536861 442.571502 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_163\">\n+    <path d=\"M -157029.536861 445.47983 \n+L 541.086116 445.47983 \n+L 541.086116 457.113141 \n+L -157029.536861 457.113141 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_164\">\n+    <path d=\"M -157029.536861 460.021469 \n+L 543.625657 460.021469 \n+L 543.625657 471.654781 \n+L -157029.536861 471.654781 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_165\">\n+    <path d=\"M -157029.536861 474.563109 \n+L 546.062415 474.563109 \n+L 546.062415 486.19642 \n+L -157029.536861 486.19642 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_166\">\n+    <path d=\"M -157029.536861 489.104748 \n+L 546.366274 489.104748 \n+L 546.366274 500.738059 \n+L -157029.536861 500.738059 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_167\">\n+    <path d=\"M -157029.536861 503.646387 \n+L 546.451441 503.646387 \n+L 546.451441 515.279699 \n+L -157029.536861 515.279699 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_168\">\n+    <path d=\"M -157029.536861 518.188027 \n+L 546.962996 518.188027 \n+L 546.962996 529.821338 \n+L -157029.536861 529.821338 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_169\">\n+    <path d=\"M -157029.536861 532.729666 \n+L 555.126085 532.729666 \n+L 555.126085 544.362977 \n+L -157029.536861 544.362977 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_170\">\n+    <path d=\"M -157029.536861 547.271305 \n+L 564.926607 547.271305 \n+L 564.926607 558.904617 \n+L -157029.536861 558.904617 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_171\">\n+    <path d=\"M -157029.536861 561.812945 \n+L 569.24621 561.812945 \n+L 569.24621 573.446256 \n+L -157029.536861 573.446256 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_172\">\n+    <path d=\"M -157029.536861 576.354584 \n+L 569.877608 576.354584 \n+L 569.877608 587.987895 \n+L -157029.536861 587.987895 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_173\">\n+    <path d=\"M -157029.536861 590.896223 \n+L 570.109085 590.896223 \n+L 570.109085 602.529535 \n+L -157029.536861 602.529535 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_174\">\n+    <path d=\"M -157029.536861 605.437863 \n+L 573.811012 605.437863 \n+L 573.811012 617.071174 \n+L -157029.536861 617.071174 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_175\">\n+    <path d=\"M -157029.536861 619.979502 \n+L 574.924527 619.979502 \n+L 574.924527 631.612814 \n+L -157029.536861 631.612814 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_176\">\n+    <path d=\"M -157029.536861 634.521141 \n+L 577.912999 634.521141 \n+L 577.912999 646.154453 \n+L -157029.536861 646.154453 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_177\">\n+    <path d=\"M -157029.536861 649.062781 \n+L 587.612142 649.062781 \n+L 587.612142 660.696092 \n+L -157029.536861 660.696092 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_178\">\n+    <path d=\"M -157029.536861 663.60442 \n+L 594.088825 663.60442 \n+L 594.088825 675.237732 \n+L -157029.536861 675.237732 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_179\">\n+    <path d=\"M -157029.536861 678.146059 \n+L 600.404611 678.146059 \n+L 600.404611 689.779371 \n+L -157029.536861 689.779371 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_180\">\n+    <path d=\"M -157029.536861 692.687699 \n+L 602.403425 692.687699 \n+L 602.403425 704.32101 \n+L -157029.536861 704.32101 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_181\">\n+    <path d=\"M -157029.536861 707.229338 \n+L 604.616106 707.229338 \n+L 604.616106 718.86265 \n+L -157029.536861 718.86265 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_182\">\n+    <path d=\"M -157029.536861 721.770977 \n+L 608.994593 721.770977 \n+L 608.994593 733.404289 \n+L -157029.536861 733.404289 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_183\">\n+    <path d=\"M -157029.536861 736.312617 \n+L 609.048915 736.312617 \n+L 609.048915 747.945928 \n+L -157029.536861 747.945928 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_184\">\n+    <path d=\"M -157029.536861 750.854256 \n+L 610.323536 750.854256 \n+L 610.323536 762.487568 \n+L -157029.536861 762.487568 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_185\">\n+    <path d=\"M -157029.536861 765.395895 \n+L 612.450614 765.395895 \n+L 612.450614 777.029207 \n+L -157029.536861 777.029207 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_186\">\n+    <path d=\"M -157029.536861 779.937535 \n+L 612.474808 779.937535 \n+L 612.474808 791.570846 \n+L -157029.536861 791.570846 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_187\">\n+    <path d=\"M -157029.536861 794.479174 \n+L 622.25659 794.479174 \n+L 622.25659 806.112486 \n+L -157029.536861 806.112486 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_188\">\n+    <path d=\"M -157029.536861 809.020814 \n+L 622.958708 809.020814 \n+L 622.958708 820.654125 \n+L -157029.536861 820.654125 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_189\">\n+    <path d=\"M -157029.536861 823.562453 \n+L 623.74092 823.562453 \n+L 623.74092 835.195764 \n+L -157029.536861 835.195764 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_190\">\n+    <path d=\"M -157029.536861 838.104092 \n+L 627.111129 838.104092 \n+L 627.111129 849.737404 \n+L -157029.536861 849.737404 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_191\">\n+    <path d=\"M -157029.536861 852.645732 \n+L 627.550077 852.645732 \n+L 627.550077 864.279043 \n+L -157029.536861 864.279043 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_192\">\n+    <path d=\"M -157029.536861 867.187371 \n+L 635.038382 867.187371 \n+L 635.038382 878.820682 \n+L -157029.536861 878.820682 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n+   </g>\n+   <g id=\"patch_193\">\n+    <path d=\"M -157029.536861 881.72901 \n+L 636.596602 881.72901 \n+L 636.596602 893.362322 \n+L -157029.536861 893.362322 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n    </g>\n-   <g id=\"patch_61\">\n-    <path d=\"M 646.334375 838.8 \n-L 646.334375 7.2 \n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   <g id=\"patch_194\">\n+    <path d=\"M -157029.536861 896.27065 \n+L 638.610851 896.27065 \n+L 638.610851 907.903961 \n+L -157029.536861 907.903961 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n    </g>\n-   <g id=\"patch_62\">\n-    <path d=\"M 88.334375 838.8 \n-L 646.334375 838.8 \n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   <g id=\"patch_195\">\n+    <path d=\"M -157029.536861 29.588945 \n+L -157029.536861 29.588945 \n+L -157029.536861 29.588945 \n+L -157029.536861 29.588945 \n+z\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #334b4b\"/>\n    </g>\n-   <g id=\"patch_63\">\n-    <path d=\"M 88.334375 7.2 \n-L 646.334375 7.2 \n-\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n+   <g id=\"patch_196\">\n+    <path d=\"M -1 23.772289 \n+L 449.130865 23.772289 \n+L 449.130865 35.4056 \n+L -1 35.4056 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n    </g>\n-   <g id=\"text_65\">\n-    <!-- 3.0 -->\n-    <g transform=\"translate(123.184246 17.254112)scale(0.1 -0.1)\">\n-     <defs>\n-      <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n-Q 3050 2419 3304 2112 \n-Q 3559 1806 3559 1356 \n-Q 3559 666 3084 287 \n-Q 2609 -91 1734 -91 \n-Q 1441 -91 1130 -33 \n-Q 819 25 488 141 \n-L 488 750 \n-Q 750 597 1062 519 \n-Q 1375 441 1716 441 \n-Q 2309 441 2620 675 \n-Q 2931 909 2931 1356 \n-Q 2931 1769 2642 2001 \n-Q 2353 2234 1838 2234 \n-L 1294 2234 \n-L 1294 2753 \n-L 1863 2753 \n-Q 2328 2753 2575 2939 \n-Q 2822 3125 2822 3475 \n-Q 2822 3834 2567 4026 \n-Q 2313 4219 1838 4219 \n-Q 1578 4219 1281 4162 \n-Q 984 4106 628 3988 \n-L 628 4550 \n-Q 988 4650 1302 4700 \n-Q 1616 4750 1894 4750 \n-Q 2613 4750 3031 4423 \n-Q 3450 4097 3450 3541 \n-Q 3450 3153 3228 2886 \n-Q 3006 2619 2597 2516 \n+   <g id=\"patch_197\">\n+    <path d=\"M -1 38.313928 \n+L 459.149451 38.313928 \n+L 459.149451 49.94724 \n+L -1 49.94724 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_198\">\n+    <path d=\"M -1 52.855568 \n+L 449.630535 52.855568 \n+L 449.630535 64.488879 \n+L -1 64.488879 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_199\">\n+    <path d=\"M -1 67.397207 \n+L 468.874669 67.397207 \n+L 468.874669 79.030518 \n+L -1 79.030518 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_200\">\n+    <path d=\"M -1 81.938846 \n+L 465.949469 81.938846 \n+L 465.949469 93.572158 \n+L -1 93.572158 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_201\">\n+    <path d=\"M -1 96.480486 \n+L 484.813613 96.480486 \n+L 484.813613 108.113797 \n+L -1 108.113797 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_202\">\n+    <path d=\"M -1 111.022125 \n+L 483.816339 111.022125 \n+L 483.816339 122.655436 \n+L -1 122.655436 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_203\">\n+    <path d=\"M -1 125.563764 \n+L 485.166104 125.563764 \n+L 485.166104 137.197076 \n+L -1 137.197076 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_204\">\n+    <path d=\"M -1 140.105404 \n+L 480.961563 140.105404 \n+L 480.961563 151.738715 \n+L -1 151.738715 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_205\">\n+    <path d=\"M -1 154.647043 \n+L 488.188929 154.647043 \n+L 488.188929 166.280355 \n+L -1 166.280355 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_206\">\n+    <path d=\"M -1 169.188682 \n+L 499.445184 169.188682 \n+L 499.445184 180.821994 \n+L -1 180.821994 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_207\">\n+    <path d=\"M -1 183.730322 \n+L 489.069785 183.730322 \n+L 489.069785 195.363633 \n+L -1 195.363633 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_208\">\n+    <path d=\"M -1 198.271961 \n+L 504.005878 198.271961 \n+L 504.005878 209.905273 \n+L -1 209.905273 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_209\">\n+    <path d=\"M -1 212.8136 \n+L 508.714689 212.8136 \n+L 508.714689 224.446912 \n+L -1 224.446912 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_210\">\n+    <path d=\"M -1 227.35524 \n+L 509.076234 227.35524 \n+L 509.076234 238.988551 \n+L -1 238.988551 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_211\">\n+    <path d=\"M -1 241.896879 \n+L 518.326084 241.896879 \n+L 518.326084 253.530191 \n+L -1 253.530191 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_212\">\n+    <path d=\"M -1 256.438518 \n+L 519.06452 256.438518 \n+L 519.06452 268.07183 \n+L -1 268.07183 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_213\">\n+    <path d=\"M -1 270.980158 \n+L 508.565219 270.980158 \n+L 508.565219 282.613469 \n+L -1 282.613469 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_214\">\n+    <path d=\"M -1 285.521797 \n+L 521.904188 285.521797 \n+L 521.904188 297.155109 \n+L -1 297.155109 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_215\">\n+    <path d=\"M -1 300.063436 \n+L 523.4095 300.063436 \n+L 523.4095 311.696748 \n+L -1 311.696748 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_216\">\n+    <path d=\"M -1 314.605076 \n+L 543.131956 314.605076 \n+L 543.131956 326.238387 \n+L -1 326.238387 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_217\">\n+    <path d=\"M -1 329.146715 \n+L 540.099488 329.146715 \n+L 540.099488 340.780027 \n+L -1 340.780027 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_218\">\n+    <path d=\"M -1 343.688355 \n+L 531.599916 343.688355 \n+L 531.599916 355.321666 \n+L -1 355.321666 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_219\">\n+    <path d=\"M -1 358.229994 \n+L 536.906098 358.229994 \n+L 536.906098 369.863305 \n+L -1 369.863305 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_220\">\n+    <path d=\"M -1 372.771633 \n+L 534.502318 372.771633 \n+L 534.502318 384.404945 \n+L -1 384.404945 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_221\">\n+    <path d=\"M -1 387.313273 \n+L 549.223394 387.313273 \n+L 549.223394 398.946584 \n+L -1 398.946584 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_222\">\n+    <path d=\"M -1 401.854912 \n+L 548.20294 401.854912 \n+L 548.20294 413.488223 \n+L -1 413.488223 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_223\">\n+    <path d=\"M -1 416.396551 \n+L 543.342957 416.396551 \n+L 543.342957 428.029863 \n+L -1 428.029863 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_224\">\n+    <path d=\"M -1 430.938191 \n+L 552.817203 430.938191 \n+L 552.817203 442.571502 \n+L -1 442.571502 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_225\">\n+    <path d=\"M -1 445.47983 \n+L 550.05141 445.47983 \n+L 550.05141 457.113141 \n+L -1 457.113141 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_226\">\n+    <path d=\"M -1 460.021469 \n+L 555.048166 460.021469 \n+L 555.048166 471.654781 \n+L -1 471.654781 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_227\">\n+    <path d=\"M -1 474.563109 \n+L 563.10385 474.563109 \n+L 563.10385 486.19642 \n+L -1 486.19642 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_228\">\n+    <path d=\"M -1 489.104748 \n+L 549.828685 489.104748 \n+L 549.828685 500.738059 \n+L -1 500.738059 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_229\">\n+    <path d=\"M -1 503.646387 \n+L 559.175184 503.646387 \n+L 559.175184 515.279699 \n+L -1 515.279699 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_230\">\n+    <path d=\"M -1 518.188027 \n+L 558.691779 518.188027 \n+L 558.691779 529.821338 \n+L -1 529.821338 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_231\">\n+    <path d=\"M -1 532.729666 \n+L 564.352996 532.729666 \n+L 564.352996 544.362977 \n+L -1 544.362977 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_232\">\n+    <path d=\"M -1 547.271305 \n+L 581.731187 547.271305 \n+L 581.731187 558.904617 \n+L -1 558.904617 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_233\">\n+    <path d=\"M -1 561.812945 \n+L 583.002888 561.812945 \n+L 583.002888 573.446256 \n+L -1 573.446256 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_234\">\n+    <path d=\"M -1 576.354584 \n+L 586.209263 576.354584 \n+L 586.209263 587.987895 \n+L -1 587.987895 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_235\">\n+    <path d=\"M -1 590.896223 \n+L 570.572308 590.896223 \n+L 570.572308 602.529535 \n+L -1 602.529535 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_236\">\n+    <path d=\"M -1 605.437863 \n+L 586.536963 605.437863 \n+L 586.536963 617.071174 \n+L -1 617.071174 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_237\">\n+    <path d=\"M -1 619.979502 \n+L 584.286967 619.979502 \n+L 584.286967 631.612814 \n+L -1 631.612814 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_238\">\n+    <path d=\"M -1 634.521141 \n+L 582.214159 634.521141 \n+L 582.214159 646.154453 \n+L -1 646.154453 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_239\">\n+    <path d=\"M -1 649.062781 \n+L 599.791388 649.062781 \n+L 599.791388 660.696092 \n+L -1 660.696092 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_240\">\n+    <path d=\"M -1 663.60442 \n+L 595.216564 663.60442 \n+L 595.216564 675.237732 \n+L -1 675.237732 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_241\">\n+    <path d=\"M -1 678.146059 \n+L 599.928309 678.146059 \n+L 599.928309 689.779371 \n+L -1 689.779371 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_242\">\n+    <path d=\"M -1 692.687699 \n+L 610.081405 692.687699 \n+L 610.081405 704.32101 \n+L -1 704.32101 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_243\">\n+    <path d=\"M -1 707.229338 \n+L 620.303838 707.229338 \n+L 620.303838 718.86265 \n+L -1 718.86265 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_244\">\n+    <path d=\"M -1 721.770977 \n+L 620.19115 721.770977 \n+L 620.19115 733.404289 \n+L -1 733.404289 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_245\">\n+    <path d=\"M -1 736.312617 \n+L 617.156363 736.312617 \n+L 617.156363 747.945928 \n+L -1 747.945928 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_246\">\n+    <path d=\"M -1 750.854256 \n+L 621.616429 750.854256 \n+L 621.616429 762.487568 \n+L -1 762.487568 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_247\">\n+    <path d=\"M -1 765.395895 \n+L 623.007069 765.395895 \n+L 623.007069 777.029207 \n+L -1 777.029207 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_248\">\n+    <path d=\"M -1 779.937535 \n+L 621.935344 779.937535 \n+L 621.935344 791.570846 \n+L -1 791.570846 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_249\">\n+    <path d=\"M -1 794.479174 \n+L 655.745307 794.479174 \n+L 655.745307 806.112486 \n+L -1 806.112486 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_250\">\n+    <path d=\"M -1 809.020814 \n+L 623.224264 809.020814 \n+L 623.224264 820.654125 \n+L -1 820.654125 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_251\">\n+    <path d=\"M -1 823.562453 \n+L 638.400651 823.562453 \n+L 638.400651 835.195764 \n+L -1 835.195764 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_252\">\n+    <path d=\"M -1 838.104092 \n+L 632.101245 838.104092 \n+L 632.101245 849.737404 \n+L -1 849.737404 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_253\">\n+    <path d=\"M -1 852.645732 \n+L 631.041728 852.645732 \n+L 631.041728 864.279043 \n+L -1 864.279043 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_254\">\n+    <path d=\"M -1 867.187371 \n+L 659.3 867.187371 \n+M 659.3 878.820682 \n+L -1 878.820682 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_255\">\n+    <path d=\"M -1 881.72901 \n+L 659.3 881.72901 \n+M 659.3 893.362322 \n+L -1 893.362322 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_256\">\n+    <path d=\"M -1 896.27065 \n+L 659.3 896.27065 \n+M 659.3 907.903961 \n+L -1 907.903961 \n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: none; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"patch_257\">\n+    <path d=\"M -157029.536861 29.588945 \n+L -157029.536861 29.588945 \n+L -157029.536861 29.588945 \n+L -157029.536861 29.588945 \n z\n-\" transform=\"scale(0.015625)\"/>\n-     </defs>\n+\" clip-path=\"url(#p224e7fb1ba)\" style=\"fill: #ffffff; stroke: #808080; stroke-width: 0.5; stroke-linejoin: miter\"/>\n+   </g>\n+   <g id=\"text_196\">\n+    <!-- 2.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(427.425444 32.37811) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-32\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_197\">\n+    <!-- 3.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(431.040887 46.91975) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_66\">\n-    <!-- 4.0 -->\n-    <g transform=\"translate(171.30291 31.843586)scale(0.1 -0.1)\">\n+   <g id=\"text_198\">\n+    <!-- 3.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(434.442936 61.461389) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-Oblique-33\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-31\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_199\">\n+    <!-- 4.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(452.135598 76.003028) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_67\">\n-    <!-- 4.2 -->\n-    <g transform=\"translate(178.734895 46.433059)scale(0.1 -0.1)\">\n+   <g id=\"text_200\">\n+    <!-- 4.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(452.998728 90.544668) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_68\">\n-    <!-- 4.3 -->\n-    <g transform=\"translate(185.202663 61.022533)scale(0.1 -0.1)\">\n+   <g id=\"text_201\">\n+    <!-- 4.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(460.025373 105.086307) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_69\">\n-    <!-- 4.5 -->\n-    <g transform=\"translate(191.500972 75.612007)scale(0.1 -0.1)\">\n+   <g id=\"text_202\">\n+    <!-- 4.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(463.761643 119.627946) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_70\">\n-    <!-- 5.3 -->\n-    <g transform=\"translate(219.316664 90.20148)scale(0.1 -0.1)\">\n+   <g id=\"text_203\">\n+    <!-- 4.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(464.460226 134.169586) scale(0.08 -0.08)\">\n+     <defs>\n+      <path id=\"DejaVuSans-Oblique-34\" d=\"M 2753 4666 \n+L 3547 4666 \n+L 2950 1625 \n+L 3616 1625 \n+L 3513 1100 \n+L 2847 1100 \n+L 2638 0 \n+L 2009 0 \n+L 2222 1100 \n+L 116 1100 \n+L 238 1709 \n+L 2753 4666 \n+z\n+M 2809 4116 \n+L 728 1625 \n+L 2322 1625 \n+L 2809 4116 \n+z\n+\" transform=\"scale(0.015625)\"/>\n+     </defs>\n+     <use xlink:href=\"#DejaVuSans-Oblique-34\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-39\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_204\">\n+    <!-- 4.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(465.196716 148.711225) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_205\">\n+    <!-- 5.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(465.963629 163.252864) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_71\">\n-    <!-- 5.4 -->\n-    <g transform=\"translate(224.698608 104.790954)scale(0.1 -0.1)\">\n+   <g id=\"text_206\">\n+    <!-- 5.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(469.635865 177.794504) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_72\">\n-    <!-- 5.6 -->\n-    <g transform=\"translate(230.6044 119.380428)scale(0.1 -0.1)\">\n-     <defs>\n-      <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n-Q 1688 2584 1439 2293 \n-Q 1191 2003 1191 1497 \n-Q 1191 994 1439 701 \n-Q 1688 409 2113 409 \n-Q 2538 409 2786 701 \n-Q 3034 994 3034 1497 \n-Q 3034 2003 2786 2293 \n-Q 2538 2584 2113 2584 \n-z\n-M 3366 4563 \n-L 3366 3988 \n-Q 3128 4100 2886 4159 \n-Q 2644 4219 2406 4219 \n-Q 1781 4219 1451 3797 \n-Q 1122 3375 1075 2522 \n-Q 1259 2794 1537 2939 \n-Q 1816 3084 2150 3084 \n-Q 2853 3084 3261 2657 \n-Q 3669 2231 3669 1497 \n-Q 3669 778 3244 343 \n-Q 2819 -91 2113 -91 \n-Q 1303 -91 875 529 \n-Q 447 1150 447 2328 \n-Q 447 3434 972 4092 \n-Q 1497 4750 2381 4750 \n-Q 2619 4750 2861 4703 \n-Q 3103 4656 3366 4563 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-     </defs>\n+   <g id=\"text_207\">\n+    <!-- 5.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(470.074358 192.336143) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_73\">\n+   <g id=\"text_208\">\n+    <!-- 6.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(479.51354 206.877782) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-36\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_209\">\n+    <!-- 6.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(482.957604 221.419422) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-36\"/>\n+     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_210\">\n     <!-- 6.7 -->\n-    <g transform=\"translate(261.548355 133.969901)scale(0.1 -0.1)\">\n-     <defs>\n-      <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n-L 3525 4666 \n-L 3525 4397 \n-L 1831 0 \n-L 1172 0 \n-L 2766 4134 \n-L 525 4134 \n-L 525 4666 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-     </defs>\n+    <g style=\"fill: #ffffff\" transform=\"translate(486.680476 235.961061) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-36\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_74\">\n-    <!-- 7.1 -->\n-    <g transform=\"translate(270.550997 148.559375)scale(0.1 -0.1)\">\n+   <g id=\"text_211\">\n+    <!-- 7.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(492.052927 250.5027) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-37\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_75\">\n-    <!-- 7.3 -->\n-    <g transform=\"translate(276.715851 163.148849)scale(0.1 -0.1)\">\n+   <g id=\"text_212\">\n+    <!-- 7.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(495.0881 265.04434) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-37\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_76\">\n-    <!-- 8.3 -->\n-    <g transform=\"translate(299.071954 177.738322)scale(0.1 -0.1)\">\n+   <g id=\"text_213\">\n+    <!-- 7.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(495.536879 279.585979) scale(0.08 -0.08)\">\n      <defs>\n-      <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n-Q 1584 2216 1326 1975 \n-Q 1069 1734 1069 1313 \n-Q 1069 891 1326 650 \n-Q 1584 409 2034 409 \n-Q 2484 409 2743 651 \n-Q 3003 894 3003 1313 \n-Q 3003 1734 2745 1975 \n-Q 2488 2216 2034 2216 \n-z\n-M 1403 2484 \n-Q 997 2584 770 2862 \n-Q 544 3141 544 3541 \n-Q 544 4100 942 4425 \n-Q 1341 4750 2034 4750 \n-Q 2731 4750 3128 4425 \n-Q 3525 4100 3525 3541 \n-Q 3525 3141 3298 2862 \n-Q 3072 2584 2669 2484 \n-Q 3125 2378 3379 2068 \n-Q 3634 1759 3634 1313 \n-Q 3634 634 3220 271 \n-Q 2806 -91 2034 -91 \n-Q 1263 -91 848 271 \n-Q 434 634 434 1313 \n-Q 434 1759 690 2068 \n-Q 947 2378 1403 2484 \n-z\n-M 1172 3481 \n-Q 1172 3119 1398 2916 \n-Q 1625 2713 2034 2713 \n-Q 2441 2713 2670 2916 \n-Q 2900 3119 2900 3481 \n-Q 2900 3844 2670 4047 \n-Q 2441 4250 2034 4250 \n-Q 1625 4250 1398 4047 \n-Q 1172 3844 1172 3481 \n+      <path id=\"DejaVuSans-Oblique-37\" d=\"M 819 4666 \n+L 4013 4666 \n+L 3956 4397 \n+L 1288 0 \n+L 594 0 \n+L 3116 4134 \n+L 716 4134 \n+L 819 4666 \n z\n \" transform=\"scale(0.015625)\"/>\n      </defs>\n-     <use xlink:href=\"#DejaVuSans-38\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-37\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-37\" x=\"95.410156\"/>\n+    </g>\n+   </g>\n+   <g id=\"text_214\">\n+    <!-- 7.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(496.178683 294.127618) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-37\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_77\">\n-    <!-- 8.4 -->\n-    <g transform=\"translate(300.832666 192.327796)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-38\"/>\n+   <g id=\"text_215\">\n+    <!-- 7.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(497.026707 308.669258) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-37\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_78\">\n-    <!-- 8.5 -->\n-    <g transform=\"translate(303.486189 206.91727)scale(0.1 -0.1)\">\n+   <g id=\"text_216\">\n+    <!-- 8.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(500.464086 323.210897) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-38\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_79\">\n-    <!-- 8.6 -->\n-    <g transform=\"translate(304.285774 221.506743)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-38\"/>\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n+   <g id=\"text_217\">\n+    <!-- 8.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(502.200488 337.752536) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-Oblique-38\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-2e\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-Oblique-34\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_80\">\n-    <!-- 8.7 -->\n-    <g transform=\"translate(306.543645 236.096217)scale(0.1 -0.1)\">\n+   <g id=\"text_218\">\n+    <!-- 8.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(503.847492 352.294176) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-38\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_81\">\n-    <!-- 9.5 -->\n-    <g transform=\"translate(322.050979 250.685691)scale(0.1 -0.1)\">\n-     <defs>\n-      <path id=\"DejaVuSans-39\" d=\"M 703 97 \n-L 703 672 \n-Q 941 559 1184 500 \n-Q 1428 441 1663 441 \n-Q 2288 441 2617 861 \n-Q 2947 1281 2994 2138 \n-Q 2813 1869 2534 1725 \n-Q 2256 1581 1919 1581 \n-Q 1219 1581 811 2004 \n-Q 403 2428 403 3163 \n-Q 403 3881 828 4315 \n-Q 1253 4750 1959 4750 \n-Q 2769 4750 3195 4129 \n-Q 3622 3509 3622 2328 \n-Q 3622 1225 3098 567 \n-Q 2575 -91 1691 -91 \n-Q 1453 -91 1209 -44 \n-Q 966 3 703 97 \n-z\n-M 1959 2075 \n-Q 2384 2075 2632 2365 \n-Q 2881 2656 2881 3163 \n-Q 2881 3666 2632 3958 \n-Q 2384 4250 1959 4250 \n-Q 1534 4250 1286 3958 \n-Q 1038 3666 1038 3163 \n-Q 1038 2656 1286 2365 \n-Q 1534 2075 1959 2075 \n-z\n-\" transform=\"scale(0.015625)\"/>\n-     </defs>\n+   <g id=\"text_219\">\n+    <!-- 9.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(507.89231 366.835815) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-39\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_82\">\n-    <!-- 9.7 -->\n-    <g transform=\"translate(325.898458 265.275164)scale(0.1 -0.1)\">\n+   <g id=\"text_220\">\n+    <!-- 9.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(510.937851 381.377455) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-39\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n     </g>\n    </g>\n-   <g id=\"text_83\">\n-    <!-- 10.3 -->\n-    <g transform=\"translate(336.531215 279.864638)scale(0.1 -0.1)\">\n+   <g id=\"text_221\">\n+    <!-- 10.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(509.354855 395.919094) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_84\">\n-    <!-- 11.5 -->\n-    <g transform=\"translate(357.022664 294.454112)scale(0.1 -0.1)\">\n+   <g id=\"text_222\">\n+    <!-- 10.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(513.838055 410.460733) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_85\">\n-    <!-- 11.7 -->\n-    <g transform=\"translate(359.535051 309.043586)scale(0.1 -0.1)\">\n+   <g id=\"text_223\">\n+    <!-- 10.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(514.868162 425.002373) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_86\">\n-    <!-- 12.5 -->\n-    <g transform=\"translate(371.137848 323.633059)scale(0.1 -0.1)\">\n+   <g id=\"text_224\">\n+    <!-- 11.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(518.981415 439.544012) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_87\">\n-    <!-- 13.3 -->\n-    <g transform=\"translate(382.371511 338.222533)scale(0.1 -0.1)\">\n+   <g id=\"text_225\">\n+    <!-- 12.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(521.191477 454.085651) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_88\">\n-    <!-- 13.4 -->\n-    <g transform=\"translate(383.594727 352.812007)scale(0.1 -0.1)\">\n+   <g id=\"text_226\">\n+    <!-- 12.5 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(523.731018 468.627291) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_89\">\n-    <!-- 13.8 -->\n-    <g transform=\"translate(387.914549 367.40148)scale(0.1 -0.1)\">\n+   <g id=\"text_227\">\n+    <!-- 12.9 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(526.167776 483.16893) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_90\">\n-    <!-- 13.8 -->\n-    <g transform=\"translate(388.310994 381.990954)scale(0.1 -0.1)\">\n+   <g id=\"text_228\">\n+    <!-- 13.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(526.471635 497.710569) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n-    </g>\n-   </g>\n-   <g id=\"text_91\">\n-    <!-- 14.3 -->\n-    <g transform=\"translate(394.084973 396.580428)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_92\">\n-    <!-- 14.4 -->\n-    <g transform=\"translate(396.219137 411.169901)scale(0.1 -0.1)\">\n+   <g id=\"text_229\">\n+    <!-- 13.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(526.556802 512.252209) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_93\">\n-    <!-- 14.6 -->\n-    <g transform=\"translate(397.847423 425.759375)scale(0.1 -0.1)\">\n+   <g id=\"text_230\">\n+    <!-- 13.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(527.068357 526.793848) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_94\">\n+   <g id=\"text_231\">\n     <!-- 14.7 -->\n-    <g transform=\"translate(399.339759 440.348849)scale(0.1 -0.1)\">\n+    <g style=\"fill: #ffffff\" transform=\"translate(535.231446 541.335487) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n      <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_95\">\n-    <!-- 15.4 -->\n-    <g transform=\"translate(408.062556 454.938322)scale(0.1 -0.1)\">\n+   <g id=\"text_232\">\n+    <!-- 17.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(545.031968 555.877127) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_96\">\n-    <!-- 15.7 -->\n-    <g transform=\"translate(410.403084 469.527796)scale(0.1 -0.1)\">\n+   <g id=\"text_233\">\n+    <!-- 18.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(549.351571 570.418766) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_97\">\n-    <!-- 16.0 -->\n-    <g transform=\"translate(414.005627 484.11727)scale(0.1 -0.1)\">\n+   <g id=\"text_234\">\n+    <!-- 18.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(549.982969 584.960405) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_98\">\n-    <!-- 16.5 -->\n-    <g transform=\"translate(419.982408 498.706743)scale(0.1 -0.1)\">\n+   <g id=\"text_235\">\n+    <!-- 18.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(550.214446 599.502045) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_99\">\n-    <!-- 17.0 -->\n-    <g transform=\"translate(424.547595 513.296217)scale(0.1 -0.1)\">\n+   <g id=\"text_236\">\n+    <!-- 19.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(553.916373 614.043684) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_100\">\n-    <!-- 17.5 -->\n-    <g transform=\"translate(429.536815 527.885691)scale(0.1 -0.1)\">\n+   <g id=\"text_237\">\n+    <!-- 19.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(555.029888 628.585323) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n-    </g>\n-   </g>\n-   <g id=\"text_101\">\n-    <!-- 21.5 -->\n-    <g transform=\"translate(466.298908 542.475164)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-32\"/>\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_102\">\n-    <!-- 21.9 -->\n-    <g transform=\"translate(469.282836 557.064638)scale(0.1 -0.1)\">\n+   <g id=\"text_238\">\n+    <!-- 20.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(558.01836 643.126963) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-39\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_103\">\n-    <!-- 22.6 -->\n-    <g transform=\"translate(474.575165 571.654112)scale(0.1 -0.1)\">\n+   <g id=\"text_239\">\n+    <!-- 23.7 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(567.717503 657.668602) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n-     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_104\">\n-    <!-- 23.1 -->\n-    <g transform=\"translate(478.743587 586.243586)scale(0.1 -0.1)\">\n+   <g id=\"text_240\">\n+    <!-- 26.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(574.194186 672.210241) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_105\">\n-    <!-- 23.1 -->\n-    <g transform=\"translate(479.082047 600.833059)scale(0.1 -0.1)\">\n+   <g id=\"text_241\">\n+    <!-- 28.6 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(580.509972 686.751881) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_106\">\n-    <!-- 23.4 -->\n-    <g transform=\"translate(481.281513 615.422533)scale(0.1 -0.1)\">\n+   <g id=\"text_242\">\n+    <!-- 29.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(582.508786 701.29352) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n      <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_107\">\n-    <!-- 27.1 -->\n-    <g transform=\"translate(506.539444 630.012007)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-32\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n+   <g id=\"text_243\">\n+    <!-- 30.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(584.721467 715.835159) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-33\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_108\">\n-    <!-- 28.1 -->\n-    <g transform=\"translate(513.385941 644.60148)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-32\"/>\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n+   <g id=\"text_244\">\n+    <!-- 32.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(589.099954 730.376799) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-33\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_109\">\n-    <!-- 32.9 -->\n-    <g transform=\"translate(540.809897 659.190954)scale(0.1 -0.1)\">\n+   <g id=\"text_245\">\n+    <!-- 32.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(589.154276 744.918438) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-39\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_110\">\n+   <g id=\"text_246\">\n     <!-- 33.0 -->\n-    <g transform=\"translate(541.346557 673.780428)scale(0.1 -0.1)\">\n+    <g style=\"fill: #ffffff\" transform=\"translate(590.428897 759.460077) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n      <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_111\">\n-    <!-- 33.9 -->\n-    <g transform=\"translate(546.060086 688.369901)scale(0.1 -0.1)\">\n+   <g id=\"text_247\">\n+    <!-- 34.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(592.555975 774.001717) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-39\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_112\">\n-    <!-- 36.7 -->\n-    <g transform=\"translate(559.805754 702.959375)scale(0.1 -0.1)\">\n+   <g id=\"text_248\">\n+    <!-- 34.1 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(592.580169 788.543356) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_113\">\n-    <!-- 37.0 -->\n-    <g transform=\"translate(561.410241 717.548849)scale(0.1 -0.1)\">\n+   <g id=\"text_249\">\n+    <!-- 39.3 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(602.361951 803.084995) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_114\">\n-    <!-- 37.7 -->\n-    <g transform=\"translate(564.694128 732.138322)scale(0.1 -0.1)\">\n+   <g id=\"text_250\">\n+    <!-- 39.8 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(603.064069 817.626635) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_115\">\n-    <!-- 38.2 -->\n-    <g transform=\"translate(567.234021 746.727796)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-33\"/>\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n+   <g id=\"text_251\">\n+    <!-- 40.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(603.846281 832.168274) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n      <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_116\">\n-    <!-- 38.3 -->\n-    <g transform=\"translate(567.381428 761.31727)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-33\"/>\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n+   <g id=\"text_252\">\n+    <!-- 42.2 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(607.21649 846.709914) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_117\">\n-    <!-- 38.5 -->\n-    <g transform=\"translate(568.631096 775.906743)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-33\"/>\n-     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n+   <g id=\"text_253\">\n+    <!-- 42.5 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(607.655438 861.251553) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-34\"/>\n+     <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n      <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_118\">\n-    <!-- 39.3 -->\n-    <g transform=\"translate(572.140669 790.496217)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-33\"/>\n-     <use xlink:href=\"#DejaVuSans-39\" x=\"63.623047\"/>\n-     <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-33\" x=\"159.033203\"/>\n-    </g>\n-   </g>\n-   <g id=\"text_119\">\n-    <!-- 44.6 -->\n-    <g transform=\"translate(594.098456 805.085691)scale(0.1 -0.1)\">\n+   <g id=\"text_254\">\n+    <!-- 47.4 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(615.143743 875.793192) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-36\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_120\">\n-    <!-- 45.4 -->\n-    <g transform=\"translate(597.544184 819.675164)scale(0.1 -0.1)\">\n+   <g id=\"text_255\">\n+    <!-- 48.5 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(616.701963 890.334832) scale(0.08 -0.08)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n-     <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n+     <use xlink:href=\"#DejaVuSans-38\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-34\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n     </g>\n    </g>\n-   <g id=\"text_121\">\n-    <!-- 47.1 -->\n-    <g transform=\"translate(603.770213 834.264638)scale(0.1 -0.1)\">\n-     <use xlink:href=\"#DejaVuSans-34\"/>\n-     <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n+   <g id=\"text_256\">\n+    <!-- 50.0 -->\n+    <g style=\"fill: #ffffff\" transform=\"translate(618.716212 904.876471) scale(0.08 -0.08)\">\n+     <use xlink:href=\"#DejaVuSans-35\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      <use xlink:href=\"#DejaVuSans-2e\" x=\"127.246094\"/>\n-     <use xlink:href=\"#DejaVuSans-31\" x=\"159.033203\"/>\n+     <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <defs>\n-  <clipPath id=\"pd97573e82b\">\n-   <rect x=\"88.334375\" y=\"7.2\" width=\"558\" height=\"831.6\"/>\n+  <clipPath id=\"p9f626abd74\">\n+   <rect x=\"93.1\" y=\"22.318125\" width=\"232.5\" height=\"887.04\"/>\n+  </clipPath>\n+  <clipPath id=\"p224e7fb1ba\">\n+   <rect x=\"418.6\" y=\"22.318125\" width=\"232.5\" height=\"887.04\"/>\n   </clipPath>\n  </defs>\n </svg>\ndiff --git a/model-card.md b/model-card.md\nindex 2ed85cf24..3c041a1c0 100644\n--- a/model-card.md\n+++ b/model-card.md\n@@ -17,12 +17,12 @@ The Whisper models are trained for speech recognition and translation tasks, cap\n | medium |   769 M    |                   |                   |\n | large  |   1550 M   |                    |                   |\n \n-In December 2022, we [released an improved large model named `large-v2`](https://github.com/openai/whisper/discussions/661).\n+In December 2022, we [released an improved large model named `large-v2`](https://github.com/openai/whisper/discussions/661), and `large-v3` in November 2023.\n \n \n ### Release date\n \n-September 2022 (original series) and December 2022 (`large-v2`)\n+September 2022 (original series), December 2022 (`large-v2`), and November 2023 (`large-v3`)\n \n ### Model type\n \n@@ -37,7 +37,7 @@ Sequence-to-sequence ASR (automatic speech recognition) and speech translation m\n \n ### Evaluated Use\n \n-The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only intended uses or to draw reasonable guidelines around what is or is not research.\n+The primary intended users of these models are AI researchers studying the robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only intended uses or to draw reasonable guidelines around what is or is not research.\n \n The models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n \n@@ -53,17 +53,17 @@ As discussed in [the accompanying paper](https://arxiv.org/abs/2212.04356), we s\n \n ## Performance and Limitations\n \n-Our studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n+Our studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, and technical language, as well as zero-shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n \n However, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n \n-Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://arxiv.org/abs/2212.04356).\n+Our models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include a higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://arxiv.org/abs/2212.04356).\n \n-In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://arxiv.org/abs/2212.04356). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n+In addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis of these limitations is provided in [the paper](https://arxiv.org/abs/2212.04356). It is likely that this behavior and hallucinations may be worse in lower-resource and/or lower-discoverability languages.\n \n \n ## Broader Implications\n \n We anticipate that Whisper models transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box  their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n \n-There are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n+There are also potential dual-use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\ndiff --git a/requirements.txt b/requirements.txt\nindex 3c11ac32c..a03dae853 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -3,4 +3,4 @@ numpy\n torch\n tqdm\n more-itertools\n-tiktoken==0.3.3\n+tiktoken\ndiff --git a/setup.py b/setup.py\nindex 1161d815a..97e43ee68 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -12,8 +12,24 @@ def read_version(fname=\"whisper/version.py\"):\n \n \n requirements = []\n+whisper_rocm = os.getenv('WHISPER_ROCM',default='0')\n if sys.platform.startswith(\"linux\") and platform.machine() == \"x86_64\":\n-    requirements.append(\"triton==2.0.0\")\n+    from check_rocm_platform import is_command, check_amd_gpu_rocminfo, check_amd_gpu_lspci, check_rocm_packages\n+    ROCM_PLATFORM = False\n+    if is_command(\"rocminfo\"):\n+        ROCM_PLATFORM = check_amd_gpu_rocminfo()\n+    elif is_command(\"lspci\"):\n+        ROCM_PLATFORM = check_amd_gpu_lspci()\n+    if not ROCM_PLATFORM:\n+        if is_command(\"hipcc\") or is_command(\"rocm-smi\"):\n+            ROCM_PLATFORM = True\n+        else:\n+            ROCM_PLATFORM = check_rocm_packages( )\n+    if ROCM_PLATFORM :\n+        print(\"rocm\")\n+        requirements.append(\"pytorch-triton-rocm>=2.0.1\")\n+    else :\n+        requirements.append(\"triton>=2.0.0,<3\")\n \n setup(\n     name=\"openai-whisper\",\ndiff --git a/tests/test_tokenizer.py b/tests/test_tokenizer.py\nindex 09d0351e1..be424e5fe 100644\n--- a/tests/test_tokenizer.py\n+++ b/tests/test_tokenizer.py\n@@ -1,7 +1,17 @@\n+import pytest\n+\n from whisper.tokenizer import get_tokenizer\n \n \n-def test_tokenizer():\n+@pytest.mark.parametrize(\"multilingual\", [True, False])\n+def test_tokenizer(multilingual):\n+    tokenizer = get_tokenizer(multilingual=False)\n+    assert tokenizer.sot in tokenizer.sot_sequence\n+    assert len(tokenizer.all_language_codes) == len(tokenizer.all_language_tokens)\n+    assert all(c < tokenizer.timestamp_begin for c in tokenizer.all_language_tokens)\n+\n+\n+def test_multilingual_tokenizer():\n     gpt2_tokenizer = get_tokenizer(multilingual=False)\n     multilingual_tokenizer = get_tokenizer(multilingual=True)\n \n@@ -20,5 +30,5 @@ def test_split_on_unicode():\n     tokens = [8404, 871, 287, 6, 246, 526, 3210, 20378]\n     words, word_tokens = multilingual_tokenizer.split_tokens_on_unicode(tokens)\n \n-    assert words == [\" elle\", \" est\", \" l\", \"'\", \"\", \"\", \"rit\", \"oire\"]\n+    assert words == [\" elle\", \" est\", \" l\", \"'\", \"\\ufffd\", \"\", \"rit\", \"oire\"]\n     assert word_tokens == [[8404], [871], [287], [6], [246], [526], [3210], [20378]]\ndiff --git a/tests/test_transcribe.py b/tests/test_transcribe.py\nindex e4f8fd0f7..599221af5 100644\n--- a/tests/test_transcribe.py\n+++ b/tests/test_transcribe.py\n@@ -25,7 +25,7 @@ def test_transcribe(model_name: str):\n     assert \"your country\" in transcription\n     assert \"do for you\" in transcription\n \n-    tokenizer = get_tokenizer(model.is_multilingual)\n+    tokenizer = get_tokenizer(model.is_multilingual, num_languages=model.num_languages)\n     all_tokens = [t for s in result[\"segments\"] for t in s[\"tokens\"]]\n     assert tokenizer.decode(all_tokens) == result[\"text\"]\n     assert tokenizer.decode_with_timestamps(all_tokens).startswith(\"<|0.00|>\")\ndiff --git a/whisper/__init__.py b/whisper/__init__.py\nindex 379133b6a..d7fbba36f 100644\n--- a/whisper/__init__.py\n+++ b/whisper/__init__.py\n@@ -25,7 +25,8 @@\n     \"medium\": \"https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt\",\n     \"large-v1\": \"https://openaipublic.azureedge.net/main/whisper/models/e4b87e7e0bf463eb8e6956e646f1e277e901512310def2c24bf0e11bd3c28e9a/large-v1.pt\",\n     \"large-v2\": \"https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt\",\n-    \"large\": \"https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt\",\n+    \"large-v3\": \"https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt\",\n+    \"large\": \"https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt\",\n }\n \n # base85-encoded (n_layers, n_heads) boolean arrays indicating the cross-attention heads that are\n@@ -41,7 +42,8 @@\n     \"medium\": b\"ABzY8B0Jh+0{>%R7}kK1fFL7w6%<-Pf*t^=N)Qr&0RR9\",\n     \"large-v1\": b\"ABzY8r9j$a0{>%R7#4sLmoOs{s)o3~84-RPdcFk!JR<kSfC2yj\",\n     \"large-v2\": b\"ABzY8zd+h!0{>%R7=D0pU<_bnWW*tkYAhobTNnu$jnkEkXqp)j;w1Tzk)UH3X%SZd&fFZ2fC2yj\",\n-    \"large\": b\"ABzY8zd+h!0{>%R7=D0pU<_bnWW*tkYAhobTNnu$jnkEkXqp)j;w1Tzk)UH3X%SZd&fFZ2fC2yj\",\n+    \"large-v3\": b\"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00\",\n+    \"large\": b\"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00\",\n }\n \n \ndiff --git a/whisper/assets/mel_filters.npz b/whisper/assets/mel_filters.npz\nindex 1a7839244..28ea26909 100644\nBinary files a/whisper/assets/mel_filters.npz and b/whisper/assets/mel_filters.npz differ\ndiff --git a/whisper/audio.py b/whisper/audio.py\nindex 4f5b6e073..cf6c66ad9 100644\n--- a/whisper/audio.py\n+++ b/whisper/audio.py\n@@ -12,7 +12,6 @@\n # hard-coded audio hyperparameters\n SAMPLE_RATE = 16000\n N_FFT = 400\n-N_MELS = 80\n HOP_LENGTH = 160\n CHUNK_LENGTH = 30\n N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n@@ -90,7 +89,7 @@ def pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):\n \n \n @lru_cache(maxsize=None)\n-def mel_filters(device, n_mels: int = N_MELS) -> torch.Tensor:\n+def mel_filters(device, n_mels: int) -> torch.Tensor:\n     \"\"\"\n     load the mel filterbank matrix for projecting STFT into a Mel spectrogram.\n     Allows decoupling librosa dependency; saved using:\n@@ -98,18 +97,19 @@ def mel_filters(device, n_mels: int = N_MELS) -> torch.Tensor:\n         np.savez_compressed(\n             \"mel_filters.npz\",\n             mel_80=librosa.filters.mel(sr=16000, n_fft=400, n_mels=80),\n+            mel_128=librosa.filters.mel(sr=16000, n_fft=400, n_mels=128),\n         )\n     \"\"\"\n-    assert n_mels == 80, f\"Unsupported n_mels: {n_mels}\"\n-    with np.load(\n-        os.path.join(os.path.dirname(__file__), \"assets\", \"mel_filters.npz\")\n-    ) as f:\n+    assert n_mels in {80, 128}, f\"Unsupported n_mels: {n_mels}\"\n+\n+    filters_path = os.path.join(os.path.dirname(__file__), \"assets\", \"mel_filters.npz\")\n+    with np.load(filters_path, allow_pickle=False) as f:\n         return torch.from_numpy(f[f\"mel_{n_mels}\"]).to(device)\n \n \n def log_mel_spectrogram(\n     audio: Union[str, np.ndarray, torch.Tensor],\n-    n_mels: int = N_MELS,\n+    n_mels: int = 80,\n     padding: int = 0,\n     device: Optional[Union[str, torch.device]] = None,\n ):\ndiff --git a/whisper/decoding.py b/whisper/decoding.py\nindex ecd98a455..49485d009 100644\n--- a/whisper/decoding.py\n+++ b/whisper/decoding.py\n@@ -32,7 +32,9 @@ def detect_language(\n         list of dictionaries containing the probability distribution over all languages.\n     \"\"\"\n     if tokenizer is None:\n-        tokenizer = get_tokenizer(model.is_multilingual)\n+        tokenizer = get_tokenizer(\n+            model.is_multilingual, num_languages=model.num_languages\n+        )\n     if (\n         tokenizer.language is None\n         or tokenizer.language_token not in tokenizer.sot_sequence\n@@ -514,7 +516,10 @@ def __init__(self, model: \"Whisper\", options: DecodingOptions):\n \n         language = options.language or \"en\"\n         tokenizer = get_tokenizer(\n-            model.is_multilingual, language=language, task=options.task\n+            model.is_multilingual,\n+            num_languages=model.num_languages,\n+            language=language,\n+            task=options.task,\n         )\n         self.tokenizer: Tokenizer = tokenizer\n         self.options: DecodingOptions = self._verify_options(options)\ndiff --git a/whisper/model.py b/whisper/model.py\nindex 3457fcfc6..a67828397 100644\n--- a/whisper/model.py\n+++ b/whisper/model.py\n@@ -197,7 +197,7 @@ def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n         \"\"\"\n         x : torch.LongTensor, shape = (batch_size, <= n_ctx)\n             the text tokens\n-        xa : torch.Tensor, shape = (batch_size, n_mels, n_audio_ctx)\n+        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)\n             the encoded audio features to be attended on\n         \"\"\"\n         offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n@@ -236,7 +236,8 @@ def __init__(self, dims: ModelDimensions):\n             self.dims.n_text_head,\n             self.dims.n_text_layer,\n         )\n-        # use the last half layers for alignment by default; see `set_alignment_heads()` below\n+        # use the last half among the decoder layers for time alignment by default;\n+        # to use a specific set of heads, see `set_alignment_heads()` below.\n         all_heads = torch.zeros(\n             self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool\n         )\n@@ -269,7 +270,11 @@ def device(self):\n \n     @property\n     def is_multilingual(self):\n-        return self.dims.n_vocab == 51865\n+        return self.dims.n_vocab >= 51865\n+\n+    @property\n+    def num_languages(self):\n+        return self.dims.n_vocab - 51765 - int(self.is_multilingual)\n \n     def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n         \"\"\"\ndiff --git a/whisper/timing.py b/whisper/timing.py\nindex 56e84d432..befcf464e 100644\n--- a/whisper/timing.py\n+++ b/whisper/timing.py\n@@ -202,7 +202,7 @@ def find_alignment(\n         hook.remove()\n \n     # heads * tokens * frames\n-    weights = torch.stack([QKs[l][h] for l, h in model.alignment_heads.indices().T])\n+    weights = torch.stack([QKs[_l][_h] for _l, _h in model.alignment_heads.indices().T])\n     weights = weights[:, :, : num_frames // 2]\n     weights = (weights * qk_scale).softmax(dim=-1)\n     std, mean = torch.std_mean(weights, dim=-2, keepdim=True, unbiased=False)\ndiff --git a/whisper/tokenizer.py b/whisper/tokenizer.py\nindex 4030e15aa..2af837570 100644\n--- a/whisper/tokenizer.py\n+++ b/whisper/tokenizer.py\n@@ -107,6 +107,7 @@\n     \"ba\": \"bashkir\",\n     \"jw\": \"javanese\",\n     \"su\": \"sundanese\",\n+    \"yue\": \"cantonese\",\n }\n \n # language code lookup by name, with a few language aliases\n@@ -123,6 +124,7 @@\n     \"moldovan\": \"ro\",\n     \"sinhalese\": \"si\",\n     \"castilian\": \"es\",\n+    \"mandarin\": \"zh\",\n }\n \n \n@@ -131,6 +133,7 @@ class Tokenizer:\n     \"\"\"A thin wrapper around `tiktoken` providing quick access to special tokens\"\"\"\n \n     encoding: tiktoken.Encoding\n+    num_languages: int\n     language: Optional[str] = None\n     task: Optional[str] = None\n     sot_sequence: Tuple[int] = ()\n@@ -145,7 +148,7 @@ def __post_init__(self):\n         translate: int = self.special_tokens[\"<|translate|>\"]\n         transcribe: int = self.special_tokens[\"<|transcribe|>\"]\n \n-        langs = tuple(LANGUAGES.keys())\n+        langs = tuple(LANGUAGES.keys())[: self.num_languages]\n         sot_sequence = [sot]\n         if self.language is not None:\n             sot_sequence.append(sot + 1 + langs.index(self.language))\n@@ -211,10 +214,13 @@ def language_token(self) -> int:\n         if self.language is None:\n             raise ValueError(\"This tokenizer does not have language token configured\")\n \n-        if token := self.special_tokens.get(f\"<|{self.language}|>\", None):\n+        return self.to_language_token(self.language)\n+\n+    def to_language_token(self, language):\n+        if token := self.special_tokens.get(f\"<|{language}|>\", None):\n             return token\n \n-        raise KeyError(f\"Language {self.language} not found in tokenizer.\")\n+        raise KeyError(f\"Language {language} not found in tokenizer.\")\n \n     @cached_property\n     def all_language_tokens(self) -> Tuple[int]:\n@@ -222,11 +228,11 @@ def all_language_tokens(self) -> Tuple[int]:\n         for token, token_id in self.special_tokens.items():\n             if token.strip(\"<|>\") in LANGUAGES:\n                 result.append(token_id)\n-        return tuple(result)\n+        return tuple(result)[: self.num_languages]\n \n     @cached_property\n     def all_language_codes(self) -> Tuple[str]:\n-        return tuple(self.decode([l]).strip(\"<|>\") for l in self.all_language_tokens)\n+        return tuple(self.decode([_l]).strip(\"<|>\") for _l in self.all_language_tokens)\n \n     @cached_property\n     def sot_sequence_including_notimestamps(self) -> Tuple[int]:\n@@ -269,7 +275,7 @@ def non_speech_tokens(self) -> Tuple[int]:\n         return tuple(sorted(result))\n \n     def split_to_word_tokens(self, tokens: List[int]):\n-        if self.language in {\"zh\", \"ja\", \"th\", \"lo\", \"my\"}:\n+        if self.language in {\"zh\", \"ja\", \"th\", \"lo\", \"my\", \"yue\"}:\n             # These languages don't typically use spaces, so it is difficult to split words\n             # without morpheme analysis. Here, we instead split words at any\n             # position where the tokens are decoded as valid unicode points\n@@ -322,7 +328,7 @@ def split_tokens_on_spaces(self, tokens: List[int]):\n \n \n @lru_cache(maxsize=None)\n-def get_encoding(name: str = \"gpt2\"):\n+def get_encoding(name: str = \"gpt2\", num_languages: int = 99):\n     vocab_path = os.path.join(os.path.dirname(__file__), \"assets\", f\"{name}.tiktoken\")\n     ranks = {\n         base64.b64decode(token): int(rank)\n@@ -334,7 +340,7 @@ def get_encoding(name: str = \"gpt2\"):\n     specials = [\n         \"<|endoftext|>\",\n         \"<|startoftranscript|>\",\n-        *[f\"<|{lang}|>\" for lang in LANGUAGES.keys()],\n+        *[f\"<|{lang}|>\" for lang in list(LANGUAGES.keys())[:num_languages]],\n         \"<|translate|>\",\n         \"<|transcribe|>\",\n         \"<|startoflm|>\",\n@@ -361,6 +367,7 @@ def get_encoding(name: str = \"gpt2\"):\n def get_tokenizer(\n     multilingual: bool,\n     *,\n+    num_languages: int = 99,\n     language: Optional[str] = None,\n     task: Optional[str] = None,  # Literal[\"transcribe\", \"translate\", None]\n ) -> Tokenizer:\n@@ -381,6 +388,8 @@ def get_tokenizer(\n         language = None\n         task = None\n \n-    encoding = get_encoding(name=encoding_name)\n+    encoding = get_encoding(name=encoding_name, num_languages=num_languages)\n \n-    return Tokenizer(encoding=encoding, language=language, task=task)\n+    return Tokenizer(\n+        encoding=encoding, num_languages=num_languages, language=language, task=task\n+    )\ndiff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex 6e43a22fa..e80bede1d 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -1,5 +1,6 @@\n import argparse\n import os\n+import traceback\n import warnings\n from typing import TYPE_CHECKING, Optional, Tuple, Union\n \n@@ -118,7 +119,7 @@ def transcribe(\n         decode_options[\"fp16\"] = False\n \n     # Pad 30-seconds of silence to the input audio, for slicing\n-    mel = log_mel_spectrogram(audio, padding=N_SAMPLES)\n+    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n     content_frames = mel.shape[-1] - N_FRAMES\n \n     if decode_options.get(\"language\", None) is None:\n@@ -139,7 +140,12 @@ def transcribe(\n \n     language: str = decode_options[\"language\"]\n     task: str = decode_options.get(\"task\", \"transcribe\")\n-    tokenizer = get_tokenizer(model.is_multilingual, language=language, task=task)\n+    tokenizer = get_tokenizer(\n+        model.is_multilingual,\n+        num_languages=model.num_languages,\n+        language=language,\n+        task=task,\n+    )\n \n     if word_timestamps and task == \"translate\":\n         warnings.warn(\"Word-level timestamps on translations may not be reliable.\")\n@@ -378,10 +384,17 @@ def new_segment(\n def cli():\n     from . import available_models\n \n+    def valid_model_name(name):\n+        if name in available_models() or os.path.exists(name):\n+            return name\n+        raise ValueError(\n+            f\"model should be one of {available_models()} or path to a model checkpoint\"\n+        )\n+\n     # fmt: off\n     parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n     parser.add_argument(\"audio\", nargs=\"+\", type=str, help=\"audio file(s) to transcribe\")\n-    parser.add_argument(\"--model\", default=\"small\", choices=available_models(), help=\"name of the Whisper model to use\")\n+    parser.add_argument(\"--model\", default=\"small\", type=valid_model_name, help=\"name of the Whisper model to use\")\n     parser.add_argument(\"--model_dir\", type=str, default=None, help=\"the path to save model files; uses ~/.cache/whisper by default\")\n     parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n     parser.add_argument(\"--output_dir\", \"-o\", type=str, default=\".\", help=\"directory to save the outputs\")\n@@ -412,6 +425,7 @@ def cli():\n     parser.add_argument(\"--highlight_words\", type=str2bool, default=False, help=\"(requires --word_timestamps True) underline each word as it is spoken in srt and vtt\")\n     parser.add_argument(\"--max_line_width\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of characters in a line before breaking the line\")\n     parser.add_argument(\"--max_line_count\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of lines in a segment\")\n+    parser.add_argument(\"--max_words_per_line\", type=optional_int, default=None, help=\"(requires --word_timestamps True, no effect with --max_line_width) the maximum number of words in a segment\")\n     parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n     # fmt: on\n \n@@ -444,17 +458,28 @@ def cli():\n     model = load_model(model_name, device=device, download_root=model_dir)\n \n     writer = get_writer(output_format, output_dir)\n-    word_options = [\"highlight_words\", \"max_line_count\", \"max_line_width\"]\n+    word_options = [\n+        \"highlight_words\",\n+        \"max_line_count\",\n+        \"max_line_width\",\n+        \"max_words_per_line\",\n+    ]\n     if not args[\"word_timestamps\"]:\n         for option in word_options:\n             if args[option]:\n                 parser.error(f\"--{option} requires --word_timestamps True\")\n     if args[\"max_line_count\"] and not args[\"max_line_width\"]:\n         warnings.warn(\"--max_line_count has no effect without --max_line_width\")\n+    if args[\"max_words_per_line\"] and args[\"max_line_width\"]:\n+        warnings.warn(\"--max_words_per_line has no effect with --max_line_width\")\n     writer_args = {arg: args.pop(arg) for arg in word_options}\n     for audio_path in args.pop(\"audio\"):\n-        result = transcribe(model, audio_path, temperature=temperature, **args)\n-        writer(result, audio_path, writer_args)\n+        try:\n+            result = transcribe(model, audio_path, temperature=temperature, **args)\n+            writer(result, audio_path, **writer_args)\n+        except Exception as e:\n+            traceback.print_exc()\n+            print(f\"Skipping {audio_path} due to {type(e).__name__}: {str(e)}\")\n \n \n if __name__ == \"__main__\":\ndiff --git a/whisper/utils.py b/whisper/utils.py\nindex ba5a10c41..7a172c401 100644\n--- a/whisper/utils.py\n+++ b/whisper/utils.py\n@@ -74,7 +74,9 @@ class ResultWriter:\n     def __init__(self, output_dir: str):\n         self.output_dir = output_dir\n \n-    def __call__(self, result: dict, audio_path: str, options: dict):\n+    def __call__(\n+        self, result: dict, audio_path: str, options: Optional[dict] = None, **kwargs\n+    ):\n         audio_basename = os.path.basename(audio_path)\n         audio_basename = os.path.splitext(audio_basename)[0]\n         output_path = os.path.join(\n@@ -82,16 +84,20 @@ def __call__(self, result: dict, audio_path: str, options: dict):\n         )\n \n         with open(output_path, \"w\", encoding=\"utf-8\") as f:\n-            self.write_result(result, file=f, options=options)\n+            self.write_result(result, file=f, options=options, **kwargs)\n \n-    def write_result(self, result: dict, file: TextIO, options: dict):\n+    def write_result(\n+        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n+    ):\n         raise NotImplementedError\n \n \n class WriteTXT(ResultWriter):\n     extension: str = \"txt\"\n \n-    def write_result(self, result: dict, file: TextIO, options: dict):\n+    def write_result(\n+        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n+    ):\n         for segment in result[\"segments\"]:\n             print(segment[\"text\"].strip(), file=file, flush=True)\n \n@@ -100,12 +106,24 @@ class SubtitlesWriter(ResultWriter):\n     always_include_hours: bool\n     decimal_marker: str\n \n-    def iterate_result(self, result: dict, options: dict):\n-        raw_max_line_width: Optional[int] = options[\"max_line_width\"]\n-        max_line_count: Optional[int] = options[\"max_line_count\"]\n-        highlight_words: bool = options[\"highlight_words\"]\n-        max_line_width = 1000 if raw_max_line_width is None else raw_max_line_width\n-        preserve_segments = max_line_count is None or raw_max_line_width is None\n+    def iterate_result(\n+        self,\n+        result: dict,\n+        options: Optional[dict] = None,\n+        *,\n+        max_line_width: Optional[int] = None,\n+        max_line_count: Optional[int] = None,\n+        highlight_words: bool = False,\n+        max_words_per_line: Optional[int] = None,\n+    ):\n+        options = options or {}\n+        max_line_width = max_line_width or options.get(\"max_line_width\")\n+        max_line_count = max_line_count or options.get(\"max_line_count\")\n+        highlight_words = highlight_words or options.get(\"highlight_words\", False)\n+        max_words_per_line = max_words_per_line or options.get(\"max_words_per_line\")\n+        preserve_segments = max_line_count is None or max_line_width is None\n+        max_line_width = max_line_width or 1000\n+        max_words_per_line = max_words_per_line or 1000\n \n         def iterate_subtitles():\n             line_len = 0\n@@ -114,38 +132,54 @@ def iterate_subtitles():\n             subtitle: list[dict] = []\n             last = result[\"segments\"][0][\"words\"][0][\"start\"]\n             for segment in result[\"segments\"]:\n-                for i, original_timing in enumerate(segment[\"words\"]):\n-                    timing = original_timing.copy()\n-                    long_pause = not preserve_segments and timing[\"start\"] - last > 3.0\n-                    has_room = line_len + len(timing[\"word\"]) <= max_line_width\n-                    seg_break = i == 0 and len(subtitle) > 0 and preserve_segments\n-                    if line_len > 0 and has_room and not long_pause and not seg_break:\n-                        # line continuation\n-                        line_len += len(timing[\"word\"])\n-                    else:\n-                        # new line\n-                        timing[\"word\"] = timing[\"word\"].strip()\n+                chunk_index = 0\n+                words_count = max_words_per_line\n+                while chunk_index < len(segment[\"words\"]):\n+                    remaining_words = len(segment[\"words\"]) - chunk_index\n+                    if max_words_per_line > len(segment[\"words\"]) - chunk_index:\n+                        words_count = remaining_words\n+                    for i, original_timing in enumerate(\n+                        segment[\"words\"][chunk_index : chunk_index + words_count]\n+                    ):\n+                        timing = original_timing.copy()\n+                        long_pause = (\n+                            not preserve_segments and timing[\"start\"] - last > 3.0\n+                        )\n+                        has_room = line_len + len(timing[\"word\"]) <= max_line_width\n+                        seg_break = i == 0 and len(subtitle) > 0 and preserve_segments\n                         if (\n-                            len(subtitle) > 0\n-                            and max_line_count is not None\n-                            and (long_pause or line_count >= max_line_count)\n-                            or seg_break\n+                            line_len > 0\n+                            and has_room\n+                            and not long_pause\n+                            and not seg_break\n                         ):\n-                            # subtitle break\n-                            yield subtitle\n-                            subtitle = []\n-                            line_count = 1\n-                        elif line_len > 0:\n-                            # line break\n-                            line_count += 1\n-                            timing[\"word\"] = \"\\n\" + timing[\"word\"]\n-                        line_len = len(timing[\"word\"].strip())\n-                    subtitle.append(timing)\n-                    last = timing[\"start\"]\n+                            # line continuation\n+                            line_len += len(timing[\"word\"])\n+                        else:\n+                            # new line\n+                            timing[\"word\"] = timing[\"word\"].strip()\n+                            if (\n+                                len(subtitle) > 0\n+                                and max_line_count is not None\n+                                and (long_pause or line_count >= max_line_count)\n+                                or seg_break\n+                            ):\n+                                # subtitle break\n+                                yield subtitle\n+                                subtitle = []\n+                                line_count = 1\n+                            elif line_len > 0:\n+                                # line break\n+                                line_count += 1\n+                                timing[\"word\"] = \"\\n\" + timing[\"word\"]\n+                            line_len = len(timing[\"word\"].strip())\n+                        subtitle.append(timing)\n+                        last = timing[\"start\"]\n+                    chunk_index += max_words_per_line\n             if len(subtitle) > 0:\n                 yield subtitle\n \n-        if \"words\" in result[\"segments\"][0]:\n+        if len(result[\"segments\"]) > 0 and \"words\" in result[\"segments\"][0]:\n             for subtitle in iterate_subtitles():\n                 subtitle_start = self.format_timestamp(subtitle[0][\"start\"])\n                 subtitle_end = self.format_timestamp(subtitle[-1][\"end\"])\n@@ -190,9 +224,11 @@ class WriteVTT(SubtitlesWriter):\n     always_include_hours: bool = False\n     decimal_marker: str = \".\"\n \n-    def write_result(self, result: dict, file: TextIO, options: dict):\n+    def write_result(\n+        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n+    ):\n         print(\"WEBVTT\\n\", file=file)\n-        for start, end, text in self.iterate_result(result, options):\n+        for start, end, text in self.iterate_result(result, options, **kwargs):\n             print(f\"{start} --> {end}\\n{text}\\n\", file=file, flush=True)\n \n \n@@ -201,9 +237,11 @@ class WriteSRT(SubtitlesWriter):\n     always_include_hours: bool = True\n     decimal_marker: str = \",\"\n \n-    def write_result(self, result: dict, file: TextIO, options: dict):\n+    def write_result(\n+        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n+    ):\n         for i, (start, end, text) in enumerate(\n-            self.iterate_result(result, options), start=1\n+            self.iterate_result(result, options, **kwargs), start=1\n         ):\n             print(f\"{i}\\n{start} --> {end}\\n{text}\\n\", file=file, flush=True)\n \n@@ -220,7 +258,9 @@ class WriteTSV(ResultWriter):\n \n     extension: str = \"tsv\"\n \n-    def write_result(self, result: dict, file: TextIO, options: dict):\n+    def write_result(\n+        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n+    ):\n         print(\"start\", \"end\", \"text\", sep=\"\\t\", file=file)\n         for segment in result[\"segments\"]:\n             print(round(1000 * segment[\"start\"]), file=file, end=\"\\t\")\n@@ -231,7 +271,9 @@ def write_result(self, result: dict, file: TextIO, options: dict):\n class WriteJSON(ResultWriter):\n     extension: str = \"json\"\n \n-    def write_result(self, result: dict, file: TextIO, options: dict):\n+    def write_result(\n+        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n+    ):\n         json.dump(result, file)\n \n \n@@ -249,9 +291,11 @@ def get_writer(\n     if output_format == \"all\":\n         all_writers = [writer(output_dir) for writer in writers.values()]\n \n-        def write_all(result: dict, file: TextIO, options: dict):\n+        def write_all(\n+            result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n+        ):\n             for writer in all_writers:\n-                writer(result, file, options)\n+                writer(result, file, options, **kwargs)\n \n         return write_all\n \ndiff --git a/whisper/version.py b/whisper/version.py\nindex 572259a53..c96dd9ce4 100644\n--- a/whisper/version.py\n+++ b/whisper/version.py\n@@ -1 +1 @@\n-__version__ = \"20230314\"\n+__version__ = \"20231117\"\n"
  },
  {
    "index": 18,
    "filtered_comments": [
      "I was trying out this exciting branch and ran into this error when running a test:\r\n```\r\n<...>/lib/python3.12/site-packages/django/db/models/lookups.py:30: in __init__\r\n    self.rhs = self.get_prep_lookup()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = TupleIn(<django.db.models.fields.composite.Cols object at 0x107560980>, <django.db.models.sql.query.Query object at 0x1074e23f0>)\r\n\r\n    def get_prep_lookup(self):\r\n        if not isinstance(self.lhs, Cols):\r\n            raise ValueError(\r\n                \"The left-hand side of the 'in' lookup must be an instance of Cols\"\r\n            )\r\n        if not isinstance(self.rhs, Iterable):\r\n>           raise ValueError(\r\n                \"The right-hand side of the 'in' lookup must be an iterable\"\r\n            )\r\nE           ValueError: The right-hand side of the 'in' lookup must be an iterable\r\n```\r\n\r\nThe issue stems from the use of `isnull` like so:\r\n\r\n```\r\nMyModel.objects.filter(\r\n    type_override__severity__isnull=False\r\n).update(severity=\"high\")\r\n```\r\n\r\nCurious if anyone ran into this as well.\r\n\r\nEdited for traceback:\r\n\r\n```\r\n<...>\r\nlib/python3.12/site-packages/django/db/models/sql/compiler.py:2080: in pre_sql_setup\r\n    self.query.add_filter(\"pk__in\", query)\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1601: in add_filter\r\n    self.add_q(Q((filter_lhs, filter_rhs)))\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1617: in add_q\r\n    clause, _ = self._add_q(q_object, self.used_aliases)\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1649: in _add_q\r\n    child_clause, needed_inner = self.build_filter(\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1563: in build_filter\r\n    condition = self.build_lookup(lookups, col, value)\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1393: in build_lookup\r\n    lookup = lookup_class(lhs, rhs)\r\nlib/python3.12/site-packages/django/db/models/lookups.py:30: in __init__\r\n    self.rhs = self.get_prep_lookup()\r\n```\r\n\r\nSo, this is part of `SQLUpdateCompiler` and is coming from the `update` code path.",
      "I may have found one other small issue. When adding a regular `primary_key=True` on a single field, a unique constraint is added. But when using this branch, it becomes an `IntegrityError` instead. Adding a `UniqueConstraint` on the composite fields is a work-a-round but ideally would be captured in this PR. Imo, this PR is sooooo close. I'm excited for it to be merged in.",
      "@grjones , thanks, I appreciate the feedback, I'll look into it. If a model defines `Meta.primary_key`, defining `primary_key=True` on a field should not be possible - could you give me a code example so I know how to reproduce the issue? I didn't know Django added unique constraints to primary keys, I'll check, but isn't that redundant?",
      "> @grjones , thanks, I appreciate the feedback, I'll look into it. If a model defines `Meta.primary_key`, defining `primary_key=True` on a field should not be possible - could you give me a code example so I know how to reproduce the issue? I didn't know Django added unique constraints to primary keys, I'll check, but isn't that redundant?\r\n\r\nI'll see if I can give you a solid failing test. My \"unique constraint\" phrasing might not be exactly right. But ultimately, I believe Django queries the DB first to see if the new object's PK already exists and throws a validation error. The composite key logic doesn't seem to be doing that and so an unhandled IntegrityError is raised instead.",
      "> @grjones , sorry for the late reply, I've been busy last week. Could you give me more specifics? What's the error message you expect?\r\n\r\nActually, I think it's mostly ok. I was using [Django Spanner](https://github.com/googleapis/python-spanner-django) and it's just not quite working with composite keys and will need to be fixed there. I wrote this and it passed. It probably shouldn't say `Id` though?\r\n\r\n```\r\nfrom django.core.exceptions import ValidationError\r\nfrom django.test import TestCase\r\n\r\nfrom .models import Tenant, User\r\n\r\n\r\nclass CompositePKCleanTests(TestCase):\r\n    \"\"\"\r\n    Test the .clean() method of composite_pk models.\r\n    \"\"\"\r\n\r\n    @classmethod\r\n    def setUpTestData(cls):\r\n        cls.tenant = Tenant.objects.create()\r\n\r\n    def test_validation_error_is_raised_when_pk_already_exists(self):\r\n        test_cases = [\r\n            {\"tenant\": self.tenant, \"id\": 2412, \"email\": \"user2412@example.com\"},\r\n            {\"tenant_id\": self.tenant.id, \"id\": 5316, \"email\": \"user5316@example.com\"},\r\n            {\"pk\": (self.tenant.id, 7424), \"email\": \"user7424@example.com\"},\r\n        ]\r\n        expected = \"{'id': ['User with this Id already exists.']}\"\r\n        for fields in test_cases:\r\n            User.objects.create(**fields)\r\n            with self.assertRaisesMessage(ValidationError, expected):\r\n                User(**fields).clean()\r\n```",
      "Thank you so much for taking the time to review my changes @LilyFoote !\r\nI have two questions:\r\n\r\n1. If `Meta.primary_key` is defined, this PR will automatically add a composite field called `primary_key` to the model. What do you think about this approach? I felt like it was easier to handle the composite primary keys this way as we can run checks against the meta class instead of traversing the model's fields for a composite field.\r\n2. I wrote a lot of tests testing the underlying queries made by the ORM. It makes a lot of sense to me, but I haven't seen this type of tests that much in the Django source code - do these tests look okay to you?",
      " \r\n> If `Meta.primary_key` is defined, this PR will automatically add a composite field called `primary_key` to the model. What do you think about this approach?\r\n\r\nI don't feel strongly that this is better or worse than another option here, so happy to go with what you think is best.\r\n\r\n> I wrote a lot of tests testing the underlying queries made by the ORM. It makes a lot of sense to me, but I haven't seen this type of tests that much in the Django source code - do these tests look okay to you?\r\n\r\nI like your tests quite a bit - they're pretty readable and comprehensive. The main issue I have with them is that they're written for specific databases instead of for generic database features. Where possible Django strongly prefers to test based on features because then the tests apply to as many databases as possible (including third party database libraries). I think the asserts of the actual SQL might be a bit tricky to adapt though, so we might need a different way to check what they're checking.\r\n\r\nAlso, after I reviewed yesterday, I thought of some more things:\r\n\r\n* We should add migrations tests to make sure that adding/removing `Meta.primary_key` works correctly and that removing a field that's part of a primary key also does something appropriate.\r\n* We might want tests for composite keys in forms and the admin. Maybe there's other areas too that we need to check the interactions.",
      "Thanks @charettes !\r\n\r\n> Something that came through my mind while reviewing is that we likely want a plan to eventually deprecate `Options.pk` in favor of `Options.primary_key`?\r\n\r\nI'm not sure what you mean by that, I don't think we can, because `Options.pk` refers to the field, while `Options.primary_key` is the list of field names.",
      "So as far as I understand, at the moment `MultiColSource` is used by Django internally to represent `JOIN`s on multiple fields - that's why it has a `sources` field.\r\n\r\nI'm not sure it's the right decision to reuse this for composite fields, which on the other hand don't need `sources`, it just needs to represent a list of `Col`s as an expression.\r\n\r\nLet me know what you think!",
      "> I'm not sure what you mean by that, I don't think we can, because Options.pk refers to the field, while Options.primary_key is the list of field names.\r\n\r\nYou're completely right. In this case is `pk` set to `CompositePrimaryKey` when `Meta.primary_key` is defined and is `primary_key` set when a non-composite primary is used as well?",
      "> > I'm not sure what you mean by that, I don't think we can, because Options.pk refers to the field, while Options.primary_key is the list of field names.\r\n> \r\n> You're completely right. In this case is `pk` set to `CompositePrimaryKey` when `Meta.primary_key` is defined and is `primary_key` set when a non-composite primary is used as well?\r\n\r\nIt would not be set, if it's a regular primary key, `Meta.primary_key` is `None`.",
      "Hey @csirmazbendeguz, thank you for the amazing work out there! I was trying to test this branch on my local with SQLite and realised a few things:\r\n\r\n1. If you run `makemigrations` for a model with a `CompositePrimaryKey`, the resulting migration file has erroneous imports. To fix this, I believe we need to add `django.db.models.fields.composite` path to the `if...elif` block [here](https://github.com/django/django/blob/main/django/db/models/fields/__init__.py#L645).\r\n2. Assume that I have the following models:\r\n\r\n    ```py\r\n    class Author(models.Model):\r\n    name = models.CharField(max_length=100)\r\n\r\n    class Book(models.Model):\r\n        id = models.CompositePrimaryKey(\"author\", \"title\")\r\n        author = models.ForeignKey(Author, on_delete=models.CASCADE, related_name=\"books\")\r\n        title = models.CharField(max_length=255)\r\n    ```\r\n\r\n    With the current implementation, following test fails:\r\n    ```py\r\n    class TestCompositeFks(TestCase):\r\n        def test_composite_fks(self):\r\n            author = Author.objects.create(name=\"Author\")\r\n            book = Book.objects.create(author=author, title=\"Title\")\r\n            list(Author.objects.filter(books__in=[book])) == book\r\n    ```\r\n    with an `OperationalError`, caused by a syntax error. Executed SQL is as following:\r\n    ```SQL\r\n    SELECT\r\n        \"books_author\".\"id\",\r\n        \"books_author\".\"name\"\r\n    FROM\r\n        \"books_author\"\r\n        INNER JOIN \"books_book\" ON (\"books_author\".\"id\" = \"books_book\".\"author_id\")\r\n    WHERE\r\n        \"books_book\".\"author_id\", \"books_book\".\"title\" IN ((1, 'Title'))\r\n    ```\r\n    because LHS in WHERE clause should have been wrapped with parantheses like this:\r\n    ```SQL\r\n    ...\r\n    WHERE\r\n        (\"books_book\".\"author_id\", \"books_book\".\"title\") IN ((1, 'Title'))\r\n    ```\r\n    Unfortunately I didn't have a time to deep-dive to this.\r\n3. Not a big issue but my code editor (VSCode) does not recognize `models.CompositePrimaryKey`, although the import is working fine. This is probably related with Pylance or something that VSCode uses to recognize fields under `models` module.\r\n\r\nAgain thanks for this amazing initiative!  ",
      "@csirmazbendeguz Thanks for your answers, now the above issues seem like fixed, created migration is correct and reverse relation lookup is working as expected. Thank you! \r\n\r\nWhile I was testing it further with the exact [same models](https://github.com/django/django/pull/18056#issuecomment-2158820017), I realized another issue:\r\n\r\n```py\r\nclass TestCompositeFks(TestCase):\r\n    def test_composite_fks(self):\r\n        author = Author.objects.create(name=\"Author\")\r\n        Book.objects.create(author=author, title=\"Title\")\r\n        author = Author.objects.annotate(book_count=Count(\"books\")).get()\r\n        assert author.book_count == 1\r\n```\r\n\r\nThis test fails with the following error:\r\n\r\n```\r\ndjango.db.utils.OperationalError: wrong number of arguments to function COUNT()\r\n```\r\n\r\nExecuted SQL is as following:\r\n\r\n```SQL\r\nSELECT\r\n    \"books_author\".\"id\",\r\n    \"books_author\".\"name\",\r\n    COUNT(\"books_book\".\"author_id\", \"books_book\".\"title\") AS \"book_count\"\r\nFROM\r\n    \"books_author\"\r\n    LEFT OUTER JOIN \"books_book\" ON (\"books_author\".\"id\" = \"books_book\".\"author_id\")\r\nGROUP BY\r\n    \"books_author\".\"id\",\r\n    \"books_author\".\"name\"\r\n```\r\n\r\nIf we could change the parameter we pass to the `COUNT` function to a concatenation as below:\r\n\r\n```SQL\r\nCOUNT(\"books_book\".\"author_id\" || '-' || \"books_book\".\"title\")\r\n```\r\n\r\nit should work fine (if I am not missing something), with the exception that for some databases we need to use `CONCAT` function instead of `||` operator, which might be resolved using the existing `db.models.functions.Concat` function.\r\n\r\nNote: I am not sure if concatenation works between every data type that is allowed to be a primary key, although this could be considered as an edge case.",
      "@omerfarukabaci , I thought about the issue of `Count(\"books\")`.\r\n\r\nMy conclusion is we can't support this.\r\n\r\nI don't think concatenating is a good solution. The only way we could support this is if we could get Django to count this with `*` instead of the primary key.\r\n\r\nThis is an edge case that is only needed for `Count` though, and it's not as simple to implement as it is to explain.\r\n\r\nI added a section to the docs about this. This is a case of using a database function with a composite primary key directly, which cannot be expected to work in general.\r\n\r\nIn your case, `Count(\"books__author_id\")` would do the trick instead.",
      "Regarding the issue raised by @sarahboyce last week...\r\n\r\nI think it is okay to merge this without support for generic relations. I added a section to the docs about this not being supported for now.\r\n\r\nThe only impact is some third-party packages using generic relations won't work with composite primary keys (e.g. `django-guardian`).\r\n\r\nLet's have a separate discussion on how to support this. I lean towards storing composite primary keys serialized as JSON in a single CharField.",
      "Btw, semantically it would be nice if it were possible to write:\r\n```python \r\nclass User(models.Model):\r\n    pk = models.CompositePrimaryKey(\"tenant_id\", \"id\")\r\n    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE)\r\n    id = models.IntegerField()\r\n```\r\n\r\nie to let `CompositePrimaryKey` replace the automatically generated `pk`. Would that be possible?",
      "> Btw, semantically it would be nice if it were possible to write:\r\n> \r\n> ```python\r\n> class User(models.Model):\r\n>     pk = models.CompositePrimaryKey(\"tenant_id\", \"id\")\r\n>     tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE)\r\n>     id = models.IntegerField()\r\n> ```\r\n> \r\n> ie to let `CompositePrimaryKey` replace the automatically generated `pk`. Would that be possible?\r\n\r\n@apollo13 , good point! It also came up when we were discussing this with @LilyFoote and @charettes . It seems like a natural thing to do, so it's worth a discussion. Here are a couple ideas that make sense to me:\r\n\r\n1. `pk` at the moment is reserved, users can't add a field named `pk`. We could remove this restriction.\r\n2. If `pk` is defined, it should always set `primary_key=True`.\r\n3. If `pk` is not defined, it should still refer to the `primary_key=True` field (e.g. `id` field). This is required for backwards-compatibility.\r\n4. If `pk` is defined, and it's an `IntegerField`, then a field called `pk` should be created in the database (same as any field, e.g. `id`).\r\n5. If `pk` is defined, and it's a `CompositePrimaryKey`, then a field called `pk` shouldn't be created in the database (same as any field, e.g. `primary_key`).\r\n\r\nMy only issue with this is, it adds extra complexity to how `pk` works. In this case, `pk` can be both a reference to the primary key field, or the primary key field itself.\r\n\r\nSo I'm not sure if it's worth doing this. It doesn't feel like an elegant or consistent solution to me.\r\n\r\n---\r\n\r\nThe other approach @charettes and @LilyFoote mentioned is to always have `pk` be a `CompositePrimaryKey` (could be renamed to `PrimaryKey`):\r\n\r\n1. `pk` cannot be defined explicitly.\r\n2. `CompositePrimaryKey` cannot be used explicitly.\r\n3. `pk` is _always_ added to the model in the background, and it's _always_ an instance of `CompositePrimaryKey`.\r\n4. Consequently, `pk` will cease to be a reference to another field, it will always be a field itself.\r\n5. If field `x` defines `primary_key=True`, `pk` is `CompositePrimaryKey(\"x\")`. `obj.pk` returns the value of `x` for backwards-compatibility (instead of a tuple).\r\n6. If `Meta.primary_key` option is `(\"a\", \"b\", \"c\")`, `pk` is `CompositePrimaryKey(\"a\", \"b\", \"c\")`. `obj.pk` returns a tuple.\r\n7. If `Meta.primary_key` is not set, it could be set to `(\"x\",)` automatically.\r\n\r\nThis is quite an invasive change. It would mean all existing models get a new field called `pk`.\r\n`meta.pk` would return a different field. Instead of `IntegerField`, it would return `CompositePrimaryKey`. Is breaking backwards-compatibility okay here?\r\n\r\nI don't have anything against it other than that. It does feel more intuitive. If the community wants this, I could fork this branch and open another PR."
    ],
    "code_diff": "diff --git a/django/contrib/admin/sites.py b/django/contrib/admin/sites.py\nindex 3399bd87b85a..201f28ef3744 100644\n--- a/django/contrib/admin/sites.py\n+++ b/django/contrib/admin/sites.py\n@@ -113,6 +113,11 @@ def register(self, model_or_iterable, admin_class=None, **options):\n                     \"The model %s is abstract, so it cannot be registered with admin.\"\n                     % model.__name__\n                 )\n+            if model._meta.is_composite_pk:\n+                raise ImproperlyConfigured(\n+                    \"The model %s has a composite primary key, so it cannot be \"\n+                    \"registered with admin.\" % model.__name__\n+                )\n \n             if self.is_registered(model):\n                 registered_admin = str(self.get_model_admin(model))\ndiff --git a/django/core/serializers/python.py b/django/core/serializers/python.py\nindex 46ef9f077161..57edebbb70e4 100644\n--- a/django/core/serializers/python.py\n+++ b/django/core/serializers/python.py\n@@ -7,6 +7,7 @@\n from django.apps import apps\n from django.core.serializers import base\n from django.db import DEFAULT_DB_ALIAS, models\n+from django.db.models import CompositePrimaryKey\n from django.utils.encoding import is_protected_type\n \n \n@@ -39,6 +40,8 @@ def get_dump_object(self, obj):\n         return data\n \n     def _value_from_field(self, obj, field):\n+        if isinstance(field, CompositePrimaryKey):\n+            return [self._value_from_field(obj, f) for f in field]\n         value = field.value_from_object(obj)\n         # Protected types (i.e., primitives like None, numbers, dates,\n         # and Decimals) are passed through as is. All other values are\ndiff --git a/django/db/backends/base/schema.py b/django/db/backends/base/schema.py\nindex 3e38c56d50d4..de4886837ecf 100644\n--- a/django/db/backends/base/schema.py\n+++ b/django/db/backends/base/schema.py\n@@ -14,6 +14,7 @@\n )\n from django.db.backends.utils import names_digest, split_identifier, truncate_name\n from django.db.models import NOT_PROVIDED, Deferrable, Index\n+from django.db.models.fields.composite import CompositePrimaryKey\n from django.db.models.sql import Query\n from django.db.transaction import TransactionManagementError, atomic\n from django.utils import timezone\n@@ -106,6 +107,7 @@ class BaseDatabaseSchemaEditor:\n     sql_check_constraint = \"CHECK (%(check)s)\"\n     sql_delete_constraint = \"ALTER TABLE %(table)s DROP CONSTRAINT %(name)s\"\n     sql_constraint = \"CONSTRAINT %(name)s %(constraint)s\"\n+    sql_pk_constraint = \"PRIMARY KEY (%(columns)s)\"\n \n     sql_create_check = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s CHECK (%(check)s)\"\n     sql_delete_check = sql_delete_constraint\n@@ -282,6 +284,11 @@ def table_sql(self, model):\n                 constraint.constraint_sql(model, self)\n                 for constraint in model._meta.constraints\n             )\n+\n+        pk = model._meta.pk\n+        if isinstance(pk, CompositePrimaryKey):\n+            constraint_sqls.append(self._pk_constraint_sql(pk.columns))\n+\n         sql = self.sql_create_table % {\n             \"table\": self.quote_name(model._meta.db_table),\n             \"definition\": \", \".join(\n@@ -1999,6 +2006,11 @@ def _constraint_names(\n                     result.append(name)\n         return result\n \n+    def _pk_constraint_sql(self, columns):\n+        return self.sql_pk_constraint % {\n+            \"columns\": \", \".join(self.quote_name(column) for column in columns)\n+        }\n+\n     def _delete_primary_key(self, model, strict=False):\n         constraint_names = self._constraint_names(model, primary_key=True)\n         if strict and len(constraint_names) != 1:\ndiff --git a/django/db/backends/oracle/schema.py b/django/db/backends/oracle/schema.py\nindex 0d70522a2afa..ba3c4778d30d 100644\n--- a/django/db/backends/oracle/schema.py\n+++ b/django/db/backends/oracle/schema.py\n@@ -211,6 +211,8 @@ def _field_should_be_indexed(self, model, field):\n         return create_index\n \n     def _is_identity_column(self, table_name, column_name):\n+        if not column_name:\n+            return False\n         with self.connection.cursor() as cursor:\n             cursor.execute(\n                 \"\"\"\ndiff --git a/django/db/backends/sqlite3/schema.py b/django/db/backends/sqlite3/schema.py\nindex c5b428fc678c..6da98522822f 100644\n--- a/django/db/backends/sqlite3/schema.py\n+++ b/django/db/backends/sqlite3/schema.py\n@@ -6,7 +6,7 @@\n from django.db.backends.base.schema import BaseDatabaseSchemaEditor\n from django.db.backends.ddl_references import Statement\n from django.db.backends.utils import strip_quotes\n-from django.db.models import NOT_PROVIDED, UniqueConstraint\n+from django.db.models import NOT_PROVIDED, CompositePrimaryKey, UniqueConstraint\n \n \n class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n@@ -104,6 +104,13 @@ def is_self_referential(f):\n             f.name: f.clone() if is_self_referential(f) else f\n             for f in model._meta.local_concrete_fields\n         }\n+\n+        # Since CompositePrimaryKey is not a concrete field (column is None),\n+        # it's not copied by default.\n+        pk = model._meta.pk\n+        if isinstance(pk, CompositePrimaryKey):\n+            body[pk.name] = pk.clone()\n+\n         # Since mapping might mix column names and default values,\n         # its values must be already quoted.\n         mapping = {\n@@ -296,6 +303,12 @@ def add_field(self, model, field):\n         # Special-case implicit M2M tables.\n         if field.many_to_many and field.remote_field.through._meta.auto_created:\n             self.create_model(field.remote_field.through)\n+        elif isinstance(field, CompositePrimaryKey):\n+            # If a CompositePrimaryKey field was added, the existing primary key field\n+            # had to be altered too, resulting in an AddField, AlterField migration.\n+            # The table cannot be re-created on AddField, it would result in a\n+            # duplicate primary key error.\n+            return\n         elif (\n             # Primary keys and unique fields are not supported in ALTER TABLE\n             # ADD COLUMN.\ndiff --git a/django/db/models/__init__.py b/django/db/models/__init__.py\nindex fe81d92d3666..ec54b652409d 100644\n--- a/django/db/models/__init__.py\n+++ b/django/db/models/__init__.py\n@@ -38,6 +38,7 @@\n )\n from django.db.models.fields import *  # NOQA\n from django.db.models.fields import __all__ as fields_all\n+from django.db.models.fields.composite import CompositePrimaryKey\n from django.db.models.fields.files import FileField, ImageField\n from django.db.models.fields.generated import GeneratedField\n from django.db.models.fields.json import JSONField\n@@ -82,6 +83,7 @@\n     \"ProtectedError\",\n     \"RestrictedError\",\n     \"Case\",\n+    \"CompositePrimaryKey\",\n     \"Exists\",\n     \"Expression\",\n     \"ExpressionList\",\ndiff --git a/django/db/models/aggregates.py b/django/db/models/aggregates.py\nindex bf94decab7a8..73f03a4916f2 100644\n--- a/django/db/models/aggregates.py\n+++ b/django/db/models/aggregates.py\n@@ -3,7 +3,8 @@\n \"\"\"\n \n from django.core.exceptions import FieldError, FullResultSet\n-from django.db.models.expressions import Case, Func, Star, Value, When\n+from django.db import NotSupportedError\n+from django.db.models.expressions import Case, ColPairs, Func, Star, Value, When\n from django.db.models.fields import IntegerField\n from django.db.models.functions.comparison import Coalesce\n from django.db.models.functions.mixins import (\n@@ -174,6 +175,22 @@ def __init__(self, expression, filter=None, **extra):\n             raise ValueError(\"Star cannot be used with filter. Please specify a field.\")\n         super().__init__(expression, filter=filter, **extra)\n \n+    def resolve_expression(self, *args, **kwargs):\n+        result = super().resolve_expression(*args, **kwargs)\n+        expr = result.source_expressions[0]\n+\n+        # In case of composite primary keys, count the first column.\n+        if isinstance(expr, ColPairs):\n+            if self.distinct:\n+                raise NotSupportedError(\n+                    \"COUNT(DISTINCT) doesn't support composite primary keys\"\n+                )\n+\n+            cols = expr.get_cols()\n+            return Count(cols[0], filter=result.filter)\n+\n+        return result\n+\n \n class Max(Aggregate):\n     function = \"MAX\"\ndiff --git a/django/db/models/base.py b/django/db/models/base.py\nindex 5b819b1406a2..a20e88749f5a 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -1,6 +1,7 @@\n import copy\n import inspect\n import warnings\n+from collections import defaultdict\n from functools import partialmethod\n from itertools import chain\n \n@@ -30,6 +31,7 @@\n from django.db.models.constants import LOOKUP_SEP\n from django.db.models.deletion import CASCADE, Collector\n from django.db.models.expressions import DatabaseDefault\n+from django.db.models.fields.composite import CompositePrimaryKey\n from django.db.models.fields.related import (\n     ForeignObjectRel,\n     OneToOneField,\n@@ -508,7 +510,7 @@ def __init__(self, *args, **kwargs):\n         for field in fields_iter:\n             is_related_object = False\n             # Virtual field\n-            if field.attname not in kwargs and field.column is None or field.generated:\n+            if field.column is None or field.generated:\n                 continue\n             if kwargs:\n                 if isinstance(field.remote_field, ForeignObjectRel):\n@@ -663,7 +665,11 @@ def _set_pk_val(self, value):\n     pk = property(_get_pk_val, _set_pk_val)\n \n     def _is_pk_set(self, meta=None):\n-        return self._get_pk_val(meta) is not None\n+        pk_val = self._get_pk_val(meta)\n+        return not (\n+            pk_val is None\n+            or (isinstance(pk_val, tuple) and any(f is None for f in pk_val))\n+        )\n \n     def get_deferred_fields(self):\n         \"\"\"\n@@ -1454,6 +1460,11 @@ def _get_unique_checks(self, exclude=None, include_meta_constraints=False):\n                 name = f.name\n                 if name in exclude:\n                     continue\n+                if isinstance(f, CompositePrimaryKey):\n+                    names = tuple(field.name for field in f.fields)\n+                    if exclude.isdisjoint(names):\n+                        unique_checks.append((model_class, names))\n+                    continue\n                 if f.unique:\n                     unique_checks.append((model_class, (name,)))\n                 if f.unique_for_date and f.unique_for_date not in exclude:\n@@ -1728,6 +1739,7 @@ def check(cls, **kwargs):\n                 *cls._check_constraints(databases),\n                 *cls._check_default_pk(),\n                 *cls._check_db_table_comment(databases),\n+                *cls._check_composite_pk(),\n             ]\n \n         return errors\n@@ -1764,6 +1776,63 @@ def _check_default_pk(cls):\n             ]\n         return []\n \n+    @classmethod\n+    def _check_composite_pk(cls):\n+        errors = []\n+        meta = cls._meta\n+        pk = meta.pk\n+\n+        if not isinstance(pk, CompositePrimaryKey):\n+            return errors\n+\n+        seen_columns = defaultdict(list)\n+\n+        for field_name in pk.field_names:\n+            hint = None\n+\n+            try:\n+                field = meta.get_field(field_name)\n+            except FieldDoesNotExist:\n+                field = None\n+\n+            if not field:\n+                hint = f\"{field_name!r} is not a valid field.\"\n+            elif not field.column:\n+                hint = f\"{field_name!r} field has no column.\"\n+            elif field.null:\n+                hint = f\"{field_name!r} field may not set 'null=True'.\"\n+            elif field.generated:\n+                hint = f\"{field_name!r} field is a generated field.\"\n+            else:\n+                seen_columns[field.column].append(field_name)\n+\n+            if hint:\n+                errors.append(\n+                    checks.Error(\n+                        f\"{field_name!r} cannot be included in the composite primary \"\n+                        \"key.\",\n+                        hint=hint,\n+                        obj=cls,\n+                        id=\"models.E042\",\n+                    )\n+                )\n+\n+        for column, field_names in seen_columns.items():\n+            if len(field_names) > 1:\n+                field_name, *rest = field_names\n+                duplicates = \", \".join(repr(field) for field in rest)\n+                errors.append(\n+                    checks.Error(\n+                        f\"{duplicates} cannot be included in the composite primary \"\n+                        \"key.\",\n+                        hint=f\"{duplicates} and {field_name!r} are the same fields.\",\n+                        obj=cls,\n+                        id=\"models.E042\",\n+                    )\n+                )\n+\n+        return errors\n+\n     @classmethod\n     def _check_db_table_comment(cls, databases):\n         if not cls._meta.db_table_comment:\ndiff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex f9cafdb4bb40..855e8cc28d24 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -656,6 +656,8 @@ def deconstruct(self):\n             path = path.replace(\"django.db.models.fields.json\", \"django.db.models\")\n         elif path.startswith(\"django.db.models.fields.proxy\"):\n             path = path.replace(\"django.db.models.fields.proxy\", \"django.db.models\")\n+        elif path.startswith(\"django.db.models.fields.composite\"):\n+            path = path.replace(\"django.db.models.fields.composite\", \"django.db.models\")\n         elif path.startswith(\"django.db.models.fields\"):\n             path = path.replace(\"django.db.models.fields\", \"django.db.models\")\n         # Return basic info - other fields should override this.\ndiff --git a/django/db/models/fields/composite.py b/django/db/models/fields/composite.py\nnew file mode 100644\nindex 000000000000..550a440dcf39\n--- /dev/null\n+++ b/django/db/models/fields/composite.py\n@@ -0,0 +1,150 @@\n+from django.core import checks\n+from django.db.models import NOT_PROVIDED, Field\n+from django.db.models.expressions import ColPairs\n+from django.db.models.fields.tuple_lookups import (\n+    TupleExact,\n+    TupleGreaterThan,\n+    TupleGreaterThanOrEqual,\n+    TupleIn,\n+    TupleIsNull,\n+    TupleLessThan,\n+    TupleLessThanOrEqual,\n+)\n+from django.utils.functional import cached_property\n+\n+\n+class CompositeAttribute:\n+    def __init__(self, field):\n+        self.field = field\n+\n+    @property\n+    def attnames(self):\n+        return [field.attname for field in self.field.fields]\n+\n+    def __get__(self, instance, cls=None):\n+        return tuple(getattr(instance, attname) for attname in self.attnames)\n+\n+    def __set__(self, instance, values):\n+        attnames = self.attnames\n+        length = len(attnames)\n+\n+        if values is None:\n+            values = (None,) * length\n+\n+        if not isinstance(values, (list, tuple)):\n+            raise ValueError(f\"{self.field.name!r} must be a list or a tuple.\")\n+        if length != len(values):\n+            raise ValueError(f\"{self.field.name!r} must have {length} elements.\")\n+\n+        for attname, value in zip(attnames, values):\n+            setattr(instance, attname, value)\n+\n+\n+class CompositePrimaryKey(Field):\n+    descriptor_class = CompositeAttribute\n+\n+    def __init__(self, *args, **kwargs):\n+        if (\n+            not args\n+            or not all(isinstance(field, str) for field in args)\n+            or len(set(args)) != len(args)\n+        ):\n+            raise ValueError(\"CompositePrimaryKey args must be unique strings.\")\n+        if len(args) == 1:\n+            raise ValueError(\"CompositePrimaryKey must include at least two fields.\")\n+        if kwargs.get(\"default\", NOT_PROVIDED) is not NOT_PROVIDED:\n+            raise ValueError(\"CompositePrimaryKey cannot have a default.\")\n+        if kwargs.get(\"db_default\", NOT_PROVIDED) is not NOT_PROVIDED:\n+            raise ValueError(\"CompositePrimaryKey cannot have a database default.\")\n+        if kwargs.setdefault(\"editable\", False):\n+            raise ValueError(\"CompositePrimaryKey cannot be editable.\")\n+        if not kwargs.setdefault(\"primary_key\", True):\n+            raise ValueError(\"CompositePrimaryKey must be a primary key.\")\n+        if not kwargs.setdefault(\"blank\", True):\n+            raise ValueError(\"CompositePrimaryKey must be blank.\")\n+\n+        self.field_names = args\n+        super().__init__(**kwargs)\n+\n+    def deconstruct(self):\n+        # args is always [] so it can be ignored.\n+        name, path, _, kwargs = super().deconstruct()\n+        return name, path, self.field_names, kwargs\n+\n+    @cached_property\n+    def fields(self):\n+        meta = self.model._meta\n+        return tuple(meta.get_field(field_name) for field_name in self.field_names)\n+\n+    @cached_property\n+    def columns(self):\n+        return tuple(field.column for field in self.fields)\n+\n+    def contribute_to_class(self, cls, name, private_only=False):\n+        super().contribute_to_class(cls, name, private_only=private_only)\n+        cls._meta.pk = self\n+        setattr(cls, self.attname, self.descriptor_class(self))\n+\n+    def get_attname_column(self):\n+        return self.get_attname(), None\n+\n+    def __iter__(self):\n+        return iter(self.fields)\n+\n+    def __len__(self):\n+        return len(self.field_names)\n+\n+    @cached_property\n+    def cached_col(self):\n+        return ColPairs(self.model._meta.db_table, self.fields, self.fields, self)\n+\n+    def get_col(self, alias, output_field=None):\n+        if alias == self.model._meta.db_table and (\n+            output_field is None or output_field == self\n+        ):\n+            return self.cached_col\n+\n+        return ColPairs(alias, self.fields, self.fields, output_field)\n+\n+    def get_pk_value_on_save(self, instance):\n+        values = []\n+\n+        for field in self.fields:\n+            value = field.value_from_object(instance)\n+            if value is None:\n+                value = field.get_pk_value_on_save(instance)\n+            values.append(value)\n+\n+        return tuple(values)\n+\n+    def _check_field_name(self):\n+        if self.name == \"pk\":\n+            return []\n+        return [\n+            checks.Error(\n+                \"'CompositePrimaryKey' must be named 'pk'.\",\n+                obj=self,\n+                id=\"fields.E013\",\n+            )\n+        ]\n+\n+\n+CompositePrimaryKey.register_lookup(TupleExact)\n+CompositePrimaryKey.register_lookup(TupleGreaterThan)\n+CompositePrimaryKey.register_lookup(TupleGreaterThanOrEqual)\n+CompositePrimaryKey.register_lookup(TupleLessThan)\n+CompositePrimaryKey.register_lookup(TupleLessThanOrEqual)\n+CompositePrimaryKey.register_lookup(TupleIn)\n+CompositePrimaryKey.register_lookup(TupleIsNull)\n+\n+\n+def unnest(fields):\n+    result = []\n+\n+    for field in fields:\n+        if isinstance(field, CompositePrimaryKey):\n+            result.extend(field.fields)\n+        else:\n+            result.append(field)\n+\n+    return result\ndiff --git a/django/db/models/fields/related.py b/django/db/models/fields/related.py\nindex b672a4b488ef..9ef2d2902475 100644\n--- a/django/db/models/fields/related.py\n+++ b/django/db/models/fields/related.py\n@@ -624,11 +624,21 @@ def _check_unique_target(self):\n         if not has_unique_constraint:\n             foreign_fields = {f.name for f in self.foreign_related_fields}\n             remote_opts = self.remote_field.model._meta\n-            has_unique_constraint = any(\n-                frozenset(ut) <= foreign_fields for ut in remote_opts.unique_together\n-            ) or any(\n-                frozenset(uc.fields) <= foreign_fields\n-                for uc in remote_opts.total_unique_constraints\n+            has_unique_constraint = (\n+                any(\n+                    frozenset(ut) <= foreign_fields\n+                    for ut in remote_opts.unique_together\n+                )\n+                or any(\n+                    frozenset(uc.fields) <= foreign_fields\n+                    for uc in remote_opts.total_unique_constraints\n+                )\n+                # If the model defines a composite primary key and the foreign key\n+                # refers to it, the target is unique.\n+                or (\n+                    frozenset(field.name for field in remote_opts.pk_fields)\n+                    == foreign_fields\n+                )\n             )\n \n         if not has_unique_constraint:\ndiff --git a/django/db/models/fields/related_lookups.py b/django/db/models/fields/related_lookups.py\nindex 8b5663dfea47..6992d75833f6 100644\n--- a/django/db/models/fields/related_lookups.py\n+++ b/django/db/models/fields/related_lookups.py\n@@ -1,5 +1,6 @@\n from django.db import NotSupportedError\n from django.db.models.expressions import ColPairs\n+from django.db.models.fields import composite\n from django.db.models.fields.tuple_lookups import TupleIn, tuple_lookups\n from django.db.models.lookups import (\n     Exact,\n@@ -19,7 +20,7 @@ def get_normalized_value(value, lhs):\n         if not value._is_pk_set():\n             raise ValueError(\"Model instances passed to related filters must be saved.\")\n         value_list = []\n-        sources = lhs.output_field.path_infos[-1].target_fields\n+        sources = composite.unnest(lhs.output_field.path_infos[-1].target_fields)\n         for source in sources:\n             while not isinstance(value, source.model) and source.remote_field:\n                 source = source.remote_field.model._meta.get_field(\n@@ -30,7 +31,8 @@ def get_normalized_value(value, lhs):\n             except AttributeError:\n                 # A case like Restaurant.objects.filter(place=restaurant_instance),\n                 # where place is a OneToOneField and the primary key of Restaurant.\n-                return (value.pk,)\n+                pk = value.pk\n+                return pk if isinstance(pk, tuple) else (pk,)\n         return tuple(value_list)\n     if not isinstance(value, tuple):\n         return (value,)\ndiff --git a/django/db/models/fields/tuple_lookups.py b/django/db/models/fields/tuple_lookups.py\nindex 6342937cd6ce..e515e971b400 100644\n--- a/django/db/models/fields/tuple_lookups.py\n+++ b/django/db/models/fields/tuple_lookups.py\n@@ -250,6 +250,8 @@ def check_rhs_is_query(self):\n \n     def check_rhs_select_length_equals_lhs_length(self):\n         len_rhs = len(self.rhs.select)\n+        if len_rhs == 1 and isinstance(self.rhs.select[0], ColPairs):\n+            len_rhs = len(self.rhs.select[0])\n         len_lhs = len(self.lhs)\n         if len_rhs != len_lhs:\n             lhs_str = self.get_lhs_str()\n@@ -304,7 +306,13 @@ def as_sqlite(self, compiler, connection):\n         return root.as_sql(compiler, connection)\n \n     def as_subquery(self, compiler, connection):\n-        return compiler.compile(In(self.lhs, self.rhs))\n+        lhs = self.lhs\n+        rhs = self.rhs\n+        if isinstance(lhs, ColPairs):\n+            rhs = rhs.clone()\n+            rhs.set_values([source.name for source in lhs.sources])\n+            lhs = Tuple(lhs)\n+        return compiler.compile(In(lhs, rhs))\n \n \n tuple_lookups = {\ndiff --git a/django/db/models/options.py b/django/db/models/options.py\nindex 68a7228cbea6..7c4cf2229a31 100644\n--- a/django/db/models/options.py\n+++ b/django/db/models/options.py\n@@ -7,7 +7,14 @@\n from django.core.exceptions import FieldDoesNotExist, ImproperlyConfigured\n from django.core.signals import setting_changed\n from django.db import connections\n-from django.db.models import AutoField, Manager, OrderWrt, UniqueConstraint\n+from django.db.models import (\n+    AutoField,\n+    CompositePrimaryKey,\n+    Manager,\n+    OrderWrt,\n+    UniqueConstraint,\n+)\n+from django.db.models.fields import composite\n from django.db.models.query_utils import PathInfo\n from django.utils.datastructures import ImmutableList, OrderedSet\n from django.utils.functional import cached_property\n@@ -973,6 +980,14 @@ def total_unique_constraints(self):\n             )\n         ]\n \n+    @cached_property\n+    def pk_fields(self):\n+        return composite.unnest([self.pk])\n+\n+    @property\n+    def is_composite_pk(self):\n+        return isinstance(self.pk, CompositePrimaryKey)\n+\n     @cached_property\n     def _property_names(self):\n         \"\"\"Return a set of the names of the properties defined on the model.\"\"\"\ndiff --git a/django/db/models/query.py b/django/db/models/query.py\nindex 21d5534cc9fc..ea8cc179f3f9 100644\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -171,11 +171,14 @@ def __iter__(self):\n                     \"Raw query must include the primary key\"\n                 )\n             fields = [self.queryset.model_fields.get(c) for c in self.queryset.columns]\n-            converters = compiler.get_converters(\n-                [f.get_col(f.model._meta.db_table) if f else None for f in fields]\n-            )\n+            cols = [f.get_col(f.model._meta.db_table) if f else None for f in fields]\n+            converters = compiler.get_converters(cols)\n             if converters:\n                 query_iterator = compiler.apply_converters(query_iterator, converters)\n+            if compiler.has_composite_fields(cols):\n+                query_iterator = compiler.composite_fields_to_tuples(\n+                    query_iterator, cols\n+                )\n             for values in query_iterator:\n                 # Associate fields to values\n                 model_init_values = [values[pos] for pos in model_init_pos]\ndiff --git a/django/db/models/sql/compiler.py b/django/db/models/sql/compiler.py\nindex 49263d5944e4..053bdc09d589 100644\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -7,7 +7,9 @@\n from django.core.exceptions import EmptyResultSet, FieldError, FullResultSet\n from django.db import DatabaseError, NotSupportedError\n from django.db.models.constants import LOOKUP_SEP\n-from django.db.models.expressions import F, OrderBy, RawSQL, Ref, Value\n+from django.db.models.expressions import ColPairs, F, OrderBy, RawSQL, Ref, Value\n+from django.db.models.fields import composite\n+from django.db.models.fields.composite import CompositePrimaryKey\n from django.db.models.functions import Cast, Random\n from django.db.models.lookups import Lookup\n from django.db.models.query_utils import select_related_descend\n@@ -283,6 +285,9 @@ def get_select(self, with_col_aliases=False):\n                 # Reference to a column.\n                 elif isinstance(expression, int):\n                     expression = cols[expression]\n+                # ColPairs cannot be aliased.\n+                if isinstance(expression, ColPairs):\n+                    alias = None\n                 selected.append((alias, expression))\n \n         for select_idx, (alias, expression) in enumerate(selected):\n@@ -997,6 +1002,7 @@ def get_default_columns(\n         # alias for a given field. This also includes None -> start_alias to\n         # be used by local fields.\n         seen_models = {None: start_alias}\n+        select_mask_fields = set(composite.unnest(select_mask))\n \n         for field in opts.concrete_fields:\n             model = field.model._meta.concrete_model\n@@ -1017,7 +1023,7 @@ def get_default_columns(\n                 # parent model data is already present in the SELECT clause,\n                 # and we want to avoid reloading the same data again.\n                 continue\n-            if select_mask and field not in select_mask:\n+            if select_mask and field not in select_mask_fields:\n                 continue\n             alias = self.query.join_parent_model(opts, model, start_alias, seen_models)\n             column = field.get_col(alias)\n@@ -1110,9 +1116,10 @@ def find_ordering_name(\n                 )\n             return results\n         targets, alias, _ = self.query.trim_joins(targets, joins, path)\n+        target_fields = composite.unnest(targets)\n         return [\n             (OrderBy(transform_function(t, alias), descending=descending), False)\n-            for t in targets\n+            for t in target_fields\n         ]\n \n     def _setup_joins(self, pieces, opts, alias):\n@@ -1504,13 +1511,25 @@ def _get_field_choices():\n         return result\n \n     def get_converters(self, expressions):\n+        i = 0\n         converters = {}\n-        for i, expression in enumerate(expressions):\n-            if expression:\n+\n+        for expression in expressions:\n+            if isinstance(expression, ColPairs):\n+                cols = expression.get_source_expressions()\n+                cols_converters = self.get_converters(cols)\n+                for j, (convs, col) in cols_converters.items():\n+                    converters[i + j] = (convs, col)\n+                i += len(expression)\n+            elif expression:\n                 backend_converters = self.connection.ops.get_db_converters(expression)\n                 field_converters = expression.get_db_converters(self.connection)\n                 if backend_converters or field_converters:\n                     converters[i] = (backend_converters + field_converters, expression)\n+                i += 1\n+            else:\n+                i += 1\n+\n         return converters\n \n     def apply_converters(self, rows, converters):\n@@ -1524,6 +1543,24 @@ def apply_converters(self, rows, converters):\n                 row[pos] = value\n             yield row\n \n+    def has_composite_fields(self, expressions):\n+        # Check for composite fields before calling the relatively costly\n+        # composite_fields_to_tuples.\n+        return any(isinstance(expression, ColPairs) for expression in expressions)\n+\n+    def composite_fields_to_tuples(self, rows, expressions):\n+        col_pair_slices = [\n+            slice(i, i + len(expression))\n+            for i, expression in enumerate(expressions)\n+            if isinstance(expression, ColPairs)\n+        ]\n+\n+        for row in map(list, rows):\n+            for pos in col_pair_slices:\n+                row[pos] = (tuple(row[pos]),)\n+\n+            yield row\n+\n     def results_iter(\n         self,\n         results=None,\n@@ -1541,8 +1578,10 @@ def results_iter(\n         rows = chain.from_iterable(results)\n         if converters:\n             rows = self.apply_converters(rows, converters)\n-            if tuple_expected:\n-                rows = map(tuple, rows)\n+        if self.has_composite_fields(fields):\n+            rows = self.composite_fields_to_tuples(rows, fields)\n+        if tuple_expected:\n+            rows = map(tuple, rows)\n         return rows\n \n     def has_results(self):\n@@ -1863,6 +1902,18 @@ def execute_sql(self, returning_fields=None):\n                     )\n                 ]\n                 cols = [field.get_col(opts.db_table) for field in self.returning_fields]\n+            elif isinstance(opts.pk, CompositePrimaryKey):\n+                returning_field = returning_fields[0]\n+                cols = [returning_field.get_col(opts.db_table)]\n+                rows = [\n+                    (\n+                        self.connection.ops.last_insert_id(\n+                            cursor,\n+                            opts.db_table,\n+                            returning_field.column,\n+                        ),\n+                    )\n+                ]\n             else:\n                 cols = [opts.pk.get_col(opts.db_table)]\n                 rows = [\n@@ -1876,8 +1927,10 @@ def execute_sql(self, returning_fields=None):\n                 ]\n         converters = self.get_converters(cols)\n         if converters:\n-            rows = list(self.apply_converters(rows, converters))\n-        return rows\n+            rows = self.apply_converters(rows, converters)\n+        if self.has_composite_fields(cols):\n+            rows = self.composite_fields_to_tuples(rows, cols)\n+        return list(rows)\n \n \n class SQLDeleteCompiler(SQLCompiler):\n@@ -2065,6 +2118,7 @@ def pre_sql_setup(self):\n         query.add_fields(fields)\n         super().pre_sql_setup()\n \n+        is_composite_pk = meta.is_composite_pk\n         must_pre_select = (\n             count > 1 and not self.connection.features.update_can_self_select\n         )\n@@ -2079,7 +2133,8 @@ def pre_sql_setup(self):\n             idents = []\n             related_ids = collections.defaultdict(list)\n             for rows in query.get_compiler(self.using).execute_sql(MULTI):\n-                idents.extend(r[0] for r in rows)\n+                pks = [row if is_composite_pk else row[0] for row in rows]\n+                idents.extend(pks)\n                 for parent, index in related_ids_index:\n                     related_ids[parent].extend(r[index] for r in rows)\n             self.query.add_filter(\"pk__in\", idents)\ndiff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex b7b93c235ae1..cca11bfcc213 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -627,8 +627,12 @@ def get_aggregation(self, using, aggregate_exprs):\n         if result is None:\n             result = empty_set_result\n         else:\n-            converters = compiler.get_converters(outer_query.annotation_select.values())\n-            result = next(compiler.apply_converters((result,), converters))\n+            cols = outer_query.annotation_select.values()\n+            converters = compiler.get_converters(cols)\n+            rows = compiler.apply_converters((result,), converters)\n+            if compiler.has_composite_fields(cols):\n+                rows = compiler.composite_fields_to_tuples(rows, cols)\n+            result = next(rows)\n \n         return dict(zip(outer_query.annotation_select, result))\n \ndiff --git a/docs/ref/checks.txt b/docs/ref/checks.txt\nindex 2308a854c73d..b0a98bde2810 100644\n--- a/docs/ref/checks.txt\n+++ b/docs/ref/checks.txt\n@@ -181,6 +181,7 @@ Model fields\n * **fields.E011**: ``<database>`` does not support default database values with\n   expressions (``db_default``).\n * **fields.E012**: ``<expression>`` cannot be used in ``db_default``.\n+* **fields.E013**: ``CompositePrimaryKey`` must be named ``pk``.\n * **fields.E100**: ``AutoField``\\s must set primary_key=True.\n * **fields.E110**: ``BooleanField``\\s do not accept null values. *This check\n   appeared before support for null values was added in Django 2.1.*\n@@ -417,6 +418,8 @@ Models\n * **models.W040**: ``<database>`` does not support indexes with non-key\n   columns.\n * **models.E041**: ``constraints`` refers to the joined field ``<field name>``.\n+* **models.E042**: ``<field name>`` cannot be included in the composite\n+  primary key.\n * **models.W042**: Auto-created primary key used when not defining a primary\n   key type, by default ``django.db.models.AutoField``.\n * **models.W043**: ``<database>`` does not support indexes on expressions.\ndiff --git a/docs/ref/models/fields.txt b/docs/ref/models/fields.txt\nindex 07e86785d91f..5b0f127c6f28 100644\n--- a/docs/ref/models/fields.txt\n+++ b/docs/ref/models/fields.txt\n@@ -707,6 +707,23 @@ or :class:`~django.forms.NullBooleanSelect` if :attr:`null=True <Field.null>`.\n The default value of ``BooleanField`` is ``None`` when :attr:`Field.default`\n isn't defined.\n \n+``CompositePrimaryKey``\n+-----------------------\n+\n+.. versionadded:: 5.2\n+\n+.. class:: CompositePrimaryKey(*field_names, **options)\n+\n+A virtual field used for defining a composite primary key.\n+\n+This field must be defined as the model's ``pk`` field. If present, Django will\n+create the underlying model table with a composite primary key.\n+\n+The ``*field_names`` argument is a list of positional field names that compose\n+the primary key.\n+\n+See :doc:`/topics/composite-primary-key` for more details.\n+\n ``CharField``\n -------------\n \n@@ -1615,6 +1632,8 @@ not an instance of ``UUID``.\n     hyphens, because PostgreSQL and MariaDB 10.7+ store them in a hyphenated\n     uuid datatype type.\n \n+.. _relationship-fields:\n+\n Relationship fields\n ===================\n \ndiff --git a/docs/releases/5.2.txt b/docs/releases/5.2.txt\nindex 88a1daa45dc7..6d6e83a7f6d8 100644\n--- a/docs/releases/5.2.txt\n+++ b/docs/releases/5.2.txt\n@@ -31,6 +31,25 @@ and only officially support the latest release of each series.\n What's new in Django 5.2\n ========================\n \n+Composite Primary Keys\n+----------------------\n+\n+The new :class:`django.db.models.CompositePrimaryKey` allows tables to be\n+created with a primary key consisting of multiple fields.\n+\n+To use a composite primary key, when creating a model set the ``pk`` field to\n+be a ``CompositePrimaryKey``::\n+\n+    from django.db import models\n+\n+\n+    class Release(models.Model):\n+        pk = models.CompositePrimaryKey(\"version\", \"name\")\n+        version = models.IntegerField()\n+        name = models.CharField(max_length=20)\n+\n+See :doc:`/topics/composite-primary-key` for more details.\n+\n Minor features\n --------------\n \ndiff --git a/docs/topics/composite-primary-key.txt b/docs/topics/composite-primary-key.txt\nnew file mode 100644\nindex 000000000000..9e5234ca9faa\n--- /dev/null\n+++ b/docs/topics/composite-primary-key.txt\n@@ -0,0 +1,183 @@\n+======================\n+Composite primary keys\n+======================\n+\n+.. versionadded:: 5.2\n+\n+In Django, each model has a primary key. By default, this primary key consists\n+of a single field.\n+\n+In most cases, a single primary key should suffice. In database design,\n+however, defining a primary key consisting of multiple fields is sometimes\n+necessary.\n+\n+To use a composite primary key, when creating a model set the ``pk`` field to\n+be a :class:`.CompositePrimaryKey`::\n+\n+    class Product(models.Model):\n+        name = models.CharField(max_length=100)\n+\n+\n+    class Order(models.Model):\n+        reference = models.CharField(max_length=20, primary_key=True)\n+\n+\n+    class OrderLineItem(models.Model):\n+        pk = models.CompositePrimaryKey(\"product_id\", \"order_id\")\n+        product = models.ForeignKey(Product, on_delete=models.CASCADE)\n+        order = models.ForeignKey(Order, on_delete=models.CASCADE)\n+        quantity = models.IntegerField()\n+\n+This will instruct Django to create a composite primary key\n+(``PRIMARY KEY (product_id, order_id)``) when creating the table.\n+\n+A composite primary key is represented by a ``tuple``:\n+\n+.. code-block:: pycon\n+\n+    >>> product = Product.objects.create(name=\"apple\")\n+    >>> order = Order.objects.create(reference=\"A755H\")\n+    >>> item = OrderLineItem.objects.create(product=product, order=order, quantity=1)\n+    >>> item.pk\n+    (1, \"A755H\")\n+\n+You can assign a ``tuple`` to a composite primary key. This sets the associated\n+field values.\n+\n+.. code-block:: pycon\n+\n+    >>> item = OrderLineItem(pk=(2, \"B142C\"))\n+    >>> item.pk\n+    (2, \"B142C\")\n+    >>> item.product_id\n+    2\n+    >>> item.order_id\n+    \"B142C\"\n+\n+A composite primary key can also be filtered by a ``tuple``:\n+\n+.. code-block:: pycon\n+\n+    >>> OrderLineItem.objects.filter(pk=(1, \"A755H\")).count()\n+    1\n+\n+We're still working on composite primary key support for\n+:ref:`relational fields <cpk-and-relations>`, including\n+:class:`.GenericForeignKey` fields, and the Django admin. Models with composite\n+primary keys cannot be registered in the Django admin at this time. You can\n+expect to see this in future releases.\n+\n+Migrating to a composite primary key\n+====================================\n+\n+Django doesn't support migrating to, or from, a composite primary key after the\n+table is created. It also doesn't support adding or removing fields from the\n+composite primary key.\n+\n+If you would like to migrate an existing table from a single primary key to a\n+composite primary key, follow your database backend's instructions to do so.\n+\n+Once the composite primary key is in place, add the ``CompositePrimaryKey``\n+field to your model. This allows Django to recognize and handle the composite\n+primary key appropriately.\n+\n+While migration operations (e.g. ``AddField``, ``AlterField``) on primary key\n+fields are not supported, ``makemigrations`` will still detect changes.\n+\n+In order to avoid errors, it's recommended to apply such migrations with\n+``--fake``.\n+\n+Alternatively, :class:`.SeparateDatabaseAndState` may be used to execute the\n+backend-specific migrations and Django-generated migrations in a single\n+operation.\n+\n+.. _cpk-and-relations:\n+\n+Composite primary keys and relations\n+====================================\n+\n+:ref:`Relationship fields <relationship-fields>`, including\n+:ref:`generic relations <generic-relations>` do not support composite primary\n+keys.\n+\n+For example, given the ``OrderLineItem`` model, the following is not\n+supported::\n+\n+    class Foo(models.Model):\n+        item = models.ForeignKey(OrderLineItem, on_delete=models.CASCADE)\n+\n+Because ``ForeignKey`` currently cannot reference models with composite primary\n+keys.\n+\n+To work around this limitation, ``ForeignObject`` can be used as an\n+alternative::\n+\n+    class Foo(models.Model):\n+        item_order_id = models.IntegerField()\n+        item_product_id = models.CharField(max_length=20)\n+        item = models.ForeignObject(\n+            OrderLineItem,\n+            on_delete=models.CASCADE,\n+            from_fields=(\"item_order_id\", \"item_product_id\"),\n+            to_fields=(\"order_id\", \"product_id\"),\n+        )\n+\n+``ForeignObject`` is much like ``ForeignKey``, except that it doesn't create\n+any columns (e.g. ``item_id``), foreign key constraints or indexes in the\n+database.\n+\n+.. warning::\n+\n+    ``ForeignObject`` is an internal API. This means it is not covered by our\n+    :ref:`deprecation policy <internal-release-deprecation-policy>`.\n+\n+Composite primary keys and database functions\n+=============================================\n+\n+Many database functions only accept a single expression.\n+\n+.. code-block:: sql\n+\n+    MAX(\"order_id\")  -- OK\n+    MAX(\"product_id\", \"order_id\")  -- ERROR\n+\n+As a consequence, they cannot be used with composite primary key references as\n+they are composed of multiple column expressions.\n+\n+.. code-block:: python\n+\n+    Max(\"order_id\")  # OK\n+    Max(\"pk\")  # ERROR\n+\n+Composite primary keys in forms\n+===============================\n+\n+As a composite primary key is a virtual field, a field which doesn't represent\n+a single database column, this field is excluded from ModelForms.\n+\n+For example, take the following form::\n+\n+    class OrderLineItemForm(forms.ModelForm):\n+        class Meta:\n+            model = OrderLineItem\n+            fields = \"__all__\"\n+\n+This form does not have a form field ``pk`` for the composite primary key:\n+\n+.. code-block:: pycon\n+\n+    >>> OrderLineItemForm()\n+    <OrderLineItemForm bound=False, valid=Unknown, fields=(product;order;quantity)>\n+\n+Setting the primary composite field ``pk`` as a form field raises an unknown\n+field :exc:`.FieldError`.\n+\n+.. admonition:: Primary key fields are read only\n+\n+    If you change the value of a primary key on an existing object and then\n+    save it, a new object will be created alongside the old one (see\n+    :attr:`.Field.primary_key`).\n+\n+    This is also true of composite primary keys. Hence, you may want to set\n+    :attr:`.Field.editable` to ``False`` on all primary key fields to exclude\n+    them from ModelForms.\ndiff --git a/docs/topics/index.txt b/docs/topics/index.txt\nindex ffb9fa9d9273..4f837c81e2c7 100644\n--- a/docs/topics/index.txt\n+++ b/docs/topics/index.txt\n@@ -19,6 +19,7 @@ Introductions to all the key parts of Django you'll need to know:\n    auth/index\n    cache\n    conditional-view-processing\n+   composite-primary-key\n    signing\n    email\n    i18n/index\ndiff --git a/tests/admin_registration/models.py b/tests/admin_registration/models.py\nindex 0ae925113346..2231c236dee4 100644\n--- a/tests/admin_registration/models.py\n+++ b/tests/admin_registration/models.py\n@@ -20,3 +20,9 @@ class Meta:\n \n class Place(Location):\n     name = models.CharField(max_length=200)\n+\n+\n+class Guest(models.Model):\n+    pk = models.CompositePrimaryKey(\"traveler\", \"place\")\n+    traveler = models.ForeignKey(Traveler, on_delete=models.CASCADE)\n+    place = models.ForeignKey(Place, on_delete=models.CASCADE)\ndiff --git a/tests/admin_registration/tests.py b/tests/admin_registration/tests.py\nindex 3b0e656f5f35..0a881caf6506 100644\n--- a/tests/admin_registration/tests.py\n+++ b/tests/admin_registration/tests.py\n@@ -5,7 +5,7 @@\n from django.core.exceptions import ImproperlyConfigured\n from django.test import SimpleTestCase\n \n-from .models import Location, Person, Place, Traveler\n+from .models import Guest, Location, Person, Place, Traveler\n \n \n class NameAdmin(admin.ModelAdmin):\n@@ -92,6 +92,14 @@ def test_abstract_model(self):\n         with self.assertRaisesMessage(ImproperlyConfigured, msg):\n             self.site.register(Location)\n \n+    def test_composite_pk_model(self):\n+        msg = (\n+            \"The model Guest has a composite primary key, so it cannot be registered \"\n+            \"with admin.\"\n+        )\n+        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n+            self.site.register(Guest)\n+\n     def test_is_registered_model(self):\n         \"Checks for registered models should return true.\"\n         self.site.register(Person)\ndiff --git a/tests/composite_pk/__init__.py b/tests/composite_pk/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/composite_pk/fixtures/tenant.json b/tests/composite_pk/fixtures/tenant.json\nnew file mode 100644\nindex 000000000000..3eeff42fefde\n--- /dev/null\n+++ b/tests/composite_pk/fixtures/tenant.json\n@@ -0,0 +1,75 @@\n+[\n+    {\n+        \"pk\": 1,\n+        \"model\": \"composite_pk.tenant\",\n+        \"fields\": {\n+            \"id\": 1,\n+            \"name\": \"Tenant 1\"\n+        }\n+    },\n+    {\n+        \"pk\": 2,\n+        \"model\": \"composite_pk.tenant\",\n+        \"fields\": {\n+            \"id\": 2,\n+            \"name\": \"Tenant 2\"\n+        }\n+    },\n+    {\n+        \"pk\": 3,\n+        \"model\": \"composite_pk.tenant\",\n+        \"fields\": {\n+            \"id\": 3,\n+            \"name\": \"Tenant 3\"\n+        }\n+    },\n+    {\n+        \"pk\": [1, 1],\n+        \"model\": \"composite_pk.user\",\n+        \"fields\": {\n+            \"tenant_id\": 1,\n+            \"id\": 1,\n+            \"email\": \"user0001@example.com\"\n+        }\n+    },\n+    {\n+        \"pk\": [1, 2],\n+        \"model\": \"composite_pk.user\",\n+        \"fields\": {\n+            \"tenant_id\": 1,\n+            \"id\": 2,\n+            \"email\": \"user0002@example.com\"\n+        }\n+    },\n+    {\n+        \"pk\": [2, 3],\n+        \"model\": \"composite_pk.user\",\n+        \"fields\": {\n+            \"email\": \"user0003@example.com\"\n+        }\n+    },\n+    {\n+        \"model\": \"composite_pk.user\",\n+        \"fields\": {\n+            \"tenant_id\": 2,\n+            \"id\": 4,\n+            \"email\": \"user0004@example.com\"\n+        }\n+    },\n+    {\n+        \"pk\": [2, \"11111111-1111-1111-1111-111111111111\"],\n+        \"model\": \"composite_pk.post\",\n+        \"fields\": {\n+            \"tenant_id\": 2,\n+            \"id\": \"11111111-1111-1111-1111-111111111111\"\n+        }\n+    },\n+    {\n+        \"pk\": [2, \"ffffffff-ffff-ffff-ffff-ffffffffffff\"],\n+        \"model\": \"composite_pk.post\",\n+        \"fields\": {\n+            \"tenant_id\": 2,\n+            \"id\": \"ffffffff-ffff-ffff-ffff-ffffffffffff\"\n+        }\n+    }\n+]\ndiff --git a/tests/composite_pk/models/__init__.py b/tests/composite_pk/models/__init__.py\nnew file mode 100644\nindex 000000000000..35c394371696\n--- /dev/null\n+++ b/tests/composite_pk/models/__init__.py\n@@ -0,0 +1,9 @@\n+from .tenant import Comment, Post, Tenant, Token, User\n+\n+__all__ = [\n+    \"Comment\",\n+    \"Post\",\n+    \"Tenant\",\n+    \"Token\",\n+    \"User\",\n+]\ndiff --git a/tests/composite_pk/models/tenant.py b/tests/composite_pk/models/tenant.py\nnew file mode 100644\nindex 000000000000..ac0b3d9715a1\n--- /dev/null\n+++ b/tests/composite_pk/models/tenant.py\n@@ -0,0 +1,50 @@\n+from django.db import models\n+\n+\n+class Tenant(models.Model):\n+    name = models.CharField(max_length=10, default=\"\", blank=True)\n+\n+\n+class Token(models.Model):\n+    pk = models.CompositePrimaryKey(\"tenant_id\", \"id\")\n+    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name=\"tokens\")\n+    id = models.SmallIntegerField()\n+    secret = models.CharField(max_length=10, default=\"\", blank=True)\n+\n+\n+class BaseModel(models.Model):\n+    pk = models.CompositePrimaryKey(\"tenant_id\", \"id\")\n+    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE)\n+    id = models.SmallIntegerField(unique=True)\n+\n+    class Meta:\n+        abstract = True\n+\n+\n+class User(BaseModel):\n+    email = models.EmailField(unique=True)\n+\n+\n+class Comment(models.Model):\n+    pk = models.CompositePrimaryKey(\"tenant\", \"id\")\n+    tenant = models.ForeignKey(\n+        Tenant,\n+        on_delete=models.CASCADE,\n+        related_name=\"comments\",\n+    )\n+    id = models.SmallIntegerField(unique=True, db_column=\"comment_id\")\n+    user_id = models.SmallIntegerField()\n+    user = models.ForeignObject(\n+        User,\n+        on_delete=models.CASCADE,\n+        from_fields=(\"tenant_id\", \"user_id\"),\n+        to_fields=(\"tenant_id\", \"id\"),\n+        related_name=\"comments\",\n+    )\n+    text = models.TextField(default=\"\", blank=True)\n+\n+\n+class Post(models.Model):\n+    pk = models.CompositePrimaryKey(\"tenant_id\", \"id\")\n+    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE)\n+    id = models.UUIDField()\ndiff --git a/tests/composite_pk/test_aggregate.py b/tests/composite_pk/test_aggregate.py\nnew file mode 100644\nindex 000000000000..b5474c5218bc\n--- /dev/null\n+++ b/tests/composite_pk/test_aggregate.py\n@@ -0,0 +1,139 @@\n+from django.db import NotSupportedError\n+from django.db.models import Count, Q\n+from django.test import TestCase\n+\n+from .models import Comment, Tenant, User\n+\n+\n+class CompositePKAggregateTests(TestCase):\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.tenant_1 = Tenant.objects.create()\n+        cls.tenant_2 = Tenant.objects.create()\n+        cls.user_1 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=1,\n+            email=\"user0001@example.com\",\n+        )\n+        cls.user_2 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=2,\n+            email=\"user0002@example.com\",\n+        )\n+        cls.user_3 = User.objects.create(\n+            tenant=cls.tenant_2,\n+            id=3,\n+            email=\"user0003@example.com\",\n+        )\n+        cls.comment_1 = Comment.objects.create(id=1, user=cls.user_2, text=\"foo\")\n+        cls.comment_2 = Comment.objects.create(id=2, user=cls.user_1, text=\"bar\")\n+        cls.comment_3 = Comment.objects.create(id=3, user=cls.user_1, text=\"foobar\")\n+        cls.comment_4 = Comment.objects.create(id=4, user=cls.user_3, text=\"foobarbaz\")\n+        cls.comment_5 = Comment.objects.create(id=5, user=cls.user_3, text=\"barbaz\")\n+        cls.comment_6 = Comment.objects.create(id=6, user=cls.user_3, text=\"baz\")\n+\n+    def test_users_annotated_with_comments_id_count(self):\n+        user_1, user_2, user_3 = User.objects.annotate(Count(\"comments__id\")).order_by(\n+            \"pk\"\n+        )\n+\n+        self.assertEqual(user_1, self.user_1)\n+        self.assertEqual(user_1.comments__id__count, 2)\n+        self.assertEqual(user_2, self.user_2)\n+        self.assertEqual(user_2.comments__id__count, 1)\n+        self.assertEqual(user_3, self.user_3)\n+        self.assertEqual(user_3.comments__id__count, 3)\n+\n+    def test_users_annotated_with_aliased_comments_id_count(self):\n+        user_1, user_2, user_3 = User.objects.annotate(\n+            comments_count=Count(\"comments__id\")\n+        ).order_by(\"pk\")\n+\n+        self.assertEqual(user_1, self.user_1)\n+        self.assertEqual(user_1.comments_count, 2)\n+        self.assertEqual(user_2, self.user_2)\n+        self.assertEqual(user_2.comments_count, 1)\n+        self.assertEqual(user_3, self.user_3)\n+        self.assertEqual(user_3.comments_count, 3)\n+\n+    def test_users_annotated_with_comments_count(self):\n+        user_1, user_2, user_3 = User.objects.annotate(Count(\"comments\")).order_by(\"pk\")\n+\n+        self.assertEqual(user_1, self.user_1)\n+        self.assertEqual(user_1.comments__count, 2)\n+        self.assertEqual(user_2, self.user_2)\n+        self.assertEqual(user_2.comments__count, 1)\n+        self.assertEqual(user_3, self.user_3)\n+        self.assertEqual(user_3.comments__count, 3)\n+\n+    def test_users_annotated_with_comments_count_filter(self):\n+        user_1, user_2, user_3 = User.objects.annotate(\n+            comments__count=Count(\n+                \"comments\", filter=Q(pk__in=[self.user_1.pk, self.user_2.pk])\n+            )\n+        ).order_by(\"pk\")\n+\n+        self.assertEqual(user_1, self.user_1)\n+        self.assertEqual(user_1.comments__count, 2)\n+        self.assertEqual(user_2, self.user_2)\n+        self.assertEqual(user_2.comments__count, 1)\n+        self.assertEqual(user_3, self.user_3)\n+        self.assertEqual(user_3.comments__count, 0)\n+\n+    def test_count_distinct_not_supported(self):\n+        with self.assertRaisesMessage(\n+            NotSupportedError, \"COUNT(DISTINCT) doesn't support composite primary keys\"\n+        ):\n+            self.assertIsNone(\n+                User.objects.annotate(comments__count=Count(\"comments\", distinct=True))\n+            )\n+\n+    def test_user_values_annotated_with_comments_id_count(self):\n+        self.assertSequenceEqual(\n+            User.objects.values(\"pk\").annotate(Count(\"comments__id\")).order_by(\"pk\"),\n+            (\n+                {\"pk\": self.user_1.pk, \"comments__id__count\": 2},\n+                {\"pk\": self.user_2.pk, \"comments__id__count\": 1},\n+                {\"pk\": self.user_3.pk, \"comments__id__count\": 3},\n+            ),\n+        )\n+\n+    def test_user_values_annotated_with_filtered_comments_id_count(self):\n+        self.assertSequenceEqual(\n+            User.objects.values(\"pk\")\n+            .annotate(\n+                comments_count=Count(\n+                    \"comments__id\",\n+                    filter=Q(comments__text__icontains=\"foo\"),\n+                )\n+            )\n+            .order_by(\"pk\"),\n+            (\n+                {\"pk\": self.user_1.pk, \"comments_count\": 1},\n+                {\"pk\": self.user_2.pk, \"comments_count\": 1},\n+                {\"pk\": self.user_3.pk, \"comments_count\": 1},\n+            ),\n+        )\n+\n+    def test_filter_and_count_users_by_comments_fields(self):\n+        users = User.objects.filter(comments__id__gt=2).order_by(\"pk\")\n+        self.assertEqual(users.count(), 4)\n+        self.assertSequenceEqual(\n+            users, (self.user_1, self.user_3, self.user_3, self.user_3)\n+        )\n+\n+        users = User.objects.filter(comments__text__icontains=\"foo\").order_by(\"pk\")\n+        self.assertEqual(users.count(), 3)\n+        self.assertSequenceEqual(users, (self.user_1, self.user_2, self.user_3))\n+\n+        users = User.objects.filter(comments__text__icontains=\"baz\").order_by(\"pk\")\n+        self.assertEqual(users.count(), 3)\n+        self.assertSequenceEqual(users, (self.user_3, self.user_3, self.user_3))\n+\n+    def test_order_by_comments_id_count(self):\n+        self.assertSequenceEqual(\n+            User.objects.annotate(comments_count=Count(\"comments__id\")).order_by(\n+                \"-comments_count\"\n+            ),\n+            (self.user_3, self.user_1, self.user_2),\n+        )\ndiff --git a/tests/composite_pk/test_checks.py b/tests/composite_pk/test_checks.py\nnew file mode 100644\nindex 000000000000..02a162c31df5\n--- /dev/null\n+++ b/tests/composite_pk/test_checks.py\n@@ -0,0 +1,242 @@\n+from django.core import checks\n+from django.db import connection, models\n+from django.db.models import F\n+from django.test import TestCase\n+from django.test.utils import isolate_apps\n+\n+\n+@isolate_apps(\"composite_pk\")\n+class CompositePKChecksTests(TestCase):\n+    maxDiff = None\n+\n+    def test_composite_pk_must_be_unique_strings(self):\n+        test_cases = (\n+            (),\n+            (0,),\n+            (1,),\n+            (\"id\", False),\n+            (\"id\", \"id\"),\n+            ((\"id\",),),\n+        )\n+\n+        for i, args in enumerate(test_cases):\n+            with (\n+                self.subTest(args=args),\n+                self.assertRaisesMessage(\n+                    ValueError, \"CompositePrimaryKey args must be unique strings.\"\n+                ),\n+            ):\n+                models.CompositePrimaryKey(*args)\n+\n+    def test_composite_pk_must_include_at_least_2_fields(self):\n+        expected_message = \"CompositePrimaryKey must include at least two fields.\"\n+        with self.assertRaisesMessage(ValueError, expected_message):\n+            models.CompositePrimaryKey(\"id\")\n+\n+    def test_composite_pk_cannot_have_a_default(self):\n+        expected_message = \"CompositePrimaryKey cannot have a default.\"\n+        with self.assertRaisesMessage(ValueError, expected_message):\n+            models.CompositePrimaryKey(\"tenant_id\", \"id\", default=(1, 1))\n+\n+    def test_composite_pk_cannot_have_a_database_default(self):\n+        expected_message = \"CompositePrimaryKey cannot have a database default.\"\n+        with self.assertRaisesMessage(ValueError, expected_message):\n+            models.CompositePrimaryKey(\"tenant_id\", \"id\", db_default=models.F(\"id\"))\n+\n+    def test_composite_pk_cannot_be_editable(self):\n+        expected_message = \"CompositePrimaryKey cannot be editable.\"\n+        with self.assertRaisesMessage(ValueError, expected_message):\n+            models.CompositePrimaryKey(\"tenant_id\", \"id\", editable=True)\n+\n+    def test_composite_pk_must_be_a_primary_key(self):\n+        expected_message = \"CompositePrimaryKey must be a primary key.\"\n+        with self.assertRaisesMessage(ValueError, expected_message):\n+            models.CompositePrimaryKey(\"tenant_id\", \"id\", primary_key=False)\n+\n+    def test_composite_pk_must_be_blank(self):\n+        expected_message = \"CompositePrimaryKey must be blank.\"\n+        with self.assertRaisesMessage(ValueError, expected_message):\n+            models.CompositePrimaryKey(\"tenant_id\", \"id\", blank=False)\n+\n+    def test_composite_pk_must_not_have_other_pk_field(self):\n+        class Foo(models.Model):\n+            pk = models.CompositePrimaryKey(\"foo_id\", \"id\")\n+            foo_id = models.IntegerField()\n+            id = models.IntegerField(primary_key=True)\n+\n+        self.assertEqual(\n+            Foo.check(databases=self.databases),\n+            [\n+                checks.Error(\n+                    \"The model cannot have more than one field with \"\n+                    \"'primary_key=True'.\",\n+                    obj=Foo,\n+                    id=\"models.E026\",\n+                ),\n+            ],\n+        )\n+\n+    def test_composite_pk_cannot_include_nullable_field(self):\n+        class Foo(models.Model):\n+            pk = models.CompositePrimaryKey(\"foo_id\", \"id\")\n+            foo_id = models.IntegerField()\n+            id = models.IntegerField(null=True)\n+\n+        self.assertEqual(\n+            Foo.check(databases=self.databases),\n+            [\n+                checks.Error(\n+                    \"'id' cannot be included in the composite primary key.\",\n+                    hint=\"'id' field may not set 'null=True'.\",\n+                    obj=Foo,\n+                    id=\"models.E042\",\n+                ),\n+            ],\n+        )\n+\n+    def test_composite_pk_can_include_fk_name(self):\n+        class Foo(models.Model):\n+            pass\n+\n+        class Bar(models.Model):\n+            pk = models.CompositePrimaryKey(\"foo\", \"id\")\n+            foo = models.ForeignKey(Foo, on_delete=models.CASCADE)\n+            id = models.SmallIntegerField()\n+\n+        self.assertEqual(Foo.check(databases=self.databases), [])\n+        self.assertEqual(Bar.check(databases=self.databases), [])\n+\n+    def test_composite_pk_cannot_include_same_field(self):\n+        class Foo(models.Model):\n+            pass\n+\n+        class Bar(models.Model):\n+            pk = models.CompositePrimaryKey(\"foo\", \"foo_id\")\n+            foo = models.ForeignKey(Foo, on_delete=models.CASCADE)\n+            id = models.SmallIntegerField()\n+\n+        self.assertEqual(Foo.check(databases=self.databases), [])\n+        self.assertEqual(\n+            Bar.check(databases=self.databases),\n+            [\n+                checks.Error(\n+                    \"'foo_id' cannot be included in the composite primary key.\",\n+                    hint=\"'foo_id' and 'foo' are the same fields.\",\n+                    obj=Bar,\n+                    id=\"models.E042\",\n+                ),\n+            ],\n+        )\n+\n+    def test_composite_pk_cannot_include_composite_pk_field(self):\n+        class Foo(models.Model):\n+            pk = models.CompositePrimaryKey(\"id\", \"pk\")\n+            id = models.SmallIntegerField()\n+\n+        self.assertEqual(\n+            Foo.check(databases=self.databases),\n+            [\n+                checks.Error(\n+                    \"'pk' cannot be included in the composite primary key.\",\n+                    hint=\"'pk' field has no column.\",\n+                    obj=Foo,\n+                    id=\"models.E042\",\n+                ),\n+            ],\n+        )\n+\n+    def test_composite_pk_cannot_include_db_column(self):\n+        class Foo(models.Model):\n+            pk = models.CompositePrimaryKey(\"foo\", \"bar\")\n+            foo = models.SmallIntegerField(db_column=\"foo_id\")\n+            bar = models.SmallIntegerField(db_column=\"bar_id\")\n+\n+        class Bar(models.Model):\n+            pk = models.CompositePrimaryKey(\"foo_id\", \"bar_id\")\n+            foo = models.SmallIntegerField(db_column=\"foo_id\")\n+            bar = models.SmallIntegerField(db_column=\"bar_id\")\n+\n+        self.assertEqual(Foo.check(databases=self.databases), [])\n+        self.assertEqual(\n+            Bar.check(databases=self.databases),\n+            [\n+                checks.Error(\n+                    \"'foo_id' cannot be included in the composite primary key.\",\n+                    hint=\"'foo_id' is not a valid field.\",\n+                    obj=Bar,\n+                    id=\"models.E042\",\n+                ),\n+                checks.Error(\n+                    \"'bar_id' cannot be included in the composite primary key.\",\n+                    hint=\"'bar_id' is not a valid field.\",\n+                    obj=Bar,\n+                    id=\"models.E042\",\n+                ),\n+            ],\n+        )\n+\n+    def test_foreign_object_can_refer_composite_pk(self):\n+        class Foo(models.Model):\n+            pass\n+\n+        class Bar(models.Model):\n+            pk = models.CompositePrimaryKey(\"foo_id\", \"id\")\n+            foo = models.ForeignKey(Foo, on_delete=models.CASCADE)\n+            id = models.IntegerField()\n+\n+        class Baz(models.Model):\n+            pk = models.CompositePrimaryKey(\"foo_id\", \"id\")\n+            foo = models.ForeignKey(Foo, on_delete=models.CASCADE)\n+            id = models.IntegerField()\n+            bar_id = models.IntegerField()\n+            bar = models.ForeignObject(\n+                Bar,\n+                on_delete=models.CASCADE,\n+                from_fields=(\"foo_id\", \"bar_id\"),\n+                to_fields=(\"foo_id\", \"id\"),\n+            )\n+\n+        self.assertEqual(Foo.check(databases=self.databases), [])\n+        self.assertEqual(Bar.check(databases=self.databases), [])\n+        self.assertEqual(Baz.check(databases=self.databases), [])\n+\n+    def test_composite_pk_must_be_named_pk(self):\n+        class Foo(models.Model):\n+            primary_key = models.CompositePrimaryKey(\"foo_id\", \"id\")\n+            foo_id = models.IntegerField()\n+            id = models.IntegerField()\n+\n+        self.assertEqual(\n+            Foo.check(databases=self.databases),\n+            [\n+                checks.Error(\n+                    \"'CompositePrimaryKey' must be named 'pk'.\",\n+                    obj=Foo._meta.get_field(\"primary_key\"),\n+                    id=\"fields.E013\",\n+                ),\n+            ],\n+        )\n+\n+    def test_composite_pk_cannot_include_generated_field(self):\n+        is_oracle = connection.vendor == \"oracle\"\n+\n+        class Foo(models.Model):\n+            pk = models.CompositePrimaryKey(\"id\", \"foo\")\n+            id = models.IntegerField()\n+            foo = models.GeneratedField(\n+                expression=F(\"id\"),\n+                output_field=models.IntegerField(),\n+                db_persist=not is_oracle,\n+            )\n+\n+        self.assertEqual(\n+            Foo.check(databases=self.databases),\n+            [\n+                checks.Error(\n+                    \"'foo' cannot be included in the composite primary key.\",\n+                    hint=\"'foo' field is a generated field.\",\n+                    obj=Foo,\n+                    id=\"models.E042\",\n+                ),\n+            ],\n+        )\ndiff --git a/tests/composite_pk/test_create.py b/tests/composite_pk/test_create.py\nnew file mode 100644\nindex 000000000000..7c9925b94656\n--- /dev/null\n+++ b/tests/composite_pk/test_create.py\n@@ -0,0 +1,138 @@\n+from django.test import TestCase\n+\n+from .models import Tenant, User\n+\n+\n+class CompositePKCreateTests(TestCase):\n+    maxDiff = None\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.tenant = Tenant.objects.create()\n+        cls.user = User.objects.create(\n+            tenant=cls.tenant,\n+            id=1,\n+            email=\"user0001@example.com\",\n+        )\n+\n+    def test_create_user(self):\n+        test_cases = (\n+            {\"tenant\": self.tenant, \"id\": 2412, \"email\": \"user2412@example.com\"},\n+            {\"tenant_id\": self.tenant.id, \"id\": 5316, \"email\": \"user5316@example.com\"},\n+            {\"pk\": (self.tenant.id, 7424), \"email\": \"user7424@example.com\"},\n+        )\n+\n+        for fields in test_cases:\n+            with self.subTest(fields=fields):\n+                count = User.objects.count()\n+                user = User(**fields)\n+                obj = User.objects.create(**fields)\n+                self.assertEqual(obj.tenant_id, self.tenant.id)\n+                self.assertEqual(obj.id, user.id)\n+                self.assertEqual(obj.pk, (self.tenant.id, user.id))\n+                self.assertEqual(obj.email, user.email)\n+                self.assertEqual(count + 1, User.objects.count())\n+\n+    def test_save_user(self):\n+        test_cases = (\n+            {\"tenant\": self.tenant, \"id\": 9241, \"email\": \"user9241@example.com\"},\n+            {\"tenant_id\": self.tenant.id, \"id\": 5132, \"email\": \"user5132@example.com\"},\n+            {\"pk\": (self.tenant.id, 3014), \"email\": \"user3014@example.com\"},\n+        )\n+\n+        for fields in test_cases:\n+            with self.subTest(fields=fields):\n+                count = User.objects.count()\n+                user = User(**fields)\n+                self.assertIsNotNone(user.id)\n+                self.assertIsNotNone(user.email)\n+                user.save()\n+                self.assertEqual(user.tenant_id, self.tenant.id)\n+                self.assertEqual(user.tenant, self.tenant)\n+                self.assertIsNotNone(user.id)\n+                self.assertEqual(user.pk, (self.tenant.id, user.id))\n+                self.assertEqual(user.email, fields[\"email\"])\n+                self.assertEqual(user.email, f\"user{user.id}@example.com\")\n+                self.assertEqual(count + 1, User.objects.count())\n+\n+    def test_bulk_create_users(self):\n+        objs = [\n+            User(tenant=self.tenant, id=8291, email=\"user8291@example.com\"),\n+            User(tenant_id=self.tenant.id, id=4021, email=\"user4021@example.com\"),\n+            User(pk=(self.tenant.id, 8214), email=\"user8214@example.com\"),\n+        ]\n+\n+        obj_1, obj_2, obj_3 = User.objects.bulk_create(objs)\n+\n+        self.assertEqual(obj_1.tenant_id, self.tenant.id)\n+        self.assertEqual(obj_1.id, 8291)\n+        self.assertEqual(obj_1.pk, (obj_1.tenant_id, obj_1.id))\n+        self.assertEqual(obj_1.email, \"user8291@example.com\")\n+        self.assertEqual(obj_2.tenant_id, self.tenant.id)\n+        self.assertEqual(obj_2.id, 4021)\n+        self.assertEqual(obj_2.pk, (obj_2.tenant_id, obj_2.id))\n+        self.assertEqual(obj_2.email, \"user4021@example.com\")\n+        self.assertEqual(obj_3.tenant_id, self.tenant.id)\n+        self.assertEqual(obj_3.id, 8214)\n+        self.assertEqual(obj_3.pk, (obj_3.tenant_id, obj_3.id))\n+        self.assertEqual(obj_3.email, \"user8214@example.com\")\n+\n+    def test_get_or_create_user(self):\n+        test_cases = (\n+            {\n+                \"pk\": (self.tenant.id, 8314),\n+                \"defaults\": {\"email\": \"user8314@example.com\"},\n+            },\n+            {\n+                \"tenant\": self.tenant,\n+                \"id\": 3142,\n+                \"defaults\": {\"email\": \"user3142@example.com\"},\n+            },\n+            {\n+                \"tenant_id\": self.tenant.id,\n+                \"id\": 4218,\n+                \"defaults\": {\"email\": \"user4218@example.com\"},\n+            },\n+        )\n+\n+        for fields in test_cases:\n+            with self.subTest(fields=fields):\n+                count = User.objects.count()\n+                user, created = User.objects.get_or_create(**fields)\n+                self.assertIs(created, True)\n+                self.assertIsNotNone(user.id)\n+                self.assertEqual(user.pk, (self.tenant.id, user.id))\n+                self.assertEqual(user.tenant_id, self.tenant.id)\n+                self.assertEqual(user.email, fields[\"defaults\"][\"email\"])\n+                self.assertEqual(user.email, f\"user{user.id}@example.com\")\n+                self.assertEqual(count + 1, User.objects.count())\n+\n+    def test_update_or_create_user(self):\n+        test_cases = (\n+            {\n+                \"pk\": (self.tenant.id, 2931),\n+                \"defaults\": {\"email\": \"user2931@example.com\"},\n+            },\n+            {\n+                \"tenant\": self.tenant,\n+                \"id\": 6428,\n+                \"defaults\": {\"email\": \"user6428@example.com\"},\n+            },\n+            {\n+                \"tenant_id\": self.tenant.id,\n+                \"id\": 5278,\n+                \"defaults\": {\"email\": \"user5278@example.com\"},\n+            },\n+        )\n+\n+        for fields in test_cases:\n+            with self.subTest(fields=fields):\n+                count = User.objects.count()\n+                user, created = User.objects.update_or_create(**fields)\n+                self.assertIs(created, True)\n+                self.assertIsNotNone(user.id)\n+                self.assertEqual(user.pk, (self.tenant.id, user.id))\n+                self.assertEqual(user.tenant_id, self.tenant.id)\n+                self.assertEqual(user.email, fields[\"defaults\"][\"email\"])\n+                self.assertEqual(user.email, f\"user{user.id}@example.com\")\n+                self.assertEqual(count + 1, User.objects.count())\ndiff --git a/tests/composite_pk/test_delete.py b/tests/composite_pk/test_delete.py\nnew file mode 100644\nindex 000000000000..9a14deb81334\n--- /dev/null\n+++ b/tests/composite_pk/test_delete.py\n@@ -0,0 +1,83 @@\n+from django.test import TestCase\n+\n+from .models import Comment, Tenant, User\n+\n+\n+class CompositePKDeleteTests(TestCase):\n+    maxDiff = None\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.tenant_1 = Tenant.objects.create()\n+        cls.tenant_2 = Tenant.objects.create()\n+        cls.user_1 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=1,\n+            email=\"user0001@example.com\",\n+        )\n+        cls.user_2 = User.objects.create(\n+            tenant=cls.tenant_2,\n+            id=2,\n+            email=\"user0002@example.com\",\n+        )\n+        cls.comment_1 = Comment.objects.create(id=1, user=cls.user_1)\n+        cls.comment_2 = Comment.objects.create(id=2, user=cls.user_2)\n+        cls.comment_3 = Comment.objects.create(id=3, user=cls.user_2)\n+\n+    def test_delete_tenant_by_pk(self):\n+        result = Tenant.objects.filter(pk=self.tenant_1.pk).delete()\n+\n+        self.assertEqual(\n+            result,\n+            (\n+                3,\n+                {\n+                    \"composite_pk.Comment\": 1,\n+                    \"composite_pk.User\": 1,\n+                    \"composite_pk.Tenant\": 1,\n+                },\n+            ),\n+        )\n+\n+        self.assertIs(Tenant.objects.filter(pk=self.tenant_1.pk).exists(), False)\n+        self.assertIs(Tenant.objects.filter(pk=self.tenant_2.pk).exists(), True)\n+        self.assertIs(User.objects.filter(pk=self.user_1.pk).exists(), False)\n+        self.assertIs(User.objects.filter(pk=self.user_2.pk).exists(), True)\n+        self.assertIs(Comment.objects.filter(pk=self.comment_1.pk).exists(), False)\n+        self.assertIs(Comment.objects.filter(pk=self.comment_2.pk).exists(), True)\n+        self.assertIs(Comment.objects.filter(pk=self.comment_3.pk).exists(), True)\n+\n+    def test_delete_user_by_pk(self):\n+        result = User.objects.filter(pk=self.user_1.pk).delete()\n+\n+        self.assertEqual(\n+            result, (2, {\"composite_pk.User\": 1, \"composite_pk.Comment\": 1})\n+        )\n+\n+        self.assertIs(User.objects.filter(pk=self.user_1.pk).exists(), False)\n+        self.assertIs(User.objects.filter(pk=self.user_2.pk).exists(), True)\n+        self.assertIs(Comment.objects.filter(pk=self.comment_1.pk).exists(), False)\n+        self.assertIs(Comment.objects.filter(pk=self.comment_2.pk).exists(), True)\n+        self.assertIs(Comment.objects.filter(pk=self.comment_3.pk).exists(), True)\n+\n+    def test_delete_comments_by_user(self):\n+        result = Comment.objects.filter(user=self.user_2).delete()\n+\n+        self.assertEqual(result, (2, {\"composite_pk.Comment\": 2}))\n+\n+        self.assertIs(Comment.objects.filter(pk=self.comment_1.pk).exists(), True)\n+        self.assertIs(Comment.objects.filter(pk=self.comment_2.pk).exists(), False)\n+        self.assertIs(Comment.objects.filter(pk=self.comment_3.pk).exists(), False)\n+\n+    def test_delete_without_pk(self):\n+        msg = (\n+            \"Comment object can't be deleted because its pk attribute is set \"\n+            \"to None.\"\n+        )\n+\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Comment().delete()\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Comment(tenant_id=1).delete()\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Comment(id=1).delete()\ndiff --git a/tests/composite_pk/test_filter.py b/tests/composite_pk/test_filter.py\nnew file mode 100644\nindex 000000000000..7e361c5925b2\n--- /dev/null\n+++ b/tests/composite_pk/test_filter.py\n@@ -0,0 +1,412 @@\n+from django.test import TestCase\n+\n+from .models import Comment, Tenant, User\n+\n+\n+class CompositePKFilterTests(TestCase):\n+    maxDiff = None\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.tenant_1 = Tenant.objects.create()\n+        cls.tenant_2 = Tenant.objects.create()\n+        cls.tenant_3 = Tenant.objects.create()\n+        cls.user_1 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=1,\n+            email=\"user0001@example.com\",\n+        )\n+        cls.user_2 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=2,\n+            email=\"user0002@example.com\",\n+        )\n+        cls.user_3 = User.objects.create(\n+            tenant=cls.tenant_2,\n+            id=3,\n+            email=\"user0003@example.com\",\n+        )\n+        cls.user_4 = User.objects.create(\n+            tenant=cls.tenant_3,\n+            id=4,\n+            email=\"user0004@example.com\",\n+        )\n+        cls.comment_1 = Comment.objects.create(id=1, user=cls.user_1)\n+        cls.comment_2 = Comment.objects.create(id=2, user=cls.user_1)\n+        cls.comment_3 = Comment.objects.create(id=3, user=cls.user_2)\n+        cls.comment_4 = Comment.objects.create(id=4, user=cls.user_3)\n+        cls.comment_5 = Comment.objects.create(id=5, user=cls.user_1)\n+\n+    def test_filter_and_count_user_by_pk(self):\n+        test_cases = (\n+            ({\"pk\": self.user_1.pk}, 1),\n+            ({\"pk\": self.user_2.pk}, 1),\n+            ({\"pk\": self.user_3.pk}, 1),\n+            ({\"pk\": (self.tenant_1.id, self.user_1.id)}, 1),\n+            ({\"pk\": (self.tenant_1.id, self.user_2.id)}, 1),\n+            ({\"pk\": (self.tenant_2.id, self.user_3.id)}, 1),\n+            ({\"pk\": (self.tenant_1.id, self.user_3.id)}, 0),\n+            ({\"pk\": (self.tenant_2.id, self.user_1.id)}, 0),\n+            ({\"pk\": (self.tenant_2.id, self.user_2.id)}, 0),\n+        )\n+\n+        for lookup, count in test_cases:\n+            with self.subTest(lookup=lookup, count=count):\n+                self.assertEqual(User.objects.filter(**lookup).count(), count)\n+\n+    def test_order_comments_by_pk_asc(self):\n+        self.assertSequenceEqual(\n+            Comment.objects.order_by(\"pk\"),\n+            (\n+                self.comment_1,  # (1, 1)\n+                self.comment_2,  # (1, 2)\n+                self.comment_3,  # (1, 3)\n+                self.comment_5,  # (1, 5)\n+                self.comment_4,  # (2, 4)\n+            ),\n+        )\n+\n+    def test_order_comments_by_pk_desc(self):\n+        self.assertSequenceEqual(\n+            Comment.objects.order_by(\"-pk\"),\n+            (\n+                self.comment_4,  # (2, 4)\n+                self.comment_5,  # (1, 5)\n+                self.comment_3,  # (1, 3)\n+                self.comment_2,  # (1, 2)\n+                self.comment_1,  # (1, 1)\n+            ),\n+        )\n+\n+    def test_filter_comments_by_pk_gt(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        test_cases = (\n+            (c11, (c12, c13, c15, c24)),\n+            (c12, (c13, c15, c24)),\n+            (c13, (c15, c24)),\n+            (c15, (c24,)),\n+            (c24, ()),\n+        )\n+\n+        for obj, objs in test_cases:\n+            with self.subTest(obj=obj, objs=objs):\n+                self.assertSequenceEqual(\n+                    Comment.objects.filter(pk__gt=obj.pk).order_by(\"pk\"), objs\n+                )\n+\n+    def test_filter_comments_by_pk_gte(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        test_cases = (\n+            (c11, (c11, c12, c13, c15, c24)),\n+            (c12, (c12, c13, c15, c24)),\n+            (c13, (c13, c15, c24)),\n+            (c15, (c15, c24)),\n+            (c24, (c24,)),\n+        )\n+\n+        for obj, objs in test_cases:\n+            with self.subTest(obj=obj, objs=objs):\n+                self.assertSequenceEqual(\n+                    Comment.objects.filter(pk__gte=obj.pk).order_by(\"pk\"), objs\n+                )\n+\n+    def test_filter_comments_by_pk_lt(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        test_cases = (\n+            (c24, (c11, c12, c13, c15)),\n+            (c15, (c11, c12, c13)),\n+            (c13, (c11, c12)),\n+            (c12, (c11,)),\n+            (c11, ()),\n+        )\n+\n+        for obj, objs in test_cases:\n+            with self.subTest(obj=obj, objs=objs):\n+                self.assertSequenceEqual(\n+                    Comment.objects.filter(pk__lt=obj.pk).order_by(\"pk\"), objs\n+                )\n+\n+    def test_filter_comments_by_pk_lte(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        test_cases = (\n+            (c24, (c11, c12, c13, c15, c24)),\n+            (c15, (c11, c12, c13, c15)),\n+            (c13, (c11, c12, c13)),\n+            (c12, (c11, c12)),\n+            (c11, (c11,)),\n+        )\n+\n+        for obj, objs in test_cases:\n+            with self.subTest(obj=obj, objs=objs):\n+                self.assertSequenceEqual(\n+                    Comment.objects.filter(pk__lte=obj.pk).order_by(\"pk\"), objs\n+                )\n+\n+    def test_filter_comments_by_pk_in(self):\n+        test_cases = (\n+            (),\n+            (self.comment_1,),\n+            (self.comment_1, self.comment_4),\n+        )\n+\n+        for objs in test_cases:\n+            with self.subTest(objs=objs):\n+                pks = [obj.pk for obj in objs]\n+                self.assertSequenceEqual(\n+                    Comment.objects.filter(pk__in=pks).order_by(\"pk\"), objs\n+                )\n+\n+    def test_filter_comments_by_user_and_order_by_pk_asc(self):\n+        self.assertSequenceEqual(\n+            Comment.objects.filter(user=self.user_1).order_by(\"pk\"),\n+            (self.comment_1, self.comment_2, self.comment_5),\n+        )\n+\n+    def test_filter_comments_by_user_and_order_by_pk_desc(self):\n+        self.assertSequenceEqual(\n+            Comment.objects.filter(user=self.user_1).order_by(\"-pk\"),\n+            (self.comment_5, self.comment_2, self.comment_1),\n+        )\n+\n+    def test_filter_comments_by_user_and_exclude_by_pk(self):\n+        self.assertSequenceEqual(\n+            Comment.objects.filter(user=self.user_1)\n+            .exclude(pk=self.comment_1.pk)\n+            .order_by(\"pk\"),\n+            (self.comment_2, self.comment_5),\n+        )\n+\n+    def test_filter_comments_by_user_and_contains(self):\n+        self.assertIs(\n+            Comment.objects.filter(user=self.user_1).contains(self.comment_1), True\n+        )\n+\n+    def test_filter_users_by_comments_in(self):\n+        c1, c2, c3, c4, c5 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        u1, u2, u3 = (\n+            self.user_1,\n+            self.user_2,\n+            self.user_3,\n+        )\n+        test_cases = (\n+            ((), ()),\n+            ((c1,), (u1,)),\n+            ((c1, c2), (u1, u1)),\n+            ((c1, c2, c3), (u1, u1, u2)),\n+            ((c1, c2, c3, c4), (u1, u1, u2, u3)),\n+            ((c1, c2, c3, c4, c5), (u1, u1, u1, u2, u3)),\n+        )\n+\n+        for comments, users in test_cases:\n+            with self.subTest(comments=comments, users=users):\n+                self.assertSequenceEqual(\n+                    User.objects.filter(comments__in=comments).order_by(\"pk\"), users\n+                )\n+\n+    def test_filter_users_by_comments_lt(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        u1, u2 = (\n+            self.user_1,\n+            self.user_2,\n+        )\n+        test_cases = (\n+            (c11, ()),\n+            (c12, (u1,)),\n+            (c13, (u1, u1)),\n+            (c15, (u1, u1, u2)),\n+            (c24, (u1, u1, u1, u2)),\n+        )\n+\n+        for comment, users in test_cases:\n+            with self.subTest(comment=comment, users=users):\n+                self.assertSequenceEqual(\n+                    User.objects.filter(comments__lt=comment).order_by(\"pk\"), users\n+                )\n+\n+    def test_filter_users_by_comments_lte(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        u1, u2, u3 = (\n+            self.user_1,\n+            self.user_2,\n+            self.user_3,\n+        )\n+        test_cases = (\n+            (c11, (u1,)),\n+            (c12, (u1, u1)),\n+            (c13, (u1, u1, u2)),\n+            (c15, (u1, u1, u1, u2)),\n+            (c24, (u1, u1, u1, u2, u3)),\n+        )\n+\n+        for comment, users in test_cases:\n+            with self.subTest(comment=comment, users=users):\n+                self.assertSequenceEqual(\n+                    User.objects.filter(comments__lte=comment).order_by(\"pk\"), users\n+                )\n+\n+    def test_filter_users_by_comments_gt(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        u1, u2, u3 = (\n+            self.user_1,\n+            self.user_2,\n+            self.user_3,\n+        )\n+        test_cases = (\n+            (c11, (u1, u1, u2, u3)),\n+            (c12, (u1, u2, u3)),\n+            (c13, (u1, u3)),\n+            (c15, (u3,)),\n+            (c24, ()),\n+        )\n+\n+        for comment, users in test_cases:\n+            with self.subTest(comment=comment, users=users):\n+                self.assertSequenceEqual(\n+                    User.objects.filter(comments__gt=comment).order_by(\"pk\"), users\n+                )\n+\n+    def test_filter_users_by_comments_gte(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        u1, u2, u3 = (\n+            self.user_1,\n+            self.user_2,\n+            self.user_3,\n+        )\n+        test_cases = (\n+            (c11, (u1, u1, u1, u2, u3)),\n+            (c12, (u1, u1, u2, u3)),\n+            (c13, (u1, u2, u3)),\n+            (c15, (u1, u3)),\n+            (c24, (u3,)),\n+        )\n+\n+        for comment, users in test_cases:\n+            with self.subTest(comment=comment, users=users):\n+                self.assertSequenceEqual(\n+                    User.objects.filter(comments__gte=comment).order_by(\"pk\"), users\n+                )\n+\n+    def test_filter_users_by_comments_exact(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+        u1, u2, u3 = (\n+            self.user_1,\n+            self.user_2,\n+            self.user_3,\n+        )\n+        test_cases = (\n+            (c11, (u1,)),\n+            (c12, (u1,)),\n+            (c13, (u2,)),\n+            (c15, (u1,)),\n+            (c24, (u3,)),\n+        )\n+\n+        for comment, users in test_cases:\n+            with self.subTest(comment=comment, users=users):\n+                self.assertSequenceEqual(\n+                    User.objects.filter(comments=comment).order_by(\"pk\"), users\n+                )\n+\n+    def test_filter_users_by_comments_isnull(self):\n+        u1, u2, u3, u4 = (\n+            self.user_1,\n+            self.user_2,\n+            self.user_3,\n+            self.user_4,\n+        )\n+\n+        with self.subTest(\"comments__isnull=True\"):\n+            self.assertSequenceEqual(\n+                User.objects.filter(comments__isnull=True).order_by(\"pk\"),\n+                (u4,),\n+            )\n+        with self.subTest(\"comments__isnull=False\"):\n+            self.assertSequenceEqual(\n+                User.objects.filter(comments__isnull=False).order_by(\"pk\"),\n+                (u1, u1, u1, u2, u3),\n+            )\n+\n+    def test_filter_comments_by_pk_isnull(self):\n+        c11, c12, c13, c24, c15 = (\n+            self.comment_1,\n+            self.comment_2,\n+            self.comment_3,\n+            self.comment_4,\n+            self.comment_5,\n+        )\n+\n+        with self.subTest(\"pk__isnull=True\"):\n+            self.assertSequenceEqual(\n+                Comment.objects.filter(pk__isnull=True).order_by(\"pk\"),\n+                (),\n+            )\n+        with self.subTest(\"pk__isnull=False\"):\n+            self.assertSequenceEqual(\n+                Comment.objects.filter(pk__isnull=False).order_by(\"pk\"),\n+                (c11, c12, c13, c15, c24),\n+            )\n+\n+    def test_filter_users_by_comments_subquery(self):\n+        subquery = Comment.objects.filter(id=3).only(\"pk\")\n+        queryset = User.objects.filter(comments__in=subquery)\n+        self.assertSequenceEqual(queryset, (self.user_2,))\ndiff --git a/tests/composite_pk/test_get.py b/tests/composite_pk/test_get.py\nnew file mode 100644\nindex 000000000000..c896ec26ed49\n--- /dev/null\n+++ b/tests/composite_pk/test_get.py\n@@ -0,0 +1,126 @@\n+from django.test import TestCase\n+\n+from .models import Comment, Tenant, User\n+\n+\n+class CompositePKGetTests(TestCase):\n+    maxDiff = None\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.tenant_1 = Tenant.objects.create()\n+        cls.tenant_2 = Tenant.objects.create()\n+        cls.user_1 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=1,\n+            email=\"user0001@example.com\",\n+        )\n+        cls.user_2 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=2,\n+            email=\"user0002@example.com\",\n+        )\n+        cls.user_3 = User.objects.create(\n+            tenant=cls.tenant_2,\n+            id=3,\n+            email=\"user0003@example.com\",\n+        )\n+        cls.comment_1 = Comment.objects.create(id=1, user=cls.user_1)\n+\n+    def test_get_user(self):\n+        test_cases = (\n+            {\"pk\": self.user_1.pk},\n+            {\"pk\": (self.tenant_1.id, self.user_1.id)},\n+            {\"id\": self.user_1.id},\n+        )\n+\n+        for lookup in test_cases:\n+            with self.subTest(lookup=lookup):\n+                self.assertEqual(User.objects.get(**lookup), self.user_1)\n+\n+    def test_get_comment(self):\n+        test_cases = (\n+            {\"pk\": self.comment_1.pk},\n+            {\"pk\": (self.tenant_1.id, self.comment_1.id)},\n+            {\"id\": self.comment_1.id},\n+            {\"user\": self.user_1},\n+            {\"user_id\": self.user_1.id},\n+            {\"user__id\": self.user_1.id},\n+            {\"user__pk\": self.user_1.pk},\n+            {\"tenant\": self.tenant_1},\n+            {\"tenant_id\": self.tenant_1.id},\n+            {\"tenant__id\": self.tenant_1.id},\n+            {\"tenant__pk\": self.tenant_1.pk},\n+        )\n+\n+        for lookup in test_cases:\n+            with self.subTest(lookup=lookup):\n+                self.assertEqual(Comment.objects.get(**lookup), self.comment_1)\n+\n+    def test_get_or_create_user(self):\n+        test_cases = (\n+            {\n+                \"pk\": self.user_1.pk,\n+                \"defaults\": {\"email\": \"user9201@example.com\"},\n+            },\n+            {\n+                \"pk\": (self.tenant_1.id, self.user_1.id),\n+                \"defaults\": {\"email\": \"user9201@example.com\"},\n+            },\n+            {\n+                \"tenant\": self.tenant_1,\n+                \"id\": self.user_1.id,\n+                \"defaults\": {\"email\": \"user3512@example.com\"},\n+            },\n+            {\n+                \"tenant_id\": self.tenant_1.id,\n+                \"id\": self.user_1.id,\n+                \"defaults\": {\"email\": \"user8239@example.com\"},\n+            },\n+        )\n+\n+        for fields in test_cases:\n+            with self.subTest(fields=fields):\n+                count = User.objects.count()\n+                user, created = User.objects.get_or_create(**fields)\n+                self.assertIs(created, False)\n+                self.assertEqual(user.id, self.user_1.id)\n+                self.assertEqual(user.pk, (self.tenant_1.id, self.user_1.id))\n+                self.assertEqual(user.tenant_id, self.tenant_1.id)\n+                self.assertEqual(user.email, self.user_1.email)\n+                self.assertEqual(count, User.objects.count())\n+\n+    def test_lookup_errors(self):\n+        m_tuple = \"'%s' lookup of 'pk' must be a tuple or a list\"\n+        m_2_elements = \"'%s' lookup of 'pk' must have 2 elements\"\n+        m_tuple_collection = (\n+            \"'in' lookup of 'pk' must be a collection of tuples or lists\"\n+        )\n+        m_2_elements_each = \"'in' lookup of 'pk' must have 2 elements each\"\n+        test_cases = (\n+            ({\"pk\": 1}, m_tuple % \"exact\"),\n+            ({\"pk\": (1, 2, 3)}, m_2_elements % \"exact\"),\n+            ({\"pk__exact\": 1}, m_tuple % \"exact\"),\n+            ({\"pk__exact\": (1, 2, 3)}, m_2_elements % \"exact\"),\n+            ({\"pk__in\": 1}, m_tuple % \"in\"),\n+            ({\"pk__in\": (1, 2, 3)}, m_tuple_collection),\n+            ({\"pk__in\": ((1, 2, 3),)}, m_2_elements_each),\n+            ({\"pk__gt\": 1}, m_tuple % \"gt\"),\n+            ({\"pk__gt\": (1, 2, 3)}, m_2_elements % \"gt\"),\n+            ({\"pk__gte\": 1}, m_tuple % \"gte\"),\n+            ({\"pk__gte\": (1, 2, 3)}, m_2_elements % \"gte\"),\n+            ({\"pk__lt\": 1}, m_tuple % \"lt\"),\n+            ({\"pk__lt\": (1, 2, 3)}, m_2_elements % \"lt\"),\n+            ({\"pk__lte\": 1}, m_tuple % \"lte\"),\n+            ({\"pk__lte\": (1, 2, 3)}, m_2_elements % \"lte\"),\n+        )\n+\n+        for kwargs, message in test_cases:\n+            with (\n+                self.subTest(kwargs=kwargs),\n+                self.assertRaisesMessage(ValueError, message),\n+            ):\n+                Comment.objects.get(**kwargs)\n+\n+    def test_get_user_by_comments(self):\n+        self.assertEqual(User.objects.get(comments=self.comment_1), self.user_1)\ndiff --git a/tests/composite_pk/test_models.py b/tests/composite_pk/test_models.py\nnew file mode 100644\nindex 000000000000..ca6ad8b5dc3a\n--- /dev/null\n+++ b/tests/composite_pk/test_models.py\n@@ -0,0 +1,153 @@\n+from django.contrib.contenttypes.models import ContentType\n+from django.core.exceptions import ValidationError\n+from django.test import TestCase\n+\n+from .models import Comment, Tenant, Token, User\n+\n+\n+class CompositePKModelsTests(TestCase):\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.tenant_1 = Tenant.objects.create()\n+        cls.tenant_2 = Tenant.objects.create()\n+        cls.user_1 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=1,\n+            email=\"user0001@example.com\",\n+        )\n+        cls.user_2 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=2,\n+            email=\"user0002@example.com\",\n+        )\n+        cls.user_3 = User.objects.create(\n+            tenant=cls.tenant_2,\n+            id=3,\n+            email=\"user0003@example.com\",\n+        )\n+        cls.comment_1 = Comment.objects.create(id=1, user=cls.user_1)\n+        cls.comment_2 = Comment.objects.create(id=2, user=cls.user_1)\n+        cls.comment_3 = Comment.objects.create(id=3, user=cls.user_2)\n+        cls.comment_4 = Comment.objects.create(id=4, user=cls.user_3)\n+\n+    def test_fields(self):\n+        # tenant_1\n+        self.assertSequenceEqual(\n+            self.tenant_1.user_set.order_by(\"pk\"),\n+            [self.user_1, self.user_2],\n+        )\n+        self.assertSequenceEqual(\n+            self.tenant_1.comments.order_by(\"pk\"),\n+            [self.comment_1, self.comment_2, self.comment_3],\n+        )\n+\n+        # tenant_2\n+        self.assertSequenceEqual(self.tenant_2.user_set.order_by(\"pk\"), [self.user_3])\n+        self.assertSequenceEqual(\n+            self.tenant_2.comments.order_by(\"pk\"), [self.comment_4]\n+        )\n+\n+        # user_1\n+        self.assertEqual(self.user_1.id, 1)\n+        self.assertEqual(self.user_1.tenant_id, self.tenant_1.id)\n+        self.assertEqual(self.user_1.tenant, self.tenant_1)\n+        self.assertEqual(self.user_1.pk, (self.tenant_1.id, self.user_1.id))\n+        self.assertSequenceEqual(\n+            self.user_1.comments.order_by(\"pk\"), [self.comment_1, self.comment_2]\n+        )\n+\n+        # user_2\n+        self.assertEqual(self.user_2.id, 2)\n+        self.assertEqual(self.user_2.tenant_id, self.tenant_1.id)\n+        self.assertEqual(self.user_2.tenant, self.tenant_1)\n+        self.assertEqual(self.user_2.pk, (self.tenant_1.id, self.user_2.id))\n+        self.assertSequenceEqual(self.user_2.comments.order_by(\"pk\"), [self.comment_3])\n+\n+        # comment_1\n+        self.assertEqual(self.comment_1.id, 1)\n+        self.assertEqual(self.comment_1.user_id, self.user_1.id)\n+        self.assertEqual(self.comment_1.user, self.user_1)\n+        self.assertEqual(self.comment_1.tenant_id, self.tenant_1.id)\n+        self.assertEqual(self.comment_1.tenant, self.tenant_1)\n+        self.assertEqual(self.comment_1.pk, (self.tenant_1.id, self.user_1.id))\n+\n+    def test_full_clean_success(self):\n+        test_cases = (\n+            # 1, 1234, {}\n+            ({\"tenant\": self.tenant_1, \"id\": 1234}, {}),\n+            ({\"tenant_id\": self.tenant_1.id, \"id\": 1234}, {}),\n+            ({\"pk\": (self.tenant_1.id, 1234)}, {}),\n+            # 1, 1, {\"id\"}\n+            ({\"tenant\": self.tenant_1, \"id\": 1}, {\"id\"}),\n+            ({\"tenant_id\": self.tenant_1.id, \"id\": 1}, {\"id\"}),\n+            ({\"pk\": (self.tenant_1.id, 1)}, {\"id\"}),\n+            # 1, 1, {\"tenant\", \"id\"}\n+            ({\"tenant\": self.tenant_1, \"id\": 1}, {\"tenant\", \"id\"}),\n+            ({\"tenant_id\": self.tenant_1.id, \"id\": 1}, {\"tenant\", \"id\"}),\n+            ({\"pk\": (self.tenant_1.id, 1)}, {\"tenant\", \"id\"}),\n+        )\n+\n+        for kwargs, exclude in test_cases:\n+            with self.subTest(kwargs):\n+                kwargs[\"email\"] = \"user0004@example.com\"\n+                User(**kwargs).full_clean(exclude=exclude)\n+\n+    def test_full_clean_failure(self):\n+        e_tenant_and_id = \"User with this Tenant and Id already exists.\"\n+        e_id = \"User with this Id already exists.\"\n+        test_cases = (\n+            # 1, 1, {}\n+            ({\"tenant\": self.tenant_1, \"id\": 1}, {}, (e_tenant_and_id, e_id)),\n+            ({\"tenant_id\": self.tenant_1.id, \"id\": 1}, {}, (e_tenant_and_id, e_id)),\n+            ({\"pk\": (self.tenant_1.id, 1)}, {}, (e_tenant_and_id, e_id)),\n+            # 2, 1, {}\n+            ({\"tenant\": self.tenant_2, \"id\": 1}, {}, (e_id,)),\n+            ({\"tenant_id\": self.tenant_2.id, \"id\": 1}, {}, (e_id,)),\n+            ({\"pk\": (self.tenant_2.id, 1)}, {}, (e_id,)),\n+            # 1, 1, {\"tenant\"}\n+            ({\"tenant\": self.tenant_1, \"id\": 1}, {\"tenant\"}, (e_id,)),\n+            ({\"tenant_id\": self.tenant_1.id, \"id\": 1}, {\"tenant\"}, (e_id,)),\n+            ({\"pk\": (self.tenant_1.id, 1)}, {\"tenant\"}, (e_id,)),\n+        )\n+\n+        for kwargs, exclude, messages in test_cases:\n+            with self.subTest(kwargs):\n+                with self.assertRaises(ValidationError) as ctx:\n+                    kwargs[\"email\"] = \"user0004@example.com\"\n+                    User(**kwargs).full_clean(exclude=exclude)\n+\n+                self.assertSequenceEqual(ctx.exception.messages, messages)\n+\n+    def test_field_conflicts(self):\n+        test_cases = (\n+            ({\"pk\": (1, 1), \"id\": 2}, (1, 1)),\n+            ({\"id\": 2, \"pk\": (1, 1)}, (1, 1)),\n+            ({\"pk\": (1, 1), \"tenant_id\": 2}, (1, 1)),\n+            ({\"tenant_id\": 2, \"pk\": (1, 1)}, (1, 1)),\n+            ({\"pk\": (2, 2), \"tenant_id\": 3, \"id\": 4}, (2, 2)),\n+            ({\"tenant_id\": 3, \"id\": 4, \"pk\": (2, 2)}, (2, 2)),\n+        )\n+\n+        for kwargs, pk in test_cases:\n+            with self.subTest(kwargs=kwargs):\n+                user = User(**kwargs)\n+                self.assertEqual(user.pk, pk)\n+\n+    def test_validate_unique(self):\n+        user = User.objects.get(pk=self.user_1.pk)\n+        user.id = None\n+\n+        with self.assertRaises(ValidationError) as ctx:\n+            user.validate_unique()\n+\n+        self.assertSequenceEqual(\n+            ctx.exception.messages, (\"User with this Email already exists.\",)\n+        )\n+\n+    def test_permissions(self):\n+        token = ContentType.objects.get_for_model(Token)\n+        user = ContentType.objects.get_for_model(User)\n+        comment = ContentType.objects.get_for_model(Comment)\n+        self.assertEqual(4, token.permission_set.count())\n+        self.assertEqual(4, user.permission_set.count())\n+        self.assertEqual(4, comment.permission_set.count())\ndiff --git a/tests/composite_pk/test_names_to_path.py b/tests/composite_pk/test_names_to_path.py\nnew file mode 100644\nindex 000000000000..de4a04f4cba7\n--- /dev/null\n+++ b/tests/composite_pk/test_names_to_path.py\n@@ -0,0 +1,134 @@\n+from django.db.models.query_utils import PathInfo\n+from django.db.models.sql import Query\n+from django.test import TestCase\n+\n+from .models import Comment, Tenant, User\n+\n+\n+class NamesToPathTests(TestCase):\n+    def test_id(self):\n+        query = Query(User)\n+        path, final_field, targets, rest = query.names_to_path([\"id\"], User._meta)\n+\n+        self.assertEqual(path, [])\n+        self.assertEqual(final_field, User._meta.get_field(\"id\"))\n+        self.assertEqual(targets, (User._meta.get_field(\"id\"),))\n+        self.assertEqual(rest, [])\n+\n+    def test_pk(self):\n+        query = Query(User)\n+        path, final_field, targets, rest = query.names_to_path([\"pk\"], User._meta)\n+\n+        self.assertEqual(path, [])\n+        self.assertEqual(final_field, User._meta.get_field(\"pk\"))\n+        self.assertEqual(targets, (User._meta.get_field(\"pk\"),))\n+        self.assertEqual(rest, [])\n+\n+    def test_tenant_id(self):\n+        query = Query(User)\n+        path, final_field, targets, rest = query.names_to_path(\n+            [\"tenant\", \"id\"], User._meta\n+        )\n+\n+        self.assertEqual(\n+            path,\n+            [\n+                PathInfo(\n+                    from_opts=User._meta,\n+                    to_opts=Tenant._meta,\n+                    target_fields=(Tenant._meta.get_field(\"id\"),),\n+                    join_field=User._meta.get_field(\"tenant\"),\n+                    m2m=False,\n+                    direct=True,\n+                    filtered_relation=None,\n+                ),\n+            ],\n+        )\n+        self.assertEqual(final_field, Tenant._meta.get_field(\"id\"))\n+        self.assertEqual(targets, (Tenant._meta.get_field(\"id\"),))\n+        self.assertEqual(rest, [])\n+\n+    def test_user_id(self):\n+        query = Query(Comment)\n+        path, final_field, targets, rest = query.names_to_path(\n+            [\"user\", \"id\"], Comment._meta\n+        )\n+\n+        self.assertEqual(\n+            path,\n+            [\n+                PathInfo(\n+                    from_opts=Comment._meta,\n+                    to_opts=User._meta,\n+                    target_fields=(\n+                        User._meta.get_field(\"tenant\"),\n+                        User._meta.get_field(\"id\"),\n+                    ),\n+                    join_field=Comment._meta.get_field(\"user\"),\n+                    m2m=False,\n+                    direct=True,\n+                    filtered_relation=None,\n+                ),\n+            ],\n+        )\n+        self.assertEqual(final_field, User._meta.get_field(\"id\"))\n+        self.assertEqual(targets, (User._meta.get_field(\"id\"),))\n+        self.assertEqual(rest, [])\n+\n+    def test_user_tenant_id(self):\n+        query = Query(Comment)\n+        path, final_field, targets, rest = query.names_to_path(\n+            [\"user\", \"tenant\", \"id\"], Comment._meta\n+        )\n+\n+        self.assertEqual(\n+            path,\n+            [\n+                PathInfo(\n+                    from_opts=Comment._meta,\n+                    to_opts=User._meta,\n+                    target_fields=(\n+                        User._meta.get_field(\"tenant\"),\n+                        User._meta.get_field(\"id\"),\n+                    ),\n+                    join_field=Comment._meta.get_field(\"user\"),\n+                    m2m=False,\n+                    direct=True,\n+                    filtered_relation=None,\n+                ),\n+                PathInfo(\n+                    from_opts=User._meta,\n+                    to_opts=Tenant._meta,\n+                    target_fields=(Tenant._meta.get_field(\"id\"),),\n+                    join_field=User._meta.get_field(\"tenant\"),\n+                    m2m=False,\n+                    direct=True,\n+                    filtered_relation=None,\n+                ),\n+            ],\n+        )\n+        self.assertEqual(final_field, Tenant._meta.get_field(\"id\"))\n+        self.assertEqual(targets, (Tenant._meta.get_field(\"id\"),))\n+        self.assertEqual(rest, [])\n+\n+    def test_comments(self):\n+        query = Query(User)\n+        path, final_field, targets, rest = query.names_to_path([\"comments\"], User._meta)\n+\n+        self.assertEqual(\n+            path,\n+            [\n+                PathInfo(\n+                    from_opts=User._meta,\n+                    to_opts=Comment._meta,\n+                    target_fields=(Comment._meta.get_field(\"pk\"),),\n+                    join_field=User._meta.get_field(\"comments\"),\n+                    m2m=True,\n+                    direct=False,\n+                    filtered_relation=None,\n+                ),\n+            ],\n+        )\n+        self.assertEqual(final_field, User._meta.get_field(\"comments\"))\n+        self.assertEqual(targets, (Comment._meta.get_field(\"pk\"),))\n+        self.assertEqual(rest, [])\ndiff --git a/tests/composite_pk/test_update.py b/tests/composite_pk/test_update.py\nnew file mode 100644\nindex 000000000000..e7117454477c\n--- /dev/null\n+++ b/tests/composite_pk/test_update.py\n@@ -0,0 +1,135 @@\n+from django.test import TestCase\n+\n+from .models import Comment, Tenant, Token, User\n+\n+\n+class CompositePKUpdateTests(TestCase):\n+    maxDiff = None\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.tenant_1 = Tenant.objects.create(name=\"A\")\n+        cls.tenant_2 = Tenant.objects.create(name=\"B\")\n+        cls.user_1 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=1,\n+            email=\"user0001@example.com\",\n+        )\n+        cls.user_2 = User.objects.create(\n+            tenant=cls.tenant_1,\n+            id=2,\n+            email=\"user0002@example.com\",\n+        )\n+        cls.user_3 = User.objects.create(\n+            tenant=cls.tenant_2,\n+            id=3,\n+            email=\"user0003@example.com\",\n+        )\n+        cls.comment_1 = Comment.objects.create(id=1, user=cls.user_1)\n+        cls.comment_2 = Comment.objects.create(id=2, user=cls.user_1)\n+        cls.comment_3 = Comment.objects.create(id=3, user=cls.user_2)\n+        cls.token_1 = Token.objects.create(id=1, tenant=cls.tenant_1)\n+        cls.token_2 = Token.objects.create(id=2, tenant=cls.tenant_2)\n+        cls.token_3 = Token.objects.create(id=3, tenant=cls.tenant_1)\n+        cls.token_4 = Token.objects.create(id=4, tenant=cls.tenant_2)\n+\n+    def test_update_user(self):\n+        email = \"user9315@example.com\"\n+        result = User.objects.filter(pk=self.user_1.pk).update(email=email)\n+        self.assertEqual(result, 1)\n+        user = User.objects.get(pk=self.user_1.pk)\n+        self.assertEqual(user.email, email)\n+\n+    def test_save_user(self):\n+        count = User.objects.count()\n+        email = \"user9314@example.com\"\n+        user = User.objects.get(pk=self.user_1.pk)\n+        user.email = email\n+        user.save()\n+        user.refresh_from_db()\n+        self.assertEqual(user.email, email)\n+        user = User.objects.get(pk=self.user_1.pk)\n+        self.assertEqual(user.email, email)\n+        self.assertEqual(count, User.objects.count())\n+\n+    def test_bulk_update_comments(self):\n+        comment_1 = Comment.objects.get(pk=self.comment_1.pk)\n+        comment_2 = Comment.objects.get(pk=self.comment_2.pk)\n+        comment_3 = Comment.objects.get(pk=self.comment_3.pk)\n+        comment_1.text = \"foo\"\n+        comment_2.text = \"bar\"\n+        comment_3.text = \"baz\"\n+\n+        result = Comment.objects.bulk_update(\n+            [comment_1, comment_2, comment_3], [\"text\"]\n+        )\n+\n+        self.assertEqual(result, 3)\n+        comment_1 = Comment.objects.get(pk=self.comment_1.pk)\n+        comment_2 = Comment.objects.get(pk=self.comment_2.pk)\n+        comment_3 = Comment.objects.get(pk=self.comment_3.pk)\n+        self.assertEqual(comment_1.text, \"foo\")\n+        self.assertEqual(comment_2.text, \"bar\")\n+        self.assertEqual(comment_3.text, \"baz\")\n+\n+    def test_update_or_create_user(self):\n+        test_cases = (\n+            {\n+                \"pk\": self.user_1.pk,\n+                \"defaults\": {\"email\": \"user3914@example.com\"},\n+            },\n+            {\n+                \"pk\": (self.tenant_1.id, self.user_1.id),\n+                \"defaults\": {\"email\": \"user9375@example.com\"},\n+            },\n+            {\n+                \"tenant\": self.tenant_1,\n+                \"id\": self.user_1.id,\n+                \"defaults\": {\"email\": \"user3517@example.com\"},\n+            },\n+            {\n+                \"tenant_id\": self.tenant_1.id,\n+                \"id\": self.user_1.id,\n+                \"defaults\": {\"email\": \"user8391@example.com\"},\n+            },\n+        )\n+\n+        for fields in test_cases:\n+            with self.subTest(fields=fields):\n+                count = User.objects.count()\n+                user, created = User.objects.update_or_create(**fields)\n+                self.assertIs(created, False)\n+                self.assertEqual(user.id, self.user_1.id)\n+                self.assertEqual(user.pk, (self.tenant_1.id, self.user_1.id))\n+                self.assertEqual(user.tenant_id, self.tenant_1.id)\n+                self.assertEqual(user.email, fields[\"defaults\"][\"email\"])\n+                self.assertEqual(count, User.objects.count())\n+\n+    def test_update_comment_by_user_email(self):\n+        result = Comment.objects.filter(user__email=self.user_1.email).update(\n+            text=\"foo\"\n+        )\n+\n+        self.assertEqual(result, 2)\n+        comment_1 = Comment.objects.get(pk=self.comment_1.pk)\n+        comment_2 = Comment.objects.get(pk=self.comment_2.pk)\n+        self.assertEqual(comment_1.text, \"foo\")\n+        self.assertEqual(comment_2.text, \"foo\")\n+\n+    def test_update_token_by_tenant_name(self):\n+        result = Token.objects.filter(tenant__name=\"A\").update(secret=\"bar\")\n+\n+        self.assertEqual(result, 2)\n+        token_1 = Token.objects.get(pk=self.token_1.pk)\n+        self.assertEqual(token_1.secret, \"bar\")\n+        token_3 = Token.objects.get(pk=self.token_3.pk)\n+        self.assertEqual(token_3.secret, \"bar\")\n+\n+    def test_cant_update_to_unsaved_object(self):\n+        msg = (\n+            \"Unsaved model instance <User: User object ((None, None))> cannot be used \"\n+            \"in an ORM query.\"\n+        )\n+\n+        with self.assertRaisesMessage(ValueError, msg):\n+            Comment.objects.update(user=User())\ndiff --git a/tests/composite_pk/test_values.py b/tests/composite_pk/test_values.py\nnew file mode 100644\nindex 000000000000..a3c7a589ccbc\n--- /dev/null\n+++ b/tests/composite_pk/test_values.py\n@@ -0,0 +1,212 @@\n+from collections import namedtuple\n+from uuid import UUID\n+\n+from django.test import TestCase\n+\n+from .models import Post, Tenant, User\n+\n+\n+class CompositePKValuesTests(TestCase):\n+    USER_1_EMAIL = \"user0001@example.com\"\n+    USER_2_EMAIL = \"user0002@example.com\"\n+    USER_3_EMAIL = \"user0003@example.com\"\n+    POST_1_ID = \"77777777-7777-7777-7777-777777777777\"\n+    POST_2_ID = \"bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb\"\n+    POST_3_ID = \"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\"\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        super().setUpTestData()\n+        cls.tenant_1 = Tenant.objects.create()\n+        cls.tenant_2 = Tenant.objects.create()\n+        cls.user_1 = User.objects.create(\n+            tenant=cls.tenant_1, id=1, email=cls.USER_1_EMAIL\n+        )\n+        cls.user_2 = User.objects.create(\n+            tenant=cls.tenant_1, id=2, email=cls.USER_2_EMAIL\n+        )\n+        cls.user_3 = User.objects.create(\n+            tenant=cls.tenant_2, id=3, email=cls.USER_3_EMAIL\n+        )\n+        cls.post_1 = Post.objects.create(tenant=cls.tenant_1, id=cls.POST_1_ID)\n+        cls.post_2 = Post.objects.create(tenant=cls.tenant_1, id=cls.POST_2_ID)\n+        cls.post_3 = Post.objects.create(tenant=cls.tenant_2, id=cls.POST_3_ID)\n+\n+    def test_values_list(self):\n+        with self.subTest('User.objects.values_list(\"pk\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values_list(\"pk\").order_by(\"pk\"),\n+                (\n+                    (self.user_1.pk,),\n+                    (self.user_2.pk,),\n+                    (self.user_3.pk,),\n+                ),\n+            )\n+        with self.subTest('User.objects.values_list(\"pk\", \"email\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values_list(\"pk\", \"email\").order_by(\"pk\"),\n+                (\n+                    (self.user_1.pk, self.USER_1_EMAIL),\n+                    (self.user_2.pk, self.USER_2_EMAIL),\n+                    (self.user_3.pk, self.USER_3_EMAIL),\n+                ),\n+            )\n+        with self.subTest('User.objects.values_list(\"pk\", \"id\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values_list(\"pk\", \"id\").order_by(\"pk\"),\n+                (\n+                    (self.user_1.pk, self.user_1.id),\n+                    (self.user_2.pk, self.user_2.id),\n+                    (self.user_3.pk, self.user_3.id),\n+                ),\n+            )\n+        with self.subTest('User.objects.values_list(\"pk\", \"tenant_id\", \"id\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values_list(\"pk\", \"tenant_id\", \"id\").order_by(\"pk\"),\n+                (\n+                    (self.user_1.pk, self.user_1.tenant_id, self.user_1.id),\n+                    (self.user_2.pk, self.user_2.tenant_id, self.user_2.id),\n+                    (self.user_3.pk, self.user_3.tenant_id, self.user_3.id),\n+                ),\n+            )\n+        with self.subTest('User.objects.values_list(\"pk\", flat=True)'):\n+            self.assertSequenceEqual(\n+                User.objects.values_list(\"pk\", flat=True).order_by(\"pk\"),\n+                (\n+                    self.user_1.pk,\n+                    self.user_2.pk,\n+                    self.user_3.pk,\n+                ),\n+            )\n+        with self.subTest('Post.objects.values_list(\"pk\", flat=True)'):\n+            self.assertSequenceEqual(\n+                Post.objects.values_list(\"pk\", flat=True).order_by(\"pk\"),\n+                (\n+                    (self.tenant_1.id, UUID(self.POST_1_ID)),\n+                    (self.tenant_1.id, UUID(self.POST_2_ID)),\n+                    (self.tenant_2.id, UUID(self.POST_3_ID)),\n+                ),\n+            )\n+        with self.subTest('Post.objects.values_list(\"pk\")'):\n+            self.assertSequenceEqual(\n+                Post.objects.values_list(\"pk\").order_by(\"pk\"),\n+                (\n+                    ((self.tenant_1.id, UUID(self.POST_1_ID)),),\n+                    ((self.tenant_1.id, UUID(self.POST_2_ID)),),\n+                    ((self.tenant_2.id, UUID(self.POST_3_ID)),),\n+                ),\n+            )\n+        with self.subTest('Post.objects.values_list(\"pk\", \"id\")'):\n+            self.assertSequenceEqual(\n+                Post.objects.values_list(\"pk\", \"id\").order_by(\"pk\"),\n+                (\n+                    ((self.tenant_1.id, UUID(self.POST_1_ID)), UUID(self.POST_1_ID)),\n+                    ((self.tenant_1.id, UUID(self.POST_2_ID)), UUID(self.POST_2_ID)),\n+                    ((self.tenant_2.id, UUID(self.POST_3_ID)), UUID(self.POST_3_ID)),\n+                ),\n+            )\n+        with self.subTest('Post.objects.values_list(\"id\", \"pk\")'):\n+            self.assertSequenceEqual(\n+                Post.objects.values_list(\"id\", \"pk\").order_by(\"pk\"),\n+                (\n+                    (UUID(self.POST_1_ID), (self.tenant_1.id, UUID(self.POST_1_ID))),\n+                    (UUID(self.POST_2_ID), (self.tenant_1.id, UUID(self.POST_2_ID))),\n+                    (UUID(self.POST_3_ID), (self.tenant_2.id, UUID(self.POST_3_ID))),\n+                ),\n+            )\n+        with self.subTest('User.objects.values_list(\"pk\", named=True)'):\n+            Row = namedtuple(\"Row\", [\"pk\"])\n+            self.assertSequenceEqual(\n+                User.objects.values_list(\"pk\", named=True).order_by(\"pk\"),\n+                (\n+                    Row(pk=self.user_1.pk),\n+                    Row(pk=self.user_2.pk),\n+                    Row(pk=self.user_3.pk),\n+                ),\n+            )\n+        with self.subTest('User.objects.values_list(\"pk\", \"pk\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values_list(\"pk\", \"pk\").order_by(\"pk\"),\n+                (\n+                    (self.user_1.pk,),\n+                    (self.user_2.pk,),\n+                    (self.user_3.pk,),\n+                ),\n+            )\n+        with self.subTest('User.objects.values_list(\"pk\", \"id\", \"pk\", \"id\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values_list(\"pk\", \"id\", \"pk\", \"id\").order_by(\"pk\"),\n+                (\n+                    (self.user_1.pk, self.user_1.id),\n+                    (self.user_2.pk, self.user_2.id),\n+                    (self.user_3.pk, self.user_3.id),\n+                ),\n+            )\n+\n+    def test_values(self):\n+        with self.subTest('User.objects.values(\"pk\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values(\"pk\").order_by(\"pk\"),\n+                (\n+                    {\"pk\": self.user_1.pk},\n+                    {\"pk\": self.user_2.pk},\n+                    {\"pk\": self.user_3.pk},\n+                ),\n+            )\n+        with self.subTest('User.objects.values(\"pk\", \"email\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values(\"pk\", \"email\").order_by(\"pk\"),\n+                (\n+                    {\"pk\": self.user_1.pk, \"email\": self.USER_1_EMAIL},\n+                    {\"pk\": self.user_2.pk, \"email\": self.USER_2_EMAIL},\n+                    {\"pk\": self.user_3.pk, \"email\": self.USER_3_EMAIL},\n+                ),\n+            )\n+        with self.subTest('User.objects.values(\"pk\", \"id\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values(\"pk\", \"id\").order_by(\"pk\"),\n+                (\n+                    {\"pk\": self.user_1.pk, \"id\": self.user_1.id},\n+                    {\"pk\": self.user_2.pk, \"id\": self.user_2.id},\n+                    {\"pk\": self.user_3.pk, \"id\": self.user_3.id},\n+                ),\n+            )\n+        with self.subTest('User.objects.values(\"pk\", \"tenant_id\", \"id\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values(\"pk\", \"tenant_id\", \"id\").order_by(\"pk\"),\n+                (\n+                    {\n+                        \"pk\": self.user_1.pk,\n+                        \"tenant_id\": self.user_1.tenant_id,\n+                        \"id\": self.user_1.id,\n+                    },\n+                    {\n+                        \"pk\": self.user_2.pk,\n+                        \"tenant_id\": self.user_2.tenant_id,\n+                        \"id\": self.user_2.id,\n+                    },\n+                    {\n+                        \"pk\": self.user_3.pk,\n+                        \"tenant_id\": self.user_3.tenant_id,\n+                        \"id\": self.user_3.id,\n+                    },\n+                ),\n+            )\n+        with self.subTest('User.objects.values(\"pk\", \"pk\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values(\"pk\", \"pk\").order_by(\"pk\"),\n+                (\n+                    {\"pk\": self.user_1.pk},\n+                    {\"pk\": self.user_2.pk},\n+                    {\"pk\": self.user_3.pk},\n+                ),\n+            )\n+        with self.subTest('User.objects.values(\"pk\", \"id\", \"pk\", \"id\")'):\n+            self.assertSequenceEqual(\n+                User.objects.values(\"pk\", \"id\", \"pk\", \"id\").order_by(\"pk\"),\n+                (\n+                    {\"pk\": self.user_1.pk, \"id\": self.user_1.id},\n+                    {\"pk\": self.user_2.pk, \"id\": self.user_2.id},\n+                    {\"pk\": self.user_3.pk, \"id\": self.user_3.id},\n+                ),\n+            )\ndiff --git a/tests/composite_pk/tests.py b/tests/composite_pk/tests.py\nnew file mode 100644\nindex 000000000000..71522cb836fe\n--- /dev/null\n+++ b/tests/composite_pk/tests.py\n@@ -0,0 +1,345 @@\n+import json\n+import unittest\n+from uuid import UUID\n+\n+import yaml\n+\n+from django import forms\n+from django.core import serializers\n+from django.core.exceptions import FieldError\n+from django.db import IntegrityError, connection\n+from django.db.models import CompositePrimaryKey\n+from django.forms import modelform_factory\n+from django.test import TestCase\n+\n+from .models import Comment, Post, Tenant, User\n+\n+\n+class CommentForm(forms.ModelForm):\n+    class Meta:\n+        model = Comment\n+        fields = \"__all__\"\n+\n+\n+class CompositePKTests(TestCase):\n+    maxDiff = None\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.tenant = Tenant.objects.create()\n+        cls.user = User.objects.create(\n+            tenant=cls.tenant,\n+            id=1,\n+            email=\"user0001@example.com\",\n+        )\n+        cls.comment = Comment.objects.create(tenant=cls.tenant, id=1, user=cls.user)\n+\n+    @staticmethod\n+    def get_constraints(table):\n+        with connection.cursor() as cursor:\n+            return connection.introspection.get_constraints(cursor, table)\n+\n+    def test_pk_updated_if_field_updated(self):\n+        user = User.objects.get(pk=self.user.pk)\n+        self.assertEqual(user.pk, (self.tenant.id, self.user.id))\n+        self.assertIs(user._is_pk_set(), True)\n+        user.tenant_id = 9831\n+        self.assertEqual(user.pk, (9831, self.user.id))\n+        self.assertIs(user._is_pk_set(), True)\n+        user.id = 4321\n+        self.assertEqual(user.pk, (9831, 4321))\n+        self.assertIs(user._is_pk_set(), True)\n+        user.pk = (9132, 3521)\n+        self.assertEqual(user.tenant_id, 9132)\n+        self.assertEqual(user.id, 3521)\n+        self.assertIs(user._is_pk_set(), True)\n+        user.id = None\n+        self.assertEqual(user.pk, (9132, None))\n+        self.assertEqual(user.tenant_id, 9132)\n+        self.assertIsNone(user.id)\n+        self.assertIs(user._is_pk_set(), False)\n+\n+    def test_hash(self):\n+        self.assertEqual(hash(User(pk=(1, 2))), hash((1, 2)))\n+        self.assertEqual(hash(User(tenant_id=2, id=3)), hash((2, 3)))\n+        msg = \"Model instances without primary key value are unhashable\"\n+\n+        with self.assertRaisesMessage(TypeError, msg):\n+            hash(User())\n+        with self.assertRaisesMessage(TypeError, msg):\n+            hash(User(tenant_id=1))\n+        with self.assertRaisesMessage(TypeError, msg):\n+            hash(User(id=1))\n+\n+    def test_pk_must_be_list_or_tuple(self):\n+        user = User.objects.get(pk=self.user.pk)\n+        test_cases = [\n+            \"foo\",\n+            1000,\n+            3.14,\n+            True,\n+            False,\n+        ]\n+\n+        for pk in test_cases:\n+            with self.assertRaisesMessage(\n+                ValueError, \"'pk' must be a list or a tuple.\"\n+            ):\n+                user.pk = pk\n+\n+    def test_pk_must_have_2_elements(self):\n+        user = User.objects.get(pk=self.user.pk)\n+        test_cases = [\n+            (),\n+            [],\n+            (1000,),\n+            [1000],\n+            (1, 2, 3),\n+            [1, 2, 3],\n+        ]\n+\n+        for pk in test_cases:\n+            with self.assertRaisesMessage(ValueError, \"'pk' must have 2 elements.\"):\n+                user.pk = pk\n+\n+    def test_composite_pk_in_fields(self):\n+        user_fields = {f.name for f in User._meta.get_fields()}\n+        self.assertEqual(user_fields, {\"pk\", \"tenant\", \"id\", \"email\", \"comments\"})\n+\n+        comment_fields = {f.name for f in Comment._meta.get_fields()}\n+        self.assertEqual(\n+            comment_fields,\n+            {\"pk\", \"tenant\", \"id\", \"user_id\", \"user\", \"text\"},\n+        )\n+\n+    def test_pk_field(self):\n+        pk = User._meta.get_field(\"pk\")\n+        self.assertIsInstance(pk, CompositePrimaryKey)\n+        self.assertIs(User._meta.pk, pk)\n+\n+    def test_error_on_user_pk_conflict(self):\n+        with self.assertRaises(IntegrityError):\n+            User.objects.create(tenant=self.tenant, id=self.user.id)\n+\n+    def test_error_on_comment_pk_conflict(self):\n+        with self.assertRaises(IntegrityError):\n+            Comment.objects.create(tenant=self.tenant, id=self.comment.id)\n+\n+    @unittest.skipUnless(connection.vendor == \"postgresql\", \"PostgreSQL specific test\")\n+    def test_get_constraints_postgresql(self):\n+        user_constraints = self.get_constraints(User._meta.db_table)\n+        user_pk = user_constraints[\"composite_pk_user_pkey\"]\n+        self.assertEqual(user_pk[\"columns\"], [\"tenant_id\", \"id\"])\n+        self.assertIs(user_pk[\"primary_key\"], True)\n+\n+        comment_constraints = self.get_constraints(Comment._meta.db_table)\n+        comment_pk = comment_constraints[\"composite_pk_comment_pkey\"]\n+        self.assertEqual(comment_pk[\"columns\"], [\"tenant_id\", \"comment_id\"])\n+        self.assertIs(comment_pk[\"primary_key\"], True)\n+\n+    @unittest.skipUnless(connection.vendor == \"sqlite\", \"SQLite specific test\")\n+    def test_get_constraints_sqlite(self):\n+        user_constraints = self.get_constraints(User._meta.db_table)\n+        user_pk = user_constraints[\"__primary__\"]\n+        self.assertEqual(user_pk[\"columns\"], [\"tenant_id\", \"id\"])\n+        self.assertIs(user_pk[\"primary_key\"], True)\n+\n+        comment_constraints = self.get_constraints(Comment._meta.db_table)\n+        comment_pk = comment_constraints[\"__primary__\"]\n+        self.assertEqual(comment_pk[\"columns\"], [\"tenant_id\", \"comment_id\"])\n+        self.assertIs(comment_pk[\"primary_key\"], True)\n+\n+    @unittest.skipUnless(connection.vendor == \"mysql\", \"MySQL specific test\")\n+    def test_get_constraints_mysql(self):\n+        user_constraints = self.get_constraints(User._meta.db_table)\n+        user_pk = user_constraints[\"PRIMARY\"]\n+        self.assertEqual(user_pk[\"columns\"], [\"tenant_id\", \"id\"])\n+        self.assertIs(user_pk[\"primary_key\"], True)\n+\n+        comment_constraints = self.get_constraints(Comment._meta.db_table)\n+        comment_pk = comment_constraints[\"PRIMARY\"]\n+        self.assertEqual(comment_pk[\"columns\"], [\"tenant_id\", \"comment_id\"])\n+        self.assertIs(comment_pk[\"primary_key\"], True)\n+\n+    @unittest.skipUnless(connection.vendor == \"oracle\", \"Oracle specific test\")\n+    def test_get_constraints_oracle(self):\n+        user_constraints = self.get_constraints(User._meta.db_table)\n+        user_pk = next(c for c in user_constraints.values() if c[\"primary_key\"])\n+        self.assertEqual(user_pk[\"columns\"], [\"tenant_id\", \"id\"])\n+        self.assertEqual(user_pk[\"primary_key\"], 1)\n+\n+        comment_constraints = self.get_constraints(Comment._meta.db_table)\n+        comment_pk = next(c for c in comment_constraints.values() if c[\"primary_key\"])\n+        self.assertEqual(comment_pk[\"columns\"], [\"tenant_id\", \"comment_id\"])\n+        self.assertEqual(comment_pk[\"primary_key\"], 1)\n+\n+    def test_in_bulk(self):\n+        \"\"\"\n+        Test the .in_bulk() method of composite_pk models.\n+        \"\"\"\n+        result = Comment.objects.in_bulk()\n+        self.assertEqual(result, {self.comment.pk: self.comment})\n+\n+        result = Comment.objects.in_bulk([self.comment.pk])\n+        self.assertEqual(result, {self.comment.pk: self.comment})\n+\n+    def test_iterator(self):\n+        \"\"\"\n+        Test the .iterator() method of composite_pk models.\n+        \"\"\"\n+        result = list(Comment.objects.iterator())\n+        self.assertEqual(result, [self.comment])\n+\n+    def test_query(self):\n+        users = User.objects.values_list(\"pk\").order_by(\"pk\")\n+        self.assertNotIn('AS \"pk\"', str(users.query))\n+\n+    def test_only(self):\n+        users = User.objects.only(\"pk\")\n+        self.assertSequenceEqual(users, (self.user,))\n+        user = users[0]\n+\n+        with self.assertNumQueries(0):\n+            self.assertEqual(user.pk, (self.user.tenant_id, self.user.id))\n+            self.assertEqual(user.tenant_id, self.user.tenant_id)\n+            self.assertEqual(user.id, self.user.id)\n+        with self.assertNumQueries(1):\n+            self.assertEqual(user.email, self.user.email)\n+\n+    def test_model_forms(self):\n+        fields = [\"tenant\", \"id\", \"user_id\", \"text\"]\n+        self.assertEqual(list(CommentForm.base_fields), fields)\n+\n+        form = modelform_factory(Comment, fields=\"__all__\")\n+        self.assertEqual(list(form().fields), fields)\n+\n+        with self.assertRaisesMessage(\n+            FieldError, \"Unknown field(s) (pk) specified for Comment\"\n+        ):\n+            self.assertIsNone(modelform_factory(Comment, fields=[\"pk\"]))\n+\n+\n+class CompositePKFixturesTests(TestCase):\n+    fixtures = [\"tenant\"]\n+\n+    def test_objects(self):\n+        tenant_1, tenant_2, tenant_3 = Tenant.objects.order_by(\"pk\")\n+        self.assertEqual(tenant_1.id, 1)\n+        self.assertEqual(tenant_1.name, \"Tenant 1\")\n+        self.assertEqual(tenant_2.id, 2)\n+        self.assertEqual(tenant_2.name, \"Tenant 2\")\n+        self.assertEqual(tenant_3.id, 3)\n+        self.assertEqual(tenant_3.name, \"Tenant 3\")\n+\n+        user_1, user_2, user_3, user_4 = User.objects.order_by(\"pk\")\n+        self.assertEqual(user_1.id, 1)\n+        self.assertEqual(user_1.tenant_id, 1)\n+        self.assertEqual(user_1.pk, (user_1.tenant_id, user_1.id))\n+        self.assertEqual(user_1.email, \"user0001@example.com\")\n+        self.assertEqual(user_2.id, 2)\n+        self.assertEqual(user_2.tenant_id, 1)\n+        self.assertEqual(user_2.pk, (user_2.tenant_id, user_2.id))\n+        self.assertEqual(user_2.email, \"user0002@example.com\")\n+        self.assertEqual(user_3.id, 3)\n+        self.assertEqual(user_3.tenant_id, 2)\n+        self.assertEqual(user_3.pk, (user_3.tenant_id, user_3.id))\n+        self.assertEqual(user_3.email, \"user0003@example.com\")\n+        self.assertEqual(user_4.id, 4)\n+        self.assertEqual(user_4.tenant_id, 2)\n+        self.assertEqual(user_4.pk, (user_4.tenant_id, user_4.id))\n+        self.assertEqual(user_4.email, \"user0004@example.com\")\n+\n+        post_1, post_2 = Post.objects.order_by(\"pk\")\n+        self.assertEqual(post_1.id, UUID(\"11111111-1111-1111-1111-111111111111\"))\n+        self.assertEqual(post_1.tenant_id, 2)\n+        self.assertEqual(post_1.pk, (post_1.tenant_id, post_1.id))\n+        self.assertEqual(post_2.id, UUID(\"ffffffff-ffff-ffff-ffff-ffffffffffff\"))\n+        self.assertEqual(post_2.tenant_id, 2)\n+        self.assertEqual(post_2.pk, (post_2.tenant_id, post_2.id))\n+\n+    def test_serialize_user_json(self):\n+        users = User.objects.filter(pk=(1, 1))\n+        result = serializers.serialize(\"json\", users)\n+        self.assertEqual(\n+            json.loads(result),\n+            [\n+                {\n+                    \"model\": \"composite_pk.user\",\n+                    \"pk\": [1, 1],\n+                    \"fields\": {\n+                        \"email\": \"user0001@example.com\",\n+                        \"id\": 1,\n+                        \"tenant\": 1,\n+                    },\n+                }\n+            ],\n+        )\n+\n+    def test_serialize_user_jsonl(self):\n+        users = User.objects.filter(pk=(1, 2))\n+        result = serializers.serialize(\"jsonl\", users)\n+        self.assertEqual(\n+            json.loads(result),\n+            {\n+                \"model\": \"composite_pk.user\",\n+                \"pk\": [1, 2],\n+                \"fields\": {\n+                    \"email\": \"user0002@example.com\",\n+                    \"id\": 2,\n+                    \"tenant\": 1,\n+                },\n+            },\n+        )\n+\n+    def test_serialize_user_yaml(self):\n+        users = User.objects.filter(pk=(2, 3))\n+        result = serializers.serialize(\"yaml\", users)\n+        self.assertEqual(\n+            yaml.safe_load(result),\n+            [\n+                {\n+                    \"model\": \"composite_pk.user\",\n+                    \"pk\": [2, 3],\n+                    \"fields\": {\n+                        \"email\": \"user0003@example.com\",\n+                        \"id\": 3,\n+                        \"tenant\": 2,\n+                    },\n+                },\n+            ],\n+        )\n+\n+    def test_serialize_user_python(self):\n+        users = User.objects.filter(pk=(2, 4))\n+        result = serializers.serialize(\"python\", users)\n+        self.assertEqual(\n+            result,\n+            [\n+                {\n+                    \"model\": \"composite_pk.user\",\n+                    \"pk\": [2, 4],\n+                    \"fields\": {\n+                        \"email\": \"user0004@example.com\",\n+                        \"id\": 4,\n+                        \"tenant\": 2,\n+                    },\n+                },\n+            ],\n+        )\n+\n+    def test_serialize_post_uuid(self):\n+        posts = Post.objects.filter(pk=(2, \"11111111-1111-1111-1111-111111111111\"))\n+        result = serializers.serialize(\"json\", posts)\n+        self.assertEqual(\n+            json.loads(result),\n+            [\n+                {\n+                    \"model\": \"composite_pk.post\",\n+                    \"pk\": [2, \"11111111-1111-1111-1111-111111111111\"],\n+                    \"fields\": {\n+                        \"id\": \"11111111-1111-1111-1111-111111111111\",\n+                        \"tenant\": 2,\n+                    },\n+                },\n+            ],\n+        )\ndiff --git a/tests/migrations/test_autodetector.py b/tests/migrations/test_autodetector.py\nindex d4345208ca43..67d258c7004b 100644\n--- a/tests/migrations/test_autodetector.py\n+++ b/tests/migrations/test_autodetector.py\n@@ -4957,6 +4957,95 @@ def deconstruct(self):\n         self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n         self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Book\")\n \n+    @mock.patch(\n+        \"django.db.migrations.questioner.MigrationQuestioner.ask_not_null_addition\"\n+    )\n+    def test_add_composite_pk(self, mocked_ask_method):\n+        before = [\n+            ModelState(\n+                \"app\",\n+                \"foo\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+        ]\n+        after = [\n+            ModelState(\n+                \"app\",\n+                \"foo\",\n+                [\n+                    (\"pk\", models.CompositePrimaryKey(\"foo_id\", \"bar_id\")),\n+                    (\"id\", models.IntegerField()),\n+                ],\n+            ),\n+        ]\n+\n+        changes = self.get_changes(before, after)\n+        self.assertEqual(mocked_ask_method.call_count, 0)\n+        self.assertNumberMigrations(changes, \"app\", 1)\n+        self.assertOperationTypes(changes, \"app\", 0, [\"AddField\", \"AlterField\"])\n+        self.assertOperationAttributes(\n+            changes,\n+            \"app\",\n+            0,\n+            0,\n+            name=\"pk\",\n+            model_name=\"foo\",\n+            preserve_default=True,\n+        )\n+        self.assertOperationAttributes(\n+            changes,\n+            \"app\",\n+            0,\n+            1,\n+            name=\"id\",\n+            model_name=\"foo\",\n+            preserve_default=True,\n+        )\n+\n+    def test_remove_composite_pk(self):\n+        before = [\n+            ModelState(\n+                \"app\",\n+                \"foo\",\n+                [\n+                    (\"pk\", models.CompositePrimaryKey(\"foo_id\", \"bar_id\")),\n+                    (\"id\", models.IntegerField()),\n+                ],\n+            ),\n+        ]\n+        after = [\n+            ModelState(\n+                \"app\",\n+                \"foo\",\n+                [\n+                    (\"id\", models.AutoField(primary_key=True)),\n+                ],\n+            ),\n+        ]\n+\n+        changes = self.get_changes(before, after)\n+        self.assertNumberMigrations(changes, \"app\", 1)\n+        self.assertOperationTypes(changes, \"app\", 0, [\"RemoveField\", \"AlterField\"])\n+        self.assertOperationAttributes(\n+            changes,\n+            \"app\",\n+            0,\n+            0,\n+            name=\"pk\",\n+            model_name=\"foo\",\n+        )\n+        self.assertOperationAttributes(\n+            changes,\n+            \"app\",\n+            0,\n+            1,\n+            name=\"id\",\n+            model_name=\"foo\",\n+            preserve_default=True,\n+        )\n+\n \n class MigrationSuggestNameTests(SimpleTestCase):\n     def test_no_operations(self):\ndiff --git a/tests/migrations/test_operations.py b/tests/migrations/test_operations.py\nindex 3ac813b899b7..d12533d3ac33 100644\n--- a/tests/migrations/test_operations.py\n+++ b/tests/migrations/test_operations.py\n@@ -6212,6 +6212,61 @@ def test_add_field_after_generated_field(self):\n         self.assertEqual(pony_new.generated, 1)\n         self.assertEqual(pony_new.static, 2)\n \n+    def test_composite_pk_operations(self):\n+        app_label = \"test_d8d90af6\"\n+        project_state = self.set_up_test_model(app_label)\n+        operation_1 = migrations.AddField(\n+            \"Pony\", \"pk\", models.CompositePrimaryKey(\"id\", \"pink\")\n+        )\n+        operation_2 = migrations.AlterField(\"Pony\", \"id\", models.IntegerField())\n+        operation_3 = migrations.RemoveField(\"Pony\", \"pk\")\n+        table_name = f\"{app_label}_pony\"\n+\n+        # 1. Add field (pk).\n+        new_state = project_state.clone()\n+        operation_1.state_forwards(app_label, new_state)\n+        with connection.schema_editor() as editor:\n+            operation_1.database_forwards(app_label, editor, project_state, new_state)\n+        self.assertColumnNotExists(table_name, \"pk\")\n+        Pony = new_state.apps.get_model(app_label, \"pony\")\n+        obj_1 = Pony.objects.create(weight=1)\n+        msg = (\n+            f\"obj_1={obj_1}, \"\n+            f\"obj_1.id={obj_1.id}, \"\n+            f\"obj_1.pink={obj_1.pink}, \"\n+            f\"obj_1.pk={obj_1.pk}, \"\n+            f\"Pony._meta.pk={repr(Pony._meta.pk)}, \"\n+            f\"Pony._meta.get_field('id')={repr(Pony._meta.get_field('id'))}\"\n+        )\n+        self.assertEqual(obj_1.pink, 3, msg)\n+        self.assertEqual(obj_1.pk, (obj_1.id, obj_1.pink), msg)\n+\n+        # 2. Alter field (id -> IntegerField()).\n+        project_state, new_state = new_state, new_state.clone()\n+        operation_2.state_forwards(app_label, new_state)\n+        with connection.schema_editor() as editor:\n+            operation_2.database_forwards(app_label, editor, project_state, new_state)\n+        Pony = new_state.apps.get_model(app_label, \"pony\")\n+        obj_1 = Pony.objects.get(id=obj_1.id)\n+        self.assertEqual(obj_1.pink, 3)\n+        self.assertEqual(obj_1.pk, (obj_1.id, obj_1.pink))\n+        obj_2 = Pony.objects.create(id=2, weight=2)\n+        self.assertEqual(obj_2.id, 2)\n+        self.assertEqual(obj_2.pink, 3)\n+        self.assertEqual(obj_2.pk, (obj_2.id, obj_2.pink))\n+\n+        # 3. Remove field (pk).\n+        project_state, new_state = new_state, new_state.clone()\n+        operation_3.state_forwards(app_label, new_state)\n+        with connection.schema_editor() as editor:\n+            operation_3.database_forwards(app_label, editor, project_state, new_state)\n+        Pony = new_state.apps.get_model(app_label, \"pony\")\n+        obj_1 = Pony.objects.get(id=obj_1.id)\n+        self.assertEqual(obj_1.pk, obj_1.id)\n+        obj_2 = Pony.objects.get(id=obj_2.id)\n+        self.assertEqual(obj_2.id, 2)\n+        self.assertEqual(obj_2.pk, obj_2.id)\n+\n \n class SwappableOperationTests(OperationTestBase):\n     \"\"\"\ndiff --git a/tests/migrations/test_state.py b/tests/migrations/test_state.py\nindex dbbdf777345c..d6ecaa1c5d0a 100644\n--- a/tests/migrations/test_state.py\n+++ b/tests/migrations/test_state.py\n@@ -1206,6 +1206,28 @@ class Meta:\n         choices_field = Author._meta.get_field(\"choice\")\n         self.assertEqual(list(choices_field.choices), choices)\n \n+    def test_composite_pk_state(self):\n+        new_apps = Apps([\"migrations\"])\n+\n+        class Foo(models.Model):\n+            pk = models.CompositePrimaryKey(\"account_id\", \"id\")\n+            account_id = models.SmallIntegerField()\n+            id = models.SmallIntegerField()\n+\n+            class Meta:\n+                app_label = \"migrations\"\n+                apps = new_apps\n+\n+        project_state = ProjectState.from_apps(new_apps)\n+        model_state = project_state.models[\"migrations\", \"foo\"]\n+        self.assertEqual(len(model_state.options), 2)\n+        self.assertEqual(model_state.options[\"constraints\"], [])\n+        self.assertEqual(model_state.options[\"indexes\"], [])\n+        self.assertEqual(len(model_state.fields), 3)\n+        self.assertIn(\"pk\", model_state.fields)\n+        self.assertIn(\"account_id\", model_state.fields)\n+        self.assertIn(\"id\", model_state.fields)\n+\n \n class StateRelationsTests(SimpleTestCase):\n     def get_base_project_state(self):\ndiff --git a/tests/migrations/test_writer.py b/tests/migrations/test_writer.py\nindex 51783b73465a..953a3cdb6cf9 100644\n--- a/tests/migrations/test_writer.py\n+++ b/tests/migrations/test_writer.py\n@@ -1138,3 +1138,22 @@ def test_register_non_serializer(self):\n             ValueError, \"'TestModel1' must inherit from 'BaseSerializer'.\"\n         ):\n             MigrationWriter.register_serializer(complex, TestModel1)\n+\n+    def test_composite_pk_import(self):\n+        migration = type(\n+            \"Migration\",\n+            (migrations.Migration,),\n+            {\n+                \"operations\": [\n+                    migrations.AddField(\n+                        \"foo\",\n+                        \"bar\",\n+                        models.CompositePrimaryKey(\"foo_id\", \"bar_id\"),\n+                    ),\n+                ],\n+            },\n+        )\n+        writer = MigrationWriter(migration)\n+        output = writer.as_string()\n+        self.assertEqual(output.count(\"import\"), 1)\n+        self.assertIn(\"from django.db import migrations, models\", output)\n"
  },
  {
    "index": 19,
    "filtered_comments": [
      "I ran into this issue using the code from the tutorial:\n\n```\n$ python manage.py shell\nTraceback (most recent call last):\n  File \"manage.py\", line 10, in <module>\n    execute_from_command_line(sys.argv)\n  File \"/home/tim/code/django/django/core/management/__init__.py\", line 336, in execute_from_command_line\n    utility.execute()\n  File \"/home/tim/code/django/django/core/management/__init__.py\", line 310, in execute\n    django.setup()\n  File \"/home/tim/code/django/django/__init__.py\", line 23, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File \"/home/tim/code/django/django/apps/registry.py\", line 115, in populate\n    app_config.ready()\n  File \"/home/tim/code/django/django/contrib/admin/apps.py\", line 22, in ready\n    self.module.autodiscover()\n  File \"/home/tim/code/django/django/contrib/admin/__init__.py\", line 24, in autodiscover\n    autodiscover_modules('admin', register_to=site)\n  File \"/home/tim/code/django/django/utils/module_loading.py\", line 73, in autodiscover_modules\n    import_module('%s.%s' % (app_config.name, module_to_search))\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/home/tim/code/django/django/contrib/auth/admin.py\", line 182, in <module>\n    admin.site.register(Group, GroupAdmin)\n  File \"/home/tim/code/django/django/contrib/admin/sites.py\", line 101, in register\n    admin_class.check(model)\n  File \"/home/tim/code/django/django/contrib/admin/options.py\", line 149, in check\n    return cls.checks_class().check(cls, model, **kwargs)\n  File \"/home/tim/code/django/django/contrib/admin/checks.py\", line 492, in check\n    errors = super(ModelAdminChecks, self).check(cls, model=model, **kwargs)\n  File \"/home/tim/code/django/django/contrib/admin/checks.py\", line 32, in check\n    errors.extend(self._check_filter_horizontal(cls, model))\n  File \"/home/tim/code/django/django/contrib/admin/checks.py\", line 245, in _check_filter_horizontal\n    for index, field_name in enumerate(cls.filter_horizontal)\n  File \"/home/tim/code/django/django/contrib/admin/checks.py\", line 253, in _check_filter_item\n    field = model._meta.get_field(field_name)\n  File \"/home/tim/code/django/django/db/models/options.py\", line 434, in get_field\n    \"The Apps registry is still not ready, this means get_field() is not able \"\ndjango.core.exceptions.AppRegistryNotReady: The Apps registry is still not ready, this means get_field() is not able to find related objects that point to this model.\n```\n",
      "There is still some usage of `get_field_by_name()` and other deprecated APIs in the tests. Run the tests with `python -Wall runtests.py` and ensure there are no errors.\n\nThere are also a fair number of flake8 errors -- some appear not related to your changes, but rather like you haven't merged in some commits from master. I think you could probably rebase and squash most of the commits now.\n",
      "@timgraham \nRE \"I ran into this issue using the code from the tutorial: ....\"\nTotally right, I added a fix for it, currently running unit tests. It looks like some admin checks are happening prior to the apps registry being ready. This should never happen actually, so I added a fix for it.\nI'll let you know once all tests pass with -Wall\n",
      "There are still many flake8 errors on your branch aren't there? This is what I see:\n\n```\n./django/db/models/manager.py:6:1: F401 'FieldDoesNotExist' imported but unused\n./django/db/models/options.py:13:1: F401 'Field' imported but unused\n./django/db/models/options.py:500:17: E126 continuation line over-indented for hanging indent\n./django/db/models/base.py:1420:9: F401 'FieldDoesNotExist' imported but unused\n./django/db/models/fields/__init__.py:45:1: E302 expected 2 blank lines, found 1\n./django/contrib/contenttypes/fields.py:41:15: W291 trailing whitespace\n./django/contrib/admin/utils.py:462:1: E302 expected 2 blank lines, found 1\n./django/contrib/admin/utils.py:481:1: E302 expected 2 blank lines, found 1\n./tests/prefetch_related/tests.py:723:45: E127 continuation line over-indented for visual indent\n./tests/apps/tests.py:18:1: F401 'AbstractPerson' imported but unused\n./tests/apps/tests.py:18:1: F401 'BasePerson' imported but unused\n./tests/apps/tests.py:18:1: F401 'Relation' imported but unused\n./tests/apps/tests.py:18:1: F401 'new_apps_2' imported but unused\n./tests/test_client_regress/tests.py:997:31: E127 continuation line over-indented for visual indent\n./tests/introspection/tests.py:133:18: E127 continuation line over-indented for visual indent\n```\n",
      "@timgraham To me it looks like the admin checks should be triggered from `AdminAppConfig.ready()`.\n",
      "# Further API change\n\n### Properties changes\n- many_to_many becomes _many_to_many and is only used internally, as there should be no more external distinction between m2m and forward fields\n- fields, concrete_fields, local_concrete_fields become all internal, (with a _ before and not documented) , as there should be no more external distinction between m2m and forward fields\n- related_objects become reverse_fields, in order to keep the same name convention\n- we add another property called \"forward_fields\"\n- make get_fields() internal, but we don't change the endpoint name for legacy reasons (there was already a get_fields())\n\n### Final _meta API\n- field_names => [\"name\", \"surname\", ...]\n- get_field(field_name) => FieldInstance\n- forward_fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n- reverse_fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n\n### Final internal _meta API\n- _fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n- _concrete_fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n- _local_concrete_fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n- _many_to_many => [FieldInstance, FieldInstance, FieldInstance, .. ]\n",
      "If we make it a public API, I am in favor of moving it to `core.exceptions` (and keeping backwards compatibility where it is now)\n",
      "\"We can move it there and then alias it back on db.models.fields\"\n\nYes, that sounds like the right thing to do.\n\nOptionally we _could_ have the `db.models.fields` version be pushed into the pending deprecation state, but I don't much mind either way on that.\n",
      "A quick question on API correctness:\n`opts.field_names` API can also return more than 1 name for each field, this usually happens with ForeignKeys, where fields can be fetched by property or property_id.\n\nThis is an example where `manager` is a ForeignKey: `{u'id', 'item', 'manager', u'manager_id', 'name'}`\n\nDo you think this is the correct way to go? or shall we exclude duplicates from `field_names`?\n",
      "Gut reaction: I'd certainly expect it to only return the canonical attribute names, and not the `_id` variants.\n\nSo long as the API gives enough information for users to be able to derive the \"_id\" style ones if needed then that would seem sufficient.\n",
      "@tomchristie interestingly Django also uses the *_id stuff internally. I suggest we keep the possibility of Django fetching fields by *_id using get_field(), but we remove duplicates in field_names\n",
      "@PirosB3 What's the hold-up in using `_meta.fields` as the main (and only) entry point? Is that backward compatibility because `fields` doesn't have \"fake\" fields like reverse relations?\n\nIf that's the case I think we have here a unique opportunity to get it right and it's easy enough to provide an upgrade path.\n"
    ],
    "code_diff": "diff --git a/django/apps/registry.py b/django/apps/registry.py\nindex fe53d965de10..68aa411d91f6 100644\n--- a/django/apps/registry.py\n+++ b/django/apps/registry.py\n@@ -337,7 +337,12 @@ def clear_cache(self):\n \n         This is mostly used in tests.\n         \"\"\"\n+        # Call expire cache on each model. This will purge\n+        # the relation tree and the fields cache.\n         self.get_models.cache_clear()\n+        if self.ready:\n+            for model in self.get_models(include_auto_created=True):\n+                model._meta._expire_cache()\n \n     ### DEPRECATED METHODS GO BELOW THIS LINE ###\n \ndiff --git a/django/contrib/admin/checks.py b/django/contrib/admin/checks.py\nindex ee47f911b177..896d59155c3c 100644\n--- a/django/contrib/admin/checks.py\n+++ b/django/contrib/admin/checks.py\n@@ -762,7 +762,7 @@ def _check_list_editable(self, cls, model):\n \n     def _check_list_editable_item(self, cls, model, field_name, label):\n         try:\n-            field = model._meta.get_field_by_name(field_name)[0]\n+            field = model._meta.get_field(field_name)\n         except FieldDoesNotExist:\n             return refer_to_missing_field(field=field_name, option=label,\n                                           model=model, obj=cls, id='admin.E121')\ndiff --git a/django/contrib/admin/options.py b/django/contrib/admin/options.py\nindex 35758cfdaf7b..3e682f9b1021 100644\n--- a/django/contrib/admin/options.py\n+++ b/django/contrib/admin/options.py\n@@ -406,7 +406,7 @@ def lookup_allowed(self, lookup, value):\n         rel_name = None\n         for part in parts[:-1]:\n             try:\n-                field, _, _, _ = model._meta.get_field_by_name(part)\n+                field = model._meta.get_field(part)\n             except FieldDoesNotExist:\n                 # Lookups on non-existent fields are ok, since they're ignored\n                 # later.\n@@ -422,7 +422,7 @@ def lookup_allowed(self, lookup, value):\n                 else:\n                     rel_name = None\n             elif isinstance(field, ForeignObjectRel):\n-                model = field.model\n+                model = field.related_model\n                 rel_name = model._meta.pk.name\n             else:\n                 rel_name = None\n@@ -473,9 +473,12 @@ def to_field_allowed(self, request, to_field):\n             for inline in admin.inlines:\n                 registered_models.add(inline.model)\n \n-        for related_object in (opts.get_all_related_objects(include_hidden=True) +\n-                               opts.get_all_related_many_to_many_objects()):\n-            related_model = related_object.model\n+        related_objects = (\n+            f for f in opts.get_fields(include_hidden=True)\n+            if (f.auto_created and not f.concrete)\n+        )\n+        for related_object in related_objects:\n+            related_model = related_object.related_model\n             if (any(issubclass(model, related_model) for model in registered_models) and\n                     related_object.field.rel.get_related_field() == field):\n                 return True\ndiff --git a/django/contrib/admin/templatetags/admin_list.py b/django/contrib/admin/templatetags/admin_list.py\nindex 544c5503ec70..266b880d9947 100644\n--- a/django/contrib/admin/templatetags/admin_list.py\n+++ b/django/contrib/admin/templatetags/admin_list.py\n@@ -326,7 +326,7 @@ def date_hierarchy(cl):\n     \"\"\"\n     if cl.date_hierarchy:\n         field_name = cl.date_hierarchy\n-        field = cl.opts.get_field_by_name(field_name)[0]\n+        field = cl.opts.get_field(field_name)\n         dates_or_datetimes = 'datetimes' if isinstance(field, models.DateTimeField) else 'dates'\n         year_field = '%s__year' % field_name\n         month_field = '%s__month' % field_name\ndiff --git a/django/contrib/admin/utils.py b/django/contrib/admin/utils.py\nindex 5da461351a71..0f617b3fb964 100644\n--- a/django/contrib/admin/utils.py\n+++ b/django/contrib/admin/utils.py\n@@ -25,7 +25,7 @@ def lookup_needs_distinct(opts, lookup_path):\n     Returns True if 'distinct()' should be used to query the given lookup path.\n     \"\"\"\n     field_name = lookup_path.split('__', 1)[0]\n-    field = opts.get_field_by_name(field_name)[0]\n+    field = opts.get_field(field_name)\n     if hasattr(field, 'get_path_info') and any(path.m2m for path in field.get_path_info()):\n         return True\n     return False\n@@ -265,7 +265,7 @@ def model_ngettext(obj, n=None):\n def lookup_field(name, obj, model_admin=None):\n     opts = obj._meta\n     try:\n-        f = opts.get_field(name)\n+        f = _get_non_gfk_field(opts, name)\n     except FieldDoesNotExist:\n         # For non-field values, the value is either a method, property or\n         # returned via a callable.\n@@ -291,6 +291,17 @@ def lookup_field(name, obj, model_admin=None):\n     return f, attr, value\n \n \n+def _get_non_gfk_field(opts, name):\n+    \"\"\"\n+    For historical reasons, the admin app relies on GenericForeignKeys as being\n+    \"not found\" by get_field(). This could likely be cleaned up.\n+    \"\"\"\n+    field = opts.get_field(name)\n+    if field.is_relation and field.one_to_many and not field.related_model:\n+        raise FieldDoesNotExist()\n+    return field\n+\n+\n def label_for_field(name, model, model_admin=None, return_attr=False):\n     \"\"\"\n     Returns a sensible label for a field name. The name can be a callable,\n@@ -301,7 +312,7 @@ def label_for_field(name, model, model_admin=None, return_attr=False):\n     \"\"\"\n     attr = None\n     try:\n-        field = model._meta.get_field_by_name(name)[0]\n+        field = _get_non_gfk_field(model._meta, name)\n         try:\n             label = field.verbose_name\n         except AttributeError:\n@@ -349,11 +360,10 @@ def label_for_field(name, model, model_admin=None, return_attr=False):\n def help_text_for_field(name, model):\n     help_text = \"\"\n     try:\n-        field_data = model._meta.get_field_by_name(name)\n+        field = _get_non_gfk_field(model._meta, name)\n     except FieldDoesNotExist:\n         pass\n     else:\n-        field = field_data[0]\n         if hasattr(field, 'help_text'):\n             help_text = field.help_text\n     return smart_text(help_text)\n@@ -425,19 +435,21 @@ def reverse_field_path(model, path):\n     parent = model\n     pieces = path.split(LOOKUP_SEP)\n     for piece in pieces:\n-        field, model, direct, m2m = parent._meta.get_field_by_name(piece)\n+        field = parent._meta.get_field(piece)\n         # skip trailing data field if extant:\n         if len(reversed_path) == len(pieces) - 1:  # final iteration\n             try:\n                 get_model_from_relation(field)\n             except NotRelationField:\n                 break\n-        if direct:\n+\n+        # Field should point to another model\n+        if field.is_relation and not (field.auto_created and not field.concrete):\n             related_name = field.related_query_name()\n             parent = field.rel.to\n         else:\n             related_name = field.field.name\n-            parent = field.model\n+            parent = field.related_model\n         reversed_path.insert(0, related_name)\n     return (parent, LOOKUP_SEP.join(reversed_path))\n \n@@ -458,7 +470,7 @@ def get_fields_from_path(model, path):\n             parent = get_model_from_relation(fields[-1])\n         else:\n             parent = model\n-        fields.append(parent._meta.get_field_by_name(piece)[0])\n+        fields.append(parent._meta.get_field(piece))\n     return fields\n \n \ndiff --git a/django/contrib/admin/validation.py b/django/contrib/admin/validation.py\nindex 92ede613ec72..67b97f776edd 100644\n--- a/django/contrib/admin/validation.py\n+++ b/django/contrib/admin/validation.py\n@@ -346,7 +346,7 @@ def validate_list_editable(self, cls, model):\n             check_isseq(cls, 'list_editable', cls.list_editable)\n             for idx, field_name in enumerate(cls.list_editable):\n                 try:\n-                    field = model._meta.get_field_by_name(field_name)[0]\n+                    field = model._meta.get_field(field_name)\n                 except FieldDoesNotExist:\n                     raise ImproperlyConfigured(\"'%s.list_editable[%d]' refers to a \"\n                         \"field, '%s', not defined on %s.%s.\"\ndiff --git a/django/contrib/admindocs/views.py b/django/contrib/admindocs/views.py\nindex 2b45301f3a41..2ffd402c95d0 100644\n--- a/django/contrib/admindocs/views.py\n+++ b/django/contrib/admindocs/views.py\n@@ -262,7 +262,7 @@ def get_context_data(self, **kwargs):\n                 })\n \n         # Gather related objects\n-        for rel in opts.get_all_related_objects() + opts.get_all_related_many_to_many_objects():\n+        for rel in opts.related_objects:\n             verbose = _(\"related `%(app_label)s.%(object_name)s` objects\") % {\n                 'app_label': rel.opts.app_label,\n                 'object_name': rel.opts.object_name,\ndiff --git a/django/contrib/contenttypes/fields.py b/django/contrib/contenttypes/fields.py\nindex 2297ad63a9b5..07d47becf86e 100644\n--- a/django/contrib/contenttypes/fields.py\n+++ b/django/contrib/contenttypes/fields.py\n@@ -21,6 +21,18 @@ class GenericForeignKey(object):\n     Provides a generic relation to any object through content-type/object-id\n     fields.\n     \"\"\"\n+    # Field flags\n+    auto_created = False\n+    concrete = False\n+    editable = False\n+    hidden = False\n+\n+    is_relation = True\n+    many_to_many = False\n+    many_to_one = False\n+    one_to_many = True\n+    one_to_one = False\n+    related_model = None\n \n     def __init__(self, ct_field=\"content_type\", fk_field=\"object_id\", for_concrete_model=True):\n         self.ct_field = ct_field\n@@ -28,12 +40,13 @@ def __init__(self, ct_field=\"content_type\", fk_field=\"object_id\", for_concrete_m\n         self.for_concrete_model = for_concrete_model\n         self.editable = False\n         self.rel = None\n+        self.column = None\n \n     def contribute_to_class(self, cls, name, **kwargs):\n         self.name = name\n         self.model = cls\n         self.cache_attr = \"_%s_cache\" % name\n-        cls._meta.add_virtual_field(self)\n+        cls._meta.add_field(self, virtual=True)\n \n         # Only run pre-initialization field assignment on non-abstract models\n         if not cls._meta.abstract:\n@@ -243,6 +256,13 @@ def __set__(self, instance, value):\n \n class GenericRelation(ForeignObject):\n     \"\"\"Provides an accessor to generic related objects (e.g. comments)\"\"\"\n+    # Field flags\n+    auto_created = False\n+\n+    many_to_many = False\n+    many_to_one = True\n+    one_to_many = False\n+    one_to_one = False\n \n     def __init__(self, to, **kwargs):\n         kwargs['verbose_name'] = kwargs.get('verbose_name', None)\n@@ -303,8 +323,7 @@ def _check_generic_foreign_key_existence(self):\n \n     def resolve_related_fields(self):\n         self.to_fields = [self.model._meta.pk.name]\n-        return [(self.rel.to._meta.get_field_by_name(self.object_id_field_name)[0],\n-                 self.model._meta.pk)]\n+        return [(self.rel.to._meta.get_field(self.object_id_field_name), self.model._meta.pk)]\n \n     def get_path_info(self):\n         opts = self.rel.to._meta\n@@ -345,7 +364,7 @@ def get_content_type(self):\n                                                  for_concrete_model=self.for_concrete_model)\n \n     def get_extra_restriction(self, where_class, alias, remote_alias):\n-        field = self.rel.to._meta.get_field_by_name(self.content_type_field_name)[0]\n+        field = self.rel.to._meta.get_field(self.content_type_field_name)\n         contenttype_pk = self.get_content_type().pk\n         cond = where_class()\n         lookup = field.get_lookup('exact')(Col(remote_alias, field, field), contenttype_pk)\ndiff --git a/django/contrib/gis/db/models/query.py b/django/contrib/gis/db/models/query.py\nindex f2e76578508b..c05688dec9bc 100644\n--- a/django/contrib/gis/db/models/query.py\n+++ b/django/contrib/gis/db/models/query.py\n@@ -758,7 +758,7 @@ def _geocol_select(self, geo_field, field_name):\n         elif geo_field not in opts.local_fields:\n             # This geographic field is inherited from another model, so we have to\n             # use the db table for the _parent_ model instead.\n-            tmp_fld, parent_model, direct, m2m = opts.get_field_by_name(geo_field.name)\n+            parent_model = geo_field.model._meta.concrete_model\n             return self.query.get_compiler(self.db)._field_column(geo_field, parent_model._meta.db_table)\n         else:\n             return self.query.get_compiler(self.db)._field_column(geo_field)\ndiff --git a/django/contrib/gis/db/models/sql/compiler.py b/django/contrib/gis/db/models/sql/compiler.py\nindex dd156ea4b621..1501c981369c 100644\n--- a/django/contrib/gis/db/models/sql/compiler.py\n+++ b/django/contrib/gis/db/models/sql/compiler.py\n@@ -118,7 +118,10 @@ def get_default_columns(self, with_aliases=False, col_aliases=None,\n         seen = self.query.included_inherited_models.copy()\n         if start_alias:\n             seen[None] = start_alias\n-        for field, model in opts.get_concrete_fields_with_model():\n+        for field in opts.concrete_fields:\n+            model = field.model._meta.concrete_model\n+            if model is opts.model:\n+                model = None\n             if from_parent and model is not None and issubclass(from_parent, model):\n                 # Avoid loading data for already loaded parents.\n                 continue\ndiff --git a/django/contrib/gis/sitemaps/views.py b/django/contrib/gis/sitemaps/views.py\nindex c0c2f835983d..d12ed53298ee 100644\n--- a/django/contrib/gis/sitemaps/views.py\n+++ b/django/contrib/gis/sitemaps/views.py\n@@ -23,7 +23,7 @@ def kml(request, label, model, field_name=None, compress=False, using=DEFAULT_DB\n \n     if field_name:\n         try:\n-            field, _, _, _ = klass._meta.get_field_by_name(field_name)\n+            field = klass._meta.get_field(field_name)\n             if not isinstance(field, GeometryField):\n                 raise FieldDoesNotExist\n         except FieldDoesNotExist:\ndiff --git a/django/contrib/gis/utils/layermapping.py b/django/contrib/gis/utils/layermapping.py\nindex 1c848f105ef6..2ff23fc38be0 100644\n--- a/django/contrib/gis/utils/layermapping.py\n+++ b/django/contrib/gis/utils/layermapping.py\n@@ -457,11 +457,10 @@ def coord_transform(self):\n \n     def geometry_field(self):\n         \"Returns the GeometryField instance associated with the geographic column.\"\n-        # Use the `get_field_by_name` on the model's options so that we\n+        # Use `get_field()` on the model's options so that we\n         # get the correct field instance if there's model inheritance.\n         opts = self.model._meta\n-        fld, model, direct, m2m = opts.get_field_by_name(self.geom_field)\n-        return fld\n+        return opts.get_field(self.geom_field)\n \n     def make_multi(self, geom_type, model_field):\n         \"\"\"\ndiff --git a/django/contrib/gis/utils/srs.py b/django/contrib/gis/utils/srs.py\nindex e5aa5a703956..1460be2de90c 100644\n--- a/django/contrib/gis/utils/srs.py\n+++ b/django/contrib/gis/utils/srs.py\n@@ -61,7 +61,7 @@ def add_srs_entry(srs, auth_name='EPSG', auth_srid=None, ref_sys_name=None,\n               }\n \n     # Backend-specific fields for the SpatialRefSys model.\n-    srs_field_names = SpatialRefSys._meta.get_all_field_names()\n+    srs_field_names = {f.name for f in SpatialRefSys._meta.get_fields()}\n     if 'srtext' in srs_field_names:\n         kwargs['srtext'] = srs.wkt\n     if 'ref_sys_name' in srs_field_names:\ndiff --git a/django/core/serializers/python.py b/django/core/serializers/python.py\nindex b4712c76be52..f8dd7aebac51 100644\n--- a/django/core/serializers/python.py\n+++ b/django/core/serializers/python.py\n@@ -8,7 +8,7 @@\n from django.apps import apps\n from django.conf import settings\n from django.core.serializers import base\n-from django.db import models, DEFAULT_DB_ALIAS\n+from django.db import DEFAULT_DB_ALIAS, models\n from django.utils.encoding import force_text, is_protected_type\n from django.utils import six\n \n@@ -101,12 +101,12 @@ def Deserializer(object_list, **options):\n         if 'pk' in d:\n             data[Model._meta.pk.attname] = Model._meta.pk.to_python(d.get(\"pk\", None))\n         m2m_data = {}\n-        model_fields = Model._meta.get_all_field_names()\n+        field_names = {f.name for f in Model._meta.get_fields()}\n \n         # Handle each field\n         for (field_name, field_value) in six.iteritems(d[\"fields\"]):\n \n-            if ignore and field_name not in model_fields:\n+            if ignore and field_name not in field_names:\n                 # skip fields no longer on model\n                 continue\n \ndiff --git a/django/core/serializers/xml_serializer.py b/django/core/serializers/xml_serializer.py\nindex de82a969c17e..0b759799d2ea 100644\n--- a/django/core/serializers/xml_serializer.py\n+++ b/django/core/serializers/xml_serializer.py\n@@ -186,7 +186,7 @@ def _handle_object(self, node):\n         # {m2m_accessor_attribute : [list_of_related_objects]})\n         m2m_data = {}\n \n-        model_fields = Model._meta.get_all_field_names()\n+        field_names = {f.name for f in Model._meta.get_fields()}\n         # Deserialize each field.\n         for field_node in node.getElementsByTagName(\"field\"):\n             # If the field is missing the name attribute, bail (are you\n@@ -198,7 +198,7 @@ def _handle_object(self, node):\n             # Get the field from the Model. This will raise a\n             # FieldDoesNotExist if, well, the field doesn't exist, which will\n             # be propagated correctly unless ignorenonexistent=True is used.\n-            if self.ignore and field_name not in model_fields:\n+            if self.ignore and field_name not in field_names:\n                 continue\n             field = Model._meta.get_field(field_name)\n \ndiff --git a/django/db/backends/creation.py b/django/db/backends/creation.py\nindex 5e0248d4df27..42a58c03e85c 100644\n--- a/django/db/backends/creation.py\n+++ b/django/db/backends/creation.py\n@@ -199,7 +199,7 @@ def sql_indexes_for_model(self, model, style):\n         for f in model._meta.local_fields:\n             output.extend(self.sql_indexes_for_field(model, f, style))\n         for fs in model._meta.index_together:\n-            fields = [model._meta.get_field_by_name(f)[0] for f in fs]\n+            fields = [model._meta.get_field(f) for f in fs]\n             output.extend(self.sql_indexes_for_fields(model, fields, style))\n         return output\n \n@@ -290,7 +290,7 @@ def sql_destroy_indexes_for_model(self, model, style):\n         for f in model._meta.local_fields:\n             output.extend(self.sql_destroy_indexes_for_field(model, f, style))\n         for fs in model._meta.index_together:\n-            fields = [model._meta.get_field_by_name(f)[0] for f in fs]\n+            fields = [model._meta.get_field(f) for f in fs]\n             output.extend(self.sql_destroy_indexes_for_fields(model, fields, style))\n         return output\n \ndiff --git a/django/db/backends/schema.py b/django/db/backends/schema.py\nindex 40db888e5016..dfda05bc19bb 100644\n--- a/django/db/backends/schema.py\n+++ b/django/db/backends/schema.py\n@@ -10,6 +10,11 @@\n logger = getLogger('django.db.backends.schema')\n \n \n+def _related_non_m2m_objects(opts):\n+    # filters out m2m objects from reverse relations.\n+    return (obj for obj in opts.related_objects if not obj.field.many_to_many)\n+\n+\n class BaseDatabaseSchemaEditor(object):\n     \"\"\"\n     This class (and its subclasses) are responsible for emitting schema-changing\n@@ -261,7 +266,7 @@ def create_model(self, model):\n \n         # Add any unique_togethers\n         for fields in model._meta.unique_together:\n-            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            columns = [model._meta.get_field(field).column for field in fields]\n             column_sqls.append(self.sql_create_table_unique % {\n                 \"columns\": \", \".join(self.quote_name(column) for column in columns),\n             })\n@@ -309,7 +314,7 @@ def alter_unique_together(self, model, old_unique_together, new_unique_together)\n         news = set(tuple(fields) for fields in new_unique_together)\n         # Deleted uniques\n         for fields in olds.difference(news):\n-            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            columns = [model._meta.get_field(field).column for field in fields]\n             constraint_names = self._constraint_names(model, columns, unique=True)\n             if len(constraint_names) != 1:\n                 raise ValueError(\"Found wrong number (%s) of constraints for %s(%s)\" % (\n@@ -320,7 +325,7 @@ def alter_unique_together(self, model, old_unique_together, new_unique_together)\n             self.execute(self._delete_constraint_sql(self.sql_delete_unique, model, constraint_names[0]))\n         # Created uniques\n         for fields in news.difference(olds):\n-            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            columns = [model._meta.get_field(field).column for field in fields]\n             self.execute(self._create_unique_sql(model, columns))\n \n     def alter_index_together(self, model, old_index_together, new_index_together):\n@@ -333,7 +338,7 @@ def alter_index_together(self, model, old_index_together, new_index_together):\n         news = set(tuple(fields) for fields in new_index_together)\n         # Deleted indexes\n         for fields in olds.difference(news):\n-            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            columns = [model._meta.get_field(field).column for field in fields]\n             constraint_names = self._constraint_names(model, list(columns), index=True)\n             if len(constraint_names) != 1:\n                 raise ValueError(\"Found wrong number (%s) of constraints for %s(%s)\" % (\n@@ -344,7 +349,7 @@ def alter_index_together(self, model, old_index_together, new_index_together):\n             self.execute(self._delete_constraint_sql(self.sql_delete_index, model, constraint_names[0]))\n         # Created indexes\n         for field_names in news.difference(olds):\n-            fields = [model._meta.get_field_by_name(field)[0] for field in field_names]\n+            fields = [model._meta.get_field(field) for field in field_names]\n             self.execute(self._create_index_sql(model, fields, suffix=\"_idx\"))\n \n     def alter_db_table(self, model, old_db_table, new_db_table):\n@@ -511,10 +516,12 @@ def _alter_field(self, model, old_field, new_field, old_type, new_type,\n         # Drop incoming FK constraints if we're a primary key and things are going\n         # to change.\n         if old_field.primary_key and new_field.primary_key and old_type != new_type:\n-            for rel in new_field.model._meta.get_all_related_objects():\n-                rel_fk_names = self._constraint_names(rel.model, [rel.field.column], foreign_key=True)\n+            # '_meta.related_field' also contains M2M reverse fields, these\n+            # will be filtered out\n+            for rel in _related_non_m2m_objects(new_field.model._meta):\n+                rel_fk_names = self._constraint_names(rel.related_model, [rel.field.column], foreign_key=True)\n                 for fk_name in rel_fk_names:\n-                    self.execute(self._delete_constraint_sql(self.sql_delete_fk, rel.model, fk_name))\n+                    self.execute(self._delete_constraint_sql(self.sql_delete_fk, rel.related_model, fk_name))\n         # Removed an index? (no strict check, as multiple indexes are possible)\n         if (old_field.db_index and not new_field.db_index and\n                 not old_field.unique and not\n@@ -661,7 +668,7 @@ def _alter_field(self, model, old_field, new_field, old_type, new_type,\n         # referring to us.\n         rels_to_update = []\n         if old_field.primary_key and new_field.primary_key and old_type != new_type:\n-            rels_to_update.extend(new_field.model._meta.get_all_related_objects())\n+            rels_to_update.extend(_related_non_m2m_objects(new_field.model._meta))\n         # Changed to become primary key?\n         # Note that we don't detect unsetting of a PK, as we assume another field\n         # will always come along and replace it.\n@@ -684,14 +691,14 @@ def _alter_field(self, model, old_field, new_field, old_type, new_type,\n                 }\n             )\n             # Update all referencing columns\n-            rels_to_update.extend(new_field.model._meta.get_all_related_objects())\n+            rels_to_update.extend(_related_non_m2m_objects(new_field.model._meta))\n         # Handle our type alters on the other end of rels from the PK stuff above\n         for rel in rels_to_update:\n             rel_db_params = rel.field.db_parameters(connection=self.connection)\n             rel_type = rel_db_params['type']\n             self.execute(\n                 self.sql_alter_column % {\n-                    \"table\": self.quote_name(rel.model._meta.db_table),\n+                    \"table\": self.quote_name(rel.related_model._meta.db_table),\n                     \"changes\": self.sql_alter_column_type % {\n                         \"column\": self.quote_name(rel.field.column),\n                         \"type\": rel_type,\n@@ -705,8 +712,9 @@ def _alter_field(self, model, old_field, new_field, old_type, new_type,\n             self.execute(self._create_fk_sql(model, new_field, \"_fk_%(to_table)s_%(to_column)s\"))\n         # Rebuild FKs that pointed to us if we previously had to drop them\n         if old_field.primary_key and new_field.primary_key and old_type != new_type:\n-            for rel in new_field.model._meta.get_all_related_objects():\n-                self.execute(self._create_fk_sql(rel.model, rel.field, \"_fk\"))\n+            for rel in new_field.model._meta.related_objects:\n+                if not rel.many_to_many:\n+                    self.execute(self._create_fk_sql(rel.related_model, rel.field, \"_fk\"))\n         # Does it have check constraints we need to add?\n         if old_db_params['check'] != new_db_params['check'] and new_db_params['check']:\n             self.execute(\n@@ -765,14 +773,14 @@ def _alter_many_to_many(self, model, old_field, new_field, strict):\n             new_field.rel.through,\n             # We need the field that points to the target model, so we can tell alter_field to change it -\n             # this is m2m_reverse_field_name() (as opposed to m2m_field_name, which points to our model)\n-            old_field.rel.through._meta.get_field_by_name(old_field.m2m_reverse_field_name())[0],\n-            new_field.rel.through._meta.get_field_by_name(new_field.m2m_reverse_field_name())[0],\n+            old_field.rel.through._meta.get_field(old_field.m2m_reverse_field_name()),\n+            new_field.rel.through._meta.get_field(new_field.m2m_reverse_field_name()),\n         )\n         self.alter_field(\n             new_field.rel.through,\n             # for self-referential models we need to alter field from the other end too\n-            old_field.rel.through._meta.get_field_by_name(old_field.m2m_field_name())[0],\n-            new_field.rel.through._meta.get_field_by_name(new_field.m2m_field_name())[0],\n+            old_field.rel.through._meta.get_field(old_field.m2m_field_name()),\n+            new_field.rel.through._meta.get_field(new_field.m2m_field_name()),\n         )\n \n     def _create_index_name(self, model, column_names, suffix=\"\"):\n@@ -844,7 +852,7 @@ def _model_indexes_sql(self, model):\n                 output.append(self._create_index_sql(model, [field], suffix=\"\"))\n \n         for field_names in model._meta.index_together:\n-            fields = [model._meta.get_field_by_name(field)[0] for field in field_names]\n+            fields = [model._meta.get_field(field) for field in field_names]\n             output.append(self._create_index_sql(model, fields, suffix=\"_idx\"))\n         return output\n \ndiff --git a/django/db/backends/sqlite3/schema.py b/django/db/backends/sqlite3/schema.py\nindex 9aeca56bf731..e0433b0c13b1 100644\n--- a/django/db/backends/sqlite3/schema.py\n+++ b/django/db/backends/sqlite3/schema.py\n@@ -227,8 +227,8 @@ def _alter_many_to_many(self, model, old_field, new_field, strict):\n                 alter_fields=[(\n                     # We need the field that points to the target model, so we can tell alter_field to change it -\n                     # this is m2m_reverse_field_name() (as opposed to m2m_field_name, which points to our model)\n-                    old_field.rel.through._meta.get_field_by_name(old_field.m2m_reverse_field_name())[0],\n-                    new_field.rel.through._meta.get_field_by_name(new_field.m2m_reverse_field_name())[0],\n+                    old_field.rel.through._meta.get_field(old_field.m2m_reverse_field_name()),\n+                    new_field.rel.through._meta.get_field(new_field.m2m_reverse_field_name()),\n                 )],\n                 override_uniques=(new_field.m2m_field_name(), new_field.m2m_reverse_field_name()),\n             )\ndiff --git a/django/db/migrations/autodetector.py b/django/db/migrations/autodetector.py\nindex e3dd87e3bd2b..8535a98e7357 100644\n--- a/django/db/migrations/autodetector.py\n+++ b/django/db/migrations/autodetector.py\n@@ -163,7 +163,7 @@ def _detect_changes(self, convert_apps=None, graph=None):\n             old_model_name = self.renamed_models.get((app_label, model_name), model_name)\n             old_model_state = self.from_state.models[app_label, old_model_name]\n             for field_name, field in old_model_state.fields:\n-                old_field = self.old_apps.get_model(app_label, old_model_name)._meta.get_field_by_name(field_name)[0]\n+                old_field = self.old_apps.get_model(app_label, old_model_name)._meta.get_field(field_name)\n                 if (hasattr(old_field, \"rel\") and getattr(old_field.rel, \"through\", None)\n                         and not old_field.rel.through._meta.auto_created):\n                     through_key = (\n@@ -685,26 +685,14 @@ def generate_deleted_models(self):\n             # and the removal of all its own related fields, and if it's\n             # a through model the field that references it.\n             dependencies = []\n-            for related_object in model._meta.get_all_related_objects():\n-                dependencies.append((\n-                    related_object.model._meta.app_label,\n-                    related_object.model._meta.object_name,\n-                    related_object.field.name,\n-                    False,\n-                ))\n-                dependencies.append((\n-                    related_object.model._meta.app_label,\n-                    related_object.model._meta.object_name,\n-                    related_object.field.name,\n-                    \"alter\",\n-                ))\n-            for related_object in model._meta.get_all_related_many_to_many_objects():\n-                dependencies.append((\n-                    related_object.model._meta.app_label,\n-                    related_object.model._meta.object_name,\n-                    related_object.field.name,\n-                    False,\n-                ))\n+            for related_object in model._meta.related_objects:\n+                related_object_app_label = related_object.related_model._meta.app_label\n+                object_name = related_object.related_model._meta.object_name\n+                field_name = related_object.field.name\n+                dependencies.append((related_object_app_label, object_name, field_name, False))\n+                if not related_object.many_to_many:\n+                    dependencies.append((related_object_app_label, object_name, field_name, \"alter\"))\n+\n             for name, field in sorted(related_fields.items()):\n                 dependencies.append((app_label, model_name, name, False))\n             # We're referenced in another field's through=\n@@ -743,7 +731,7 @@ def generate_renamed_fields(self):\n         for app_label, model_name, field_name in sorted(self.new_field_keys - self.old_field_keys):\n             old_model_name = self.renamed_models.get((app_label, model_name), model_name)\n             old_model_state = self.from_state.models[app_label, old_model_name]\n-            field = self.new_apps.get_model(app_label, model_name)._meta.get_field_by_name(field_name)[0]\n+            field = self.new_apps.get_model(app_label, model_name)._meta.get_field(field_name)\n             # Scan to see if this is actually a rename!\n             field_dec = self.deep_deconstruct(field)\n             for rem_app_label, rem_model_name, rem_field_name in sorted(self.old_field_keys - self.new_field_keys):\n@@ -776,7 +764,7 @@ def generate_added_fields(self):\n             self._generate_added_field(app_label, model_name, field_name)\n \n     def _generate_added_field(self, app_label, model_name, field_name):\n-        field = self.new_apps.get_model(app_label, model_name)._meta.get_field_by_name(field_name)[0]\n+        field = self.new_apps.get_model(app_label, model_name)._meta.get_field(field_name)\n         # Fields that are foreignkeys/m2ms depend on stuff\n         dependencies = []\n         if field.rel and field.rel.to:\n@@ -847,8 +835,8 @@ def generate_altered_fields(self):\n             # Did the field change?\n             old_model_name = self.renamed_models.get((app_label, model_name), model_name)\n             old_field_name = self.renamed_fields.get((app_label, model_name, field_name), field_name)\n-            old_field = self.old_apps.get_model(app_label, old_model_name)._meta.get_field_by_name(old_field_name)[0]\n-            new_field = self.new_apps.get_model(app_label, model_name)._meta.get_field_by_name(field_name)[0]\n+            old_field = self.old_apps.get_model(app_label, old_model_name)._meta.get_field(old_field_name)\n+            new_field = self.new_apps.get_model(app_label, model_name)._meta.get_field(field_name)\n             # Implement any model renames on relations; these are handled by RenameModel\n             # so we need to exclude them from the comparison\n             if hasattr(new_field, \"rel\") and getattr(new_field.rel, \"to\", None):\ndiff --git a/django/db/migrations/operations/fields.py b/django/db/migrations/operations/fields.py\nindex 54251cf6edc6..6ce5e372969d 100644\n--- a/django/db/migrations/operations/fields.py\n+++ b/django/db/migrations/operations/fields.py\n@@ -44,7 +44,7 @@ def database_forwards(self, app_label, schema_editor, from_state, to_state):\n         to_model = to_state.apps.get_model(app_label, self.model_name)\n         if self.allowed_to_migrate(schema_editor.connection.alias, to_model):\n             from_model = from_state.apps.get_model(app_label, self.model_name)\n-            field = to_model._meta.get_field_by_name(self.name)[0]\n+            field = to_model._meta.get_field(self.name)\n             if not self.preserve_default:\n                 field.default = self.field.default\n             schema_editor.add_field(\n@@ -57,7 +57,7 @@ def database_forwards(self, app_label, schema_editor, from_state, to_state):\n     def database_backwards(self, app_label, schema_editor, from_state, to_state):\n         from_model = from_state.apps.get_model(app_label, self.model_name)\n         if self.allowed_to_migrate(schema_editor.connection.alias, from_model):\n-            schema_editor.remove_field(from_model, from_model._meta.get_field_by_name(self.name)[0])\n+            schema_editor.remove_field(from_model, from_model._meta.get_field(self.name))\n \n     def describe(self):\n         return \"Add field %s to %s\" % (self.name, self.model_name)\n@@ -100,13 +100,13 @@ def state_forwards(self, app_label, state):\n     def database_forwards(self, app_label, schema_editor, from_state, to_state):\n         from_model = from_state.apps.get_model(app_label, self.model_name)\n         if self.allowed_to_migrate(schema_editor.connection.alias, from_model):\n-            schema_editor.remove_field(from_model, from_model._meta.get_field_by_name(self.name)[0])\n+            schema_editor.remove_field(from_model, from_model._meta.get_field(self.name))\n \n     def database_backwards(self, app_label, schema_editor, from_state, to_state):\n         to_model = to_state.apps.get_model(app_label, self.model_name)\n         if self.allowed_to_migrate(schema_editor.connection.alias, to_model):\n             from_model = from_state.apps.get_model(app_label, self.model_name)\n-            schema_editor.add_field(from_model, to_model._meta.get_field_by_name(self.name)[0])\n+            schema_editor.add_field(from_model, to_model._meta.get_field(self.name))\n \n     def describe(self):\n         return \"Remove field %s from %s\" % (self.name, self.model_name)\n@@ -158,8 +158,8 @@ def database_forwards(self, app_label, schema_editor, from_state, to_state):\n         to_model = to_state.apps.get_model(app_label, self.model_name)\n         if self.allowed_to_migrate(schema_editor.connection.alias, to_model):\n             from_model = from_state.apps.get_model(app_label, self.model_name)\n-            from_field = from_model._meta.get_field_by_name(self.name)[0]\n-            to_field = to_model._meta.get_field_by_name(self.name)[0]\n+            from_field = from_model._meta.get_field(self.name)\n+            to_field = to_model._meta.get_field(self.name)\n             # If the field is a relatedfield with an unresolved rel.to, just\n             # set it equal to the other field side. Bandaid fix for AlterField\n             # migrations that are part of a RenameModel change.\n@@ -231,8 +231,8 @@ def database_forwards(self, app_label, schema_editor, from_state, to_state):\n             from_model = from_state.apps.get_model(app_label, self.model_name)\n             schema_editor.alter_field(\n                 from_model,\n-                from_model._meta.get_field_by_name(self.old_name)[0],\n-                to_model._meta.get_field_by_name(self.new_name)[0],\n+                from_model._meta.get_field(self.old_name),\n+                to_model._meta.get_field(self.new_name),\n             )\n \n     def database_backwards(self, app_label, schema_editor, from_state, to_state):\n@@ -241,8 +241,8 @@ def database_backwards(self, app_label, schema_editor, from_state, to_state):\n             from_model = from_state.apps.get_model(app_label, self.model_name)\n             schema_editor.alter_field(\n                 from_model,\n-                from_model._meta.get_field_by_name(self.new_name)[0],\n-                to_model._meta.get_field_by_name(self.old_name)[0],\n+                from_model._meta.get_field(self.new_name),\n+                to_model._meta.get_field(self.old_name),\n             )\n \n     def describe(self):\ndiff --git a/django/db/migrations/operations/models.py b/django/db/migrations/operations/models.py\nindex f07f667c51f3..6dd66ae4541b 100644\n--- a/django/db/migrations/operations/models.py\n+++ b/django/db/migrations/operations/models.py\n@@ -138,25 +138,27 @@ def deconstruct(self):\n         )\n \n     def state_forwards(self, app_label, state):\n-        # Get all of the related objects we need to repoint\n         apps = state.apps\n         model = apps.get_model(app_label, self.old_name)\n         model._meta.apps = apps\n-        related_objects = model._meta.get_all_related_objects()\n-        related_m2m_objects = model._meta.get_all_related_many_to_many_objects()\n+        # Get all of the related objects we need to repoint\n+        all_related_objects = (\n+            f for f in model._meta.get_fields(include_hidden=True)\n+            if f.auto_created and not f.concrete and not (f.hidden or f.many_to_many)\n+        )\n         # Rename the model\n         state.models[app_label, self.new_name.lower()] = state.models[app_label, self.old_name.lower()]\n         state.models[app_label, self.new_name.lower()].name = self.new_name\n         state.remove_model(app_label, self.old_name)\n         # Repoint the FKs and M2Ms pointing to us\n-        for related_object in (related_objects + related_m2m_objects):\n+        for related_object in all_related_objects:\n             # Use the new related key for self referential related objects.\n-            if related_object.model == model:\n+            if related_object.related_model == model:\n                 related_key = (app_label, self.new_name.lower())\n             else:\n                 related_key = (\n-                    related_object.model._meta.app_label,\n-                    related_object.model._meta.object_name.lower(),\n+                    related_object.related_model._meta.app_label,\n+                    related_object.related_model._meta.object_name.lower(),\n                 )\n             new_fields = []\n             for name, field in state.models[related_key].fields:\n@@ -179,21 +181,19 @@ def database_forwards(self, app_label, schema_editor, from_state, to_state):\n                 new_model._meta.db_table,\n             )\n             # Alter the fields pointing to us\n-            related_objects = old_model._meta.get_all_related_objects()\n-            related_m2m_objects = old_model._meta.get_all_related_many_to_many_objects()\n-            for related_object in (related_objects + related_m2m_objects):\n-                if related_object.model == old_model:\n+            for related_object in old_model._meta.related_objects:\n+                if related_object.related_model == old_model:\n                     model = new_model\n                     related_key = (app_label, self.new_name.lower())\n                 else:\n-                    model = related_object.model\n+                    model = related_object.related_model\n                     related_key = (\n-                        related_object.model._meta.app_label,\n-                        related_object.model._meta.object_name.lower(),\n+                        related_object.related_model._meta.app_label,\n+                        related_object.related_model._meta.object_name.lower(),\n                     )\n                 to_field = to_state.apps.get_model(\n                     *related_key\n-                )._meta.get_field_by_name(related_object.field.name)[0]\n+                )._meta.get_field(related_object.field.name)\n                 schema_editor.alter_field(\n                     model,\n                     related_object.field,\n@@ -394,11 +394,11 @@ def database_forwards(self, app_label, schema_editor, from_state, to_state):\n             from_model = from_state.apps.get_model(app_label, self.name)\n             # Remove a field if we need to\n             if from_model._meta.order_with_respect_to and not to_model._meta.order_with_respect_to:\n-                schema_editor.remove_field(from_model, from_model._meta.get_field_by_name(\"_order\")[0])\n+                schema_editor.remove_field(from_model, from_model._meta.get_field(\"_order\"))\n             # Add a field if we need to (altering the column is untouched as\n             # it's likely a rename)\n             elif to_model._meta.order_with_respect_to and not from_model._meta.order_with_respect_to:\n-                field = to_model._meta.get_field_by_name(\"_order\")[0]\n+                field = to_model._meta.get_field(\"_order\")\n                 if not field.has_default():\n                     field.default = 0\n                 schema_editor.add_field(\ndiff --git a/django/db/migrations/state.py b/django/db/migrations/state.py\nindex 8a45a0d2f299..6626e31442ca 100644\n--- a/django/db/migrations/state.py\n+++ b/django/db/migrations/state.py\n@@ -50,15 +50,15 @@ def reload_model(self, app_label, model_name):\n             model_name = model_name.lower()\n             try:\n                 related_old = {\n-                    f.model for f in\n-                    self.apps.get_model(app_label, model_name)._meta.get_all_related_objects()\n+                    f.related_model for f in\n+                    self.apps.get_model(app_label, model_name)._meta.related_objects\n                 }\n             except LookupError:\n                 related_old = set()\n             self._reload_one_model(app_label, model_name)\n             # Reload models if there are relations\n             model = self.apps.get_model(app_label, model_name)\n-            related_m2m = {f.rel.to for f, _ in model._meta.get_m2m_with_model()}\n+            related_m2m = {f.related_model for f in model._meta.many_to_many}\n             for rel_model in related_old.union(related_m2m):\n                 self._reload_one_model(rel_model._meta.app_label, rel_model._meta.model_name)\n             if related_m2m:\ndiff --git a/django/db/models/base.py b/django/db/models/base.py\nindex ec5d4c7c1c39..945cd0154b79 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -2,6 +2,7 @@\n \n import copy\n import inspect\n+from itertools import chain\n import sys\n import warnings\n \n@@ -175,12 +176,12 @@ def __new__(cls, name, bases, attrs):\n             new_class.add_to_class(obj_name, obj)\n \n         # All the fields of any type declared on this model\n-        new_fields = (\n-            new_class._meta.local_fields +\n-            new_class._meta.local_many_to_many +\n+        new_fields = chain(\n+            new_class._meta.local_fields,\n+            new_class._meta.local_many_to_many,\n             new_class._meta.virtual_fields\n         )\n-        field_names = set(f.name for f in new_fields)\n+        field_names = {f.name for f in new_fields}\n \n         # Basic setup for proxy models.\n         if is_proxy:\n@@ -202,6 +203,7 @@ def __new__(cls, name, bases, attrs):\n                 raise TypeError(\"Proxy model '%s' has no non-abstract model base class.\" % name)\n             new_class._meta.setup_proxy(base)\n             new_class._meta.concrete_model = base._meta.concrete_model\n+            base._meta.concrete_model._meta.proxied_children.append(new_class._meta)\n         else:\n             new_class._meta.concrete_model = new_class\n \n@@ -342,7 +344,7 @@ def make_foreign_order_accessors(field, model, cls):\n \n         # Give the class a docstring -- its definition.\n         if cls.__doc__ is None:\n-            cls.__doc__ = \"%s(%s)\" % (cls.__name__, \", \".join(f.attname for f in opts.fields))\n+            cls.__doc__ = \"%s(%s)\" % (cls.__name__, \", \".join(f.name for f in opts.fields))\n \n         get_absolute_url_override = settings.ABSOLUTE_URL_OVERRIDES.get(\n             '%s.%s' % (opts.app_label, opts.model_name)\n@@ -630,7 +632,7 @@ def serializable_value(self, field_name):\n         and not use this method.\n         \"\"\"\n         try:\n-            field = self._meta.get_field_by_name(field_name)[0]\n+            field = self._meta.get_field(field_name)\n         except FieldDoesNotExist:\n             return getattr(self, field_name)\n         return getattr(self, field.attname)\n@@ -1438,12 +1440,17 @@ def _check_unique_together(cls):\n     def _check_local_fields(cls, fields, option):\n         from django.db import models\n \n+        # In order to avoid hitting the relation tree prematurely, we use our\n+        # own fields_map instead of using get_field()\n+        forward_fields_map = {\n+            field.name: field for field in cls._meta._get_fields(reverse=False)\n+        }\n+\n         errors = []\n         for field_name in fields:\n             try:\n-                field = cls._meta.get_field(field_name,\n-                    many_to_many=True)\n-            except FieldDoesNotExist:\n+                field = forward_fields_map[field_name]\n+            except KeyError:\n                 errors.append(\n                     checks.Error(\n                         \"'%s' refers to the non-existent field '%s'.\" % (option, field_name),\n@@ -1484,7 +1491,6 @@ def _check_local_fields(cls, fields, option):\n     def _check_ordering(cls):\n         \"\"\" Check \"ordering\" option -- is it a list of strings and do all fields\n         exist? \"\"\"\n-\n         if not cls._meta.ordering:\n             return []\n \n@@ -1500,7 +1506,6 @@ def _check_ordering(cls):\n             ]\n \n         errors = []\n-\n         fields = cls._meta.ordering\n \n         # Skip '?' fields.\n@@ -1518,28 +1523,30 @@ def _check_ordering(cls):\n \n         # Skip ordering on pk. This is always a valid order_by field\n         # but is an alias and therefore won't be found by opts.get_field.\n-        fields = (f for f in fields if f != 'pk')\n+        fields = {f for f in fields if f != 'pk'}\n \n-        for field_name in fields:\n-            try:\n-                cls._meta.get_field(field_name, many_to_many=False)\n-            except FieldDoesNotExist:\n-                if field_name.endswith('_id'):\n-                    try:\n-                        field = cls._meta.get_field(field_name[:-3], many_to_many=False)\n-                    except FieldDoesNotExist:\n-                        pass\n-                    else:\n-                        if field.attname == field_name:\n-                            continue\n-                errors.append(\n-                    checks.Error(\n-                        \"'ordering' refers to the non-existent field '%s'.\" % field_name,\n-                        hint=None,\n-                        obj=cls,\n-                        id='models.E015',\n-                    )\n+        # Check for invalid or non-existent fields in ordering.\n+        invalid_fields = []\n+\n+        # Any field name that is not present in field_names does not exist.\n+        # Also, ordering by m2m fields is not allowed.\n+        opts = cls._meta\n+        valid_fields = set(chain.from_iterable(\n+            (f.name, f.attname) if not (f.auto_created and not f.concrete) else (f.field.related_query_name(),)\n+            for f in chain(opts.fields, opts.related_objects)\n+        ))\n+\n+        invalid_fields.extend(fields - valid_fields)\n+\n+        for invalid_field in invalid_fields:\n+            errors.append(\n+                checks.Error(\n+                    \"'ordering' refers to the non-existent field '%s'.\" % invalid_field,\n+                    hint=None,\n+                    obj=cls,\n+                    id='models.E015',\n                 )\n+            )\n         return errors\n \n     @classmethod\ndiff --git a/django/db/models/deletion.py b/django/db/models/deletion.py\nindex c61f865be2c0..016fc5637e7c 100644\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -1,4 +1,5 @@\n from collections import OrderedDict\n+from itertools import chain\n from operator import attrgetter\n \n from django.db import connections, transaction, IntegrityError\n@@ -51,6 +52,23 @@ def DO_NOTHING(collector, field, sub_objs, using):\n     pass\n \n \n+def get_candidate_relations_to_delete(opts):\n+    # Collect models that contain candidate relations to delete. This may include\n+    # relations coming from proxy models.\n+    candidate_models = {opts}\n+    candidate_models = candidate_models.union(opts.concrete_model._meta.proxied_children)\n+    # For each model, get all candidate fields.\n+    candidate_model_fields = chain.from_iterable(\n+        opts.get_fields(include_hidden=True) for opts in candidate_models\n+    )\n+    # The candidate relations are the ones that come from N-1 and 1-1 relations.\n+    # N-N  (i.e., many-to-many) relations aren't candidates for deletion.\n+    return (\n+        f for f in candidate_model_fields\n+        if f.auto_created and not f.concrete and (f.one_to_one or f.many_to_one)\n+    )\n+\n+\n class Collector(object):\n     def __init__(self, using):\n         self.using = using\n@@ -134,8 +152,7 @@ def can_fast_delete(self, objs, from_field=None):\n             return False\n         # Foreign keys pointing to this model, both from m2m and other\n         # models.\n-        for related in opts.get_all_related_objects(\n-                include_hidden=True, include_proxy_eq=True):\n+        for related in get_candidate_relations_to_delete(opts):\n             if related.field.rel.on_delete is not DO_NOTHING:\n                 return False\n         for field in model._meta.virtual_fields:\n@@ -184,7 +201,7 @@ def collect(self, objs, source=None, nullable=False, collect_related=True,\n         model = new_objs[0].__class__\n \n         # Recursively collect concrete model's parent models, but not their\n-        # related objects. These will be found by meta.get_all_related_objects()\n+        # related objects. These will be found by meta.get_fields()\n         concrete_model = model._meta.concrete_model\n         for ptr in six.itervalues(concrete_model._meta.parents):\n             if ptr:\n@@ -199,8 +216,7 @@ def collect(self, objs, source=None, nullable=False, collect_related=True,\n                              reverse_dependency=True)\n \n         if collect_related:\n-            for related in model._meta.get_all_related_objects(\n-                    include_hidden=True, include_proxy_eq=True):\n+            for related in get_candidate_relations_to_delete(model._meta):\n                 field = related.field\n                 if field.rel.on_delete == DO_NOTHING:\n                     continue\n@@ -225,7 +241,7 @@ def related_objects(self, related, objs):\n         Gets a QuerySet of objects related to ``objs`` via the relation ``related``.\n \n         \"\"\"\n-        return related.model._base_manager.using(self.using).filter(\n+        return related.related_model._base_manager.using(self.using).filter(\n             **{\"%s__in\" % related.field.name: objs}\n         )\n \ndiff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex 8dc9c554ddc2..a1995452a3c5 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -31,7 +31,9 @@\n from django.utils import six\n from django.utils.itercompat import is_iterable\n \n-# imported for backwards compatibility\n+# When the _meta object was formalized, this exception was moved to\n+# django.core.exceptions. It is retained here for backwards compatibility\n+# purposes.\n from django.core.exceptions import FieldDoesNotExist  # NOQA\n \n # Avoid \"TypeError: Item in ``from list'' not a string\" -- unicode_literals\n@@ -61,7 +63,7 @@ class NOT_PROVIDED:\n \n \n def _load_field(app_label, model_name, field_name):\n-    return apps.get_model(app_label, model_name)._meta.get_field_by_name(field_name)[0]\n+    return apps.get_model(app_label, model_name)._meta.get_field(field_name)\n \n \n # A guide to Field parameters:\n@@ -116,6 +118,15 @@ class Field(RegisterLookupMixin):\n     system_check_deprecated_details = None\n     system_check_removed_details = None\n \n+    # Field flags\n+    hidden = False\n+\n+    many_to_many = None\n+    many_to_one = None\n+    one_to_many = None\n+    one_to_one = None\n+    related_model = None\n+\n     # Generic field type description, usually overridden by subclasses\n     def _description(self):\n         return _('Field of type: %(field_type)s') % {\n@@ -137,6 +148,7 @@ def __init__(self, verbose_name=None, name=None, primary_key=False,\n         self.max_length, self._unique = max_length, unique\n         self.blank, self.null = blank, null\n         self.rel = rel\n+        self.is_relation = self.rel is not None\n         self.default = default\n         self.editable = editable\n         self.serialize = serialize\n@@ -603,6 +615,7 @@ def set_attributes_from_name(self, name):\n         if not self.name:\n             self.name = name\n         self.attname, self.column = self.get_attname_column()\n+        self.concrete = self.column is not None\n         if self.verbose_name is None and self.name:\n             self.verbose_name = self.name.replace('_', ' ')\n \n@@ -610,7 +623,7 @@ def contribute_to_class(self, cls, name, virtual_only=False):\n         self.set_attributes_from_name(name)\n         self.model = cls\n         if virtual_only:\n-            cls._meta.add_virtual_field(self)\n+            cls._meta.add_field(self, virtual=True)\n         else:\n             cls._meta.add_field(self)\n         if self.choices:\ndiff --git a/django/db/models/fields/related.py b/django/db/models/fields/related.py\nindex 2e05e57e5145..9ef6c1350a4d 100644\n--- a/django/db/models/fields/related.py\n+++ b/django/db/models/fields/related.py\n@@ -98,6 +98,18 @@ def do_pending_lookups(sender, **kwargs):\n \n \n class RelatedField(Field):\n+    # Field flags\n+    one_to_many = False\n+    one_to_one = False\n+    many_to_many = False\n+    many_to_one = False\n+\n+    @cached_property\n+    def related_model(self):\n+        # Can't cache this property until all the models are loaded.\n+        apps.check_models_ready()\n+        return self.rel.to\n+\n     def check(self, **kwargs):\n         errors = super(RelatedField, self).check(**kwargs)\n         errors.extend(self._check_related_name_is_valid())\n@@ -235,13 +247,10 @@ def _check_clashes(self):\n         # Check clashes between accessors/reverse query names of `field` and\n         # any other field accessor -- i. e. Model.foreign accessor clashes with\n         # Model.m2m accessor.\n-        potential_clashes = rel_opts.get_all_related_many_to_many_objects()\n-        potential_clashes += rel_opts.get_all_related_objects()\n-        potential_clashes = (r for r in potential_clashes\n-            if r.field is not self)\n+        potential_clashes = (r for r in rel_opts.related_objects if r.field is not self)\n         for clash_field in potential_clashes:\n             clash_name = \"%s.%s\" % (  # i. e. \"Model.m2m\"\n-                clash_field.model._meta.object_name,\n+                clash_field.related_model._meta.object_name,\n                 clash_field.field.name)\n             if clash_field.get_accessor_name() == rel_name:\n                 errors.append(\n@@ -392,7 +401,7 @@ def RelatedObjectDoesNotExist(self):\n         # consistency with `ReverseSingleRelatedObjectDescriptor`.\n         return type(\n             str('RelatedObjectDoesNotExist'),\n-            (self.related.model.DoesNotExist, AttributeError),\n+            (self.related.related_model.DoesNotExist, AttributeError),\n             {}\n         )\n \n@@ -400,11 +409,11 @@ def is_cached(self, instance):\n         return hasattr(instance, self.cache_name)\n \n     def get_queryset(self, **hints):\n-        manager = self.related.model._default_manager\n+        manager = self.related.related_model._default_manager\n         # If the related manager indicates that it should be used for\n         # related fields, respect that.\n         if not getattr(manager, 'use_for_related_fields', False):\n-            manager = self.related.model._base_manager\n+            manager = self.related.related_model._base_manager\n         return manager.db_manager(hints=hints).all()\n \n     def get_prefetch_queryset(self, instances, queryset=None):\n@@ -441,7 +450,7 @@ def __get__(self, instance, instance_type=None):\n                     params['%s__%s' % (self.related.field.name, rh_field.name)] = getattr(instance, rh_field.attname)\n                 try:\n                     rel_obj = self.get_queryset(instance=instance).get(**params)\n-                except self.related.model.DoesNotExist:\n+                except self.related.related_model.DoesNotExist:\n                     rel_obj = None\n                 else:\n                     setattr(rel_obj, self.related.field.get_cache_name(), instance)\n@@ -470,7 +479,7 @@ def __set__(self, instance, value):\n                     self.related.get_accessor_name(),\n                 )\n             )\n-        elif value is not None and not isinstance(value, self.related.model):\n+        elif value is not None and not isinstance(value, self.related.related_model):\n             raise ValueError(\n                 'Cannot assign \"%r\": \"%s.%s\" must be a \"%s\" instance.' % (\n                     value,\n@@ -825,9 +834,9 @@ def related_manager_cls(self):\n         # Dynamically create a class that subclasses the related model's default\n         # manager.\n         return create_foreign_related_manager(\n-            self.related.model._default_manager.__class__,\n+            self.related.related_model._default_manager.__class__,\n             self.related.field,\n-            self.related.model,\n+            self.related.related_model,\n         )\n \n \n@@ -1148,7 +1157,7 @@ def related_manager_cls(self):\n         # Dynamically create a class that subclasses the related\n         # model's default manager.\n         return create_many_related_manager(\n-            self.related.model._default_manager.__class__,\n+            self.related.related_model._default_manager.__class__,\n             self.related.field.rel\n         )\n \n@@ -1156,7 +1165,7 @@ def __get__(self, instance, instance_type=None):\n         if instance is None:\n             return self\n \n-        rel_model = self.related.model\n+        rel_model = self.related.related_model\n \n         manager = self.related_manager_cls(\n             model=rel_model,\n@@ -1255,6 +1264,12 @@ def __set__(self, instance, value):\n \n \n class ForeignObjectRel(object):\n+    # Field flags\n+    auto_created = True\n+    concrete = False\n+    editable = False\n+    is_relation = True\n+\n     def __init__(self, field, to, related_name=None, limit_choices_to=None,\n                  parent_link=False, on_delete=None, related_query_name=None):\n         self.field = field\n@@ -1267,32 +1282,55 @@ def __init__(self, field, to, related_name=None, limit_choices_to=None,\n         self.on_delete = on_delete\n         self.symmetrical = False\n \n-    # This and the following cached_properties can't be initialized in\n+    # Some of the following cached_properties can't be initialized in\n     # __init__ as the field doesn't have its model yet. Calling these methods\n     # before field.contribute_to_class() has been called will result in\n     # AttributeError\n     @cached_property\n     def model(self):\n-        if not self.field.model:\n-            raise AttributeError(\n-                \"This property can't be accessed before self.field.contribute_to_class has been called.\")\n-        return self.field.model\n+        return self.to\n \n     @cached_property\n     def opts(self):\n-        return self.model._meta\n+        return self.related_model._meta\n \n     @cached_property\n     def to_opts(self):\n         return self.to._meta\n \n     @cached_property\n-    def parent_model(self):\n-        return self.to\n+    def hidden(self):\n+        return self.is_hidden()\n \n     @cached_property\n     def name(self):\n-        return '%s.%s' % (self.opts.app_label, self.opts.model_name)\n+        return self.field.related_query_name()\n+\n+    @cached_property\n+    def related_model(self):\n+        if not self.field.model:\n+            raise AttributeError(\n+                \"This property can't be accessed before self.field.contribute_to_class has been called.\")\n+        return self.field.model\n+\n+    @cached_property\n+    def many_to_many(self):\n+        return self.field.many_to_many\n+\n+    @cached_property\n+    def many_to_one(self):\n+        return self.field.one_to_many\n+\n+    @cached_property\n+    def one_to_many(self):\n+        return self.field.many_to_one\n+\n+    @cached_property\n+    def one_to_one(self):\n+        return self.field.one_to_one\n+\n+    def __repr__(self):\n+        return '<%s: %s.%s>' % (type(self).__name__, self.opts.app_label, self.opts.model_name)\n \n     def get_choices(self, include_blank=True, blank_choice=BLANK_CHOICE_DASH,\n                     limit_to_currently_related=False):\n@@ -1304,10 +1342,10 @@ def get_choices(self, include_blank=True, blank_choice=BLANK_CHOICE_DASH,\n         initially for utilization by RelatedFieldListFilter.\n         \"\"\"\n         first_choice = blank_choice if include_blank else []\n-        queryset = self.model._default_manager.all()\n+        queryset = self.related_model._default_manager.all()\n         if limit_to_currently_related:\n             queryset = queryset.complex_filter(\n-                {'%s__isnull' % self.parent_model._meta.model_name: False}\n+                {'%s__isnull' % self.related_model._meta.model_name: False}\n             )\n         lst = [(x._get_pk_val(), smart_text(x)) for x in queryset]\n         return first_choice + lst\n@@ -1318,7 +1356,7 @@ def get_db_prep_lookup(self, lookup_type, value, connection, prepared=False):\n \n     def is_hidden(self):\n         \"Should the related object be hidden?\"\n-        return self.related_name and self.related_name[-1] == '+'\n+        return self.related_name is not None and self.related_name[-1] == '+'\n \n     def get_joining_columns(self):\n         return self.field.get_reverse_joining_columns()\n@@ -1349,7 +1387,7 @@ def get_accessor_name(self, model=None):\n         # Due to backwards compatibility ModelForms need to be able to provide\n         # an alternate model. See BaseInlineFormSet.get_default_prefix().\n         opts = model._meta if model else self.opts\n-        model = model or self.model\n+        model = model or self.related_model\n         if self.multiple:\n             # If this is a symmetrical m2m relation on self, there is no reverse accessor.\n             if self.symmetrical and model == self.to:\n@@ -1383,11 +1421,11 @@ def get_related_field(self):\n         Returns the Field in the 'to' object to which this relationship is\n         tied.\n         \"\"\"\n-        data = self.to._meta.get_field_by_name(self.field_name)\n-        if not data[2]:\n+        field = self.to._meta.get_field(self.field_name)\n+        if not field.concrete:\n             raise FieldDoesNotExist(\"No related field named '%s'\" %\n                     self.field_name)\n-        return data[0]\n+        return field\n \n     def set_field_name(self):\n         self.field_name = self.field_name or self.to._meta.pk.name\n@@ -1419,6 +1457,10 @@ def __init__(self, field, to, related_name=None, limit_choices_to=None,\n         self.through_fields = through_fields\n         self.db_constraint = db_constraint\n \n+    def is_hidden(self):\n+        \"Should the related object be hidden?\"\n+        return self.related_name is not None and self.related_name[-1] == '+'\n+\n     def get_related_field(self):\n         \"\"\"\n         Returns the field in the 'to' object to which this relationship is tied.\n@@ -1436,8 +1478,13 @@ def get_related_field(self):\n \n \n class ForeignObject(RelatedField):\n+    # Field flags\n+    many_to_many = False\n+    many_to_one = False\n+    one_to_many = True\n+    one_to_one = False\n+\n     requires_unique_target = True\n-    generate_reverse_relation = True\n     related_accessor_class = ForeignRelatedObjectsDescriptor\n \n     def __init__(self, to, from_fields, to_fields, swappable=True, **kwargs):\n@@ -1556,9 +1603,9 @@ def resolve_related_fields(self):\n             from_field_name = self.from_fields[index]\n             to_field_name = self.to_fields[index]\n             from_field = (self if from_field_name == 'self'\n-                          else self.opts.get_field_by_name(from_field_name)[0])\n+                          else self.opts.get_field(from_field_name))\n             to_field = (self.rel.to._meta.pk if to_field_name is None\n-                        else self.rel.to._meta.get_field_by_name(to_field_name)[0])\n+                        else self.rel.to._meta.get_field(to_field_name))\n             related_fields.append((from_field, to_field))\n         return related_fields\n \n@@ -1731,7 +1778,7 @@ def contribute_to_class(self, cls, name, virtual_only=False):\n     def contribute_to_related_class(self, cls, related):\n         # Internal FK's - i.e., those with a related name ending with '+' -\n         # and swapped models don't get a related descriptor.\n-        if not self.rel.is_hidden() and not related.model._meta.swapped:\n+        if not self.rel.is_hidden() and not related.related_model._meta.swapped:\n             setattr(cls, related.get_accessor_name(), self.related_accessor_class(related))\n             # While 'limit_choices_to' might be a callable, simply pass\n             # it along for later - this is too early because it's still\n@@ -1741,6 +1788,12 @@ def contribute_to_related_class(self, cls, related):\n \n \n class ForeignKey(ForeignObject):\n+    # Field flags\n+    many_to_many = False\n+    many_to_one = False\n+    one_to_many = True\n+    one_to_one = False\n+\n     empty_strings_allowed = False\n     default_error_messages = {\n         'invalid': _('%(model)s instance with %(field)s %(value)r does not exist.')\n@@ -1951,6 +2004,12 @@ class OneToOneField(ForeignKey):\n     always returns the object pointed to (since there will only ever be one),\n     rather than returning a list.\n     \"\"\"\n+    # Field flags\n+    many_to_many = False\n+    many_to_one = False\n+    one_to_many = False\n+    one_to_one = True\n+\n     related_accessor_class = SingleRelatedObjectDescriptor\n     description = _(\"One-to-one relationship\")\n \n@@ -2036,6 +2095,12 @@ def set_managed(field, model, cls):\n \n \n class ManyToManyField(RelatedField):\n+    # Field flags\n+    many_to_many = True\n+    many_to_one = False\n+    one_to_many = False\n+    one_to_one = False\n+\n     description = _(\"Many-to-many relationship\")\n \n     def __init__(self, to, db_constraint=True, swappable=True, **kwargs):\n@@ -2050,7 +2115,6 @@ def __init__(self, to, db_constraint=True, swappable=True, **kwargs):\n             # Class names must be ASCII in Python 2.x, so we forcibly coerce it\n             # here to break early if there's a problem.\n             to = str(to)\n-\n         kwargs['verbose_name'] = kwargs.get('verbose_name', None)\n         kwargs['rel'] = ManyToManyRel(\n             self, to,\n@@ -2357,8 +2421,8 @@ def _get_path_info(self, direct=False):\n         \"\"\"\n         pathinfos = []\n         int_model = self.rel.through\n-        linkfield1 = int_model._meta.get_field_by_name(self.m2m_field_name())[0]\n-        linkfield2 = int_model._meta.get_field_by_name(self.m2m_reverse_field_name())[0]\n+        linkfield1 = int_model._meta.get_field(self.m2m_field_name())\n+        linkfield2 = int_model._meta.get_field(self.m2m_reverse_field_name())\n         if direct:\n             join1infos = linkfield1.get_reverse_path_info()\n             join2infos = linkfield2.get_path_info()\n@@ -2398,8 +2462,8 @@ def _get_m2m_attr(self, related, attr):\n         else:\n             link_field_name = None\n         for f in self.rel.through._meta.fields:\n-            if hasattr(f, 'rel') and f.rel and f.rel.to == related.model and \\\n-                    (link_field_name is None or link_field_name == f.name):\n+            if (f.is_relation and f.rel.to == related.related_model and\n+                    (link_field_name is None or link_field_name == f.name)):\n                 setattr(self, cache_attr, getattr(f, attr))\n                 return getattr(self, cache_attr)\n \n@@ -2414,8 +2478,9 @@ def _get_m2m_reverse_attr(self, related, attr):\n         else:\n             link_field_name = None\n         for f in self.rel.through._meta.fields:\n-            if hasattr(f, 'rel') and f.rel and f.rel.to == related.parent_model:\n-                if link_field_name is None and related.model == related.parent_model:\n+            # NOTE f.rel.to != f.related_model\n+            if f.is_relation and f.rel.to == related.model:\n+                if link_field_name is None and related.related_model == related.model:\n                     # If this is an m2m-intermediate to self,\n                     # the first foreign key you find will be\n                     # the source column. Keep searching for\n@@ -2479,7 +2544,7 @@ def resolve_through_model(field, model, cls):\n     def contribute_to_related_class(self, cls, related):\n         # Internal M2Ms (i.e., those with a related name ending with '+')\n         # and swapped models don't get a related descriptor.\n-        if not self.rel.is_hidden() and not related.model._meta.swapped:\n+        if not self.rel.is_hidden() and not related.related_model._meta.swapped:\n             setattr(cls, related.get_accessor_name(), ManyRelatedObjectsDescriptor(related))\n \n         # Set up the accessors for the column names on the m2m table\ndiff --git a/django/db/models/manager.py b/django/db/models/manager.py\nindex 179ea7cee3a0..aa2df3f0e860 100644\n--- a/django/db/models/manager.py\n+++ b/django/db/models/manager.py\n@@ -2,7 +2,6 @@\n from importlib import import_module\n import inspect\n \n-from django.core.exceptions import FieldDoesNotExist\n from django.db import router\n from django.db.models.query import QuerySet\n from django.utils import six\n@@ -23,15 +22,12 @@ def ensure_default_manager(cls):\n         setattr(cls, 'objects', SwappedManagerDescriptor(cls))\n         return\n     if not getattr(cls, '_default_manager', None):\n-        # Create the default manager, if needed.\n-        try:\n-            cls._meta.get_field('objects')\n+        if any(f.name == 'objects' for f in cls._meta.fields):\n             raise ValueError(\n                 \"Model %s must specify a custom Manager, because it has a \"\n                 \"field named 'objects'\" % cls.__name__\n             )\n-        except FieldDoesNotExist:\n-            pass\n+        # Create the default manager, if needed.\n         cls.add_to_class('objects', Manager())\n         cls._base_manager = cls.objects\n     elif not getattr(cls, '_base_manager', None):\ndiff --git a/django/db/models/options.py b/django/db/models/options.py\nindex 79954a87e610..6b48d2733bee 100644\n--- a/django/db/models/options.py\n+++ b/django/db/models/options.py\n@@ -1,20 +1,31 @@\n from __future__ import unicode_literals\n \n from bisect import bisect\n-from collections import OrderedDict\n+from collections import OrderedDict, defaultdict\n+from itertools import chain\n+import warnings\n \n from django.apps import apps\n from django.conf import settings\n from django.core.exceptions import FieldDoesNotExist\n-from django.db.models.fields.related import ManyToManyRel\n+from django.db.models.fields.related import ManyToManyField\n from django.db.models.fields import AutoField\n from django.db.models.fields.proxy import OrderWrt\n from django.utils import six\n+from django.utils.datastructures import ImmutableList\n+from django.utils.deprecation import RemovedInDjango20Warning\n from django.utils.encoding import force_text, smart_text, python_2_unicode_compatible\n from django.utils.functional import cached_property\n+from django.utils.lru_cache import lru_cache\n from django.utils.text import camel_case_to_spaces\n from django.utils.translation import activate, deactivate_all, get_language, string_concat\n \n+EMPTY_RELATION_TREE = tuple()\n+\n+IMMUTABLE_WARNING = (\n+    \"The return type of '%s' should never be mutated. If you want to manipulate this list \"\n+    \"for your own use, make a copy first.\"\n+)\n \n DEFAULT_NAMES = ('verbose_name', 'verbose_name_plural', 'db_table', 'ordering',\n                  'unique_together', 'permissions', 'get_latest_by',\n@@ -24,6 +35,24 @@\n                  'select_on_save', 'default_related_name')\n \n \n+class raise_deprecation(object):\n+    def __init__(self, suggested_alternative):\n+        self.suggested_alternative = suggested_alternative\n+\n+    def __call__(self, fn):\n+        def wrapper(*args, **kwargs):\n+            warnings.warn(\n+                \"'%s is an unofficial API that has been deprecated. \"\n+                \"You may be able to replace it with '%s'\" % (\n+                    fn.__name__,\n+                    self.suggested_alternative,\n+                ),\n+                RemovedInDjango20Warning, stacklevel=2\n+            )\n+            return fn(*args, **kwargs)\n+        return wrapper\n+\n+\n def normalize_together(option_together):\n     \"\"\"\n     option_together can be either a tuple of tuples, or a single\n@@ -46,9 +75,19 @@ def normalize_together(option_together):\n         return option_together\n \n \n+def make_immutable_fields_list(name, data):\n+    return ImmutableList(data, warning=IMMUTABLE_WARNING % name)\n+\n+\n @python_2_unicode_compatible\n class Options(object):\n+    FORWARD_PROPERTIES = ('fields', 'many_to_many', 'concrete_fields',\n+                          'local_concrete_fields', '_forward_fields_map')\n+    REVERSE_PROPERTIES = ('related_objects', 'fields_map', '_relation_tree')\n+\n     def __init__(self, meta, app_label=None):\n+        self._get_fields_cache = {}\n+        self.proxied_children = []\n         self.local_fields = []\n         self.local_many_to_many = []\n         self.virtual_fields = []\n@@ -103,6 +142,31 @@ def __init__(self, meta, app_label=None):\n \n         self.default_related_name = None\n \n+    @lru_cache(maxsize=None)\n+    def _map_model(self, link):\n+        # This helper function is used to allow backwards compatibility with\n+        # the previous API. No future methods should use this function.\n+        # It maps a field to (field, model or related_model,) depending on the\n+        # field type.\n+        model = link.model._meta.concrete_model\n+        if model is self.model:\n+            model = None\n+        return link, model\n+\n+    @lru_cache(maxsize=None)\n+    def _map_model_details(self, link):\n+        # This helper function is used to allow backwards compatibility with\n+        # the previous API. No future methods should use this function.\n+        # This function maps a field to a tuple of:\n+        #  (field, model or related_model, direct, is_m2m) depending on the\n+        # field type.\n+        direct = not link.auto_created or link.concrete\n+        model = link.model._meta.concrete_model\n+        if model is self.model:\n+            model = None\n+        m2m = link.is_relation and link.many_to_many\n+        return link, model, direct, m2m\n+\n     @property\n     def app_config(self):\n         # Don't go through get_app_config to avoid triggering imports.\n@@ -183,7 +247,17 @@ def contribute_to_class(self, cls, name):\n \n     def _prepare(self, model):\n         if self.order_with_respect_to:\n-            self.order_with_respect_to = self.get_field(self.order_with_respect_to)\n+            # The app registry will not be ready at this point, so we cannot\n+            # use get_field().\n+            query = self.order_with_respect_to\n+            try:\n+                self.order_with_respect_to = next(\n+                    f for f in self._get_fields(reverse=False)\n+                    if f.name == query or f.attname == query\n+                )\n+            except StopIteration:\n+                raise FieldDoesNotExist('%s has no field named %r' % (self.object_name, query))\n+\n             self.ordering = ('_order',)\n             if not any(isinstance(field, OrderWrt) for field in model._meta.local_fields):\n                 model.add_to_class('_order', OrderWrt())\n@@ -208,56 +282,41 @@ def _prepare(self, model):\n                         auto_created=True)\n                 model.add_to_class('id', auto)\n \n-    def add_field(self, field):\n+    def add_field(self, field, virtual=False):\n         # Insert the given field in the order in which it was created, using\n         # the \"creation_counter\" attribute of the field.\n         # Move many-to-many related fields from self.fields into\n         # self.many_to_many.\n-        if field.rel and isinstance(field.rel, ManyToManyRel):\n+        if virtual:\n+            self.virtual_fields.append(field)\n+        elif field.is_relation and field.many_to_many:\n             self.local_many_to_many.insert(bisect(self.local_many_to_many, field), field)\n-            if hasattr(self, '_m2m_cache'):\n-                del self._m2m_cache\n         else:\n             self.local_fields.insert(bisect(self.local_fields, field), field)\n             self.setup_pk(field)\n-            if hasattr(self, '_field_cache'):\n-                del self._field_cache\n-                del self._field_name_cache\n-                # The fields, concrete_fields and local_concrete_fields are\n-                # implemented as cached properties for performance reasons.\n-                # The attrs will not exists if the cached property isn't\n-                # accessed yet, hence the try-excepts.\n-                try:\n-                    del self.fields\n-                except AttributeError:\n-                    pass\n-                try:\n-                    del self.concrete_fields\n-                except AttributeError:\n-                    pass\n-                try:\n-                    del self.local_concrete_fields\n-                except AttributeError:\n-                    pass\n-\n-        if hasattr(self, '_name_map'):\n-            del self._name_map\n \n-    def add_virtual_field(self, field):\n-        self.virtual_fields.append(field)\n+        # If the field being added is a relation to another known field,\n+        # expire the cache on this field and the forward cache on the field\n+        # being referenced, because there will be new relationships in the\n+        # cache. Otherwise, expire the cache of references *to* this field.\n+        # The mechanism for getting at the related model is slightly odd -\n+        # ideally, we'd just ask for field.related_model. However, related_model\n+        # is a cached property, and all the models haven't been loaded yet, so\n+        # we need to make sure we don't cache a string reference.\n+        if field.is_relation and hasattr(field.rel, 'to') and field.rel.to:\n+            try:\n+                field.rel.to._meta._expire_cache(forward=False)\n+            except AttributeError:\n+                pass\n+            self._expire_cache()\n+        else:\n+            self._expire_cache(reverse=False)\n \n     def setup_pk(self, field):\n         if not self.pk and field.primary_key:\n             self.pk = field\n             field.serialize = False\n \n-    def pk_index(self):\n-        \"\"\"\n-        Returns the index of the primary key field in the self.concrete_fields\n-        list.\n-        \"\"\"\n-        return self.concrete_fields.index(self.pk)\n-\n     def setup_proxy(self, target):\n         \"\"\"\n         Does the internal setup so that the current model is a proxy for\n@@ -273,6 +332,7 @@ def __repr__(self):\n     def __str__(self):\n         return \"%s.%s\" % (smart_text(self.app_label), smart_text(self.model_name))\n \n+    @property\n     def verbose_name_raw(self):\n         \"\"\"\n         There are a few places where the untranslated verbose name is needed\n@@ -284,9 +344,9 @@ def verbose_name_raw(self):\n         raw = force_text(self.verbose_name)\n         activate(lang)\n         return raw\n-    verbose_name_raw = property(verbose_name_raw)\n \n-    def _swapped(self):\n+    @property\n+    def swapped(self):\n         \"\"\"\n         Has this model been swapped out for another? If so, return the model\n         name of the replacement; otherwise, return None.\n@@ -310,253 +370,253 @@ def _swapped(self):\n                 if '%s.%s' % (swapped_label, swapped_object.lower()) not in (None, model_label):\n                     return swapped_for\n         return None\n-    swapped = property(_swapped)\n \n     @cached_property\n     def fields(self):\n         \"\"\"\n-        The getter for self.fields. This returns the list of field objects\n-        available to this model (including through parent models).\n-\n-        Callers are not permitted to modify this list, since it's a reference\n-        to this instance (not a copy).\n-        \"\"\"\n-        try:\n-            self._field_name_cache\n-        except AttributeError:\n-            self._fill_fields_cache()\n-        return self._field_name_cache\n+        Returns a list of all forward fields on the model and its parents,\n+        excluding ManyToManyFields.\n+\n+        Private API intended only to be used by Django itself; get_fields()\n+        combined with filtering of field properties is the public API for\n+        obtaining this field list.\n+        \"\"\"\n+        # For legacy reasons, the fields property should only contain forward\n+        # fields that are not virtual or with a m2m cardinality. Therefore we\n+        # pass these three filters as filters to the generator.\n+        # The third lambda is a longwinded way of checking f.related_model - we don't\n+        # use that property directly because related_model is a cached property,\n+        # and all the models may not have been loaded yet; we don't want to cache\n+        # the string reference to the related_model.\n+        is_not_an_m2m_field = lambda f: not (f.is_relation and f.many_to_many)\n+        is_not_a_generic_relation = lambda f: not (f.is_relation and f.many_to_one)\n+        is_not_a_generic_foreign_key = lambda f: not (\n+            f.is_relation and f.one_to_many and not (hasattr(f.rel, 'to') and f.rel.to)\n+        )\n+        return make_immutable_fields_list(\n+            \"fields\",\n+            (f for f in self._get_fields(reverse=False) if\n+            is_not_an_m2m_field(f) and is_not_a_generic_relation(f)\n+            and is_not_a_generic_foreign_key(f))\n+        )\n \n     @cached_property\n     def concrete_fields(self):\n-        return [f for f in self.fields if f.column is not None]\n+        \"\"\"\n+        Returns a list of all concrete fields on the model and its parents.\n+\n+        Private API intended only to be used by Django itself; get_fields()\n+        combined with filtering of field properties is the public API for\n+        obtaining this field list.\n+        \"\"\"\n+        return make_immutable_fields_list(\n+            \"concrete_fields\", (f for f in self.fields if f.concrete)\n+        )\n \n     @cached_property\n     def local_concrete_fields(self):\n-        return [f for f in self.local_fields if f.column is not None]\n-\n-    def get_fields_with_model(self):\n         \"\"\"\n-        Returns a sequence of (field, model) pairs for all fields. The \"model\"\n-        element is None for fields on the current model. Mostly of use when\n-        constructing queries so that we know which model a field belongs to.\n+        Returns a list of all concrete fields on the model.\n+\n+        Private API intended only to be used by Django itself; get_fields()\n+        combined with filtering of field properties is the public API for\n+        obtaining this field list.\n         \"\"\"\n-        try:\n-            self._field_cache\n-        except AttributeError:\n-            self._fill_fields_cache()\n-        return self._field_cache\n+        return make_immutable_fields_list(\n+            \"local_concrete_fields\", (f for f in self.local_fields if f.concrete)\n+        )\n \n-    def get_concrete_fields_with_model(self):\n-        return [(field, model) for field, model in self.get_fields_with_model() if\n-                field.column is not None]\n+    @raise_deprecation(suggested_alternative=\"get_fields()\")\n+    def get_fields_with_model(self):\n+        return [self._map_model(f) for f in self.get_fields()]\n \n-    def _fill_fields_cache(self):\n-        cache = []\n-        for parent in self.parents:\n-            for field, model in parent._meta.get_fields_with_model():\n-                if model:\n-                    cache.append((field, model))\n-                else:\n-                    cache.append((field, parent))\n-        cache.extend((f, None) for f in self.local_fields)\n-        self._field_cache = tuple(cache)\n-        self._field_name_cache = [x for x, _ in cache]\n-\n-    def _many_to_many(self):\n-        try:\n-            self._m2m_cache\n-        except AttributeError:\n-            self._fill_m2m_cache()\n-        return list(self._m2m_cache)\n-    many_to_many = property(_many_to_many)\n+    @raise_deprecation(suggested_alternative=\"get_fields()\")\n+    def get_concrete_fields_with_model(self):\n+        return [self._map_model(f) for f in self.concrete_fields]\n \n-    def get_m2m_with_model(self):\n-        \"\"\"\n-        The many-to-many version of get_fields_with_model().\n+    @cached_property\n+    def many_to_many(self):\n         \"\"\"\n-        try:\n-            self._m2m_cache\n-        except AttributeError:\n-            self._fill_m2m_cache()\n-        return list(six.iteritems(self._m2m_cache))\n+        Returns a list of all many to many fields on the model and its parents.\n \n-    def _fill_m2m_cache(self):\n-        cache = OrderedDict()\n-        for parent in self.parents:\n-            for field, model in parent._meta.get_m2m_with_model():\n-                if model:\n-                    cache[field] = model\n-                else:\n-                    cache[field] = parent\n-        for field in self.local_many_to_many:\n-            cache[field] = None\n-        self._m2m_cache = cache\n-\n-    def get_field(self, name, many_to_many=True):\n+        Private API intended only to be used by Django itself; get_fields()\n+        combined with filtering of field properties is the public API for\n+        obtaining this list.\n         \"\"\"\n-        Returns the requested field by name. Raises FieldDoesNotExist on error.\n-        \"\"\"\n-        to_search = (self.fields + self.many_to_many) if many_to_many else self.fields\n-        for f in to_search:\n-            if f.name == name:\n-                return f\n-        raise FieldDoesNotExist('%s has no field named %r' % (self.object_name, name))\n+        return make_immutable_fields_list(\n+            \"many_to_many\",\n+            (f for f in self._get_fields(reverse=False)\n+            if f.is_relation and f.many_to_many)\n+        )\n \n-    def get_field_by_name(self, name):\n+    @cached_property\n+    def related_objects(self):\n         \"\"\"\n-        Returns the (field_object, model, direct, m2m), where field_object is\n-        the Field instance for the given name, model is the model containing\n-        this field (None for local fields), direct is True if the field exists\n-        on this model, and m2m is True for many-to-many relations. When\n-        'direct' is False, 'field_object' is the corresponding ForeignObjectRel\n-        for this field (since the field doesn't have an instance associated\n-        with it).\n-\n-        Uses a cache internally, so after the first access, this is very fast.\n+        Returns all related objects pointing to the current model. The related\n+        objects can come from a one-to-one, one-to-many, or many-to-many field\n+        relation type.\n+\n+        Private API intended only to be used by Django itself; get_fields()\n+        combined with filtering of field properties is the public API for\n+        obtaining this field list.\n         \"\"\"\n-        try:\n+        all_related_fields = self._get_fields(forward=False, reverse=True, include_hidden=True)\n+        return make_immutable_fields_list(\n+            \"related_objects\",\n+            (obj for obj in all_related_fields\n+            if not obj.hidden or obj.field.many_to_many)\n+        )\n+\n+    @raise_deprecation(suggested_alternative=\"get_fields()\")\n+    def get_m2m_with_model(self):\n+        return [self._map_model(f) for f in self.many_to_many]\n+\n+    @cached_property\n+    def _forward_fields_map(self):\n+        res = {}\n+        # call get_fields() with export_ordered_set=True in order to have a\n+        # field_instance -> names map\n+        fields = self._get_fields(reverse=False)\n+        for field in fields:\n+            res[field.name] = field\n+            # Due to the way Django's internals work, get_field() should also\n+            # be able to fetch a field by attname. In the case of a concrete\n+            # field with relation, includes the *_id name too\n             try:\n-                return self._name_map[name]\n+                res[field.attname] = field\n             except AttributeError:\n-                cache = self.init_name_map()\n-                return cache[name]\n-        except KeyError:\n-            raise FieldDoesNotExist('%s has no field named %r'\n-                    % (self.object_name, name))\n+                pass\n+        return res\n \n-    def get_all_field_names(self):\n+    @cached_property\n+    def fields_map(self):\n+        res = {}\n+        fields = self._get_fields(forward=False, include_hidden=True)\n+        for field in fields:\n+            res[field.name] = field\n+            # Due to the way Django's internals work, get_field() should also\n+            # be able to fetch a field by attname. In the case of a concrete\n+            # field with relation, includes the *_id name too\n+            try:\n+                res[field.attname] = field\n+            except AttributeError:\n+                pass\n+        return res\n+\n+    def get_field(self, field_name, many_to_many=None):\n         \"\"\"\n-        Returns a list of all field names that are possible for this model\n-        (including reverse relation names). This is used for pretty printing\n-        debugging output (a list of choices), so any internal-only field names\n-        are not included.\n+        Returns a field instance given a field name. The field can be either a\n+        forward or reverse field, unless many_to_many is specified; if it is,\n+        only forward fields will be returned.\n+\n+        The many_to_many argument exists for backwards compatibility reasons;\n+        it has been deprecated and will be removed in Django 2.0.\n         \"\"\"\n+        m2m_in_kwargs = many_to_many is not None\n+        if m2m_in_kwargs:\n+            # Always throw a warning if many_to_many is used regardless of\n+            # whether it alters the return type or not.\n+            warnings.warn(\n+                \"The 'many_to_many' argument on get_field() is deprecated; \"\n+                \"use a filter on field.many_to_many instead.\",\n+                RemovedInDjango20Warning\n+            )\n+\n         try:\n-            cache = self._name_map\n-        except AttributeError:\n-            cache = self.init_name_map()\n-        names = sorted(cache.keys())\n-        # Internal-only names end with \"+\" (symmetrical m2m related names being\n-        # the main example). Trim them.\n-        return [val for val in names if not val.endswith('+')]\n-\n-    def init_name_map(self):\n-        \"\"\"\n-        Initialises the field name -> field object mapping.\n-        \"\"\"\n-        cache = {}\n-        # We intentionally handle related m2m objects first so that symmetrical\n-        # m2m accessor names can be overridden, if necessary.\n-        for f, model in self.get_all_related_m2m_objects_with_model():\n-            cache[f.field.related_query_name()] = (f, model, False, True)\n-        for f, model in self.get_all_related_objects_with_model():\n-            cache[f.field.related_query_name()] = (f, model, False, False)\n-        for f, model in self.get_m2m_with_model():\n-            cache[f.name] = cache[f.attname] = (f, model, True, True)\n-        for f, model in self.get_fields_with_model():\n-            cache[f.name] = cache[f.attname] = (f, model, True, False)\n-        for f in self.virtual_fields:\n-            if f.rel:\n-                cache[f.name] = cache[f.attname] = (\n-                    f, None if f.model == self.model else f.model, True, False)\n-        if apps.ready:\n-            self._name_map = cache\n-        return cache\n+            # In order to avoid premature loading of the relation tree\n+            # (expensive) we prefer checking if the field is a forward field.\n+            field = self._forward_fields_map[field_name]\n+\n+            if many_to_many is False and field.many_to_many:\n+                raise FieldDoesNotExist(\n+                    '%s has no field named %r' % (self.object_name, field_name)\n+                )\n+\n+            return field\n+        except KeyError:\n+            # If the app registry is not ready, reverse fields are\n+            # unavailable, therefore we throw a FieldDoesNotExist exception.\n+            if not self.apps.ready:\n+                raise FieldDoesNotExist(\n+                    \"%s has no field named %r. The app cache isn't \"\n+                    \"ready yet, so if this is a forward field, it won't \"\n+                    \"be available yet.\" % (self.object_name, field_name)\n+                )\n+\n+        try:\n+            if m2m_in_kwargs:\n+                # Previous API does not allow searching reverse fields.\n+                raise FieldDoesNotExist('%s has no field named %r' % (self.object_name, field_name))\n \n+            # Retrieve field instance by name from cached or just-computed\n+            # field map.\n+            return self.fields_map[field_name]\n+        except KeyError:\n+            raise FieldDoesNotExist('%s has no field named %r' % (self.object_name, field_name))\n+\n+    @raise_deprecation(suggested_alternative=\"get_field()\")\n+    def get_field_by_name(self, name):\n+        return self._map_model_details(self.get_field(name))\n+\n+    @raise_deprecation(suggested_alternative=\"get_fields()\")\n+    def get_all_field_names(self):\n+        names = set()\n+        fields = self.get_fields()\n+        for field in fields:\n+            # For backwards compatibility GenericForeignKey should not be\n+            # included in the results.\n+            if field.is_relation and field.one_to_many and field.related_model is None:\n+                continue\n+\n+            names.add(field.name)\n+            if hasattr(field, 'attname'):\n+                names.add(field.attname)\n+        return list(names)\n+\n+    @raise_deprecation(suggested_alternative=\"get_fields()\")\n     def get_all_related_objects(self, local_only=False, include_hidden=False,\n                                 include_proxy_eq=False):\n-        return [k for k, v in self.get_all_related_objects_with_model(\n-                local_only=local_only, include_hidden=include_hidden,\n-                include_proxy_eq=include_proxy_eq)]\n \n-    def get_all_related_objects_with_model(self, local_only=False,\n-                                           include_hidden=False,\n+        include_parents = local_only is False\n+        fields = self._get_fields(\n+            forward=False, reverse=True,\n+            include_parents=include_parents,\n+            include_hidden=include_hidden,\n+        )\n+        fields = (obj for obj in fields if not isinstance(obj.field, ManyToManyField))\n+\n+        if include_proxy_eq:\n+            children = chain.from_iterable(c._relation_tree\n+                                           for c in self.concrete_model._meta.proxied_children\n+                                           if c is not self)\n+            relations = (f.rel for f in children\n+                         if include_hidden or not f.rel.field.rel.is_hidden())\n+            fields = chain(fields, relations)\n+        return list(fields)\n+\n+    @raise_deprecation(suggested_alternative=\"get_fields()\")\n+    def get_all_related_objects_with_model(self, local_only=False, include_hidden=False,\n                                            include_proxy_eq=False):\n-        \"\"\"\n-        Returns a list of (related-object, model) pairs. Similar to\n-        get_fields_with_model().\n-        \"\"\"\n-        try:\n-            self._related_objects_cache\n-        except AttributeError:\n-            self._fill_related_objects_cache()\n-        predicates = []\n-        if local_only:\n-            predicates.append(lambda k, v: not v)\n-        if not include_hidden:\n-            predicates.append(lambda k, v: not k.field.rel.is_hidden())\n-        cache = (self._related_objects_proxy_cache if include_proxy_eq\n-                 else self._related_objects_cache)\n-        return [t for t in cache.items() if all(p(*t) for p in predicates)]\n-\n-    def _fill_related_objects_cache(self):\n-        cache = OrderedDict()\n-        parent_list = self.get_parent_list()\n-        for parent in self.parents:\n-            for obj, model in parent._meta.get_all_related_objects_with_model(include_hidden=True):\n-                if (obj.field.creation_counter < 0 or obj.field.rel.parent_link) and obj.model not in parent_list:\n-                    continue\n-                if not model:\n-                    cache[obj] = parent\n-                else:\n-                    cache[obj] = model\n-        # Collect also objects which are in relation to some proxy child/parent of self.\n-        proxy_cache = cache.copy()\n-        for klass in self.apps.get_models(include_auto_created=True):\n-            if not klass._meta.swapped:\n-                for f in klass._meta.local_fields + klass._meta.virtual_fields:\n-                    if (hasattr(f, 'rel') and f.rel and not isinstance(f.rel.to, six.string_types)\n-                            and f.generate_reverse_relation):\n-                        if self == f.rel.to._meta:\n-                            cache[f.rel] = None\n-                            proxy_cache[f.rel] = None\n-                        elif self.concrete_model == f.rel.to._meta.concrete_model:\n-                            proxy_cache[f.rel] = None\n-        self._related_objects_cache = cache\n-        self._related_objects_proxy_cache = proxy_cache\n+        return [\n+            self._map_model(f) for f in self.get_all_related_objects(\n+                local_only=local_only,\n+                include_hidden=include_hidden,\n+                include_proxy_eq=include_proxy_eq,\n+            )\n+        ]\n \n+    @raise_deprecation(suggested_alternative=\"get_fields()\")\n     def get_all_related_many_to_many_objects(self, local_only=False):\n-        try:\n-            cache = self._related_many_to_many_cache\n-        except AttributeError:\n-            cache = self._fill_related_many_to_many_cache()\n-        if local_only:\n-            return [k for k, v in cache.items() if not v]\n-        return list(cache)\n+        fields = self._get_fields(\n+            forward=False, reverse=True,\n+            include_parents=local_only is not True, include_hidden=True\n+        )\n+        return [obj for obj in fields if isinstance(obj.field, ManyToManyField)]\n \n+    @raise_deprecation(suggested_alternative=\"get_fields()\")\n     def get_all_related_m2m_objects_with_model(self):\n-        \"\"\"\n-        Returns a list of (related-m2m-object, model) pairs. Similar to\n-        get_fields_with_model().\n-        \"\"\"\n-        try:\n-            cache = self._related_many_to_many_cache\n-        except AttributeError:\n-            cache = self._fill_related_many_to_many_cache()\n-        return list(six.iteritems(cache))\n-\n-    def _fill_related_many_to_many_cache(self):\n-        cache = OrderedDict()\n-        parent_list = self.get_parent_list()\n-        for parent in self.parents:\n-            for obj, model in parent._meta.get_all_related_m2m_objects_with_model():\n-                if obj.field.creation_counter < 0 and obj.model not in parent_list:\n-                    continue\n-                if not model:\n-                    cache[obj] = parent\n-                else:\n-                    cache[obj] = model\n-        for klass in self.apps.get_models():\n-            if not klass._meta.swapped:\n-                for f in klass._meta.local_many_to_many:\n-                    if (f.rel\n-                            and not isinstance(f.rel.to, six.string_types)\n-                            and self == f.rel.to._meta):\n-                        cache[f.rel] = None\n-        if apps.ready:\n-            self._related_many_to_many_cache = cache\n-        return cache\n+        fields = self._get_fields(forward=False, reverse=True, include_hidden=True)\n+        return [self._map_model(obj) for obj in fields if isinstance(obj.field, ManyToManyField)]\n \n     def get_base_chain(self, model):\n         \"\"\"\n@@ -605,3 +665,173 @@ def get_ancestor_link(self, ancestor):\n                 # of the chain to the ancestor is that parent\n                 # links\n                 return self.parents[parent] or parent_link\n+\n+    def _populate_directed_relation_graph(self):\n+        \"\"\"\n+        This method is used by each model to find its reverse objects. As this\n+        method is very expensive and is accessed frequently (it looks up every\n+        field in a model, in every app), it is computed on first access and then\n+        is set as a property on every model.\n+        \"\"\"\n+        related_objects_graph = defaultdict(list)\n+\n+        all_models = self.apps.get_models(include_auto_created=True)\n+        for model in all_models:\n+            fields_with_relations = (\n+                f for f in model._meta._get_fields(reverse=False)\n+                if f.is_relation and f.related_model is not None\n+            )\n+            if model._meta.auto_created:\n+                fields_with_relations = (\n+                    f for f in fields_with_relations\n+                    if not f.many_to_many\n+                )\n+\n+            for f in fields_with_relations:\n+                if not isinstance(f.rel.to, six.string_types):\n+                    # Set options_instance -> field\n+                    related_objects_graph[f.rel.to._meta].append(f)\n+\n+        for model in all_models:\n+            # Set the relation_tree using the internal __dict__. In this way\n+            # we avoid calling the cached property. In attribute lookup,\n+            # __dict__ takes precedence over a data descriptor (such as\n+            # @cached_property). This means that the _meta._relation_tree is\n+            # only called if related_objects is not in __dict__.\n+            related_objects = related_objects_graph[model._meta]\n+\n+            # If related_objects are empty, it makes sense to set\n+            # EMPTY_RELATION_TREE. This will avoid allocating multiple empty\n+            # relation trees.\n+            relation_tree = EMPTY_RELATION_TREE\n+            if related_objects:\n+                relation_tree = related_objects\n+            model._meta.__dict__['_relation_tree'] = relation_tree\n+\n+    @cached_property\n+    def _relation_tree(self):\n+        # If cache is not present, populate the cache\n+        self._populate_directed_relation_graph()\n+        # It may happen, often when the registry is not ready, that a not yet\n+        # registered model is queried. In this very rare case we simply return\n+        # an EMPTY_RELATION_TREE. When the registry will be ready, cache will\n+        # be flushed and this model will be computed properly.\n+        return self.__dict__.get('_relation_tree', EMPTY_RELATION_TREE)\n+\n+    def _expire_cache(self, forward=True, reverse=True):\n+        # This method is usually called by apps.cache_clear(), when the\n+        # registry is finalized, or when a new field is added.\n+        properties_to_expire = []\n+        if forward:\n+            properties_to_expire.extend(self.FORWARD_PROPERTIES)\n+        if reverse and not self.abstract:\n+            properties_to_expire.extend(self.REVERSE_PROPERTIES)\n+\n+        for cache_key in properties_to_expire:\n+            try:\n+                delattr(self, cache_key)\n+            except AttributeError:\n+                pass\n+\n+        self._get_fields_cache = {}\n+\n+    def get_fields(self, include_parents=True, include_hidden=False):\n+        \"\"\"\n+        Returns a list of fields associated to the model. By default will only\n+        return forward fields. This can be changed by enabling or disabling\n+        field types using the parameters:\n+\n+        - include_parents: include fields derived from inheritance\n+        - include_hidden:  include fields that have a related_name that\n+                           starts with a \"+\"\n+        \"\"\"\n+        return self._get_fields(include_parents=include_parents, include_hidden=include_hidden)\n+\n+    def _get_fields(self, forward=True, reverse=True, include_parents=True, include_hidden=False,\n+                    export_ordered_set=False):\n+        # This helper function is used to allow recursion in ``get_fields()``\n+        # implementation and to provide a fast way for Django's internals to\n+        # access specific subsets of fields.\n+\n+        # Creates a cache key composed of all arguments\n+        cache_key = (forward, reverse, include_parents, include_hidden, export_ordered_set)\n+        try:\n+            # In order to avoid list manipulation. Always return a shallow copy\n+            # of the results.\n+            return self._get_fields_cache[cache_key]\n+        except KeyError:\n+            pass\n+\n+        # Using an OrderedDict preserves the order of insertion. This is\n+        # important when displaying a ModelForm or the contrib.admin panel\n+        # and no specific ordering is provided.\n+        fields = OrderedDict()\n+        options = {\n+            'include_parents': include_parents,\n+            'include_hidden': include_hidden,\n+            'export_ordered_set': True,\n+        }\n+\n+        # Abstract models cannot hold reverse fields.\n+        if reverse and not self.abstract:\n+            if include_parents:\n+                parent_list = self.get_parent_list()\n+                # Recursively call _get_fields() on each parent, with the same\n+                # options provided in this call.\n+                for parent in self.parents:\n+                    for obj, _ in six.iteritems(parent._meta._get_fields(forward=False, **options)):\n+                        if obj.many_to_many:\n+                            # In order for a reverse ManyToManyRel object to be\n+                            # valid, its creation counter must be > 0 and must\n+                            # be in the parent list.\n+                            if not (obj.field.creation_counter < 0 and obj.related_model not in parent_list):\n+                                fields[obj] = True\n+\n+                        elif not ((obj.field.creation_counter < 0 or obj.field.rel.parent_link)\n+                                  and obj.related_model not in parent_list):\n+                            fields[obj] = True\n+\n+            # Tree is computed once and cached until the app cache is expired.\n+            # It is composed of a list of fields pointing to the current model\n+            # from other models. If the model is a proxy model, then we also\n+            # add the concrete model.\n+            all_fields = (\n+                self._relation_tree if not self.proxy else\n+                chain(self._relation_tree, self.concrete_model._meta._relation_tree)\n+            )\n+\n+            # Pull out all related objects from forward fields\n+            for field in (f.rel for f in all_fields):\n+                # If hidden fields should be included or the relation is not\n+                # intentionally hidden, add to the fields dict.\n+                if include_hidden or not field.hidden:\n+                    fields[field] = True\n+        if forward:\n+            if include_parents:\n+                for parent in self.parents:\n+                    # Add the forward fields of each parent.\n+                    fields.update(parent._meta._get_fields(reverse=False, **options))\n+            fields.update(\n+                (field, True,)\n+                for field in chain(self.local_fields, self.local_many_to_many)\n+            )\n+\n+        if not export_ordered_set:\n+            # By default, fields contains field instances as keys and all\n+            # possible names if the field instance as values. When\n+            # _get_fields() is called, we only want to return field instances,\n+            # so we just preserve the keys.\n+            fields = list(fields.keys())\n+\n+            # Virtual fields are not inheritable, therefore they are inserted\n+            # only when the recursive _get_fields() call comes to an end.\n+            if forward:\n+                fields.extend(self.virtual_fields)\n+            fields = make_immutable_fields_list(\"get_fields()\", fields)\n+\n+        # Store result into cache for later access\n+        self._get_fields_cache[cache_key] = fields\n+\n+        # In order to avoid list manipulation. Always\n+        # return a shallow copy of the results\n+        return fields\ndiff --git a/django/db/models/query.py b/django/db/models/query.py\nindex 318e3ecf846f..7a447f031311 100644\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -252,9 +252,8 @@ def iterator(self):\n         # If only/defer clauses have been specified,\n         # build the list of fields that are to be loaded.\n         if only_load:\n-            for field, model in self.model._meta.get_concrete_fields_with_model():\n-                if model is None:\n-                    model = self.model\n+            for field in self.model._meta.concrete_fields:\n+                model = field.model._meta.model\n                 try:\n                     if field.name in only_load[model]:\n                         # Add a field that has been explicitly included\n@@ -818,7 +817,7 @@ def annotate(self, *args, **kwargs):\n         obj = self._clone()\n         names = getattr(self, '_fields', None)\n         if names is None:\n-            names = set(self.model._meta.get_all_field_names())\n+            names = {f.name for f in self.model._meta.get_fields()}\n \n         # Add the annotations to the query\n         for alias, annotation in annotations.items():\n@@ -1329,7 +1328,8 @@ def get_klass_info(klass, max_depth=0, cur_depth=0, requested=None,\n         skip = set()\n         init_list = []\n         # Build the list of fields that *haven't* been requested\n-        for field, model in klass._meta.get_concrete_fields_with_model():\n+        for field in klass._meta.concrete_fields:\n+            model = field.model._meta.concrete_model\n             if from_parent and model and issubclass(from_parent, model):\n                 # Avoid loading fields already loaded for parent model for\n                 # child models.\n@@ -1381,18 +1381,19 @@ def get_klass_info(klass, max_depth=0, cur_depth=0, requested=None,\n \n     reverse_related_fields = []\n     if restricted:\n-        for o in klass._meta.get_all_related_objects():\n+        for o in klass._meta.related_objects:\n             if o.field.unique and select_related_descend(o.field, restricted, requested,\n-                                                         only_load.get(o.model), reverse=True):\n+                                                         only_load.get(o.related_model), reverse=True):\n                 next = requested[o.field.related_query_name()]\n-                parent = klass if issubclass(o.model, klass) else None\n-                klass_info = get_klass_info(o.model, max_depth=max_depth, cur_depth=cur_depth + 1,\n+                parent = klass if issubclass(o.related_model, klass) else None\n+                klass_info = get_klass_info(o.related_model, max_depth=max_depth, cur_depth=cur_depth + 1,\n                                             requested=next, only_load=only_load, from_parent=parent)\n                 reverse_related_fields.append((o.field, klass_info))\n     if field_names:\n         pk_idx = field_names.index(klass._meta.pk.attname)\n     else:\n-        pk_idx = klass._meta.pk_index()\n+        meta = klass._meta\n+        pk_idx = meta.concrete_fields.index(meta.pk)\n \n     return klass, field_names, field_count, related_fields, reverse_related_fields, pk_idx\n \n@@ -1485,7 +1486,10 @@ def get_cached_row(row, index_start, using, klass_info, offset=0,\n     for f, klass_info in reverse_related_fields:\n         # Transfer data from this object to childs.\n         parent_data = []\n-        for rel_field, rel_model in klass_info[0]._meta.get_fields_with_model():\n+        for rel_field in klass_info[0]._meta.fields:\n+            rel_model = rel_field.model._meta.concrete_model\n+            if rel_model == klass_info[0]._meta.model:\n+                rel_model = None\n             if rel_model is not None and isinstance(obj, rel_model):\n                 parent_data.append((rel_field, getattr(obj, rel_field.attname)))\n         # Recursively retrieve the data for the related object\ndiff --git a/django/db/models/query_utils.py b/django/db/models/query_utils.py\nindex 69bc878caa22..e8b6cfb8c19b 100644\n--- a/django/db/models/query_utils.py\n+++ b/django/db/models/query_utils.py\n@@ -109,7 +109,7 @@ def __get__(self, instance, owner):\n             # self.field_name is the attname of the field, but only() takes the\n             # actual name, so we need to translate it here.\n             try:\n-                f = opts.get_field_by_name(self.field_name)[0]\n+                f = opts.get_field(self.field_name)\n             except FieldDoesNotExist:\n                 f = [f for f in opts.fields if f.attname == self.field_name][0]\n             name = f.name\n@@ -136,7 +136,7 @@ def _check_parent_chain(self, instance, name):\n         field is a primary key field.\n         \"\"\"\n         opts = instance._meta\n-        f = opts.get_field_by_name(name)[0]\n+        f = opts.get_field(name)\n         link_field = opts.get_ancestor_link(f.model)\n         if f.primary_key and f != link_field:\n             return getattr(instance, link_field.attname)\ndiff --git a/django/db/models/sql/compiler.py b/django/db/models/sql/compiler.py\nindex e8948cb9e1eb..1c0b99e897dd 100644\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -298,7 +298,12 @@ def get_default_columns(self, with_aliases=False, col_aliases=None,\n         # be used by local fields.\n         seen_models = {None: start_alias}\n \n-        for field, model in opts.get_concrete_fields_with_model():\n+        for field in opts.concrete_fields:\n+            model = field.model._meta.concrete_model\n+            # A proxy model will have a different model and concrete_model. We\n+            # will assign None if the field belongs to this model.\n+            if model == opts.model:\n+                model = None\n             if from_parent and model is not None and issubclass(from_parent, model):\n                 # Avoid loading data for already loaded parents.\n                 continue\n@@ -601,10 +606,10 @@ def fill_related_selections(self, opts=None, root_alias=None, cur_depth=1,\n         connections to the root model).\n         \"\"\"\n         def _get_field_choices():\n-            direct_choices = (f.name for (f, _) in opts.get_fields_with_model() if f.rel)\n+            direct_choices = (f.name for f in opts.fields if f.is_relation)\n             reverse_choices = (\n                 f.field.related_query_name()\n-                for f in opts.get_all_related_objects() if f.field.unique\n+                for f in opts.related_objects if f.field.unique\n             )\n             return chain(direct_choices, reverse_choices)\n \n@@ -628,12 +633,13 @@ def _get_field_choices():\n             else:\n                 restricted = False\n \n-        for f, model in opts.get_fields_with_model():\n+        for f in opts.fields:\n+            field_model = f.model._meta.concrete_model\n             fields_found.add(f.name)\n \n             if restricted:\n                 next = requested.get(f.name, {})\n-                if not f.rel:\n+                if not f.is_relation:\n                     # If a non-related field is used like a relation,\n                     # or if a single non-relational field is given.\n                     if next or (cur_depth == 1 and f.name in requested):\n@@ -647,10 +653,6 @@ def _get_field_choices():\n             else:\n                 next = False\n \n-            # The get_fields_with_model() returns None for fields that live\n-            # in the field's local model. So, for those fields we want to use\n-            # the f.model - that is the field's local model.\n-            field_model = model or f.model\n             if not select_related_descend(f, restricted, requested,\n                                           only_load.get(field_model)):\n                 continue\n@@ -666,9 +668,9 @@ def _get_field_choices():\n \n         if restricted:\n             related_fields = [\n-                (o.field, o.model)\n-                for o in opts.get_all_related_objects()\n-                if o.field.unique\n+                (o.field, o.related_model)\n+                for o in opts.related_objects\n+                if o.field.unique and not o.many_to_many\n             ]\n             for f, model in related_fields:\n                 if not select_related_descend(f, restricted, requested,\n@@ -760,7 +762,7 @@ def results_iter(self):\n                     if self.query.select:\n                         fields = [f.field for f in self.query.select]\n                     elif self.query.default_cols:\n-                        fields = self.query.get_meta().concrete_fields\n+                        fields = list(self.query.get_meta().concrete_fields)\n                     else:\n                         fields = []\n \ndiff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex eb1641909156..1d1dbd8162c0 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -11,6 +11,7 @@\n \n from collections import Mapping, OrderedDict\n import copy\n+from itertools import chain\n import warnings\n \n from django.core.exceptions import FieldDoesNotExist, FieldError\n@@ -33,6 +34,13 @@\n __all__ = ['Query', 'RawQuery']\n \n \n+def get_field_names_from_opts(opts):\n+    return set(chain.from_iterable(\n+        (f.name, f.attname) if f.concrete else (f.name,)\n+        for f in opts.get_fields()\n+    ))\n+\n+\n class RawQuery(object):\n     \"\"\"\n     A single raw SQL query\n@@ -593,9 +601,9 @@ def deferred_to_data(self, target, callback):\n             opts = orig_opts\n             for name in parts[:-1]:\n                 old_model = cur_model\n-                source = opts.get_field_by_name(name)[0]\n+                source = opts.get_field(name)\n                 if is_reverse_o2o(source):\n-                    cur_model = source.model\n+                    cur_model = source.related_model\n                 else:\n                     cur_model = source.rel.to\n                 opts = cur_model._meta\n@@ -605,8 +613,11 @@ def deferred_to_data(self, target, callback):\n                 if not is_reverse_o2o(source):\n                     must_include[old_model].add(source)\n                 add_to_dict(must_include, cur_model, opts.pk)\n-            field, model, _, _ = opts.get_field_by_name(parts[-1])\n-            if model is None:\n+            field = opts.get_field(parts[-1])\n+            is_reverse_object = field.auto_created and not field.concrete\n+            model = field.related_model if is_reverse_object else field.model\n+            model = model._meta.concrete_model\n+            if model == opts.model:\n                 model = cur_model\n             if not is_reverse_o2o(field):\n                 add_to_dict(seen, model, field)\n@@ -618,10 +629,11 @@ def deferred_to_data(self, target, callback):\n             # models.\n             workset = {}\n             for model, values in six.iteritems(seen):\n-                for field, m in model._meta.get_fields_with_model():\n+                for field in model._meta.fields:\n                     if field in values:\n                         continue\n-                    add_to_dict(workset, m or model, field)\n+                    m = field.model._meta.concrete_model\n+                    add_to_dict(workset, m, field)\n             for model, values in six.iteritems(must_include):\n                 # If we haven't included a model in workset, we don't add the\n                 # corresponding must_include fields for that model, since an\n@@ -934,8 +946,9 @@ def setup_inherited_models(self):\n         root_alias = self.tables[0]\n         seen = {None: root_alias}\n \n-        for field, model in opts.get_fields_with_model():\n-            if model not in seen:\n+        for field in opts.fields:\n+            model = field.model._meta.concrete_model\n+            if model is not opts.model and model not in seen:\n                 self.join_parent_model(opts, model, root_alias, seen)\n         self.included_inherited_models = seen\n \n@@ -1368,7 +1381,19 @@ def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):\n             if name == 'pk':\n                 name = opts.pk.name\n             try:\n-                field, model, _, _ = opts.get_field_by_name(name)\n+                field = opts.get_field(name)\n+\n+                # Fields that contain one-to-many relations with a generic\n+                # model (like a GenericForeignKey) cannot generate reverse\n+                # relations and therefore cannot be used for reverse querying.\n+                if field.is_relation and not field.related_model:\n+                    raise FieldError(\n+                        \"Field %r does not generate an automatic reverse \"\n+                        \"relation and therefore cannot be used for reverse \"\n+                        \"querying. If it is a GenericForeignKey, consider \"\n+                        \"adding a GenericRelation.\" % name\n+                    )\n+                model = field.model._meta.concrete_model\n             except FieldDoesNotExist:\n                 # is it an annotation?\n                 if self._annotations and name in self._annotations:\n@@ -1382,14 +1407,15 @@ def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):\n                 # one step.\n                 pos -= 1\n                 if pos == -1 or fail_on_missing:\n-                    available = opts.get_all_field_names() + list(self.annotation_select)\n+                    field_names = list(get_field_names_from_opts(opts))\n+                    available = sorted(field_names + list(self.annotation_select))\n                     raise FieldError(\"Cannot resolve keyword %r into field. \"\n                                      \"Choices are: %s\" % (name, \", \".join(available)))\n                 break\n             # Check if we need any joins for concrete inheritance cases (the\n             # field lives in parent, but we are currently in one of its\n             # children)\n-            if model:\n+            if model is not opts.model:\n                 # The field lives on a base class of the current model.\n                 # Skip the chain of proxy to the concrete proxied model\n                 proxied_model = opts.concrete_model\n@@ -1432,7 +1458,7 @@ def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):\n         return path, final_field, targets, names[pos + 1:]\n \n     def raise_field_error(self, opts, name):\n-        available = opts.get_all_field_names() + list(self.annotation_select)\n+        available = list(get_field_names_from_opts(opts)) + list(self.annotation_select)\n         raise FieldError(\"Cannot resolve keyword %r into field. \"\n                          \"Choices are: %s\" % (name, \", \".join(available)))\n \n@@ -1693,7 +1719,7 @@ def add_fields(self, field_names, allow_m2m=True):\n                 # from the model on which the lookup failed.\n                 raise\n             else:\n-                names = sorted(opts.get_all_field_names() + list(self.extra)\n+                names = sorted(list(get_field_names_from_opts(opts)) + list(self.extra)\n                                + list(self.annotation_select))\n                 raise FieldError(\"Cannot resolve keyword %r into field. \"\n                                  \"Choices are: %s\" % (name, \", \".join(names)))\ndiff --git a/django/db/models/sql/subqueries.py b/django/db/models/sql/subqueries.py\nindex 12bde13bf348..bae9f11c2359 100644\n--- a/django/db/models/sql/subqueries.py\n+++ b/django/db/models/sql/subqueries.py\n@@ -120,13 +120,15 @@ def add_update_values(self, values):\n         \"\"\"\n         values_seq = []\n         for name, val in six.iteritems(values):\n-            field, model, direct, m2m = self.get_meta().get_field_by_name(name)\n-            if not direct or m2m:\n+            field = self.get_meta().get_field(name)\n+            direct = not (field.auto_created and not field.concrete) or not field.concrete\n+            model = field.model._meta.concrete_model\n+            if not direct or (field.is_relation and field.many_to_many):\n                 raise FieldError(\n                     'Cannot update model field %r (only non-relations and '\n                     'foreign keys permitted).' % field\n                 )\n-            if model:\n+            if model is not self.get_meta().model:\n                 self.add_related_update(model, field, val)\n                 continue\n             values_seq.append((field, model, val))\ndiff --git a/django/forms/models.py b/django/forms/models.py\nindex c57c8af0b964..443d0559a6f5 100644\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -6,6 +6,7 @@\n from __future__ import unicode_literals\n \n from collections import OrderedDict\n+from itertools import chain\n import warnings\n \n from django.core.exceptions import (\n@@ -89,7 +90,7 @@ def save_m2m():\n         # Note that for historical reasons we want to include also\n         # virtual_fields here. (GenericRelation was previously a fake\n         # m2m field).\n-        for f in opts.many_to_many + opts.virtual_fields:\n+        for f in chain(opts.many_to_many, opts.virtual_fields):\n             if not hasattr(f, 'save_form_data'):\n                 continue\n             if fields and f.name not in fields:\n@@ -127,7 +128,7 @@ def model_to_dict(instance, fields=None, exclude=None):\n     from django.db.models.fields.related import ManyToManyField\n     opts = instance._meta\n     data = {}\n-    for f in opts.concrete_fields + opts.virtual_fields + opts.many_to_many:\n+    for f in chain(opts.concrete_fields, opts.virtual_fields, opts.many_to_many):\n         if not getattr(f, 'editable', False):\n             continue\n         if fields and f.name not in fields:\n@@ -186,7 +187,7 @@ def fields_for_model(model, fields=None, exclude=None, widgets=None,\n     from django.db.models.fields import Field as ModelField\n     sortable_virtual_fields = [f for f in opts.virtual_fields\n                                if isinstance(f, ModelField)]\n-    for f in sorted(opts.concrete_fields + sortable_virtual_fields + opts.many_to_many):\n+    for f in sorted(chain(opts.concrete_fields, sortable_virtual_fields, opts.many_to_many)):\n         if not getattr(f, 'editable', False):\n             continue\n         if fields is not None and f.name not in fields:\ndiff --git a/docs/howto/custom-model-fields.txt b/docs/howto/custom-model-fields.txt\nindex 568831523c52..647474222229 100644\n--- a/docs/howto/custom-model-fields.txt\n+++ b/docs/howto/custom-model-fields.txt\n@@ -217,9 +217,9 @@ The ``Field.__init__()`` method takes the following parameters:\n * :attr:`~django.db.models.Field.db_tablespace`: Only for index creation, if the\n   backend supports :doc:`tablespaces </topics/db/tablespaces>`. You can usually\n   ignore this option.\n-* ``auto_created``: ``True`` if the field was automatically created, as for the\n-  :class:`~django.db.models.OneToOneField` used by model inheritance. For\n-  advanced use only.\n+* :attr:`~django.db.models.Field.auto_created`: ``True`` if the field was\n+  automatically created, as for the :class:`~django.db.models.OneToOneField`\n+  used by model inheritance. For advanced use only.\n \n All of the options without an explanation in the above list have the same\n meaning they do for normal Django fields. See the :doc:`field documentation\ndiff --git a/docs/internals/deprecation.txt b/docs/internals/deprecation.txt\nindex b74f550ba866..e2c89d360b60 100644\n--- a/docs/internals/deprecation.txt\n+++ b/docs/internals/deprecation.txt\n@@ -56,6 +56,19 @@ details on these changes.\n \n * ``django.template.resolve_variable`` will be removed.\n \n+* The following private APIs will be removed from\n+  :class:`django.db.models.options.Options` (``Model._meta``):\n+\n+  * ``get_field_by_name()``\n+  * ``get_all_field_names()``\n+  * ``get_fields_with_model()``\n+  * ``get_concrete_fields_with_model()``\n+  * ``get_m2m_with_model()``\n+  * ``get_all_related_objects()``\n+  * ``get_all_related_objects_with_model()``\n+  * ``get_all_related_many_to_many_objects()``\n+  * ``get_all_related_m2m_objects_with_model()``\n+\n * The ``error_message`` argument of ``django.forms.RegexField`` will be removed.\n \n * The ``unordered_list`` filter will no longer support old style lists.\ndiff --git a/docs/ref/models/fields.txt b/docs/ref/models/fields.txt\nindex 35997801fb08..bc214fae00c4 100644\n--- a/docs/ref/models/fields.txt\n+++ b/docs/ref/models/fields.txt\n@@ -1790,3 +1790,95 @@ Field API reference\n \n         This method must be added to fields prior to 1.7 to migrate its data\n         using :doc:`/topics/migrations`.\n+\n+.. _model-field-attributes:\n+\n+=========================\n+Field attribute reference\n+=========================\n+\n+.. versionadded:: 1.8\n+\n+Every ``Field`` instance contains several attributes that allow\n+introspecting its behavior. Use these attributes instead of ``isinstance``\n+checks when you need to write code that depends on a field's functionality.\n+These attributes can be used together with the :ref:`Model._meta API\n+<model-meta-field-api>` to narrow down a search for specific field types.\n+Custom model fields should implement these flags.\n+\n+Attributes for fields\n+=====================\n+\n+.. attribute:: Field.auto_created\n+\n+     Boolean flag that indicates if the field was automatically created, such\n+     as the ``OneToOneField`` used by model inheritance.\n+\n+.. attribute:: Field.concrete\n+\n+    Boolean flag that indicates if the field has a database column associated\n+    with it.\n+\n+.. attribute:: Field.hidden\n+\n+    Boolean flag that indicates if a field is used to back another non-hidden\n+    field's functionality (e.g. the ``content_type`` and ``object_id`` fields\n+    that make up a ``GenericForeignKey``). The ``hidden`` flag is used to\n+    distinguish what constitutes the public subset of fields on the model from\n+    all the fields on the model.\n+\n+    .. note::\n+\n+        :meth:`Options.get_fields()\n+        <django.db.models.options.Options.get_fields()>`\n+        excludes hidden fields by default. Pass in ``include_hidden=True`` to\n+        return hidden fields in the results.\n+\n+.. attribute:: Field.is_relation\n+\n+    Boolean flag that indicates if a field contains references to one or\n+    more other models for its functionality (e.g. ``ForeignKey``,\n+    ``ManyToManyField``, ``OneToOneField``, etc.).\n+\n+.. attribute:: Field.model\n+\n+    Returns the model on which the field is defined. If a field is defined on\n+    a superclass of a model, ``model`` will refer to the superclass, not the\n+    class of the instance.\n+\n+Attributes for fields with relations\n+====================================\n+\n+These attributes are used to query for the cardinality and other details of a\n+relation. These attribute are present on all fields; however, they will only\n+have meaningful values if the field is a relation type\n+(:attr:`Field.is_relation=True <Field.is_relation>`).\n+\n+.. attribute:: Field.one_to_many\n+\n+    Boolean flag that is ``True`` if the field has a one-to-many relation, such\n+    as a ``ForeignKey``; ``False`` otherwise.\n+\n+.. attribute:: Field.one_to_one\n+\n+    Boolean flag that is ``True`` if the field has a one-to-one relation, such\n+    as a ``OneToOneField``; ``False`` otherwise.\n+\n+.. attribute:: Field.many_to_many\n+\n+    Boolean flag that is ``True`` if the field has a many-to-many relation;\n+    ``False`` otherwise. The only field included with Django where this is\n+    ``True`` is ``ManyToManyField``.\n+\n+.. attribute:: Field.many_to_one\n+\n+    Boolean flag that is ``True`` if the field has a many-to-one relation, such\n+    as a ``GenericRelation`` or the reverse of a ``ForeignKey``; ``False``\n+    otherwise.\n+\n+.. attribute:: Field.related_model\n+\n+    Points to the model the field relates to. For example, ``Author`` in\n+    ``ForeignKey(Author)``. If a field has a generic relation (such as a\n+    ``GenericForeignKey`` or a ``GenericRelation``) then ``related_model``\n+    will be ``None``.\ndiff --git a/docs/ref/models/index.txt b/docs/ref/models/index.txt\nindex d1d3680dd871..284743e7d09a 100644\n--- a/docs/ref/models/index.txt\n+++ b/docs/ref/models/index.txt\n@@ -8,6 +8,7 @@ Model API reference. For introductory material, see :doc:`/topics/db/models`.\n    :maxdepth: 1\n \n    fields\n+   meta\n    relations\n    class\n    options\ndiff --git a/docs/ref/models/meta.txt b/docs/ref/models/meta.txt\nnew file mode 100644\nindex 000000000000..c00888c79dea\n--- /dev/null\n+++ b/docs/ref/models/meta.txt\n@@ -0,0 +1,289 @@\n+===================\n+Model ``_meta`` API\n+===================\n+\n+.. module:: django.db.models.options\n+   :synopsis: Model meta-class layer\n+\n+.. class:: Options\n+\n+The model ``_meta`` API is at the core of the Django ORM. It enables other\n+parts of the system such as lookups, queries, forms, and the admin to\n+understand the capabilities of each model. The API is accessible through\n+the ``_meta`` attribute of each model class, which is an instance of an\n+``django.db.models.options.Options`` object.\n+\n+Methods that it provides can be used to:\n+\n+* Retrieve all field instances of a model\n+* Retrieve a single field instance of a model by name\n+\n+.. versionchanged:: 1.8\n+\n+    The Model ``_meta`` API has always existed as a Django internal, but\n+    wasn't formally documented and supported. As part of the effort to\n+    make this API public, some of the already existing API entry points\n+    have changed slightly. A :ref:`migration guide <migrating-old-meta-api>`\n+    has been provided to assist in converting your code to use the new,\n+    official API.\n+\n+.. _model-meta-field-api:\n+\n+Field access API\n+================\n+\n+Retrieving a single field instance of a model by name\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. method:: Options.get_field(field_name)\n+\n+    Returns the field instance given a name of a field.\n+\n+    ``field_name`` can be the name of a field on the model, a field\n+    on an abstract or inherited model, or a field defined on another\n+    model that points to the model. In the latter case, the ``field_name``\n+    will be the ``related_name`` defined by the user or the name automatically\n+    generated by Django itself.\n+\n+    :attr:`Hidden fields <django.db.models.Field.hidden>` cannot be retrieved\n+    by name.\n+\n+    If a field with the given name is not found a\n+    :class:`~django.core.exceptions.FieldDoesNotExist` exception will be\n+    raised.\n+\n+    .. code-block:: python\n+\n+        >>> from django.contrib.auth.models import User\n+\n+        # A field on the model\n+        >>> User._meta.get_field('username')\n+        <django.db.models.fields.CharField: username>\n+\n+        # A field from another model that has a relation with the current model\n+        >>> User._meta.get_field('logentry')\n+        <ManyToOneRel: admin.logentry>\n+\n+        # A non existent field\n+        >>> User._meta.get_field('does_not_exist')\n+        Traceback (most recent call last):\n+            ...\n+        FieldDoesNotExist: User has no field named 'does_not_exist'\n+\n+    .. deprecated:: 1.8\n+\n+        :meth:`Options.get_field()` previously accepted a ``many_to_many``\n+        parameter which could be set to ``False`` to avoid searching\n+        ``ManyToManyField``\\s. The old behavior has been preserved for\n+        backwards compatibility; however, the parameter and this behavior\n+        has been deprecated.\n+\n+        If you wish to filter out ``ManyToManyField``\\s, you can inspect the\n+        :attr:`Field.many_to_many <django.db.models.Field.many_to_many>`\n+        attribute after calling ``get_field()``.\n+\n+Retrieving all field instances of a model\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+.. method:: Options.get_fields(include_parents=True, include_hidden=False)\n+\n+    .. versionadded:: 1.8\n+\n+    Returns a tuple of fields associated with a model. ``get_fields()`` accepts\n+    two parameters that can be used to control which fields are returned:\n+\n+    ``include_parents``\n+        ``True`` by default. Recursively includes fields defined on parent\n+        classes. If set to ``False``, ``get_fields()`` will only search for\n+        fields declared directly on the current model. Fields from models that\n+        directly inherit from abstract models or proxy classes are considered\n+        to be local, not on the parent.\n+\n+    ``include_hidden``\n+        ``False`` by default. If set to ``True``, ``get_fields()`` will include\n+        fields that are used to back other field's functionality. This will\n+        also include any fields that have a ``related_name`` (such\n+        as :class:`~django.db.models.ManyToManyField`, or\n+        :class:`~django.db.models.ForeignKey`) that start with a \"+\".\n+\n+    .. code-block:: python\n+\n+        >>> from django.contrib.auth.models import User\n+        >>> User._meta.get_fields()\n+        (<ManyToOneRel: admin.logentry>,\n+         <django.db.models.fields.AutoField: id>,\n+         <django.db.models.fields.CharField: password>,\n+         <django.db.models.fields.DateTimeField: last_login>,\n+         <django.db.models.fields.BooleanField: is_superuser>,\n+         <django.db.models.fields.CharField: username>,\n+         <django.db.models.fields.CharField: first_name>,\n+         <django.db.models.fields.CharField: last_name>,\n+         <django.db.models.fields.EmailField: email>,\n+         <django.db.models.fields.BooleanField: is_staff>,\n+         <django.db.models.fields.BooleanField: is_active>,\n+         <django.db.models.fields.DateTimeField: date_joined>,\n+         <django.db.models.fields.related.ManyToManyField: groups>,\n+         <django.db.models.fields.related.ManyToManyField: user_permissions>)\n+\n+        # Also include hidden fields.\n+        >>> User._meta.get_fields(include_hidden=True)\n+        (<ManyToOneRel: auth.user_groups>,\n+         <ManyToOneRel: auth.user_user_permissions>,\n+         <ManyToOneRel: admin.logentry>,\n+         <django.db.models.fields.AutoField: id>,\n+         <django.db.models.fields.CharField: password>,\n+         <django.db.models.fields.DateTimeField: last_login>,\n+         <django.db.models.fields.BooleanField: is_superuser>,\n+         <django.db.models.fields.CharField: username>,\n+         <django.db.models.fields.CharField: first_name>,\n+         <django.db.models.fields.CharField: last_name>,\n+         <django.db.models.fields.EmailField: email>,\n+         <django.db.models.fields.BooleanField: is_staff>,\n+         <django.db.models.fields.BooleanField: is_active>,\n+         <django.db.models.fields.DateTimeField: date_joined>,\n+         <django.db.models.fields.related.ManyToManyField: groups>,\n+         <django.db.models.fields.related.ManyToManyField: user_permissions>)\n+\n+.. _migrating-old-meta-api:\n+\n+Migrating from the old API\n+==========================\n+\n+As part of the formalization of the ``Model._meta`` API (from the\n+:class:`django.db.models.options.Options` class), a number of methods and\n+properties have been deprecated and will be removed in Django 2.0.\n+\n+These old APIs can be replicated by either:\n+\n+* invoking :meth:`Options.get_field()\n+  <django.db.models.options.Options.get_field()>`, or;\n+\n+* invoking :meth:`Options.get_fields()\n+  <django.db.models.options.Options.get_fields()>` to retrieve a list of all\n+  fields, and then filtering this list using the :ref:`field attributes\n+  <model-field-attributes>` that describe (or retrieve, in the case of\n+  ``_with_model`` variants) the properties of the desired fields.\n+\n+Although it would be possible to provide replacements for the old API, a\n+simple drop-in replacement probably won't be the best approach in practice.\n+Taking the time to refactor any field loops to make better use of the new\n+API - and possibly include fields that were previously excluded - will\n+almost certainly result in better code.\n+\n+Assuming you have a model named ``MyModel``, the following substitutions\n+can be made to convert your code to the new API:\n+\n+* ``MyModel._meta.get_field(name)``::\n+\n+      f = MyModel._meta.get_field(name)\n+\n+  then check if:\n+\n+  - ``f.auto_created == False``, because the new ``get_field()``\n+    API will find \"reverse\" relations), and:\n+\n+  - ``f.is_relation and f.related_model is None``, because the new\n+    ``get_field()`` API will find\n+    :class:`~django.contrib.contenttypes.fields.GenericForeignKey` relations;\n+\n+* ``MyModel._meta.get_field_by_name(name)``:\n+\n+  ``get_field_by_name()`` returned four values:\n+  ``(field, model, direct,  m2m)``:\n+\n+  - ``field`` can be found by ``MyModel._meta.get_field(name)``\n+\n+  - ``model`` can be found through the\n+    :attr:`~django.db.models.Field.model` attribute on the field.\n+\n+  - ``direct`` can be found by: ``not field.auto_created or field.concrete``\n+\n+    The :attr:`~django.db.models.Field.auto_created` check excludes\n+    all \"forward\" and \"reverse\" relations that are created by Django, but\n+    this also includes ``AutoField`` and ``OneToOneField`` on proxy models.\n+    We avoid filtering out these attributes using the\n+    :attr:`concrete <django.db.models.Field.concrete>` attribute.\n+\n+  - ``m2m`` can be found through the\n+    :attr:`~django.db.models.Field.many_to_many` attribute on the field.\n+\n+* ``MyModel._meta.get_fields_with_model()``::\n+\n+      [\n+          (f, f.model if f.model != MyModel else None)\n+          for f in MyModel._meta.get_fields()\n+          if not f.is_relation\n+              or f.one_to_one\n+              or (f.one_to_many and f.related_model)\n+      ]\n+\n+* ``MyModel._meta.get_concrete_fields_with_model()``::\n+\n+      [\n+          (f, f.model if f.model != MyModel else None)\n+          for f in MyModel._meta.get_fields()\n+          if f.concrete and (\n+              not f.is_relation\n+              or f.one_to_one\n+              or (f.one_to_many and f.related_model)\n+          )\n+      ]\n+\n+* ``MyModel._meta.get_m2m_with_model()``::\n+\n+      [\n+          (f, f.model if f.model != MyModel else None)\n+          for f in MyModel._meta.get_fields()\n+          if f.many_to_many and not f.auto_created\n+      ]\n+\n+* ``MyModel._meta.get_all_related_objects()``::\n+\n+      [\n+          f for f in MyModel._meta.get_fields()\n+          if f.many_to_one and f.auto_created\n+      ]\n+\n+* ``MyModel._meta.get_all_related_objects_with_model()``::\n+\n+      [\n+          (f, f.model if f.model != MyModel else None)\n+          for f in MyModel._meta.get_fields()\n+          if f.many_to_one and f.auto_created\n+      ]\n+\n+* ``MyModel._meta.get_all_related_many_to_many_objects()``::\n+\n+      [\n+          f for f in MyModel._meta.get_fields(include_hidden=True)\n+          if f.many_to_many and f.auto_created\n+      ]\n+\n+* ``MyModel._meta.get_all_related_m2m_objects_with_model()``::\n+\n+      [\n+          (f, f.model if f.model != MyModel else None)\n+          for f in MyModel._meta.get_fields(include_hidden=True)\n+          if f.many_to_many and f.auto_created\n+      ]\n+\n+* ``MyModel._meta.get_all_field_names()``::\n+\n+      from itertools import chain\n+      list(set(chain.from_iterable(\n+          (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)\n+          for field in MyModel._meta.get_fields()\n+          # For complete backwards compatibility, you may want to exclude\n+          # GenericForeignKey from the results.\n+          if not (field.one_to_many and field.related_model is None)\n+      )))\n+\n+  This provides a 100% backwards compatible replacement, ensuring that both\n+  field names and attribute names ``ForeignKey``\\s are included, but fields\n+  associated with``GenericForeignKey``\\s are not. A simpler version would be::\n+\n+      [f.name for f in MyModel._meta.get_fields()]\n+\n+  While this isn't 100% backwards compatible, it may be sufficient in many\n+  situations.\n+\ndiff --git a/docs/releases/1.8.txt b/docs/releases/1.8.txt\nindex 460d637d9def..457850159f59 100644\n--- a/docs/releases/1.8.txt\n+++ b/docs/releases/1.8.txt\n@@ -29,6 +29,22 @@ Like Django 1.7, Django 1.8 requires Python 2.7 or above, though we\n What's new in Django 1.8\n ========================\n \n+``Model._meta`` API\n+~~~~~~~~~~~~~~~~~~~\n+\n+Django now has a formalized API for :doc:`Model._meta </ref/models/meta>`,\n+providing an officially supported way to :ref:`retrieve fields\n+<model-meta-field-api>` and filter fields based on their :ref:`attributes\n+<model-field-attributes>`.\n+\n+The ``Model._meta`` object has been part of Django since the days of pre-0.96\n+\"Magic Removal\" -- it just wasn't an official, stable API. In recognition of\n+this, we've endeavored to maintain backwards-compatibility with the old\n+API endpoint where possible. However, API endpoints that aren't part of the\n+new official API have been deprecated and will eventually be removed. A\n+:ref:`guide to migrating from the old API to the new API\n+<migrating-old-meta-api>` has been provided.\n+\n Security enhancements\n ~~~~~~~~~~~~~~~~~~~~~\n \n@@ -998,6 +1014,26 @@ Miscellaneous\n Features deprecated in 1.8\n ==========================\n \n+Selected methods in ``django.db.models.options.Options``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+As part of the formalization of the ``Model._meta`` API (from the\n+:class:`django.db.models.options.Options` class), a number of methods have been\n+deprecated and will be removed in in Django 2.0:\n+\n+* ``get_all_field_names()``\n+* ``get_all_related_objects()``\n+* ``get_all_related_objects_with_model()``\n+* ``get_all_related_many_to_many_objects()``\n+* ``get_all_related_m2m_objects_with_model()``\n+* ``get_concrete_fields_with_model()``\n+* ``get_field_by_name()``\n+* ``get_fields_with_model()``\n+* ``get_m2m_with_model()``\n+\n+A :ref:`migration guide <migrating-old-meta-api>` has been provided to assist\n+in converting your code from the old API to the new, official API.\n+\n Loading ``cycle`` and ``firstof`` template tags from ``future`` library\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \ndiff --git a/tests/backends/tests.py b/tests/backends/tests.py\nindex 7fbb57ba0caa..9eeb90335c2e 100644\n--- a/tests/backends/tests.py\n+++ b/tests/backends/tests.py\n@@ -1020,7 +1020,7 @@ def test_many_to_many(self):\n         self.assertEqual(models.Object.objects.count(), 2)\n         self.assertEqual(obj.related_objects.count(), 1)\n \n-        intermediary_model = models.Object._meta.get_field_by_name(\"related_objects\")[0].rel.through\n+        intermediary_model = models.Object._meta.get_field(\"related_objects\").rel.through\n         intermediary_model.objects.create(from_object_id=obj.id, to_object_id=12345)\n         self.assertEqual(obj.related_objects.count(), 1)\n         self.assertEqual(intermediary_model.objects.count(), 2)\ndiff --git a/tests/basic/tests.py b/tests/basic/tests.py\nindex 74515047f557..3ba94d058d14 100644\n--- a/tests/basic/tests.py\n+++ b/tests/basic/tests.py\n@@ -776,7 +776,7 @@ def test_refresh_no_fields(self):\n \n class TestRelatedObjectDeprecation(TestCase):\n     def test_field_related_deprecation(self):\n-        field = SelfRef._meta.get_field_by_name('selfref')[0]\n+        field = SelfRef._meta.get_field('selfref')\n         with warnings.catch_warnings(record=True) as warns:\n             warnings.simplefilter('always')\n             self.assertIsInstance(field.related, ForeignObjectRel)\ndiff --git a/tests/fixtures/tests.py b/tests/fixtures/tests.py\nindex adef8ec5e5be..0201693dba6f 100644\n--- a/tests/fixtures/tests.py\n+++ b/tests/fixtures/tests.py\n@@ -3,6 +3,7 @@\n import os\n import warnings\n \n+from django.apps import apps\n from django.contrib.sites.models import Site\n from django.core import management\n from django.db import connection, IntegrityError\n@@ -76,6 +77,7 @@ def test_initial_data(self):\n         ])\n \n     def test_loading_and_dumping(self):\n+        apps.clear_cache()\n         Site.objects.all().delete()\n         # Load fixture 1. Single JSON file, with two objects.\n         management.call_command('loaddata', 'fixture1.json', verbosity=0)\ndiff --git a/tests/generic_relations/tests.py b/tests/generic_relations/tests.py\nindex e7f7b02f5b2d..c6230b057e18 100644\n--- a/tests/generic_relations/tests.py\n+++ b/tests/generic_relations/tests.py\n@@ -403,7 +403,8 @@ def test_update_or_create_defaults(self):\n         self.assertEqual(tag.content_object.id, diamond.id)\n \n     def test_query_content_type(self):\n-        with six.assertRaisesRegex(self, FieldError, \"^Cannot resolve keyword 'content_object' into field.\"):\n+        msg = \"Field 'content_object' does not generate an automatic reverse relation\"\n+        with self.assertRaisesMessage(FieldError, msg):\n             TaggedItem.objects.get(content_object='')\n \n \ndiff --git a/tests/generic_relations_regress/tests.py b/tests/generic_relations_regress/tests.py\nindex 65afa9f298d6..88243bade0ce 100644\n--- a/tests/generic_relations_regress/tests.py\n+++ b/tests/generic_relations_regress/tests.py\n@@ -260,7 +260,7 @@ def test_editable_generic_rel(self):\n         form = GenericRelationForm({'links': None})\n         self.assertTrue(form.is_valid())\n         form.save()\n-        links = HasLinkThing._meta.get_field_by_name('links')[0]\n+        links = HasLinkThing._meta.get_field('links')\n         self.assertEqual(links.save_form_data_calls, 1)\n \n     def test_ticket_22998(self):\ndiff --git a/tests/m2m_and_m2o/tests.py b/tests/m2m_and_m2o/tests.py\nindex e950a839d21b..2317f62300ec 100644\n--- a/tests/m2m_and_m2o/tests.py\n+++ b/tests/m2m_and_m2o/tests.py\n@@ -5,6 +5,12 @@\n \n \n class RelatedObjectTests(TestCase):\n+\n+    def test_related_objects_have_name_attribute(self):\n+        for field_name in ('test_issue_client', 'test_issue_cc'):\n+            obj = User._meta.get_field(field_name)\n+            self.assertEqual(field_name, obj.field.related_query_name())\n+\n     def test_m2m_and_m2o(self):\n         r = User.objects.create(username=\"russell\")\n         g = User.objects.create(username=\"gustav\")\ndiff --git a/tests/many_to_one/tests.py b/tests/many_to_one/tests.py\nindex 0195e5d2d8b7..e82d8eefe608 100644\n--- a/tests/many_to_one/tests.py\n+++ b/tests/many_to_one/tests.py\n@@ -437,11 +437,11 @@ def test_values_list_exception(self):\n         expected_message = \"Cannot resolve keyword 'notafield' into field. Choices are: %s\"\n \n         self.assertRaisesMessage(FieldError,\n-                                 expected_message % ', '.join(Reporter._meta.get_all_field_names()),\n+                                 expected_message % ', '.join(sorted(f.name for f in Reporter._meta.get_fields())),\n                                  Article.objects.values_list,\n                                  'reporter__notafield')\n         self.assertRaisesMessage(FieldError,\n-                                 expected_message % ', '.join(['EXTRA'] + Article._meta.get_all_field_names()),\n+                                 expected_message % ', '.join(['EXTRA'] + sorted(f.name for f in Article._meta.get_fields())),\n                                  Article.objects.extra(select={'EXTRA': 'EXTRA_SELECT'}).values_list,\n                                  'notafield')\n \ndiff --git a/tests/migrations/test_state.py b/tests/migrations/test_state.py\nindex 6f510813afb6..e069649810eb 100644\n--- a/tests/migrations/test_state.py\n+++ b/tests/migrations/test_state.py\n@@ -202,8 +202,8 @@ def test_render(self):\n         ))\n \n         new_apps = project_state.apps\n-        self.assertEqual(new_apps.get_model(\"migrations\", \"Tag\")._meta.get_field_by_name(\"name\")[0].max_length, 100)\n-        self.assertEqual(new_apps.get_model(\"migrations\", \"Tag\")._meta.get_field_by_name(\"hidden\")[0].null, False)\n+        self.assertEqual(new_apps.get_model(\"migrations\", \"Tag\")._meta.get_field(\"name\").max_length, 100)\n+        self.assertEqual(new_apps.get_model(\"migrations\", \"Tag\")._meta.get_field(\"hidden\").null, False)\n \n         self.assertEqual(len(new_apps.get_model(\"migrations\", \"SubTag\")._meta.local_fields), 2)\n \ndiff --git a/tests/model_fields/models.py b/tests/model_fields/models.py\nindex 2a7bebc9ddf7..8927ae8830b9 100644\n--- a/tests/model_fields/models.py\n+++ b/tests/model_fields/models.py\n@@ -8,6 +8,11 @@\n     Image = None\n \n from django.core.files.storage import FileSystemStorage\n+from django.contrib.contenttypes.fields import GenericForeignKey, GenericRelation\n+from django.contrib.contenttypes.models import ContentType\n+from django.db.models.fields.related import (\n+    ForeignObject, ForeignKey, ManyToManyField, OneToOneField,\n+)\n from django.db import models\n from django.db.models.fields.files import ImageFieldFile, ImageField\n from django.utils import six\n@@ -295,6 +300,52 @@ class PersonTwoImages(models.Model):\n                                   height_field='headshot_height',\n                                   width_field='headshot_width')\n \n+\n+class AllFieldsModel(models.Model):\n+    big_integer = models.BigIntegerField()\n+    binary = models.BinaryField()\n+    boolean = models.BooleanField(default=False)\n+    char = models.CharField(max_length=10)\n+    csv = models.CommaSeparatedIntegerField(max_length=10)\n+    date = models.DateField()\n+    datetime = models.DateTimeField()\n+    decimal = models.DecimalField(decimal_places=2, max_digits=2)\n+    duration = models.DurationField()\n+    email = models.EmailField()\n+    file_path = models.FilePathField()\n+    floatf = models.FloatField()\n+    integer = models.IntegerField()\n+    ip_address = models.IPAddressField()\n+    generic_ip = models.GenericIPAddressField()\n+    null_boolean = models.NullBooleanField()\n+    positive_integer = models.PositiveIntegerField()\n+    positive_small_integer = models.PositiveSmallIntegerField()\n+    slug = models.SlugField()\n+    small_integer = models.SmallIntegerField()\n+    text = models.TextField()\n+    time = models.TimeField()\n+    url = models.URLField()\n+    uuid = models.UUIDField()\n+\n+    fo = ForeignObject(\n+        'self',\n+        from_fields=['abstract_non_concrete_id'],\n+        to_fields=['id'],\n+        related_name='reverse'\n+    )\n+    fk = ForeignKey(\n+        'self',\n+        related_name='reverse2'\n+    )\n+    m2m = ManyToManyField('self')\n+    oto = OneToOneField('self')\n+\n+    object_id = models.PositiveIntegerField()\n+    content_type = models.ForeignKey(ContentType)\n+    gfk = GenericForeignKey()\n+    gr = GenericRelation(DataModel)\n+\n+\n ###############################################################################\n \n \ndiff --git a/tests/model_fields/test_field_flags.py b/tests/model_fields/test_field_flags.py\nnew file mode 100644\nindex 000000000000..08a57db5019c\n--- /dev/null\n+++ b/tests/model_fields/test_field_flags.py\n@@ -0,0 +1,220 @@\n+from django import test\n+\n+from django.contrib.contenttypes.fields import (\n+    GenericForeignKey, GenericRelation,\n+)\n+from django.db import models\n+from django.db.models.fields.related import (\n+    ForeignObject, ForeignKey, OneToOneField, ManyToManyField,\n+    ManyToOneRel, ForeignObjectRel,\n+)\n+\n+from .models import AllFieldsModel\n+\n+\n+NON_CONCRETE_FIELDS = (\n+    ForeignObject,\n+    GenericForeignKey,\n+    GenericRelation,\n+)\n+\n+NON_EDITABLE_FIELDS = (\n+    models.BinaryField,\n+    GenericForeignKey,\n+    GenericRelation,\n+)\n+\n+RELATION_FIELDS = (\n+    ForeignKey,\n+    ForeignObject,\n+    ManyToManyField,\n+    OneToOneField,\n+    GenericForeignKey,\n+    GenericRelation,\n+)\n+\n+ONE_TO_MANY_CLASSES = {\n+    ForeignObject,\n+    ForeignKey,\n+    GenericForeignKey,\n+}\n+\n+MANY_TO_ONE_CLASSES = {\n+    ForeignObjectRel,\n+    ManyToOneRel,\n+    GenericRelation,\n+}\n+\n+MANY_TO_MANY_CLASSES = {\n+    ManyToManyField,\n+}\n+\n+ONE_TO_ONE_CLASSES = {\n+    OneToOneField,\n+}\n+\n+FLAG_PROPERTIES = (\n+    'concrete',\n+    'editable',\n+    'is_relation',\n+    'model',\n+    'hidden',\n+    'one_to_many',\n+    'many_to_one',\n+    'many_to_many',\n+    'one_to_one',\n+    'related_model',\n+)\n+\n+FLAG_PROPERTIES_FOR_RELATIONS = (\n+    'one_to_many',\n+    'many_to_one',\n+    'many_to_many',\n+    'one_to_one',\n+)\n+\n+\n+class FieldFlagsTests(test.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        super(FieldFlagsTests, cls).setUpClass()\n+        cls.fields = (\n+            list(AllFieldsModel._meta.fields) +\n+            list(AllFieldsModel._meta.virtual_fields)\n+        )\n+\n+        cls.all_fields = (\n+            cls.fields +\n+            list(AllFieldsModel._meta.many_to_many) +\n+            list(AllFieldsModel._meta.virtual_fields)\n+        )\n+\n+        cls.fields_and_reverse_objects = (\n+            cls.all_fields +\n+            list(AllFieldsModel._meta.related_objects)\n+        )\n+\n+    def test_each_field_should_have_a_concrete_attribute(self):\n+        self.assertTrue(all(f.concrete.__class__ == bool for f in self.fields))\n+\n+    def test_each_field_should_have_an_editable_attribute(self):\n+        self.assertTrue(all(f.editable.__class__ == bool for f in self.all_fields))\n+\n+    def test_each_field_should_have_a_has_rel_attribute(self):\n+        self.assertTrue(all(f.is_relation.__class__ == bool for f in self.all_fields))\n+\n+    def test_each_object_should_have_auto_created(self):\n+        self.assertTrue(\n+            all(f.auto_created.__class__ == bool\n+            for f in self.fields_and_reverse_objects)\n+        )\n+\n+    def test_non_concrete_fields(self):\n+        for field in self.fields:\n+            if type(field) in NON_CONCRETE_FIELDS:\n+                self.assertFalse(field.concrete)\n+            else:\n+                self.assertTrue(field.concrete)\n+\n+    def test_non_editable_fields(self):\n+        for field in self.all_fields:\n+            if type(field) in NON_EDITABLE_FIELDS:\n+                self.assertFalse(field.editable)\n+            else:\n+                self.assertTrue(field.editable)\n+\n+    def test_related_fields(self):\n+        for field in self.all_fields:\n+            if type(field) in RELATION_FIELDS:\n+                self.assertTrue(field.is_relation)\n+            else:\n+                self.assertFalse(field.is_relation)\n+\n+    def test_field_names_should_always_be_available(self):\n+        for field in self.fields_and_reverse_objects:\n+            self.assertTrue(field.name)\n+\n+    def test_all_field_types_should_have_flags(self):\n+        for field in self.fields_and_reverse_objects:\n+            for flag in FLAG_PROPERTIES:\n+                self.assertTrue(hasattr(field, flag), \"Field %s does not have flag %s\" % (field, flag))\n+            if field.is_relation:\n+                true_cardinality_flags = sum(\n+                    getattr(field, flag) is True\n+                    for flag in FLAG_PROPERTIES_FOR_RELATIONS\n+                )\n+                # If the field has a relation, there should be only one of the\n+                # 4 cardinality flags available.\n+                self.assertEqual(1, true_cardinality_flags)\n+\n+    def test_cardinality_m2m(self):\n+        m2m_type_fields = (\n+            f for f in self.all_fields\n+            if f.is_relation and f.many_to_many\n+        )\n+        # Test classes are what we expect\n+        self.assertEqual(MANY_TO_MANY_CLASSES, {f.__class__ for f in m2m_type_fields})\n+\n+        # Ensure all m2m reverses are m2m\n+        for field in m2m_type_fields:\n+            reverse_field = field.rel\n+            self.assertTrue(reverse_field.is_relation)\n+            self.assertTrue(reverse_field.many_to_many)\n+            self.assertTrue(reverse_field.related_model)\n+\n+    def test_cardinality_o2m(self):\n+        o2m_type_fields = [\n+            f for f in self.fields_and_reverse_objects\n+            if f.is_relation and f.one_to_many\n+        ]\n+        # Test classes are what we expect\n+        self.assertEqual(ONE_TO_MANY_CLASSES, {f.__class__ for f in o2m_type_fields})\n+\n+        # Ensure all o2m reverses are m2o\n+        for field in o2m_type_fields:\n+            if field.concrete:\n+                reverse_field = field.rel\n+                self.assertTrue(reverse_field.is_relation and reverse_field.many_to_one)\n+\n+    def test_cardinality_m2o(self):\n+        m2o_type_fields = [\n+            f for f in self.fields_and_reverse_objects\n+            if f.is_relation and f.many_to_one\n+        ]\n+        # Test classes are what we expect\n+        self.assertEqual(MANY_TO_ONE_CLASSES, {f.__class__ for f in m2o_type_fields})\n+\n+        # Ensure all m2o reverses are o2m\n+        for obj in m2o_type_fields:\n+            if hasattr(obj, 'field'):\n+                reverse_field = obj.field\n+                self.assertTrue(reverse_field.is_relation and reverse_field.one_to_many)\n+\n+    def test_cardinality_o2o(self):\n+        o2o_type_fields = [\n+            f for f in self.all_fields\n+            if f.is_relation and f.one_to_one\n+        ]\n+        # Test classes are what we expect\n+        self.assertEqual(ONE_TO_ONE_CLASSES, {f.__class__ for f in o2o_type_fields})\n+\n+        # Ensure all o2o reverses are o2o\n+        for obj in o2o_type_fields:\n+            if hasattr(obj, 'field'):\n+                reverse_field = obj.field\n+                self.assertTrue(reverse_field.is_relation and reverse_field.one_to_one)\n+\n+    def test_hidden_flag(self):\n+        incl_hidden = set(AllFieldsModel._meta.get_fields(include_hidden=True))\n+        no_hidden = set(AllFieldsModel._meta.get_fields())\n+        fields_that_should_be_hidden = (incl_hidden - no_hidden)\n+        for f in incl_hidden:\n+            self.assertEqual(f in fields_that_should_be_hidden, f.hidden)\n+\n+    def test_model_and_reverse_model_should_equal_on_relations(self):\n+        for field in AllFieldsModel._meta.get_fields():\n+            is_concrete_forward_field = field.concrete and field.related_model\n+            if is_concrete_forward_field:\n+                reverse_field = field.rel\n+                self.assertEqual(field.model, reverse_field.related_model)\n+                self.assertEqual(field.related_model, reverse_field.model)\ndiff --git a/tests/model_fields/tests.py b/tests/model_fields/tests.py\nindex e63cc007dd96..d891645ca0c3 100644\n--- a/tests/model_fields/tests.py\n+++ b/tests/model_fields/tests.py\n@@ -198,7 +198,7 @@ class FKUniqueTrue(models.Model):\n         self.assertEqual(warnings, expected_warnings)\n \n     def test_related_name_converted_to_text(self):\n-        rel_name = Bar._meta.get_field_by_name('a')[0].rel.related_name\n+        rel_name = Bar._meta.get_field('a').rel.related_name\n         self.assertIsInstance(rel_name, six.text_type)\n \n \ndiff --git a/tests/model_meta/results.py b/tests/model_meta/results.py\nnew file mode 100644\nindex 000000000000..1efc49ed235b\n--- /dev/null\n+++ b/tests/model_meta/results.py\n@@ -0,0 +1,796 @@\n+from .models import (\n+    AbstractPerson, BasePerson, Person, Relating, Relation,\n+)\n+\n+TEST_RESULTS = {\n+    'get_all_field_names': {\n+        Person: [\n+            'baseperson_ptr',\n+            'baseperson_ptr_id',\n+            'content_type_abstract',\n+            'content_type_abstract_id',\n+            'content_type_base',\n+            'content_type_base_id',\n+            'content_type_concrete',\n+            'content_type_concrete_id',\n+            'data_abstract',\n+            'data_base',\n+            'data_inherited',\n+            'data_not_concrete_abstract',\n+            'data_not_concrete_base',\n+            'data_not_concrete_inherited',\n+            'fk_abstract',\n+            'fk_abstract_id',\n+            'fk_base',\n+            'fk_base_id',\n+            'fk_inherited',\n+            'fk_inherited_id',\n+            'followers_abstract',\n+            'followers_base',\n+            'followers_concrete',\n+            'following_abstract',\n+            'following_base',\n+            'following_inherited',\n+            'friends_abstract',\n+            'friends_base',\n+            'friends_inherited',\n+            'generic_relation_abstract',\n+            'generic_relation_base',\n+            'generic_relation_concrete',\n+            'id',\n+            'm2m_abstract',\n+            'm2m_base',\n+            'm2m_inherited',\n+            'object_id_abstract',\n+            'object_id_base',\n+            'object_id_concrete',\n+            'relating_basepeople',\n+            'relating_baseperson',\n+            'relating_people',\n+            'relating_person',\n+        ],\n+        BasePerson: [\n+            'content_type_abstract',\n+            'content_type_abstract_id',\n+            'content_type_base',\n+            'content_type_base_id',\n+            'data_abstract',\n+            'data_base',\n+            'data_not_concrete_abstract',\n+            'data_not_concrete_base',\n+            'fk_abstract',\n+            'fk_abstract_id',\n+            'fk_base',\n+            'fk_base_id',\n+            'followers_abstract',\n+            'followers_base',\n+            'following_abstract',\n+            'following_base',\n+            'friends_abstract',\n+            'friends_base',\n+            'generic_relation_abstract',\n+            'generic_relation_base',\n+            'id',\n+            'm2m_abstract',\n+            'm2m_base',\n+            'object_id_abstract',\n+            'object_id_base',\n+            'person',\n+            'relating_basepeople',\n+            'relating_baseperson'\n+        ],\n+        AbstractPerson: [\n+            'content_type_abstract',\n+            'content_type_abstract_id',\n+            'data_abstract',\n+            'data_not_concrete_abstract',\n+            'fk_abstract',\n+            'fk_abstract_id',\n+            'following_abstract',\n+            'friends_abstract',\n+            'generic_relation_abstract',\n+            'm2m_abstract',\n+            'object_id_abstract',\n+        ],\n+        Relating: [\n+            'basepeople',\n+            'basepeople_hidden',\n+            'baseperson',\n+            'baseperson_hidden',\n+            'baseperson_hidden_id',\n+            'baseperson_id',\n+            'id',\n+            'people',\n+            'people_hidden',\n+            'person',\n+            'person_hidden',\n+            'person_hidden_id',\n+            'person_id',\n+            'proxyperson',\n+            'proxyperson_hidden',\n+            'proxyperson_hidden_id',\n+            'proxyperson_id',\n+        ],\n+    },\n+    'fields': {\n+        Person: [\n+            'id',\n+            'data_abstract',\n+            'fk_abstract_id',\n+            'data_not_concrete_abstract',\n+            'content_type_abstract_id',\n+            'object_id_abstract',\n+            'data_base',\n+            'fk_base_id',\n+            'data_not_concrete_base',\n+            'content_type_base_id',\n+            'object_id_base',\n+            'baseperson_ptr_id',\n+            'data_inherited',\n+            'fk_inherited_id',\n+            'data_not_concrete_inherited',\n+            'content_type_concrete_id',\n+            'object_id_concrete',\n+        ],\n+        BasePerson: [\n+            'id',\n+            'data_abstract',\n+            'fk_abstract_id',\n+            'data_not_concrete_abstract',\n+            'content_type_abstract_id',\n+            'object_id_abstract',\n+            'data_base',\n+            'fk_base_id',\n+            'data_not_concrete_base',\n+            'content_type_base_id',\n+            'object_id_base',\n+        ],\n+        AbstractPerson: [\n+            'data_abstract',\n+            'fk_abstract_id',\n+            'data_not_concrete_abstract',\n+            'content_type_abstract_id',\n+            'object_id_abstract',\n+        ],\n+        Relating: [\n+            'id',\n+            'baseperson_id',\n+            'baseperson_hidden_id',\n+            'person_id',\n+            'person_hidden_id',\n+            'proxyperson_id',\n+            'proxyperson_hidden_id',\n+        ],\n+    },\n+    'local_fields': {\n+        Person: [\n+            'baseperson_ptr_id',\n+            'data_inherited',\n+            'fk_inherited_id',\n+            'data_not_concrete_inherited',\n+            'content_type_concrete_id',\n+            'object_id_concrete',\n+        ],\n+        BasePerson: [\n+            'id',\n+            'data_abstract',\n+            'fk_abstract_id',\n+            'data_not_concrete_abstract',\n+            'content_type_abstract_id',\n+            'object_id_abstract',\n+            'data_base',\n+            'fk_base_id',\n+            'data_not_concrete_base',\n+            'content_type_base_id',\n+            'object_id_base',\n+        ],\n+        AbstractPerson: [\n+            'data_abstract',\n+            'fk_abstract_id',\n+            'data_not_concrete_abstract',\n+            'content_type_abstract_id',\n+            'object_id_abstract',\n+        ],\n+        Relating: [\n+            'id',\n+            'baseperson_id',\n+            'baseperson_hidden_id',\n+            'person_id',\n+            'person_hidden_id',\n+            'proxyperson_id',\n+            'proxyperson_hidden_id',\n+        ],\n+    },\n+    'local_concrete_fields': {\n+        Person: [\n+            'baseperson_ptr_id',\n+            'data_inherited',\n+            'fk_inherited_id',\n+            'content_type_concrete_id',\n+            'object_id_concrete',\n+        ],\n+        BasePerson: [\n+            'id',\n+            'data_abstract',\n+            'fk_abstract_id',\n+            'content_type_abstract_id',\n+            'object_id_abstract',\n+            'data_base',\n+            'fk_base_id',\n+            'content_type_base_id',\n+            'object_id_base',\n+        ],\n+        AbstractPerson: [\n+            'data_abstract',\n+            'fk_abstract_id',\n+            'content_type_abstract_id',\n+            'object_id_abstract',\n+        ],\n+        Relating: [\n+            'id',\n+            'baseperson_id',\n+            'baseperson_hidden_id',\n+            'person_id',\n+            'person_hidden_id',\n+            'proxyperson_id',\n+            'proxyperson_hidden_id',\n+        ],\n+    },\n+    'many_to_many': {\n+        Person: [\n+            'm2m_abstract',\n+            'friends_abstract',\n+            'following_abstract',\n+            'm2m_base',\n+            'friends_base',\n+            'following_base',\n+            'm2m_inherited',\n+            'friends_inherited',\n+            'following_inherited',\n+        ],\n+        BasePerson: [\n+            'm2m_abstract',\n+            'friends_abstract',\n+            'following_abstract',\n+            'm2m_base',\n+            'friends_base',\n+            'following_base',\n+        ],\n+        AbstractPerson: [\n+            'm2m_abstract',\n+            'friends_abstract',\n+            'following_abstract',\n+        ],\n+        Relating: [\n+            'basepeople',\n+            'basepeople_hidden',\n+            'people',\n+            'people_hidden',\n+        ],\n+    },\n+    'many_to_many_with_model': {\n+        Person: [\n+            BasePerson,\n+            BasePerson,\n+            BasePerson,\n+            BasePerson,\n+            BasePerson,\n+            BasePerson,\n+            None,\n+            None,\n+            None,\n+        ],\n+        BasePerson: [\n+            None,\n+            None,\n+            None,\n+            None,\n+            None,\n+            None,\n+        ],\n+        AbstractPerson: [\n+            None,\n+            None,\n+            None,\n+        ],\n+        Relating: [\n+            None,\n+            None,\n+            None,\n+            None,\n+        ],\n+    },\n+    'get_all_related_objects_with_model_legacy': {\n+        Person: (\n+            ('relating_baseperson', BasePerson),\n+            ('relating_person', None),\n+        ),\n+        BasePerson: (\n+            ('person', None),\n+            ('relating_baseperson', None),\n+        ),\n+        Relation: (\n+            ('fk_abstract_rel', None),\n+            ('fo_abstract_rel', None),\n+            ('fk_base_rel', None),\n+            ('fo_base_rel', None),\n+            ('fk_concrete_rel', None),\n+            ('fo_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_objects_with_model_hidden_local': {\n+        Person: (\n+            ('+', None),\n+            ('+', None),\n+            ('Person_following_inherited+', None),\n+            ('Person_following_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('Relating_people+', None),\n+            ('Relating_people_hidden+', None),\n+            ('followers_concrete', None),\n+            ('friends_inherited_rel_+', None),\n+            ('relating_people', None),\n+            ('relating_person', None),\n+        ),\n+        BasePerson: (\n+            ('+', None),\n+            ('+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Relating_basepeople+', None),\n+            ('Relating_basepeople_hidden+', None),\n+            ('followers_abstract', None),\n+            ('followers_base', None),\n+            ('friends_abstract_rel_+', None),\n+            ('friends_base_rel_+', None),\n+            ('person', None),\n+            ('relating_basepeople', None),\n+            ('relating_baseperson', None),\n+        ),\n+        Relation: (\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('fk_abstract_rel', None),\n+            ('fk_base_rel', None),\n+            ('fk_concrete_rel', None),\n+            ('fo_abstract_rel', None),\n+            ('fo_base_rel', None),\n+            ('fo_concrete_rel', None),\n+            ('m2m_abstract_rel', None),\n+            ('m2m_base_rel', None),\n+            ('m2m_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_objects_with_model_hidden': {\n+        Person: (\n+            ('+', BasePerson),\n+            ('+', BasePerson),\n+            ('+', None),\n+            ('+', None),\n+            ('BasePerson_following_abstract+', BasePerson),\n+            ('BasePerson_following_abstract+', BasePerson),\n+            ('BasePerson_following_base+', BasePerson),\n+            ('BasePerson_following_base+', BasePerson),\n+            ('BasePerson_friends_abstract+', BasePerson),\n+            ('BasePerson_friends_abstract+', BasePerson),\n+            ('BasePerson_friends_base+', BasePerson),\n+            ('BasePerson_friends_base+', BasePerson),\n+            ('BasePerson_m2m_abstract+', BasePerson),\n+            ('BasePerson_m2m_base+', BasePerson),\n+            ('Person_following_inherited+', None),\n+            ('Person_following_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('Relating_basepeople+', BasePerson),\n+            ('Relating_basepeople_hidden+', BasePerson),\n+            ('Relating_people+', None),\n+            ('Relating_people_hidden+', None),\n+            ('followers_abstract', BasePerson),\n+            ('followers_base', BasePerson),\n+            ('followers_concrete', None),\n+            ('friends_abstract_rel_+', BasePerson),\n+            ('friends_base_rel_+', BasePerson),\n+            ('friends_inherited_rel_+', None),\n+            ('relating_basepeople', BasePerson),\n+            ('relating_baseperson', BasePerson),\n+            ('relating_people', None),\n+            ('relating_person', None),\n+        ),\n+        BasePerson: (\n+            ('+', None),\n+            ('+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Relating_basepeople+', None),\n+            ('Relating_basepeople_hidden+', None),\n+            ('followers_abstract', None),\n+            ('followers_base', None),\n+            ('friends_abstract_rel_+', None),\n+            ('friends_base_rel_+', None),\n+            ('person', None),\n+            ('relating_basepeople', None),\n+            ('relating_baseperson', None),\n+        ),\n+        Relation: (\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('fk_abstract_rel', None),\n+            ('fk_base_rel', None),\n+            ('fk_concrete_rel', None),\n+            ('fo_abstract_rel', None),\n+            ('fo_base_rel', None),\n+            ('fo_concrete_rel', None),\n+            ('m2m_abstract_rel', None),\n+            ('m2m_base_rel', None),\n+            ('m2m_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_objects_with_model_local': {\n+        Person: (\n+            ('followers_concrete', None),\n+            ('relating_person', None),\n+            ('relating_people', None),\n+        ),\n+        BasePerson: (\n+            ('followers_abstract', None),\n+            ('followers_base', None),\n+            ('person', None),\n+            ('relating_baseperson', None),\n+            ('relating_basepeople', None),\n+        ),\n+        Relation: (\n+            ('fk_abstract_rel', None),\n+            ('fo_abstract_rel', None),\n+            ('fk_base_rel', None),\n+            ('fo_base_rel', None),\n+            ('m2m_abstract_rel', None),\n+            ('m2m_base_rel', None),\n+            ('fk_concrete_rel', None),\n+            ('fo_concrete_rel', None),\n+            ('m2m_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_objects_with_model': {\n+        Person: (\n+            ('followers_abstract', BasePerson),\n+            ('followers_base', BasePerson),\n+            ('relating_baseperson', BasePerson),\n+            ('relating_basepeople', BasePerson),\n+            ('followers_concrete', None),\n+            ('relating_person', None),\n+            ('relating_people', None),\n+        ),\n+        BasePerson: (\n+            ('followers_abstract', None),\n+            ('followers_base', None),\n+            ('person', None),\n+            ('relating_baseperson', None),\n+            ('relating_basepeople', None),\n+        ),\n+        Relation: (\n+            ('fk_abstract_rel', None),\n+            ('fo_abstract_rel', None),\n+            ('fk_base_rel', None),\n+            ('fo_base_rel', None),\n+            ('m2m_abstract_rel', None),\n+            ('m2m_base_rel', None),\n+            ('fk_concrete_rel', None),\n+            ('fo_concrete_rel', None),\n+            ('m2m_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_objects_with_model_local_legacy': {\n+        Person: (\n+            ('relating_person', None),\n+        ),\n+        BasePerson: (\n+            ('person', None),\n+            ('relating_baseperson', None)\n+        ),\n+        Relation: (\n+            ('fk_abstract_rel', None),\n+            ('fo_abstract_rel', None),\n+            ('fk_base_rel', None),\n+            ('fo_base_rel', None),\n+            ('fk_concrete_rel', None),\n+            ('fo_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_objects_with_model_hidden_legacy': {\n+        BasePerson: (\n+            ('+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Relating_basepeople+', None),\n+            ('Relating_basepeople_hidden+', None),\n+            ('person', None),\n+            ('relating_baseperson', None),\n+        ),\n+        Person: (\n+            ('+', BasePerson),\n+            ('+', None),\n+            ('BasePerson_following_abstract+', BasePerson),\n+            ('BasePerson_following_abstract+', BasePerson),\n+            ('BasePerson_following_base+', BasePerson),\n+            ('BasePerson_following_base+', BasePerson),\n+            ('BasePerson_friends_abstract+', BasePerson),\n+            ('BasePerson_friends_abstract+', BasePerson),\n+            ('BasePerson_friends_base+', BasePerson),\n+            ('BasePerson_friends_base+', BasePerson),\n+            ('BasePerson_m2m_abstract+', BasePerson),\n+            ('BasePerson_m2m_base+', BasePerson),\n+            ('Person_following_inherited+', None),\n+            ('Person_following_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('Relating_basepeople+', BasePerson),\n+            ('Relating_basepeople_hidden+', BasePerson),\n+            ('Relating_people+', None),\n+            ('Relating_people_hidden+', None),\n+            ('relating_baseperson', BasePerson),\n+            ('relating_person', None),\n+        ),\n+        Relation: (\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('fk_abstract_rel', None),\n+            ('fk_base_rel', None),\n+            ('fk_concrete_rel', None),\n+            ('fo_abstract_rel', None),\n+            ('fo_base_rel', None),\n+            ('fo_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_objects_with_model_hidden_local_legacy': {\n+        BasePerson: (\n+            ('+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Relating_basepeople+', None),\n+            ('Relating_basepeople_hidden+', None),\n+            ('person', None),\n+            ('relating_baseperson', None),\n+        ),\n+        Person: (\n+            ('+', None),\n+            ('Person_following_inherited+', None),\n+            ('Person_following_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('Relating_people+', None),\n+            ('Relating_people_hidden+', None),\n+            ('relating_person', None),\n+        ),\n+        Relation: (\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('fk_abstract_rel', None),\n+            ('fk_base_rel', None),\n+            ('fk_concrete_rel', None),\n+            ('fo_abstract_rel', None),\n+            ('fo_base_rel', None),\n+            ('fo_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_objects_with_model_proxy_legacy': {\n+        BasePerson: (\n+            ('person', None),\n+            ('relating_baseperson', None),\n+        ),\n+        Person: (\n+            ('relating_baseperson', BasePerson),\n+            ('relating_person', None), ('relating_proxyperson', None),\n+        ),\n+        Relation: (\n+            ('fk_abstract_rel', None), ('fo_abstract_rel', None),\n+            ('fk_base_rel', None), ('fo_base_rel', None),\n+            ('fk_concrete_rel', None), ('fo_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_objects_with_model_proxy_hidden_legacy': {\n+        BasePerson: (\n+            ('+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_abstract+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_following_base+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_abstract+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_friends_base+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Relating_basepeople+', None),\n+            ('Relating_basepeople_hidden+', None),\n+            ('person', None),\n+            ('relating_baseperson', None),\n+        ),\n+        Person: (\n+            ('+', BasePerson),\n+            ('+', None),\n+            ('+', None),\n+            ('BasePerson_following_abstract+', BasePerson),\n+            ('BasePerson_following_abstract+', BasePerson),\n+            ('BasePerson_following_base+', BasePerson),\n+            ('BasePerson_following_base+', BasePerson),\n+            ('BasePerson_friends_abstract+', BasePerson),\n+            ('BasePerson_friends_abstract+', BasePerson),\n+            ('BasePerson_friends_base+', BasePerson),\n+            ('BasePerson_friends_base+', BasePerson),\n+            ('BasePerson_m2m_abstract+', BasePerson),\n+            ('BasePerson_m2m_base+', BasePerson),\n+            ('Person_following_inherited+', None),\n+            ('Person_following_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_friends_inherited+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('Relating_basepeople+', BasePerson),\n+            ('Relating_basepeople_hidden+', BasePerson),\n+            ('Relating_people+', None),\n+            ('Relating_people_hidden+', None),\n+            ('relating_baseperson', BasePerson),\n+            ('relating_person', None),\n+            ('relating_proxyperson', None),\n+        ),\n+        Relation: (\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('+', None),\n+            ('BasePerson_m2m_abstract+', None),\n+            ('BasePerson_m2m_base+', None),\n+            ('Person_m2m_inherited+', None),\n+            ('fk_abstract_rel', None),\n+            ('fk_base_rel', None),\n+            ('fk_concrete_rel', None),\n+            ('fo_abstract_rel', None),\n+            ('fo_base_rel', None),\n+            ('fo_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_many_to_many_with_model_legacy': {\n+        BasePerson: (\n+            ('friends_abstract_rel_+', None),\n+            ('followers_abstract', None),\n+            ('friends_base_rel_+', None),\n+            ('followers_base', None),\n+            ('relating_basepeople', None),\n+            ('+', None),\n+        ),\n+        Person: (\n+            ('friends_abstract_rel_+', BasePerson),\n+            ('followers_abstract', BasePerson),\n+            ('friends_base_rel_+', BasePerson),\n+            ('followers_base', BasePerson),\n+            ('relating_basepeople', BasePerson),\n+            ('+', BasePerson),\n+            ('friends_inherited_rel_+', None),\n+            ('followers_concrete', None),\n+            ('relating_people', None),\n+            ('+', None),\n+        ),\n+        Relation: (\n+            ('m2m_abstract_rel', None),\n+            ('m2m_base_rel', None),\n+            ('m2m_concrete_rel', None),\n+        ),\n+    },\n+    'get_all_related_many_to_many_local_legacy': {\n+        BasePerson: [\n+            'friends_abstract_rel_+',\n+            'followers_abstract',\n+            'friends_base_rel_+',\n+            'followers_base',\n+            'relating_basepeople',\n+            '+',\n+        ],\n+        Person: [\n+            'friends_inherited_rel_+',\n+            'followers_concrete',\n+            'relating_people',\n+            '+',\n+        ],\n+        Relation: [\n+            'm2m_abstract_rel',\n+            'm2m_base_rel',\n+            'm2m_concrete_rel',\n+        ],\n+    },\n+    'virtual_fields': {\n+        AbstractPerson: [\n+            'generic_relation_abstract',\n+            'content_object_abstract',\n+        ],\n+        BasePerson: [\n+            'generic_relation_base',\n+            'content_object_base',\n+            'generic_relation_abstract',\n+            'content_object_abstract',\n+        ],\n+        Person: [\n+            'content_object_concrete',\n+            'generic_relation_concrete',\n+            'generic_relation_base',\n+            'content_object_base',\n+            'generic_relation_abstract',\n+            'content_object_abstract',\n+        ],\n+    },\n+}\ndiff --git a/tests/model_meta/test.py b/tests/model_meta/test.py\ndeleted file mode 100644\nindex ea861d5ad5b4..000000000000\n--- a/tests/model_meta/test.py\n+++ /dev/null\n@@ -1,661 +0,0 @@\n-from django import test\n-from django.contrib.contenttypes.fields import GenericRelation\n-from django.core.exceptions import FieldDoesNotExist\n-from django.db.models.fields import related, CharField, Field\n-\n-from .models import (\n-    AbstractPerson, BasePerson, Person, Relating, Relation\n-)\n-\n-TEST_RESULTS = {\n-    'fields': {\n-        Person: [\n-            'id',\n-            'data_abstract',\n-            'fk_abstract_id',\n-            'data_not_concrete_abstract',\n-            'content_type_abstract_id',\n-            'object_id_abstract',\n-            'data_base',\n-            'fk_base_id',\n-            'data_not_concrete_base',\n-            'content_type_base_id',\n-            'object_id_base',\n-            'baseperson_ptr_id',\n-            'data_inherited',\n-            'fk_inherited_id',\n-            'data_not_concrete_inherited',\n-            'content_type_concrete_id',\n-            'object_id_concrete',\n-        ],\n-        BasePerson: [\n-            'id',\n-            'data_abstract',\n-            'fk_abstract_id',\n-            'data_not_concrete_abstract',\n-            'content_type_abstract_id',\n-            'object_id_abstract',\n-            'data_base',\n-            'fk_base_id',\n-            'data_not_concrete_base',\n-            'content_type_base_id',\n-            'object_id_base',\n-        ],\n-        AbstractPerson: [\n-            'data_abstract',\n-            'fk_abstract_id',\n-            'data_not_concrete_abstract',\n-            'content_type_abstract_id',\n-            'object_id_abstract',\n-        ],\n-        Relating: [\n-            'id',\n-            'baseperson_id',\n-            'baseperson_hidden_id',\n-            'person_id',\n-            'person_hidden_id',\n-            'proxyperson_id',\n-            'proxyperson_hidden_id',\n-        ],\n-    },\n-    'local_fields': {\n-        Person: [\n-            'baseperson_ptr_id',\n-            'data_inherited',\n-            'fk_inherited_id',\n-            'data_not_concrete_inherited',\n-            'content_type_concrete_id',\n-            'object_id_concrete',\n-        ],\n-        BasePerson: [\n-            'id',\n-            'data_abstract',\n-            'fk_abstract_id',\n-            'data_not_concrete_abstract',\n-            'content_type_abstract_id',\n-            'object_id_abstract',\n-            'data_base',\n-            'fk_base_id',\n-            'data_not_concrete_base',\n-            'content_type_base_id',\n-            'object_id_base',\n-        ],\n-        AbstractPerson: [\n-            'data_abstract',\n-            'fk_abstract_id',\n-            'data_not_concrete_abstract',\n-            'content_type_abstract_id',\n-            'object_id_abstract',\n-        ],\n-        Relating: [\n-            'id',\n-            'baseperson_id',\n-            'baseperson_hidden_id',\n-            'person_id',\n-            'person_hidden_id',\n-            'proxyperson_id',\n-            'proxyperson_hidden_id',\n-        ],\n-    },\n-    'local_concrete_fields': {\n-        Person: [\n-            'baseperson_ptr_id',\n-            'data_inherited',\n-            'fk_inherited_id',\n-            'content_type_concrete_id',\n-            'object_id_concrete',\n-        ],\n-        BasePerson: [\n-            'id',\n-            'data_abstract',\n-            'fk_abstract_id',\n-            'content_type_abstract_id',\n-            'object_id_abstract',\n-            'data_base',\n-            'fk_base_id',\n-            'content_type_base_id',\n-            'object_id_base',\n-        ],\n-        AbstractPerson: [\n-            'data_abstract',\n-            'fk_abstract_id',\n-            'content_type_abstract_id',\n-            'object_id_abstract',\n-        ],\n-        Relating: [\n-            'id',\n-            'baseperson_id',\n-            'baseperson_hidden_id',\n-            'person_id',\n-            'person_hidden_id',\n-            'proxyperson_id',\n-            'proxyperson_hidden_id',\n-        ],\n-    },\n-    'many_to_many': {\n-        Person: [\n-            'm2m_abstract',\n-            'friends_abstract',\n-            'following_abstract',\n-            'm2m_base',\n-            'friends_base',\n-            'following_base',\n-            'm2m_inherited',\n-            'friends_inherited',\n-            'following_inherited',\n-        ],\n-        BasePerson: [\n-            'm2m_abstract',\n-            'friends_abstract',\n-            'following_abstract',\n-            'm2m_base',\n-            'friends_base',\n-            'following_base',\n-        ],\n-        AbstractPerson: [\n-            'm2m_abstract',\n-            'friends_abstract',\n-            'following_abstract',\n-        ],\n-        Relating: [\n-            'basepeople',\n-            'basepeople_hidden',\n-            'people',\n-            'people_hidden',\n-        ],\n-    },\n-    'many_to_many_with_model': {\n-        Person: [\n-            BasePerson,\n-            BasePerson,\n-            BasePerson,\n-            BasePerson,\n-            BasePerson,\n-            BasePerson,\n-            None,\n-            None,\n-            None,\n-        ],\n-        BasePerson: [\n-            None,\n-            None,\n-            None,\n-            None,\n-            None,\n-            None,\n-        ],\n-        AbstractPerson: [\n-            None,\n-            None,\n-            None,\n-        ],\n-        Relating: [\n-            None,\n-            None,\n-            None,\n-            None,\n-        ],\n-    },\n-    'get_all_related_objects_with_model': {\n-        Person: (\n-            ('relating_baseperson', BasePerson),\n-            ('relating_person', None),\n-        ),\n-        BasePerson: (\n-            ('person', None),\n-            ('relating_baseperson', None),\n-        ),\n-        Relation: (\n-            ('fk_abstract_rel', None),\n-            ('fo_abstract_rel', None),\n-            ('fk_base_rel', None),\n-            ('fo_base_rel', None),\n-            ('fk_concrete_rel', None),\n-            ('fo_concrete_rel', None),\n-        ),\n-    },\n-    'get_all_related_objects_with_model_local': {\n-        Person: (\n-            ('relating_person', None),\n-        ),\n-        BasePerson: (\n-            ('person', None),\n-            ('relating_baseperson', None)\n-        ),\n-        Relation: (\n-            ('fk_abstract_rel', None),\n-            ('fo_abstract_rel', None),\n-            ('fk_base_rel', None),\n-            ('fo_base_rel', None),\n-            ('fk_concrete_rel', None),\n-            ('fo_concrete_rel', None),\n-        ),\n-    },\n-    'get_all_related_objects_with_model_hidden': {\n-        BasePerson: (\n-            ('model_meta.baseperson_friends_base', None),\n-            ('model_meta.baseperson_friends_base', None),\n-            ('model_meta.baseperson_m2m_base', None),\n-            ('model_meta.baseperson_following_base', None),\n-            ('model_meta.baseperson_following_base', None),\n-            ('model_meta.baseperson_m2m_abstract', None),\n-            ('model_meta.baseperson_friends_abstract', None),\n-            ('model_meta.baseperson_friends_abstract', None),\n-            ('model_meta.baseperson_following_abstract', None),\n-            ('model_meta.baseperson_following_abstract', None),\n-            ('model_meta.person', None),\n-            ('model_meta.relating_basepeople', None),\n-            ('model_meta.relating_basepeople_hidden', None),\n-            ('model_meta.relating', None),\n-            ('model_meta.relating', None),\n-        ),\n-        Person: (\n-            ('model_meta.baseperson_friends_base', BasePerson),\n-            ('model_meta.baseperson_friends_base', BasePerson),\n-            ('model_meta.baseperson_m2m_base', BasePerson),\n-            ('model_meta.baseperson_following_base', BasePerson),\n-            ('model_meta.baseperson_following_base', BasePerson),\n-            ('model_meta.baseperson_m2m_abstract', BasePerson),\n-            ('model_meta.baseperson_friends_abstract', BasePerson),\n-            ('model_meta.baseperson_friends_abstract', BasePerson),\n-            ('model_meta.baseperson_following_abstract', BasePerson),\n-            ('model_meta.baseperson_following_abstract', BasePerson),\n-            ('model_meta.relating_basepeople', BasePerson),\n-            ('model_meta.relating_basepeople_hidden', BasePerson),\n-            ('model_meta.relating', BasePerson),\n-            ('model_meta.relating', BasePerson),\n-            ('model_meta.person_m2m_inherited', None),\n-            ('model_meta.person_friends_inherited', None),\n-            ('model_meta.person_friends_inherited', None),\n-            ('model_meta.person_following_inherited', None),\n-            ('model_meta.person_following_inherited', None),\n-            ('model_meta.relating_people', None),\n-            ('model_meta.relating_people_hidden', None),\n-            ('model_meta.relating', None),\n-            ('model_meta.relating', None),\n-        ),\n-        Relation: (\n-            ('model_meta.baseperson_m2m_base', None),\n-            ('model_meta.baseperson_m2m_abstract', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.person_m2m_inherited', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.proxyperson', None),\n-            ('model_meta.proxyperson', None),\n-            ('model_meta.proxyperson', None),\n-        ),\n-    },\n-    'get_all_related_objects_with_model_hidden_local': {\n-        BasePerson: (\n-            ('model_meta.baseperson_friends_base', None),\n-            ('model_meta.baseperson_friends_base', None),\n-            ('model_meta.baseperson_m2m_base', None),\n-            ('model_meta.baseperson_following_base', None),\n-            ('model_meta.baseperson_following_base', None),\n-            ('model_meta.baseperson_m2m_abstract', None),\n-            ('model_meta.baseperson_friends_abstract', None),\n-            ('model_meta.baseperson_friends_abstract', None),\n-            ('model_meta.baseperson_following_abstract', None),\n-            ('model_meta.baseperson_following_abstract', None),\n-            ('model_meta.person', None),\n-            ('model_meta.relating_basepeople', None),\n-            ('model_meta.relating_basepeople_hidden', None),\n-            ('model_meta.relating', None),\n-            ('model_meta.relating', None),\n-        ),\n-        Person: (\n-            ('model_meta.person_m2m_inherited', None),\n-            ('model_meta.person_friends_inherited', None),\n-            ('model_meta.person_friends_inherited', None),\n-            ('model_meta.person_following_inherited', None),\n-            ('model_meta.person_following_inherited', None),\n-            ('model_meta.relating_people', None),\n-            ('model_meta.relating_people_hidden', None),\n-            ('model_meta.relating', None),\n-            ('model_meta.relating', None),\n-        ),\n-        Relation: (\n-            ('model_meta.baseperson_m2m_base', None),\n-            ('model_meta.baseperson_m2m_abstract', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.person_m2m_inherited', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.proxyperson', None),\n-            ('model_meta.proxyperson', None),\n-            ('model_meta.proxyperson', None),\n-        ),\n-    },\n-    'get_all_related_objects_with_model_proxy': {\n-        BasePerson: (\n-            ('person', None),\n-            ('relating_baseperson', None),\n-        ),\n-        Person: (\n-            ('relating_baseperson', BasePerson),\n-            ('relating_person', None), ('relating_proxyperson', None),\n-        ),\n-        Relation: (\n-            ('fk_abstract_rel', None), ('fo_abstract_rel', None),\n-            ('fk_base_rel', None), ('fo_base_rel', None),\n-            ('fk_concrete_rel', None), ('fo_concrete_rel', None),\n-        ),\n-    },\n-    'get_all_related_objects_with_model_proxy_hidden': {\n-        BasePerson: (\n-            ('model_meta.baseperson_friends_base', None),\n-            ('model_meta.baseperson_friends_base', None),\n-            ('model_meta.baseperson_m2m_base', None),\n-            ('model_meta.baseperson_following_base', None),\n-            ('model_meta.baseperson_following_base', None),\n-            ('model_meta.baseperson_m2m_abstract', None),\n-            ('model_meta.baseperson_friends_abstract', None),\n-            ('model_meta.baseperson_friends_abstract', None),\n-            ('model_meta.baseperson_following_abstract', None),\n-            ('model_meta.baseperson_following_abstract', None),\n-            ('model_meta.person', None),\n-            ('model_meta.relating_basepeople', None),\n-            ('model_meta.relating_basepeople_hidden', None),\n-            ('model_meta.relating', None),\n-            ('model_meta.relating', None),\n-        ),\n-        Person: (\n-            ('model_meta.baseperson_friends_base', BasePerson),\n-            ('model_meta.baseperson_friends_base', BasePerson),\n-            ('model_meta.baseperson_m2m_base', BasePerson),\n-            ('model_meta.baseperson_following_base', BasePerson),\n-            ('model_meta.baseperson_following_base', BasePerson),\n-            ('model_meta.baseperson_m2m_abstract', BasePerson),\n-            ('model_meta.baseperson_friends_abstract', BasePerson),\n-            ('model_meta.baseperson_friends_abstract', BasePerson),\n-            ('model_meta.baseperson_following_abstract', BasePerson),\n-            ('model_meta.baseperson_following_abstract', BasePerson),\n-            ('model_meta.relating_basepeople', BasePerson),\n-            ('model_meta.relating_basepeople_hidden', BasePerson),\n-            ('model_meta.relating', BasePerson),\n-            ('model_meta.relating', BasePerson),\n-            ('model_meta.person_m2m_inherited', None),\n-            ('model_meta.person_friends_inherited', None),\n-            ('model_meta.person_friends_inherited', None),\n-            ('model_meta.person_following_inherited', None),\n-            ('model_meta.person_following_inherited', None),\n-            ('model_meta.relating_people', None),\n-            ('model_meta.relating_people_hidden', None),\n-            ('model_meta.relating', None),\n-            ('model_meta.relating', None),\n-            ('model_meta.relating', None),\n-            ('model_meta.relating', None),\n-        ),\n-        Relation: (\n-            ('model_meta.baseperson_m2m_base', None),\n-            ('model_meta.baseperson_m2m_abstract', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.baseperson', None),\n-            ('model_meta.person_m2m_inherited', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.person', None),\n-            ('model_meta.proxyperson', None),\n-            ('model_meta.proxyperson', None),\n-            ('model_meta.proxyperson', None),\n-        ),\n-    },\n-    'get_all_related_many_to_many_with_model': {\n-        BasePerson: (\n-            ('friends_abstract_rel_+', None),\n-            ('followers_abstract', None),\n-            ('friends_base_rel_+', None),\n-            ('followers_base', None),\n-            ('relating_basepeople', None),\n-            ('+', None),\n-        ),\n-        Person: (\n-            ('friends_abstract_rel_+', BasePerson),\n-            ('followers_abstract', BasePerson),\n-            ('friends_base_rel_+', BasePerson),\n-            ('followers_base', BasePerson),\n-            ('relating_basepeople', BasePerson),\n-            ('+', BasePerson),\n-            ('friends_inherited_rel_+', None),\n-            ('followers_concrete', None),\n-            ('relating_people', None),\n-            ('+', None),\n-        ),\n-        Relation: (\n-            ('m2m_abstract_rel', None),\n-            ('m2m_base_rel', None),\n-            ('m2m_concrete_rel', None),\n-        ),\n-    },\n-    'get_all_related_many_to_many_local': {\n-        BasePerson: [\n-            'friends_abstract_rel_+',\n-            'followers_abstract',\n-            'friends_base_rel_+',\n-            'followers_base',\n-            'relating_basepeople',\n-            '+',\n-        ],\n-        Person: [\n-            'friends_inherited_rel_+',\n-            'followers_concrete',\n-            'relating_people',\n-            '+',\n-        ],\n-        Relation: [\n-            'm2m_abstract_rel',\n-            'm2m_base_rel',\n-            'm2m_concrete_rel',\n-        ],\n-    },\n-    'virtual_fields': {\n-        AbstractPerson: [\n-            'generic_relation_abstract',\n-            'content_object_abstract',\n-        ],\n-        BasePerson: [\n-            'generic_relation_base',\n-            'content_object_base',\n-            'generic_relation_abstract',\n-            'content_object_abstract',\n-        ],\n-        Person: [\n-            'content_object_concrete',\n-            'generic_relation_concrete',\n-            'generic_relation_base',\n-            'content_object_base',\n-            'generic_relation_abstract',\n-            'content_object_abstract',\n-        ],\n-    },\n-}\n-\n-\n-class OptionsBaseTests(test.TestCase):\n-\n-    def _map_rq_names(self, res):\n-        return tuple((o.field.related_query_name(), m) for o, m in res)\n-\n-    def _map_names(self, res):\n-        return tuple((f.name, m) for f, m in res)\n-\n-\n-class DataTests(OptionsBaseTests):\n-\n-    def test_fields(self):\n-        for model, expected_result in TEST_RESULTS['fields'].items():\n-            fields = model._meta.fields\n-            self.assertEqual([f.attname for f in fields], expected_result)\n-\n-    def test_local_fields(self):\n-        is_data_field = lambda f: isinstance(f, Field) and not isinstance(f, related.ManyToManyField)\n-\n-        for model, expected_result in TEST_RESULTS['local_fields'].items():\n-            fields = model._meta.local_fields\n-            self.assertEqual([f.attname for f in fields], expected_result)\n-            self.assertTrue(all([f.model is model for f in fields]))\n-            self.assertTrue(all([is_data_field(f) for f in fields]))\n-\n-    def test_local_concrete_fields(self):\n-        for model, expected_result in TEST_RESULTS['local_concrete_fields'].items():\n-            fields = model._meta.local_concrete_fields\n-            self.assertEqual([f.attname for f in fields], expected_result)\n-            self.assertTrue(all([f.column is not None for f in fields]))\n-\n-\n-class M2MTests(OptionsBaseTests):\n-\n-    def test_many_to_many(self):\n-        for model, expected_result in TEST_RESULTS['many_to_many'].items():\n-            fields = model._meta.many_to_many\n-            self.assertEqual([f.attname for f in fields], expected_result)\n-            self.assertTrue(all([isinstance(f.rel, related.ManyToManyRel) for f in fields]))\n-\n-    def test_many_to_many_with_model(self):\n-        for model, expected_result in TEST_RESULTS['many_to_many_with_model'].items():\n-            models = [model for field, model in model._meta.get_m2m_with_model()]\n-            self.assertEqual(models, expected_result)\n-\n-\n-class RelatedObjectsTests(OptionsBaseTests):\n-    def setUp(self):\n-        self.key_name = lambda r: r[0]\n-\n-    def test_related_objects(self):\n-        result_key = 'get_all_related_objects_with_model'\n-        for model, expected in TEST_RESULTS[result_key].items():\n-            objects = model._meta.get_all_related_objects_with_model()\n-            self.assertEqual(self._map_rq_names(objects), expected)\n-\n-    def test_related_objects_local(self):\n-        result_key = 'get_all_related_objects_with_model_local'\n-        for model, expected in TEST_RESULTS[result_key].items():\n-            objects = model._meta.get_all_related_objects_with_model(local_only=True)\n-            self.assertEqual(self._map_rq_names(objects), expected)\n-\n-    def test_related_objects_include_hidden(self):\n-        result_key = 'get_all_related_objects_with_model_hidden'\n-        for model, expected in TEST_RESULTS[result_key].items():\n-            objects = model._meta.get_all_related_objects_with_model(include_hidden=True)\n-            self.assertEqual(\n-                sorted(self._map_names(objects), key=self.key_name),\n-                sorted(expected, key=self.key_name)\n-            )\n-\n-    def test_related_objects_include_hidden_local_only(self):\n-        result_key = 'get_all_related_objects_with_model_hidden_local'\n-        for model, expected in TEST_RESULTS[result_key].items():\n-            objects = model._meta.get_all_related_objects_with_model(\n-                include_hidden=True, local_only=True)\n-            self.assertEqual(\n-                sorted(self._map_names(objects), key=self.key_name),\n-                sorted(expected, key=self.key_name)\n-            )\n-\n-    def test_related_objects_proxy(self):\n-        result_key = 'get_all_related_objects_with_model_proxy'\n-        for model, expected in TEST_RESULTS[result_key].items():\n-            objects = model._meta.get_all_related_objects_with_model(\n-                include_proxy_eq=True)\n-            self.assertEqual(self._map_rq_names(objects), expected)\n-\n-    def test_related_objects_proxy_hidden(self):\n-        result_key = 'get_all_related_objects_with_model_proxy_hidden'\n-        for model, expected in TEST_RESULTS[result_key].items():\n-            objects = model._meta.get_all_related_objects_with_model(\n-                include_proxy_eq=True, include_hidden=True)\n-            self.assertEqual(\n-                sorted(self._map_names(objects), key=self.key_name),\n-                sorted(expected, key=self.key_name)\n-            )\n-\n-\n-class RelatedM2MTests(OptionsBaseTests):\n-\n-    def test_related_m2m_with_model(self):\n-        result_key = 'get_all_related_many_to_many_with_model'\n-        for model, expected in TEST_RESULTS[result_key].items():\n-            objects = model._meta.get_all_related_m2m_objects_with_model()\n-            self.assertEqual(self._map_rq_names(objects), expected)\n-\n-    def test_related_m2m_local_only(self):\n-        result_key = 'get_all_related_many_to_many_local'\n-        for model, expected in TEST_RESULTS[result_key].items():\n-            objects = model._meta.get_all_related_many_to_many_objects(local_only=True)\n-            self.assertEqual([o.field.related_query_name() for o in objects], expected)\n-\n-    def test_related_m2m_asymmetrical(self):\n-        m2m = Person._meta.many_to_many\n-        self.assertIn('following_base', [f.attname for f in m2m])\n-        related_m2m = Person._meta.get_all_related_many_to_many_objects()\n-        self.assertIn('followers_base', [o.field.related_query_name() for o in related_m2m])\n-\n-    def test_related_m2m_symmetrical(self):\n-        m2m = Person._meta.many_to_many\n-        self.assertIn('friends_base', [f.attname for f in m2m])\n-        related_m2m = Person._meta.get_all_related_many_to_many_objects()\n-        self.assertIn('friends_inherited_rel_+', [o.field.related_query_name() for o in related_m2m])\n-\n-\n-class VirtualFieldsTests(OptionsBaseTests):\n-\n-    def test_virtual_fields(self):\n-        for model, expected_names in TEST_RESULTS['virtual_fields'].items():\n-            objects = model._meta.virtual_fields\n-            self.assertEqual(sorted([f.name for f in objects]), sorted(expected_names))\n-\n-\n-class GetFieldByNameTests(OptionsBaseTests):\n-\n-    def test_get_data_field(self):\n-        field_info = Person._meta.get_field_by_name('data_abstract')\n-        self.assertEqual(field_info[1:], (BasePerson, True, False))\n-        self.assertIsInstance(field_info[0], CharField)\n-\n-    def test_get_m2m_field(self):\n-        field_info = Person._meta.get_field_by_name('m2m_base')\n-        self.assertEqual(field_info[1:], (BasePerson, True, True))\n-        self.assertIsInstance(field_info[0], related.ManyToManyField)\n-\n-    def test_get_related_object(self):\n-        field_info = Person._meta.get_field_by_name('relating_baseperson')\n-        self.assertEqual(field_info[1:], (BasePerson, False, False))\n-        self.assertIsInstance(field_info[0], related.ForeignObjectRel)\n-\n-    def test_get_related_m2m(self):\n-        field_info = Person._meta.get_field_by_name('relating_people')\n-        self.assertEqual(field_info[1:], (None, False, True))\n-        self.assertIsInstance(field_info[0], related.ForeignObjectRel)\n-\n-    def test_get_generic_foreign_key(self):\n-        # For historic reasons generic foreign keys aren't available.\n-        with self.assertRaises(FieldDoesNotExist):\n-            Person._meta.get_field_by_name('content_object_base')\n-\n-    def test_get_generic_relation(self):\n-        field_info = Person._meta.get_field_by_name('generic_relation_base')\n-        self.assertEqual(field_info[1:], (None, True, False))\n-        self.assertIsInstance(field_info[0], GenericRelation)\ndiff --git a/tests/model_meta/test_legacy.py b/tests/model_meta/test_legacy.py\nnew file mode 100644\nindex 000000000000..60bfb1641ff4\n--- /dev/null\n+++ b/tests/model_meta/test_legacy.py\n@@ -0,0 +1,166 @@\n+import warnings\n+\n+from django import test\n+from django.contrib.contenttypes.fields import GenericRelation\n+from django.core.exceptions import FieldDoesNotExist\n+from django.db.models.fields import related, CharField\n+from django.utils.deprecation import RemovedInDjango20Warning\n+\n+from .models import BasePerson, Person\n+from .results import TEST_RESULTS\n+\n+\n+class OptionsBaseTests(test.TestCase):\n+\n+    def _map_related_query_names(self, res):\n+        return tuple((o.field.related_query_name(), m) for o, m in res)\n+\n+    def _map_names(self, res):\n+        return tuple((f.name, m) for f, m in res)\n+\n+\n+class M2MTests(OptionsBaseTests):\n+\n+    def test_many_to_many_with_model(self):\n+        for model, expected_result in TEST_RESULTS['many_to_many_with_model'].items():\n+            with warnings.catch_warnings(record=True) as warning:\n+                warnings.simplefilter(\"always\")\n+                models = [model for field, model in model._meta.get_m2m_with_model()]\n+                self.assertEqual([RemovedInDjango20Warning], [w.message.__class__ for w in warning])\n+            self.assertEqual(models, expected_result)\n+\n+\n+@test.ignore_warnings(category=RemovedInDjango20Warning)\n+class RelatedObjectsTests(OptionsBaseTests):\n+    key_name = lambda self, r: r[0]\n+\n+    def test_related_objects(self):\n+        result_key = 'get_all_related_objects_with_model_legacy'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = model._meta.get_all_related_objects_with_model()\n+            self.assertEqual(self._map_related_query_names(objects), expected)\n+\n+    def test_related_objects_local(self):\n+        result_key = 'get_all_related_objects_with_model_local_legacy'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = model._meta.get_all_related_objects_with_model(local_only=True)\n+            self.assertEqual(self._map_related_query_names(objects), expected)\n+\n+    def test_related_objects_include_hidden(self):\n+        result_key = 'get_all_related_objects_with_model_hidden_legacy'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = model._meta.get_all_related_objects_with_model(include_hidden=True)\n+            self.assertEqual(\n+                sorted(self._map_names(objects), key=self.key_name),\n+                sorted(expected, key=self.key_name)\n+            )\n+\n+    def test_related_objects_include_hidden_local_only(self):\n+        result_key = 'get_all_related_objects_with_model_hidden_local_legacy'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = model._meta.get_all_related_objects_with_model(\n+                include_hidden=True, local_only=True)\n+            self.assertEqual(\n+                sorted(self._map_names(objects), key=self.key_name),\n+                sorted(expected, key=self.key_name)\n+            )\n+\n+    def test_related_objects_proxy(self):\n+        result_key = 'get_all_related_objects_with_model_proxy_legacy'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = model._meta.get_all_related_objects_with_model(\n+                include_proxy_eq=True)\n+            self.assertEqual(self._map_related_query_names(objects), expected)\n+\n+    def test_related_objects_proxy_hidden(self):\n+        result_key = 'get_all_related_objects_with_model_proxy_hidden_legacy'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = model._meta.get_all_related_objects_with_model(\n+                include_proxy_eq=True, include_hidden=True)\n+            self.assertEqual(\n+                sorted(self._map_names(objects), key=self.key_name),\n+                sorted(expected, key=self.key_name)\n+            )\n+\n+\n+@test.ignore_warnings(category=RemovedInDjango20Warning)\n+class RelatedM2MTests(OptionsBaseTests):\n+\n+    def test_related_m2m_with_model(self):\n+        result_key = 'get_all_related_many_to_many_with_model_legacy'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = model._meta.get_all_related_m2m_objects_with_model()\n+            self.assertEqual(self._map_related_query_names(objects), expected)\n+\n+    def test_related_m2m_local_only(self):\n+        result_key = 'get_all_related_many_to_many_local_legacy'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = model._meta.get_all_related_many_to_many_objects(local_only=True)\n+            self.assertEqual([o.field.related_query_name() for o in objects], expected)\n+\n+    def test_related_m2m_asymmetrical(self):\n+        m2m = Person._meta.many_to_many\n+        self.assertTrue('following_base' in [f.attname for f in m2m])\n+        related_m2m = Person._meta.get_all_related_many_to_many_objects()\n+        self.assertTrue('followers_base' in [o.field.related_query_name() for o in related_m2m])\n+\n+    def test_related_m2m_symmetrical(self):\n+        m2m = Person._meta.many_to_many\n+        self.assertTrue('friends_base' in [f.attname for f in m2m])\n+        related_m2m = Person._meta.get_all_related_many_to_many_objects()\n+        self.assertIn('friends_inherited_rel_+', [o.field.related_query_name() for o in related_m2m])\n+\n+\n+@test.ignore_warnings(category=RemovedInDjango20Warning)\n+class GetFieldByNameTests(OptionsBaseTests):\n+\n+    def test_get_data_field(self):\n+        field_info = Person._meta.get_field_by_name('data_abstract')\n+        self.assertEqual(field_info[1:], (BasePerson, True, False))\n+        self.assertIsInstance(field_info[0], CharField)\n+\n+    def test_get_m2m_field(self):\n+        field_info = Person._meta.get_field_by_name('m2m_base')\n+        self.assertEqual(field_info[1:], (BasePerson, True, True))\n+        self.assertIsInstance(field_info[0], related.ManyToManyField)\n+\n+    def test_get_related_object(self):\n+        field_info = Person._meta.get_field_by_name('relating_baseperson')\n+        self.assertEqual(field_info[1:], (BasePerson, False, False))\n+        self.assertTrue(field_info[0].auto_created)\n+\n+    def test_get_related_m2m(self):\n+        field_info = Person._meta.get_field_by_name('relating_people')\n+        self.assertEqual(field_info[1:], (None, False, True))\n+        self.assertTrue(field_info[0].auto_created)\n+\n+    def test_get_generic_relation(self):\n+        field_info = Person._meta.get_field_by_name('generic_relation_base')\n+        self.assertEqual(field_info[1:], (None, True, False))\n+        self.assertIsInstance(field_info[0], GenericRelation)\n+\n+    def test_get_m2m_field_invalid(self):\n+        with warnings.catch_warnings(record=True) as warning:\n+            warnings.simplefilter(\"always\")\n+            self.assertRaises(\n+                FieldDoesNotExist,\n+                Person._meta.get_field,\n+                **{'field_name': 'm2m_base', 'many_to_many': False}\n+            )\n+            self.assertEqual(Person._meta.get_field('m2m_base', many_to_many=True).name, 'm2m_base')\n+\n+            # 2 RemovedInDjango20Warning messages should be raised, one for each call of get_field()\n+            # with the 'many_to_many' argument.\n+            self.assertEqual(\n+                [RemovedInDjango20Warning, RemovedInDjango20Warning],\n+                [w.message.__class__ for w in warning]\n+            )\n+\n+\n+@test.ignore_warnings(category=RemovedInDjango20Warning)\n+class GetAllFieldNamesTestCase(OptionsBaseTests):\n+\n+    def test_get_all_field_names(self):\n+        for model, expected_names in TEST_RESULTS['get_all_field_names'].items():\n+            objects = model._meta.get_all_field_names()\n+            self.assertEqual(sorted(map(str, objects)), sorted(expected_names))\ndiff --git a/tests/model_meta/tests.py b/tests/model_meta/tests.py\nnew file mode 100644\nindex 000000000000..31ffa4ac9383\n--- /dev/null\n+++ b/tests/model_meta/tests.py\n@@ -0,0 +1,247 @@\n+from django.apps import apps\n+from django.contrib.contenttypes.fields import GenericForeignKey, GenericRelation\n+from django.core.exceptions import FieldDoesNotExist\n+from django.db.models.fields import related, CharField, Field\n+from django.db.models.options import IMMUTABLE_WARNING, EMPTY_RELATION_TREE\n+from django.test import TestCase\n+\n+from .models import Relation, AbstractPerson, BasePerson, Person, ProxyPerson, Relating\n+from .results import TEST_RESULTS\n+\n+\n+class OptionsBaseTests(TestCase):\n+\n+    def _map_related_query_names(self, res):\n+        return tuple((o.name, m) for o, m in res)\n+\n+    def _map_names(self, res):\n+        return tuple((f.name, m) for f, m in res)\n+\n+    def _model(self, current_model, field):\n+        model = field.model._meta.concrete_model\n+        return None if model == current_model else model\n+\n+    def _details(self, current_model, relation):\n+        direct = isinstance(relation, Field) or isinstance(relation, GenericForeignKey)\n+        model = relation.model._meta.concrete_model\n+        if model == current_model:\n+            model = None\n+\n+        field = relation if direct else relation.field\n+        m2m = isinstance(field, related.ManyToManyField)\n+        return relation, model, direct, m2m\n+\n+\n+class GetFieldsTests(OptionsBaseTests):\n+\n+    def test_get_fields_is_immutable(self):\n+        msg = IMMUTABLE_WARNING % \"get_fields()\"\n+        for _ in range(2):\n+            # Running unit test twice to ensure both non-cached and cached result\n+            # are immutable.\n+            fields = Person._meta.get_fields()\n+            with self.assertRaisesMessage(AttributeError, msg):\n+                fields += [\"errors\"]\n+\n+\n+class DataTests(OptionsBaseTests):\n+\n+    def test_fields(self):\n+        for model, expected_result in TEST_RESULTS['fields'].items():\n+            fields = model._meta.fields\n+            self.assertEqual([f.attname for f in fields], expected_result)\n+\n+    def test_local_fields(self):\n+        is_data_field = lambda f: isinstance(f, Field) and not isinstance(f, related.ManyToManyField)\n+\n+        for model, expected_result in TEST_RESULTS['local_fields'].items():\n+            fields = model._meta.local_fields\n+            self.assertEqual([f.attname for f in fields], expected_result)\n+            for f in fields:\n+                self.assertEqual(f.model, model)\n+                self.assertTrue(is_data_field(f))\n+\n+    def test_local_concrete_fields(self):\n+        for model, expected_result in TEST_RESULTS['local_concrete_fields'].items():\n+            fields = model._meta.local_concrete_fields\n+            self.assertEqual([f.attname for f in fields], expected_result)\n+            for f in fields:\n+                self.assertIsNotNone(f.column)\n+\n+\n+class M2MTests(OptionsBaseTests):\n+\n+    def test_many_to_many(self):\n+        for model, expected_result in TEST_RESULTS['many_to_many'].items():\n+            fields = model._meta.many_to_many\n+            self.assertEqual([f.attname for f in fields], expected_result)\n+            for f in fields:\n+                self.assertTrue(f.many_to_many and f.is_relation)\n+\n+    def test_many_to_many_with_model(self):\n+        for model, expected_result in TEST_RESULTS['many_to_many_with_model'].items():\n+            models = [self._model(model, field) for field in model._meta.many_to_many]\n+            self.assertEqual(models, expected_result)\n+\n+\n+class RelatedObjectsTests(OptionsBaseTests):\n+    key_name = lambda self, r: r[0]\n+\n+    def test_related_objects(self):\n+        result_key = 'get_all_related_objects_with_model'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = [\n+                (field, self._model(model, field))\n+                for field in model._meta.get_fields()\n+                if field.auto_created and not field.concrete\n+            ]\n+            self.assertEqual(self._map_related_query_names(objects), expected)\n+\n+    def test_related_objects_local(self):\n+        result_key = 'get_all_related_objects_with_model_local'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = [\n+                (field, self._model(model, field))\n+                for field in model._meta.get_fields(include_parents=False)\n+                if field.auto_created and not field.concrete\n+            ]\n+            self.assertEqual(self._map_related_query_names(objects), expected)\n+\n+    def test_related_objects_include_hidden(self):\n+        result_key = 'get_all_related_objects_with_model_hidden'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = [\n+                (field, self._model(model, field))\n+                for field in model._meta.get_fields(include_hidden=True)\n+                if field.auto_created and not field.concrete\n+            ]\n+            self.assertEqual(\n+                sorted(self._map_names(objects), key=self.key_name),\n+                sorted(expected, key=self.key_name)\n+            )\n+\n+    def test_related_objects_include_hidden_local_only(self):\n+        result_key = 'get_all_related_objects_with_model_hidden_local'\n+        for model, expected in TEST_RESULTS[result_key].items():\n+            objects = [\n+                (field, self._model(model, field))\n+                for field in model._meta.get_fields(include_hidden=True, include_parents=False)\n+                if field.auto_created and not field.concrete\n+            ]\n+            self.assertEqual(\n+                sorted(self._map_names(objects), key=self.key_name),\n+                sorted(expected, key=self.key_name)\n+            )\n+\n+\n+class VirtualFieldsTests(OptionsBaseTests):\n+\n+    def test_virtual_fields(self):\n+        for model, expected_names in TEST_RESULTS['virtual_fields'].items():\n+            objects = model._meta.virtual_fields\n+            self.assertEqual(sorted([f.name for f in objects]), sorted(expected_names))\n+\n+\n+class GetFieldByNameTests(OptionsBaseTests):\n+\n+    def test_get_data_field(self):\n+        field_info = self._details(Person, Person._meta.get_field('data_abstract'))\n+        self.assertEqual(field_info[1:], (BasePerson, True, False))\n+        self.assertIsInstance(field_info[0], CharField)\n+\n+    def test_get_m2m_field(self):\n+        field_info = self._details(Person, Person._meta.get_field('m2m_base'))\n+        self.assertEqual(field_info[1:], (BasePerson, True, True))\n+        self.assertIsInstance(field_info[0], related.ManyToManyField)\n+\n+    def test_get_related_object(self):\n+        field_info = self._details(Person, Person._meta.get_field('relating_baseperson'))\n+        self.assertEqual(field_info[1:], (BasePerson, False, False))\n+        self.assertIsInstance(field_info[0], related.ForeignObjectRel)\n+\n+    def test_get_related_m2m(self):\n+        field_info = self._details(Person, Person._meta.get_field('relating_people'))\n+        self.assertEqual(field_info[1:], (None, False, True))\n+        self.assertIsInstance(field_info[0], related.ForeignObjectRel)\n+\n+    def test_get_generic_relation(self):\n+        field_info = self._details(Person, Person._meta.get_field('generic_relation_base'))\n+        self.assertEqual(field_info[1:], (None, True, False))\n+        self.assertIsInstance(field_info[0], GenericRelation)\n+\n+    def test_get_fields_only_searaches_forward_on_apps_not_ready(self):\n+        opts = Person._meta\n+        # If apps registry is not ready, get_field() searches over only\n+        # forward fields.\n+        opts.apps.ready = False\n+        try:\n+            # 'data_abstract' is a forward field, and therefore will be found\n+            self.assertTrue(opts.get_field('data_abstract'))\n+            msg = (\n+                \"Person has no field named 'relating_baseperson'. The app \"\n+                \"cache isn't ready yet, so if this is a forward field, it \"\n+                \"won't be available yet.\"\n+            )\n+            # 'data_abstract' is a reverse field, and will raise an exception\n+            with self.assertRaisesMessage(FieldDoesNotExist, msg):\n+                opts.get_field('relating_baseperson')\n+        finally:\n+            opts.apps.ready = True\n+\n+\n+class RelationTreeTests(TestCase):\n+    all_models = (Relation, AbstractPerson, BasePerson, Person, ProxyPerson, Relating)\n+\n+    def setUp(self):\n+        apps.clear_cache()\n+\n+    def test_clear_cache_clears_relation_tree(self):\n+        # The apps.clear_cache is setUp() should have deleted all trees.\n+        # Exclude abstract models that are not included in the Apps registry\n+        # and have no cache.\n+        all_models_with_cache = (m for m in self.all_models if not m._meta.abstract)\n+        for m in all_models_with_cache:\n+            self.assertNotIn('_relation_tree', m._meta.__dict__)\n+\n+    def test_first_relation_tree_access_populates_all(self):\n+        # On first access, relation tree should have populated cache.\n+        self.assertTrue(self.all_models[0]._meta._relation_tree)\n+\n+        # AbstractPerson does not have any relations, so relation_tree\n+        # should just return an EMPTY_RELATION_TREE.\n+        self.assertEqual(AbstractPerson._meta._relation_tree, EMPTY_RELATION_TREE)\n+\n+        # All the other models should already have their relation tree\n+        # in the internal __dict__ .\n+        all_models_but_abstractperson = (m for m in self.all_models if m is not AbstractPerson)\n+        for m in all_models_but_abstractperson:\n+            self.assertIn('_relation_tree', m._meta.__dict__)\n+\n+    def test_relations_related_objects(self):\n+        # Testing non hidden related objects\n+        self.assertEqual(\n+            sorted([field.related_query_name() for field in Relation._meta._relation_tree\n+                   if not field.rel.field.rel.is_hidden()]),\n+            sorted([\n+                'fk_abstract_rel', 'fk_abstract_rel', 'fk_abstract_rel', 'fk_base_rel', 'fk_base_rel',\n+                'fk_base_rel', 'fk_concrete_rel', 'fk_concrete_rel', 'fo_abstract_rel', 'fo_abstract_rel',\n+                'fo_abstract_rel', 'fo_base_rel', 'fo_base_rel', 'fo_base_rel', 'fo_concrete_rel',\n+                'fo_concrete_rel', 'm2m_abstract_rel', 'm2m_abstract_rel', 'm2m_abstract_rel',\n+                'm2m_base_rel', 'm2m_base_rel', 'm2m_base_rel', 'm2m_concrete_rel', 'm2m_concrete_rel',\n+            ])\n+        )\n+        # Testing hidden related objects\n+        self.assertEqual(\n+            sorted([field.related_query_name() for field in BasePerson._meta._relation_tree]),\n+            sorted([\n+                '+', '+', 'BasePerson_following_abstract+', 'BasePerson_following_abstract+',\n+                'BasePerson_following_base+', 'BasePerson_following_base+', 'BasePerson_friends_abstract+',\n+                'BasePerson_friends_abstract+', 'BasePerson_friends_base+', 'BasePerson_friends_base+',\n+                'BasePerson_m2m_abstract+', 'BasePerson_m2m_base+', 'Relating_basepeople+',\n+                'Relating_basepeople_hidden+', 'followers_abstract', 'followers_abstract', 'followers_abstract',\n+                'followers_base', 'followers_base', 'followers_base', 'friends_abstract_rel_+', 'friends_abstract_rel_+',\n+                'friends_abstract_rel_+', 'friends_base_rel_+', 'friends_base_rel_+', 'friends_base_rel_+', 'person',\n+                'person', 'relating_basepeople', 'relating_baseperson',\n+            ])\n+        )\n+        self.assertEqual([field.related_query_name() for field in AbstractPerson._meta._relation_tree], [])\ndiff --git a/tests/queries/tests.py b/tests/queries/tests.py\nindex aed122a7c13b..900149b873a0 100644\n--- a/tests/queries/tests.py\n+++ b/tests/queries/tests.py\n@@ -2093,7 +2093,7 @@ def test_no_fields_cloning(self):\n         testing is impossible, this is a sanity check against invalid use of\n         deepcopy. refs #16759.\n         \"\"\"\n-        opts_class = type(Note._meta.get_field_by_name(\"misc\")[0])\n+        opts_class = type(Note._meta.get_field(\"misc\"))\n         note_deepcopy = getattr(opts_class, \"__deepcopy__\", None)\n         opts_class.__deepcopy__ = lambda obj, memo: self.fail(\"Model fields shouldn't be cloned\")\n         try:\ndiff --git a/tests/queryset_pickle/tests.py b/tests/queryset_pickle/tests.py\nindex 22b61199f474..91fe304f9d80 100644\n--- a/tests/queryset_pickle/tests.py\n+++ b/tests/queryset_pickle/tests.py\n@@ -79,7 +79,7 @@ def test_model_pickle_m2m(self):\n         m1 = M2MModel.objects.create()\n         g1 = Group.objects.create(name='foof')\n         m1.groups.add(g1)\n-        m2m_through = M2MModel._meta.get_field_by_name('groups')[0].rel.through\n+        m2m_through = M2MModel._meta.get_field('groups').rel.through\n         original = m2m_through.objects.get()\n         dumped = pickle.dumps(original)\n         reloaded = pickle.loads(dumped)\ndiff --git a/tests/schema/tests.py b/tests/schema/tests.py\nindex e9de2131a967..b10ad07251b0 100644\n--- a/tests/schema/tests.py\n+++ b/tests/schema/tests.py\n@@ -138,7 +138,7 @@ def test_fk(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Book,\n-                Book._meta.get_field_by_name(\"author\")[0],\n+                Book._meta.get_field(\"author\"),\n                 new_field,\n                 strict=True,\n             )\n@@ -393,7 +393,7 @@ def test_alter(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Author,\n-                Author._meta.get_field_by_name(\"name\")[0],\n+                Author._meta.get_field(\"name\"),\n                 new_field,\n                 strict=True,\n             )\n@@ -424,7 +424,7 @@ def test_alter_text_field(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Note,\n-                Note._meta.get_field_by_name(\"info\")[0],\n+                Note._meta.get_field(\"info\"),\n                 new_field,\n                 strict=True,\n             )\n@@ -451,7 +451,7 @@ def test_alter_null_to_not_null(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Author,\n-                Author._meta.get_field_by_name(\"height\")[0],\n+                Author._meta.get_field(\"height\"),\n                 new_field\n             )\n         # Ensure the field is right afterwards\n@@ -479,7 +479,7 @@ def test_alter_null_to_not_null_keeping_default(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 AuthorWithDefaultHeight,\n-                AuthorWithDefaultHeight._meta.get_field_by_name(\"height\")[0],\n+                AuthorWithDefaultHeight._meta.get_field(\"height\"),\n                 new_field,\n             )\n         # Ensure the field is right afterwards\n@@ -512,7 +512,7 @@ def test_alter_fk(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Book,\n-                Book._meta.get_field_by_name(\"author\")[0],\n+                Book._meta.get_field(\"author\"),\n                 new_field,\n                 strict=True,\n             )\n@@ -542,7 +542,7 @@ def test_alter_implicit_id_to_explicit(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Author,\n-                Author._meta.get_field_by_name(\"id\")[0],\n+                Author._meta.get_field(\"id\"),\n                 new_field,\n                 strict=True,\n             )\n@@ -568,7 +568,7 @@ def test_rename(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Author,\n-                Author._meta.get_field_by_name(\"name\")[0],\n+                Author._meta.get_field(\"name\"),\n                 new_field,\n                 strict=True,\n             )\n@@ -587,7 +587,7 @@ def test_m2m_create(self):\n             editor.create_model(TagM2MTest)\n             editor.create_model(BookWithM2M)\n         # Ensure there is now an m2m table there\n-        columns = self.column_classes(BookWithM2M._meta.get_field_by_name(\"tags\")[0].rel.through)\n+        columns = self.column_classes(BookWithM2M._meta.get_field(\"tags\").rel.through)\n         self.assertEqual(columns['tagm2mtest_id'][0], \"IntegerField\")\n \n     def test_m2m_create_through(self):\n@@ -661,7 +661,7 @@ def test_m2m_through_alter(self):\n         self.assertEqual(len(self.column_classes(AuthorTag)), 3)\n         # \"Alter\" the field's blankness. This should not actually do anything.\n         with connection.schema_editor() as editor:\n-            old_field = AuthorWithM2MThrough._meta.get_field_by_name(\"tags\")[0]\n+            old_field = AuthorWithM2MThrough._meta.get_field(\"tags\")\n             new_field = ManyToManyField(\"schema.TagM2MTest\", related_name=\"authors\", through=\"AuthorTag\")\n             new_field.contribute_to_class(AuthorWithM2MThrough, \"tags\")\n             editor.alter_field(\n@@ -683,7 +683,7 @@ def test_m2m_repoint(self):\n             editor.create_model(TagM2MTest)\n             editor.create_model(UniqueTest)\n         # Ensure the M2M exists and points to TagM2MTest\n-        constraints = self.get_constraints(BookWithM2M._meta.get_field_by_name(\"tags\")[0].rel.through._meta.db_table)\n+        constraints = self.get_constraints(BookWithM2M._meta.get_field(\"tags\").rel.through._meta.db_table)\n         if connection.features.supports_foreign_keys:\n             for name, details in constraints.items():\n                 if details['columns'] == [\"tagm2mtest_id\"] and details['foreign_key']:\n@@ -698,11 +698,11 @@ def test_m2m_repoint(self):\n             with connection.schema_editor() as editor:\n                 editor.alter_field(\n                     Author,\n-                    BookWithM2M._meta.get_field_by_name(\"tags\")[0],\n+                    BookWithM2M._meta.get_field(\"tags\"),\n                     new_field,\n                 )\n             # Ensure old M2M is gone\n-            self.assertRaises(DatabaseError, self.column_classes, BookWithM2M._meta.get_field_by_name(\"tags\")[0].rel.through)\n+            self.assertRaises(DatabaseError, self.column_classes, BookWithM2M._meta.get_field(\"tags\").rel.through)\n             # Ensure the new M2M exists and points to UniqueTest\n             constraints = self.get_constraints(new_field.rel.through._meta.db_table)\n             if connection.features.supports_foreign_keys:\n@@ -715,10 +715,10 @@ def test_m2m_repoint(self):\n         finally:\n             # Cleanup through table separately\n             with connection.schema_editor() as editor:\n-                editor.remove_field(BookWithM2M, BookWithM2M._meta.get_field_by_name(\"uniques\")[0])\n+                editor.remove_field(BookWithM2M, BookWithM2M._meta.get_field(\"uniques\"))\n             # Cleanup model states\n             BookWithM2M._meta.local_many_to_many.remove(new_field)\n-            del BookWithM2M._meta._m2m_cache\n+            BookWithM2M._meta._expire_cache()\n \n     @unittest.skipUnless(connection.features.supports_column_check_constraints, \"No check constraints\")\n     def test_check_constraints(self):\n@@ -741,7 +741,7 @@ def test_check_constraints(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Author,\n-                Author._meta.get_field_by_name(\"height\")[0],\n+                Author._meta.get_field(\"height\"),\n                 new_field,\n                 strict=True,\n             )\n@@ -754,7 +754,7 @@ def test_check_constraints(self):\n             editor.alter_field(\n                 Author,\n                 new_field,\n-                Author._meta.get_field_by_name(\"height\")[0],\n+                Author._meta.get_field(\"height\"),\n                 strict=True,\n             )\n         constraints = self.get_constraints(Author._meta.db_table)\n@@ -781,7 +781,7 @@ def test_unique(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Tag,\n-                Tag._meta.get_field_by_name(\"slug\")[0],\n+                Tag._meta.get_field(\"slug\"),\n                 new_field,\n                 strict=True,\n             )\n@@ -809,8 +809,8 @@ def test_unique(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Tag,\n-                Tag._meta.get_field_by_name(\"slug\")[0],\n-                TagUniqueRename._meta.get_field_by_name(\"slug2\")[0],\n+                Tag._meta.get_field(\"slug\"),\n+                TagUniqueRename._meta.get_field(\"slug2\"),\n                 strict=True,\n             )\n         # Ensure the field is still unique\n@@ -976,7 +976,7 @@ def test_indexes(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 Book,\n-                Book._meta.get_field_by_name(\"title\")[0],\n+                Book._meta.get_field(\"title\"),\n                 new_field,\n                 strict=True,\n             )\n@@ -990,7 +990,7 @@ def test_indexes(self):\n             editor.alter_field(\n                 Book,\n                 new_field,\n-                Book._meta.get_field_by_name(\"title\")[0],\n+                Book._meta.get_field(\"title\"),\n                 strict=True,\n             )\n         # Ensure the table is there and has the index again\n@@ -1002,7 +1002,7 @@ def test_indexes(self):\n         with connection.schema_editor() as editor:\n             editor.add_field(\n                 Book,\n-                BookWithSlug._meta.get_field_by_name(\"slug\")[0],\n+                BookWithSlug._meta.get_field(\"slug\"),\n             )\n         self.assertIn(\n             \"slug\",\n@@ -1014,7 +1014,7 @@ def test_indexes(self):\n         with connection.schema_editor() as editor:\n             editor.alter_field(\n                 BookWithSlug,\n-                BookWithSlug._meta.get_field_by_name(\"slug\")[0],\n+                BookWithSlug._meta.get_field(\"slug\"),\n                 new_field2,\n                 strict=True,\n             )\n@@ -1039,10 +1039,10 @@ def test_primary_key(self):\n         new_field.set_attributes_from_name(\"slug\")\n         new_field.model = Tag\n         with connection.schema_editor() as editor:\n-            editor.remove_field(Tag, Tag._meta.get_field_by_name(\"id\")[0])\n+            editor.remove_field(Tag, Tag._meta.get_field(\"id\"))\n             editor.alter_field(\n                 Tag,\n-                Tag._meta.get_field_by_name(\"slug\")[0],\n+                Tag._meta.get_field(\"slug\"),\n                 new_field,\n             )\n         # Ensure the PK changed\n"
  },
  {
    "index": 20,
    "filtered_comments": [
      "I might be able to try test this out later this week. Seems to me we should give this a release or two in the real world before deprecating raw_id_fields. Like, make sure it's good enough in practice to replace raw_id_fields.\n",
      "Does `/foreignkey_json/` do permission checks? It seems to me it should require the same permissions as raw_id_fields (so if you're logged in to the admin, you can't just query that table unless you actually have permission, but maybe that's ok)\n\nEdit: Actually, maybe we just need to make sure that you have the change permission for the Model that has the foreign key. That way it follows the permissions of a normal ChoiceField.\n",
      "@guettli no, not really. We have `django-select2` and `django-autocomplete-light` for that.\nThe really tricky part is to know which queryset to server as a JSON. `django-select2` solves this by using the cache as a persistent storage shared by all application servers. In `django-autocomplete-light` you'll have to specify that explicitly.\n\nI don't see a way to get this into core. This should remain a admin only feature for now, just like the raw ID field.\n",
      "@timgraham I'm slowly getting there. One question, I want to add system checks, as we have for `raw_id_fields`. Should I add new once or make the one for `raw_id_fields` check the `autocomplete_fields` as well?\nhttps://docs.djangoproject.com/en/1.9/ref/checks/#admin\n",
      "No string opinion but I guess I don't see much harm in reusing the existing error codes for the checks if they work similarly.\n",
      "Please add tests in `tests/admin_checks` for the new checks.\n"
    ],
    "code_diff": "diff --git a/django/contrib/admin/checks.py b/django/contrib/admin/checks.py\nindex 830a190ff0bd..a9398db7e7a3 100644\n--- a/django/contrib/admin/checks.py\n+++ b/django/contrib/admin/checks.py\n@@ -66,6 +66,7 @@ class BaseModelAdminChecks:\n \n     def check(self, admin_obj, **kwargs):\n         errors = []\n+        errors.extend(self._check_autocomplete_fields(admin_obj))\n         errors.extend(self._check_raw_id_fields(admin_obj))\n         errors.extend(self._check_fields(admin_obj))\n         errors.extend(self._check_fieldsets(admin_obj))\n@@ -80,6 +81,61 @@ def check(self, admin_obj, **kwargs):\n         errors.extend(self._check_readonly_fields(admin_obj))\n         return errors\n \n+    def _check_autocomplete_fields(self, obj):\n+        \"\"\"\n+        Check that `autocomplete_fields` is a list or tuple of model fields.\n+        \"\"\"\n+        if not isinstance(obj.autocomplete_fields, (list, tuple)):\n+            return must_be('a list or tuple', option='autocomplete_fields', obj=obj, id='admin.E036')\n+        else:\n+            return list(chain.from_iterable([\n+                self._check_autocomplete_fields_item(obj, obj.model, field_name, 'autocomplete_fields[%d]' % index)\n+                for index, field_name in enumerate(obj.autocomplete_fields)\n+            ]))\n+\n+    def _check_autocomplete_fields_item(self, obj, model, field_name, label):\n+        \"\"\"\n+        Check that an item in `autocomplete_fields` is a ForeignKey or a\n+        ManyToManyField and that the item has a related ModelAdmin with\n+        search_fields defined.\n+        \"\"\"\n+        try:\n+            field = model._meta.get_field(field_name)\n+        except FieldDoesNotExist:\n+            return refer_to_missing_field(field=field_name, option=label, model=model, obj=obj, id='admin.E037')\n+        else:\n+            if not (field.many_to_many or field.many_to_one):\n+                return must_be(\n+                    'a foreign key or a many-to-many field',\n+                    option=label, obj=obj, id='admin.E038'\n+                )\n+            related_admin = obj.admin_site._registry.get(field.remote_field.model)\n+            if related_admin is None:\n+                return [\n+                    checks.Error(\n+                        'An admin for model \"%s\" has to be registered '\n+                        'to be referenced by %s.autocomplete_fields.' % (\n+                            field.remote_field.model.__name__,\n+                            type(obj).__name__,\n+                        ),\n+                        obj=obj.__class__,\n+                        id='admin.E039',\n+                    )\n+                ]\n+            elif not related_admin.search_fields:\n+                return [\n+                    checks.Error(\n+                        '%s must define \"search_fields\", because it\\'s '\n+                        'referenced by %s.autocomplete_fields.' % (\n+                            related_admin.__class__.__name__,\n+                            type(obj).__name__,\n+                        ),\n+                        obj=obj.__class__,\n+                        id='admin.E040',\n+                    )\n+                ]\n+            return []\n+\n     def _check_raw_id_fields(self, obj):\n         \"\"\" Check that `raw_id_fields` only contains field names that are listed\n         on the model. \"\"\"\ndiff --git a/django/contrib/admin/options.py b/django/contrib/admin/options.py\nindex a1c469c91e2a..7a4ff947a82d 100644\n--- a/django/contrib/admin/options.py\n+++ b/django/contrib/admin/options.py\n@@ -19,6 +19,10 @@\n     get_deleted_objects, lookup_needs_distinct, model_format_dict,\n     model_ngettext, quote, unquote,\n )\n+from django.contrib.admin.views.autocomplete import AutocompleteJsonView\n+from django.contrib.admin.widgets import (\n+    AutocompleteSelect, AutocompleteSelectMultiple,\n+)\n from django.contrib.auth import get_permission_codename\n from django.core.exceptions import (\n     FieldDoesNotExist, FieldError, PermissionDenied, ValidationError,\n@@ -94,6 +98,7 @@ class IncorrectLookupParameters(Exception):\n class BaseModelAdmin(metaclass=forms.MediaDefiningClass):\n     \"\"\"Functionality common to both ModelAdmin and InlineAdmin.\"\"\"\n \n+    autocomplete_fields = ()\n     raw_id_fields = ()\n     fields = None\n     exclude = None\n@@ -213,7 +218,10 @@ def formfield_for_foreignkey(self, db_field, request, **kwargs):\n         Get a form Field for a ForeignKey.\n         \"\"\"\n         db = kwargs.get('using')\n-        if db_field.name in self.raw_id_fields:\n+\n+        if db_field.name in self.get_autocomplete_fields(request):\n+            kwargs['widget'] = AutocompleteSelect(db_field.remote_field, using=db)\n+        elif db_field.name in self.raw_id_fields:\n             kwargs['widget'] = widgets.ForeignKeyRawIdWidget(db_field.remote_field, self.admin_site, using=db)\n         elif db_field.name in self.radio_fields:\n             kwargs['widget'] = widgets.AdminRadioSelect(attrs={\n@@ -238,7 +246,10 @@ def formfield_for_manytomany(self, db_field, request, **kwargs):\n             return None\n         db = kwargs.get('using')\n \n-        if db_field.name in self.raw_id_fields:\n+        autocomplete_fields = self.get_autocomplete_fields(request)\n+        if db_field.name in autocomplete_fields:\n+            kwargs['widget'] = AutocompleteSelectMultiple(db_field.remote_field, using=db)\n+        elif db_field.name in self.raw_id_fields:\n             kwargs['widget'] = widgets.ManyToManyRawIdWidget(db_field.remote_field, self.admin_site, using=db)\n         elif db_field.name in list(self.filter_vertical) + list(self.filter_horizontal):\n             kwargs['widget'] = widgets.FilteredSelectMultiple(\n@@ -252,12 +263,20 @@ def formfield_for_manytomany(self, db_field, request, **kwargs):\n                 kwargs['queryset'] = queryset\n \n         form_field = db_field.formfield(**kwargs)\n-        if isinstance(form_field.widget, SelectMultiple) and not isinstance(form_field.widget, CheckboxSelectMultiple):\n+        if (isinstance(form_field.widget, SelectMultiple) and\n+                not isinstance(form_field.widget, (CheckboxSelectMultiple, AutocompleteSelectMultiple))):\n             msg = _('Hold down \"Control\", or \"Command\" on a Mac, to select more than one.')\n             help_text = form_field.help_text\n             form_field.help_text = format_lazy('{} {}', help_text, msg) if help_text else msg\n         return form_field\n \n+    def get_autocomplete_fields(self, request):\n+        \"\"\"\n+        Return a list of ForeignKey and/or ManyToMany fields which should use\n+        an autocomplete widget.\n+        \"\"\"\n+        return self.autocomplete_fields\n+\n     def get_view_on_site_url(self, obj=None):\n         if obj is None or not self.view_on_site:\n             return None\n@@ -561,6 +580,7 @@ def wrapper(*args, **kwargs):\n         urlpatterns = [\n             url(r'^$', wrap(self.changelist_view), name='%s_%s_changelist' % info),\n             url(r'^add/$', wrap(self.add_view), name='%s_%s_add' % info),\n+            url(r'^autocomplete/$', wrap(self.autocomplete_view), name='%s_%s_autocomplete' % info),\n             url(r'^(.+)/history/$', wrap(self.history_view), name='%s_%s_history' % info),\n             url(r'^(.+)/delete/$', wrap(self.delete_view), name='%s_%s_delete' % info),\n             url(r'^(.+)/change/$', wrap(self.change_view), name='%s_%s_change' % info),\n@@ -1527,6 +1547,9 @@ def _changeform_view(self, request, object_id, form_url, extra_context):\n \n         return self.render_change_form(request, context, add=add, change=not add, obj=obj, form_url=form_url)\n \n+    def autocomplete_view(self, request):\n+        return AutocompleteJsonView.as_view(model_admin=self)(request)\n+\n     def add_view(self, request, form_url='', extra_context=None):\n         return self.changeform_view(request, None, form_url, extra_context)\n \ndiff --git a/django/contrib/admin/static/admin/css/autocomplete.css b/django/contrib/admin/static/admin/css/autocomplete.css\nnew file mode 100644\nindex 000000000000..c1a332d9baa4\n--- /dev/null\n+++ b/django/contrib/admin/static/admin/css/autocomplete.css\n@@ -0,0 +1,261 @@\n+select.admin-autocomplete {\n+    width: 20em;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container {\n+    min-height: 30px;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--single,\n+.select2-container--admin-autocomplete .select2-selection--multiple {\n+    min-height: 30px;\n+    padding: 0;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--focus .select2-selection,\n+.select2-container--admin-autocomplete.select2-container--open .select2-selection {\n+    border-color: #999;\n+    min-height: 30px;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--focus .select2-selection.select2-selection--single,\n+.select2-container--admin-autocomplete.select2-container--open .select2-selection.select2-selection--single {\n+    padding: 0;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--focus .select2-selection.select2-selection--multiple,\n+.select2-container--admin-autocomplete.select2-container--open .select2-selection.select2-selection--multiple {\n+    padding: 0;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--single {\n+    background-color: #fff;\n+    border: 1px solid #ccc;\n+    border-radius: 4px;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--single .select2-selection__rendered {\n+    color: #444;\n+    line-height: 30px;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--single .select2-selection__clear {\n+    cursor: pointer;\n+    float: right;\n+    font-weight: bold;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--single .select2-selection__placeholder {\n+    color: #999;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--single .select2-selection__arrow {\n+    height: 26px;\n+    position: absolute;\n+    top: 1px;\n+    right: 1px;\n+    width: 20px;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--single .select2-selection__arrow b {\n+    border-color: #888 transparent transparent transparent;\n+    border-style: solid;\n+    border-width: 5px 4px 0 4px;\n+    height: 0;\n+    left: 50%;\n+    margin-left: -4px;\n+    margin-top: -2px;\n+    position: absolute;\n+    top: 50%;\n+    width: 0;\n+}\n+\n+.select2-container--admin-autocomplete[dir=\"rtl\"] .select2-selection--single .select2-selection__clear {\n+    float: left;\n+}\n+\n+.select2-container--admin-autocomplete[dir=\"rtl\"] .select2-selection--single .select2-selection__arrow {\n+    left: 1px;\n+    right: auto;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--disabled .select2-selection--single {\n+    background-color: #eee;\n+    cursor: default;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--disabled .select2-selection--single .select2-selection__clear {\n+    display: none;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--open .select2-selection--single .select2-selection__arrow b {\n+    border-color: transparent transparent #888 transparent;\n+    border-width: 0 4px 5px 4px;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--multiple {\n+    background-color: white;\n+    border: 1px solid #ccc;\n+    border-radius: 4px;\n+    cursor: text;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--multiple .select2-selection__rendered {\n+    box-sizing: border-box;\n+    list-style: none;\n+    margin: 0;\n+    padding: 0 5px;\n+    width: 100%;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--multiple .select2-selection__rendered li {\n+    list-style: none;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--multiple .select2-selection__placeholder {\n+    color: #999;\n+    margin-top: 5px;\n+    float: left;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--multiple .select2-selection__clear {\n+    cursor: pointer;\n+    float: right;\n+    font-weight: bold;\n+    margin-top: 5px;\n+    margin-right: 10px;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--multiple .select2-selection__choice {\n+    background-color: #e4e4e4;\n+    border: 1px solid #ccc;\n+    border-radius: 4px;\n+    cursor: default;\n+    float: left;\n+    margin-right: 5px;\n+    margin-top: 5px;\n+    padding: 0 5px;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--multiple .select2-selection__choice__remove {\n+    color: #999;\n+    cursor: pointer;\n+    display: inline-block;\n+    font-weight: bold;\n+    margin-right: 2px;\n+}\n+\n+.select2-container--admin-autocomplete .select2-selection--multiple .select2-selection__choice__remove:hover {\n+    color: #333;\n+}\n+\n+.select2-container--admin-autocomplete[dir=\"rtl\"] .select2-selection--multiple .select2-selection__choice, .select2-container--admin-autocomplete[dir=\"rtl\"] .select2-selection--multiple .select2-selection__placeholder, .select2-container--admin-autocomplete[dir=\"rtl\"] .select2-selection--multiple .select2-search--inline {\n+    float: right;\n+}\n+\n+.select2-container--admin-autocomplete[dir=\"rtl\"] .select2-selection--multiple .select2-selection__choice {\n+    margin-left: 5px;\n+    margin-right: auto;\n+}\n+\n+.select2-container--admin-autocomplete[dir=\"rtl\"] .select2-selection--multiple .select2-selection__choice__remove {\n+    margin-left: 2px;\n+    margin-right: auto;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--focus .select2-selection--multiple {\n+    border: solid #999 1px;\n+    outline: 0;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--disabled .select2-selection--multiple {\n+    background-color: #eee;\n+    cursor: default;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--disabled .select2-selection__choice__remove {\n+    display: none;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--open.select2-container--above .select2-selection--single, .select2-container--admin-autocomplete.select2-container--open.select2-container--above .select2-selection--multiple {\n+    border-top-left-radius: 0;\n+    border-top-right-radius: 0;\n+}\n+\n+.select2-container--admin-autocomplete.select2-container--open.select2-container--below .select2-selection--single, .select2-container--admin-autocomplete.select2-container--open.select2-container--below .select2-selection--multiple {\n+    border-bottom-left-radius: 0;\n+    border-bottom-right-radius: 0;\n+}\n+\n+.select2-container--admin-autocomplete .select2-search--dropdown .select2-search__field {\n+    border: 1px solid #ccc;\n+}\n+\n+.select2-container--admin-autocomplete .select2-search--inline .select2-search__field {\n+    background: transparent;\n+    border: none;\n+    outline: 0;\n+    box-shadow: none;\n+    -webkit-appearance: textfield;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results > .select2-results__options {\n+    max-height: 200px;\n+    overflow-y: auto;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option[role=group] {\n+    padding: 0;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option[aria-disabled=true] {\n+    color: #999;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option[aria-selected=true] {\n+    background-color: #ddd;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option .select2-results__option {\n+    padding-left: 1em;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option .select2-results__option .select2-results__group {\n+    padding-left: 0;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option .select2-results__option .select2-results__option {\n+    margin-left: -1em;\n+    padding-left: 2em;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option .select2-results__option .select2-results__option .select2-results__option {\n+    margin-left: -2em;\n+    padding-left: 3em;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option .select2-results__option .select2-results__option .select2-results__option .select2-results__option {\n+    margin-left: -3em;\n+    padding-left: 4em;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option .select2-results__option .select2-results__option .select2-results__option .select2-results__option .select2-results__option {\n+    margin-left: -4em;\n+    padding-left: 5em;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option .select2-results__option .select2-results__option .select2-results__option .select2-results__option .select2-results__option .select2-results__option {\n+    margin-left: -5em;\n+    padding-left: 6em;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__option--highlighted[aria-selected] {\n+    background-color: #79aec8;\n+    color: white;\n+}\n+\n+.select2-container--admin-autocomplete .select2-results__group {\n+    cursor: default;\n+    display: block;\n+    padding: 6px;\n+}\ndiff --git a/django/contrib/admin/static/admin/js/admin/RelatedObjectLookups.js b/django/contrib/admin/static/admin/js/admin/RelatedObjectLookups.js\nindex 3fb1e5255043..3d4d916654a4 100644\n--- a/django/contrib/admin/static/admin/js/admin/RelatedObjectLookups.js\n+++ b/django/contrib/admin/static/admin/js/admin/RelatedObjectLookups.js\n@@ -108,6 +108,12 @@\n                 this.value = newId;\n             }\n         });\n+        selects.next().find('.select2-selection__rendered').each(function() {\n+            // The element can have a clear button as a child.\n+            // Use the lastChild to modify only the displayed value.\n+            this.lastChild.textContent = newRepr;\n+            this.title = newRepr;\n+        });\n         win.close();\n     }\n \ndiff --git a/django/contrib/admin/static/admin/js/autocomplete.js b/django/contrib/admin/static/admin/js/autocomplete.js\nnew file mode 100644\nindex 000000000000..15321f974da2\n--- /dev/null\n+++ b/django/contrib/admin/static/admin/js/autocomplete.js\n@@ -0,0 +1,38 @@\n+(function($) {\n+    'use strict';\n+    var init = function($element, options) {\n+        var settings = $.extend({\n+            ajax: {\n+                data: function(params) {\n+                    return {\n+                        term: params.term,\n+                        page: params.page\n+                    };\n+                }\n+            }\n+        }, options);\n+        $element.select2(settings);\n+    };\n+\n+    $.fn.djangoAdminSelect2 = function(options) {\n+        var settings = $.extend({}, options);\n+        $.each(this, function(i, element) {\n+            var $element = $(element);\n+            init($element, settings);\n+        });\n+        return this;\n+    };\n+\n+    $(function() {\n+        $('.admin-autocomplete').djangoAdminSelect2();\n+    });\n+\n+    $(document).on('formset:added', (function() {\n+        return function(event, $newFormset) {\n+            var $widget = $newFormset.find('.admin-autocomplete');\n+            // Exclude already initialized Select2 inputs.\n+            $widget = $widget.not('.select2-hidden-accessible');\n+            return init($widget);\n+        };\n+    })(this));\n+}(django.jQuery));\ndiff --git a/django/contrib/admin/views/autocomplete.py b/django/contrib/admin/views/autocomplete.py\nnew file mode 100644\nindex 000000000000..5d826dd44eec\n--- /dev/null\n+++ b/django/contrib/admin/views/autocomplete.py\n@@ -0,0 +1,52 @@\n+from django.http import Http404, JsonResponse\n+from django.views.generic.list import BaseListView\n+\n+\n+class AutocompleteJsonView(BaseListView):\n+    \"\"\"Handle AutocompleteWidget's AJAX requests for data.\"\"\"\n+    paginate_by = 20\n+    model_admin = None\n+\n+    def get(self, request, *args, **kwargs):\n+        \"\"\"\n+        Return a JsonResponse with search results of the form:\n+        {\n+            results: [{id: \"123\" text: \"foo\"}],\n+            pagination: {more: true}\n+        }\n+        \"\"\"\n+        if not self.model_admin.get_search_fields(request):\n+            raise Http404(\n+                '%s must have search_fields for the autocomplete_view.' %\n+                type(self.model_admin).__name__\n+            )\n+        if not self.has_perm(request):\n+            return JsonResponse({'error': '403 Forbidden'}, status=403)\n+\n+        self.term = request.GET.get('term', '')\n+        self.paginator_class = self.model_admin.paginator\n+        self.object_list = self.get_queryset()\n+        context = self.get_context_data()\n+        return JsonResponse({\n+            'results': [\n+                {'id': str(obj.pk), 'text': str(obj)}\n+                for obj in context['object_list']\n+            ],\n+            'pagination': {'more': context['page_obj'].has_next()},\n+        })\n+\n+    def get_paginator(self, *args, **kwargs):\n+        \"\"\"Use the ModelAdmin's paginator.\"\"\"\n+        return self.model_admin.get_paginator(self.request, *args, **kwargs)\n+\n+    def get_queryset(self):\n+        \"\"\"Return queryset based on ModelAdmin.get_search_results().\"\"\"\n+        qs = self.model_admin.get_queryset(self.request)\n+        qs, search_use_distinct = self.model_admin.get_search_results(self.request, qs, self.term)\n+        if search_use_distinct:\n+            qs = qs.distinct()\n+        return qs\n+\n+    def has_perm(self, request, obj=None):\n+        \"\"\"Check if user has permission to access the related model.\"\"\"\n+        return self.model_admin.has_change_permission(request, obj=obj)\ndiff --git a/django/contrib/admin/widgets.py b/django/contrib/admin/widgets.py\nindex 0f5f3c7dabf4..b9d45a10eed4 100644\n--- a/django/contrib/admin/widgets.py\n+++ b/django/contrib/admin/widgets.py\n@@ -2,6 +2,7 @@\n Form Widget classes specific to the Django admin site.\n \"\"\"\n import copy\n+import json\n \n from django import forms\n from django.conf import settings\n@@ -11,7 +12,7 @@\n from django.utils.html import smart_urlquote\n from django.utils.safestring import mark_safe\n from django.utils.text import Truncator\n-from django.utils.translation import gettext as _\n+from django.utils.translation import get_language, gettext as _\n \n \n class FilteredSelectMultiple(forms.SelectMultiple):\n@@ -380,3 +381,115 @@ def __init__(self, attrs=None):\n \n class AdminBigIntegerFieldWidget(AdminIntegerFieldWidget):\n     class_name = 'vBigIntegerField'\n+\n+\n+# Mapping of lower case language codes [returned by Django's get_language()]\n+# to language codes supported by select2.\n+# See django/contrib/admin/static/admin/js/vendor/select2/i18n/*\n+SELECT2_TRANSLATIONS = {x.lower(): x for x in [\n+    'ar', 'az', 'bg', 'ca', 'cs', 'da', 'de', 'el', 'en', 'es', 'et',\n+    'eu', 'fa', 'fi', 'fr', 'gl', 'he', 'hi', 'hr', 'hu', 'id', 'is',\n+    'it', 'ja', 'km', 'ko', 'lt', 'lv', 'mk', 'ms', 'nb', 'nl', 'pl',\n+    'pt-BR', 'pt', 'ro', 'ru', 'sk', 'sr-Cyrl', 'sr', 'sv', 'th',\n+    'tr', 'uk', 'vi', 'zh-CN', 'zh-TW',\n+]}\n+\n+\n+class AutocompleteMixin:\n+    \"\"\"\n+    Select widget mixin that loads options from AutocompleteJsonView via AJAX.\n+\n+    Renders the necessary data attributes for select2 and adds the static form\n+    media.\n+    \"\"\"\n+    url_name = 'admin:%s_%s_autocomplete'\n+\n+    def __init__(self, rel, attrs=None, choices=(), using=None):\n+        self.rel = rel\n+        self.db = using\n+        self.choices = choices\n+        if attrs is not None:\n+            self.attrs = attrs.copy()\n+        else:\n+            self.attrs = {}\n+\n+    def get_url(self):\n+        model = self.rel.model\n+        return reverse(self.url_name % (model._meta.app_label, model._meta.model_name))\n+\n+    def build_attrs(self, base_attrs, extra_attrs=None):\n+        \"\"\"\n+        Set select2's AJAX attributes.\n+\n+        Attributes can be set using the html5 data attribute.\n+        Nested attributes require a double dash as per\n+        https://select2.org/configuration/data-attributes#nested-subkey-options\n+        \"\"\"\n+        attrs = super().build_attrs(base_attrs, extra_attrs=extra_attrs)\n+        attrs.setdefault('class', '')\n+        attrs.update({\n+            'data-ajax--cache': 'true',\n+            'data-ajax--type': 'GET',\n+            'data-ajax--url': self.get_url(),\n+            'data-theme': 'admin-autocomplete',\n+            'data-allow-clear': json.dumps(not self.is_required),\n+            'data-placeholder': '',  # Allows clearing of the input.\n+            'class': attrs['class'] + 'admin-autocomplete',\n+        })\n+        return attrs\n+\n+    def optgroups(self, name, value, attr=None):\n+        \"\"\"Return selected options based on the ModelChoiceIterator.\"\"\"\n+        default = (None, [], 0)\n+        groups = [default]\n+        has_selected = False\n+        selected_choices = {\n+            str(v) for v in value\n+            if str(v) not in self.choices.field.empty_values\n+        }\n+        if not self.is_required and not self.allow_multiple_selected:\n+            default[1].append(self.create_option(name, '', '', False, 0))\n+        choices = (\n+            (obj.pk, self.choices.field.label_from_instance(obj))\n+            for obj in self.choices.queryset.using(self.db).filter(pk__in=selected_choices)\n+        )\n+        for option_value, option_label in choices:\n+            selected = (\n+                str(option_value) in value and\n+                (has_selected is False or self.allow_multiple_selected)\n+            )\n+            if selected is True and has_selected is False:\n+                has_selected = True\n+            index = len(default[1])\n+            subgroup = default[1]\n+            subgroup.append(self.create_option(name, option_value, option_label, selected_choices, index))\n+        return groups\n+\n+    @property\n+    def media(self):\n+        extra = '' if settings.DEBUG else '.min'\n+        i18n_name = SELECT2_TRANSLATIONS.get(get_language())\n+        i18n_file = ('admin/js/vendor/select2/i18n/%s.js' % i18n_name,) if i18n_name else ()\n+        return forms.Media(\n+            js=(\n+                'admin/js/vendor/jquery/jquery%s.js' % extra,\n+                'admin/js/vendor/select2/select2.full%s.js' % extra,\n+            ) + i18n_file + (\n+                'admin/js/jquery.init.js',\n+                'admin/js/autocomplete.js',\n+            ),\n+            css={\n+                'screen': (\n+                    'admin/css/vendor/select2/select2%s.css' % extra,\n+                    'admin/css/autocomplete.css',\n+                ),\n+            },\n+        )\n+\n+\n+class AutocompleteSelect(AutocompleteMixin, forms.Select):\n+    pass\n+\n+\n+class AutocompleteSelectMultiple(AutocompleteMixin, forms.SelectMultiple):\n+    pass\ndiff --git a/docs/ref/checks.txt b/docs/ref/checks.txt\nindex b4d1fd408945..0ca1d176f451 100644\n--- a/docs/ref/checks.txt\n+++ b/docs/ref/checks.txt\n@@ -527,6 +527,15 @@ with the admin site:\n * **admin.E034**: The value of ``readonly_fields`` must be a list or tuple.\n * **admin.E035**: The value of ``readonly_fields[n]`` is not a callable, an\n   attribute of ``<ModelAdmin class>``, or an attribute of ``<model>``.\n+* **admin.E036**: The value of ``autocomplete_fields`` must be a list or tuple.\n+* **admin.E037**: The value of ``autocomplete_fields[n]`` refers to\n+  ``<field name>``, which is not an attribute of ``<model>``.\n+* **admin.E038**: The value of ``autocomplete_fields[n]`` must be a foreign\n+  key or a many-to-many field.\n+* **admin.E039**: An admin for model ``<model>`` has to be registered to be\n+  referenced by ``<modeladmin>.autocomplete_fields``.\n+* **admin.E040**: ``<modeladmin>`` must define ``search_fields``, because\n+  it's referenced by ``<other_modeladmin>.autocomplete_fields``.\n \n ``ModelAdmin``\n ~~~~~~~~~~~~~~\ndiff --git a/docs/ref/contrib/admin/index.txt b/docs/ref/contrib/admin/index.txt\nindex 51f9e70b732b..965150cc3a4b 100644\n--- a/docs/ref/contrib/admin/index.txt\n+++ b/docs/ref/contrib/admin/index.txt\n@@ -519,11 +519,13 @@ subclass::\n         If you want to use a custom widget with a relation field (i.e.\n         :class:`~django.db.models.ForeignKey` or\n         :class:`~django.db.models.ManyToManyField`), make sure you haven't\n-        included that field's name in ``raw_id_fields`` or ``radio_fields``.\n+        included that field's name in ``raw_id_fields``, ``radio_fields``, or\n+        ``autocomplete_fields``.\n \n         ``formfield_overrides`` won't let you change the widget on relation\n-        fields that have ``raw_id_fields`` or ``radio_fields`` set. That's\n-        because ``raw_id_fields`` and ``radio_fields`` imply custom widgets of\n+        fields that have ``raw_id_fields``, ``radio_fields``, or\n+        ``autocomplete_fields`` set. That's because ``raw_id_fields``,\n+        ``radio_fields``, and ``autocomplete_fields`` imply custom widgets of\n         their own.\n \n .. attribute:: ModelAdmin.inlines\n@@ -1071,6 +1073,58 @@ subclass::\n     Don't include a field in ``radio_fields`` unless it's a ``ForeignKey`` or has\n     ``choices`` set.\n \n+.. attribute:: ModelAdmin.autocomplete_fields\n+\n+    .. versionadded:: 2.0\n+\n+    ``autocomplete_fields`` is a list of ``ForeignKey`` and/or\n+    ``ManyToManyField`` fields you would like to change to `Select2\n+    <https://select2.org/>`_ autocomplete inputs.\n+\n+    By default, the admin uses a select-box interface (``<select>``) for fields\n+    that are . Sometimes you don't want to incur the overhead of selecting all\n+    the related instances to display in the dropdown.\n+\n+    The Select2 input looks similar to the default input but comes with a\n+    search feature that loads the options asynchronously. This is faster and\n+    more user-friendly if the related model has many instances.\n+\n+    You must define :attr:`~ModelAdmin.search_fields` on the related object's\n+    ``ModelAdmin`` because the autocomplete search uses it.\n+\n+    Ordering and pagination of the results are controlled by the related\n+    ``ModelAdmin``'s :meth:`~ModelAdmin.get_ordering` and\n+    :meth:`~ModelAdmin.get_paginator` methods.\n+\n+    In the following example, ``ChoiceAdmin`` has an autocomplete field for the\n+    ``ForeignKey`` to the ``Question``. The results are filtered by the\n+    ``question_text`` field and ordered by the ``date_created`` field::\n+\n+        class QuestionAdmin(admin.ModelAdmin):\n+            ordering = ['date_created']\n+            search_fields = ['question_text']\n+\n+        class ChoiceAdmin(admin.ModelAdmin):\n+            autocomplete_fields = ['question']\n+\n+    .. admonition:: Performance considerations for large datasets\n+\n+        Ordering using :attr:`ModelAdmin.ordering` may cause performance\n+        problems as sorting on a large queryset will be slow.\n+\n+        Also, if your search fields include fields that aren't indexed by the\n+        database, you might encounter poor performance on extremely large\n+        tables.\n+\n+        For those cases, it's a good idea to write your own\n+        :func:`ModelAdmin.get_search_results` implementation using a\n+        full-text indexed search.\n+\n+        You may also want to change the ``Paginator`` on very large tables\n+        as the default paginator always performs a ``count()`` query.\n+        For example, you could override the default implementation of the\n+        ``Paginator.count`` property.\n+\n .. attribute:: ModelAdmin.raw_id_fields\n \n     By default, Django's admin uses a select-box interface (<select>) for\n@@ -1431,6 +1485,15 @@ templates used by the :class:`ModelAdmin` views:\n     pre- or post-save operations for objects related to the parent. Note\n     that at this point the parent object and its form have already been saved.\n \n+.. method:: ModelAdmin.get_autocomplete_fields(request)\n+\n+    .. versionadded:: 2.0\n+\n+    The ``get_readonly_fields()`` method is given the ``HttpRequest`` and is\n+    expected to return a ``list`` or ``tuple`` of field names that will be\n+    displayed with an autocomplete widget as described above in the\n+    :attr:`ModelAdmin.autocomplete_fields` section.\n+\n .. method:: ModelAdmin.get_readonly_fields(request, obj=None)\n \n     The ``get_readonly_fields`` method is given the ``HttpRequest`` and the\ndiff --git a/docs/releases/2.0.txt b/docs/releases/2.0.txt\nindex fd9cf8dfd3d6..b45e49480002 100644\n--- a/docs/releases/2.0.txt\n+++ b/docs/releases/2.0.txt\n@@ -66,7 +66,10 @@ Minor features\n :mod:`django.contrib.admin`\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-* ...\n+* The new :attr:`.ModelAdmin.autocomplete_fields` attribute and\n+  :meth:`.ModelAdmin.get_autocomplete_fields` method allow using an\n+  `Select2 <https://select2.org>`_ search widget for ``ForeignKey`` and\n+  ``ManyToManyField``.\n \n :mod:`django.contrib.admindocs`\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ndiff --git a/docs/spelling_wordlist b/docs/spelling_wordlist\nindex 4cfdc6590634..08b70f4ac3eb 100644\n--- a/docs/spelling_wordlist\n+++ b/docs/spelling_wordlist\n@@ -27,6 +27,7 @@ attr\n auth\n autoclobber\n autocommit\n+autocomplete\n autocompletion\n autodetect\n autodetectable\ndiff --git a/tests/admin_views/admin.py b/tests/admin_views/admin.py\nindex 6139f0460f71..2c58baea7ad6 100644\n--- a/tests/admin_views/admin.py\n+++ b/tests/admin_views/admin.py\n@@ -97,6 +97,7 @@ class ArticleAdmin(admin.ModelAdmin):\n     )\n     list_editable = ('section',)\n     list_filter = ('date', 'section')\n+    autocomplete_fields = ('section',)\n     view_on_site = False\n     fieldsets = (\n         ('Some fields', {\n@@ -497,6 +498,10 @@ class PizzaAdmin(admin.ModelAdmin):\n     readonly_fields = ('toppings',)\n \n \n+class StudentAdmin(admin.ModelAdmin):\n+    search_fields = ('name',)\n+\n+\n class WorkHourAdmin(admin.ModelAdmin):\n     list_display = ('datum', 'employee')\n     list_filter = ('employee',)\n@@ -603,6 +608,16 @@ class AlbumAdmin(admin.ModelAdmin):\n     list_filter = ['title']\n \n \n+class QuestionAdmin(admin.ModelAdmin):\n+    ordering = ['-posted']\n+    search_fields = ['question']\n+    autocomplete_fields = ['related_questions']\n+\n+\n+class AnswerAdmin(admin.ModelAdmin):\n+    autocomplete_fields = ['question']\n+\n+\n class PrePopulatedPostLargeSlugAdmin(admin.ModelAdmin):\n     prepopulated_fields = {\n         'slug': ('title',)\n@@ -664,12 +679,17 @@ class CustomTemplateFilterColorAdmin(admin.ModelAdmin):\n class RelatedPrepopulatedInline1(admin.StackedInline):\n     fieldsets = (\n         (None, {\n-            'fields': (('pubdate', 'status'), ('name', 'slug1', 'slug2',),)\n+            'fields': (\n+                ('fk', 'm2m'),\n+                ('pubdate', 'status'),\n+                ('name', 'slug1', 'slug2',),\n+            ),\n         }),\n     )\n     formfield_overrides = {models.CharField: {'strip': False}}\n     model = RelatedPrepopulated\n     extra = 1\n+    autocomplete_fields = ['fk', 'm2m']\n     prepopulated_fields = {'slug1': ['name', 'pubdate'],\n                            'slug2': ['status', 'name']}\n \n@@ -677,12 +697,19 @@ class RelatedPrepopulatedInline1(admin.StackedInline):\n class RelatedPrepopulatedInline2(admin.TabularInline):\n     model = RelatedPrepopulated\n     extra = 1\n+    autocomplete_fields = ['fk', 'm2m']\n     prepopulated_fields = {'slug1': ['name', 'pubdate'],\n                            'slug2': ['status', 'name']}\n \n \n+class RelatedPrepopulatedInline3(admin.TabularInline):\n+    model = RelatedPrepopulated\n+    extra = 0\n+    autocomplete_fields = ['fk', 'm2m']\n+\n+\n class MainPrepopulatedAdmin(admin.ModelAdmin):\n-    inlines = [RelatedPrepopulatedInline1, RelatedPrepopulatedInline2]\n+    inlines = [RelatedPrepopulatedInline1, RelatedPrepopulatedInline2, RelatedPrepopulatedInline3]\n     fieldsets = (\n         (None, {\n             'fields': (('pubdate', 'status'), ('name', 'slug1', 'slug2', 'slug3'))\n@@ -894,7 +921,10 @@ def get_formsets_with_inlines(self, request, obj=None):\n site.site_url = '/my-site-url/'\n site.register(Article, ArticleAdmin)\n site.register(CustomArticle, CustomArticleAdmin)\n-site.register(Section, save_as=True, inlines=[ArticleInline], readonly_fields=['name_property'])\n+site.register(\n+    Section, save_as=True, inlines=[ArticleInline],\n+    readonly_fields=['name_property'], search_fields=['name'],\n+)\n site.register(ModelWithStringPrimaryKey)\n site.register(Color)\n site.register(Thing, ThingAdmin)\n@@ -956,6 +986,7 @@ def get_formsets_with_inlines(self, request, obj=None):\n site.register(ReferencedByGenRel)\n site.register(GenRelReference)\n site.register(ParentWithUUIDPK)\n+site.register(RelatedPrepopulated, search_fields=['name'])\n site.register(RelatedWithUUIDPKModel)\n \n # We intentionally register Promo and ChapterXtra1 but not Chapter nor ChapterXtra2.\n@@ -973,8 +1004,8 @@ def get_formsets_with_inlines(self, request, obj=None):\n site.register(ReadablePizza)\n site.register(Topping, ToppingAdmin)\n site.register(Album, AlbumAdmin)\n-site.register(Question)\n-site.register(Answer, date_hierarchy='question__posted')\n+site.register(Question, QuestionAdmin)\n+site.register(Answer, AnswerAdmin, date_hierarchy='question__posted')\n site.register(Answer2, date_hierarchy='question__expires')\n site.register(PrePopulatedPost, PrePopulatedPostAdmin)\n site.register(ComplexSortedPerson, ComplexSortedPersonAdmin)\ndiff --git a/tests/admin_views/customadmin.py b/tests/admin_views/customadmin.py\nindex 2fbbb7859590..5ee8c0c15976 100644\n--- a/tests/admin_views/customadmin.py\n+++ b/tests/admin_views/customadmin.py\n@@ -49,7 +49,7 @@ class CustomPwdTemplateUserAdmin(UserAdmin):\n site = Admin2(name=\"admin2\")\n \n site.register(models.Article, base_admin.ArticleAdmin)\n-site.register(models.Section, inlines=[base_admin.ArticleInline])\n+site.register(models.Section, inlines=[base_admin.ArticleInline], search_fields=['name'])\n site.register(models.Thing, base_admin.ThingAdmin)\n site.register(models.Fabric, base_admin.FabricAdmin)\n site.register(models.ChapterXtra1, base_admin.ChapterXtra1Admin)\ndiff --git a/tests/admin_views/models.py b/tests/admin_views/models.py\nindex dd4921d1ce14..fc229cf3bd3f 100644\n--- a/tests/admin_views/models.py\n+++ b/tests/admin_views/models.py\n@@ -600,6 +600,10 @@ class Question(models.Model):\n     question = models.CharField(max_length=20)\n     posted = models.DateField(default=datetime.date.today)\n     expires = models.DateTimeField(null=True, blank=True)\n+    related_questions = models.ManyToManyField('self')\n+\n+    def __str__(self):\n+        return self.question\n \n \n class Answer(models.Model):\n@@ -746,6 +750,8 @@ class MainPrepopulated(models.Model):\n class RelatedPrepopulated(models.Model):\n     parent = models.ForeignKey(MainPrepopulated, models.CASCADE)\n     name = models.CharField(max_length=75)\n+    fk = models.ForeignKey('self', models.CASCADE, blank=True, null=True)\n+    m2m = models.ManyToManyField('self', blank=True)\n     pubdate = models.DateField()\n     status = models.CharField(\n         max_length=20,\n@@ -906,7 +912,6 @@ class InlineReference(models.Model):\n     )\n \n \n-# Models for #23604 and #23915\n class Recipe(models.Model):\n     rname = models.CharField(max_length=20, unique=True)\n \n@@ -957,3 +962,12 @@ def __str__(self):\n \n class RelatedWithUUIDPKModel(models.Model):\n     parent = models.ForeignKey(ParentWithUUIDPK, on_delete=models.SET_NULL, null=True, blank=True)\n+\n+\n+class Author(models.Model):\n+    pass\n+\n+\n+class Authorship(models.Model):\n+    book = models.ForeignKey(Book, models.CASCADE)\n+    author = models.ForeignKey(Author, models.CASCADE)\ndiff --git a/tests/admin_views/test_autocomplete_view.py b/tests/admin_views/test_autocomplete_view.py\nnew file mode 100644\nindex 000000000000..8396ceb5d1d5\n--- /dev/null\n+++ b/tests/admin_views/test_autocomplete_view.py\n@@ -0,0 +1,231 @@\n+import json\n+\n+from django.contrib import admin\n+from django.contrib.admin import site\n+from django.contrib.admin.tests import AdminSeleniumTestCase\n+from django.contrib.admin.views.autocomplete import AutocompleteJsonView\n+from django.contrib.auth.models import Permission, User\n+from django.contrib.contenttypes.models import ContentType\n+from django.http import Http404\n+from django.test import RequestFactory, override_settings\n+from django.urls import reverse, reverse_lazy\n+\n+from .admin import AnswerAdmin, QuestionAdmin\n+from .models import Answer, Author, Authorship, Book, Question\n+from .tests import AdminViewBasicTestCase\n+\n+PAGINATOR_SIZE = AutocompleteJsonView.paginate_by\n+\n+\n+class AuthorAdmin(admin.ModelAdmin):\n+    search_fields = ['id']\n+\n+\n+class AuthorshipInline(admin.TabularInline):\n+    model = Authorship\n+    autocomplete_fields = ['author']\n+\n+\n+class BookAdmin(admin.ModelAdmin):\n+    inlines = [AuthorshipInline]\n+\n+\n+site.register(Question, QuestionAdmin)\n+site.register(Answer, AnswerAdmin)\n+site.register(Author, AuthorAdmin)\n+site.register(Book, BookAdmin)\n+\n+\n+class AutocompleteJsonViewTests(AdminViewBasicTestCase):\n+    as_view_args = {'model_admin': QuestionAdmin(Question, site)}\n+    factory = RequestFactory()\n+    url = reverse_lazy('admin:admin_views_question_autocomplete')\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.user = User.objects.create_user(\n+            username='user', password='secret',\n+            email='user@example.com', is_staff=True,\n+        )\n+        super().setUpTestData()\n+\n+    def test_success(self):\n+        q = Question.objects.create(question='Is this a question?')\n+        request = self.factory.get(self.url, {'term': 'is'})\n+        request.user = self.superuser\n+        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n+        self.assertEqual(response.status_code, 200)\n+        data = json.loads(response.content.decode('utf-8'))\n+        self.assertEqual(data, {\n+            'results': [{'id': str(q.pk), 'text': q.question}],\n+            'pagination': {'more': False},\n+        })\n+\n+    def test_must_be_logged_in(self):\n+        response = self.client.get(self.url, {'term': ''})\n+        self.assertEqual(response.status_code, 200)\n+        self.client.logout()\n+        response = self.client.get(self.url, {'term': ''})\n+        self.assertEqual(response.status_code, 302)\n+\n+    def test_has_change_permission_required(self):\n+        \"\"\"\n+        Users require the change permission for the related model to the\n+        autocomplete view for it.\n+        \"\"\"\n+        request = self.factory.get(self.url, {'term': 'is'})\n+        self.user.is_staff = True\n+        self.user.save()\n+        request.user = self.user\n+        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n+        self.assertEqual(response.status_code, 403)\n+        self.assertJSONEqual(response.content.decode('utf-8'), {'error': '403 Forbidden'})\n+        # Add the change permission and retry.\n+        p = Permission.objects.get(\n+            content_type=ContentType.objects.get_for_model(Question),\n+            codename='change_question',\n+        )\n+        self.user.user_permissions.add(p)\n+        request.user = User.objects.get(pk=self.user.pk)\n+        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n+        self.assertEqual(response.status_code, 200)\n+\n+    def test_search_use_distinct(self):\n+        \"\"\"\n+        Searching across model relations use QuerySet.distinct() to avoid\n+        duplicates.\n+        \"\"\"\n+        q1 = Question.objects.create(question='question 1')\n+        q2 = Question.objects.create(question='question 2')\n+        q2.related_questions.add(q1)\n+        q3 = Question.objects.create(question='question 3')\n+        q3.related_questions.add(q1)\n+        request = self.factory.get(self.url, {'term': 'question'})\n+        request.user = self.superuser\n+\n+        class DistinctQuestionAdmin(QuestionAdmin):\n+            search_fields = ['related_questions__question', 'question']\n+\n+        model_admin = DistinctQuestionAdmin(Question, site)\n+        response = AutocompleteJsonView.as_view(model_admin=model_admin)(request)\n+        self.assertEqual(response.status_code, 200)\n+        data = json.loads(response.content.decode('utf-8'))\n+        self.assertEqual(len(data['results']), 3)\n+\n+    def test_missing_search_fields(self):\n+        class EmptySearchAdmin(QuestionAdmin):\n+            search_fields = []\n+\n+        model_admin = EmptySearchAdmin(Question, site)\n+        msg = 'EmptySearchAdmin must have search_fields for the autocomplete_view.'\n+        with self.assertRaisesMessage(Http404, msg):\n+            model_admin.autocomplete_view(self.factory.get(self.url))\n+\n+    def test_get_paginator(self):\n+        \"\"\"Search results are paginated.\"\"\"\n+        Question.objects.bulk_create(Question(question=str(i)) for i in range(PAGINATOR_SIZE + 10))\n+        model_admin = QuestionAdmin(Question, site)\n+        model_admin.ordering = ['pk']\n+        # The first page of results.\n+        request = self.factory.get(self.url, {'term': ''})\n+        request.user = self.superuser\n+        response = AutocompleteJsonView.as_view(model_admin=model_admin)(request)\n+        self.assertEqual(response.status_code, 200)\n+        data = json.loads(response.content.decode('utf-8'))\n+        self.assertEqual(data, {\n+            'results': [{'id': str(q.pk), 'text': q.question} for q in Question.objects.all()[:PAGINATOR_SIZE]],\n+            'pagination': {'more': True},\n+        })\n+        # The second page of results.\n+        request = self.factory.get(self.url, {'term': '', 'page': '2'})\n+        request.user = self.superuser\n+        response = AutocompleteJsonView.as_view(model_admin=model_admin)(request)\n+        self.assertEqual(response.status_code, 200)\n+        data = json.loads(response.content.decode('utf-8'))\n+        self.assertEqual(data, {\n+            'results': [{'id': str(q.pk), 'text': q.question} for q in Question.objects.all()[PAGINATOR_SIZE:]],\n+            'pagination': {'more': False},\n+        })\n+\n+\n+@override_settings(ROOT_URLCONF='admin_views.urls')\n+class SeleniumTests(AdminSeleniumTestCase):\n+    available_apps = ['admin_views'] + AdminSeleniumTestCase.available_apps\n+\n+    def setUp(self):\n+        self.superuser = User.objects.create_superuser(\n+            username='super', password='secret', email='super@example.com',\n+        )\n+        self.admin_login(username='super', password='secret', login_url=reverse('admin:index'))\n+\n+    def test_select(self):\n+        from selenium.webdriver.common.keys import Keys\n+        from selenium.webdriver.support.ui import Select\n+        self.selenium.get(self.live_server_url + reverse('admin:admin_views_answer_add'))\n+        elem = self.selenium.find_element_by_css_selector('.select2-selection')\n+        elem.click()  # Open the autocomplete dropdown.\n+        results = self.selenium.find_element_by_css_selector('.select2-results')\n+        self.assertTrue(results.is_displayed())\n+        option = self.selenium.find_element_by_css_selector('.select2-results__option')\n+        self.assertEqual(option.text, 'No results found')\n+        elem.click()  # Close the autocomplete dropdown.\n+        q1 = Question.objects.create(question='Who am I?')\n+        Question.objects.bulk_create(Question(question=str(i)) for i in range(PAGINATOR_SIZE + 10))\n+        elem.click()  # Reopen the dropdown now that some objects exist.\n+        result_container = self.selenium.find_element_by_css_selector('.select2-results')\n+        self.assertTrue(result_container.is_displayed())\n+        results = result_container.find_elements_by_css_selector('.select2-results__option')\n+        # PAGINATOR_SIZE results and \"Loading more results\".\n+        self.assertEqual(len(results), PAGINATOR_SIZE + 1)\n+        search = self.selenium.find_element_by_css_selector('.select2-search__field')\n+        # Load next page of results by scrolling to the bottom of the list.\n+        for _ in range(len(results)):\n+            search.send_keys(Keys.ARROW_DOWN)\n+        results = result_container.find_elements_by_css_selector('.select2-results__option')\n+        # All objects and \"Loading more results\".\n+        self.assertEqual(len(results), PAGINATOR_SIZE + 11)\n+        # Limit the results with the search field.\n+        search.send_keys('Who')\n+        results = result_container.find_elements_by_css_selector('.select2-results__option')\n+        self.assertEqual(len(results), 1)\n+        # Select the result.\n+        search.send_keys(Keys.RETURN)\n+        select = Select(self.selenium.find_element_by_id('id_question'))\n+        self.assertEqual(select.first_selected_option.get_attribute('value'), str(q1.pk))\n+\n+    def test_select_multiple(self):\n+        from selenium.webdriver.common.keys import Keys\n+        from selenium.webdriver.support.ui import Select\n+        self.selenium.get(self.live_server_url + reverse('admin:admin_views_question_add'))\n+        elem = self.selenium.find_element_by_css_selector('.select2-selection')\n+        elem.click()  # Open the autocomplete dropdown.\n+        results = self.selenium.find_element_by_css_selector('.select2-results')\n+        self.assertTrue(results.is_displayed())\n+        option = self.selenium.find_element_by_css_selector('.select2-results__option')\n+        self.assertEqual(option.text, 'No results found')\n+        elem.click()  # Close the autocomplete dropdown.\n+        Question.objects.create(question='Who am I?')\n+        Question.objects.bulk_create(Question(question=str(i)) for i in range(PAGINATOR_SIZE + 10))\n+        elem.click()  # Reopen the dropdown now that some objects exist.\n+        result_container = self.selenium.find_element_by_css_selector('.select2-results')\n+        self.assertTrue(result_container.is_displayed())\n+        results = result_container.find_elements_by_css_selector('.select2-results__option')\n+        self.assertEqual(len(results), PAGINATOR_SIZE + 1)\n+        search = self.selenium.find_element_by_css_selector('.select2-search__field')\n+        # Load next page of results by scrolling to the bottom of the list.\n+        for _ in range(len(results)):\n+            search.send_keys(Keys.ARROW_DOWN)\n+        results = result_container.find_elements_by_css_selector('.select2-results__option')\n+        self.assertEqual(len(results), 31)\n+        # Limit the results with the search field.\n+        search.send_keys('Who')\n+        results = result_container.find_elements_by_css_selector('.select2-results__option')\n+        self.assertEqual(len(results), 1)\n+        # Select the result.\n+        search.send_keys(Keys.RETURN)\n+        # Reopen the dropdown and add the first result to the selection.\n+        elem.click()\n+        search.send_keys(Keys.ARROW_DOWN)\n+        search.send_keys(Keys.RETURN)\n+        select = Select(self.selenium.find_element_by_id('id_related_questions'))\n+        self.assertEqual(len(select.all_selected_options), 2)\ndiff --git a/tests/admin_views/tests.py b/tests/admin_views/tests.py\nindex d259e58e58d0..10e1303659d5 100644\n--- a/tests/admin_views/tests.py\n+++ b/tests/admin_views/tests.py\n@@ -3996,6 +3996,7 @@ def test_prepopulated_fields(self):\n         \"\"\"\n         self.admin_login(username='super', password='secret', login_url=reverse('admin:index'))\n         self.selenium.get(self.live_server_url + reverse('admin:admin_views_mainprepopulated_add'))\n+        self.wait_for('.select2')\n \n         # Main form ----------------------------------------------------------\n         self.selenium.find_element_by_id('id_pubdate').send_keys('2012-02-18')\n@@ -4019,9 +4020,18 @@ def test_prepopulated_fields(self):\n         slug2 = self.selenium.find_element_by_id('id_relatedprepopulated_set-0-slug2').get_attribute('value')\n         self.assertEqual(slug1, 'here-stacked-inline-2011-12-17')\n         self.assertEqual(slug2, 'option-one-here-stacked-inline')\n+        initial_select2_inputs = self.selenium.find_elements_by_class_name('select2-selection')\n+        # Inline formsets have empty/invisible forms.\n+        # 4 visible select2 inputs and 6 hidden inputs.\n+        num_initial_select2_inputs = len(initial_select2_inputs)\n+        self.assertEqual(num_initial_select2_inputs, 10)\n \n         # Add an inline\n         self.selenium.find_elements_by_link_text('Add another Related prepopulated')[0].click()\n+        self.assertEqual(\n+            len(self.selenium.find_elements_by_class_name('select2-selection')),\n+            num_initial_select2_inputs + 2\n+        )\n         self.selenium.find_element_by_id('id_relatedprepopulated_set-1-pubdate').send_keys('1999-01-25')\n         self.get_select_option('#id_relatedprepopulated_set-1-status', 'option two').click()\n         self.selenium.find_element_by_id('id_relatedprepopulated_set-1-name').send_keys(\n@@ -4049,6 +4059,10 @@ def test_prepopulated_fields(self):\n \n         # Add an inline\n         self.selenium.find_elements_by_link_text('Add another Related prepopulated')[1].click()\n+        self.assertEqual(\n+            len(self.selenium.find_elements_by_class_name('select2-selection')),\n+            num_initial_select2_inputs + 4\n+        )\n         self.selenium.find_element_by_id('id_relatedprepopulated_set-2-1-pubdate').send_keys('1981-08-22')\n         self.get_select_option('#id_relatedprepopulated_set-2-1-status', 'option one').click()\n         self.selenium.find_element_by_id('id_relatedprepopulated_set-2-1-name').send_keys(\n@@ -4058,7 +4072,14 @@ def test_prepopulated_fields(self):\n         slug2 = self.selenium.find_element_by_id('id_relatedprepopulated_set-2-1-slug2').get_attribute('value')\n         self.assertEqual(slug1, 'tabular-inline-ignored-characters-1981-08-22')\n         self.assertEqual(slug2, 'option-one-tabular-inline-ignored-characters')\n-\n+        # Add an inline without an initial inline.\n+        # The button is outside of the browser frame.\n+        self.selenium.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n+        self.selenium.find_elements_by_link_text('Add another Related prepopulated')[2].click()\n+        self.assertEqual(\n+            len(self.selenium.find_elements_by_class_name('select2-selection')),\n+            num_initial_select2_inputs + 6\n+        )\n         # Save and check that everything is properly stored in the database\n         self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n         self.wait_page_loaded()\n@@ -4232,6 +4253,10 @@ def test_list_editable_popups(self):\n         self.selenium.switch_to.window(self.selenium.window_handles[0])\n         select = Select(self.selenium.find_element_by_id('id_form-0-section'))\n         self.assertEqual(select.first_selected_option.text, '<i>edited section</i>')\n+        # Rendered select2 input.\n+        select2_display = self.selenium.find_element_by_class_name('select2-selection__rendered')\n+        # Clear button (\\n) is included in text.\n+        self.assertEqual(select2_display.text, '\\n<i>edited section</i>')\n \n         # Add popup\n         self.selenium.find_element_by_id('add_id_form-0-section').click()\n@@ -4243,6 +4268,9 @@ def test_list_editable_popups(self):\n         self.selenium.switch_to.window(self.selenium.window_handles[0])\n         select = Select(self.selenium.find_element_by_id('id_form-0-section'))\n         self.assertEqual(select.first_selected_option.text, 'new section')\n+        select2_display = self.selenium.find_element_by_class_name('select2-selection__rendered')\n+        # Clear button (\\n) is included in text.\n+        self.assertEqual(select2_display.text, '\\nnew section')\n \n     def test_inline_uuid_pk_edit_with_popup(self):\n         from selenium.webdriver.support.ui import Select\ndiff --git a/tests/admin_widgets/models.py b/tests/admin_widgets/models.py\nindex 422f8b0286be..bd00b114d3ef 100644\n--- a/tests/admin_widgets/models.py\n+++ b/tests/admin_widgets/models.py\n@@ -27,6 +27,7 @@ def __str__(self):\n \n class Album(models.Model):\n     band = models.ForeignKey(Band, models.CASCADE)\n+    featuring = models.ManyToManyField(Band, related_name='featured')\n     name = models.CharField(max_length=100)\n     cover_art = models.FileField(upload_to='albums')\n     backside_art = MyFileField(upload_to='albums_back', null=True)\ndiff --git a/tests/admin_widgets/test_autocomplete_widget.py b/tests/admin_widgets/test_autocomplete_widget.py\nnew file mode 100644\nindex 000000000000..fd79ef93690d\n--- /dev/null\n+++ b/tests/admin_widgets/test_autocomplete_widget.py\n@@ -0,0 +1,133 @@\n+from django import forms\n+from django.contrib.admin.widgets import AutocompleteSelect\n+from django.forms import ModelChoiceField\n+from django.test import TestCase, override_settings\n+from django.utils import translation\n+\n+from .models import Album, Band\n+\n+\n+class AlbumForm(forms.ModelForm):\n+    class Meta:\n+        model = Album\n+        fields = ['band', 'featuring']\n+        widgets = {\n+            'band': AutocompleteSelect(\n+                Album._meta.get_field('band').remote_field,\n+                attrs={'class': 'my-class'},\n+            ),\n+            'featuring': AutocompleteSelect(\n+                Album._meta.get_field('featuring').remote_field,\n+            )\n+        }\n+\n+\n+class NotRequiredBandForm(forms.Form):\n+    band = ModelChoiceField(\n+        queryset=Album.objects.all(),\n+        widget=AutocompleteSelect(Album._meta.get_field('band').remote_field),\n+        required=False,\n+    )\n+\n+\n+class RequiredBandForm(forms.Form):\n+    band = ModelChoiceField(\n+        queryset=Album.objects.all(),\n+        widget=AutocompleteSelect(Album._meta.get_field('band').remote_field),\n+        required=True,\n+    )\n+\n+\n+@override_settings(ROOT_URLCONF='admin_widgets.urls')\n+class AutocompleteMixinTests(TestCase):\n+    empty_option = '<option value=\"\"></option>'\n+    maxDiff = 1000\n+\n+    def test_build_attrs(self):\n+        form = AlbumForm()\n+        attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n+        self.assertEqual(attrs, {\n+            'class': 'my-classadmin-autocomplete',\n+            'data-ajax--cache': 'true',\n+            'data-ajax--type': 'GET',\n+            'data-ajax--url': '/admin_widgets/band/autocomplete/',\n+            'data-theme': 'admin-autocomplete',\n+            'data-allow-clear': 'false',\n+            'data-placeholder': ''\n+        })\n+\n+    def test_build_attrs_not_required_field(self):\n+        form = NotRequiredBandForm()\n+        attrs = form['band'].field.widget.build_attrs({})\n+        self.assertJSONEqual(attrs['data-allow-clear'], True)\n+\n+    def test_build_attrs_required_field(self):\n+        form = RequiredBandForm()\n+        attrs = form['band'].field.widget.build_attrs({})\n+        self.assertJSONEqual(attrs['data-allow-clear'], False)\n+\n+    def test_get_url(self):\n+        rel = Album._meta.get_field('band').remote_field\n+        w = AutocompleteSelect(rel)\n+        url = w.get_url()\n+        self.assertEqual(url, '/admin_widgets/band/autocomplete/')\n+\n+    def test_render_options(self):\n+        beatles = Band.objects.create(name='The Beatles', style='rock')\n+        who = Band.objects.create(name='The Who', style='rock')\n+        # With 'band', a ForeignKey.\n+        form = AlbumForm(initial={'band': beatles.pk})\n+        output = form.as_table()\n+        selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n+        option = '<option value=\"%s\">The Who</option>' % who.pk\n+        self.assertIn(selected_option, output)\n+        self.assertNotIn(option, output)\n+        # With 'featuring', a ManyToManyField.\n+        form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n+        output = form.as_table()\n+        selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n+        option = '<option value=\"%s\" selected>The Who</option>' % who.pk\n+        self.assertIn(selected_option, output)\n+        self.assertIn(option, output)\n+\n+    def test_render_options_required_field(self):\n+        \"\"\"Empty option is present if the field isn't required.\"\"\"\n+        form = NotRequiredBandForm()\n+        output = form.as_table()\n+        self.assertIn(self.empty_option, output)\n+\n+    def test_render_options_not_required_field(self):\n+        \"\"\"Empty option isn't present if the field isn't required.\"\"\"\n+        form = RequiredBandForm()\n+        output = form.as_table()\n+        self.assertNotIn(self.empty_option, output)\n+\n+    def test_media(self):\n+        rel = Album._meta.get_field('band').remote_field\n+        base_files = (\n+            'admin/js/vendor/jquery/jquery.min.js',\n+            'admin/js/vendor/select2/select2.full.min.js',\n+            # Language file is inserted here.\n+            'admin/js/jquery.init.js',\n+            'admin/js/autocomplete.js',\n+        )\n+        languages = (\n+            ('de', 'de'),\n+            # Language with code 00 does not exist.\n+            ('00', None),\n+            # Language files are case sensitive.\n+            ('sr-cyrl', 'sr-Cyrl'),\n+            ('zh-cn', 'zh-CN'),\n+        )\n+        for lang, select_lang in languages:\n+            with self.subTest(lang=lang):\n+                if select_lang:\n+                    expected_files = (\n+                        base_files[:2] +\n+                        (('admin/js/vendor/select2/i18n/%s.js' % select_lang),) +\n+                        base_files[2:]\n+                    )\n+                else:\n+                    expected_files = base_files\n+                with translation.override(lang):\n+                    self.assertEqual(AutocompleteSelect(rel).media._js, expected_files)\ndiff --git a/tests/modeladmin/models.py b/tests/modeladmin/models.py\nindex 861a2dbb9df7..c0d3c772c93a 100644\n--- a/tests/modeladmin/models.py\n+++ b/tests/modeladmin/models.py\n@@ -14,6 +14,15 @@ def __str__(self):\n         return self.name\n \n \n+class Song(models.Model):\n+    name = models.CharField(max_length=100)\n+    band = models.ForeignKey(Band, models.CASCADE)\n+    featuring = models.ManyToManyField(Band, related_name='featured')\n+\n+    def __str__(self):\n+        return self.name\n+\n+\n class Concert(models.Model):\n     main_band = models.ForeignKey(Band, models.CASCADE, related_name='main_concerts')\n     opening_band = models.ForeignKey(Band, models.CASCADE, related_name='opening_concerts', blank=True)\ndiff --git a/tests/modeladmin/test_checks.py b/tests/modeladmin/test_checks.py\nindex acca6b18a2b2..eaca153bd8d7 100644\n--- a/tests/modeladmin/test_checks.py\n+++ b/tests/modeladmin/test_checks.py\n@@ -6,14 +6,16 @@\n from django.forms.models import BaseModelFormSet\n from django.test import SimpleTestCase\n \n-from .models import Band, ValidationTestInlineModel, ValidationTestModel\n+from .models import Band, Song, ValidationTestInlineModel, ValidationTestModel\n \n \n class CheckTestCase(SimpleTestCase):\n \n-    def assertIsInvalid(self, model_admin, model, msg, id=None, hint=None, invalid_obj=None):\n+    def assertIsInvalid(self, model_admin, model, msg, id=None, hint=None, invalid_obj=None, admin_site=None):\n+        if admin_site is None:\n+            admin_site = AdminSite()\n         invalid_obj = invalid_obj or model_admin\n-        admin_obj = model_admin(model, AdminSite())\n+        admin_obj = model_admin(model, admin_site)\n         self.assertEqual(admin_obj.check(), [Error(msg, hint=hint, obj=invalid_obj, id=id)])\n \n     def assertIsInvalidRegexp(self, model_admin, model, msg, id=None, hint=None, invalid_obj=None):\n@@ -30,8 +32,10 @@ def assertIsInvalidRegexp(self, model_admin, model, msg, id=None, hint=None, inv\n         self.assertEqual(error.id, id)\n         self.assertRegex(error.msg, msg)\n \n-    def assertIsValid(self, model_admin, model):\n-        admin_obj = model_admin(model, AdminSite())\n+    def assertIsValid(self, model_admin, model, admin_site=None):\n+        if admin_site is None:\n+            admin_site = AdminSite()\n+        admin_obj = model_admin(model, admin_site)\n         self.assertEqual(admin_obj.check(), [])\n \n \n@@ -1153,3 +1157,89 @@ class ProductAdmin(ModelAdmin):\n             \"'list_display_links'.\",\n             id='admin.E123',\n         )\n+\n+\n+class AutocompleteFieldsTests(CheckTestCase):\n+    def test_autocomplete_e036(self):\n+        class Admin(ModelAdmin):\n+            autocomplete_fields = 'name'\n+\n+        self.assertIsInvalid(\n+            Admin, Band,\n+            msg=\"The value of 'autocomplete_fields' must be a list or tuple.\",\n+            id='admin.E036',\n+            invalid_obj=Admin,\n+        )\n+\n+    def test_autocomplete_e037(self):\n+        class Admin(ModelAdmin):\n+            autocomplete_fields = ('nonexistent',)\n+\n+        self.assertIsInvalid(\n+            Admin, ValidationTestModel,\n+            msg=(\n+                \"The value of 'autocomplete_fields[0]' refers to 'nonexistent', \"\n+                \"which is not an attribute of 'modeladmin.ValidationTestModel'.\"\n+            ),\n+            id='admin.E037',\n+            invalid_obj=Admin,\n+        )\n+\n+    def test_autocomplete_e38(self):\n+        class Admin(ModelAdmin):\n+            autocomplete_fields = ('name',)\n+\n+        self.assertIsInvalid(\n+            Admin, ValidationTestModel,\n+            msg=(\n+                \"The value of 'autocomplete_fields[0]' must be a foreign \"\n+                \"key or a many-to-many field.\"\n+            ),\n+            id='admin.E038',\n+            invalid_obj=Admin,\n+        )\n+\n+    def test_autocomplete_e039(self):\n+        class Admin(ModelAdmin):\n+            autocomplete_fields = ('band',)\n+\n+        self.assertIsInvalid(\n+            Admin, Song,\n+            msg=(\n+                'An admin for model \"Band\" has to be registered '\n+                'to be referenced by Admin.autocomplete_fields.'\n+            ),\n+            id='admin.E039',\n+            invalid_obj=Admin,\n+        )\n+\n+    def test_autocomplete_e040(self):\n+        class NoSearchFieldsAdmin(ModelAdmin):\n+            pass\n+\n+        class AutocompleteAdmin(ModelAdmin):\n+            autocomplete_fields = ('featuring',)\n+\n+        site = AdminSite()\n+        site.register(Band, NoSearchFieldsAdmin)\n+        self.assertIsInvalid(\n+            AutocompleteAdmin, Song,\n+            msg=(\n+                'NoSearchFieldsAdmin must define \"search_fields\", because '\n+                'it\\'s referenced by AutocompleteAdmin.autocomplete_fields.'\n+            ),\n+            id='admin.E040',\n+            invalid_obj=AutocompleteAdmin,\n+            admin_site=site,\n+        )\n+\n+    def test_autocomplete_is_valid(self):\n+        class SearchFieldsAdmin(ModelAdmin):\n+            search_fields = 'name'\n+\n+        class AutocompleteAdmin(ModelAdmin):\n+            autocomplete_fields = ('featuring',)\n+\n+        site = AdminSite()\n+        site.register(Band, SearchFieldsAdmin)\n+        self.assertIsValid(AutocompleteAdmin, Song, admin_site=site)\ndiff --git a/tests/modeladmin/tests.py b/tests/modeladmin/tests.py\nindex 25b9dfed69f3..67bed3d69752 100644\n--- a/tests/modeladmin/tests.py\n+++ b/tests/modeladmin/tests.py\n@@ -7,14 +7,17 @@\n     get_content_type_for_model,\n )\n from django.contrib.admin.sites import AdminSite\n-from django.contrib.admin.widgets import AdminDateWidget, AdminRadioSelect\n+from django.contrib.admin.widgets import (\n+    AdminDateWidget, AdminRadioSelect, AutocompleteSelect,\n+    AutocompleteSelectMultiple,\n+)\n from django.contrib.auth.models import User\n from django.db import models\n from django.forms.widgets import Select\n from django.test import SimpleTestCase, TestCase\n from django.test.utils import isolate_apps\n \n-from .models import Band, Concert\n+from .models import Band, Concert, Song\n \n \n class MockRequest:\n@@ -638,6 +641,31 @@ def test_log_actions(self):\n                     self.assertEqual(fetched.change_message, str(message))\n                     self.assertEqual(fetched.object_repr, str(self.band))\n \n+    def test_get_autocomplete_fields(self):\n+        class NameAdmin(ModelAdmin):\n+            search_fields = ['name']\n+\n+        class SongAdmin(ModelAdmin):\n+            autocomplete_fields = ['featuring']\n+            fields = ['featuring', 'band']\n+\n+        class OtherSongAdmin(SongAdmin):\n+            def get_autocomplete_fields(self, request):\n+                return ['band']\n+\n+        self.site.register(Band, NameAdmin)\n+        try:\n+            # Uses autocomplete_fields if not overridden.\n+            model_admin = SongAdmin(Song, self.site)\n+            form = model_admin.get_form(request)()\n+            self.assertIsInstance(form.fields['featuring'].widget.widget, AutocompleteSelectMultiple)\n+            # Uses overridden get_autocomplete_fields\n+            model_admin = OtherSongAdmin(Song, self.site)\n+            form = model_admin.get_form(request)()\n+            self.assertIsInstance(form.fields['band'].widget.widget, AutocompleteSelect)\n+        finally:\n+            self.site.unregister(Band)\n+\n \n class ModelAdminPermissionTests(SimpleTestCase):\n \n"
  },
  {
    "index": 21,
    "filtered_comments": [
      "Andrew - it's occurred to me that this may not address the situation when someone starts a project then switches to a swapped user model - I am **NOT** talking about the data migration fore user data (people are own for that) - but raising the issue of what, if anything, needs to be done when a model._meta.swapped goes from False to True from one migration to the next.\n\nNot really being familiar with the core approach here, I don't even know if anything needs to be done, just realizing that this was probably developed before _meta.swapped was introduced and pointing that out. cc @freakboy3742 \n",
      "I just created a migration and the file shows:\n\n```\n    dependencies = [(u'testb', '0001_initial')]\n```\n\n-- That doesn't work on 3.2, you might also want to import unicode_literals, depending on whether you want text or bytes for everything.\n\nEDIT:// The fields also have unicode markers from time to time:\n\n```\n            fields = [(u'id', models.AutoField(verbose_name=u'ID', serialize=False, auto_created=True, primary_key=True),), ('char', models.CharField(max_length=256),), ('fk', models.ForeignKey(to=u'testb.TestB', to_field=u'id'),)],\n```\n",
      "I get migrations which are getting unapplied without me asking for it:\n\n```\nflorian@apollo13:~/.virtualenvs/522125f0c8c708a8/migrationtest$ ./manage.py migrate\nOperations to perform:\n  Synchronize unmigrated apps: sessions, admin, messages, testc, auth, staticfiles, contenttypes\n  Apply all migrations: testa, testb\nSynchronizing apps without migrations:\n  Creating tables...\n    Creating table django_admin_log\n    Creating table auth_permission\n    Creating table auth_group_permissions\n    Creating table auth_group\n    Creating table auth_user_groups\n    Creating table auth_user_user_permissions\n    Creating table auth_user\n    Creating table django_content_type\n    Creating table django_session\n    Creating table testc_test\n  Installing custom SQL...\n  Installing indexes...\nInstalled 0 object(s) from 0 fixture(s)\nRunning migrations:\n  Applying testb.0001_initial... OK\n  Applying testa.0001_initial... OK\n  Applying testa.0002_auto... OK\n  Unapplying testa.0002_auto... OK\n  Unapplying testa.0001_initial... OK\n\nYou just installed Django's auth system, which means you don't have any superusers defined.\nWould you like to create one now? (yes/no): no\nflorian@apollo13:~/.virtualenvs/522125f0c8c708a8/migrationtest$ ./manage.py migrate\nOperations to perform:\n  Synchronize unmigrated apps: sessions, admin, messages, testc, auth, staticfiles, contenttypes\n  Apply all migrations: testa, testb\nSynchronizing apps without migrations:\n  Creating tables...\n  Installing custom SQL...\n  Installing indexes...\nInstalled 0 object(s) from 0 fixture(s)\nRunning migrations:\n  Applying testa.0001_initial... OK\n  Applying testa.0002_auto... OK\n  Unapplying testa.0002_auto... OK\n  Unapplying testa.0001_initial... OK\n```\n\nPing me in IRC if you need details.\n",
      "@andrewgodwin more details (from irc):\n\n```\n<apollo13> andrewgodwin: are you around? testa has a fk to testb, testb changes are \"stable\" so to say\n<apollo13> andrewgodwin: also, since django itself asks \"Would you like to create one now? (yes/no): no\"  -- we might wanna change [y/n] to stay consistent? [sry]\n```\n\nand migrating manually works:\n\n```\nflorian@apollo13:~/.virtualenvs/522125f0c8c708a8/migrationtest$ ./manage.py migrate testa 0001_initial\nOperations to perform:\n  Target specific migration: 0001_initial, from testa\nRunning migrations:\n  Applying testa.0001_initial... OK\n```\n",
      "Trying to migrate apps not in INSTALLED_APPS shows a confusing error:\n\n```\nflorian@apollo13:~/.virtualenvs/522125f0c8c708a8/migrationtest$ ./manage.py migrate fsdgdshdsfdsgdfshgds 0001_initial\nCommandError: App 'fsdgdshdsfdsgdfshgds' does not have migrations (you cannot selectively sync unmigrated apps)\n```\n\nSomething like \"this app doesn't exist\" would be better.\n",
      "I see different failures from what you described in the ML, and even more failures if I run just the tests for migrations and schema; I'm pretty sure several tests here depend on operations performed in other tests. This probably applies mostly to database systems which do not support transactional DDL, like Oracle and MySQL.\n"
    ],
    "code_diff": "diff --git a/django/conf/global_settings.py b/django/conf/global_settings.py\nindex ab3cdab59eb7..6dd25e18f9bb 100644\n--- a/django/conf/global_settings.py\n+++ b/django/conf/global_settings.py\n@@ -609,3 +609,10 @@\n     'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n #    'django.contrib.staticfiles.finders.DefaultStorageFinder',\n )\n+\n+##############\n+# MIGRATIONS #\n+##############\n+\n+# Migration module overrides for apps, by app label.\n+MIGRATION_MODULES = {}\ndiff --git a/django/contrib/auth/management/__init__.py b/django/contrib/auth/management/__init__.py\nindex 8335f35facc1..74d090958724 100644\n--- a/django/contrib/auth/management/__init__.py\n+++ b/django/contrib/auth/management/__init__.py\n@@ -65,7 +65,7 @@ def create_permissions(app, created_models, verbosity, db=DEFAULT_DB_ALIAS, **kw\n     except UnavailableApp:\n         return\n \n-    if not router.allow_syncdb(db, auth_app.Permission):\n+    if not router.allow_migrate(db, auth_app.Permission):\n         return\n \n     from django.contrib.contenttypes.models import ContentType\n@@ -188,7 +188,7 @@ def get_default_username(check_db=True):\n             return ''\n     return default_username\n \n-signals.post_syncdb.connect(create_permissions,\n+signals.post_migrate.connect(create_permissions,\n     dispatch_uid=\"django.contrib.auth.management.create_permissions\")\n-signals.post_syncdb.connect(create_superuser,\n+signals.post_migrate.connect(create_superuser,\n     sender=auth_app, dispatch_uid=\"django.contrib.auth.management.create_superuser\")\ndiff --git a/django/contrib/contenttypes/management.py b/django/contrib/contenttypes/management.py\nindex 64d1c418efec..4278bbd1e75e 100644\n--- a/django/contrib/contenttypes/management.py\n+++ b/django/contrib/contenttypes/management.py\n@@ -16,7 +16,7 @@ def update_contenttypes(app, created_models, verbosity=2, db=DEFAULT_DB_ALIAS, *\n     except UnavailableApp:\n         return\n \n-    if not router.allow_syncdb(db, ContentType):\n+    if not router.allow_migrate(db, ContentType):\n         return\n \n     ContentType.objects.clear_cache()\n@@ -88,7 +88,7 @@ def update_all_contenttypes(verbosity=2, **kwargs):\n     for app in get_apps():\n         update_contenttypes(app, None, verbosity, **kwargs)\n \n-signals.post_syncdb.connect(update_contenttypes)\n+signals.post_migrate.connect(update_contenttypes)\n \n if __name__ == \"__main__\":\n     update_all_contenttypes()\ndiff --git a/django/contrib/gis/db/backends/spatialite/creation.py b/django/contrib/gis/db/backends/spatialite/creation.py\nindex d13a6ae9db3a..22457dd4de52 100644\n--- a/django/contrib/gis/db/backends/spatialite/creation.py\n+++ b/django/contrib/gis/db/backends/spatialite/creation.py\n@@ -47,7 +47,7 @@ def create_test_db(self, verbosity=1, autoclobber=False):\n \n         # We need to then do a flush to ensure that any data installed by\n         # custom SQL has been removed. The only test data should come from\n-        # test fixtures, or autogenerated from post_syncdb triggers.\n+        # test fixtures, or autogenerated from post_migrate triggers.\n         # This has the side effect of loading initial data (which was\n         # intentionally skipped in the syncdb).\n         call_command('flush',\ndiff --git a/django/contrib/gis/tests/layermap/tests.py b/django/contrib/gis/tests/layermap/tests.py\nindex 86b15b24d411..632cb98aebde 100644\n--- a/django/contrib/gis/tests/layermap/tests.py\n+++ b/django/contrib/gis/tests/layermap/tests.py\n@@ -311,7 +311,7 @@ def db_for_write(self, model, **hints):\n     def allow_relation(self, obj1, obj2, **hints):\n         return None\n \n-    def allow_syncdb(self, db, model):\n+    def allow_migrate(self, db, model):\n         return True\n \n \ndiff --git a/django/contrib/sites/management.py b/django/contrib/sites/management.py\nindex 7a29e82d4cdf..d9e3a2126c8f 100644\n--- a/django/contrib/sites/management.py\n+++ b/django/contrib/sites/management.py\n@@ -11,7 +11,7 @@\n \n def create_default_site(app, created_models, verbosity, db, **kwargs):\n     # Only create the default sites in databases where Django created the table\n-    if Site in created_models and router.allow_syncdb(db, Site) :\n+    if Site in created_models and router.allow_migrate(db, Site) :\n         # The default settings set SITE_ID = 1, and some tests in Django's test\n         # suite rely on this value. However, if database sequences are reused\n         # (e.g. in the test suite after flush/syncdb), it isn't guaranteed that\n@@ -33,4 +33,4 @@ def create_default_site(app, created_models, verbosity, db, **kwargs):\n \n     Site.objects.clear_cache()\n \n-signals.post_syncdb.connect(create_default_site, sender=site_app)\n+signals.post_migrate.connect(create_default_site, sender=site_app)\ndiff --git a/django/core/management/commands/createcachetable.py b/django/core/management/commands/createcachetable.py\nindex d7ce3e93fd47..27668f272d29 100644\n--- a/django/core/management/commands/createcachetable.py\n+++ b/django/core/management/commands/createcachetable.py\n@@ -24,7 +24,7 @@ class Command(LabelCommand):\n     def handle_label(self, tablename, **options):\n         db = options.get('database')\n         cache = BaseDatabaseCache(tablename, {})\n-        if not router.allow_syncdb(db, cache.cache_model_class):\n+        if not router.allow_migrate(db, cache.cache_model_class):\n             return\n         connection = connections[db]\n         fields = (\ndiff --git a/django/core/management/commands/dumpdata.py b/django/core/management/commands/dumpdata.py\nindex fd9418a728ea..c74eede84640 100644\n--- a/django/core/management/commands/dumpdata.py\n+++ b/django/core/management/commands/dumpdata.py\n@@ -118,7 +118,7 @@ def get_objects():\n             for model in sort_dependencies(app_list.items()):\n                 if model in excluded_models:\n                     continue\n-                if not model._meta.proxy and router.allow_syncdb(using, model):\n+                if not model._meta.proxy and router.allow_migrate(using, model):\n                     if use_base_manager:\n                         objects = model._base_manager\n                     else:\ndiff --git a/django/core/management/commands/flush.py b/django/core/management/commands/flush.py\nindex e5a7253e7333..ea0952cb5379 100644\n--- a/django/core/management/commands/flush.py\n+++ b/django/core/management/commands/flush.py\n@@ -7,7 +7,7 @@\n from django.core.management import call_command\n from django.core.management.base import NoArgsCommand, CommandError\n from django.core.management.color import no_style\n-from django.core.management.sql import sql_flush, emit_post_sync_signal\n+from django.core.management.sql import sql_flush, emit_post_migrate_signal\n from django.utils.six.moves import input\n from django.utils import six\n \n@@ -23,8 +23,8 @@ class Command(NoArgsCommand):\n             help='Tells Django not to load any initial data after database synchronization.'),\n     )\n     help = ('Returns the database to the state it was in immediately after '\n-           'syncdb was executed. This means that all data will be removed '\n-           'from the database, any post-synchronization handlers will be '\n+           'migrate was first executed. This means that all data will be removed '\n+           'from the database, any post-migration handlers will be '\n            're-executed, and the initial_data fixture will be re-installed.')\n \n     def handle_noargs(self, **options):\n@@ -35,7 +35,7 @@ def handle_noargs(self, **options):\n         # The following are stealth options used by Django's internals.\n         reset_sequences = options.get('reset_sequences', True)\n         allow_cascade = options.get('allow_cascade', False)\n-        inhibit_post_syncdb = options.get('inhibit_post_syncdb', False)\n+        inhibit_post_migrate = options.get('inhibit_post_migrate', False)\n \n         self.style = no_style()\n \n@@ -54,7 +54,7 @@ def handle_noargs(self, **options):\n         if interactive:\n             confirm = input(\"\"\"You have requested a flush of the database.\n This will IRREVERSIBLY DESTROY all data currently in the %r database,\n-and return each table to the state it was in after syncdb.\n+and return each table to a fresh state.\n Are you sure you want to do this?\n \n     Type 'yes' to continue, or 'no' to cancel: \"\"\" % connection.settings_dict['NAME'])\n@@ -77,8 +77,8 @@ def handle_noargs(self, **options):\n                     \"The full error: %s\") % (connection.settings_dict['NAME'], e)\n                 six.reraise(CommandError, CommandError(new_msg), sys.exc_info()[2])\n \n-            if not inhibit_post_syncdb:\n-                self.emit_post_syncdb(verbosity, interactive, db)\n+            if not inhibit_post_migrate:\n+                self.emit_post_migrate(verbosity, interactive, db)\n \n             # Reinstall the initial_data fixture.\n             if options.get('load_initial_data'):\n@@ -89,13 +89,13 @@ def handle_noargs(self, **options):\n             self.stdout.write(\"Flush cancelled.\\n\")\n \n     @staticmethod\n-    def emit_post_syncdb(verbosity, interactive, database):\n-        # Emit the post sync signal. This allows individual applications to\n-        # respond as if the database had been sync'd from scratch.\n+    def emit_post_migrate(verbosity, interactive, database):\n+        # Emit the post migrate signal. This allows individual applications to\n+        # respond as if the database had been migrated from scratch.\n         all_models = []\n         for app in models.get_apps():\n             all_models.extend([\n                 m for m in models.get_models(app, include_auto_created=True)\n-                if router.allow_syncdb(database, m)\n+                if router.allow_migrate(database, m)\n             ])\n-        emit_post_sync_signal(set(all_models), verbosity, interactive, database)\n+        emit_post_migrate_signal(set(all_models), verbosity, interactive, database)\ndiff --git a/django/core/management/commands/loaddata.py b/django/core/management/commands/loaddata.py\nindex 1997f2956b53..a6e22d91737e 100644\n--- a/django/core/management/commands/loaddata.py\n+++ b/django/core/management/commands/loaddata.py\n@@ -134,7 +134,7 @@ def load_label(self, fixture_label):\n \n                 for obj in objects:\n                     objects_in_fixture += 1\n-                    if router.allow_syncdb(self.using, obj.object.__class__):\n+                    if router.allow_migrate(self.using, obj.object.__class__):\n                         loaded_objects_in_fixture += 1\n                         self.models.add(obj.object.__class__)\n                         try:\ndiff --git a/django/core/management/commands/makemigrations.py b/django/core/management/commands/makemigrations.py\nnew file mode 100644\nindex 000000000000..d802e2924a1e\n--- /dev/null\n+++ b/django/core/management/commands/makemigrations.py\n@@ -0,0 +1,84 @@\n+import sys\n+import os\n+from optparse import make_option\n+\n+from django.core.management.base import BaseCommand\n+from django.core.exceptions import ImproperlyConfigured\n+from django.db import connections, DEFAULT_DB_ALIAS\n+from django.db.migrations.loader import MigrationLoader\n+from django.db.migrations.autodetector import MigrationAutodetector, InteractiveMigrationQuestioner\n+from django.db.migrations.state import ProjectState\n+from django.db.migrations.writer import MigrationWriter\n+from django.db.models.loading import cache\n+\n+\n+class Command(BaseCommand):\n+    option_list = BaseCommand.option_list + (\n+        make_option('--empty', action='store_true', dest='empty', default=False,\n+            help='Make a blank migration.'),\n+    )\n+\n+    help = \"Creates new migration(s) for apps.\"\n+    usage_str = \"Usage: ./manage.py makemigrations [--empty] [app [app ...]]\"\n+\n+    def handle(self, *app_labels, **options):\n+\n+        self.verbosity = int(options.get('verbosity'))\n+        self.interactive = options.get('interactive')\n+\n+        # Make sure the app they asked for exists\n+        app_labels = set(app_labels)\n+        bad_app_labels = set()\n+        for app_label in app_labels:\n+            try:\n+                cache.get_app(app_label)\n+            except ImproperlyConfigured:\n+                bad_app_labels.add(app_label)\n+        if bad_app_labels:\n+            for app_label in bad_app_labels:\n+                self.stderr.write(\"App '%s' could not be found. Is it in INSTALLED_APPS?\" % app_label)\n+            sys.exit(2)\n+\n+        # Load the current graph state. Takes a connection, but it's not used\n+        # (makemigrations doesn't look at the database state).\n+        loader = MigrationLoader(connections[DEFAULT_DB_ALIAS])\n+\n+        # Detect changes\n+        autodetector = MigrationAutodetector(\n+            loader.graph.project_state(),\n+            ProjectState.from_app_cache(cache),\n+            InteractiveMigrationQuestioner(specified_apps=app_labels),\n+        )\n+        changes = autodetector.changes(graph=loader.graph, trim_to_apps=app_labels or None)\n+\n+        # No changes? Tell them.\n+        if not changes:\n+            if len(app_labels) == 1:\n+                self.stdout.write(\"No changes detected in app '%s'\" % app_labels.pop())\n+            elif len(app_labels) > 1:\n+                self.stdout.write(\"No changes detected in apps '%s'\" % (\"', '\".join(app_labels)))\n+            else:\n+                self.stdout.write(\"No changes detected\")\n+            return\n+\n+        directory_created = {}\n+        for app_label, migrations in changes.items():\n+            self.stdout.write(self.style.MIGRATE_HEADING(\"Migrations for '%s':\" % app_label) + \"\\n\")\n+            for migration in migrations:\n+                # Describe the migration\n+                writer = MigrationWriter(migration)\n+                self.stdout.write(\"  %s:\\n\" % (self.style.MIGRATE_LABEL(writer.filename),))\n+                for operation in migration.operations:\n+                    self.stdout.write(\"    - %s\\n\" % operation.describe())\n+                # Write it\n+                migrations_directory = os.path.dirname(writer.path)\n+                if not directory_created.get(app_label, False):\n+                    if not os.path.isdir(migrations_directory):\n+                        os.mkdir(migrations_directory)\n+                    init_path = os.path.join(migrations_directory, \"__init__.py\")\n+                    if not os.path.isfile(init_path):\n+                        open(init_path, \"w\").close()\n+                    # We just do this once per app\n+                    directory_created[app_label] = True\n+                with open(writer.path, \"w\") as fh:\n+                    fh.write(writer.as_string())\ndiff --git a/django/core/management/commands/migrate.py b/django/core/management/commands/migrate.py\nnew file mode 100644\nindex 000000000000..dbec389beda1\n--- /dev/null\n+++ b/django/core/management/commands/migrate.py\n@@ -0,0 +1,245 @@\n+from optparse import make_option\n+from collections import OrderedDict\n+from importlib import import_module\n+import itertools\n+import traceback\n+\n+from django.conf import settings\n+from django.core.management import call_command\n+from django.core.management.base import BaseCommand, CommandError\n+from django.core.management.color import no_style\n+from django.core.management.sql import custom_sql_for_model, emit_post_migrate_signal, emit_pre_migrate_signal\n+from django.db import connections, router, transaction, models, DEFAULT_DB_ALIAS\n+from django.db.migrations.executor import MigrationExecutor\n+from django.db.migrations.loader import AmbiguityError\n+from django.utils.module_loading import module_has_submodule\n+\n+\n+class Command(BaseCommand):\n+    option_list = BaseCommand.option_list + (\n+        make_option('--noinput', action='store_false', dest='interactive', default=True,\n+            help='Tells Django to NOT prompt the user for input of any kind.'),\n+        make_option('--no-initial-data', action='store_false', dest='load_initial_data', default=True,\n+            help='Tells Django not to load any initial data after database synchronization.'),\n+        make_option('--database', action='store', dest='database',\n+            default=DEFAULT_DB_ALIAS, help='Nominates a database to synchronize. '\n+                'Defaults to the \"default\" database.'),\n+        make_option('--fake', action='store_true', dest='fake', default=False,\n+            help='Mark migrations as run without actually running them'),\n+    )\n+\n+    help = \"Updates database schema. Manages both apps with migrations and those without.\"\n+\n+    def handle(self, *args, **options):\n+\n+        self.verbosity = int(options.get('verbosity'))\n+        self.interactive = options.get('interactive')\n+        self.show_traceback = options.get('traceback')\n+        self.load_initial_data = options.get('load_initial_data')\n+        self.test_database = options.get('test_database', False)\n+\n+        # Import the 'management' module within each installed app, to register\n+        # dispatcher events.\n+        for app_name in settings.INSTALLED_APPS:\n+            if module_has_submodule(import_module(app_name), \"management\"):\n+                import_module('.management', app_name)\n+\n+        # Get the database we're operating from\n+        db = options.get('database')\n+        connection = connections[db]\n+\n+        # Work out which apps have migrations and which do not\n+        executor = MigrationExecutor(connection, self.migration_progress_callback)\n+\n+        # If they supplied command line arguments, work out what they mean.\n+        run_syncdb = False\n+        target_app_labels_only = True\n+        if len(args) > 2:\n+            raise CommandError(\"Too many command-line arguments (expecting 'appname' or 'appname migrationname')\")\n+        elif len(args) == 2:\n+            app_label, migration_name = args\n+            if app_label not in executor.loader.migrated_apps:\n+                raise CommandError(\"App '%s' does not have migrations (you cannot selectively sync unmigrated apps)\" % app_label)\n+            if migration_name == \"zero\":\n+                targets = [(app_label, None)]\n+            else:\n+                try:\n+                    migration = executor.loader.get_migration_by_prefix(app_label, migration_name)\n+                except AmbiguityError:\n+                    raise CommandError(\"More than one migration matches '%s' in app '%s'. Please be more specific.\" % (app_label, migration_name))\n+                except KeyError:\n+                    raise CommandError(\"Cannot find a migration matching '%s' from app '%s'. Is it in INSTALLED_APPS?\" % (app_label, migration_name))\n+                targets = [(app_label, migration.name)]\n+            target_app_labels_only = False\n+        elif len(args) == 1:\n+            app_label = args[0]\n+            if app_label not in executor.loader.migrated_apps:\n+                raise CommandError(\"App '%s' does not have migrations (you cannot selectively sync unmigrated apps)\" % app_label)\n+            targets = [key for key in executor.loader.graph.leaf_nodes() if key[0] == app_label]\n+        else:\n+            targets = executor.loader.graph.leaf_nodes()\n+            run_syncdb = True\n+\n+        plan = executor.migration_plan(targets)\n+\n+        # Print some useful info\n+        if self.verbosity >= 1:\n+            self.stdout.write(self.style.MIGRATE_HEADING(\"Operations to perform:\"))\n+            if run_syncdb:\n+                self.stdout.write(self.style.MIGRATE_LABEL(\"  Synchronize unmigrated apps: \") + (\", \".join(executor.loader.unmigrated_apps) or \"(none)\"))\n+            if target_app_labels_only:\n+                self.stdout.write(self.style.MIGRATE_LABEL(\"  Apply all migrations: \") + (\", \".join(set(a for a, n in targets)) or \"(none)\"))\n+            else:\n+                if targets[0][1] is None:\n+                    self.stdout.write(self.style.MIGRATE_LABEL(\"  Unapply all migrations: \") + \"%s\" % (targets[0][0], ))\n+                else:\n+                    self.stdout.write(self.style.MIGRATE_LABEL(\"  Target specific migration: \") + \"%s, from %s\" % (targets[0][1], targets[0][0]))\n+\n+        # Run the syncdb phase.\n+        # If you ever manage to get rid of this, I owe you many, many drinks.\n+        # Note that pre_migrate is called from inside here, as it needs\n+        # the list of models about to be installed.\n+        if run_syncdb:\n+            if self.verbosity >= 1:\n+                self.stdout.write(self.style.MIGRATE_HEADING(\"Synchronizing apps without migrations:\"))\n+            created_models = self.sync_apps(connection, executor.loader.unmigrated_apps)\n+        else:\n+            created_models = []\n+\n+        # Migrate!\n+        if self.verbosity >= 1:\n+            self.stdout.write(self.style.MIGRATE_HEADING(\"Running migrations:\"))\n+        if not plan:\n+            if self.verbosity >= 1:\n+                self.stdout.write(\"  No migrations needed.\")\n+        else:\n+            executor.migrate(targets, plan, fake=options.get(\"fake\", False))\n+\n+        # Send the post_migrate signal, so individual apps can do whatever they need\n+        # to do at this point.\n+        emit_post_migrate_signal(created_models, self.verbosity, self.interactive, connection.alias)\n+\n+    def migration_progress_callback(self, action, migration):\n+        if self.verbosity >= 1:\n+            if action == \"apply_start\":\n+                self.stdout.write(\"  Applying %s...\" % migration, ending=\"\")\n+                self.stdout.flush()\n+            elif action == \"apply_success\":\n+                self.stdout.write(self.style.MIGRATE_SUCCESS(\" OK\"))\n+            elif action == \"unapply_start\":\n+                self.stdout.write(\"  Unapplying %s...\" % migration, ending=\"\")\n+                self.stdout.flush()\n+            elif action == \"unapply_success\":\n+                self.stdout.write(self.style.MIGRATE_SUCCESS(\" OK\"))\n+\n+    def sync_apps(self, connection, apps):\n+        \"Runs the old syncdb-style operation on a list of apps.\"\n+        cursor = connection.cursor()\n+\n+        # Get a list of already installed *models* so that references work right.\n+        tables = connection.introspection.table_names()\n+        seen_models = connection.introspection.installed_models(tables)\n+        created_models = set()\n+        pending_references = {}\n+\n+        # Build the manifest of apps and models that are to be synchronized\n+        all_models = [\n+            (app.__name__.split('.')[-2],\n+                [\n+                    m for m in models.get_models(app, include_auto_created=True)\n+                    if router.allow_migrate(connection.alias, m)\n+                ])\n+            for app in models.get_apps() if app.__name__.split('.')[-2] in apps\n+        ]\n+\n+        def model_installed(model):\n+            opts = model._meta\n+            converter = connection.introspection.table_name_converter\n+            # Note that if a model is unmanaged we short-circuit and never try to install it\n+            return not ((converter(opts.db_table) in tables) or\n+                (opts.auto_created and converter(opts.auto_created._meta.db_table) in tables))\n+\n+        manifest = OrderedDict(\n+            (app_name, list(filter(model_installed, model_list)))\n+            for app_name, model_list in all_models\n+        )\n+\n+        create_models = set([x for x in itertools.chain(*manifest.values())])\n+        emit_pre_migrate_signal(create_models, self.verbosity, self.interactive, connection.alias)\n+\n+        # Create the tables for each model\n+        if self.verbosity >= 1:\n+            self.stdout.write(\"  Creating tables...\\n\")\n+        with transaction.atomic(using=connection.alias, savepoint=False):\n+            for app_name, model_list in manifest.items():\n+                for model in model_list:\n+                    # Create the model's database table, if it doesn't already exist.\n+                    if self.verbosity >= 3:\n+                        self.stdout.write(\"    Processing %s.%s model\\n\" % (app_name, model._meta.object_name))\n+                    sql, references = connection.creation.sql_create_model(model, no_style(), seen_models)\n+                    seen_models.add(model)\n+                    created_models.add(model)\n+                    for refto, refs in references.items():\n+                        pending_references.setdefault(refto, []).extend(refs)\n+                        if refto in seen_models:\n+                            sql.extend(connection.creation.sql_for_pending_references(refto, no_style(), pending_references))\n+                    sql.extend(connection.creation.sql_for_pending_references(model, no_style(), pending_references))\n+                    if self.verbosity >= 1 and sql:\n+                        self.stdout.write(\"    Creating table %s\\n\" % model._meta.db_table)\n+                    for statement in sql:\n+                        cursor.execute(statement)\n+                    tables.append(connection.introspection.table_name_converter(model._meta.db_table))\n+\n+        # We force a commit here, as that was the previous behaviour.\n+        # If you can prove we don't need this, remove it.\n+        transaction.set_dirty(using=connection.alias)\n+\n+        # The connection may have been closed by a syncdb handler.\n+        cursor = connection.cursor()\n+\n+        # Install custom SQL for the app (but only if this\n+        # is a model we've just created)\n+        if self.verbosity >= 1:\n+            self.stdout.write(\"  Installing custom SQL...\\n\")\n+        for app_name, model_list in manifest.items():\n+            for model in model_list:\n+                if model in created_models:\n+                    custom_sql = custom_sql_for_model(model, no_style(), connection)\n+                    if custom_sql:\n+                        if self.verbosity >= 2:\n+                            self.stdout.write(\"    Installing custom SQL for %s.%s model\\n\" % (app_name, model._meta.object_name))\n+                        try:\n+                            with transaction.commit_on_success_unless_managed(using=connection.alias):\n+                                for sql in custom_sql:\n+                                    cursor.execute(sql)\n+                        except Exception as e:\n+                            self.stderr.write(\"    Failed to install custom SQL for %s.%s model: %s\\n\" % (app_name, model._meta.object_name, e))\n+                            if self.show_traceback:\n+                                traceback.print_exc()\n+                    else:\n+                        if self.verbosity >= 3:\n+                            self.stdout.write(\"    No custom SQL for %s.%s model\\n\" % (app_name, model._meta.object_name))\n+\n+        if self.verbosity >= 1:\n+            self.stdout.write(\"  Installing indexes...\\n\")\n+\n+        # Install SQL indices for all newly created models\n+        for app_name, model_list in manifest.items():\n+            for model in model_list:\n+                if model in created_models:\n+                    index_sql = connection.creation.sql_indexes_for_model(model, no_style())\n+                    if index_sql:\n+                        if self.verbosity >= 2:\n+                            self.stdout.write(\"    Installing index for %s.%s model\\n\" % (app_name, model._meta.object_name))\n+                        try:\n+                            with transaction.commit_on_success_unless_managed(using=connection.alias):\n+                                for sql in index_sql:\n+                                    cursor.execute(sql)\n+                        except Exception as e:\n+                            self.stderr.write(\"    Failed to install index for %s.%s model: %s\\n\" % (app_name, model._meta.object_name, e))\n+\n+        # Load initial_data fixtures (unless that has been disabled)\n+        if self.load_initial_data:\n+            call_command('loaddata', 'initial_data', verbosity=self.verbosity, database=connection.alias, skip_validation=True)\n+\n+        return created_models\ndiff --git a/django/core/management/commands/syncdb.py b/django/core/management/commands/syncdb.py\nindex d51699e95a5e..17ea51f4d5aa 100644\n--- a/django/core/management/commands/syncdb.py\n+++ b/django/core/management/commands/syncdb.py\n@@ -1,15 +1,8 @@\n-from collections import OrderedDict\n-from importlib import import_module\n+import warnings\n from optparse import make_option\n-import itertools\n-import traceback\n-\n-from django.conf import settings\n+from django.db import DEFAULT_DB_ALIAS\n from django.core.management import call_command\n from django.core.management.base import NoArgsCommand\n-from django.core.management.color import no_style\n-from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal, emit_pre_sync_signal\n-from django.db import connections, router, transaction, models, DEFAULT_DB_ALIAS\n \n \n class Command(NoArgsCommand):\n@@ -22,141 +15,8 @@ class Command(NoArgsCommand):\n             default=DEFAULT_DB_ALIAS, help='Nominates a database to synchronize. '\n                 'Defaults to the \"default\" database.'),\n     )\n-    help = \"Create the database tables for all apps in INSTALLED_APPS whose tables haven't already been created.\"\n+    help = \"Deprecated - use 'migrate' instead.\"\n \n     def handle_noargs(self, **options):\n-\n-        verbosity = int(options.get('verbosity'))\n-        interactive = options.get('interactive')\n-        show_traceback = options.get('traceback')\n-        load_initial_data = options.get('load_initial_data')\n-\n-        self.style = no_style()\n-\n-        # Import the 'management' module within each installed app, to register\n-        # dispatcher events.\n-        for app_name in settings.INSTALLED_APPS:\n-            try:\n-                import_module('.management', app_name)\n-            except ImportError as exc:\n-                # This is slightly hackish. We want to ignore ImportErrors\n-                # if the \"management\" module itself is missing -- but we don't\n-                # want to ignore the exception if the management module exists\n-                # but raises an ImportError for some reason. The only way we\n-                # can do this is to check the text of the exception. Note that\n-                # we're a bit broad in how we check the text, because different\n-                # Python implementations may not use the same text.\n-                # CPython uses the text \"No module named management\"\n-                # PyPy uses \"No module named myproject.myapp.management\"\n-                msg = exc.args[0]\n-                if not msg.startswith('No module named') or 'management' not in msg:\n-                    raise\n-\n-        db = options.get('database')\n-        connection = connections[db]\n-        cursor = connection.cursor()\n-\n-        # Get a list of already installed *models* so that references work right.\n-        tables = connection.introspection.table_names()\n-        seen_models = connection.introspection.installed_models(tables)\n-        created_models = set()\n-        pending_references = {}\n-\n-        # Build the manifest of apps and models that are to be synchronized\n-        all_models = [\n-            (app.__name__.split('.')[-2],\n-                [m for m in models.get_models(app, include_auto_created=True)\n-                if router.allow_syncdb(db, m)])\n-            for app in models.get_apps()\n-        ]\n-\n-        def model_installed(model):\n-            opts = model._meta\n-            converter = connection.introspection.table_name_converter\n-            return not ((converter(opts.db_table) in tables) or\n-                (opts.auto_created and converter(opts.auto_created._meta.db_table) in tables))\n-\n-        manifest = OrderedDict(\n-            (app_name, list(filter(model_installed, model_list)))\n-            for app_name, model_list in all_models\n-        )\n-\n-        create_models = set([x for x in itertools.chain(*manifest.values())])\n-        emit_pre_sync_signal(create_models, verbosity, interactive, db)\n-\n-        # Create the tables for each model\n-        if verbosity >= 1:\n-            self.stdout.write(\"Creating tables ...\\n\")\n-        with transaction.commit_on_success_unless_managed(using=db):\n-            for app_name, model_list in manifest.items():\n-                for model in model_list:\n-                    # Create the model's database table, if it doesn't already exist.\n-                    if verbosity >= 3:\n-                        self.stdout.write(\"Processing %s.%s model\\n\" % (app_name, model._meta.object_name))\n-                    sql, references = connection.creation.sql_create_model(model, self.style, seen_models)\n-                    seen_models.add(model)\n-                    created_models.add(model)\n-                    for refto, refs in references.items():\n-                        pending_references.setdefault(refto, []).extend(refs)\n-                        if refto in seen_models:\n-                            sql.extend(connection.creation.sql_for_pending_references(refto, self.style, pending_references))\n-                    sql.extend(connection.creation.sql_for_pending_references(model, self.style, pending_references))\n-                    if verbosity >= 1 and sql:\n-                        self.stdout.write(\"Creating table %s\\n\" % model._meta.db_table)\n-                    for statement in sql:\n-                        cursor.execute(statement)\n-                    tables.append(connection.introspection.table_name_converter(model._meta.db_table))\n-\n-        # Send the post_syncdb signal, so individual apps can do whatever they need\n-        # to do at this point.\n-        emit_post_sync_signal(created_models, verbosity, interactive, db)\n-\n-        # The connection may have been closed by a syncdb handler.\n-        cursor = connection.cursor()\n-\n-        # Install custom SQL for the app (but only if this\n-        # is a model we've just created)\n-        if verbosity >= 1:\n-            self.stdout.write(\"Installing custom SQL ...\\n\")\n-        for app_name, model_list in manifest.items():\n-            for model in model_list:\n-                if model in created_models:\n-                    custom_sql = custom_sql_for_model(model, self.style, connection)\n-                    if custom_sql:\n-                        if verbosity >= 2:\n-                            self.stdout.write(\"Installing custom SQL for %s.%s model\\n\" % (app_name, model._meta.object_name))\n-                        try:\n-                            with transaction.commit_on_success_unless_managed(using=db):\n-                                for sql in custom_sql:\n-                                    cursor.execute(sql)\n-                        except Exception as e:\n-                            self.stderr.write(\"Failed to install custom SQL for %s.%s model: %s\\n\" % \\\n-                                                (app_name, model._meta.object_name, e))\n-                            if show_traceback:\n-                                traceback.print_exc()\n-                    else:\n-                        if verbosity >= 3:\n-                            self.stdout.write(\"No custom SQL for %s.%s model\\n\" % (app_name, model._meta.object_name))\n-\n-        if verbosity >= 1:\n-            self.stdout.write(\"Installing indexes ...\\n\")\n-        # Install SQL indices for all newly created models\n-        for app_name, model_list in manifest.items():\n-            for model in model_list:\n-                if model in created_models:\n-                    index_sql = connection.creation.sql_indexes_for_model(model, self.style)\n-                    if index_sql:\n-                        if verbosity >= 2:\n-                            self.stdout.write(\"Installing index for %s.%s model\\n\" % (app_name, model._meta.object_name))\n-                        try:\n-                            with transaction.commit_on_success_unless_managed(using=db):\n-                                for sql in index_sql:\n-                                    cursor.execute(sql)\n-                        except Exception as e:\n-                            self.stderr.write(\"Failed to install index for %s.%s model: %s\\n\" % \\\n-                                                (app_name, model._meta.object_name, e))\n-\n-        # Load initial_data fixtures (unless that has been disabled)\n-        if load_initial_data:\n-            call_command('loaddata', 'initial_data', verbosity=verbosity,\n-                         database=db, skip_validation=True)\n+        warnings.warn(\"The syncdb command will be removed in Django 1.9\", PendingDeprecationWarning)\n+        call_command(\"migrate\", **options)\ndiff --git a/django/core/management/sql.py b/django/core/management/sql.py\nindex c5806086f931..2e977c0c07a4 100644\n--- a/django/core/management/sql.py\n+++ b/django/core/management/sql.py\n@@ -206,25 +206,25 @@ def custom_sql_for_model(model, style, connection):\n     return output\n \n \n-def emit_pre_sync_signal(create_models, verbosity, interactive, db):\n-    # Emit the pre_sync signal for every application.\n+def emit_pre_migrate_signal(create_models, verbosity, interactive, db):\n+    # Emit the pre_migrate signal for every application.\n     for app in models.get_apps():\n         app_name = app.__name__.split('.')[-2]\n         if verbosity >= 2:\n-            print(\"Running pre-sync handlers for application %s\" % app_name)\n-        models.signals.pre_syncdb.send(sender=app, app=app,\n+            print(\"Running pre-migrate handlers for application %s\" % app_name)\n+        models.signals.pre_migrate.send(sender=app, app=app,\n                                        create_models=create_models,\n                                        verbosity=verbosity,\n                                        interactive=interactive,\n                                        db=db)\n \n \n-def emit_post_sync_signal(created_models, verbosity, interactive, db):\n-    # Emit the post_sync signal for every application.\n+def emit_post_migrate_signal(created_models, verbosity, interactive, db):\n+    # Emit the post_migrate signal for every application.\n     for app in models.get_apps():\n         app_name = app.__name__.split('.')[-2]\n         if verbosity >= 2:\n-            print(\"Running post-sync handlers for application %s\" % app_name)\n-        models.signals.post_syncdb.send(sender=app, app=app,\n+            print(\"Running post-migrate handlers for application %s\" % app_name)\n+        models.signals.post_migrate.send(sender=app, app=app,\n             created_models=created_models, verbosity=verbosity,\n             interactive=interactive, db=db)\ndiff --git a/django/db/backends/__init__.py b/django/db/backends/__init__.py\nindex 07d45c9175d6..6274d5bc55df 100644\n--- a/django/db/backends/__init__.py\n+++ b/django/db/backends/__init__.py\n@@ -521,6 +521,10 @@ def _start_transaction_under_autocommit(self):\n         \"\"\"\n         raise NotImplementedError\n \n+    def schema_editor(self):\n+        \"Returns a new instance of this backend's SchemaEditor\"\n+        raise NotImplementedError()\n+\n \n class BaseDatabaseFeatures(object):\n     allows_group_by_pk = False\n@@ -630,11 +634,32 @@ class BaseDatabaseFeatures(object):\n     # when autocommit is disabled? http://bugs.python.org/issue8145#msg109965\n     autocommits_when_autocommit_is_off = False\n \n+    # Can we roll back DDL in a transaction?\n+    can_rollback_ddl = False\n+\n+    # Can we issue more than one ALTER COLUMN clause in an ALTER TABLE?\n+    supports_combined_alters = False\n+\n+    # What's the maximum length for index names?\n+    max_index_name_length = 63\n+\n+    # Does it support foreign keys?\n+    supports_foreign_keys = True\n+\n+    # Does it support CHECK constraints?\n+    supports_check_constraints = True\n+\n     # Does the backend support 'pyformat' style (\"... %(name)s ...\", {'name': value})\n     # parameter passing? Note this can be provided by the backend even if not\n     # supported by the Python driver\n     supports_paramstyle_pyformat = True\n \n+    # Does the backend require literal defaults, rather than parameterised ones?\n+    requires_literal_defaults = False\n+\n+    # Does the backend require a connection reset after each material schema change?\n+    connection_persists_old_columns = False\n+\n     def __init__(self, connection):\n         self.connection = connection\n \n@@ -1227,7 +1252,7 @@ def django_table_names(self, only_existing=False):\n             for model in models.get_models(app):\n                 if not model._meta.managed:\n                     continue\n-                if not router.allow_syncdb(self.connection.alias, model):\n+                if not router.allow_migrate(self.connection.alias, model):\n                     continue\n                 tables.add(model._meta.db_table)\n                 tables.update([f.m2m_db_table() for f in model._meta.local_many_to_many])\n@@ -1247,7 +1272,7 @@ def installed_models(self, tables):\n         all_models = []\n         for app in models.get_apps():\n             for model in models.get_models(app):\n-                if router.allow_syncdb(self.connection.alias, model):\n+                if router.allow_migrate(self.connection.alias, model):\n                     all_models.append(model)\n         tables = list(map(self.table_name_converter, tables))\n         return set([\n@@ -1268,7 +1293,7 @@ def sequence_list(self):\n                     continue\n                 if model._meta.swapped:\n                     continue\n-                if not router.allow_syncdb(self.connection.alias, model):\n+                if not router.allow_migrate(self.connection.alias, model):\n                     continue\n                 for f in model._meta.local_fields:\n                     if isinstance(f, models.AutoField):\n@@ -1310,6 +1335,25 @@ def get_indexes(self, cursor, table_name):\n         \"\"\"\n         raise NotImplementedError\n \n+    def get_constraints(self, cursor, table_name):\n+        \"\"\"\n+        Retrieves any constraints or keys (unique, pk, fk, check, index)\n+        across one or more columns.\n+\n+        Returns a dict mapping constraint names to their attributes,\n+        where attributes is a dict with keys:\n+         * columns: List of columns this covers\n+         * primary_key: True if primary key, False otherwise\n+         * unique: True if this is a unique constraint, False otherwise\n+         * foreign_key: (table, column) of target, or None\n+         * check: True if check constraint, False otherwise\n+         * index: True if index, False otherwise.\n+\n+        Some backends may return special constraint names that don't exist\n+        if they don't name constraints of a certain type (e.g. SQLite)\n+        \"\"\"\n+        raise NotImplementedError\n+\n \n class BaseDatabaseClient(object):\n     \"\"\"\ndiff --git a/django/db/backends/creation.py b/django/db/backends/creation.py\nindex 4d646b05dde1..1d90f0342583 100644\n--- a/django/db/backends/creation.py\n+++ b/django/db/backends/creation.py\n@@ -23,11 +23,13 @@ class BaseDatabaseCreation(object):\n     destruction of test databases.\n     \"\"\"\n     data_types = {}\n+    data_type_check_constraints = {}\n \n     def __init__(self, connection):\n         self.connection = connection\n \n-    def _digest(self, *args):\n+    @classmethod\n+    def _digest(cls, *args):\n         \"\"\"\n         Generates a 32-bit digest of a set of arguments that can be used to\n         shorten identifying names.\n@@ -330,18 +332,19 @@ def create_test_db(self, verbosity=1, autoclobber=False):\n         settings.DATABASES[self.connection.alias][\"NAME\"] = test_database_name\n         self.connection.settings_dict[\"NAME\"] = test_database_name\n \n-        # Report syncdb messages at one level lower than that requested.\n+        # Report migrate messages at one level lower than that requested.\n         # This ensures we don't get flooded with messages during testing\n         # (unless you really ask to be flooded)\n-        call_command('syncdb',\n+        call_command('migrate',\n             verbosity=max(verbosity - 1, 0),\n             interactive=False,\n             database=self.connection.alias,\n-            load_initial_data=False)\n+            load_initial_data=False,\n+            test_database=True)\n \n         # We need to then do a flush to ensure that any data installed by\n         # custom SQL has been removed. The only test data should come from\n-        # test fixtures, or autogenerated from post_syncdb triggers.\n+        # test fixtures, or autogenerated from post_migrate triggers.\n         # This has the side effect of loading initial data (which was\n         # intentionally skipped in the syncdb).\n         call_command('flush',\ndiff --git a/django/db/backends/mysql/base.py b/django/db/backends/mysql/base.py\nindex 719b8ca29b08..15eacec679be 100644\n--- a/django/db/backends/mysql/base.py\n+++ b/django/db/backends/mysql/base.py\n@@ -44,6 +44,9 @@\n from django.db.backends.mysql.introspection import DatabaseIntrospection\n from django.db.backends.mysql.validation import DatabaseValidation\n from django.utils.encoding import force_str, force_text\n+from django.db.backends.mysql.schema import DatabaseSchemaEditor\n+from django.utils.encoding import force_str\n+from django.utils.functional import cached_property\n from django.utils.safestring import SafeBytes, SafeText\n from django.utils import six\n from django.utils import timezone\n@@ -171,6 +174,7 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n     requires_explicit_null_ordering_when_grouping = True\n     allows_primary_key_0 = False\n     uses_savepoints = True\n+    supports_check_constraints = False\n \n     def __init__(self, connection):\n         super(DatabaseFeatures, self).__init__(connection)\n@@ -514,6 +518,10 @@ def check_constraints(self, table_names=None):\n                         table_name, column_name, bad_row[1],\n                         referenced_table_name, referenced_column_name))\n \n+    def schema_editor(self):\n+        \"Returns a new instance of this backend's SchemaEditor\"\n+        return DatabaseSchemaEditor(self)\n+\n     def is_usable(self):\n         try:\n             self.connection.ping()\ndiff --git a/django/db/backends/mysql/introspection.py b/django/db/backends/mysql/introspection.py\nindex ec9f3e99f80f..d7a29057de4f 100644\n--- a/django/db/backends/mysql/introspection.py\n+++ b/django/db/backends/mysql/introspection.py\n@@ -1,6 +1,6 @@\n import re\n from .base import FIELD_TYPE\n-\n+from django.utils.datastructures import OrderedSet\n from django.db.backends import BaseDatabaseIntrospection, FieldInfo\n from django.utils.encoding import force_text\n \n@@ -115,5 +115,71 @@ def get_indexes(self, cursor, table_name):\n         for row in rows:\n             if row[2] in multicol_indexes:\n                 continue\n-            indexes[row[4]] = {'primary_key': (row[2] == 'PRIMARY'), 'unique': not bool(row[1])}\n+            if row[4] not in indexes:\n+                indexes[row[4]] = {'primary_key': False, 'unique': False}\n+            # It's possible to have the unique and PK constraints in separate indexes.\n+            if row[2] == 'PRIMARY':\n+                indexes[row[4]]['primary_key'] = True\n+            if not bool(row[1]):\n+                indexes[row[4]]['unique'] = True\n         return indexes\n+\n+    def get_constraints(self, cursor, table_name):\n+        \"\"\"\n+        Retrieves any constraints or keys (unique, pk, fk, check, index) across one or more columns.\n+        \"\"\"\n+        constraints = {}\n+        # Get the actual constraint names and columns\n+        name_query = \"\"\"\n+            SELECT kc.`constraint_name`, kc.`column_name`,\n+                kc.`referenced_table_name`, kc.`referenced_column_name`\n+            FROM information_schema.key_column_usage AS kc\n+            WHERE\n+                kc.table_schema = %s AND\n+                kc.table_name = %s\n+        \"\"\"\n+        cursor.execute(name_query, [self.connection.settings_dict['NAME'], table_name])\n+        for constraint, column, ref_table, ref_column in cursor.fetchall():\n+            if constraint not in constraints:\n+                constraints[constraint] = {\n+                    'columns': OrderedSet(),\n+                    'primary_key': False,\n+                    'unique': False,\n+                    'index': False,\n+                    'check': False,\n+                    'foreign_key': (ref_table, ref_column) if ref_column else None,\n+                }\n+            constraints[constraint]['columns'].add(column)\n+        # Now get the constraint types\n+        type_query = \"\"\"\n+            SELECT c.constraint_name, c.constraint_type\n+            FROM information_schema.table_constraints AS c\n+            WHERE\n+                c.table_schema = %s AND\n+                c.table_name = %s\n+        \"\"\"\n+        cursor.execute(type_query, [self.connection.settings_dict['NAME'], table_name])\n+        for constraint, kind in cursor.fetchall():\n+            if kind.lower() == \"primary key\":\n+                constraints[constraint]['primary_key'] = True\n+                constraints[constraint]['unique'] = True\n+            elif kind.lower() == \"unique\":\n+                constraints[constraint]['unique'] = True\n+        # Now add in the indexes\n+        cursor.execute(\"SHOW INDEX FROM %s\" % self.connection.ops.quote_name(table_name))\n+        for table, non_unique, index, colseq, column in [x[:5] for x in cursor.fetchall()]:\n+            if index not in constraints:\n+                constraints[index] = {\n+                    'columns': OrderedSet(),\n+                    'primary_key': False,\n+                    'unique': False,\n+                    'index': True,\n+                    'check': False,\n+                    'foreign_key': None,\n+                }\n+            constraints[index]['index'] = True\n+            constraints[index]['columns'].add(column)\n+        # Convert the sorted sets to lists\n+        for constraint in constraints.values():\n+            constraint['columns'] = list(constraint['columns'])\n+        return constraints\ndiff --git a/django/db/backends/mysql/schema.py b/django/db/backends/mysql/schema.py\nnew file mode 100644\nindex 000000000000..dc74b2db2ada\n--- /dev/null\n+++ b/django/db/backends/mysql/schema.py\n@@ -0,0 +1,26 @@\n+from django.db.backends.schema import BaseDatabaseSchemaEditor\n+\n+\n+class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n+\n+    sql_rename_table = \"RENAME TABLE %(old_table)s TO %(new_table)s\"\n+\n+    sql_alter_column_null = \"MODIFY %(column)s %(type)s NULL\"\n+    sql_alter_column_not_null = \"MODIFY %(column)s %(type)s NOT NULL\"\n+    sql_alter_column_type = \"MODIFY %(column)s %(type)s\"\n+    sql_rename_column = \"ALTER TABLE %(table)s CHANGE %(old_column)s %(new_column)s %(type)s\"\n+\n+    sql_delete_unique = \"ALTER TABLE %(table)s DROP INDEX %(name)s\"\n+\n+    sql_create_fk = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) REFERENCES %(to_table)s (%(to_column)s)\"\n+    sql_delete_fk = \"ALTER TABLE %(table)s DROP FOREIGN KEY %(name)s\"\n+\n+    sql_delete_index = \"DROP INDEX %(name)s ON %(table)s\"\n+\n+    sql_delete_pk = \"ALTER TABLE %(table)s DROP PRIMARY KEY\"\n+\n+    alter_string_set_null = 'MODIFY %(column)s %(type)s NULL;'\n+    alter_string_drop_null = 'MODIFY %(column)s %(type)s NOT NULL;'\n+\n+    sql_create_pk = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s PRIMARY KEY (%(columns)s)\"\n+    sql_delete_pk = \"ALTER TABLE %(table)s DROP PRIMARY KEY\"\ndiff --git a/django/db/backends/oracle/base.py b/django/db/backends/oracle/base.py\nindex 8b556b8449dc..fe6ac0e70c90 100644\n--- a/django/db/backends/oracle/base.py\n+++ b/django/db/backends/oracle/base.py\n@@ -55,6 +55,7 @@ def _setup_environment(environ):\n from django.db.backends.oracle.client import DatabaseClient\n from django.db.backends.oracle.creation import DatabaseCreation\n from django.db.backends.oracle.introspection import DatabaseIntrospection\n+from django.db.backends.oracle.schema import DatabaseSchemaEditor\n from django.utils.encoding import force_bytes, force_text\n \n \n@@ -90,6 +91,11 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n     has_bulk_insert = True\n     supports_tablespaces = True\n     supports_sequence_reset = False\n+    supports_combined_alters = False\n+    max_index_name_length = 30\n+    nulls_order_largest = True\n+    requires_literal_defaults = True\n+    connection_persists_old_columns = True\n     nulls_order_largest = True\n \n \n@@ -621,6 +627,10 @@ def _commit(self):\n                    and x.code == 2091 and 'ORA-02291' in x.message:\n                     six.reraise(utils.IntegrityError, utils.IntegrityError(*tuple(e.args)), sys.exc_info()[2])\n                 raise\n+    \n+    def schema_editor(self):\n+        \"Returns a new instance of this backend's SchemaEditor\"\n+        return DatabaseSchemaEditor(self)\n \n     # Oracle doesn't support savepoint commits.  Ignore them.\n     def _savepoint_commit(self, sid):\ndiff --git a/django/db/backends/oracle/creation.py b/django/db/backends/oracle/creation.py\nindex 55f6ee4d7e4a..b1a8782aa978 100644\n--- a/django/db/backends/oracle/creation.py\n+++ b/django/db/backends/oracle/creation.py\n@@ -22,7 +22,7 @@ class DatabaseCreation(BaseDatabaseCreation):\n     data_types = {\n         'AutoField': 'NUMBER(11)',\n         'BinaryField': 'BLOB',\n-        'BooleanField': 'NUMBER(1) CHECK (%(qn_column)s IN (0,1))',\n+        'BooleanField': 'NUMBER(1)',\n         'CharField': 'NVARCHAR2(%(max_length)s)',\n         'CommaSeparatedIntegerField': 'VARCHAR2(%(max_length)s)',\n         'DateField': 'DATE',\n@@ -35,10 +35,10 @@ class DatabaseCreation(BaseDatabaseCreation):\n         'BigIntegerField': 'NUMBER(19)',\n         'IPAddressField': 'VARCHAR2(15)',\n         'GenericIPAddressField': 'VARCHAR2(39)',\n-        'NullBooleanField': 'NUMBER(1) CHECK ((%(qn_column)s IN (0,1)) OR (%(qn_column)s IS NULL))',\n+        'NullBooleanField': 'NUMBER(1)',\n         'OneToOneField': 'NUMBER(11)',\n-        'PositiveIntegerField': 'NUMBER(11) CHECK (%(qn_column)s >= 0)',\n-        'PositiveSmallIntegerField': 'NUMBER(11) CHECK (%(qn_column)s >= 0)',\n+        'PositiveIntegerField': 'NUMBER(11)',\n+        'PositiveSmallIntegerField': 'NUMBER(11)',\n         'SlugField': 'NVARCHAR2(%(max_length)s)',\n         'SmallIntegerField': 'NUMBER(11)',\n         'TextField': 'NCLOB',\n@@ -46,6 +46,13 @@ class DatabaseCreation(BaseDatabaseCreation):\n         'URLField': 'VARCHAR2(%(max_length)s)',\n     }\n \n+    data_type_check_constraints = {\n+        'BooleanField': '%(qn_column)s IN (0,1)',\n+        'NullBooleanField': '(%(qn_column)s IN (0,1)) OR (%(qn_column)s IS NULL)',\n+        'PositiveIntegerField': '%(qn_column)s >= 0',\n+        'PositiveSmallIntegerField': '%(qn_column)s >= 0',\n+    }\n+\n     def __init__(self, connection):\n         super(DatabaseCreation, self).__init__(connection)\n \ndiff --git a/django/db/backends/oracle/introspection.py b/django/db/backends/oracle/introspection.py\nindex a2fad9250926..70c38c8de85f 100644\n--- a/django/db/backends/oracle/introspection.py\n+++ b/django/db/backends/oracle/introspection.py\n@@ -134,3 +134,143 @@ def get_indexes(self, cursor, table_name):\n             indexes[row[0]] = {'primary_key': bool(row[1]),\n                                'unique': bool(row[2])}\n         return indexes\n+\n+    def get_constraints(self, cursor, table_name):\n+        \"\"\"\n+        Retrieves any constraints or keys (unique, pk, fk, check, index) across one or more columns.\n+        \"\"\"\n+        constraints = {}\n+        # Loop over the constraints, getting PKs and uniques\n+        cursor.execute(\"\"\"\n+            SELECT\n+                user_constraints.constraint_name,\n+                LOWER(cols.column_name) AS column_name,\n+                CASE user_constraints.constraint_type\n+                    WHEN 'P' THEN 1\n+                    ELSE 0\n+                END AS is_primary_key,\n+                CASE user_indexes.uniqueness\n+                    WHEN 'UNIQUE' THEN 1\n+                    ELSE 0\n+                END AS is_unique,\n+                CASE user_constraints.constraint_type\n+                    WHEN 'C' THEN 1\n+                    ELSE 0\n+                END AS is_check_constraint\n+            FROM\n+                user_constraints\n+            INNER JOIN\n+                user_indexes ON user_indexes.index_name = user_constraints.index_name\n+            LEFT OUTER JOIN\n+                user_cons_columns cols ON user_constraints.constraint_name = cols.constraint_name\n+            WHERE\n+                (\n+                    user_constraints.constraint_type = 'P' OR\n+                    user_constraints.constraint_type = 'U'\n+                )\n+                AND user_constraints.table_name = UPPER(%s)\n+            ORDER BY cols.position\n+        \"\"\", [table_name])\n+        for constraint, column, pk, unique, check in cursor.fetchall():\n+            # If we're the first column, make the record\n+            if constraint not in constraints:\n+                constraints[constraint] = {\n+                    \"columns\": [],\n+                    \"primary_key\": pk,\n+                    \"unique\": unique,\n+                    \"foreign_key\": None,\n+                    \"check\": check,\n+                    \"index\": True,  # All P and U come with index, see inner join above\n+                }\n+            # Record the details\n+            constraints[constraint]['columns'].append(column)\n+        # Check constraints\n+        cursor.execute(\"\"\"\n+            SELECT\n+                cons.constraint_name,\n+                LOWER(cols.column_name) AS column_name\n+            FROM\n+                user_constraints cons\n+            LEFT OUTER JOIN\n+                user_cons_columns cols ON cons.constraint_name = cols.constraint_name\n+            WHERE\n+                cons.constraint_type = 'C' AND\n+                cons.table_name = UPPER(%s)\n+            ORDER BY cols.position\n+        \"\"\", [table_name])\n+        for constraint, column in cursor.fetchall():\n+            # If we're the first column, make the record\n+            if constraint not in constraints:\n+                constraints[constraint] = {\n+                    \"columns\": [],\n+                    \"primary_key\": False,\n+                    \"unique\": False,\n+                    \"foreign_key\": None,\n+                    \"check\": True,\n+                    \"index\": False,\n+                }\n+            # Record the details\n+            constraints[constraint]['columns'].append(column)\n+        # Foreign key constraints\n+        cursor.execute(\"\"\"\n+            SELECT\n+                cons.constraint_name,\n+                LOWER(cols.column_name) AS column_name,\n+                LOWER(rcons.table_name),\n+                LOWER(rcols.column_name)\n+            FROM\n+                user_constraints cons\n+            INNER JOIN\n+                user_constraints rcons ON cons.r_constraint_name = rcons.constraint_name\n+            INNER JOIN\n+                user_cons_columns rcols ON rcols.constraint_name = rcons.constraint_name\n+            LEFT OUTER JOIN\n+                user_cons_columns cols ON cons.constraint_name = cols.constraint_name\n+            WHERE\n+                cons.constraint_type = 'R' AND\n+                cons.table_name = UPPER(%s)\n+            ORDER BY cols.position\n+        \"\"\", [table_name])\n+        for constraint, column, other_table, other_column in cursor.fetchall():\n+            # If we're the first column, make the record\n+            if constraint not in constraints:\n+                constraints[constraint] = {\n+                    \"columns\": [],\n+                    \"primary_key\": False,\n+                    \"unique\": False,\n+                    \"foreign_key\": (other_table, other_column),\n+                    \"check\": False,\n+                    \"index\": False,\n+                }\n+            # Record the details\n+            constraints[constraint]['columns'].append(column)\n+        # Now get indexes\n+        cursor.execute(\"\"\"\n+            SELECT\n+                index_name,\n+                LOWER(column_name)\n+            FROM\n+                user_ind_columns cols\n+            WHERE\n+                table_name = UPPER(%s) AND\n+                NOT EXISTS (\n+                    SELECT 1\n+                    FROM user_constraints cons\n+                    WHERE cols.index_name = cons.index_name\n+                )\n+            ORDER BY cols.column_position\n+        \"\"\", [table_name])\n+        for constraint, column in cursor.fetchall():\n+            # If we're the first column, make the record\n+            if constraint not in constraints:\n+                constraints[constraint] = {\n+                    \"columns\": [],\n+                    \"primary_key\": False,\n+                    \"unique\": False,\n+                    \"foreign_key\": None,\n+                    \"check\": False,\n+                    \"index\": True,\n+                }\n+            # Record the details\n+            constraints[constraint]['columns'].append(column)\n+        return constraints\ndiff --git a/django/db/backends/oracle/schema.py b/django/db/backends/oracle/schema.py\nnew file mode 100644\nindex 000000000000..18d67b254f9d\n--- /dev/null\n+++ b/django/db/backends/oracle/schema.py\n@@ -0,0 +1,103 @@\n+import copy\n+import datetime\n+from django.utils import six\n+from django.db.backends.schema import BaseDatabaseSchemaEditor\n+from django.db.utils import DatabaseError\n+\n+\n+class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n+    \n+    sql_create_column = \"ALTER TABLE %(table)s ADD %(column)s %(definition)s\"\n+    sql_alter_column_type = \"MODIFY %(column)s %(type)s\"\n+    sql_alter_column_null = \"MODIFY %(column)s NULL\"\n+    sql_alter_column_not_null = \"MODIFY %(column)s NOT NULL\"\n+    sql_alter_column_default = \"MODIFY %(column)s DEFAULT %(default)s\"\n+    sql_alter_column_no_default = \"MODIFY %(column)s DEFAULT NULL\"\n+    sql_delete_column = \"ALTER TABLE %(table)s DROP COLUMN %(column)s\"\n+    sql_delete_table = \"DROP TABLE %(table)s CASCADE CONSTRAINTS\"\n+    \n+    def delete_model(self, model):\n+        # Run superclass action\n+        super(DatabaseSchemaEditor, self).delete_model(model)\n+        # Clean up any autoincrement trigger\n+        self.execute(\"\"\"\n+            DECLARE\n+                i INTEGER;\n+            BEGIN\n+                SELECT COUNT(*) INTO i FROM USER_CATALOG\n+                    WHERE TABLE_NAME = '%(sq_name)s' AND TABLE_TYPE = 'SEQUENCE';\n+                IF i = 1 THEN\n+                    EXECUTE IMMEDIATE 'DROP SEQUENCE \"%(sq_name)s\"';\n+                END IF;\n+            END;\n+        /\"\"\" % {'sq_name': self.connection.ops._get_sequence_name(model._meta.db_table)})\n+\n+    def alter_field(self, model, old_field, new_field, strict=False):\n+        try:\n+            # Run superclass action\n+            super(DatabaseSchemaEditor, self).alter_field(model, old_field, new_field, strict)\n+        except DatabaseError as e:\n+            description = str(e)\n+            # If we're changing to/from LOB fields, we need to do a\n+            # SQLite-ish workaround\n+            if 'ORA-22858' in description or 'ORA-22859' in description:\n+                self._alter_field_lob_workaround(model, old_field, new_field)\n+            else:\n+                raise\n+\n+    def _alter_field_lob_workaround(self, model, old_field, new_field):\n+        \"\"\"\n+        Oracle refuses to change a column type from/to LOB to/from a regular\n+        column. In Django, this shows up when the field is changed from/to\n+        a TextField.\n+        What we need to do instead is:\n+        - Add the desired field with a temporary name\n+        - Update the table to transfer values from old to new\n+        - Drop old column\n+        - Rename the new column\n+        \"\"\"\n+        # Make a new field that's like the new one but with a temporary\n+        # column name.\n+        new_temp_field = copy.deepcopy(new_field)\n+        new_temp_field.column = self._generate_temp_name(new_field.column)\n+        # Add it\n+        self.add_field(model, new_temp_field)\n+        # Transfer values across\n+        self.execute(\"UPDATE %s set %s=%s\" % (\n+            self.quote_name(model._meta.db_table),\n+            self.quote_name(new_temp_field.column),\n+            self.quote_name(old_field.column),\n+        ))\n+        # Drop the old field\n+        self.remove_field(model, old_field)\n+        # Rename the new field\n+        self.alter_field(model, new_temp_field, new_field)\n+        # Close the connection to force cx_Oracle to get column types right\n+        # on a new cursor\n+        self.connection.close()\n+\n+    def normalize_name(self, name):\n+        \"\"\"\n+        Get the properly shortened and uppercased identifier as returned by quote_name(), but without the actual quotes.\n+        \"\"\"\n+        nn = self.quote_name(name)\n+        if nn[0] == '\"' and nn[-1] == '\"':\n+            nn = nn[1:-1]\n+        return nn\n+\n+    def _generate_temp_name(self, for_name):\n+        \"\"\"\n+        Generates temporary names for workarounds that need temp columns\n+        \"\"\"\n+        suffix = hex(hash(for_name)).upper()[1:]\n+        return self.normalize_name(for_name + \"_\" + suffix)\n+\n+    def prepare_default(self, value):\n+        if isinstance(value, (datetime.date, datetime.time, datetime.datetime)):\n+            return \"'%s'\" % value\n+        elif isinstance(value, six.string_types):\n+            return repr(value)\n+        elif isinstance(value, bool):\n+            return \"1\" if value else \"0\"\n+        else:\n+            return str(value)\ndiff --git a/django/db/backends/postgresql_psycopg2/base.py b/django/db/backends/postgresql_psycopg2/base.py\nindex f0a82c22d67c..76b2935a1f4f 100644\n--- a/django/db/backends/postgresql_psycopg2/base.py\n+++ b/django/db/backends/postgresql_psycopg2/base.py\n@@ -14,6 +14,7 @@\n from django.db.backends.postgresql_psycopg2.creation import DatabaseCreation\n from django.db.backends.postgresql_psycopg2.version import get_version\n from django.db.backends.postgresql_psycopg2.introspection import DatabaseIntrospection\n+from django.db.backends.postgresql_psycopg2.schema import DatabaseSchemaEditor\n from django.utils.encoding import force_str\n from django.utils.functional import cached_property\n from django.utils.safestring import SafeText, SafeBytes\n@@ -55,6 +56,8 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n     supports_tablespaces = True\n     supports_transactions = True\n     can_distinct_on_fields = True\n+    can_rollback_ddl = True\n+    supports_combined_alters = True\n     nulls_order_largest = True\n \n \n@@ -202,6 +205,10 @@ def is_usable(self):\n         else:\n             return True\n \n+    def schema_editor(self):\n+        \"Returns a new instance of this backend's SchemaEditor\"\n+        return DatabaseSchemaEditor(self)\n+\n     @cached_property\n     def psycopg2_version(self):\n         version = psycopg2.__version__.split(' ', 1)[0]\ndiff --git a/django/db/backends/postgresql_psycopg2/creation.py b/django/db/backends/postgresql_psycopg2/creation.py\nindex cbf901555d52..954e198764db 100644\n--- a/django/db/backends/postgresql_psycopg2/creation.py\n+++ b/django/db/backends/postgresql_psycopg2/creation.py\n@@ -25,14 +25,19 @@ class DatabaseCreation(BaseDatabaseCreation):\n         'GenericIPAddressField': 'inet',\n         'NullBooleanField': 'boolean',\n         'OneToOneField': 'integer',\n-        'PositiveIntegerField': 'integer CHECK (\"%(column)s\" >= 0)',\n-        'PositiveSmallIntegerField': 'smallint CHECK (\"%(column)s\" >= 0)',\n+        'PositiveIntegerField': 'integer',\n+        'PositiveSmallIntegerField': 'smallint',\n         'SlugField': 'varchar(%(max_length)s)',\n         'SmallIntegerField': 'smallint',\n         'TextField': 'text',\n         'TimeField': 'time',\n     }\n \n+    data_type_check_constraints = {\n+        'PositiveIntegerField': '\"%(column)s\" >= 0',\n+        'PositiveSmallIntegerField': '\"%(column)s\" >= 0',\n+    }\n+\n     def sql_table_creation_suffix(self):\n         assert self.connection.settings_dict['TEST_COLLATION'] is None, \"PostgreSQL does not support collation setting at database creation time.\"\n         if self.connection.settings_dict['TEST_CHARSET']:\ndiff --git a/django/db/backends/postgresql_psycopg2/introspection.py b/django/db/backends/postgresql_psycopg2/introspection.py\nindex 24017853144c..57d9a67abf4d 100644\n--- a/django/db/backends/postgresql_psycopg2/introspection.py\n+++ b/django/db/backends/postgresql_psycopg2/introspection.py\n@@ -107,5 +107,100 @@ def get_indexes(self, cursor, table_name):\n             # Here, we skip any indexes across multiple fields.\n             if ' ' in row[1]:\n                 continue\n-            indexes[row[0]] = {'primary_key': row[3], 'unique': row[2]}\n+            if row[0] not in indexes:\n+                indexes[row[0]] = {'primary_key': False, 'unique': False}\n+            # It's possible to have the unique and PK constraints in separate indexes.\n+            if row[3]:\n+                indexes[row[0]]['primary_key'] = True\n+            if row[2]:\n+                indexes[row[0]]['unique'] = True\n         return indexes\n+\n+    def get_constraints(self, cursor, table_name):\n+        \"\"\"\n+        Retrieves any constraints or keys (unique, pk, fk, check, index) across one or more columns.\n+        \"\"\"\n+        constraints = {}\n+        # Loop over the key table, collecting things as constraints\n+        # This will get PKs, FKs, and uniques, but not CHECK\n+        cursor.execute(\"\"\"\n+            SELECT\n+                kc.constraint_name,\n+                kc.column_name,\n+                c.constraint_type,\n+                array(SELECT table_name::text || '.' || column_name::text FROM information_schema.constraint_column_usage WHERE constraint_name = kc.constraint_name)\n+            FROM information_schema.key_column_usage AS kc\n+            JOIN information_schema.table_constraints AS c ON\n+                kc.table_schema = c.table_schema AND\n+                kc.table_name = c.table_name AND\n+                kc.constraint_name = c.constraint_name\n+            WHERE\n+                kc.table_schema = %s AND\n+                kc.table_name = %s\n+        \"\"\", [\"public\", table_name])\n+        for constraint, column, kind, used_cols in cursor.fetchall():\n+            # If we're the first column, make the record\n+            if constraint not in constraints:\n+                constraints[constraint] = {\n+                    \"columns\": [],\n+                    \"primary_key\": kind.lower() == \"primary key\",\n+                    \"unique\": kind.lower() in [\"primary key\", \"unique\"],\n+                    \"foreign_key\": tuple(used_cols[0].split(\".\", 1)) if kind.lower() == \"foreign key\" else None,\n+                    \"check\": False,\n+                    \"index\": False,\n+                }\n+            # Record the details\n+            constraints[constraint]['columns'].append(column)\n+        # Now get CHECK constraint columns\n+        cursor.execute(\"\"\"\n+            SELECT kc.constraint_name, kc.column_name\n+            FROM information_schema.constraint_column_usage AS kc\n+            JOIN information_schema.table_constraints AS c ON\n+                kc.table_schema = c.table_schema AND\n+                kc.table_name = c.table_name AND\n+                kc.constraint_name = c.constraint_name\n+            WHERE\n+                c.constraint_type = 'CHECK' AND\n+                kc.table_schema = %s AND\n+                kc.table_name = %s\n+        \"\"\", [\"public\", table_name])\n+        for constraint, column in cursor.fetchall():\n+            # If we're the first column, make the record\n+            if constraint not in constraints:\n+                constraints[constraint] = {\n+                    \"columns\": [],\n+                    \"primary_key\": False,\n+                    \"unique\": False,\n+                    \"foreign_key\": None,\n+                    \"check\": True,\n+                    \"index\": False,\n+                }\n+            # Record the details\n+            constraints[constraint]['columns'].append(column)\n+        # Now get indexes\n+        cursor.execute(\"\"\"\n+            SELECT\n+                c2.relname,\n+                ARRAY(\n+                    SELECT (SELECT attname FROM pg_catalog.pg_attribute WHERE attnum = i AND attrelid = c.oid)\n+                    FROM unnest(idx.indkey) i\n+                ),\n+                idx.indisunique,\n+                idx.indisprimary\n+            FROM pg_catalog.pg_class c, pg_catalog.pg_class c2,\n+                pg_catalog.pg_index idx\n+            WHERE c.oid = idx.indrelid\n+                AND idx.indexrelid = c2.oid\n+                AND c.relname = %s\n+        \"\"\", [table_name])\n+        for index, columns, unique, primary in cursor.fetchall():\n+            if index not in constraints:\n+                constraints[index] = {\n+                    \"columns\": list(columns),\n+                    \"primary_key\": primary,\n+                    \"unique\": unique,\n+                    \"foreign_key\": None,\n+                    \"check\": False,\n+                    \"index\": True,\n+                }\n+        return constraints\ndiff --git a/django/db/backends/postgresql_psycopg2/schema.py b/django/db/backends/postgresql_psycopg2/schema.py\nnew file mode 100644\nindex 000000000000..b86e0857bbb9\n--- /dev/null\n+++ b/django/db/backends/postgresql_psycopg2/schema.py\n@@ -0,0 +1,5 @@\n+from django.db.backends.schema import BaseDatabaseSchemaEditor\n+\n+\n+class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n+    pass\ndiff --git a/django/db/backends/schema.py b/django/db/backends/schema.py\nnew file mode 100644\nindex 000000000000..64098499f6f0\n--- /dev/null\n+++ b/django/db/backends/schema.py\n@@ -0,0 +1,729 @@\n+import hashlib\n+import operator\n+import sys\n+\n+from django.db.backends.creation import BaseDatabaseCreation\n+from django.db.backends.util import truncate_name\n+from django.db.models.fields.related import ManyToManyField\n+from django.db.transaction import atomic\n+from django.utils.log import getLogger\n+from django.utils.six.moves import reduce\n+\n+logger = getLogger('django.db.backends.schema')\n+\n+\n+class BaseDatabaseSchemaEditor(object):\n+    \"\"\"\n+    This class (and its subclasses) are responsible for emitting schema-changing\n+    statements to the databases - model creation/removal/alteration, field\n+    renaming, index fiddling, and so on.\n+\n+    It is intended to eventually completely replace DatabaseCreation.\n+\n+    This class should be used by creating an instance for each set of schema\n+    changes (e.g. a syncdb run, a migration file), and by first calling start(),\n+    then the relevant actions, and then commit(). This is necessary to allow\n+    things like circular foreign key references - FKs will only be created once\n+    commit() is called.\n+    \"\"\"\n+\n+    # Overrideable SQL templates\n+    sql_create_table = \"CREATE TABLE %(table)s (%(definition)s)\"\n+    sql_create_table_unique = \"UNIQUE (%(columns)s)\"\n+    sql_rename_table = \"ALTER TABLE %(old_table)s RENAME TO %(new_table)s\"\n+    sql_retablespace_table = \"ALTER TABLE %(table)s SET TABLESPACE %(new_tablespace)s\"\n+    sql_delete_table = \"DROP TABLE %(table)s CASCADE\"\n+\n+    sql_create_column = \"ALTER TABLE %(table)s ADD COLUMN %(column)s %(definition)s\"\n+    sql_alter_column = \"ALTER TABLE %(table)s %(changes)s\"\n+    sql_alter_column_type = \"ALTER COLUMN %(column)s TYPE %(type)s\"\n+    sql_alter_column_null = \"ALTER COLUMN %(column)s DROP NOT NULL\"\n+    sql_alter_column_not_null = \"ALTER COLUMN %(column)s SET NOT NULL\"\n+    sql_alter_column_default = \"ALTER COLUMN %(column)s SET DEFAULT %(default)s\"\n+    sql_alter_column_no_default = \"ALTER COLUMN %(column)s DROP DEFAULT\"\n+    sql_delete_column = \"ALTER TABLE %(table)s DROP COLUMN %(column)s CASCADE\"\n+    sql_rename_column = \"ALTER TABLE %(table)s RENAME COLUMN %(old_column)s TO %(new_column)s\"\n+\n+    sql_create_check = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s CHECK (%(check)s)\"\n+    sql_delete_check = \"ALTER TABLE %(table)s DROP CONSTRAINT %(name)s\"\n+\n+    sql_create_unique = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s UNIQUE (%(columns)s)\"\n+    sql_delete_unique = \"ALTER TABLE %(table)s DROP CONSTRAINT %(name)s\"\n+\n+    sql_create_fk = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) REFERENCES %(to_table)s (%(to_column)s) DEFERRABLE INITIALLY DEFERRED\"\n+    sql_delete_fk = \"ALTER TABLE %(table)s DROP CONSTRAINT %(name)s\"\n+\n+    sql_create_index = \"CREATE INDEX %(name)s ON %(table)s (%(columns)s)%(extra)s;\"\n+    sql_delete_index = \"DROP INDEX %(name)s\"\n+\n+    sql_create_pk = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s PRIMARY KEY (%(columns)s)\"\n+    sql_delete_pk = \"ALTER TABLE %(table)s DROP CONSTRAINT %(name)s\"\n+\n+    def __init__(self, connection):\n+        self.connection = connection\n+\n+    # State-managing methods\n+\n+    def __enter__(self):\n+        self.deferred_sql = []\n+        atomic(self.connection.alias, self.connection.features.can_rollback_ddl).__enter__()\n+        return self\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+        if exc_type is None:\n+            for sql in self.deferred_sql:\n+                self.execute(sql)\n+            atomic(self.connection.alias, self.connection.features.can_rollback_ddl).__exit__(None, None, None)\n+        else:\n+            # Continue propagating exception\n+            return None\n+\n+    # Core utility functions\n+\n+    def execute(self, sql, params=[]):\n+        \"\"\"\n+        Executes the given SQL statement, with optional parameters.\n+        \"\"\"\n+        # Get the cursor\n+        cursor = self.connection.cursor()\n+        # Log the command we're running, then run it\n+        logger.debug(\"%s; (params %r)\" % (sql, params))\n+        cursor.execute(sql, params)\n+\n+    def quote_name(self, name):\n+        return self.connection.ops.quote_name(name)\n+\n+    # Field <-> database mapping functions\n+\n+    def column_sql(self, model, field, include_default=False):\n+        \"\"\"\n+        Takes a field and returns its column definition.\n+        The field must already have had set_attributes_from_name called.\n+        \"\"\"\n+        # Get the column's type and use that as the basis of the SQL\n+        db_params = field.db_parameters(connection=self.connection)\n+        sql = db_params['type']\n+        params = []\n+        # Check for fields that aren't actually columns (e.g. M2M)\n+        if sql is None:\n+            return None\n+        # Optionally add the tablespace if it's an implicitly indexed column\n+        tablespace = field.db_tablespace or model._meta.db_tablespace\n+        if tablespace and self.connection.features.supports_tablespaces and field.unique:\n+            sql += \" %s\" % self.connection.ops.tablespace_sql(tablespace, inline=True)\n+        # Work out nullability\n+        null = field.null\n+        # If we were told to include a default value, do so\n+        default_value = self.effective_default(field)\n+        if include_default and default_value is not None:\n+            if self.connection.features.requires_literal_defaults:\n+                # Some databases can't take defaults as a parameter (oracle)\n+                # If this is the case, the individual schema backend should\n+                # implement prepare_default\n+                sql += \" DEFAULT %s\" % self.prepare_default(default_value)\n+            else:\n+                sql += \" DEFAULT %s\"\n+                params += [default_value]\n+        # Oracle treats the empty string ('') as null, so coerce the null\n+        # option whenever '' is a possible value.\n+        if (field.empty_strings_allowed and not field.primary_key and\n+                self.connection.features.interprets_empty_strings_as_nulls):\n+            null = True\n+        if null:\n+            sql += \" NULL\"\n+        else:\n+            sql += \" NOT NULL\"\n+        # Primary key/unique outputs\n+        if field.primary_key:\n+            sql += \" PRIMARY KEY\"\n+        elif field.unique:\n+            sql += \" UNIQUE\"\n+        # Return the sql\n+        return sql, params\n+\n+    def prepare_default(self, value):\n+        \"\"\"\n+        Only used for backends which have requires_literal_defaults feature\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def effective_default(self, field):\n+        \"\"\"\n+        Returns a field's effective database default value\n+        \"\"\"\n+        if field.has_default():\n+            default = field.get_default()\n+        elif not field.null and field.blank and field.empty_strings_allowed:\n+            default = \"\"\n+        else:\n+            default = None\n+        # If it's a callable, call it\n+        if callable(default):\n+            default = default()\n+        return default\n+\n+    # Actions\n+\n+    def create_model(self, model):\n+        \"\"\"\n+        Takes a model and creates a table for it in the database.\n+        Will also create any accompanying indexes or unique constraints.\n+        \"\"\"\n+        # Create column SQL, add FK deferreds if needed\n+        column_sqls = []\n+        params = []\n+        for field in model._meta.local_fields:\n+            # SQL\n+            definition, extra_params = self.column_sql(model, field)\n+            if definition is None:\n+                continue\n+            # Check constraints can go on the column SQL here\n+            db_params = field.db_parameters(connection=self.connection)\n+            if db_params['check']:\n+                definition += \" CHECK (%s)\" % db_params['check']\n+            # Add the SQL to our big list\n+            column_sqls.append(\"%s %s\" % (\n+                self.quote_name(field.column),\n+                definition,\n+            ))\n+            params.extend(extra_params)\n+            # Indexes\n+            if field.db_index and not field.unique:\n+                self.deferred_sql.append(\n+                    self.sql_create_index % {\n+                        \"name\": self._create_index_name(model, [field.column], suffix=\"\"),\n+                        \"table\": self.quote_name(model._meta.db_table),\n+                        \"columns\": self.quote_name(field.column),\n+                        \"extra\": \"\",\n+                    }\n+                )\n+            # FK\n+            if field.rel and self.connection.features.supports_foreign_keys:\n+                to_table = field.rel.to._meta.db_table\n+                to_column = field.rel.to._meta.get_field(field.rel.field_name).column\n+                self.deferred_sql.append(\n+                    self.sql_create_fk % {\n+                        \"name\": self._create_index_name(model, [field.column], suffix=\"_fk_%s_%s\" % (to_table, to_column)),\n+                        \"table\": self.quote_name(model._meta.db_table),\n+                        \"column\": self.quote_name(field.column),\n+                        \"to_table\": self.quote_name(to_table),\n+                        \"to_column\": self.quote_name(to_column),\n+                    }\n+                )\n+            # Autoincrement SQL\n+            if field.get_internal_type() == \"AutoField\":\n+                autoinc_sql = self.connection.ops.autoinc_sql(model._meta.db_table, field.column)\n+                if autoinc_sql:\n+                    self.deferred_sql.extend(autoinc_sql)\n+        # Add any unique_togethers\n+        for fields in model._meta.unique_together:\n+            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            column_sqls.append(self.sql_create_table_unique % {\n+                \"columns\": \", \".join(self.quote_name(column) for column in columns),\n+            })\n+        # Make the table\n+        sql = self.sql_create_table % {\n+            \"table\": model._meta.db_table,\n+            \"definition\": \", \".join(column_sqls)\n+        }\n+        self.execute(sql, params)\n+        # Add any index_togethers\n+        for fields in model._meta.index_together:\n+            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            self.execute(self.sql_create_index % {\n+                \"table\": self.quote_name(model._meta.db_table),\n+                \"name\": self._create_index_name(model, columns, suffix=\"_idx\"),\n+                \"columns\": \", \".join(self.quote_name(column) for column in columns),\n+                \"extra\": \"\",\n+            })\n+        # Make M2M tables\n+        for field in model._meta.local_many_to_many:\n+            self.create_model(field.rel.through)\n+\n+    def delete_model(self, model):\n+        \"\"\"\n+        Deletes a model from the database.\n+        \"\"\"\n+        # Delete the table\n+        self.execute(self.sql_delete_table % {\n+            \"table\": self.quote_name(model._meta.db_table),\n+        })\n+\n+    def alter_unique_together(self, model, old_unique_together, new_unique_together):\n+        \"\"\"\n+        Deals with a model changing its unique_together.\n+        Note: The input unique_togethers must be doubly-nested, not the single-\n+        nested [\"foo\", \"bar\"] format.\n+        \"\"\"\n+        olds = set(tuple(fields) for fields in old_unique_together)\n+        news = set(tuple(fields) for fields in new_unique_together)\n+        # Deleted uniques\n+        for fields in olds.difference(news):\n+            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            constraint_names = self._constraint_names(model, columns, unique=True)\n+            if len(constraint_names) != 1:\n+                raise ValueError(\"Found wrong number (%s) of constraints for %s(%s)\" % (\n+                    len(constraint_names),\n+                    model._meta.db_table,\n+                    \", \".join(columns),\n+                ))\n+            self.execute(\n+                self.sql_delete_unique % {\n+                    \"table\": self.quote_name(model._meta.db_table),\n+                    \"name\": constraint_names[0],\n+                },\n+            )\n+        # Created uniques\n+        for fields in news.difference(olds):\n+            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            self.execute(self.sql_create_unique % {\n+                \"table\": self.quote_name(model._meta.db_table),\n+                \"name\": self._create_index_name(model, columns, suffix=\"_uniq\"),\n+                \"columns\": \", \".join(self.quote_name(column) for column in columns),\n+            })\n+\n+    def alter_index_together(self, model, old_index_together, new_index_together):\n+        \"\"\"\n+        Deals with a model changing its index_together.\n+        Note: The input index_togethers must be doubly-nested, not the single-\n+        nested [\"foo\", \"bar\"] format.\n+        \"\"\"\n+        olds = set(tuple(fields) for fields in old_index_together)\n+        news = set(tuple(fields) for fields in new_index_together)\n+        # Deleted indexes\n+        for fields in olds.difference(news):\n+            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            constraint_names = self._constraint_names(model, list(columns), index=True)\n+            if len(constraint_names) != 1:\n+                raise ValueError(\"Found wrong number (%s) of constraints for %s(%s)\" % (\n+                    len(constraint_names),\n+                    model._meta.db_table,\n+                    \", \".join(columns),\n+                ))\n+            self.execute(\n+                self.sql_delete_index % {\n+                    \"table\": self.quote_name(model._meta.db_table),\n+                    \"name\": constraint_names[0],\n+                },\n+            )\n+        # Created indexes\n+        for fields in news.difference(olds):\n+            columns = [model._meta.get_field_by_name(field)[0].column for field in fields]\n+            self.execute(self.sql_create_index % {\n+                \"table\": self.quote_name(model._meta.db_table),\n+                \"name\": self._create_index_name(model, columns, suffix=\"_idx\"),\n+                \"columns\": \", \".join(self.quote_name(column) for column in columns),\n+                \"extra\": \"\",\n+            })\n+\n+    def alter_db_table(self, model, old_db_table, new_db_table):\n+        \"\"\"\n+        Renames the table a model points to.\n+        \"\"\"\n+        self.execute(self.sql_rename_table % {\n+            \"old_table\": self.quote_name(old_db_table),\n+            \"new_table\": self.quote_name(new_db_table),\n+        })\n+\n+    def alter_db_tablespace(self, model, old_db_tablespace, new_db_tablespace):\n+        \"\"\"\n+        Moves a model's table between tablespaces\n+        \"\"\"\n+        self.execute(self.sql_retablespace_table % {\n+            \"table\": self.quote_name(model._meta.db_table),\n+            \"old_tablespace\": self.quote_name(old_db_tablespace),\n+            \"new_tablespace\": self.quote_name(new_db_tablespace),\n+        })\n+\n+    def add_field(self, model, field):\n+        \"\"\"\n+        Creates a field on a model.\n+        Usually involves adding a column, but may involve adding a\n+        table instead (for M2M fields)\n+        \"\"\"\n+        # Special-case implicit M2M tables\n+        if isinstance(field, ManyToManyField) and field.rel.through._meta.auto_created:\n+            return self.create_model(field.rel.through)\n+        # Get the column's definition\n+        definition, params = self.column_sql(model, field, include_default=True)\n+        # It might not actually have a column behind it\n+        if definition is None:\n+            return\n+        # Check constraints can go on the column SQL here\n+        db_params = field.db_parameters(connection=self.connection)\n+        if db_params['check']:\n+            definition += \" CHECK (%s)\" % db_params['check']\n+        # Build the SQL and run it\n+        sql = self.sql_create_column % {\n+            \"table\": self.quote_name(model._meta.db_table),\n+            \"column\": self.quote_name(field.column),\n+            \"definition\": definition,\n+        }\n+        self.execute(sql, params)\n+        # Drop the default if we need to\n+        # (Django usually does not use in-database defaults)\n+        if field.default is not None:\n+            sql = self.sql_alter_column % {\n+                \"table\": self.quote_name(model._meta.db_table),\n+                \"changes\": self.sql_alter_column_no_default % {\n+                    \"column\": self.quote_name(field.column),\n+                }\n+            }\n+            self.execute(sql)\n+        # Add an index, if required\n+        if field.db_index and not field.unique:\n+            self.deferred_sql.append(\n+                self.sql_create_index % {\n+                    \"name\": self._create_index_name(model, [field.column], suffix=\"\"),\n+                    \"table\": self.quote_name(model._meta.db_table),\n+                    \"columns\": self.quote_name(field.column),\n+                    \"extra\": \"\",\n+                }\n+            )\n+        # Add any FK constraints later\n+        if field.rel and self.connection.features.supports_foreign_keys:\n+            to_table = field.rel.to._meta.db_table\n+            to_column = field.rel.to._meta.get_field(field.rel.field_name).column\n+            self.deferred_sql.append(\n+                self.sql_create_fk % {\n+                    \"name\": '%s_refs_%s_%x' % (\n+                        field.column,\n+                        to_column,\n+                        abs(hash((model._meta.db_table, to_table)))\n+                    ),\n+                    \"table\": self.quote_name(model._meta.db_table),\n+                    \"column\": self.quote_name(field.column),\n+                    \"to_table\": self.quote_name(to_table),\n+                    \"to_column\": self.quote_name(to_column),\n+                }\n+            )\n+        # Reset connection if required\n+        if self.connection.features.connection_persists_old_columns:\n+            self.connection.close()\n+\n+    def remove_field(self, model, field):\n+        \"\"\"\n+        Removes a field from a model. Usually involves deleting a column,\n+        but for M2Ms may involve deleting a table.\n+        \"\"\"\n+        # Special-case implicit M2M tables\n+        if isinstance(field, ManyToManyField) and field.rel.through._meta.auto_created:\n+            return self.delete_model(field.rel.through)\n+        # It might not actually have a column behind it\n+        if field.db_parameters(connection=self.connection)['type'] is None:\n+            return\n+        # Get the column's definition\n+        definition, params = self.column_sql(model, field)\n+        # Delete the column\n+        sql = self.sql_delete_column % {\n+            \"table\": self.quote_name(model._meta.db_table),\n+            \"column\": self.quote_name(field.column),\n+        }\n+        self.execute(sql)\n+        # Reset connection if required\n+        if self.connection.features.connection_persists_old_columns:\n+            self.connection.close()\n+\n+    def alter_field(self, model, old_field, new_field, strict=False):\n+        \"\"\"\n+        Allows a field's type, uniqueness, nullability, default, column,\n+        constraints etc. to be modified.\n+        Requires a copy of the old field as well so we can only perform\n+        changes that are required.\n+        If strict is true, raises errors if the old column does not match old_field precisely.\n+        \"\"\"\n+        # Ensure this field is even column-based\n+        old_db_params = old_field.db_parameters(connection=self.connection)\n+        old_type = old_db_params['type']\n+        new_db_params = new_field.db_parameters(connection=self.connection)\n+        new_type = new_db_params['type']\n+        if old_type is None and new_type is None and (old_field.rel.through and new_field.rel.through and old_field.rel.through._meta.auto_created and new_field.rel.through._meta.auto_created):\n+            return self._alter_many_to_many(model, old_field, new_field, strict)\n+        elif old_type is None or new_type is None:\n+            raise ValueError(\"Cannot alter field %s into %s - they are not compatible types (probably means only one is an M2M with implicit through model)\" % (\n+                old_field,\n+                new_field,\n+            ))\n+        # Has unique been removed?\n+        if old_field.unique and (not new_field.unique or (not old_field.primary_key and new_field.primary_key)):\n+            # Find the unique constraint for this field\n+            constraint_names = self._constraint_names(model, [old_field.column], unique=True)\n+            if strict and len(constraint_names) != 1:\n+                raise ValueError(\"Found wrong number (%s) of unique constraints for %s.%s\" % (\n+                    len(constraint_names),\n+                    model._meta.db_table,\n+                    old_field.column,\n+                ))\n+            for constraint_name in constraint_names:\n+                self.execute(\n+                    self.sql_delete_unique % {\n+                        \"table\": self.quote_name(model._meta.db_table),\n+                        \"name\": constraint_name,\n+                    },\n+                )\n+        # Removed an index?\n+        if old_field.db_index and not new_field.db_index and not old_field.unique and not (not new_field.unique and old_field.unique):\n+            # Find the index for this field\n+            index_names = self._constraint_names(model, [old_field.column], index=True)\n+            if strict and len(index_names) != 1:\n+                raise ValueError(\"Found wrong number (%s) of indexes for %s.%s\" % (\n+                    len(index_names),\n+                    model._meta.db_table,\n+                    old_field.column,\n+                ))\n+            for index_name in index_names:\n+                self.execute(\n+                    self.sql_delete_index % {\n+                        \"table\": self.quote_name(model._meta.db_table),\n+                        \"name\": index_name,\n+                    }\n+                )\n+        # Drop any FK constraints, we'll remake them later\n+        if old_field.rel:\n+            fk_names = self._constraint_names(model, [old_field.column], foreign_key=True)\n+            if strict and len(fk_names) != 1:\n+                raise ValueError(\"Found wrong number (%s) of foreign key constraints for %s.%s\" % (\n+                    len(fk_names),\n+                    model._meta.db_table,\n+                    old_field.column,\n+                ))\n+            for fk_name in fk_names:\n+                self.execute(\n+                    self.sql_delete_fk % {\n+                        \"table\": self.quote_name(model._meta.db_table),\n+                        \"name\": fk_name,\n+                    }\n+                )\n+        # Change check constraints?\n+        if old_db_params['check'] != new_db_params['check'] and old_db_params['check']:\n+            constraint_names = self._constraint_names(model, [old_field.column], check=True)\n+            if strict and len(constraint_names) != 1:\n+                raise ValueError(\"Found wrong number (%s) of check constraints for %s.%s\" % (\n+                    len(constraint_names),\n+                    model._meta.db_table,\n+                    old_field.column,\n+                ))\n+            for constraint_name in constraint_names:\n+                self.execute(\n+                    self.sql_delete_check % {\n+                        \"table\": self.quote_name(model._meta.db_table),\n+                        \"name\": constraint_name,\n+                    }\n+                )\n+        # Have they renamed the column?\n+        if old_field.column != new_field.column:\n+            self.execute(self.sql_rename_column % {\n+                \"table\": self.quote_name(model._meta.db_table),\n+                \"old_column\": self.quote_name(old_field.column),\n+                \"new_column\": self.quote_name(new_field.column),\n+                \"type\": new_type,\n+            })\n+        # Next, start accumulating actions to do\n+        actions = []\n+        # Type change?\n+        if old_type != new_type:\n+            actions.append((\n+                self.sql_alter_column_type % {\n+                    \"column\": self.quote_name(new_field.column),\n+                    \"type\": new_type,\n+                },\n+                [],\n+            ))\n+        # Default change?\n+        old_default = self.effective_default(old_field)\n+        new_default = self.effective_default(new_field)\n+        if old_default != new_default:\n+            if new_default is None:\n+                actions.append((\n+                    self.sql_alter_column_no_default % {\n+                        \"column\": self.quote_name(new_field.column),\n+                    },\n+                    [],\n+                ))\n+            else:\n+                if self.connection.features.requires_literal_defaults:\n+                    # Some databases can't take defaults as a parameter (oracle)\n+                    # If this is the case, the individual schema backend should\n+                    # implement prepare_default\n+                    actions.append((\n+                        self.sql_alter_column_default % {\n+                            \"column\": self.quote_name(new_field.column),\n+                            \"default\": self.prepare_default(new_default),\n+                        },\n+                        [],\n+                    ))\n+                else:\n+                    actions.append((\n+                        self.sql_alter_column_default % {\n+                            \"column\": self.quote_name(new_field.column),\n+                            \"default\": \"%s\",\n+                        },\n+                        [new_default],\n+                    ))\n+        # Nullability change?\n+        if old_field.null != new_field.null:\n+            if new_field.null:\n+                actions.append((\n+                    self.sql_alter_column_null % {\n+                        \"column\": self.quote_name(new_field.column),\n+                        \"type\": new_type,\n+                    },\n+                    [],\n+                ))\n+            else:\n+                actions.append((\n+                    self.sql_alter_column_not_null % {\n+                        \"column\": self.quote_name(new_field.column),\n+                        \"type\": new_type,\n+                    },\n+                    [],\n+                ))\n+        if actions:\n+            # Combine actions together if we can (e.g. postgres)\n+            if self.connection.features.supports_combined_alters:\n+                sql, params = tuple(zip(*actions))\n+                actions = [(\", \".join(sql), reduce(operator.add, params))]\n+            # Apply those actions\n+            for sql, params in actions:\n+                self.execute(\n+                    self.sql_alter_column % {\n+                        \"table\": self.quote_name(model._meta.db_table),\n+                        \"changes\": sql,\n+                    },\n+                    params,\n+                )\n+        # Added a unique?\n+        if not old_field.unique and new_field.unique:\n+            self.execute(\n+                self.sql_create_unique % {\n+                    \"table\": self.quote_name(model._meta.db_table),\n+                    \"name\": self._create_index_name(model, [new_field.column], suffix=\"_uniq\"),\n+                    \"columns\": self.quote_name(new_field.column),\n+                }\n+            )\n+        # Added an index?\n+        if not old_field.db_index and new_field.db_index and not new_field.unique and not (not old_field.unique and new_field.unique):\n+            self.execute(\n+                self.sql_create_index % {\n+                    \"table\": self.quote_name(model._meta.db_table),\n+                    \"name\": self._create_index_name(model, [new_field.column], suffix=\"_uniq\"),\n+                    \"columns\": self.quote_name(new_field.column),\n+                    \"extra\": \"\",\n+                }\n+            )\n+        # Changed to become primary key?\n+        # Note that we don't detect unsetting of a PK, as we assume another field\n+        # will always come along and replace it.\n+        if not old_field.primary_key and new_field.primary_key:\n+            # First, drop the old PK\n+            constraint_names = self._constraint_names(model, primary_key=True)\n+            if strict and len(constraint_names) != 1:\n+                raise ValueError(\"Found wrong number (%s) of PK constraints for %s\" % (\n+                    len(constraint_names),\n+                    model._meta.db_table,\n+                ))\n+            for constraint_name in constraint_names:\n+                self.execute(\n+                    self.sql_delete_pk % {\n+                        \"table\": self.quote_name(model._meta.db_table),\n+                        \"name\": constraint_name,\n+                    },\n+                )\n+            # Make the new one\n+            self.execute(\n+                self.sql_create_pk % {\n+                    \"table\": self.quote_name(model._meta.db_table),\n+                    \"name\": self._create_index_name(model, [new_field.column], suffix=\"_pk\"),\n+                    \"columns\": self.quote_name(new_field.column),\n+                }\n+            )\n+        # Does it have a foreign key?\n+        if new_field.rel:\n+            self.execute(\n+                self.sql_create_fk % {\n+                    \"table\": self.quote_name(model._meta.db_table),\n+                    \"name\": self._create_index_name(model, [new_field.column], suffix=\"_fk\"),\n+                    \"column\": self.quote_name(new_field.column),\n+                    \"to_table\": self.quote_name(new_field.rel.to._meta.db_table),\n+                    \"to_column\": self.quote_name(new_field.rel.get_related_field().column),\n+                }\n+            )\n+        # Does it have check constraints we need to add?\n+        if old_db_params['check'] != new_db_params['check'] and new_db_params['check']:\n+            self.execute(\n+                self.sql_create_check % {\n+                    \"table\": self.quote_name(model._meta.db_table),\n+                    \"name\": self._create_index_name(model, [new_field.column], suffix=\"_check\"),\n+                    \"column\": self.quote_name(new_field.column),\n+                    \"check\": new_db_params['check'],\n+                }\n+            )\n+        # Reset connection if required\n+        if self.connection.features.connection_persists_old_columns:\n+            self.connection.close()\n+\n+    def _alter_many_to_many(self, model, old_field, new_field, strict):\n+        \"\"\"\n+        Alters M2Ms to repoint their to= endpoints.\n+        \"\"\"\n+        # Rename the through table\n+        self.alter_db_table(old_field.rel.through, old_field.rel.through._meta.db_table, new_field.rel.through._meta.db_table)\n+        # Repoint the FK to the other side\n+        self.alter_field(\n+            new_field.rel.through,\n+            # We need the field that points to the target model, so we can tell alter_field to change it -\n+            # this is m2m_reverse_field_name() (as opposed to m2m_field_name, which points to our model)\n+            old_field.rel.through._meta.get_field_by_name(old_field.m2m_reverse_field_name())[0],\n+            new_field.rel.through._meta.get_field_by_name(new_field.m2m_reverse_field_name())[0],\n+        )\n+\n+    def _create_index_name(self, model, column_names, suffix=\"\"):\n+        \"\"\"\n+        Generates a unique name for an index/unique constraint.\n+        \"\"\"\n+        # If there is just one column in the index, use a default algorithm from Django\n+        if len(column_names) == 1 and not suffix:\n+            return truncate_name(\n+                '%s_%s' % (model._meta.db_table, BaseDatabaseCreation._digest(column_names[0])),\n+                self.connection.ops.max_name_length()\n+            )\n+        # Else generate the name for the index using a different algorithm\n+        table_name = model._meta.db_table.replace('\"', '').replace('.', '_')\n+        index_unique_name = '_%x' % abs(hash((table_name, ','.join(column_names))))\n+        # If the index name is too long, truncate it\n+        index_name = ('%s_%s%s%s' % (table_name, column_names[0], index_unique_name, suffix)).replace('\"', '').replace('.', '_')\n+        if len(index_name) > self.connection.features.max_index_name_length:\n+            part = ('_%s%s%s' % (column_names[0], index_unique_name, suffix))\n+            index_name = '%s%s' % (table_name[:(self.connection.features.max_index_name_length - len(part))], part)\n+        # It shouldn't start with an underscore (Oracle hates this)\n+        if index_name[0] == \"_\":\n+            index_name = index_name[1:]\n+        # If it's STILL too long, just hash it down\n+        if len(index_name) > self.connection.features.max_index_name_length:\n+            index_name = hashlib.md5(index_name).hexdigest()[:self.connection.features.max_index_name_length]\n+        # It can't start with a number on Oracle, so prepend D if we need to\n+        if index_name[0].isdigit():\n+            index_name = \"D%s\" % index_name[:-1]\n+        return index_name\n+\n+    def _constraint_names(self, model, column_names=None, unique=None, primary_key=None, index=None, foreign_key=None, check=None):\n+        \"\"\"\n+        Returns all constraint names matching the columns and conditions\n+        \"\"\"\n+        column_names = list(column_names) if column_names else None\n+        constraints = self.connection.introspection.get_constraints(self.connection.cursor(), model._meta.db_table)\n+        result = []\n+        for name, infodict in constraints.items():\n+            if column_names is None or column_names == infodict['columns']:\n+                if unique is not None and infodict['unique'] != unique:\n+                    continue\n+                if primary_key is not None and infodict['primary_key'] != primary_key:\n+                    continue\n+                if index is not None and infodict['index'] != index:\n+                    continue\n+                if check is not None and infodict['check'] != check:\n+                    continue\n+                if foreign_key is not None and not infodict['foreign_key']:\n+                    continue\n+                result.append(name)\n+        return result\ndiff --git a/django/db/backends/sqlite3/base.py b/django/db/backends/sqlite3/base.py\nindex f396bf7408c2..841ca92f3e5f 100644\n--- a/django/db/backends/sqlite3/base.py\n+++ b/django/db/backends/sqlite3/base.py\n@@ -18,6 +18,7 @@\n from django.db.backends.sqlite3.client import DatabaseClient\n from django.db.backends.sqlite3.creation import DatabaseCreation\n from django.db.backends.sqlite3.introspection import DatabaseIntrospection\n+from django.db.backends.sqlite3.schema import DatabaseSchemaEditor\n from django.db.models import fields\n from django.db.models.sql import aggregates\n from django.utils.dateparse import parse_date, parse_datetime, parse_time\n@@ -100,6 +101,8 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n     supports_mixed_date_datetime_comparisons = False\n     has_bulk_insert = True\n     can_combine_inserts_with_and_without_auto_increment_pk = False\n+    supports_foreign_keys = False\n+    supports_check_constraints = False\n     autocommits_when_autocommit_is_off = True\n     supports_paramstyle_pyformat = False\n \n@@ -432,6 +435,10 @@ def _start_transaction_under_autocommit(self):\n         \"\"\"\n         self.cursor().execute(\"BEGIN\")\n \n+    def schema_editor(self):\n+        \"Returns a new instance of this backend's SchemaEditor\"\n+        return DatabaseSchemaEditor(self)\n+\n FORMAT_QMARK_REGEX = re.compile(r'(?<!%)%s')\n \n \ndiff --git a/django/db/backends/sqlite3/introspection.py b/django/db/backends/sqlite3/introspection.py\nindex 385888ed40b7..92777dd910a2 100644\n--- a/django/db/backends/sqlite3/introspection.py\n+++ b/django/db/backends/sqlite3/introspection.py\n@@ -158,7 +158,7 @@ def get_indexes(self, cursor, table_name):\n             if len(info) != 1:\n                 continue\n             name = info[0][2] # seqno, cid, name\n-            indexes[name] = {'primary_key': False,\n+            indexes[name] = {'primary_key': indexes.get(name, {}).get(\"primary_key\", False),\n                              'unique': unique}\n         return indexes\n \n@@ -168,7 +168,10 @@ def get_primary_key_column(self, cursor, table_name):\n         \"\"\"\n         # Don't use PRAGMA because that causes issues with some transactions\n         cursor.execute(\"SELECT sql FROM sqlite_master WHERE tbl_name = %s AND type = %s\", [table_name, \"table\"])\n-        results = cursor.fetchone()[0].strip()\n+        row = cursor.fetchone()\n+        if row is None:\n+            raise ValueError(\"Table %s does not exist\" % table_name)\n+        results = row[0].strip()\n         results = results[results.index('(') + 1:results.rindex(')')]\n         for field_desc in results.split(','):\n             field_desc = field_desc.strip()\n@@ -186,3 +189,41 @@ def _table_info(self, cursor, name):\n                  'null_ok': not field[3],\n                  'pk': field[5]     # undocumented\n                  } for field in cursor.fetchall()]\n+\n+    def get_constraints(self, cursor, table_name):\n+        \"\"\"\n+        Retrieves any constraints or keys (unique, pk, fk, check, index) across one or more columns.\n+        \"\"\"\n+        constraints = {}\n+        # Get the index info\n+        cursor.execute(\"PRAGMA index_list(%s)\" % self.connection.ops.quote_name(table_name))\n+        for number, index, unique in cursor.fetchall():\n+            # Get the index info for that index\n+            cursor.execute('PRAGMA index_info(%s)' % self.connection.ops.quote_name(index))\n+            for index_rank, column_rank, column in cursor.fetchall():\n+                if index not in constraints:\n+                    constraints[index] = {\n+                        \"columns\": [],\n+                        \"primary_key\": False,\n+                        \"unique\": bool(unique),\n+                        \"foreign_key\": False,\n+                        \"check\": False,\n+                        \"index\": True,\n+                    }\n+                constraints[index]['columns'].append(column)\n+        # Get the PK\n+        pk_column = self.get_primary_key_column(cursor, table_name)\n+        if pk_column:\n+            # SQLite doesn't actually give a name to the PK constraint,\n+            # so we invent one. This is fine, as the SQLite backend never\n+            # deletes PK constraints by name, as you can't delete constraints\n+            # in SQLite; we remake the table with a new PK instead.\n+            constraints[\"__primary__\"] = {\n+                \"columns\": [pk_column],\n+                \"primary_key\": True,\n+                \"unique\": False,  # It's not actually a unique constraint.\n+                \"foreign_key\": False,\n+                \"check\": False,\n+                \"index\": False,\n+            }\n+        return constraints\ndiff --git a/django/db/backends/sqlite3/schema.py b/django/db/backends/sqlite3/schema.py\nnew file mode 100644\nindex 000000000000..19bffc75206f\n--- /dev/null\n+++ b/django/db/backends/sqlite3/schema.py\n@@ -0,0 +1,155 @@\n+from django.db.backends.schema import BaseDatabaseSchemaEditor\n+from django.db.models.fields.related import ManyToManyField\n+from django.db.models.loading import BaseAppCache\n+\n+\n+class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n+\n+    sql_delete_table = \"DROP TABLE %(table)s\"\n+\n+    def _remake_table(self, model, create_fields=[], delete_fields=[], alter_fields=[], rename_fields=[], override_uniques=None):\n+        \"\"\"\n+        Shortcut to transform a model from old_model into new_model\n+        \"\"\"\n+        # Work out the new fields dict / mapping\n+        body = dict((f.name, f) for f in model._meta.local_fields)\n+        mapping = dict((f.column, f.column) for f in model._meta.local_fields)\n+        # If any of the new or altered fields is introducing a new PK,\n+        # remove the old one\n+        restore_pk_field = None\n+        if any(f.primary_key for f in create_fields) or any(n.primary_key for o, n in alter_fields):\n+            for name, field in list(body.items()):\n+                if field.primary_key:\n+                    field.primary_key = False\n+                    restore_pk_field = field\n+                    if field.auto_created:\n+                        del body[name]\n+                        del mapping[field.column]\n+        # Add in any created fields\n+        for field in create_fields:\n+            body[field.name] = field\n+        # Add in any altered fields\n+        for (old_field, new_field) in alter_fields:\n+            del body[old_field.name]\n+            del mapping[old_field.column]\n+            body[new_field.name] = new_field\n+            mapping[new_field.column] = old_field.column\n+        # Remove any deleted fields\n+        for field in delete_fields:\n+            del body[field.name]\n+            del mapping[field.column]\n+        # Work inside a new AppCache\n+        app_cache = BaseAppCache()\n+        # Construct a new model for the new state\n+        meta_contents = {\n+            'app_label': model._meta.app_label,\n+            'db_table': model._meta.db_table + \"__new\",\n+            'unique_together': model._meta.unique_together if override_uniques is None else override_uniques,\n+            'app_cache': app_cache,\n+        }\n+        meta = type(\"Meta\", tuple(), meta_contents)\n+        body['Meta'] = meta\n+        body['__module__'] = model.__module__\n+        temp_model = type(model._meta.object_name, model.__bases__, body)\n+        # Create a new table with that format\n+        self.create_model(temp_model)\n+        # Copy data from the old table\n+        field_maps = list(mapping.items())\n+        self.execute(\"INSERT INTO %s (%s) SELECT %s FROM %s;\" % (\n+            self.quote_name(temp_model._meta.db_table),\n+            ', '.join([x for x, y in field_maps]),\n+            ', '.join([y for x, y in field_maps]),\n+            self.quote_name(model._meta.db_table),\n+        ))\n+        # Delete the old table\n+        self.delete_model(model)\n+        # Rename the new to the old\n+        self.alter_db_table(model, temp_model._meta.db_table, model._meta.db_table)\n+        # Run deferred SQL on correct table\n+        for sql in self.deferred_sql:\n+            self.execute(sql.replace(temp_model._meta.db_table, model._meta.db_table))\n+        self.deferred_sql = []\n+        # Fix any PK-removed field\n+        if restore_pk_field:\n+            restore_pk_field.primary_key = True\n+\n+    def add_field(self, model, field):\n+        \"\"\"\n+        Creates a field on a model.\n+        Usually involves adding a column, but may involve adding a\n+        table instead (for M2M fields)\n+        \"\"\"\n+        # Special-case implicit M2M tables\n+        if isinstance(field, ManyToManyField) and field.rel.through._meta.auto_created:\n+            return self.create_model(field.rel.through)\n+        # Detect bad field combinations\n+        if (not field.null and\n+           (not field.has_default() or field.get_default() is None) and\n+           not field.empty_strings_allowed):\n+            raise ValueError(\"You cannot add a null=False column without a default value on SQLite.\")\n+        self._remake_table(model, create_fields=[field])\n+\n+    def remove_field(self, model, field):\n+        \"\"\"\n+        Removes a field from a model. Usually involves deleting a column,\n+        but for M2Ms may involve deleting a table.\n+        \"\"\"\n+        # Special-case implicit M2M tables\n+        if isinstance(field, ManyToManyField) and field.rel.through._meta.auto_created:\n+            return self.delete_model(field.rel.through)\n+        # For everything else, remake.\n+        self._remake_table(model, delete_fields=[field])\n+\n+    def alter_field(self, model, old_field, new_field, strict=False):\n+        \"\"\"\n+        Allows a field's type, uniqueness, nullability, default, column,\n+        constraints etc. to be modified.\n+        Requires a copy of the old field as well so we can only perform\n+        changes that are required.\n+        If strict is true, raises errors if the old column does not match old_field precisely.\n+        \"\"\"\n+        old_db_params = old_field.db_parameters(connection=self.connection)\n+        old_type = old_db_params['type']\n+        new_db_params = new_field.db_parameters(connection=self.connection)\n+        new_type = new_db_params['type']\n+        if old_type is None and new_type is None and (old_field.rel.through and new_field.rel.through and old_field.rel.through._meta.auto_created and new_field.rel.through._meta.auto_created):\n+            return self._alter_many_to_many(model, old_field, new_field, strict)\n+        elif old_type is None or new_type is None:\n+            raise ValueError(\"Cannot alter field %s into %s - they are not compatible types (probably means only one is an M2M with implicit through model)\" % (\n+                old_field,\n+                new_field,\n+            ))\n+        # Alter by remaking table\n+        self._remake_table(model, alter_fields=[(old_field, new_field)])\n+\n+    def alter_unique_together(self, model, old_unique_together, new_unique_together):\n+        \"\"\"\n+        Deals with a model changing its unique_together.\n+        Note: The input unique_togethers must be doubly-nested, not the single-\n+        nested [\"foo\", \"bar\"] format.\n+        \"\"\"\n+        self._remake_table(model, override_uniques=new_unique_together)\n+\n+    def _alter_many_to_many(self, model, old_field, new_field, strict):\n+        \"\"\"\n+        Alters M2Ms to repoint their to= endpoints.\n+        \"\"\"\n+        # Make a new through table\n+        self.create_model(new_field.rel.through)\n+        # Copy the data across\n+        self.execute(\"INSERT INTO %s (%s) SELECT %s FROM %s;\" % (\n+            self.quote_name(new_field.rel.through._meta.db_table),\n+            ', '.join([\n+                \"id\",\n+                new_field.m2m_column_name(),\n+                new_field.m2m_reverse_name(),\n+            ]),\n+            ', '.join([\n+                \"id\",\n+                old_field.m2m_column_name(),\n+                old_field.m2m_reverse_name(),\n+            ]),\n+            self.quote_name(old_field.rel.through._meta.db_table),\n+        ))\n+        # Delete the old through table\n+        self.delete_model(old_field.rel.through)\ndiff --git a/django/db/migrations/__init__.py b/django/db/migrations/__init__.py\nnew file mode 100644\nindex 000000000000..e072786473fe\n--- /dev/null\n+++ b/django/db/migrations/__init__.py\n@@ -0,0 +1,2 @@\n+from .migration import Migration\n+from .operations import *\ndiff --git a/django/db/migrations/autodetector.py b/django/db/migrations/autodetector.py\nnew file mode 100644\nindex 000000000000..334c26d97333\n--- /dev/null\n+++ b/django/db/migrations/autodetector.py\n@@ -0,0 +1,440 @@\n+import re\n+import sys\n+from django.utils import datetime_safe\n+from django.utils.six.moves import input\n+from django.db.migrations import operations\n+from django.db.migrations.migration import Migration\n+from django.db.models.loading import cache\n+\n+\n+class MigrationAutodetector(object):\n+    \"\"\"\n+    Takes a pair of ProjectStates, and compares them to see what the\n+    first would need doing to make it match the second (the second\n+    usually being the project's current state).\n+\n+    Note that this naturally operates on entire projects at a time,\n+    as it's likely that changes interact (for example, you can't\n+    add a ForeignKey without having a migration to add the table it\n+    depends on first). A user interface may offer single-app usage\n+    if it wishes, with the caveat that it may not always be possible.\n+    \"\"\"\n+\n+    def __init__(self, from_state, to_state, questioner=None):\n+        self.from_state = from_state\n+        self.to_state = to_state\n+        self.questioner = questioner or MigrationQuestioner()\n+\n+    def changes(self, graph, trim_to_apps=None):\n+        \"\"\"\n+        Main entry point to produce a list of appliable changes.\n+        Takes a graph to base names on and an optional set of apps\n+        to try and restrict to (restriction is not guaranteed)\n+        \"\"\"\n+        changes = self._detect_changes()\n+        changes = self._arrange_for_graph(changes, graph)\n+        if trim_to_apps:\n+            changes = self._trim_to_apps(changes, trim_to_apps)\n+        return changes\n+\n+    def _detect_changes(self):\n+        \"\"\"\n+        Returns a dict of migration plans which will achieve the\n+        change from from_state to to_state. The dict has app labels\n+        as keys and a list of migrations as values.\n+\n+        The resulting migrations aren't specially named, but the names\n+        do matter for dependencies inside the set.\n+        \"\"\"\n+        # We'll store migrations as lists by app names for now\n+        self.migrations = {}\n+        old_app_cache = self.from_state.render()\n+        new_app_cache = self.to_state.render()\n+        # Adding models. Phase 1 is adding models with no outward relationships.\n+        added_models = set(self.to_state.models.keys()) - set(self.from_state.models.keys())\n+        pending_add = {}\n+        for app_label, model_name in added_models:\n+            model_state = self.to_state.models[app_label, model_name]\n+            # Are there any relationships out from this model? if so, punt it to the next phase.\n+            related_fields = []\n+            for field in new_app_cache.get_model(app_label, model_name)._meta.fields:\n+                if field.rel:\n+                    if field.rel.to:\n+                        related_fields.append((field.name, field.rel.to._meta.app_label.lower(), field.rel.to._meta.object_name.lower()))\n+                    if hasattr(field.rel, \"through\") and not field.rel.though._meta.auto_created:\n+                        related_fields.append((field.name, field.rel.through._meta.app_label.lower(), field.rel.through._meta.object_name.lower()))\n+            if related_fields:\n+                pending_add[app_label, model_name] = related_fields\n+            else:\n+                self.add_to_migration(\n+                    app_label,\n+                    operations.CreateModel(\n+                        name = model_state.name,\n+                        fields = model_state.fields,\n+                        options = model_state.options,\n+                        bases = model_state.bases,\n+                    )\n+                )\n+        # Phase 2 is progressively adding pending models, splitting up into two\n+        # migrations if required.\n+        pending_new_fks = []\n+        while pending_add:\n+            # Is there one we can add that has all dependencies satisfied?\n+            satisfied = [(m, rf) for m, rf in pending_add.items() if all((al, mn) not in pending_add for f, al, mn in rf)]\n+            if satisfied:\n+                (app_label, model_name), related_fields = sorted(satisfied)[0]\n+                model_state = self.to_state.models[app_label, model_name]\n+                self.add_to_migration(\n+                    app_label,\n+                    operations.CreateModel(\n+                        name = model_state.name,\n+                        fields = model_state.fields,\n+                        options = model_state.options,\n+                        bases = model_state.bases,\n+                    )\n+                )\n+                for field_name, other_app_label, other_model_name in related_fields:\n+                    self.add_dependency(app_label, other_app_label)\n+                del pending_add[app_label, model_name]\n+            # Ah well, we'll need to split one. Pick deterministically.\n+            else:\n+                (app_label, model_name), related_fields = sorted(pending_add.items())[0]\n+                model_state = self.to_state.models[app_label, model_name]\n+                # Work out the fields that need splitting out\n+                bad_fields = dict((f, (al, mn)) for f, al, mn in related_fields if (al, mn) in pending_add)\n+                # Create the model, without those\n+                self.add_to_migration(\n+                    app_label,\n+                    operations.CreateModel(\n+                        name = model_state.name,\n+                        fields = [(n, f) for n, f in model_state.fields if n not in bad_fields],\n+                        options = model_state.options,\n+                        bases = model_state.bases,\n+                    )\n+                )\n+                # Add the bad fields to be made in a phase 3\n+                for field_name, (other_app_label, other_model_name) in bad_fields.items():\n+                    pending_new_fks.append((app_label, model_name, field_name, other_app_label))\n+                del pending_add[app_label, model_name]\n+        # Phase 3 is adding the final set of FKs as separate new migrations\n+        for app_label, model_name, field_name, other_app_label in pending_new_fks:\n+            model_state = self.to_state.models[app_label, model_name]\n+            self.add_to_migration(\n+                app_label,\n+                operations.AddField(\n+                    model_name = model_name,\n+                    name = field_name,\n+                    field = model_state.get_field_by_name(field_name),\n+                ),\n+                new = True,\n+            )\n+            self.add_dependency(app_label, other_app_label)\n+        # Removing models\n+        removed_models = set(self.from_state.models.keys()) - set(self.to_state.models.keys())\n+        for app_label, model_name in removed_models:\n+            model_state = self.from_state.models[app_label, model_name]\n+            self.add_to_migration(\n+                app_label,\n+                operations.DeleteModel(\n+                    model_state.name,\n+                )\n+            )\n+        # Changes within models\n+        kept_models = set(self.from_state.models.keys()).intersection(self.to_state.models.keys())\n+        for app_label, model_name in kept_models:\n+            old_model_state = self.from_state.models[app_label, model_name]\n+            new_model_state = self.to_state.models[app_label, model_name]\n+            # New fields\n+            old_field_names = set([x for x, y in old_model_state.fields])\n+            new_field_names = set([x for x, y in new_model_state.fields])\n+            for field_name in new_field_names - old_field_names:\n+                field = new_model_state.get_field_by_name(field_name)\n+                # Scan to see if this is actually a rename!\n+                field_dec = field.deconstruct()[1:]\n+                found_rename = False\n+                for removed_field_name in (old_field_names - new_field_names):\n+                    if old_model_state.get_field_by_name(removed_field_name).deconstruct()[1:] == field_dec:\n+                        if self.questioner.ask_rename(model_name, removed_field_name, field_name, field):\n+                            self.add_to_migration(\n+                                app_label,\n+                                operations.RenameField(\n+                                    model_name = model_name,\n+                                    old_name = removed_field_name,\n+                                    new_name = field_name,\n+                                )\n+                            )\n+                            old_field_names.remove(removed_field_name)\n+                            new_field_names.remove(field_name)\n+                            found_rename = True\n+                            break\n+                if found_rename:\n+                    continue\n+                # You can't just add NOT NULL fields with no default\n+                if not field.null and not field.has_default():\n+                    field.default = self.questioner.ask_not_null_addition(field_name, model_name)\n+                self.add_to_migration(\n+                    app_label,\n+                    operations.AddField(\n+                        model_name = model_name,\n+                        name = field_name,\n+                        field = field,\n+                    )\n+                )\n+            # Old fields\n+            for field_name in old_field_names - new_field_names:\n+                self.add_to_migration(\n+                    app_label,\n+                    operations.RemoveField(\n+                        model_name = model_name,\n+                        name = field_name,\n+                    )\n+                )\n+            # The same fields\n+            for field_name in old_field_names.intersection(new_field_names):\n+                # Did the field change?\n+                old_field_dec = old_model_state.get_field_by_name(field_name).deconstruct()\n+                new_field_dec = new_model_state.get_field_by_name(field_name).deconstruct()\n+                if old_field_dec != new_field_dec:\n+                    self.add_to_migration(\n+                        app_label,\n+                        operations.AlterField(\n+                            model_name = model_name,\n+                            name = field_name,\n+                            field = new_model_state.get_field_by_name(field_name),\n+                        )\n+                    )\n+            # unique_together changes\n+            if old_model_state.options.get(\"unique_together\", set()) != new_model_state.options.get(\"unique_together\", set()):\n+                self.add_to_migration(\n+                    app_label,\n+                    operations.AlterUniqueTogether(\n+                        name = model_name,\n+                        unique_together = new_model_state.options.get(\"unique_together\", set()),\n+                    )\n+                )\n+        # Alright, now add internal dependencies\n+        for app_label, migrations in self.migrations.items():\n+            for m1, m2 in zip(migrations, migrations[1:]):\n+                m2.dependencies.append((app_label, m1.name))\n+        # Clean up dependencies\n+        for app_label, migrations in self.migrations.items():\n+            for migration in migrations:\n+                migration.dependencies = list(set(migration.dependencies))\n+        return self.migrations\n+\n+    def add_to_migration(self, app_label, operation, new=False):\n+        migrations = self.migrations.setdefault(app_label, [])\n+        if not migrations or new:\n+            subclass = type(\"Migration\", (Migration,), {\"operations\": [], \"dependencies\": []})\n+            instance = subclass(\"auto_%i\" % (len(migrations) + 1), app_label)\n+            migrations.append(instance)\n+        migrations[-1].operations.append(operation)\n+\n+    def add_dependency(self, app_label, other_app_label):\n+        \"\"\"\n+        Adds a dependency to app_label's newest migration on\n+        other_app_label's latest migration.\n+        \"\"\"\n+        if self.migrations.get(other_app_label, []):\n+            dependency = (other_app_label, self.migrations[other_app_label][-1].name)\n+        else:\n+            dependency = (other_app_label, \"__first__\")\n+        self.migrations[app_label][-1].dependencies.append(dependency)\n+\n+    def _arrange_for_graph(self, changes, graph):\n+        \"\"\"\n+        Takes in a result from changes() and a MigrationGraph,\n+        and fixes the names and dependencies of the changes so they\n+        extend the graph from the leaf nodes for each app.\n+        \"\"\"\n+        leaves = graph.leaf_nodes()\n+        name_map = {}\n+        for app_label, migrations in list(changes.items()):\n+            if not migrations:\n+                continue\n+            # Find the app label's current leaf node\n+            app_leaf = None\n+            for leaf in leaves:\n+                if leaf[0] == app_label:\n+                    app_leaf = leaf\n+                    break\n+            # Do they want an initial migration for this app?\n+            if app_leaf is None and not self.questioner.ask_initial(app_label):\n+                # They don't.\n+                for migration in migrations:\n+                    name_map[(app_label, migration.name)] = (app_label, \"__first__\")\n+                del changes[app_label]\n+            # Work out the next number in the sequence\n+            if app_leaf is None:\n+                next_number = 1\n+            else:\n+                next_number = (self.parse_number(app_leaf[1]) or 0) + 1\n+            # Name each migration\n+            for i, migration in enumerate(migrations):\n+                if i == 0 and app_leaf:\n+                    migration.dependencies.append(app_leaf)\n+                if i == 0 and not app_leaf:\n+                    new_name = \"0001_initial\"\n+                else:\n+                    new_name = \"%04i_%s\" % (next_number, self.suggest_name(migration.operations))\n+                name_map[(app_label, migration.name)] = (app_label, new_name)\n+                migration.name = new_name\n+        # Now fix dependencies\n+        for app_label, migrations in changes.items():\n+            for migration in migrations:\n+                migration.dependencies = [name_map.get(d, d) for d in migration.dependencies]\n+        return changes\n+\n+    def _trim_to_apps(self, changes, app_labels):\n+        \"\"\"\n+        Takes changes from arrange_for_graph and set of app labels and\n+        returns a modified set of changes which trims out as many migrations\n+        that are not in app_labels as possible.\n+        Note that some other migrations may still be present, as they may be\n+        required dependencies.\n+        \"\"\"\n+        # Gather other app dependencies in a first pass\n+        app_dependencies = {}\n+        for app_label, migrations in changes.items():\n+            for migration in migrations:\n+                for dep_app_label, name in migration.dependencies:\n+                    app_dependencies.setdefault(app_label, set()).add(dep_app_label)\n+        required_apps = set(app_labels)\n+        # Keep resolving till there's no change\n+        old_required_apps = None\n+        while old_required_apps != required_apps:\n+            old_required_apps = set(required_apps)\n+            for app_label in list(required_apps):\n+                required_apps.update(app_dependencies.get(app_label, set()))\n+        # Remove all migrations that aren't needed\n+        for app_label in list(changes.keys()):\n+            if app_label not in required_apps:\n+                del changes[app_label]\n+        return changes\n+\n+    @classmethod\n+    def suggest_name(cls, ops):\n+        \"\"\"\n+        Given a set of operations, suggests a name for the migration\n+        they might represent. Names not guaranteed to be unique; they\n+        must be prefixed by a number or date.\n+        \"\"\"\n+        if len(ops) == 1:\n+            if isinstance(ops[0], operations.CreateModel):\n+                return ops[0].name.lower()\n+            elif isinstance(ops[0], operations.DeleteModel):\n+                return \"delete_%s\" % ops[0].name.lower()\n+            elif isinstance(ops[0], operations.AddField):\n+                return \"%s_%s\" % (ops[0].model_name.lower(), ops[0].name.lower())\n+            elif isinstance(ops[0], operations.RemoveField):\n+                return \"remove_%s_%s\" % (ops[0].model_name.lower(), ops[0].name.lower())\n+        elif all(isinstance(o, operations.CreateModel) for o in ops):\n+            return \"_\".join(sorted(o.name.lower() for o in ops))\n+        return \"auto\"\n+\n+    @classmethod\n+    def parse_number(cls, name):\n+        \"\"\"\n+        Given a migration name, tries to extract a number from the\n+        beginning of it. If no number found, returns None.\n+        \"\"\"\n+        if re.match(r\"^\\d+_\", name):\n+            return int(name.split(\"_\")[0])\n+        return None\n+\n+\n+class MigrationQuestioner(object):\n+    \"\"\"\n+    Gives the autodetector responses to questions it might have.\n+    This base class has a built-in noninteractive mode, but the\n+    interactive subclass is what the command-line arguments will use.\n+    \"\"\"\n+\n+    def __init__(self, defaults=None):\n+        self.defaults = defaults or {}\n+\n+    def ask_initial(self, app_label):\n+        \"Should we create an initial migration for the app?\"\n+        return self.defaults.get(\"ask_initial\", False)\n+\n+    def ask_not_null_addition(self, field_name, model_name):\n+        \"Adding a NOT NULL field to a model\"\n+        # None means quit\n+        return None\n+\n+    def ask_rename(self, model_name, old_name, new_name, field_instance):\n+        \"Was this field really renamed?\"\n+        return self.defaults.get(\"ask_rename\", False)\n+\n+\n+class InteractiveMigrationQuestioner(MigrationQuestioner):\n+\n+    def __init__(self, specified_apps=set()):\n+        self.specified_apps = specified_apps\n+\n+    def _boolean_input(self, question, default=None):\n+        result = input(\"%s \" % question)\n+        if not result and default is not None:\n+            return default\n+        while len(result) < 1 or result[0].lower() not in \"yn\":\n+            result = input(\"Please answer yes or no: \")\n+        return result[0].lower() == \"y\"\n+\n+    def _choice_input(self, question, choices):\n+        print(question)\n+        for i, choice in enumerate(choices):\n+            print(\" %s) %s\" % (i + 1, choice))\n+        result = input(\"Select an option: \")\n+        while True:\n+            try:\n+                value = int(result)\n+                if 0 < value <= len(choices):\n+                    return value\n+            except ValueError:\n+                pass\n+            result = input(\"Please select a valid option: \")\n+\n+    def ask_initial(self, app_label):\n+        \"Should we create an initial migration for the app?\"\n+        # Don't ask for django.contrib apps\n+        app = cache.get_app(app_label)\n+        if app.__name__.startswith(\"django.contrib\"):\n+            return False\n+        # If it was specified on the command line, definitely true\n+        if app_label in self.specified_apps:\n+            return True\n+        # Now ask\n+        return self._boolean_input(\"Do you want to enable migrations for app '%s'? [y/N]\" % app_label, False)\n+\n+    def ask_not_null_addition(self, field_name, model_name):\n+        \"Adding a NOT NULL field to a model\"\n+        choice = self._choice_input(\n+            \"You are trying to add a non-nullable field '%s' to %s without a default;\\n\" % (field_name, model_name) +\n+            \"this is not possible. Please select a fix:\",\n+            [\n+                \"Provide a one-off default now (will be set on all existing rows)\",\n+                \"Quit, and let me add a default in models.py\",\n+            ]\n+        )\n+        if choice == 2:\n+            sys.exit(3)\n+        else:\n+            print(\"Please enter the default value now, as valid Python\")\n+            print(\"The datetime module is available, so you can do e.g. datetime.date.today()\")\n+            while True:\n+                code = input(\">>> \")\n+                if not code:\n+                    print(\"Please enter some code, or 'exit' (with no quotes) to exit.\")\n+                elif code == \"exit\":\n+                    sys.exit(1)\n+                else:\n+                    try:\n+                        return eval(code, {}, {\"datetime\": datetime_safe})\n+                    except (SyntaxError, NameError) as e:\n+                        print(\"Invalid input: %s\" % e)\n+                    else:\n+                        break\n+\n+    def ask_rename(self, model_name, old_name, new_name, field_instance):\n+        \"Was this field really renamed?\"\n+        return self._boolean_input(\"Did you rename %s.%s to %s.%s (a %s)? [y/N]\" % (model_name, old_name, model_name, new_name, field_instance.__class__.__name__), False)\ndiff --git a/django/db/migrations/executor.py b/django/db/migrations/executor.py\nnew file mode 100644\nindex 000000000000..fe0ac6b061d9\n--- /dev/null\n+++ b/django/db/migrations/executor.py\n@@ -0,0 +1,90 @@\n+from .loader import MigrationLoader\n+from .recorder import MigrationRecorder\n+\n+\n+class MigrationExecutor(object):\n+    \"\"\"\n+    End-to-end migration execution - loads migrations, and runs them\n+    up or down to a specified set of targets.\n+    \"\"\"\n+\n+    def __init__(self, connection, progress_callback=None):\n+        self.connection = connection\n+        self.loader = MigrationLoader(self.connection)\n+        self.loader.load_disk()\n+        self.recorder = MigrationRecorder(self.connection)\n+        self.progress_callback = progress_callback\n+\n+    def migration_plan(self, targets):\n+        \"\"\"\n+        Given a set of targets, returns a list of (Migration instance, backwards?).\n+        \"\"\"\n+        plan = []\n+        applied = self.recorder.applied_migrations()\n+        for target in targets:\n+            # If the target is (appname, None), that means unmigrate everything\n+            if target[1] is None:\n+                for root in self.loader.graph.root_nodes():\n+                    if root[0] == target[0]:\n+                        for migration in self.loader.graph.backwards_plan(root):\n+                            if migration in applied:\n+                                plan.append((self.loader.graph.nodes[migration], True))\n+                                applied.remove(migration)\n+            # If the migration is already applied, do backwards mode,\n+            # otherwise do forwards mode.\n+            elif target in applied:\n+                backwards_plan = self.loader.graph.backwards_plan(target)[:-1]\n+                # We only do this if the migration is not the most recent one\n+                # in its app - that is, another migration with the same app\n+                # label is in the backwards plan\n+                if any(node[0] == target[0] for node in backwards_plan):\n+                    for migration in backwards_plan:\n+                        if migration in applied:\n+                            plan.append((self.loader.graph.nodes[migration], True))\n+                            applied.remove(migration)\n+            else:\n+                for migration in self.loader.graph.forwards_plan(target):\n+                    if migration not in applied:\n+                        plan.append((self.loader.graph.nodes[migration], False))\n+                        applied.add(migration)\n+        return plan\n+\n+    def migrate(self, targets, plan=None, fake=False):\n+        \"\"\"\n+        Migrates the database up to the given targets.\n+        \"\"\"\n+        if plan is None:\n+            plan = self.migration_plan(targets)\n+        for migration, backwards in plan:\n+            if not backwards:\n+                self.apply_migration(migration, fake=fake)\n+            else:\n+                self.unapply_migration(migration, fake=fake)\n+\n+    def apply_migration(self, migration, fake=False):\n+        \"\"\"\n+        Runs a migration forwards.\n+        \"\"\"\n+        if self.progress_callback:\n+            self.progress_callback(\"apply_start\", migration)\n+        if not fake:\n+            with self.connection.schema_editor() as schema_editor:\n+                project_state = self.loader.graph.project_state((migration.app_label, migration.name), at_end=False)\n+                migration.apply(project_state, schema_editor)\n+        self.recorder.record_applied(migration.app_label, migration.name)\n+        if self.progress_callback:\n+            self.progress_callback(\"apply_success\", migration)\n+\n+    def unapply_migration(self, migration, fake=False):\n+        \"\"\"\n+        Runs a migration backwards.\n+        \"\"\"\n+        if self.progress_callback:\n+            self.progress_callback(\"unapply_start\", migration)\n+        if not fake:\n+            with self.connection.schema_editor() as schema_editor:\n+                project_state = self.loader.graph.project_state((migration.app_label, migration.name), at_end=False)\n+                migration.unapply(project_state, schema_editor)\n+        self.recorder.record_unapplied(migration.app_label, migration.name)\n+        if self.progress_callback:\n+            self.progress_callback(\"unapply_success\", migration)\ndiff --git a/django/db/migrations/graph.py b/django/db/migrations/graph.py\nnew file mode 100644\nindex 000000000000..fcd83913c8bf\n--- /dev/null\n+++ b/django/db/migrations/graph.py\n@@ -0,0 +1,152 @@\n+from django.utils.datastructures import OrderedSet\n+from django.db.migrations.state import ProjectState\n+\n+\n+class MigrationGraph(object):\n+    \"\"\"\n+    Represents the digraph of all migrations in a project.\n+\n+    Each migration is a node, and each dependency is an edge. There are\n+    no implicit dependencies between numbered migrations - the numbering is\n+    merely a convention to aid file listing. Every new numbered migration\n+    has a declared dependency to the previous number, meaning that VCS\n+    branch merges can be detected and resolved.\n+\n+    Migrations files can be marked as replacing another set of migrations -\n+    this is to support the \"squash\" feature. The graph handler isn't responsible\n+    for these; instead, the code to load them in here should examine the\n+    migration files and if the replaced migrations are all either unapplied\n+    or not present, it should ignore the replaced ones, load in just the\n+    replacing migration, and repoint any dependencies that pointed to the\n+    replaced migrations to point to the replacing one.\n+\n+    A node should be a tuple: (app_path, migration_name). The tree special-cases\n+    things within an app - namely, root nodes and leaf nodes ignore dependencies\n+    to other apps.\n+    \"\"\"\n+\n+    def __init__(self):\n+        self.nodes = {}\n+        self.dependencies = {}\n+        self.dependents = {}\n+\n+    def add_node(self, node, implementation):\n+        self.nodes[node] = implementation\n+\n+    def add_dependency(self, child, parent):\n+        if child not in self.nodes:\n+            raise KeyError(\"Dependency references nonexistent child node %r\" % (child,))\n+        if parent not in self.nodes:\n+            raise KeyError(\"Dependency references nonexistent parent node %r\" % (parent,))\n+        self.dependencies.setdefault(child, set()).add(parent)\n+        self.dependents.setdefault(parent, set()).add(child)\n+\n+    def forwards_plan(self, node):\n+        \"\"\"\n+        Given a node, returns a list of which previous nodes (dependencies)\n+        must be applied, ending with the node itself.\n+        This is the list you would follow if applying the migrations to\n+        a database.\n+        \"\"\"\n+        if node not in self.nodes:\n+            raise ValueError(\"Node %r not a valid node\" % (node, ))\n+        return self.dfs(node, lambda x: self.dependencies.get(x, set()))\n+\n+    def backwards_plan(self, node):\n+        \"\"\"\n+        Given a node, returns a list of which dependent nodes (dependencies)\n+        must be unapplied, ending with the node itself.\n+        This is the list you would follow if removing the migrations from\n+        a database.\n+        \"\"\"\n+        if node not in self.nodes:\n+            raise ValueError(\"Node %r not a valid node\" % (node, ))\n+        return self.dfs(node, lambda x: self.dependents.get(x, set()))\n+\n+    def root_nodes(self):\n+        \"\"\"\n+        Returns all root nodes - that is, nodes with no dependencies inside\n+        their app. These are the starting point for an app.\n+        \"\"\"\n+        roots = set()\n+        for node in self.nodes:\n+            if not any(key[0] == node[0] for key in self.dependencies.get(node, set())):\n+                roots.add(node)\n+        return roots\n+\n+    def leaf_nodes(self):\n+        \"\"\"\n+        Returns all leaf nodes - that is, nodes with no dependents in their app.\n+        These are the \"most current\" version of an app's schema.\n+        Having more than one per app is technically an error, but one that\n+        gets handled further up, in the interactive command - it's usually the\n+        result of a VCS merge and needs some user input.\n+        \"\"\"\n+        leaves = set()\n+        for node in self.nodes:\n+            if not any(key[0] == node[0] for key in self.dependents.get(node, set())):\n+                leaves.add(node)\n+        return leaves\n+\n+    def dfs(self, start, get_children):\n+        \"\"\"\n+        Dynamic programming based depth first search, for finding dependencies.\n+        \"\"\"\n+        cache = {}\n+        def _dfs(start, get_children, path):\n+            # If we already computed this, use that (dynamic programming)\n+            if (start, get_children) in cache:\n+                return cache[(start, get_children)]\n+            # If we've traversed here before, that's a circular dep\n+            if start in path:\n+                raise CircularDependencyError(path[path.index(start):] + [start])\n+            # Build our own results list, starting with us\n+            results = []\n+            results.append(start)\n+            # We need to add to results all the migrations this one depends on\n+            children = sorted(get_children(start))\n+            path.append(start)\n+            for n in children:\n+                results = _dfs(n, get_children, path) + results\n+            path.pop()\n+            # Use OrderedSet to ensure only one instance of each result\n+            results = list(OrderedSet(results))\n+            # Populate DP cache\n+            cache[(start, get_children)] = results\n+            # Done!\n+            return results\n+        return _dfs(start, get_children, [])\n+\n+    def __str__(self):\n+        return \"Graph: %s nodes, %s edges\" % (len(self.nodes), sum(len(x) for x in self.dependencies.values()))\n+\n+    def project_state(self, nodes=None, at_end=True):\n+        \"\"\"\n+        Given a migration node or nodes, returns a complete ProjectState for it.\n+        If at_end is False, returns the state before the migration has run.\n+        If nodes is not provided, returns the overall most current project state.\n+        \"\"\"\n+        if nodes is None:\n+            nodes = list(self.leaf_nodes())\n+        if len(nodes) == 0:\n+            return ProjectState()\n+        if not isinstance(nodes[0], tuple):\n+            nodes = [nodes]\n+        plan = []\n+        for node in nodes:\n+            for migration in self.forwards_plan(node):\n+                if migration not in plan:\n+                    if not at_end and migration in nodes:\n+                        continue\n+                    plan.append(migration)\n+        project_state = ProjectState()\n+        for node in plan:\n+            project_state = self.nodes[node].mutate_state(project_state)\n+        return project_state\n+\n+\n+class CircularDependencyError(Exception):\n+    \"\"\"\n+    Raised when there's an impossible-to-resolve circular dependency.\n+    \"\"\"\n+    pass\ndiff --git a/django/db/migrations/loader.py b/django/db/migrations/loader.py\nnew file mode 100644\nindex 000000000000..6ad69597879a\n--- /dev/null\n+++ b/django/db/migrations/loader.py\n@@ -0,0 +1,167 @@\n+import os\n+from importlib import import_module\n+from django.utils.functional import cached_property\n+from django.db.models.loading import cache\n+from django.db.migrations.recorder import MigrationRecorder\n+from django.db.migrations.graph import MigrationGraph\n+from django.conf import settings\n+\n+\n+class MigrationLoader(object):\n+    \"\"\"\n+    Loads migration files from disk, and their status from the database.\n+\n+    Migration files are expected to live in the \"migrations\" directory of\n+    an app. Their names are entirely unimportant from a code perspective,\n+    but will probably follow the 1234_name.py convention.\n+\n+    On initialisation, this class will scan those directories, and open and\n+    read the python files, looking for a class called Migration, which should\n+    inherit from django.db.migrations.Migration. See\n+    django.db.migrations.migration for what that looks like.\n+\n+    Some migrations will be marked as \"replacing\" another set of migrations.\n+    These are loaded into a separate set of migrations away from the main ones.\n+    If all the migrations they replace are either unapplied or missing from\n+    disk, then they are injected into the main set, replacing the named migrations.\n+    Any dependency pointers to the replaced migrations are re-pointed to the\n+    new migration.\n+\n+    This does mean that this class MUST also talk to the database as well as\n+    to disk, but this is probably fine. We're already not just operating\n+    in memory.\n+    \"\"\"\n+\n+    def __init__(self, connection):\n+        self.connection = connection\n+        self.disk_migrations = None\n+        self.applied_migrations = None\n+\n+    @classmethod\n+    def migrations_module(cls, app_label):\n+        if app_label in settings.MIGRATION_MODULES:\n+            return settings.MIGRATION_MODULES[app_label]\n+        app = cache.get_app(app_label)\n+        return \".\".join(app.__name__.split(\".\")[:-1] + [\"migrations\"])\n+\n+    def load_disk(self):\n+        \"\"\"\n+        Loads the migrations from all INSTALLED_APPS from disk.\n+        \"\"\"\n+        self.disk_migrations = {}\n+        self.unmigrated_apps = set()\n+        self.migrated_apps = set()\n+        for app in cache.get_apps():\n+            # Get the migrations module directory\n+            app_label = app.__name__.split(\".\")[-2]\n+            module_name = self.migrations_module(app_label)\n+            try:\n+                module = import_module(module_name)\n+            except ImportError as e:\n+                # I hate doing this, but I don't want to squash other import errors.\n+                # Might be better to try a directory check directly.\n+                if \"No module named\" in str(e) and \"migrations\" in str(e):\n+                    self.unmigrated_apps.add(app_label)\n+                    continue\n+            self.migrated_apps.add(app_label)\n+            directory = os.path.dirname(module.__file__)\n+            # Scan for .py[c|o] files\n+            migration_names = set()\n+            for name in os.listdir(directory):\n+                if name.endswith(\".py\") or name.endswith(\".pyc\") or name.endswith(\".pyo\"):\n+                    import_name = name.rsplit(\".\", 1)[0]\n+                    if import_name[0] not in \"_.~\":\n+                        migration_names.add(import_name)\n+            # Load them\n+            for migration_name in migration_names:\n+                migration_module = import_module(\"%s.%s\" % (module_name, migration_name))\n+                if not hasattr(migration_module, \"Migration\"):\n+                    raise BadMigrationError(\"Migration %s in app %s has no Migration class\" % (migration_name, app_label))\n+                self.disk_migrations[app_label, migration_name] = migration_module.Migration(migration_name, app_label)\n+\n+    def get_migration_by_prefix(self, app_label, name_prefix):\n+        \"Returns the migration(s) which match the given app label and name _prefix_\"\n+        # Make sure we have the disk data\n+        if self.disk_migrations is None:\n+            self.load_disk()\n+        # Do the search\n+        results = []\n+        for l, n in self.disk_migrations:\n+            if l == app_label and n.startswith(name_prefix):\n+                results.append((l, n))\n+        if len(results) > 1:\n+            raise AmbiguityError(\"There is more than one migration for '%s' with the prefix '%s'\" % (app_label, name_prefix))\n+        elif len(results) == 0:\n+            raise KeyError(\"There no migrations for '%s' with the prefix '%s'\" % (app_label, name_prefix))\n+        else:\n+            return self.disk_migrations[results[0]]\n+\n+    @cached_property\n+    def graph(self):\n+        \"\"\"\n+        Builds a migration dependency graph using both the disk and database.\n+        \"\"\"\n+        # Make sure we have the disk data\n+        if self.disk_migrations is None:\n+            self.load_disk()\n+        # And the database data\n+        if self.applied_migrations is None:\n+            recorder = MigrationRecorder(self.connection)\n+            self.applied_migrations = recorder.applied_migrations()\n+        # Do a first pass to separate out replacing and non-replacing migrations\n+        normal = {}\n+        replacing = {}\n+        for key, migration in self.disk_migrations.items():\n+            if migration.replaces:\n+                replacing[key] = migration\n+            else:\n+                normal[key] = migration\n+        # Calculate reverse dependencies - i.e., for each migration, what depends on it?\n+        # This is just for dependency re-pointing when applying replacements,\n+        # so we ignore run_before here.\n+        reverse_dependencies = {}\n+        for key, migration in normal.items():\n+            for parent in migration.dependencies:\n+                reverse_dependencies.setdefault(parent, set()).add(key)\n+        # Carry out replacements if we can - that is, if all replaced migrations\n+        # are either unapplied or missing.\n+        for key, migration in replacing.items():\n+            # Do the check\n+            can_replace = True\n+            for target in migration.replaces:\n+                if target in self.applied_migrations:\n+                    can_replace = False\n+                    break\n+            if not can_replace:\n+                continue\n+            # Alright, time to replace. Step through the replaced migrations\n+            # and remove, repointing dependencies if needs be.\n+            for replaced in migration.replaces:\n+                if replaced in normal:\n+                    del normal[replaced]\n+                for child_key in reverse_dependencies.get(replaced, set()):\n+                    normal[child_key].dependencies.remove(replaced)\n+                    normal[child_key].dependencies.append(key)\n+            normal[key] = migration\n+        # Finally, make a graph and load everything into it\n+        graph = MigrationGraph()\n+        for key, migration in normal.items():\n+            graph.add_node(key, migration)\n+        for key, migration in normal.items():\n+            for parent in migration.dependencies:\n+                graph.add_dependency(key, parent)\n+        return graph\n+\n+\n+class BadMigrationError(Exception):\n+    \"\"\"\n+    Raised when there's a bad migration (unreadable/bad format/etc.)\n+    \"\"\"\n+    pass\n+\n+\n+class AmbiguityError(Exception):\n+    \"\"\"\n+    Raised when more than one migration matches a name prefix\n+    \"\"\"\n+    pass\ndiff --git a/django/db/migrations/migration.py b/django/db/migrations/migration.py\nnew file mode 100644\nindex 000000000000..277c5faa3fce\n--- /dev/null\n+++ b/django/db/migrations/migration.py\n@@ -0,0 +1,101 @@\n+class Migration(object):\n+    \"\"\"\n+    The base class for all migrations.\n+\n+    Migration files will import this from django.db.migrations.Migration\n+    and subclass it as a class called Migration. It will have one or more\n+    of the following attributes:\n+\n+     - operations: A list of Operation instances, probably from django.db.migrations.operations\n+     - dependencies: A list of tuples of (app_path, migration_name)\n+     - run_before: A list of tuples of (app_path, migration_name)\n+     - replaces: A list of migration_names\n+\n+    Note that all migrations come out of migrations and into the Loader or\n+    Graph as instances, having been initialised with their app label and name.\n+    \"\"\"\n+\n+    # Operations to apply during this migration, in order.\n+    operations = []\n+\n+    # Other migrations that should be run before this migration.\n+    # Should be a list of (app, migration_name).\n+    dependencies = []\n+\n+    # Other migrations that should be run after this one (i.e. have\n+    # this migration added to their dependencies). Useful to make third-party\n+    # apps' migrations run after your AUTH_USER replacement, for example.\n+    run_before = []\n+\n+    # Migration names in this app that this migration replaces. If this is\n+    # non-empty, this migration will only be applied if all these migrations\n+    # are not applied.\n+    replaces = []\n+\n+    def __init__(self, name, app_label):\n+        self.name = name\n+        self.app_label = app_label\n+\n+    def __eq__(self, other):\n+        if not isinstance(other, Migration):\n+            return False\n+        return (self.name == other.name) and (self.app_label == other.app_label)\n+\n+    def __ne__(self, other):\n+        return not (self == other)\n+\n+    def __repr__(self):\n+        return \"<Migration %s.%s>\" % (self.app_label, self.name)\n+\n+    def __str__(self):\n+        return \"%s.%s\" % (self.app_label, self.name)\n+\n+    def __hash__(self):\n+        return hash(\"%s.%s\" % (self.app_label, self.name))\n+\n+    def mutate_state(self, project_state):\n+        \"\"\"\n+        Takes a ProjectState and returns a new one with the migration's\n+        operations applied to it.\n+        \"\"\"\n+        new_state = project_state.clone()\n+        for operation in self.operations:\n+            operation.state_forwards(self.app_label, new_state)\n+        return new_state\n+\n+    def apply(self, project_state, schema_editor):\n+        \"\"\"\n+        Takes a project_state representing all migrations prior to this one\n+        and a schema_editor for a live database and applies the migration\n+        in a forwards order.\n+\n+        Returns the resulting project state for efficient re-use by following\n+        Migrations.\n+        \"\"\"\n+        for operation in self.operations:\n+            # Get the state after the operation has run\n+            new_state = project_state.clone()\n+            operation.state_forwards(self.app_label, new_state)\n+            # Run the operation\n+            operation.database_forwards(self.app_label, schema_editor, project_state, new_state)\n+            # Switch states\n+            project_state = new_state\n+        return project_state\n+\n+    def unapply(self, project_state, schema_editor):\n+        \"\"\"\n+        Takes a project_state representing all migrations prior to this one\n+        and a schema_editor for a live database and applies the migration\n+        in a reverse order.\n+        \"\"\"\n+        # We need to pre-calculate the stack of project states\n+        to_run = []\n+        for operation in self.operations:\n+            new_state = project_state.clone()\n+            operation.state_forwards(self.app_label, new_state)\n+            to_run.append((operation, project_state, new_state))\n+            project_state = new_state\n+        # Now run them in reverse\n+        to_run.reverse()\n+        for operation, to_state, from_state in to_run:\n+            operation.database_backwards(self.app_label, schema_editor, from_state, to_state)\ndiff --git a/django/db/migrations/operations/__init__.py b/django/db/migrations/operations/__init__.py\nnew file mode 100644\nindex 000000000000..1240a5d1f5c2\n--- /dev/null\n+++ b/django/db/migrations/operations/__init__.py\n@@ -0,0 +1,2 @@\n+from .models import CreateModel, DeleteModel, AlterModelTable, AlterUniqueTogether, AlterIndexTogether\n+from .fields import AddField, RemoveField, AlterField, RenameField\ndiff --git a/django/db/migrations/operations/base.py b/django/db/migrations/operations/base.py\nnew file mode 100644\nindex 000000000000..dcdb1ad30bca\n--- /dev/null\n+++ b/django/db/migrations/operations/base.py\n@@ -0,0 +1,62 @@\n+class Operation(object):\n+    \"\"\"\n+    Base class for migration operations.\n+\n+    It's responsible for both mutating the in-memory model state\n+    (see db/migrations/state.py) to represent what it performs, as well\n+    as actually performing it against a live database.\n+\n+    Note that some operations won't modify memory state at all (e.g. data\n+    copying operations), and some will need their modifications to be\n+    optionally specified by the user (e.g. custom Python code snippets)\n+    \"\"\"\n+\n+    # If this migration can be run in reverse.\n+    # Some operations are impossible to reverse, like deleting data.\n+    reversible = True\n+\n+    def __new__(cls, *args, **kwargs):\n+        # We capture the arguments to make returning them trivial\n+        self = object.__new__(cls)\n+        self._constructor_args = (args, kwargs)\n+        return self\n+\n+    def deconstruct(self):\n+        \"\"\"\n+        Returns a 3-tuple of class import path (or just name if it lives\n+        under django.db.migrations), positional arguments, and keyword\n+        arguments.\n+        \"\"\"\n+        return (\n+            self.__class__.__name__,\n+            self._constructor_args[0],\n+            self._constructor_args[1],\n+        )\n+\n+    def state_forwards(self, app_label, state):\n+        \"\"\"\n+        Takes the state from the previous migration, and mutates it\n+        so that it matches what this migration would perform.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        \"\"\"\n+        Performs the mutation on the database schema in the normal\n+        (forwards) direction.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        \"\"\"\n+        Performs the mutation on the database schema in the reverse\n+        direction - e.g. if this were CreateModel, it would in fact\n+        drop the model's table.\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+    def describe(self):\n+        \"\"\"\n+        Outputs a brief summary of what the action does.\n+        \"\"\"\n+        return \"%s: %s\" % (self.__class__.__name__, self._constructor_args)\ndiff --git a/django/db/migrations/operations/fields.py b/django/db/migrations/operations/fields.py\nnew file mode 100644\nindex 000000000000..7c619d49ceeb\n--- /dev/null\n+++ b/django/db/migrations/operations/fields.py\n@@ -0,0 +1,132 @@\n+from django.db import router\n+from .base import Operation\n+\n+\n+class AddField(Operation):\n+    \"\"\"\n+    Adds a field to a model.\n+    \"\"\"\n+\n+    def __init__(self, model_name, name, field):\n+        self.model_name = model_name.lower()\n+        self.name = name\n+        self.field = field\n+\n+    def state_forwards(self, app_label, state):\n+        state.models[app_label, self.model_name.lower()].fields.append((self.name, self.field))\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        from_model = from_state.render().get_model(app_label, self.model_name)\n+        to_model = to_state.render().get_model(app_label, self.model_name)\n+        if router.allow_migrate(schema_editor.connection.alias, to_model):\n+            schema_editor.add_field(from_model, to_model._meta.get_field_by_name(self.name)[0])\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        from_model = from_state.render().get_model(app_label, self.model_name)\n+        if router.allow_migrate(schema_editor.connection.alias, from_model):\n+            schema_editor.remove_field(from_model, from_model._meta.get_field_by_name(self.name)[0])\n+\n+    def describe(self):\n+        return \"Add field %s to %s\" % (self.name, self.model_name)\n+\n+\n+class RemoveField(Operation):\n+    \"\"\"\n+    Removes a field from a model.\n+    \"\"\"\n+\n+    def __init__(self, model_name, name):\n+        self.model_name = model_name.lower()\n+        self.name = name\n+\n+    def state_forwards(self, app_label, state):\n+        new_fields = []\n+        for name, instance in state.models[app_label, self.model_name.lower()].fields:\n+            if name != self.name:\n+                new_fields.append((name, instance))\n+        state.models[app_label, self.model_name.lower()].fields = new_fields\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        from_model = from_state.render().get_model(app_label, self.model_name)\n+        if router.allow_migrate(schema_editor.connection.alias, from_model):\n+            schema_editor.remove_field(from_model, from_model._meta.get_field_by_name(self.name)[0])\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        from_model = from_state.render().get_model(app_label, self.model_name)\n+        to_model = to_state.render().get_model(app_label, self.model_name)\n+        if router.allow_migrate(schema_editor.connection.alias, to_model):\n+            schema_editor.add_field(from_model, to_model._meta.get_field_by_name(self.name)[0])\n+\n+    def describe(self):\n+        return \"Remove field %s from %s\" % (self.name, self.model_name)\n+\n+\n+class AlterField(Operation):\n+    \"\"\"\n+    Alters a field's database column (e.g. null, max_length) to the provided new field\n+    \"\"\"\n+\n+    def __init__(self, model_name, name, field):\n+        self.model_name = model_name.lower()\n+        self.name = name\n+        self.field = field\n+\n+    def state_forwards(self, app_label, state):\n+        state.models[app_label, self.model_name.lower()].fields = [\n+            (n, self.field if n == self.name else f) for n, f in state.models[app_label, self.model_name.lower()].fields\n+        ]\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        from_model = from_state.render().get_model(app_label, self.model_name)\n+        to_model = to_state.render().get_model(app_label, self.model_name)\n+        if router.allow_migrate(schema_editor.connection.alias, to_model):\n+            schema_editor.alter_field(\n+                from_model,\n+                from_model._meta.get_field_by_name(self.name)[0],\n+                to_model._meta.get_field_by_name(self.name)[0],\n+            )\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        self.database_forwards(app_label, schema_editor, from_state, to_state)\n+\n+    def describe(self):\n+        return \"Alter field %s on %s\" % (self.name, self.model_name)\n+\n+\n+class RenameField(Operation):\n+    \"\"\"\n+    Renames a field on the model. Might affect db_column too.\n+    \"\"\"\n+\n+    def __init__(self, model_name, old_name, new_name):\n+        self.model_name = model_name.lower()\n+        self.old_name = old_name\n+        self.new_name = new_name\n+\n+    def state_forwards(self, app_label, state):\n+        state.models[app_label, self.model_name.lower()].fields = [\n+            (self.new_name if n == self.old_name else n, f) for n, f in state.models[app_label, self.model_name.lower()].fields\n+        ]\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        from_model = from_state.render().get_model(app_label, self.model_name)\n+        to_model = to_state.render().get_model(app_label, self.model_name)\n+        if router.allow_migrate(schema_editor.connection.alias, to_model):\n+            schema_editor.alter_field(\n+                from_model,\n+                from_model._meta.get_field_by_name(self.old_name)[0],\n+                to_model._meta.get_field_by_name(self.new_name)[0],\n+            )\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        from_model = from_state.render().get_model(app_label, self.model_name)\n+        to_model = to_state.render().get_model(app_label, self.model_name)\n+        if router.allow_migrate(schema_editor.connection.alias, to_model):\n+            schema_editor.alter_field(\n+                from_model,\n+                from_model._meta.get_field_by_name(self.new_name)[0],\n+                to_model._meta.get_field_by_name(self.old_name)[0],\n+            )\n+\n+    def describe(self):\n+        return \"Rename field %s on %s to %s\" % (self.old_name, self.model_name, self.new_name)\ndiff --git a/django/db/migrations/operations/models.py b/django/db/migrations/operations/models.py\nnew file mode 100644\nindex 000000000000..406efa6ef115\n--- /dev/null\n+++ b/django/db/migrations/operations/models.py\n@@ -0,0 +1,157 @@\n+from .base import Operation\n+from django.db import models, router\n+from django.db.migrations.state import ModelState\n+\n+\n+class CreateModel(Operation):\n+    \"\"\"\n+    Create a model's table.\n+    \"\"\"\n+\n+    def __init__(self, name, fields, options=None, bases=None):\n+        self.name = name\n+        self.fields = fields\n+        self.options = options or {}\n+        self.bases = bases or (models.Model,)\n+\n+    def state_forwards(self, app_label, state):\n+        state.models[app_label, self.name.lower()] = ModelState(app_label, self.name, self.fields, self.options, self.bases)\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        app_cache = to_state.render()\n+        model = app_cache.get_model(app_label, self.name)\n+        if router.allow_migrate(schema_editor.connection.alias, model):\n+            schema_editor.create_model(model)\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        app_cache = from_state.render()\n+        model = app_cache.get_model(app_label, self.name)\n+        if router.allow_migrate(schema_editor.connection.alias, model):\n+            schema_editor.delete_model(model)\n+\n+    def describe(self):\n+        return \"Create model %s\" % (self.name, )\n+\n+\n+class DeleteModel(Operation):\n+    \"\"\"\n+    Drops a model's table.\n+    \"\"\"\n+\n+    def __init__(self, name):\n+        self.name = name\n+\n+    def state_forwards(self, app_label, state):\n+        del state.models[app_label, self.name.lower()]\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        app_cache = from_state.render()\n+        model = app_cache.get_model(app_label, self.name)\n+        if router.allow_migrate(schema_editor.connection.alias, model):\n+            schema_editor.delete_model(model)\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        app_cache = to_state.render()\n+        model = app_cache.get_model(app_label, self.name)\n+        if router.allow_migrate(schema_editor.connection.alias, model):\n+            schema_editor.create_model(model)\n+\n+    def describe(self):\n+        return \"Delete model %s\" % (self.name, )\n+\n+\n+class AlterModelTable(Operation):\n+    \"\"\"\n+    Renames a model's table\n+    \"\"\"\n+\n+    def __init__(self, name, table):\n+        self.name = name\n+        self.table = table\n+\n+    def state_forwards(self, app_label, state):\n+        state.models[app_label, self.name.lower()].options[\"db_table\"] = self.table\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        old_app_cache = from_state.render()\n+        new_app_cache = to_state.render()\n+        old_model = old_app_cache.get_model(app_label, self.name)\n+        new_model = new_app_cache.get_model(app_label, self.name)\n+        if router.allow_migrate(schema_editor.connection.alias, new_model):\n+            schema_editor.alter_db_table(\n+                new_model,\n+                old_model._meta.db_table,\n+                new_model._meta.db_table,\n+            )\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        return self.database_forwards(app_label, schema_editor, from_state, to_state)\n+\n+    def describe(self):\n+        return \"Rename table for %s to %s\" % (self.name, self.table)\n+\n+\n+class AlterUniqueTogether(Operation):\n+    \"\"\"\n+    Changes the value of index_together to the target one.\n+    Input value of unique_together must be a set of tuples.\n+    \"\"\"\n+\n+    def __init__(self, name, unique_together):\n+        self.name = name\n+        self.unique_together = set(tuple(cons) for cons in unique_together)\n+\n+    def state_forwards(self, app_label, state):\n+        model_state = state.models[app_label, self.name.lower()]\n+        model_state.options[\"unique_together\"] = self.unique_together\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        old_app_cache = from_state.render()\n+        new_app_cache = to_state.render()\n+        old_model = old_app_cache.get_model(app_label, self.name)\n+        new_model = new_app_cache.get_model(app_label, self.name)\n+        if router.allow_migrate(schema_editor.connection.alias, new_model):\n+            schema_editor.alter_unique_together(\n+                new_model,\n+                getattr(old_model._meta, \"unique_together\", set()),\n+                getattr(new_model._meta, \"unique_together\", set()),\n+            )\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        return self.database_forwards(app_label, schema_editor, from_state, to_state)\n+\n+    def describe(self):\n+        return \"Alter unique_together for %s (%s constraints)\" % (self.name, len(self.unique_together))\n+\n+\n+class AlterIndexTogether(Operation):\n+    \"\"\"\n+    Changes the value of index_together to the target one.\n+    Input value of index_together must be a set of tuples.\n+    \"\"\"\n+\n+    def __init__(self, name, index_together):\n+        self.name = name\n+        self.index_together = set(tuple(cons) for cons in index_together)\n+\n+    def state_forwards(self, app_label, state):\n+        model_state = state.models[app_label, self.name.lower()]\n+        model_state.options[\"index_together\"] = self.index_together\n+\n+    def database_forwards(self, app_label, schema_editor, from_state, to_state):\n+        old_app_cache = from_state.render()\n+        new_app_cache = to_state.render()\n+        old_model = old_app_cache.get_model(app_label, self.name)\n+        new_model = new_app_cache.get_model(app_label, self.name)\n+        if router.allow_migrate(schema_editor.connection.alias, new_model):\n+            schema_editor.alter_index_together(\n+                new_model,\n+                getattr(old_model._meta, \"index_together\", set()),\n+                getattr(new_model._meta, \"index_together\", set()),\n+            )\n+\n+    def database_backwards(self, app_label, schema_editor, from_state, to_state):\n+        return self.database_forwards(app_label, schema_editor, from_state, to_state)\n+\n+    def describe(self):\n+        return \"Alter index_together for %s (%s constraints)\" % (self.name, len(self.index_together))\ndiff --git a/django/db/migrations/recorder.py b/django/db/migrations/recorder.py\nnew file mode 100644\nindex 000000000000..c66d122068f4\n--- /dev/null\n+++ b/django/db/migrations/recorder.py\n@@ -0,0 +1,69 @@\n+from django.db import models\n+from django.db.models.loading import BaseAppCache\n+from django.utils.timezone import now\n+\n+\n+class MigrationRecorder(object):\n+    \"\"\"\n+    Deals with storing migration records in the database.\n+\n+    Because this table is actually itself used for dealing with model\n+    creation, it's the one thing we can't do normally via syncdb or migrations.\n+    We manually handle table creation/schema updating (using schema backend)\n+    and then have a floating model to do queries with.\n+\n+    If a migration is unapplied its row is removed from the table. Having\n+    a row in the table always means a migration is applied.\n+    \"\"\"\n+\n+    class Migration(models.Model):\n+        app = models.CharField(max_length=255)\n+        name = models.CharField(max_length=255)\n+        applied = models.DateTimeField(default=now)\n+        class Meta:\n+            app_cache = BaseAppCache()\n+            app_label = \"migrations\"\n+            db_table = \"django_migrations\"\n+\n+    def __init__(self, connection):\n+        self.connection = connection\n+\n+    def ensure_schema(self):\n+        \"\"\"\n+        Ensures the table exists and has the correct schema.\n+        \"\"\"\n+        # If the table's there, that's fine - we've never changed its schema\n+        # in the codebase.\n+        if self.Migration._meta.db_table in self.connection.introspection.get_table_list(self.connection.cursor()):\n+            return\n+        # Make the table\n+        with self.connection.schema_editor() as editor:\n+            editor.create_model(self.Migration)\n+\n+    def applied_migrations(self):\n+        \"\"\"\n+        Returns a set of (app, name) of applied migrations.\n+        \"\"\"\n+        self.ensure_schema()\n+        return set(tuple(x) for x in self.Migration.objects.values_list(\"app\", \"name\"))\n+\n+    def record_applied(self, app, name):\n+        \"\"\"\n+        Records that a migration was applied.\n+        \"\"\"\n+        self.ensure_schema()\n+        self.Migration.objects.create(app=app, name=name)\n+\n+    def record_unapplied(self, app, name):\n+        \"\"\"\n+        Records that a migration was unapplied.\n+        \"\"\"\n+        self.ensure_schema()\n+        self.Migration.objects.filter(app=app, name=name).delete()\n+\n+    @classmethod\n+    def flush(cls):\n+        \"\"\"\n+        Deletes all migration records. Useful if you're testing migrations.\n+        \"\"\"\n+        cls.Migration.objects.all().delete()\ndiff --git a/django/db/migrations/state.py b/django/db/migrations/state.py\nnew file mode 100644\nindex 000000000000..8f0078d7316e\n--- /dev/null\n+++ b/django/db/migrations/state.py\n@@ -0,0 +1,142 @@\n+from django.db import models\n+from django.db.models.loading import BaseAppCache\n+from django.db.models.options import DEFAULT_NAMES\n+from django.utils.module_loading import import_by_path\n+\n+\n+class ProjectState(object):\n+    \"\"\"\n+    Represents the entire project's overall state.\n+    This is the item that is passed around - we do it here rather than at the\n+    app level so that cross-app FKs/etc. resolve properly.\n+    \"\"\"\n+\n+    def __init__(self, models=None):\n+        self.models = models or {}\n+        self.app_cache = None\n+\n+    def add_model_state(self, model_state):\n+        self.models[(model_state.app_label, model_state.name.lower())] = model_state\n+\n+    def clone(self):\n+        \"Returns an exact copy of this ProjectState\"\n+        return ProjectState(\n+            models = dict((k, v.clone()) for k, v in self.models.items())\n+        )\n+\n+    def render(self):\n+        \"Turns the project state into actual models in a new AppCache\"\n+        if self.app_cache is None:\n+            self.app_cache = BaseAppCache()\n+            for model in self.models.values():\n+                model.render(self.app_cache)\n+        return self.app_cache\n+\n+    @classmethod\n+    def from_app_cache(cls, app_cache):\n+        \"Takes in an AppCache and returns a ProjectState matching it\"\n+        models = {}\n+        for model in app_cache.get_models():\n+            model_state = ModelState.from_model(model)\n+            models[(model_state.app_label, model_state.name.lower())] = model_state\n+        return cls(models)\n+\n+\n+class ModelState(object):\n+    \"\"\"\n+    Represents a Django Model. We don't use the actual Model class\n+    as it's not designed to have its options changed - instead, we\n+    mutate this one and then render it into a Model as required.\n+\n+    Note that while you are allowed to mutate .fields, you are not allowed\n+    to mutate the Field instances inside there themselves - you must instead\n+    assign new ones, as these are not detached during a clone.\n+    \"\"\"\n+\n+    def __init__(self, app_label, name, fields, options=None, bases=None):\n+        self.app_label = app_label\n+        self.name = name\n+        self.fields = fields\n+        self.options = options or {}\n+        self.bases = bases or (models.Model, )\n+        # Sanity-check that fields is NOT a dict. It must be ordered.\n+        if isinstance(self.fields, dict):\n+            raise ValueError(\"ModelState.fields cannot be a dict - it must be a list of 2-tuples.\")\n+\n+    @classmethod\n+    def from_model(cls, model):\n+        \"\"\"\n+        Feed me a model, get a ModelState representing it out.\n+        \"\"\"\n+        # Deconstruct the fields\n+        fields = []\n+        for field in model._meta.fields:\n+            name, path, args, kwargs = field.deconstruct()\n+            field_class = import_by_path(path)\n+            fields.append((name, field_class(*args, **kwargs)))\n+        # Extract the options\n+        options = {}\n+        for name in DEFAULT_NAMES:\n+            # Ignore some special options\n+            if name in [\"app_cache\", \"app_label\"]:\n+                continue\n+            elif name in model._meta.original_attrs:\n+                if name == \"unique_together\":\n+                    options[name] = set(model._meta.original_attrs[\"unique_together\"])\n+                else:\n+                    options[name] = model._meta.original_attrs[name]\n+        # Make our record\n+        bases = tuple(model for model in model.__bases__ if (not hasattr(model, \"_meta\") or not model._meta.abstract))\n+        if not bases:\n+            bases = (models.Model, )\n+        return cls(\n+            model._meta.app_label,\n+            model._meta.object_name,\n+            fields,\n+            options,\n+            bases,\n+        )\n+\n+    def clone(self):\n+        \"Returns an exact copy of this ModelState\"\n+        # We deep-clone the fields using deconstruction\n+        fields = []\n+        for name, field in self.fields:\n+            _, path, args, kwargs = field.deconstruct()\n+            field_class = import_by_path(path)\n+            fields.append((name, field_class(*args, **kwargs)))\n+        # Now make a copy\n+        return self.__class__(\n+            app_label = self.app_label,\n+            name = self.name,\n+            fields = fields,\n+            options = dict(self.options),\n+            bases = self.bases,\n+        )\n+\n+    def render(self, app_cache):\n+        \"Creates a Model object from our current state into the given app_cache\"\n+        # First, make a Meta object\n+        meta_contents = {'app_label': self.app_label, \"app_cache\": app_cache}\n+        meta_contents.update(self.options)\n+        if \"unique_together\" in meta_contents:\n+            meta_contents[\"unique_together\"] = list(meta_contents[\"unique_together\"])\n+        meta = type(\"Meta\", tuple(), meta_contents)\n+        # Then, work out our bases\n+        # TODO: Use the actual bases\n+        # Turn fields into a dict for the body, add other bits\n+        body = dict(self.fields)\n+        body['Meta'] = meta\n+        body['__module__'] = \"__fake__\"\n+        # Then, make a Model object\n+        return type(\n+            self.name,\n+            tuple(self.bases),\n+            body,\n+        )\n+\n+    def get_field_by_name(self, name):\n+        for fname, field in self.fields:\n+            if fname == name:\n+                return field\n+        raise ValueError(\"No field called %s on model %s\" % (name, self.name))\ndiff --git a/django/db/migrations/writer.py b/django/db/migrations/writer.py\nnew file mode 100644\nindex 000000000000..753aeacd0409\n--- /dev/null\n+++ b/django/db/migrations/writer.py\n@@ -0,0 +1,180 @@\n+from __future__ import unicode_literals\n+import datetime\n+import types\n+import os\n+from importlib import import_module\n+from django.utils import six\n+from django.db import models\n+from django.db.models.loading import cache\n+from django.db.migrations.loader import MigrationLoader\n+\n+\n+class MigrationWriter(object):\n+    \"\"\"\n+    Takes a Migration instance and is able to produce the contents\n+    of the migration file from it.\n+    \"\"\"\n+\n+    def __init__(self, migration):\n+        self.migration = migration\n+\n+    def as_string(self):\n+        \"\"\"\n+        Returns a string of the file contents.\n+        \"\"\"\n+        items = {\n+            \"dependencies\": repr(self.migration.dependencies),\n+        }\n+        imports = set()\n+        # Deconstruct operations\n+        operation_strings = []\n+        for operation in self.migration.operations:\n+            name, args, kwargs = operation.deconstruct()\n+            arg_strings = []\n+            for arg in args:\n+                arg_string, arg_imports = self.serialize(arg)\n+                arg_strings.append(arg_string)\n+                imports.update(arg_imports)\n+            for kw, arg in kwargs.items():\n+                arg_string, arg_imports = self.serialize(arg)\n+                imports.update(arg_imports)\n+                arg_strings.append(\"%s = %s\" % (kw, arg_string))\n+            operation_strings.append(\"migrations.%s(%s\\n        )\" % (name, \"\".join(\"\\n            %s,\" % arg for arg in arg_strings)))\n+        items[\"operations\"] = \"[%s\\n    ]\" % \"\".join(\"\\n        %s,\" % s for s in operation_strings)\n+        # Format imports nicely\n+        imports.discard(\"from django.db import models\")\n+        if not imports:\n+            items[\"imports\"] = \"\"\n+        else:\n+            items[\"imports\"] = \"\\n\".join(imports) + \"\\n\"\n+        return (MIGRATION_TEMPLATE % items).encode(\"utf8\")\n+\n+    @property\n+    def filename(self):\n+        return \"%s.py\" % self.migration.name\n+\n+    @property\n+    def path(self):\n+        migrations_module_name = MigrationLoader.migrations_module(self.migration.app_label)\n+        app_module = cache.get_app(self.migration.app_label)\n+        # See if we can import the migrations module directly\n+        try:\n+            migrations_module = import_module(migrations_module_name)\n+            basedir = os.path.dirname(migrations_module.__file__)\n+        except ImportError:\n+            # Alright, see if it's a direct submodule of the app\n+            oneup = \".\".join(migrations_module_name.split(\".\")[:-1])\n+            app_oneup = \".\".join(app_module.__name__.split(\".\")[:-1])\n+            if oneup == app_oneup:\n+                basedir = os.path.join(os.path.dirname(app_module.__file__), migrations_module_name.split(\".\")[-1])\n+            else:\n+                raise ImportError(\"Cannot open migrations module %s for app %s\" % (migrations_module_name, self.migration.app_label))\n+        return os.path.join(basedir, self.filename)\n+\n+    @classmethod\n+    def serialize(cls, value):\n+        \"\"\"\n+        Serializes the value to a string that's parsable by Python, along\n+        with any needed imports to make that string work.\n+        More advanced than repr() as it can encode things\n+        like datetime.datetime.now.\n+        \"\"\"\n+        # Sequences\n+        if isinstance(value, (list, set, tuple)):\n+            imports = set()\n+            strings = []\n+            for item in value:\n+                item_string, item_imports = cls.serialize(item)\n+                imports.update(item_imports)\n+                strings.append(item_string)\n+            if isinstance(value, set):\n+                format = \"set([%s])\"\n+            elif isinstance(value, tuple):\n+                format = \"(%s,)\"\n+            else:\n+                format = \"[%s]\"\n+            return format % (\", \".join(strings)), imports\n+        # Dictionaries\n+        elif isinstance(value, dict):\n+            imports = set()\n+            strings = []\n+            for k, v in value.items():\n+                k_string, k_imports = cls.serialize(k)\n+                v_string, v_imports = cls.serialize(v)\n+                imports.update(k_imports)\n+                imports.update(v_imports)\n+                strings.append((k_string, v_string))\n+            return \"{%s}\" % (\", \".join([\"%s: %s\" % (k, v) for k, v in strings])), imports\n+        # Datetimes\n+        elif isinstance(value, (datetime.datetime, datetime.date)):\n+            return repr(value), set([\"import datetime\"])\n+        # Simple types\n+        elif isinstance(value, six.integer_types + (float, six.binary_type, six.text_type, bool, type(None))):\n+            return repr(value), set()\n+        # Django fields\n+        elif isinstance(value, models.Field):\n+            attr_name, path, args, kwargs = value.deconstruct()\n+            module, name = path.rsplit(\".\", 1)\n+            if module == \"django.db.models\":\n+                imports = set([\"from django.db import models\"])\n+                name = \"models.%s\" % name\n+            else:\n+                imports = set([\"import %s\" % module])\n+                name = path\n+            arg_strings = []\n+            for arg in args:\n+                arg_string, arg_imports = cls.serialize(arg)\n+                arg_strings.append(arg_string)\n+                imports.update(arg_imports)\n+            for kw, arg in kwargs.items():\n+                arg_string, arg_imports = cls.serialize(arg)\n+                imports.update(arg_imports)\n+                arg_strings.append(\"%s=%s\" % (kw, arg_string))\n+            return \"%s(%s)\" % (name, \", \".join(arg_strings)), imports\n+        # Functions\n+        elif isinstance(value, (types.FunctionType, types.BuiltinFunctionType)):\n+            # Special-cases, as these don't have im_class\n+            special_cases = [\n+                (datetime.datetime.now, \"datetime.datetime.now\", [\"import datetime\"]),\n+                (datetime.datetime.utcnow, \"datetime.datetime.utcnow\", [\"import datetime\"]),\n+                (datetime.date.today, \"datetime.date.today\", [\"import datetime\"]),\n+            ]\n+            for func, string, imports in special_cases:\n+                if func == value:  # For some reason \"utcnow is not utcnow\"\n+                    return string, set(imports)\n+            # Method?\n+            if hasattr(value, \"im_class\"):\n+                klass = value.im_class\n+                module = klass.__module__\n+                return \"%s.%s.%s\" % (module, klass.__name__, value.__name__), set([\"import %s\" % module])\n+            else:\n+                module = value.__module__\n+                if module is None:\n+                    raise ValueError(\"Cannot serialize function %r: No module\" % value)\n+                return \"%s.%s\" % (module, value.__name__), set([\"import %s\" % module])\n+        # Classes\n+        elif isinstance(value, type):\n+            special_cases = [\n+                (models.Model, \"models.Model\", []),\n+            ]\n+            for case, string, imports in special_cases:\n+                if case is value:\n+                    return string, set(imports)\n+            if hasattr(value, \"__module__\"):\n+                module = value.__module__\n+                return \"%s.%s\" % (module, value.__name__), set([\"import %s\" % module])\n+        # Uh oh.\n+        else:\n+            raise ValueError(\"Cannot serialize: %r\" % value)\n+\n+\n+MIGRATION_TEMPLATE = \"\"\"# encoding: utf8\n+from django.db import models, migrations\n+%(imports)s\n+\n+class Migration(migrations.Migration):\n+\n+    dependencies = %(dependencies)s\n+\n+    operations = %(operations)s\n+\"\"\"\ndiff --git a/django/db/models/base.py b/django/db/models/base.py\nindex a5b0f188b4d8..d63017c4386c 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -146,7 +146,7 @@ def __new__(cls, name, bases, attrs):\n                 new_class._base_manager = new_class._base_manager._copy_to_model(new_class)\n \n         # Bail out early if we have already created this class.\n-        m = get_model(new_class._meta.app_label, name,\n+        m = new_class._meta.app_cache.get_model(new_class._meta.app_label, name,\n                       seed_cache=False, only_installed=False)\n         if m is not None:\n             return m\n@@ -264,13 +264,13 @@ def __new__(cls, name, bases, attrs):\n             return new_class\n \n         new_class._prepare()\n-        register_models(new_class._meta.app_label, new_class)\n-\n+        \n+        new_class._meta.app_cache.register_models(new_class._meta.app_label, new_class)\n         # Because of the way imports happen (recursively), we may or may not be\n         # the first time this model tries to register with the framework. There\n         # should only be one class for each model, so we always return the\n         # registered version.\n-        return get_model(new_class._meta.app_label, name,\n+        return new_class._meta.app_cache.get_model(new_class._meta.app_label, name,\n                          seed_cache=False, only_installed=False)\n \n     def copy_managers(cls, base_managers):\ndiff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex fb4cbbb11a0b..4135c60ad37c 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -368,12 +368,32 @@ def db_type(self, connection):\n         # mapped to one of the built-in Django field types. In this case, you\n         # can implement db_type() instead of get_internal_type() to specify\n         # exactly which wacky database column type you want to use.\n+        params = self.db_parameters(connection)\n+        if params['type']:\n+            if params['check']:\n+                return \"%s CHECK (%s)\" % (params['type'], params['check'])\n+            else:\n+                return params['type']\n+        return None\n+\n+    def db_parameters(self, connection):\n+        \"\"\"\n+        Replacement for db_type, providing a range of different return\n+        values (type, checks)\n+        \"\"\"\n         data = DictWrapper(self.__dict__, connection.ops.quote_name, \"qn_\")\n         try:\n-            return (connection.creation.data_types[self.get_internal_type()]\n-                    % data)\n+            type_string = connection.creation.data_types[self.get_internal_type()] % data\n         except KeyError:\n-            return None\n+            type_string = None\n+        try:\n+            check_string = connection.creation.data_type_check_constraints[self.get_internal_type()] % data\n+        except KeyError:\n+            check_string = None\n+        return {\n+            \"type\": type_string,\n+            \"check\": check_string,\n+        }\n \n     @property\n     def unique(self):\ndiff --git a/django/db/models/fields/related.py b/django/db/models/fields/related.py\nindex 78569042a509..4ff93e701f0f 100644\n--- a/django/db/models/fields/related.py\n+++ b/django/db/models/fields/related.py\n@@ -2,7 +2,7 @@\n \n from django.db import connection, connections, router\n from django.db.backends import util\n-from django.db.models import signals, get_model\n+from django.db.models import signals\n from django.db.models.fields import (AutoField, Field, IntegerField,\n     PositiveIntegerField, PositiveSmallIntegerField, FieldDoesNotExist)\n from django.db.models.related import RelatedObject, PathInfo\n@@ -18,8 +18,6 @@\n \n RECURSIVE_RELATIONSHIP_CONSTANT = 'self'\n \n-pending_lookups = {}\n-\n \n def add_lazy_relation(cls, field, relation, operation):\n     \"\"\"\n@@ -70,14 +68,14 @@ class MyModel(Model):\n     # string right away. If get_model returns None, it means that the related\n     # model isn't loaded yet, so we need to pend the relation until the class\n     # is prepared.\n-    model = get_model(app_label, model_name,\n+    model = cls._meta.app_cache.get_model(app_label, model_name,\n                       seed_cache=False, only_installed=False)\n     if model:\n         operation(field, model, cls)\n     else:\n         key = (app_label, model_name)\n         value = (cls, field, operation)\n-        pending_lookups.setdefault(key, []).append(value)\n+        cls._meta.app_cache.pending_lookups.setdefault(key, []).append(value)\n \n \n def do_pending_lookups(sender, **kwargs):\n@@ -85,7 +83,7 @@ def do_pending_lookups(sender, **kwargs):\n     Handle any pending relations to the sending model. Sent from class_prepared.\n     \"\"\"\n     key = (sender._meta.app_label, sender.__name__)\n-    for cls, field, operation in pending_lookups.pop(key, []):\n+    for cls, field, operation in sender._meta.app_cache.pending_lookups.pop(key, []):\n         operation(field, sender, cls)\n \n signals.class_prepared.connect(do_pending_lookups)\n@@ -941,6 +939,8 @@ def __init__(self, to, from_fields, to_fields, **kwargs):\n     def resolve_related_fields(self):\n         if len(self.from_fields) < 1 or len(self.from_fields) != len(self.to_fields):\n             raise ValueError('Foreign Object from and to fields must be the same non-zero length')\n+        if isinstance(self.rel.to, six.string_types):\n+            raise ValueError('Related model %r cannot been resolved' % self.rel.to)\n         related_fields = []\n         for index in range(len(self.from_fields)):\n             from_field_name = self.from_fields[index]\n@@ -1281,6 +1281,9 @@ def db_type(self, connection):\n             return IntegerField().db_type(connection=connection)\n         return rel_field.db_type(connection=connection)\n \n+    def db_parameters(self, connection):\n+        return {\"type\": self.db_type(connection), \"check\": []}\n+\n \n class OneToOneField(ForeignKey):\n     \"\"\"\n@@ -1351,6 +1354,7 @@ def set_managed(field, model, cls):\n         'unique_together': (from_, to),\n         'verbose_name': '%(from)s-%(to)s relationship' % {'from': from_, 'to': to},\n         'verbose_name_plural': '%(from)s-%(to)s relationships' % {'from': from_, 'to': to},\n+        'app_cache': field.model._meta.app_cache,\n     })\n     # Construct and return the new class.\n     return type(str(name), (models.Model,), {\n@@ -1561,3 +1565,11 @@ def formfield(self, **kwargs):\n                 initial = initial()\n             defaults['initial'] = [i._get_pk_val() for i in initial]\n         return super(ManyToManyField, self).formfield(**defaults)\n+\n+    def db_type(self, connection):\n+        # A ManyToManyField is not represented by a single column,\n+        # so return None.\n+        return None\n+\n+    def db_parameters(self, connection):\n+        return {\"type\": None, \"check\": None}\ndiff --git a/django/db/models/loading.py b/django/db/models/loading.py\nindex 6819134ae034..2858b8b6992d 100644\n--- a/django/db/models/loading.py\n+++ b/django/db/models/loading.py\n@@ -31,28 +31,30 @@ class UnavailableApp(Exception):\n     pass\n \n \n-class AppCache(object):\n+def _initialize():\n     \"\"\"\n-    A cache that stores installed applications and their models. Used to\n-    provide reverse-relations and for app introspection (e.g. admin).\n+    Returns a dictionary to be used as the initial value of the\n+    [shared] state of the app cache.\n     \"\"\"\n-    # Use the Borg pattern to share state between all instances. Details at\n-    # http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/66531.\n-    __shared_state = dict(\n+    return dict(\n         # Keys of app_store are the model modules for each application.\n         app_store=ModelDict(),\n \n         # Mapping of installed app_labels to model modules for that app.\n-        app_labels={},\n+        app_labels = {},\n \n         # Mapping of app_labels to a dictionary of model names to model code.\n         # May contain apps that are not installed.\n         app_models=ModelDict(),\n \n         # Mapping of app_labels to errors raised when trying to import the app.\n-        app_errors={},\n+        app_errors = {},\n+\n+        # Pending lookups for lazy relations\n+        pending_lookups = {},\n \n         # -- Everything below here is only used when populating the cache --\n+        loads_installed = True,\n         loaded=False,\n         handled=set(),\n         postponed=[],\n@@ -61,10 +63,27 @@ class AppCache(object):\n         available_apps=None,\n     )\n \n+\n+class BaseAppCache(object):\n+    \"\"\"\n+    A cache that stores installed applications and their models. Used to\n+    provide reverse-relations and for app introspection (e.g. admin).\n+\n+    This provides the base (non-Borg) AppCache class - the AppCache\n+    subclass adds borg-like behaviour for the few cases where it's needed,\n+    and adds the code that auto-loads from INSTALLED_APPS.\n+    \"\"\"\n+\n     def __init__(self):\n-        self.__dict__ = self.__shared_state\n+        self.__dict__ = _initialize()\n+        # This stops _populate loading from INSTALLED_APPS and ignores the\n+        # only_installed arguments to get_model[s]\n+        self.loads_installed = False\n \n     def _populate(self):\n+        \"\"\"\n+        Stub method - this base class does no auto-loading.\n+        \"\"\"\n         \"\"\"\n         Fill in all the cache information. This method is threadsafe, in the\n         sense that every caller will see the same state upon return, and if the\n@@ -72,6 +91,9 @@ def _populate(self):\n         \"\"\"\n         if self.loaded:\n             return\n+        if not self.loads_installed:\n+            self.loaded = True\n+            return\n         # Note that we want to use the import lock here - the app loading is\n         # in many cases initiated implicitly by importing, and thus it is\n         # possible to end up in deadlock when one thread initiates loading\n@@ -233,12 +255,15 @@ def get_models(self, app_mod=None,\n \n         By default, models that aren't part of installed apps will *not*\n         be included in the list of models. However, if you specify\n-        only_installed=False, they will be.\n+        only_installed=False, they will be. If you're using a non-default\n+        AppCache, this argument does nothing - all models will be included.\n \n         By default, models that have been swapped out will *not* be\n         included in the list of models. However, if you specify\n         include_swapped, they will be.\n         \"\"\"\n+        if not self.loads_installed:\n+            only_installed = False\n         cache_key = (app_mod, include_auto_created, include_deferred, only_installed, include_swapped)\n         model_list = None\n         try:\n@@ -287,6 +312,8 @@ def get_model(self, app_label, model_name,\n         Raises UnavailableApp when set_available_apps() in in effect and\n         doesn't include app_label.\n         \"\"\"\n+        if not self.loads_installed:\n+            only_installed = False\n         if seed_cache:\n             self._populate()\n         if only_installed and app_label not in self.app_labels:\n@@ -332,8 +359,24 @@ def set_available_apps(self, available):\n     def unset_available_apps(self):\n         self.available_apps = None\n \n+\n+class AppCache(BaseAppCache):\n+    \"\"\"\n+    A cache that stores installed applications and their models. Used to\n+    provide reverse-relations and for app introspection (e.g. admin).\n+\n+    Borg version of the BaseAppCache class.\n+    \"\"\"\n+\n+    __shared_state = _initialize()\n+\n+    def __init__(self):\n+        self.__dict__ = self.__shared_state\n+\n+\n cache = AppCache()\n \n+\n # These methods were always module level, so are kept that way for backwards\n # compatibility.\n get_apps = cache.get_apps\ndiff --git a/django/db/models/options.py b/django/db/models/options.py\nindex 5f269f83b9c6..14f73c301fd2 100644\n--- a/django/db/models/options.py\n+++ b/django/db/models/options.py\n@@ -9,7 +9,7 @@\n from django.db.models.fields.related import ManyToManyRel\n from django.db.models.fields import AutoField, FieldDoesNotExist\n from django.db.models.fields.proxy import OrderWrt\n-from django.db.models.loading import get_models, app_cache_ready\n+from django.db.models.loading import app_cache_ready, cache\n from django.utils import six\n from django.utils.functional import cached_property\n from django.utils.encoding import force_text, smart_text, python_2_unicode_compatible\n@@ -22,8 +22,7 @@\n                  'unique_together', 'permissions', 'get_latest_by',\n                  'order_with_respect_to', 'app_label', 'db_tablespace',\n                  'abstract', 'managed', 'proxy', 'swappable', 'auto_created',\n-                 'index_together', 'default_permissions')\n-\n+                 'index_together', 'app_cache', 'default_permissions')\n \n @python_2_unicode_compatible\n class Options(object):\n@@ -71,6 +70,9 @@ def __init__(self, meta, app_label=None):\n         # from *other* models. Needed for some admin checks. Internal use only.\n         self.related_fkey_lookups = []\n \n+        # A custom AppCache to use, if you're making a separate model set.\n+        self.app_cache = cache\n+\n     def contribute_to_class(self, cls, name):\n         from django.db import connection\n         from django.db.backends.util import truncate_name\n@@ -83,6 +85,10 @@ def contribute_to_class(self, cls, name):\n         self.model_name = self.object_name.lower()\n         self.verbose_name = get_verbose_name(self.object_name)\n \n+        # Store the original user-defined values for each option,\n+        # for use when serializing the model definition\n+        self.original_attrs = {}\n+\n         # Next, apply any overridden values from 'class Meta'.\n         if self.meta:\n             meta_attrs = self.meta.__dict__.copy()\n@@ -95,8 +101,10 @@ def contribute_to_class(self, cls, name):\n             for attr_name in DEFAULT_NAMES:\n                 if attr_name in meta_attrs:\n                     setattr(self, attr_name, meta_attrs.pop(attr_name))\n+                    self.original_attrs[attr_name] = getattr(self, attr_name)\n                 elif hasattr(self.meta, attr_name):\n                     setattr(self, attr_name, getattr(self.meta, attr_name))\n+                    self.original_attrs[attr_name] = getattr(self, attr_name)\n \n             # unique_together can be either a tuple of tuples, or a single\n             # tuple of two strings. Normalize it to a tuple of tuples, so that\n@@ -487,7 +495,7 @@ def _fill_related_objects_cache(self):\n                     cache[obj] = model\n         # Collect also objects which are in relation to some proxy child/parent of self.\n         proxy_cache = cache.copy()\n-        for klass in get_models(include_auto_created=True, only_installed=False):\n+        for klass in self.app_cache.get_models(include_auto_created=True, only_installed=False):\n             if not klass._meta.swapped:\n                 for f in klass._meta.local_fields:\n                     if f.rel and not isinstance(f.rel.to, six.string_types) and f.generate_reverse_relation:\n@@ -530,7 +538,7 @@ def _fill_related_many_to_many_cache(self):\n                     cache[obj] = parent\n                 else:\n                     cache[obj] = model\n-        for klass in get_models(only_installed=False):\n+        for klass in self.app_cache.get_models(only_installed=False):\n             if not klass._meta.swapped:\n                 for f in klass._meta.local_many_to_many:\n                     if (f.rel\ndiff --git a/django/db/models/signals.py b/django/db/models/signals.py\nindex 07824421d882..6b7605839ccb 100644\n--- a/django/db/models/signals.py\n+++ b/django/db/models/signals.py\n@@ -12,7 +12,9 @@\n pre_delete = Signal(providing_args=[\"instance\", \"using\"], use_caching=True)\n post_delete = Signal(providing_args=[\"instance\", \"using\"], use_caching=True)\n \n-pre_syncdb = Signal(providing_args=[\"app\", \"create_models\", \"verbosity\", \"interactive\", \"db\"])\n-post_syncdb = Signal(providing_args=[\"class\", \"app\", \"created_models\", \"verbosity\", \"interactive\", \"db\"])\n+pre_migrate = Signal(providing_args=[\"app\", \"create_models\", \"verbosity\", \"interactive\", \"db\"])\n+pre_syncdb = pre_migrate\n+post_migrate = Signal(providing_args=[\"class\", \"app\", \"created_models\", \"verbosity\", \"interactive\", \"db\"])\n+post_syncdb = post_migrate\n \n m2m_changed = Signal(providing_args=[\"action\", \"instance\", \"reverse\", \"model\", \"pk_set\", \"using\"], use_caching=True)\ndiff --git a/django/db/utils.py b/django/db/utils.py\nindex a1a2c0b56494..bcfb06f584e4 100644\n--- a/django/db/utils.py\n+++ b/django/db/utils.py\n@@ -262,10 +262,13 @@ def allow_relation(self, obj1, obj2, **hints):\n                     return allow\n         return obj1._state.db == obj2._state.db\n \n-    def allow_syncdb(self, db, model):\n+    def allow_migrate(self, db, model):\n         for router in self.routers:\n             try:\n-                method = router.allow_syncdb\n+                try:\n+                    method = router.allow_migrate\n+                except AttributeError:\n+                    method = router.allow_syncdb\n             except AttributeError:\n                 # If the router doesn't have a method, skip to the next one.\n                 pass\ndiff --git a/django/test/testcases.py b/django/test/testcases.py\nindex 6f3f1c00e4f7..5b72d4e8b81b 100644\n--- a/django/test/testcases.py\n+++ b/django/test/testcases.py\n@@ -718,7 +718,7 @@ def _pre_setup(self):\n         \"\"\"Performs any pre-test setup. This includes:\n \n         * If the class has an 'available_apps' attribute, restricting the app\n-          cache to these applications, then firing post_syncdb -- it must run\n+          cache to these applications, then firing post_migrate -- it must run\n           with the correct set of applications for the test case.\n         * If the class has a 'fixtures' attribute, installing these fixtures.\n         \"\"\"\n@@ -726,8 +726,7 @@ def _pre_setup(self):\n         if self.available_apps is not None:\n             cache.set_available_apps(self.available_apps)\n             for db_name in self._databases_names(include_mirrors=False):\n-                flush.Command.emit_post_syncdb(\n-                        verbosity=0, interactive=False, database=db_name)\n+                flush.Command.emit_post_migrate(verbosity=0, interactive=False, database=db_name)\n         try:\n             self._fixture_setup()\n         except Exception:\n@@ -772,7 +771,7 @@ def _post_teardown(self):\n         \"\"\"Performs any post-test things. This includes:\n \n         * Flushing the contents of the database, to leave a clean slate. If\n-          the class has an 'available_apps' attribute, post_syncdb isn't fired.\n+          the class has an 'available_apps' attribute, post_migrate isn't fired.\n         * Force-closing the connection, so the next test gets a clean cursor.\n         \"\"\"\n         try:\n@@ -790,14 +789,14 @@ def _post_teardown(self):\n             cache.unset_available_apps()\n \n     def _fixture_teardown(self):\n-        # Allow TRUNCATE ... CASCADE and don't emit the post_syncdb signal\n+        # Allow TRUNCATE ... CASCADE and don't emit the post_migrate signal\n         # when flushing only a subset of the apps\n         for db_name in self._databases_names(include_mirrors=False):\n             call_command('flush', verbosity=0, interactive=False,\n                          database=db_name, skip_validation=True,\n                          reset_sequences=False,\n                          allow_cascade=self.available_apps is not None,\n-                         inhibit_post_syncdb=self.available_apps is not None)\n+                         inhibit_post_migrate=self.available_apps is not None)\n \n     def assertQuerysetEqual(self, qs, values, transform=repr, ordered=True):\n         items = six.moves.map(transform, qs)\ndiff --git a/django/utils/datastructures.py b/django/utils/datastructures.py\nindex d6447ec0c790..a0ee3e06ef56 100644\n--- a/django/utils/datastructures.py\n+++ b/django/utils/datastructures.py\n@@ -1,5 +1,6 @@\n import copy\n import warnings\n+from collections import OrderedDict\n from django.utils import six\n \n class MergeDict(object):\n@@ -236,6 +237,36 @@ def clear(self):\n         super(SortedDict, self).clear()\n         self.keyOrder = []\n \n+class OrderedSet(object):\n+    \"\"\"\n+    A set which keeps the ordering of the inserted items.\n+    Currently backs onto OrderedDict.\n+    \"\"\"\n+\n+    def __init__(self, iterable=None):\n+        self.dict = OrderedDict(((x, None) for x in iterable) if iterable else [])\n+\n+    def add(self, item):\n+        self.dict[item] = None\n+\n+    def remove(self, item):\n+        del self.dict[item]\n+\n+    def discard(self, item):\n+        try:\n+            self.remove(item)\n+        except KeyError:\n+            pass\n+\n+    def __iter__(self):\n+        return iter(self.dict.keys())\n+\n+    def __contains__(self, item):\n+        return item in self.dict\n+\n+    def __nonzero__(self):\n+        return bool(self.dict)\n+\n class MultiValueDictKeyError(KeyError):\n     pass\n \ndiff --git a/django/utils/functional.py b/django/utils/functional.py\nindex fd10a84b26e0..9cc703fe844a 100644\n--- a/django/utils/functional.py\n+++ b/django/utils/functional.py\n@@ -148,6 +148,11 @@ def __cast(self):\n             else:\n                 return func(*self.__args, **self.__kw)\n \n+        def __ne__(self, other):\n+            if isinstance(other, Promise):\n+                other = other.__cast()\n+            return self.__cast() != other\n+\n         def __eq__(self, other):\n             if isinstance(other, Promise):\n                 other = other.__cast()\ndiff --git a/django/utils/termcolors.py b/django/utils/termcolors.py\nindex bb14837716bb..95d0d17f0f8d 100644\n--- a/django/utils/termcolors.py\n+++ b/django/utils/termcolors.py\n@@ -86,6 +86,10 @@ def make_style(opts=(), **kwargs):\n         'HTTP_BAD_REQUEST':  {},\n         'HTTP_NOT_FOUND':    {},\n         'HTTP_SERVER_ERROR': {},\n+        'MIGRATE_HEADING':   {},\n+        'MIGRATE_LABEL':     {},\n+        'MIGRATE_SUCCESS':   {},\n+        'MIGRATE_FAILURE':   {},\n     },\n     DARK_PALETTE: {\n         'ERROR':        { 'fg': 'red', 'opts': ('bold',) },\n@@ -101,6 +105,10 @@ def make_style(opts=(), **kwargs):\n         'HTTP_BAD_REQUEST':  { 'fg': 'red', 'opts': ('bold',) },\n         'HTTP_NOT_FOUND':    { 'fg': 'yellow' },\n         'HTTP_SERVER_ERROR': { 'fg': 'magenta', 'opts': ('bold',) },\n+        'MIGRATE_HEADING':   { 'fg': 'cyan', 'opts': ('bold',) },\n+        'MIGRATE_LABEL':     { 'opts': ('bold',) },\n+        'MIGRATE_SUCCESS':   { 'fg': 'green', 'opts': ('bold',) },\n+        'MIGRATE_FAILURE':   { 'fg': 'red', 'opts': ('bold',) },\n     },\n     LIGHT_PALETTE: {\n         'ERROR':        { 'fg': 'red', 'opts': ('bold',) },\n@@ -116,6 +124,10 @@ def make_style(opts=(), **kwargs):\n         'HTTP_BAD_REQUEST':  { 'fg': 'red', 'opts': ('bold',) },\n         'HTTP_NOT_FOUND':    { 'fg': 'red' },\n         'HTTP_SERVER_ERROR': { 'fg': 'magenta', 'opts': ('bold',) },\n+        'MIGRATE_HEADING':   { 'fg': 'cyan', 'opts': ('bold',) },\n+        'MIGRATE_LABEL':     { 'opts': ('bold',) },\n+        'MIGRATE_SUCCESS':   { 'fg': 'green', 'opts': ('bold',) },\n+        'MIGRATE_FAILURE':   { 'fg': 'red', 'opts': ('bold',) },\n     }\n }\n DEFAULT_PALETTE = DARK_PALETTE\ndiff --git a/docs/howto/legacy-databases.txt b/docs/howto/legacy-databases.txt\nindex 0bea8b41c497..1cf8329e79f8 100644\n--- a/docs/howto/legacy-databases.txt\n+++ b/docs/howto/legacy-databases.txt\n@@ -81,10 +81,10 @@ access to your precious data on a model by model basis.\n Install the core Django tables\n ==============================\n \n-Next, run the :djadmin:`syncdb` command to install any extra needed database\n+Next, run the :djadmin:`migrate` command to install any extra needed database\n records such as admin permissions and content types::\n \n-    python manage.py syncdb\n+    python manage.py migrate\n \n Test and tweak\n ==============\ndiff --git a/docs/index.txt b/docs/index.txt\nindex 8f46db8eb9f3..c58be5fcfca8 100644\n--- a/docs/index.txt\n+++ b/docs/index.txt\n@@ -71,6 +71,9 @@ manipulating the data of your Web application. Learn more about it below:\n   :doc:`Instance methods <ref/models/instances>` |\n   :doc:`Accessing related objects <ref/models/relations>`\n \n+* **Migrations:**\n+  :doc:`Introduction to Migrations<topics/migrations>`\n+\n * **Advanced:**\n   :doc:`Managers <topics/db/managers>` |\n   :doc:`Raw SQL <topics/db/sql>` |\ndiff --git a/docs/internals/contributing/writing-documentation.txt b/docs/internals/contributing/writing-documentation.txt\nindex 2944dea5046e..d2cfaddc8980 100644\n--- a/docs/internals/contributing/writing-documentation.txt\n+++ b/docs/internals/contributing/writing-documentation.txt\n@@ -165,9 +165,9 @@ __ http://sphinx.pocoo.org/markup/desc.html\n \n * ``django-admin`` commands::\n \n-        .. django-admin:: syncdb\n+        .. django-admin:: migrate\n \n-  To link, use ``:djadmin:`syncdb```.\n+  To link, use ``:djadmin:`migrate```.\n \n * ``django-admin`` command-line options::\n \ndiff --git a/docs/internals/deprecation.txt b/docs/internals/deprecation.txt\nindex 29e9231896a0..7b8298597d19 100644\n--- a/docs/internals/deprecation.txt\n+++ b/docs/internals/deprecation.txt\n@@ -419,6 +419,17 @@ these changes.\n \n * ``django.utils.unittest`` will be removed.\n \n+* The ``syncdb`` command will be removed.\n+\n+* ``django.db.models.signals.pre_syncdb`` and\n+  ``django.db.models.signals.post_syncdb`` will be removed, and\n+  ``django.db.models.signals.pre_migrate`` and\n+  ``django.db.models.signals.post_migrate`` will lose their\n+  ``create_models`` and ``created_models`` arguments.\n+\n+* ``allow_syncdb`` on database routers will no longer automatically become\n+  ``allow_migrate``.\n+\n * If models are organized in a package, Django will no longer look for\n   :ref:`initial SQL data<initial-sql>` in ``myapp/models/sql/``. Move your\n   custom SQL files to ``myapp/sql/``.\ndiff --git a/docs/intro/overview.txt b/docs/intro/overview.txt\nindex 55366fb2c651..415e831faf37 100644\n--- a/docs/intro/overview.txt\n+++ b/docs/intro/overview.txt\n@@ -53,10 +53,11 @@ automatically:\n \n .. code-block:: bash\n \n-    manage.py syncdb\n+    manage.py migrate\n \n-The :djadmin:`syncdb` command looks at all your available models and creates\n-tables in your database for whichever tables don't already exist.\n+The :djadmin:`migrate` command looks at all your available models and creates\n+tables in your database for whichever tables don't already exist, as well as\n+optionally providing :doc:`much richer schema control </topics/migrations>`.\n \n Enjoy the free API\n ==================\ndiff --git a/docs/intro/reusable-apps.txt b/docs/intro/reusable-apps.txt\nindex 7fa1ffc8d9e4..51c1228cc1d4 100644\n--- a/docs/intro/reusable-apps.txt\n+++ b/docs/intro/reusable-apps.txt\n@@ -155,7 +155,7 @@ this. For a small app like polls, this process isn't too difficult.\n \n           url(r'^polls/', include('polls.urls')),\n \n-    3. Run `python manage.py syncdb` to create the polls models.\n+    3. Run `python manage.py migrate` to create the polls models.\n \n     4. Start the development server and visit http://127.0.0.1:8000/admin/\n        to create a poll (you'll need the Admin app enabled).\ndiff --git a/docs/man/django-admin.1 b/docs/man/django-admin.1\nindex 4d937b488b8c..f1b568daf570 100644\n--- a/docs/man/django-admin.1\n+++ b/docs/man/django-admin.1\n@@ -45,8 +45,7 @@ Outputs to standard output all data in the database associated with the named\n application(s).\n .TP\n .BI flush\n-Returns the database to the state it was in immediately after syncdb was\n-executed.\n+Removes all data from the database and then re-installs any initial data.\n .TP\n .B inspectdb\n Introspects the database tables in the database specified in settings.py and outputs a Django\n@@ -114,9 +113,9 @@ the current directory or the optional destination.\n Creates a Django project directory structure for the given project name\n in the current directory or the optional destination.\n .TP\n-.BI syncdb\n-Creates the database tables for all apps in INSTALLED_APPS whose tables\n-haven't already been created.\n+.BI migrate\n+Runs migrations for apps containing migrations, and just creates missing tables\n+for apps without migrations.\n .TP\n .BI \"test [\" \"\\-\\-verbosity\" \"] [\" \"\\-\\-failfast\" \"] [\" \"appname ...\" \"]\"\n Runs the test suite for the specified applications, or the entire project if\ndiff --git a/docs/ref/contrib/comments/index.txt b/docs/ref/contrib/comments/index.txt\nindex 6db69d816844..c08ac21d4ed3 100644\n--- a/docs/ref/contrib/comments/index.txt\n+++ b/docs/ref/contrib/comments/index.txt\n@@ -31,7 +31,7 @@ To get started using the ``comments`` app, follow these steps:\n #. Install the comments framework by adding ``'django.contrib.comments'`` to\n    :setting:`INSTALLED_APPS`.\n \n-#. Run ``manage.py syncdb`` so that Django will create the comment tables.\n+#. Run ``manage.py migrate`` so that Django will create the comment tables.\n \n #. Add the comment app's URLs to your project's ``urls.py``:\n \ndiff --git a/docs/ref/contrib/contenttypes.txt b/docs/ref/contrib/contenttypes.txt\nindex bfe92b88a674..21e65f168b9d 100644\n--- a/docs/ref/contrib/contenttypes.txt\n+++ b/docs/ref/contrib/contenttypes.txt\n@@ -86,7 +86,7 @@ The ``ContentType`` model\n Let's look at an example to see how this works. If you already have\n the :mod:`~django.contrib.contenttypes` application installed, and then add\n :mod:`the sites application <django.contrib.sites>` to your\n-:setting:`INSTALLED_APPS` setting and run ``manage.py syncdb`` to install it,\n+:setting:`INSTALLED_APPS` setting and run ``manage.py migrate`` to install it,\n the model :class:`django.contrib.sites.models.Site` will be installed into\n your database. Along with it a new instance of\n :class:`~django.contrib.contenttypes.models.ContentType` will be\ndiff --git a/docs/ref/contrib/flatpages.txt b/docs/ref/contrib/flatpages.txt\nindex 11d74d75c3c3..be9fe0c636c3 100644\n--- a/docs/ref/contrib/flatpages.txt\n+++ b/docs/ref/contrib/flatpages.txt\n@@ -55,14 +55,14 @@ or:\n 3. Add ``'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware'``\n    to your :setting:`MIDDLEWARE_CLASSES` setting.\n \n-4. Run the command :djadmin:`manage.py syncdb <syncdb>`.\n+4. Run the command :djadmin:`manage.py migrate <migrate>`.\n \n .. currentmodule:: django.contrib.flatpages.middleware\n \n How it works\n ============\n \n-``manage.py syncdb`` creates two tables in your database: ``django_flatpage``\n+``manage.py migrate`` creates two tables in your database: ``django_flatpage``\n and ``django_flatpage_sites``. ``django_flatpage`` is a simple lookup table\n that simply maps a URL to a title and bunch of text content.\n ``django_flatpage_sites`` associates a flatpage with a site.\ndiff --git a/docs/ref/contrib/index.txt b/docs/ref/contrib/index.txt\nindex e5cea01eadb9..727fab01dc98 100644\n--- a/docs/ref/contrib/index.txt\n+++ b/docs/ref/contrib/index.txt\n@@ -15,7 +15,7 @@ those packages have.\n     For most of these add-ons -- specifically, the add-ons that include either\n     models or template tags -- you'll need to add the package name (e.g.,\n     ``'django.contrib.admin'``) to your :setting:`INSTALLED_APPS` setting and\n-    re-run ``manage.py syncdb``.\n+    re-run ``manage.py migrate``.\n \n .. _\"batteries included\" philosophy: http://docs.python.org/tutorial/stdlib.html#batteries-included\n \ndiff --git a/docs/ref/contrib/redirects.txt b/docs/ref/contrib/redirects.txt\nindex 0c0cb2a3c2cc..eefbb9672156 100644\n--- a/docs/ref/contrib/redirects.txt\n+++ b/docs/ref/contrib/redirects.txt\n@@ -18,12 +18,12 @@ To install the redirects app, follow these steps:\n 2. Add ``'django.contrib.redirects'`` to your :setting:`INSTALLED_APPS` setting.\n 3. Add ``'django.contrib.redirects.middleware.RedirectFallbackMiddleware'``\n    to your :setting:`MIDDLEWARE_CLASSES` setting.\n-4. Run the command :djadmin:`manage.py syncdb <syncdb>`.\n+4. Run the command :djadmin:`manage.py migrate <migrate>`.\n \n How it works\n ============\n \n-``manage.py syncdb`` creates a ``django_redirect`` table in your database. This\n+``manage.py migrate`` creates a ``django_redirect`` table in your database. This\n is a simple lookup table with ``site_id``, ``old_path`` and ``new_path`` fields.\n \n The ``RedirectFallbackMiddleware`` does all of the work. Each time any Django\ndiff --git a/docs/ref/contrib/sites.txt b/docs/ref/contrib/sites.txt\nindex 131c9645e8ae..fd2e917c1d96 100644\n--- a/docs/ref/contrib/sites.txt\n+++ b/docs/ref/contrib/sites.txt\n@@ -264,10 +264,10 @@ To enable the sites framework, follow these steps:\n \n     SITE_ID = 1\n \n-3. Run :djadmin:`syncdb`.\n+3. Run :djadmin:`migrate`.\n \n ``django.contrib.sites`` registers a\n-:data:`~django.db.models.signals.post_syncdb` signal handler which creates a\n+:data:`~django.db.models.signals.post_migrate` signal handler which creates a\n default site named ``example.com`` with the domain ``example.com``. This site\n will also be created after Django creates the test database. To set the\n correct name and domain for your project, you can use an :doc:`initial data\ndiff --git a/docs/ref/databases.txt b/docs/ref/databases.txt\nindex 545f5df84c66..707184c3ac08 100644\n--- a/docs/ref/databases.txt\n+++ b/docs/ref/databases.txt\n@@ -220,7 +220,7 @@ If you upgrade an existing project to MySQL 5.5.5 and subsequently add some\n tables, ensure that your tables are using the same storage engine (i.e. MyISAM\n vs. InnoDB). Specifically, if tables that have a ``ForeignKey`` between them\n use different storage engines, you may see an error like the following when\n-running ``syncdb``::\n+running ``migrate``::\n \n     _mysql_exceptions.OperationalError: (\n         1005, \"Can't create table '\\\\db_name\\\\.#sql-4a8_ab' (errno: 150)\"\n@@ -659,7 +659,7 @@ required.\n .. _`Oracle Database Server`: http://www.oracle.com/\n .. _`cx_Oracle`: http://cx-oracle.sourceforge.net/\n \n-In order for the ``python manage.py syncdb`` command to work, your Oracle\n+In order for the ``python manage.py migrate`` command to work, your Oracle\n database user must have privileges to run the following commands:\n \n * CREATE TABLE\n@@ -748,7 +748,7 @@ Oracle imposes a name length limit of 30 characters. To accommodate this, the\n backend truncates database identifiers to fit, replacing the final four\n characters of the truncated name with a repeatable MD5 hash value.\n \n-When running syncdb, an ``ORA-06552`` error may be encountered if\n+When running ``migrate``, an ``ORA-06552`` error may be encountered if\n certain Oracle keywords are used as the name of a model field or the\n value of a ``db_column`` option.  Django quotes all identifiers used\n in queries to prevent most such problems, but this error can still\ndiff --git a/docs/ref/django-admin.txt b/docs/ref/django-admin.txt\nindex 146740b8eb07..1385648d5dd7 100644\n--- a/docs/ref/django-admin.txt\n+++ b/docs/ref/django-admin.txt\n@@ -242,10 +242,8 @@ flush\n \n .. django-admin:: flush\n \n-Returns the database to the state it was in immediately after :djadmin:`syncdb`\n-was executed. This means that all data will be removed from the database, any\n-post-synchronization handlers will be re-executed, and the ``initial_data``\n-fixture will be re-installed.\n+Removes all data from the database, re-executes any post-synchronization\n+handlers, and reinstalls any initial data fixtures.\n \n The :djadminopt:`--noinput` option may be provided to suppress all user\n prompts.\n@@ -568,6 +566,52 @@ Use the ``--keep-pot`` option to prevent django from deleting the temporary\n .pot file it generates before creating the .po file. This is useful for\n debugging errors which may prevent the final language files from being created.\n \n+makemigrations [<appname>]\n+--------------------------\n+\n+.. django-admin:: makemigrations\n+\n+.. versionadded:: 1.7\n+\n+Creates new migrations based on the changes detected to your models.\n+Migrations, their relationship with apps and more are covered in depth in\n+:doc:`the migrations documentation</topics/migrations>`.\n+\n+Providing one or more app names as arguments will limit the migrations created\n+to the app(s) specified and any dependencies needed (the table at the other end\n+of a ``ForeignKey``, for example).\n+\n+.. django-admin-option:: --empty\n+\n+The ``--empty`` option will cause ``makemigrations`` to output an empty\n+migration for the specified apps, for manual editing. This option is only\n+for advanced users and should not be used unless you are familiar with\n+the migration format, migration operations, and the dependencies between\n+your migrations.\n+\n+migrate [<appname> [<migrationname>]]\n+-------------------------------------\n+\n+.. django-admin:: migrate\n+\n+.. versionadded:: 1.7\n+\n+Synchronizes the database state with the current set of models and migrations.\n+Migrations, their relationship with apps and more are covered in depth in\n+:doc:`the migrations documentation</topics/migrations>`.\n+\n+The behavior of this command changes depending on the arguments provided:\n+\n+* No arguments: All migrated apps have all of their migrations run,\n+  and all unmigrated apps are synchronized with the database,\n+* ``<appname>``: The specified app has its migrations run, up to the most\n+  recent migration. This may involve running other apps' migrations too, due\n+  to dependencies.\n+* ``<appname> <migrationname>``: Brings the database schema to a state where it\n+  would have just run the given migration, but no further - this may involve\n+  unapplying migrations if you have previously migrated past the named\n+  migration. Use the name `zero` to unapply all migrations for an app.\n+\n runfcgi [options]\n -----------------\n \n@@ -1102,45 +1146,13 @@ syncdb\n \n .. django-admin:: syncdb\n \n-Creates the database tables for all apps in :setting:`INSTALLED_APPS` whose\n-tables have not already been created.\n-\n-Use this command when you've added new applications to your project and want to\n-install them in the database. This includes any apps shipped with Django that\n-might be in :setting:`INSTALLED_APPS` by default. When you start a new project,\n-run this command to install the default apps.\n-\n-.. admonition:: Syncdb will not alter existing tables\n-\n-   ``syncdb`` will only create tables for models which have not yet been\n-   installed. It will *never* issue ``ALTER TABLE`` statements to match\n-   changes made to a model class after installation. Changes to model classes\n-   and database schemas often involve some form of ambiguity and, in those\n-   cases, Django would have to guess at the correct changes to make. There is\n-   a risk that critical data would be lost in the process.\n-\n-   If you have made changes to a model and wish to alter the database tables\n-   to match, use the ``sql`` command to display the new SQL structure and\n-   compare that to your existing table schema to work out the changes.\n-\n-If you're installing the ``django.contrib.auth`` application, ``syncdb`` will\n-give you the option of creating a superuser immediately.\n-\n-``syncdb`` will also search for and install any fixture named ``initial_data``\n-with an appropriate extension (e.g. ``json`` or ``xml``). See the\n-documentation for ``loaddata`` for details on the specification of fixture\n-data files.\n-\n-The :djadminopt:`--noinput` option may be provided to suppress all user\n-prompts.\n+.. deprecated:: 1.7\n \n-The :djadminopt:`--database` option can be used to specify the database to\n-synchronize.\n+    This command has been deprecated in favour of the :djadmin:`migrate`\n+    command, which performs both the old behaviour as well as executing\n+    migrations. It is now just an alias to that command.\n \n-``--no-initial-data``\n-~~~~~~~~~~~~~~~~~~~~~\n-\n-Use ``--no-initial-data`` to avoid loading the initial_data fixture.\n+Alias for :djadmin:`migrate`.\n \n test <app or test identifier>\n -----------------------------\n@@ -1278,7 +1290,7 @@ This command is only available if Django's :doc:`authentication system\n \n Creates a superuser account (a user who has all permissions). This is\n useful if you need to create an initial superuser account but did not\n-do so during ``syncdb``, or if you need to programmatically generate\n+do so during the first ``migrate``, or if you need to programmatically generate\n superuser accounts for your site(s).\n \n When run interactively, this command will prompt for a password for\n@@ -1362,7 +1374,7 @@ allows for the following options:\n \n Example usage::\n \n-    django-admin.py syncdb --pythonpath='/home/djangoprojects/myproject'\n+    django-admin.py migrate --pythonpath='/home/djangoprojects/myproject'\n \n Adds the given filesystem path to the Python `import search path`_. If this\n isn't provided, ``django-admin.py`` will use the ``PYTHONPATH`` environment\n@@ -1377,7 +1389,7 @@ setting the Python path for you.\n \n Example usage::\n \n-    django-admin.py syncdb --settings=mysite.settings\n+    django-admin.py migrate --settings=mysite.settings\n \n Explicitly specifies the settings module to use. The settings module should be\n in Python package syntax, e.g. ``mysite.settings``. If this isn't provided,\n@@ -1391,7 +1403,7 @@ Note that this option is unnecessary in ``manage.py``, because it uses\n \n Example usage::\n \n-    django-admin.py syncdb --traceback\n+    django-admin.py migrate --traceback\n \n By default, ``django-admin.py`` will show a simple error message whenever an\n :class:`~django.core.management.CommandError` occurs, but a full stack trace\n@@ -1407,7 +1419,7 @@ will also output a full stack trace when a ``CommandError`` is raised.\n \n Example usage::\n \n-    django-admin.py syncdb --verbosity 2\n+    django-admin.py migrate --verbosity 2\n \n Use ``--verbosity`` to specify the amount of notification and debug information\n that ``django-admin.py`` should print to the console.\ndiff --git a/docs/ref/models/options.txt b/docs/ref/models/options.txt\nindex 7ad0242df745..baa24f63cbc5 100644\n--- a/docs/ref/models/options.txt\n+++ b/docs/ref/models/options.txt\n@@ -106,9 +106,9 @@ Django quotes column and table names behind the scenes.\n .. attribute:: Options.managed\n \n     Defaults to ``True``, meaning Django will create the appropriate database\n-    tables in :djadmin:`syncdb` and remove them as part of a :djadmin:`flush`\n-    management command. That is, Django *manages* the database tables'\n-    lifecycles.\n+    tables in :djadmin:`migrate` or as part of migrations and remove them as\n+    part of a :djadmin:`flush` management command. That is, Django\n+    *manages* the database tables' lifecycles.\n \n     If ``False``, no database table creation or deletion operations will be\n     performed for this model. This is useful if the model represents an existing\n@@ -192,9 +192,9 @@ Django quotes column and table names behind the scenes.\n .. admonition:: Changing order_with_respect_to\n \n         ``order_with_respect_to`` adds an additional field/database column\n-        named ``_order``, so be sure to handle that as you would any other\n-        change to your models if you add or change ``order_with_respect_to``\n-        after your initial :djadmin:`syncdb`.\n+        named ``_order``, so be sure to make and apply the appropriate \n+        migrations if you add or change ``order_with_respect_to``\n+        after your initial :djadmin:`migrate`.\n \n ``ordering``\n ------------\ndiff --git a/docs/ref/signals.txt b/docs/ref/signals.txt\nindex 71756c98c622..8381a49a094b 100644\n--- a/docs/ref/signals.txt\n+++ b/docs/ref/signals.txt\n@@ -356,40 +356,36 @@ Management signals\n \n Signals sent by :doc:`django-admin </ref/django-admin>`.\n \n-pre_syncdb\n-----------\n+pre_migrate\n+-----------\n \n-.. data:: django.db.models.signals.pre_syncdb\n+.. data:: django.db.models.signals.pre_migrate\n    :module:\n \n-Sent by the :djadmin:`syncdb` command before it starts to install an\n+Sent by the :djadmin:`migrate` command before it starts to install an\n application.\n \n Any handlers that listen to this signal need to be written in a particular\n place: a ``management`` module in one of your :setting:`INSTALLED_APPS`. If\n handlers are registered anywhere else they may not be loaded by\n-:djadmin:`syncdb`.\n+:djadmin:`migrate`.\n \n Arguments sent with this signal:\n \n ``sender``\n-    The ``models`` module that was just installed. That is, if\n-    :djadmin:`syncdb` just installed an app called ``\"foo.bar.myapp\"``,\n-    ``sender`` will be the ``foo.bar.myapp.models`` module.\n+    The ``models`` module of the app about to be migrated/synced.\n+    For example, if :djadmin:`migrate` is about to install\n+    an app called ``\"foo.bar.myapp\"``, ``sender`` will be the\n+    ``foo.bar.myapp.models`` module.\n \n ``app``\n     Same as ``sender``.\n \n-``create_models``\n-    A list of the model classes from any app which :djadmin:`syncdb` plans to\n-    create.\n-\n-\n ``verbosity``\n     Indicates how much information manage.py is printing on screen. See\n     the :djadminopt:`--verbosity` flag for details.\n \n-    Functions which listen for :data:`pre_syncdb` should adjust what they\n+    Functions which listen for :data:`pre_migrate` should adjust what they\n     output to the screen based on the value of this argument.\n \n ``interactive``\n@@ -403,42 +399,55 @@ Arguments sent with this signal:\n ``db``\n     The alias of database on which a command will operate.\n \n-post_syncdb\n------------\n+pre_syncdb\n+----------\n \n-.. data:: django.db.models.signals.post_syncdb\n+.. data:: django.db.models.signals.pre_syncdb\n+   :module:\n+\n+.. deprecated:: 1.7\n+\n+    This signal has been renamed to :data:`~django.db.models.signals.pre_migrate`.\n+\n+Alias of :data:`django.db.models.signals.pre_migrate`. As long as this alias\n+is present, for backwards-compatability this signal has an extra argument it sends:\n+\n+``create_models``\n+    A list of the model classes from any app which :djadmin:`migrate` is\n+    going to create, **only if the app has no migrations**.\n+\n+post_migrate\n+------------\n+\n+.. data:: django.db.models.signals.post_migrate\n    :module:\n \n-Sent by the :djadmin:`syncdb` command after it installs an application, and the\n+Sent by the :djadmin:`migrate` command after it installs an application, and the\n :djadmin:`flush` command.\n \n Any handlers that listen to this signal need to be written in a particular\n place: a ``management`` module in one of your :setting:`INSTALLED_APPS`. If\n handlers are registered anywhere else they may not be loaded by\n-:djadmin:`syncdb`. It is important that handlers of this signal perform\n+:djadmin:`migrate`. It is important that handlers of this signal perform\n idempotent changes (e.g. no database alterations) as this may cause the\n :djadmin:`flush` management command to fail if it also ran during the\n-:djadmin:`syncdb` command.\n+:djadmin:`migrate` command.\n \n Arguments sent with this signal:\n \n ``sender``\n     The ``models`` module that was just installed. That is, if\n-    :djadmin:`syncdb` just installed an app called ``\"foo.bar.myapp\"``,\n+    :djadmin:`migrate` just installed an app called ``\"foo.bar.myapp\"``,\n     ``sender`` will be the ``foo.bar.myapp.models`` module.\n \n ``app``\n     Same as ``sender``.\n \n-``created_models``\n-    A list of the model classes from any app which :djadmin:`syncdb` has\n-    created so far.\n-\n ``verbosity``\n     Indicates how much information manage.py is printing on screen. See\n     the :djadminopt:`--verbosity` flag for details.\n \n-    Functions which listen for :data:`post_syncdb` should adjust what they\n+    Functions which listen for :data:`post_migrate` should adjust what they\n     output to the screen based on the value of this argument.\n \n ``interactive``\n@@ -455,14 +464,31 @@ Arguments sent with this signal:\n \n For example, ``yourapp/management/__init__.py`` could be written like::\n \n-    from django.db.models.signals import post_syncdb\n+    from django.db.models.signals import post_migrate\n     import yourapp.models\n \n     def my_callback(sender, **kwargs):\n         # Your specific logic here\n         pass\n \n-    post_syncdb.connect(my_callback, sender=yourapp.models)\n+    post_migrate.connect(my_callback, sender=yourapp.models)\n+\n+post_syncdb\n+-----------\n+\n+.. data:: django.db.models.signals.post_syncdb\n+   :module:\n+\n+.. deprecated:: 1.7\n+\n+    This signal has been renamed to :data:`~django.db.models.signals.post_migrate`.\n+\n+Alias of :data:`django.db.models.signals.post_migrate`. As long as this alias\n+is present, for backwards-compatability this signal has an extra argument it sends:\n+\n+``created_models``\n+    A list of the model classes from any app which :djadmin:`migrate` has\n+    created, **only if the app has no migrations**.\n \n Request/response signals\n ========================\ndiff --git a/docs/releases/1.7.txt b/docs/releases/1.7.txt\nindex 6480a2505f23..4c295f58e0eb 100644\n--- a/docs/releases/1.7.txt\n+++ b/docs/releases/1.7.txt\n@@ -30,6 +30,61 @@ security support until the release of Django 1.8.\n What's new in Django 1.7\n ========================\n \n+Schema migrations\n+~~~~~~~~~~~~~~~~~\n+\n+Django now has built-in support for schema migrations. It allows models\n+to be updated, changed, and deleted by creating migration files that represent\n+the model changes and which can be run on any development, staging or production\n+database.\n+\n+Migrations are covered in :doc:`their own documentation</topics/migrations>`,\n+but a few of the key features are:\n+\n+* ``syncdb`` has been deprecated and replaced by ``migrate``. Don't worry - \n+  calls to ``syncdb`` will still work as before.\n+\n+* A new ``makemigrations`` command provides an easy way to autodetect changes\n+  to your models and make migrations for them.\n+\n+* :data:`~django.db.models.signals.post_syncdb` and\n+  :data:`~django.db.models.signals.post_syncdb` have been renamed to\n+  :data:`~django.db.models.signals.pre_migrate` and\n+  :data:`~django.db.models.signals.post_migrate` respectively. The\n+  ``create_models``/``created_models`` argument has also been deprecated.\n+\n+* The ``allow_syncdb`` method on database routers is now called ``allow_migrate``,\n+  but still performs the same function. Routers with ``allow_syncdb`` methods\n+  will still work, but that method name is deprecated and you should change\n+  it as soon as possible (nothing more than renaming is required).\n+\n+New method on Field subclasses\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+To help power both schema migrations and composite keys, the :class:`~django.db.models.Field` API now\n+has a new required method: ``deconstruct()``.\n+\n+This method takes no arguments, and returns a tuple of four items:\n+\n+* ``name``: The field's attribute name on its parent model, or None if it is not part of a model\n+* ``path``: A dotted, Python path to the class of this field, including the class name.\n+* ``args``: Positional arguments, as a list\n+* ``kwargs``: Keyword arguments, as a dict\n+\n+These four values allow any field to be serialized into a file, as well as\n+allowing the field to be copied safely, both essential parts of these new features.\n+\n+This change should not affect you unless you write custom Field subclasses;\n+if you do, you may need to reimplement the ``deconstruct()`` method if your\n+subclass changes the method signature of ``__init__`` in any way. If your\n+field just inherits from a built-in Django field and doesn't override ``__init__``,\n+no changes are necessary.\n+\n+If you do need to override ``deconstruct()``, a good place to start is the\n+built-in Django fields (``django/db/models/fields/__init__.py``) as several\n+fields, including ``DecimalField`` and ``DateField``, override it and show how\n+to call the method on the superclass and simply add or remove extra arguments.\n+\n Calling custom ``QuerySet`` methods from the ``Manager``\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n@@ -170,6 +225,18 @@ Backwards incompatible changes in 1.7\n     deprecation timeline for a given feature, its removal may appear as a\n     backwards incompatible change.\n \n+allow_syncdb/allow_migrate\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+While Django will still look at ``allow_syncdb`` methods even though they\n+should be renamed to ``allow_migrate``, there is a subtle difference in which\n+models get passed to these methods.\n+\n+For apps with migrations, ``allow_migrate`` will now get passed\n+:ref:`historical models <historical-models>`, which are special versioned models\n+without custom attributes, methods or managers. Make sure your ``allow_migrate``\n+methods are only referring to fields or other items in ``model._meta``.\n+\n Miscellaneous\n ~~~~~~~~~~~~~\n \n@@ -249,3 +316,10 @@ work until Django 1.9.\n it will go through a regular deprecation path. This attribute was mostly used\n by methods that bypassed ``ModelAdmin.get_fieldsets()`` but this was considered\n a bug and has been addressed.\n+\n+``syncdb``\n+~~~~~~~~~~\n+\n+The ``syncdb`` command has been deprecated in favour of the new ``migrate``\n+command. ``migrate`` takes the same arguments as ``syncdb`` used to plus a few\n+more, so it's safe to just change the name you're calling and nothing else.\ndiff --git a/docs/topics/auth/customizing.txt b/docs/topics/auth/customizing.txt\nindex 8a44d58ef618..746e92e637a2 100644\n--- a/docs/topics/auth/customizing.txt\n+++ b/docs/topics/auth/customizing.txt\n@@ -275,7 +275,7 @@ can or cannot do with Task instances, specific to your application::\n             )\n \n The only thing this does is create those extra permissions when you run\n-:djadmin:`manage.py syncdb <syncdb>`. Your code is in charge of checking the\n+:djadmin:`manage.py migrate <migrate>`. Your code is in charge of checking the\n value of these permissions when an user is trying to access the functionality\n provided by the application (viewing tasks, changing the status of tasks,\n closing tasks.) Continuing the above example, the following checks if a user may\n@@ -378,14 +378,12 @@ use as your User model.\n    Changing :setting:`AUTH_USER_MODEL` has a big effect on your database\n    structure. It changes the tables that are available, and it will affect the\n    construction of foreign keys and many-to-many relationships. If you intend\n-   to set :setting:`AUTH_USER_MODEL`, you should set it before running\n-   ``manage.py syncdb`` for the first time.\n+   to set :setting:`AUTH_USER_MODEL`, you should set it before creating\n+   any migrations or running ``manage.py migrate`` for the first time.\n \n-   If you have an existing project and you want to migrate to using a custom\n-   User model, you may need to look into using a migration tool like South_\n-   to ease the transition.\n-\n-.. _South: http://south.aeracode.org\n+   Changing this setting after you have tables created is not supported\n+   by :djadmin:`makemigrations` and will result in you having to manually\n+   write a set of migrations to fix your schema.\n \n Referencing the User model\n --------------------------\ndiff --git a/docs/topics/auth/default.txt b/docs/topics/auth/default.txt\nindex fdeb20fd1034..4d86a7330ede 100644\n--- a/docs/topics/auth/default.txt\n+++ b/docs/topics/auth/default.txt\n@@ -65,7 +65,7 @@ interactively <auth-admin>`.\n Creating superusers\n -------------------\n \n-:djadmin:`manage.py syncdb <syncdb>` prompts you to create a superuser the\n+:djadmin:`manage.py migrate <migrate>` prompts you to create a superuser the\n first time you run it with ``'django.contrib.auth'`` in your\n :setting:`INSTALLED_APPS`. If you need to create a superuser at a later date,\n you can use a command line utility::\n@@ -190,13 +190,13 @@ setting, it will ensure that three default permissions -- add, change and\n delete -- are created for each Django model defined in one of your installed\n applications.\n \n-These permissions will be created when you run :djadmin:`manage.py syncdb\n-<syncdb>`; the first time you run ``syncdb`` after adding\n+These permissions will be created when you run :djadmin:`manage.py migrate\n+<migrate>`; the first time you run ``migrate`` after adding\n ``django.contrib.auth`` to :setting:`INSTALLED_APPS`, the default permissions\n will be created for all previously-installed models, as well as for any new\n models being installed at that time. Afterward, it will create default\n-permissions for new models each time you run :djadmin:`manage.py syncdb\n-<syncdb>`.\n+permissions for new models each time you run :djadmin:`manage.py migrate\n+<migrate>`.\n \n Assuming you have an application with an\n :attr:`~django.db.models.Options.app_label` ``foo`` and a model named ``Bar``,\ndiff --git a/docs/topics/auth/index.txt b/docs/topics/auth/index.txt\nindex 8447d449ceab..81b6996d0080 100644\n--- a/docs/topics/auth/index.txt\n+++ b/docs/topics/auth/index.txt\n@@ -67,7 +67,7 @@ and two items in your :setting:`MIDDLEWARE_CLASSES` setting:\n 2. :class:`~django.contrib.auth.middleware.AuthenticationMiddleware` associates\n    users with requests using sessions.\n \n-With these settings in place, running the command ``manage.py syncdb`` creates\n+With these settings in place, running the command ``manage.py migrate`` creates\n the necessary database tables for auth related models, creates permissions for\n any models defined in your installed apps, and prompts you to create\n a superuser account the first time you run it.\ndiff --git a/docs/topics/cache.txt b/docs/topics/cache.txt\nindex a2491f21983a..092df1f8766a 100644\n--- a/docs/topics/cache.txt\n+++ b/docs/topics/cache.txt\n@@ -219,8 +219,8 @@ operations to ``cache_slave``, and all write operations to\n                 return 'cache_master'\n             return None\n \n-        def allow_syncdb(self, db, model):\n-            \"Only synchronize the cache model on master\"\n+        def allow_migrate(self, db, model):\n+            \"Only install the cache model on master\"\n             if model._meta.app_label in ('django_cache',):\n                 return db == 'cache_master'\n             return None\ndiff --git a/docs/topics/db/models.txt b/docs/topics/db/models.txt\nindex 2b565758e7ff..b0011e1098ed 100644\n--- a/docs/topics/db/models.txt\n+++ b/docs/topics/db/models.txt\n@@ -77,7 +77,8 @@ application by the :djadmin:`manage.py startapp <startapp>` script),\n     )\n \n When you add new apps to :setting:`INSTALLED_APPS`, be sure to run\n-:djadmin:`manage.py syncdb <syncdb>`.\n+:djadmin:`manage.py migrate <migrate>`, optionally making migrations\n+for them first with :djadmin:`manage.py makemigrations <makemigrations>`.\n \n Fields\n ======\n@@ -956,7 +957,7 @@ The reverse name of the ``common.ChildA.m2m`` field will be\n reverse name of the ``rare.ChildB.m2m`` field will be ``rare_childb_related``.\n It is up to you how you use the ``'%(class)s'`` and ``'%(app_label)s`` portion\n to construct your related name, but if you forget to use it, Django will raise\n-errors when you validate your models (or run :djadmin:`syncdb`).\n+errors when you validate your models (or run :djadmin:`migrate`).\n \n If you don't specify a :attr:`~django.db.models.ForeignKey.related_name`\n attribute for a field in an abstract base class, the default reverse name will\n@@ -1049,7 +1050,7 @@ are putting those types of relations on a subclass of another model,\n you **must** specify the\n :attr:`~django.db.models.ForeignKey.related_name` attribute on each\n such field. If you forget, Django will raise an error when you run\n-:djadmin:`validate` or :djadmin:`syncdb`.\n+:djadmin:`validate` or :djadmin:`migrate`.\n \n For example, using the above ``Place`` class again, let's create another\n subclass with a :class:`~django.db.models.ManyToManyField`::\ndiff --git a/docs/topics/db/multi-db.txt b/docs/topics/db/multi-db.txt\nindex 3a902a97930a..c098aa33e308 100644\n--- a/docs/topics/db/multi-db.txt\n+++ b/docs/topics/db/multi-db.txt\n@@ -155,14 +155,23 @@ A database Router is a class that provides up to four methods:\n     used by foreign key and many to many operations to determine if a\n     relation should be allowed between two objects.\n \n-.. method:: allow_syncdb(db, model)\n+.. method:: allow_migrate(db, model)\n \n-    Determine if the ``model`` should be synchronized onto the\n+    Determine if the ``model`` should have tables/indexes created in the\n     database with alias ``db``. Return True if the model should be\n-    synchronized, False if it should not be synchronized, or None if\n+    migrated, False if it should not be migrated, or None if\n     the router has no opinion. This method can be used to determine\n     the availability of a model on a given database.\n \n+    Note that migrations will just silently not perform any operations\n+    on a model for which this returns ``False``. This may result in broken\n+    ForeignKeys, extra tables or missing tables if you change it once you\n+    have applied some migrations.\n+\n+    The value passed for ``model`` may be a\n+    :ref:`historical model <historical-models>`, and thus not have any\n+    custom attributes, methods or managers. You should only rely on ``_meta``.\n+\n A router doesn't have to provide *all* these methods -- it may omit one\n or more of them. If one of the methods is omitted, Django will skip\n that router when performing the relevant check.\n@@ -288,7 +297,7 @@ send queries for the ``auth`` app to ``auth_db``::\n                return True\n             return None\n \n-        def allow_syncdb(self, db, model):\n+        def allow_migrate(self, db, model):\n             \"\"\"\n             Make sure the auth app only appears in the 'auth_db'\n             database.\n@@ -328,7 +337,7 @@ from::\n                 return True\n             return None\n \n-        def allow_syncdb(self, db, model):\n+        def allow_migrate(self, db, model):\n             \"\"\"\n             All non-auth models end up in this pool.\n             \"\"\"\n@@ -347,7 +356,7 @@ be queried in the order the are listed in the\n result, decisions concerning the models in ``auth`` are processed\n before any other decision is made. If the :setting:`DATABASE_ROUTERS`\n setting listed the two routers in the other order,\n-``MasterSlaveRouter.allow_syncdb()`` would be processed first. The\n+``MasterSlaveRouter.allow_migrate()`` would be processed first. The\n catch-all nature of the MasterSlaveRouter implementation would mean\n that all models would be available on all databases.\n \ndiff --git a/docs/topics/http/sessions.txt b/docs/topics/http/sessions.txt\nindex 637991b3b528..24b9ef446224 100644\n--- a/docs/topics/http/sessions.txt\n+++ b/docs/topics/http/sessions.txt\n@@ -44,7 +44,7 @@ Using database-backed sessions\n If you want to use a database-backed session, you need to add\n ``'django.contrib.sessions'`` to your :setting:`INSTALLED_APPS` setting.\n \n-Once you have configured your installation, run ``manage.py syncdb``\n+Once you have configured your installation, run ``manage.py migrate``\n to install the single database table that stores session data.\n \n .. _cached-sessions-backend:\ndiff --git a/docs/topics/index.txt b/docs/topics/index.txt\nindex f8f60b2953f6..b248e1026839 100644\n--- a/docs/topics/index.txt\n+++ b/docs/topics/index.txt\n@@ -12,6 +12,7 @@ Introductions to all the key parts of Django you'll need to know:\n    forms/index\n    templates\n    class-based-views/index\n+   migrations\n    files\n    testing/index\n    auth/index\ndiff --git a/docs/topics/install.txt b/docs/topics/install.txt\nindex 5bcc3f64ec1e..3e813b2a4115 100644\n--- a/docs/topics/install.txt\n+++ b/docs/topics/install.txt\n@@ -121,14 +121,12 @@ database bindings are installed.\n * If you're using an unofficial 3rd party backend, please consult the\n   documentation provided for any additional requirements.\n \n-If you plan to use Django's ``manage.py syncdb`` command to automatically\n+If you plan to use Django's ``manage.py migrate`` command to automatically\n create database tables for your models (after first installing Django and\n creating a project), you'll need to ensure that Django has permission to create\n and alter tables in the database you're using; if you plan to manually create\n the tables, you can simply grant Django ``SELECT``, ``INSERT``, ``UPDATE`` and\n-``DELETE`` permissions. On some databases, Django will need ``ALTER TABLE``\n-privileges during ``syncdb`` but won't issue ``ALTER TABLE`` statements on a\n-table once ``syncdb`` has created it. After creating a database user with these\n+``DELETE`` permissions. After creating a database user with these\n permissions, you'll specify the details in your project's settings file,\n see :setting:`DATABASES` for details.\n \ndiff --git a/docs/topics/migrations.txt b/docs/topics/migrations.txt\nnew file mode 100644\nindex 000000000000..5862c5defeca\n--- /dev/null\n+++ b/docs/topics/migrations.txt\n@@ -0,0 +1,297 @@\n+==========\n+Migrations\n+==========\n+\n+.. module:: django.db.migrations\n+   :synopsis: Schema migration support for Django models\n+\n+.. versionadded:: 1.7\n+\n+Migrations are Django's way of propagating changes you make to your models\n+(adding a field, deleting a model, etc.) into your database schema. They're\n+designed to be mostly automatic, but you'll need to know when to make\n+migrations, when to run them, and the common problems you might run into.\n+\n+A Brief History\n+---------------\n+\n+Prior to version 1.7, Django only supported adding new models to the\n+database; it was not possible to alter or remove existing models via the\n+``syncdb`` command (the predecessor to ``migrate``).\n+\n+Third-party tools, most notably `South <http://south.aeracode.org>`_,\n+provided support for these additional types of change, but it was considered\n+important enough that support was brought into core Django.\n+\n+Two Commands\n+------------\n+\n+There are two commands which you will use to interact with migrations\n+and Django's handling of database schema:\n+\n+* :djadmin:`migrate`, which is responsible for applying migrations, as well as\n+  unapplying and listing their status.\n+\n+* :djadmin:`makemigrations`, which is responsible for creating new migrations\n+  based on the changes you have made to your models.\n+\n+It's worth noting that migrations are created and run on a per-app basis.\n+In particular, it's possible to have apps that *do not use migrations* (these\n+are referred to as \"unmigrated\" apps) - these apps will instead mimic the\n+legacy behaviour of just adding new models.\n+\n+You should think of migrations as a version control system for your database\n+schema. ``makemigrations`` is responsible for packaging up your model changes\n+into individual migration files - analagous to commits - and ``migrate`` is\n+responsible for applying those to your database.\n+\n+The migration files for each app live in a \"migrations\" directory inside\n+of that app, and are designed to be committed to, and distributed as part\n+of, its codebase. You should be making them once on your development machine\n+and then running the same migrations on your colleagues' machines, your\n+staging machines, and eventually your production machines.\n+\n+Migrations will run the same way every time and produce consistent results,\n+meaning that what you see in development and staging is exactly what will\n+happen in production - no unexpected surprises.\n+\n+Backend Support\n+---------------\n+\n+Migrations are supported on all backends that Django ships with, as well\n+as any third-party backends if they have programmed in support for schema\n+alteration (done via the ``SchemaEditor`` class).\n+\n+However, some databases are more capable than others when it comes to\n+schema migrations; some of the caveats are covered below.\n+\n+PostgreSQL\n+~~~~~~~~~~\n+\n+PostgreSQL is the most capable of all the databases here in terms of schema\n+support; the only caveat is that adding columns with default values will\n+lock a table for a time proportional to the number of rows in it.\n+\n+For this reason, it's recommended you always create new columns with\n+``null=True``, as this way they will be added immediately.\n+\n+MySQL\n+~~~~~\n+\n+MySQL lacks support for transactions around schema alteration operations,\n+meaning that if a migration fails to apply you will have to manually unpick\n+the changes in order to try again (it's impossible to roll back to an\n+earlier point).\n+\n+In addition, MySQL will lock tables for almost every schema operation and\n+generally takes a time proportional to the number of rows in the table to\n+add or remove columns. On slower hardware this can be worse than a minute\n+per million rows - adding a few columns to a table with just a few million\n+rows could lock your site up for over ten minutes.\n+\n+Finally, MySQL has reasonably small limits on name lengths for columns, tables\n+and indexes, as well as a limit on the combined size of all columns an index\n+covers. This means that indexes that are possible on other backends will\n+fail to be created under MySQL.\n+\n+SQLite\n+~~~~~~\n+\n+SQLite has very little built-in schema alteration support, and so Django\n+attempts to emulate it by:\n+\n+* Creating a new table with the new schema\n+* Copying the data across\n+* Dropping the old table\n+* Renaming the new table to match the original name\n+\n+This process generally works well, but it can be slow and occasionally\n+buggy. It is not recommended that you run and migrate SQLite in a\n+production environment unless you are very aware of the risks and\n+its limitations; the support Django ships with is designed to allow\n+developers to use SQLite on their local machines to develop less complex\n+Django projects without the need for a full database.\n+\n+Workflow\n+--------\n+\n+Working with migrations is simple. Make changes to your models - say, add\n+a field and remove a model - and then run :djadmin:`makemigrations`::\n+\n+    $ python manage.py makemigrations\n+    Migrations for 'books':\n+      0003_auto.py:\n+        - Alter field author on book\n+\n+Your models will be scanned and compared to the versions currently\n+contained in your migration files, and then a new set of migrations\n+will be written out. Make sure to read the output to see what\n+``makemigrations`` thinks you have changed - it's not perfect, and for\n+complex changes it might not be detecting what you expect.\n+\n+Once you have your new migration files, you should apply them to your\n+database to make sure they work as expected::\n+\n+    $ python manage.py migrate\n+    Operations to perform:\n+      Synchronize unmigrated apps: sessions, admin, messages, auth, staticfiles, contenttypes\n+      Apply all migrations: books\n+    Synchronizing apps without migrations:\n+      Creating tables...\n+      Installing custom SQL...\n+      Installing indexes...\n+    Installed 0 object(s) from 0 fixture(s)\n+    Running migrations:\n+      Applying books.0003_auto... OK\n+\n+The command runs in two stages; first, it synchronizes unmigrated apps\n+(performing the same functionality that ``syncdb`` used to provide), and\n+then it runs any migrations that have not yet been applied.\n+\n+Once the migration is applied, commit the migration and the models change\n+to your version control system as a single commit - that way, when other\n+developers (or your production servers) check out the code, they'll\n+get both the changes to your models and the accompanying migration at the\n+same time.\n+\n+Version control\n+~~~~~~~~~~~~~~~\n+\n+Because migrations are stored in version control, you'll occasionally\n+come across situations where you and another developer have both committed\n+a migration to the same app at the same time, resulting in two migrations\n+with the same number.\n+\n+Don't worry - the numbers are just there for developers' reference, Django\n+just cares that each migration has a different name. Migrations specify which\n+other migrations they depend on - including earlier migrations in the same\n+app - in the file, so it's possible to detect when there's two new migrations\n+for the same app that aren't ordered.\n+\n+When this happens, Django will prompt you and give you some options. If it\n+thinks it's safe enough, it will offer to automatically linearize the two\n+migrations for you. If not, you'll have to go in and modify the migrations\n+yourself - don't worry, this isn't difficult, and is explained more in\n+:ref:`migration-files` below.\n+\n+Dependencies\n+------------\n+\n+While migrations are per-app, the tables and relationships implied by\n+your models are too complex to be created for just one app at a time. When\n+you make a migration that requires something else to run - for example,\n+you add a ForeignKey in your ``books`` app to your ``authors`` app - the\n+resulting migration will contain a dependency on a migration in ``authors``.\n+\n+This means that when you run the migrations, the ``authors`` migration runs\n+first and creates the table the ``ForeignKey`` references, and then the migration\n+that makes the ``ForeignKey`` column runs afterwards and creates the constraint.\n+If this didn't happen, the migration would try to create the ForeignKey column\n+without the table it's referencing existing and your database would\n+throw an error.\n+\n+This dependency behaviour affects most migration operations where you\n+restrict to a single app. Restricting to a single app (either in\n+``makemigrations`` or ``migrate``) is a best-efforts promise, and not\n+a guarantee; any other apps that need to be used to get dependencies correct\n+will be.\n+\n+.. migration-files:\n+\n+Migration files\n+---------------\n+\n+Migrations are stored as an on-disk format, referred to here as\n+\"migration files\". These files are actually just normal Python files with\n+an agreed-upon object layout, written in a declarative style.\n+\n+A basic migration file looks like this::\n+\n+    from django.db import migrations, models\n+\n+    class Migration(migrations.Migration):\n+\n+        dependencies = [(\"migrations\", \"0001_initial\")]\n+\n+        operations = [\n+            migrations.DeleteModel(\"Tribble\"),\n+            migrations.AddField(\"Author\", \"rating\", models.IntegerField(default=0)),\n+        ]\n+\n+What Django looks for when it loads a migration file (as a Python module) is\n+a subclass of ``django.db.migrations.Migration`` called ``Migration``. It then\n+inspects this object for four attributes, only two of which are used\n+most of the time:\n+\n+* ``dependencies``, a list of migrations this one depends on.\n+* ``operations``, a list of Operation classes that define what this migration\n+  does.\n+\n+The operations are the key; they are a set of declarative instructions which\n+tell Django what schema changes need to be made. Django scans them and\n+builds an in-memory representation of all of the schema changes to all apps,\n+and uses this to generate the SQL which makes the schema changes.\n+\n+That in-memory structure is also used to work out what the differences are\n+between your models and the current state of your migrations; Django runs\n+through all the changes, in order, on an in-memory set of models to come\n+up with the state of your models last time you ran ``makemigrations``. It\n+then uses these models to compare against the ones in your ``models.py`` files\n+to work out what you have changed.\n+\n+You should rarely, if ever, need to edit migration files by hand, but\n+it's entirely possible to write them manually if you need to. Some of the\n+more complex operations are not autodetectable and are only available via\n+a hand-written migration, so don't be scared about editing them if you have to.\n+\n+Adding migrations to apps\n+-------------------------\n+\n+Adding migrations to new apps is straightforward - they come preconfigured to\n+accept migrations, and so just run :djadmin:`makemigrations` once you've made\n+some changes.\n+\n+If your app already has models and database tables, and doesn't have migrations\n+yet (for example, you created it against a previous Django version), you'll\n+need to convert it to use migrations; this is a simple process::\n+\n+    python manage.py makemigrations --force yourappname\n+\n+This will make a new initial migration for your app (the ``--force`` argument\n+is to override Django's default behaviour, as it thinks your app does not want\n+migrations). Now, when you run :djadmin:`migrate`, Django will detect that\n+you have an initial migration *and* that the tables it wants to create already\n+exist, and will mark the migration as already applied.\n+\n+Note that this only works given two things:\n+\n+* You have not changed your models since you made their tables. For migrations\n+  to work, you must make the initial migration *first* and then make changes,\n+  as Django compares changes against migration files, not the database.\n+\n+* You have not manually edited your database - Django won't be able to detect\n+  that your database doesn't match your models, you'll just get errors when\n+  migrations try to modify those tables.\n+\n+\n+.. historical-models:\n+\n+Historical models\n+-----------------\n+\n+When you run migrations, Django is working from historical versions of\n+your models stored in the migration files. If you write Python code\n+using the ``django.db.migrations.RunPython`` operation, or if you have\n+``allow_migrate`` methods on your database routers, you will be exposed\n+to these versions of your models.\n+\n+Because it's impossible to serialize arbitrary Python code, these historical\n+models will not have any custom methods or managers that you have defined.\n+They will, however, have the same fields, relationships and ``Meta`` options\n+(also versioned, so they may be different from your current ones).\n+\n+In addition, the base classes of the model are just stored as pointers,\n+so you must always keep base classes around for as long as there is a migration\n+that contains a reference to them. On the plus side, methods and managers\n+from these base classes inherit normally, so if you absolutely need access\n+to these you can opt to move them into a superclass.\ndiff --git a/docs/topics/serialization.txt b/docs/topics/serialization.txt\nindex 8078e0f2c64b..0f247155996a 100644\n--- a/docs/topics/serialization.txt\n+++ b/docs/topics/serialization.txt\n@@ -293,7 +293,7 @@ serialize an object that refers to a content type, then you need to have a way\n to refer to that content type to begin with. Since ``ContentType`` objects are\n automatically created by Django during the database synchronization process,\n the primary key of a given content type isn't easy to predict; it will\n-depend on how and when :djadmin:`syncdb` was executed. This is true for all\n+depend on how and when :djadmin:`migrate` was executed. This is true for all\n models which automatically generate objects, notably including\n :class:`~django.contrib.auth.models.Permission`,\n :class:`~django.contrib.auth.models.Group`, and\ndiff --git a/docs/topics/testing/advanced.txt b/docs/topics/testing/advanced.txt\nindex 9ed06447c00e..d8d59c687287 100644\n--- a/docs/topics/testing/advanced.txt\n+++ b/docs/topics/testing/advanced.txt\n@@ -182,7 +182,7 @@ Advanced features of ``TransactionTestCase``\n \n     By default, ``available_apps`` is set to ``None``. After each test, Django\n     calls :djadmin:`flush` to reset the database state. This empties all tables\n-    and emits the :data:`~django.db.models.signals.post_syncdb` signal, which\n+    and emits the :data:`~django.db.models.signals.post_migrate` signal, which\n     re-creates one content type and three permissions for each model. This\n     operation gets expensive proportionally to the number of models.\n \n@@ -190,13 +190,13 @@ Advanced features of ``TransactionTestCase``\n     behave as if only the models from these applications were available. The\n     behavior of ``TransactionTestCase`` changes as follows:\n \n-    - :data:`~django.db.models.signals.post_syncdb` is fired before each\n+    - :data:`~django.db.models.signals.post_migrate` is fired before each\n       test to create the content types and permissions for each model in\n       available apps, in case they're missing.\n     - After each test, Django empties only tables corresponding to models in\n       available apps. However, at the database level, truncation may cascade to\n       related models in unavailable apps. Furthermore\n-      :data:`~django.db.models.signals.post_syncdb` isn't fired; it will be\n+      :data:`~django.db.models.signals.post_migrate` isn't fired; it will be\n       fired by the next ``TransactionTestCase``, after the correct set of\n       applications is selected.\n \n@@ -205,10 +205,10 @@ Advanced features of ``TransactionTestCase``\n     cause unrelated tests to fail. Be careful with tests that use sessions;\n     the default session engine stores them in the database.\n \n-    Since :data:`~django.db.models.signals.post_syncdb` isn't emitted after\n+    Since :data:`~django.db.models.signals.post_migrate` isn't emitted after\n     flushing the database, its state after a ``TransactionTestCase`` isn't the\n     same as after a ``TestCase``: it's missing the rows created by listeners\n-    to :data:`~django.db.models.signals.post_syncdb`. Considering the\n+    to :data:`~django.db.models.signals.post_migrate`. Considering the\n     :ref:`order in which tests are executed <order-of-tests>`, this isn't an\n     issue, provided either all ``TransactionTestCase`` in a given test suite\n     declare ``available_apps``, or none of them.\n@@ -276,7 +276,7 @@ testing behavior. This behavior involves:\n \n #. Creating the test databases.\n \n-#. Running ``syncdb`` to install models and initial data into the test\n+#. Running ``migrate`` to install models and initial data into the test\n    databases.\n \n #. Running the tests that were found.\n@@ -467,7 +467,7 @@ can be useful during testing.\n \n .. function:: create_test_db([verbosity=1, autoclobber=False])\n \n-    Creates a new test database and runs ``syncdb`` against it.\n+    Creates a new test database and runs ``migrate`` against it.\n \n     ``verbosity`` has the same behavior as in ``run_tests()``.\n \ndiff --git a/docs/topics/testing/overview.txt b/docs/topics/testing/overview.txt\nindex 89b43c42c5a7..89b38f7573a4 100644\n--- a/docs/topics/testing/overview.txt\n+++ b/docs/topics/testing/overview.txt\n@@ -1170,9 +1170,9 @@ documentation<dumpdata>` for more details.\n \n .. note::\n \n-    If you've ever run :djadmin:`manage.py syncdb<syncdb>`, you've\n+    If you've ever run :djadmin:`manage.py migrate<migrate>`, you've\n     already used a fixture without even knowing it! When you call\n-    :djadmin:`syncdb` in the database for the first time, Django\n+    :djadmin:`migrate` in the database for the first time, Django\n     installs a fixture called ``initial_data``. This gives you a way\n     of populating a new database with any initial data, such as a\n     default set of categories.\ndiff --git a/tests/app_cache/__init__.py b/tests/app_cache/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/app_cache/models.py b/tests/app_cache/models.py\nnew file mode 100644\nindex 000000000000..1b4d33c2f992\n--- /dev/null\n+++ b/tests/app_cache/models.py\n@@ -0,0 +1,17 @@\n+from django.db import models\n+from django.db.models.loading import BaseAppCache\n+\n+# We're testing app cache presence on load, so this is handy.\n+\n+new_app_cache = BaseAppCache()\n+\n+\n+class TotallyNormal(models.Model):\n+    name = models.CharField(max_length=255)\n+\n+\n+class SoAlternative(models.Model):\n+    name = models.CharField(max_length=255)\n+\n+    class Meta:\n+        app_cache = new_app_cache\ndiff --git a/tests/app_cache/tests.py b/tests/app_cache/tests.py\nnew file mode 100644\nindex 000000000000..b72b862de3c6\n--- /dev/null\n+++ b/tests/app_cache/tests.py\n@@ -0,0 +1,44 @@\n+from __future__ import absolute_import\n+from django.test import TestCase\n+from django.db.models.loading import cache, BaseAppCache\n+from django.db import models\n+from .models import TotallyNormal, SoAlternative, new_app_cache\n+\n+\n+class AppCacheTests(TestCase):\n+    \"\"\"\n+    Tests the AppCache borg and non-borg versions\n+    \"\"\"\n+\n+    def test_models_py(self):\n+        \"\"\"\n+        Tests that the models in the models.py file were loaded correctly.\n+        \"\"\"\n+        self.assertEqual(cache.get_model(\"app_cache\", \"TotallyNormal\"), TotallyNormal)\n+        self.assertEqual(cache.get_model(\"app_cache\", \"SoAlternative\"), None)\n+\n+        self.assertEqual(new_app_cache.get_model(\"app_cache\", \"TotallyNormal\"), None)\n+        self.assertEqual(new_app_cache.get_model(\"app_cache\", \"SoAlternative\"), SoAlternative)\n+\n+    def test_dynamic_load(self):\n+        \"\"\"\n+        Makes a new model at runtime and ensures it goes into the right place.\n+        \"\"\"\n+        old_models = cache.get_models(cache.get_app(\"app_cache\"))\n+        # Construct a new model in a new app cache\n+        body = {}\n+        new_app_cache = BaseAppCache()\n+        meta_contents = {\n+            'app_label': \"app_cache\",\n+            'app_cache': new_app_cache,\n+        }\n+        meta = type(\"Meta\", tuple(), meta_contents)\n+        body['Meta'] = meta\n+        body['__module__'] = TotallyNormal.__module__\n+        temp_model = type(\"SouthPonies\", (models.Model,), body)\n+        # Make sure it appeared in the right place!\n+        self.assertEqual(\n+            old_models,\n+            cache.get_models(cache.get_app(\"app_cache\")),\n+        )\n+        self.assertEqual(new_app_cache.get_model(\"app_cache\", \"SouthPonies\"), temp_model)\ndiff --git a/tests/cache/tests.py b/tests/cache/tests.py\nindex a2256665210e..287a0786175e 100644\n--- a/tests/cache/tests.py\n+++ b/tests/cache/tests.py\n@@ -895,7 +895,7 @@ def db_for_write(self, model, **hints):\n         if model._meta.app_label == 'django_cache':\n             return 'other'\n \n-    def allow_syncdb(self, db, model):\n+    def allow_migrate(self, db, model):\n         if model._meta.app_label == 'django_cache':\n             return db == 'other'\n \ndiff --git a/tests/migrations/__init__.py b/tests/migrations/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/migrations/models.py b/tests/migrations/models.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/migrations/test_autodetector.py b/tests/migrations/test_autodetector.py\nnew file mode 100644\nindex 000000000000..de3b156ba6c3\n--- /dev/null\n+++ b/tests/migrations/test_autodetector.py\n@@ -0,0 +1,274 @@\n+# encoding: utf8\n+from django.test import TestCase\n+from django.db.migrations.autodetector import MigrationAutodetector, MigrationQuestioner\n+from django.db.migrations.state import ProjectState, ModelState\n+from django.db.migrations.graph import MigrationGraph\n+from django.db import models\n+\n+\n+class AutodetectorTests(TestCase):\n+    \"\"\"\n+    Tests the migration autodetector.\n+    \"\"\"\n+\n+    author_empty = ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))])\n+    author_name = ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=200))])\n+    author_name_longer = ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=400))])\n+    author_name_renamed = ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"names\", models.CharField(max_length=200))])\n+    author_with_book = ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=200)), (\"book\", models.ForeignKey(\"otherapp.Book\"))])\n+    other_pony = ModelState(\"otherapp\", \"Pony\", [(\"id\", models.AutoField(primary_key=True))])\n+    other_stable = ModelState(\"otherapp\", \"Stable\", [(\"id\", models.AutoField(primary_key=True))])\n+    third_thing = ModelState(\"thirdapp\", \"Thing\", [(\"id\", models.AutoField(primary_key=True))])\n+    book = ModelState(\"otherapp\", \"Book\", [(\"id\", models.AutoField(primary_key=True)), (\"author\", models.ForeignKey(\"testapp.Author\")), (\"title\", models.CharField(max_length=200))])\n+    book_unique = ModelState(\"otherapp\", \"Book\", [(\"id\", models.AutoField(primary_key=True)), (\"author\", models.ForeignKey(\"testapp.Author\")), (\"title\", models.CharField(max_length=200))], {\"unique_together\": [(\"author\", \"title\")]})\n+    book_unique_2 = ModelState(\"otherapp\", \"Book\", [(\"id\", models.AutoField(primary_key=True)), (\"author\", models.ForeignKey(\"testapp.Author\")), (\"title\", models.CharField(max_length=200))], {\"unique_together\": [(\"title\", \"author\")]})\n+    edition = ModelState(\"thirdapp\", \"Edition\", [(\"id\", models.AutoField(primary_key=True)), (\"book\", models.ForeignKey(\"otherapp.Book\"))])\n+\n+    def make_project_state(self, model_states):\n+        \"Shortcut to make ProjectStates from lists of predefined models\"\n+        project_state = ProjectState()\n+        for model_state in model_states:\n+            project_state.add_model_state(model_state.clone())\n+        return project_state\n+\n+    def test_arrange_for_graph(self):\n+        \"Tests auto-naming of migrations for graph matching.\"\n+        # Make a fake graph\n+        graph = MigrationGraph()\n+        graph.add_node((\"testapp\", \"0001_initial\"), None)\n+        graph.add_node((\"testapp\", \"0002_foobar\"), None)\n+        graph.add_node((\"otherapp\", \"0001_initial\"), None)\n+        graph.add_dependency((\"testapp\", \"0002_foobar\"), (\"testapp\", \"0001_initial\"))\n+        graph.add_dependency((\"testapp\", \"0002_foobar\"), (\"otherapp\", \"0001_initial\"))\n+        # Use project state to make a new migration change set\n+        before = self.make_project_state([])\n+        after = self.make_project_state([self.author_empty, self.other_pony, self.other_stable])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Run through arrange_for_graph\n+        changes = autodetector._arrange_for_graph(changes, graph)\n+        # Make sure there's a new name, deps match, etc.\n+        self.assertEqual(changes[\"testapp\"][0].name, \"0003_author\")\n+        self.assertEqual(changes[\"testapp\"][0].dependencies, [(\"testapp\", \"0002_foobar\")])\n+        self.assertEqual(changes[\"otherapp\"][0].name, \"0002_pony_stable\")\n+        self.assertEqual(changes[\"otherapp\"][0].dependencies, [(\"otherapp\", \"0001_initial\")])\n+\n+    def test_trim_apps(self):\n+        \"Tests that trim does not remove dependencies but does remove unwanted apps\"\n+        # Use project state to make a new migration change set\n+        before = self.make_project_state([])\n+        after = self.make_project_state([self.author_empty, self.other_pony, self.other_stable, self.third_thing])\n+        autodetector = MigrationAutodetector(before, after, MigrationQuestioner({\"ask_initial\": True}))\n+        changes = autodetector._detect_changes()\n+        # Run through arrange_for_graph\n+        graph = MigrationGraph()\n+        changes = autodetector._arrange_for_graph(changes, graph)\n+        changes[\"testapp\"][0].dependencies.append((\"otherapp\", \"0001_initial\"))\n+        changes = autodetector._trim_to_apps(changes, set([\"testapp\"]))\n+        # Make sure there's the right set of migrations\n+        self.assertEqual(changes[\"testapp\"][0].name, \"0001_initial\")\n+        self.assertEqual(changes[\"otherapp\"][0].name, \"0001_initial\")\n+        self.assertNotIn(\"thirdapp\", changes)\n+\n+    def test_new_model(self):\n+        \"Tests autodetection of new models\"\n+        # Make state\n+        before = self.make_project_state([])\n+        after = self.make_project_state([self.author_empty])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['testapp']), 1)\n+        # Right number of actions?\n+        migration = changes['testapp'][0]\n+        self.assertEqual(len(migration.operations), 1)\n+        # Right action?\n+        action = migration.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"CreateModel\")\n+        self.assertEqual(action.name, \"Author\")\n+\n+    def test_old_model(self):\n+        \"Tests deletion of old models\"\n+        # Make state\n+        before = self.make_project_state([self.author_empty])\n+        after = self.make_project_state([])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['testapp']), 1)\n+        # Right number of actions?\n+        migration = changes['testapp'][0]\n+        self.assertEqual(len(migration.operations), 1)\n+        # Right action?\n+        action = migration.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"DeleteModel\")\n+        self.assertEqual(action.name, \"Author\")\n+\n+    def test_add_field(self):\n+        \"Tests autodetection of new fields\"\n+        # Make state\n+        before = self.make_project_state([self.author_empty])\n+        after = self.make_project_state([self.author_name])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['testapp']), 1)\n+        # Right number of actions?\n+        migration = changes['testapp'][0]\n+        self.assertEqual(len(migration.operations), 1)\n+        # Right action?\n+        action = migration.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"AddField\")\n+        self.assertEqual(action.name, \"name\")\n+\n+    def test_remove_field(self):\n+        \"Tests autodetection of removed fields\"\n+        # Make state\n+        before = self.make_project_state([self.author_name])\n+        after = self.make_project_state([self.author_empty])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['testapp']), 1)\n+        # Right number of actions?\n+        migration = changes['testapp'][0]\n+        self.assertEqual(len(migration.operations), 1)\n+        # Right action?\n+        action = migration.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"RemoveField\")\n+        self.assertEqual(action.name, \"name\")\n+\n+    def test_alter_field(self):\n+        \"Tests autodetection of new fields\"\n+        # Make state\n+        before = self.make_project_state([self.author_name])\n+        after = self.make_project_state([self.author_name_longer])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['testapp']), 1)\n+        # Right number of actions?\n+        migration = changes['testapp'][0]\n+        self.assertEqual(len(migration.operations), 1)\n+        # Right action?\n+        action = migration.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"AlterField\")\n+        self.assertEqual(action.name, \"name\")\n+\n+    def test_rename_field(self):\n+        \"Tests autodetection of renamed fields\"\n+        # Make state\n+        before = self.make_project_state([self.author_name])\n+        after = self.make_project_state([self.author_name_renamed])\n+        autodetector = MigrationAutodetector(before, after, MigrationQuestioner({\"ask_rename\": True}))\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['testapp']), 1)\n+        # Right number of actions?\n+        migration = changes['testapp'][0]\n+        self.assertEqual(len(migration.operations), 1)\n+        # Right action?\n+        action = migration.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"RenameField\")\n+        self.assertEqual(action.old_name, \"name\")\n+        self.assertEqual(action.new_name, \"names\")\n+\n+    def test_fk_dependency(self):\n+        \"Tests that having a ForeignKey automatically adds a dependency\"\n+        # Make state\n+        before = self.make_project_state([])\n+        after = self.make_project_state([self.author_name, self.book, self.edition])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['testapp']), 1)\n+        self.assertEqual(len(changes['otherapp']), 1)\n+        self.assertEqual(len(changes['thirdapp']), 1)\n+        # Right number of actions?\n+        migration1 = changes['testapp'][0]\n+        self.assertEqual(len(migration1.operations), 1)\n+        migration2 = changes['otherapp'][0]\n+        self.assertEqual(len(migration2.operations), 1)\n+        migration3 = changes['thirdapp'][0]\n+        self.assertEqual(len(migration3.operations), 1)\n+        # Right actions?\n+        action = migration1.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"CreateModel\")\n+        action = migration2.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"CreateModel\")\n+        action = migration3.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"CreateModel\")\n+        # Right dependencies?\n+        self.assertEqual(migration1.dependencies, [])\n+        self.assertEqual(migration2.dependencies, [(\"testapp\", \"auto_1\")])\n+        self.assertEqual(migration3.dependencies, [(\"otherapp\", \"auto_1\")])\n+\n+    def test_circular_fk_dependency(self):\n+        \"\"\"\n+        Tests that having a circular ForeignKey dependency automatically\n+        resolves the situation into 2 migrations on one side and 1 on the other.\n+        \"\"\"\n+        # Make state\n+        before = self.make_project_state([])\n+        after = self.make_project_state([self.author_with_book, self.book])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['testapp']), 1)\n+        self.assertEqual(len(changes['otherapp']), 2)\n+        # Right number of actions?\n+        migration1 = changes['testapp'][0]\n+        self.assertEqual(len(migration1.operations), 1)\n+        migration2 = changes['otherapp'][0]\n+        self.assertEqual(len(migration2.operations), 1)\n+        migration3 = changes['otherapp'][1]\n+        self.assertEqual(len(migration2.operations), 1)\n+        # Right actions?\n+        action = migration1.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"CreateModel\")\n+        action = migration2.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"CreateModel\")\n+        self.assertEqual(len(action.fields), 2)\n+        action = migration3.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"AddField\")\n+        self.assertEqual(action.name, \"author\")\n+        # Right dependencies?\n+        self.assertEqual(migration1.dependencies, [(\"otherapp\", \"auto_1\")])\n+        self.assertEqual(migration2.dependencies, [])\n+        self.assertEqual(set(migration3.dependencies), set([(\"otherapp\", \"auto_1\"), (\"testapp\", \"auto_1\")]))\n+\n+    def test_unique_together(self):\n+        \"Tests unique_together detection\"\n+        # Make state\n+        before = self.make_project_state([self.author_empty, self.book])\n+        after = self.make_project_state([self.author_empty, self.book_unique])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['otherapp']), 1)\n+        # Right number of actions?\n+        migration = changes['otherapp'][0]\n+        self.assertEqual(len(migration.operations), 1)\n+        # Right action?\n+        action = migration.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"AlterUniqueTogether\")\n+        self.assertEqual(action.name, \"book\")\n+        self.assertEqual(action.unique_together, set([(\"author\", \"title\")]))\n+\n+    def test_unique_together_ordering(self):\n+        \"Tests that unique_together also triggers on ordering changes\"\n+        # Make state\n+        before = self.make_project_state([self.author_empty, self.book_unique])\n+        after = self.make_project_state([self.author_empty, self.book_unique_2])\n+        autodetector = MigrationAutodetector(before, after)\n+        changes = autodetector._detect_changes()\n+        # Right number of migrations?\n+        self.assertEqual(len(changes['otherapp']), 1)\n+        # Right number of actions?\n+        migration = changes['otherapp'][0]\n+        self.assertEqual(len(migration.operations), 1)\n+        # Right action?\n+        action = migration.operations[0]\n+        self.assertEqual(action.__class__.__name__, \"AlterUniqueTogether\")\n+        self.assertEqual(action.name, \"book\")\n+        self.assertEqual(action.unique_together, set([(\"title\", \"author\")]))\ndiff --git a/tests/migrations/test_base.py b/tests/migrations/test_base.py\nnew file mode 100644\nindex 000000000000..7ab09b04a5bb\n--- /dev/null\n+++ b/tests/migrations/test_base.py\n@@ -0,0 +1,41 @@\n+from django.test import TransactionTestCase\n+from django.db import connection\n+\n+\n+class MigrationTestBase(TransactionTestCase):\n+    \"\"\"\n+    Contains an extended set of asserts for testing migrations and schema operations.\n+    \"\"\"\n+\n+    available_apps = [\"migrations\"]\n+\n+    def assertTableExists(self, table):\n+        self.assertIn(table, connection.introspection.get_table_list(connection.cursor()))\n+\n+    def assertTableNotExists(self, table):\n+        self.assertNotIn(table, connection.introspection.get_table_list(connection.cursor()))\n+\n+    def assertColumnExists(self, table, column):\n+        self.assertIn(column, [c.name for c in connection.introspection.get_table_description(connection.cursor(), table)])\n+\n+    def assertColumnNotExists(self, table, column):\n+        self.assertNotIn(column, [c.name for c in connection.introspection.get_table_description(connection.cursor(), table)])\n+\n+    def assertColumnNull(self, table, column):\n+        self.assertEqual([c.null_ok for c in connection.introspection.get_table_description(connection.cursor(), table) if c.name == column][0], True)\n+\n+    def assertColumnNotNull(self, table, column):\n+        self.assertEqual([c.null_ok for c in connection.introspection.get_table_description(connection.cursor(), table) if c.name == column][0], False)\n+\n+    def assertIndexExists(self, table, columns, value=True):\n+        self.assertEqual(\n+            value,\n+            any(\n+                c[\"index\"]\n+                for c in connection.introspection.get_constraints(connection.cursor(), table).values()\n+                if c['columns'] == list(columns)\n+            ),\n+        )\n+\n+    def assertIndexNotExists(self, table, columns):\n+        return self.assertIndexExists(table, columns, False)\ndiff --git a/tests/migrations/test_commands.py b/tests/migrations/test_commands.py\nnew file mode 100644\nindex 000000000000..d775d1eba755\n--- /dev/null\n+++ b/tests/migrations/test_commands.py\n@@ -0,0 +1,37 @@\n+from django.core.management import call_command\n+from django.test.utils import override_settings\n+from .test_base import MigrationTestBase\n+\n+\n+class CommandTests(MigrationTestBase):\n+    \"\"\"\n+    Tests running the commands (migrate, makemigrations).\n+    \"\"\"\n+\n+    @override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"})\n+    def test_migrate(self):\n+        \"\"\"\n+        Tests basic usage of the migrate command.\n+        \"\"\"\n+        # Make sure no tables are created\n+        self.assertTableNotExists(\"migrations_author\")\n+        self.assertTableNotExists(\"migrations_tribble\")\n+        self.assertTableNotExists(\"migrations_book\")\n+        # Run the migrations to 0001 only\n+        call_command(\"migrate\", \"migrations\", \"0001\", verbosity=0)\n+        # Make sure the right tables exist\n+        self.assertTableExists(\"migrations_author\")\n+        self.assertTableExists(\"migrations_tribble\")\n+        self.assertTableNotExists(\"migrations_book\")\n+        # Run migrations all the way\n+        call_command(\"migrate\", verbosity=0)\n+        # Make sure the right tables exist\n+        self.assertTableExists(\"migrations_author\")\n+        self.assertTableNotExists(\"migrations_tribble\")\n+        self.assertTableExists(\"migrations_book\")\n+        # Unmigrate everything\n+        call_command(\"migrate\", \"migrations\", \"zero\", verbosity=0)\n+        # Make sure it's all gone\n+        self.assertTableNotExists(\"migrations_author\")\n+        self.assertTableNotExists(\"migrations_tribble\")\n+        self.assertTableNotExists(\"migrations_book\")\ndiff --git a/tests/migrations/test_executor.py b/tests/migrations/test_executor.py\nnew file mode 100644\nindex 000000000000..dbdea900a508\n--- /dev/null\n+++ b/tests/migrations/test_executor.py\n@@ -0,0 +1,77 @@\n+from django.test import TransactionTestCase\n+from django.test.utils import override_settings\n+from django.db import connection\n+from django.db.migrations.executor import MigrationExecutor\n+\n+\n+class ExecutorTests(TransactionTestCase):\n+    \"\"\"\n+    Tests the migration executor (full end-to-end running).\n+\n+    Bear in mind that if these are failing you should fix the other\n+    test failures first, as they may be propagating into here.\n+    \"\"\"\n+\n+    available_apps = [\"migrations\", \"django.contrib.sessions\"]\n+\n+    @override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"})\n+    def test_run(self):\n+        \"\"\"\n+        Tests running a simple set of migrations.\n+        \"\"\"\n+        executor = MigrationExecutor(connection)\n+        executor.recorder.flush()\n+        # Let's look at the plan first and make sure it's up to scratch\n+        plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n+        self.assertEqual(\n+            plan,\n+            [\n+                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n+                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n+            ],\n+        )\n+        # Were the tables there before?\n+        self.assertNotIn(\"migrations_author\", connection.introspection.get_table_list(connection.cursor()))\n+        self.assertNotIn(\"migrations_book\", connection.introspection.get_table_list(connection.cursor()))\n+        # Alright, let's try running it\n+        executor.migrate([(\"migrations\", \"0002_second\")])\n+        # Are the tables there now?\n+        self.assertIn(\"migrations_author\", connection.introspection.get_table_list(connection.cursor()))\n+        self.assertIn(\"migrations_book\", connection.introspection.get_table_list(connection.cursor()))\n+        # Alright, let's undo what we did\n+        executor.migrate([(\"migrations\", None)])\n+        # Are the tables gone?\n+        self.assertNotIn(\"migrations_author\", connection.introspection.get_table_list(connection.cursor()))\n+        self.assertNotIn(\"migrations_book\", connection.introspection.get_table_list(connection.cursor()))\n+\n+    @override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\", \"sessions\": \"migrations.test_migrations_2\"})\n+    def test_empty_plan(self):\n+        \"\"\"\n+        Tests that re-planning a full migration of a fully-migrated set doesn't\n+        perform spurious unmigrations and remigrations.\n+\n+        There was previously a bug where the executor just always performed the\n+        backwards plan for applied migrations - which even for the most recent\n+        migration in an app, might include other, dependent apps, and these\n+        were being unmigrated.\n+        \"\"\"\n+        # Make the initial plan, check it\n+        # We use 'sessions' here as the second app as it's always present\n+        # in INSTALLED_APPS, so we can happily assign it test migrations.\n+        executor = MigrationExecutor(connection)\n+        plan = executor.migration_plan([(\"migrations\", \"0002_second\"), (\"sessions\", \"0001_initial\")])\n+        self.assertEqual(\n+            plan,\n+            [\n+                (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n+                (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n+                (executor.loader.graph.nodes[\"sessions\", \"0001_initial\"], False),\n+            ],\n+        )\n+        # Fake-apply all migrations\n+        executor.migrate([(\"migrations\", \"0002_second\"), (\"sessions\", \"0001_initial\")], fake=True)\n+        # Now plan a second time and make sure it's empty\n+        plan = executor.migration_plan([(\"migrations\", \"0002_second\"), (\"sessions\", \"0001_initial\")])\n+        self.assertEqual(plan, [])\n+        # Erase all the fake records\n+        executor.recorder.flush()\ndiff --git a/tests/migrations/test_graph.py b/tests/migrations/test_graph.py\nnew file mode 100644\nindex 000000000000..e3d5a28283a7\n--- /dev/null\n+++ b/tests/migrations/test_graph.py\n@@ -0,0 +1,135 @@\n+from django.test import TestCase\n+from django.db.migrations.graph import MigrationGraph, CircularDependencyError\n+\n+\n+class GraphTests(TestCase):\n+    \"\"\"\n+    Tests the digraph structure.\n+    \"\"\"\n+\n+    def test_simple_graph(self):\n+        \"\"\"\n+        Tests a basic dependency graph:\n+\n+        app_a:  0001 <-- 0002 <--- 0003 <-- 0004\n+                                 /\n+        app_b:  0001 <-- 0002 <-/\n+        \"\"\"\n+        # Build graph\n+        graph = MigrationGraph()\n+        graph.add_node((\"app_a\", \"0001\"), None)\n+        graph.add_node((\"app_a\", \"0002\"), None)\n+        graph.add_node((\"app_a\", \"0003\"), None)\n+        graph.add_node((\"app_a\", \"0004\"), None)\n+        graph.add_node((\"app_b\", \"0001\"), None)\n+        graph.add_node((\"app_b\", \"0002\"), None)\n+        graph.add_dependency((\"app_a\", \"0004\"), (\"app_a\", \"0003\"))\n+        graph.add_dependency((\"app_a\", \"0003\"), (\"app_a\", \"0002\"))\n+        graph.add_dependency((\"app_a\", \"0002\"), (\"app_a\", \"0001\"))\n+        graph.add_dependency((\"app_a\", \"0003\"), (\"app_b\", \"0002\"))\n+        graph.add_dependency((\"app_b\", \"0002\"), (\"app_b\", \"0001\"))\n+        # Test root migration case\n+        self.assertEqual(\n+            graph.forwards_plan((\"app_a\", \"0001\")),\n+            [('app_a', '0001')],\n+        )\n+        # Test branch B only\n+        self.assertEqual(\n+            graph.forwards_plan((\"app_b\", \"0002\")),\n+            [(\"app_b\", \"0001\"), (\"app_b\", \"0002\")],\n+        )\n+        # Test whole graph\n+        self.assertEqual(\n+            graph.forwards_plan((\"app_a\", \"0004\")),\n+            [('app_b', '0001'), ('app_b', '0002'), ('app_a', '0001'), ('app_a', '0002'), ('app_a', '0003'), ('app_a', '0004')],\n+        )\n+        # Test reverse to b:0002\n+        self.assertEqual(\n+            graph.backwards_plan((\"app_b\", \"0002\")),\n+            [('app_a', '0004'), ('app_a', '0003'), ('app_b', '0002')],\n+        )\n+        # Test roots and leaves\n+        self.assertEqual(\n+            graph.root_nodes(),\n+            set([('app_a', '0001'), ('app_b', '0001')]),\n+        )\n+        self.assertEqual(\n+            graph.leaf_nodes(),\n+            set([('app_a', '0004'), ('app_b', '0002')]),\n+        )\n+\n+    def test_complex_graph(self):\n+        \"\"\"\n+        Tests a complex dependency graph:\n+\n+        app_a:  0001 <-- 0002 <--- 0003 <-- 0004\n+                      \\        \\ /         /\n+        app_b:  0001 <-\\ 0002 <-X         /\n+                      \\          \\       /\n+        app_c:         \\ 0001 <-- 0002 <-\n+        \"\"\"\n+        # Build graph\n+        graph = MigrationGraph()\n+        graph.add_node((\"app_a\", \"0001\"), None)\n+        graph.add_node((\"app_a\", \"0002\"), None)\n+        graph.add_node((\"app_a\", \"0003\"), None)\n+        graph.add_node((\"app_a\", \"0004\"), None)\n+        graph.add_node((\"app_b\", \"0001\"), None)\n+        graph.add_node((\"app_b\", \"0002\"), None)\n+        graph.add_node((\"app_c\", \"0001\"), None)\n+        graph.add_node((\"app_c\", \"0002\"), None)\n+        graph.add_dependency((\"app_a\", \"0004\"), (\"app_a\", \"0003\"))\n+        graph.add_dependency((\"app_a\", \"0003\"), (\"app_a\", \"0002\"))\n+        graph.add_dependency((\"app_a\", \"0002\"), (\"app_a\", \"0001\"))\n+        graph.add_dependency((\"app_a\", \"0003\"), (\"app_b\", \"0002\"))\n+        graph.add_dependency((\"app_b\", \"0002\"), (\"app_b\", \"0001\"))\n+        graph.add_dependency((\"app_a\", \"0004\"), (\"app_c\", \"0002\"))\n+        graph.add_dependency((\"app_c\", \"0002\"), (\"app_c\", \"0001\"))\n+        graph.add_dependency((\"app_c\", \"0001\"), (\"app_b\", \"0001\"))\n+        graph.add_dependency((\"app_c\", \"0002\"), (\"app_a\", \"0002\"))\n+        # Test branch C only\n+        self.assertEqual(\n+            graph.forwards_plan((\"app_c\", \"0002\")),\n+            [('app_b', '0001'), ('app_c', '0001'), ('app_a', '0001'), ('app_a', '0002'), ('app_c', '0002')],\n+        )\n+        # Test whole graph\n+        self.assertEqual(\n+            graph.forwards_plan((\"app_a\", \"0004\")),\n+            [('app_b', '0001'), ('app_c', '0001'), ('app_a', '0001'), ('app_a', '0002'), ('app_c', '0002'), ('app_b', '0002'), ('app_a', '0003'), ('app_a', '0004')],\n+        )\n+        # Test reverse to b:0001\n+        self.assertEqual(\n+            graph.backwards_plan((\"app_b\", \"0001\")),\n+            [('app_a', '0004'), ('app_c', '0002'), ('app_c', '0001'), ('app_a', '0003'), ('app_b', '0002'), ('app_b', '0001')],\n+        )\n+        # Test roots and leaves\n+        self.assertEqual(\n+            graph.root_nodes(),\n+            set([('app_a', '0001'), ('app_b', '0001'), ('app_c', '0001')]),\n+        )\n+        self.assertEqual(\n+            graph.leaf_nodes(),\n+            set([('app_a', '0004'), ('app_b', '0002'), ('app_c', '0002')]),\n+        )\n+\n+    def test_circular_graph(self):\n+        \"\"\"\n+        Tests a circular dependency graph.\n+        \"\"\"\n+        # Build graph\n+        graph = MigrationGraph()\n+        graph.add_node((\"app_a\", \"0001\"), None)\n+        graph.add_node((\"app_a\", \"0002\"), None)\n+        graph.add_node((\"app_a\", \"0003\"), None)\n+        graph.add_node((\"app_b\", \"0001\"), None)\n+        graph.add_node((\"app_b\", \"0002\"), None)\n+        graph.add_dependency((\"app_a\", \"0003\"), (\"app_a\", \"0002\"))\n+        graph.add_dependency((\"app_a\", \"0002\"), (\"app_a\", \"0001\"))\n+        graph.add_dependency((\"app_a\", \"0001\"), (\"app_b\", \"0002\"))\n+        graph.add_dependency((\"app_b\", \"0002\"), (\"app_b\", \"0001\"))\n+        graph.add_dependency((\"app_b\", \"0001\"), (\"app_a\", \"0003\"))\n+        # Test whole graph\n+        self.assertRaises(\n+            CircularDependencyError,\n+            graph.forwards_plan, (\"app_a\", \"0003\"),\n+        )\ndiff --git a/tests/migrations/test_loader.py b/tests/migrations/test_loader.py\nnew file mode 100644\nindex 000000000000..b9ad9726ae31\n--- /dev/null\n+++ b/tests/migrations/test_loader.py\n@@ -0,0 +1,79 @@\n+from django.test import TestCase\n+from django.test.utils import override_settings\n+from django.db import connection\n+from django.db.migrations.loader import MigrationLoader, AmbiguityError\n+from django.db.migrations.recorder import MigrationRecorder\n+\n+\n+class RecorderTests(TestCase):\n+    \"\"\"\n+    Tests recording migrations as applied or not.\n+    \"\"\"\n+\n+    def test_apply(self):\n+        \"\"\"\n+        Tests marking migrations as applied/unapplied.\n+        \"\"\"\n+        recorder = MigrationRecorder(connection)\n+        self.assertEqual(\n+            recorder.applied_migrations(),\n+            set(),\n+        )\n+        recorder.record_applied(\"myapp\", \"0432_ponies\")\n+        self.assertEqual(\n+            recorder.applied_migrations(),\n+            set([(\"myapp\", \"0432_ponies\")]),\n+        )\n+        recorder.record_unapplied(\"myapp\", \"0432_ponies\")\n+        self.assertEqual(\n+            recorder.applied_migrations(),\n+            set(),\n+        )\n+\n+\n+class LoaderTests(TestCase):\n+    \"\"\"\n+    Tests the disk and database loader, and running through migrations\n+    in memory.\n+    \"\"\"\n+\n+    @override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"})\n+    def test_load(self):\n+        \"\"\"\n+        Makes sure the loader can load the migrations for the test apps,\n+        and then render them out to a new AppCache.\n+        \"\"\"\n+        # Load and test the plan\n+        migration_loader = MigrationLoader(connection)\n+        self.assertEqual(\n+            migration_loader.graph.forwards_plan((\"migrations\", \"0002_second\")),\n+            [(\"migrations\", \"0001_initial\"), (\"migrations\", \"0002_second\")],\n+        )\n+        # Now render it out!\n+        project_state = migration_loader.graph.project_state((\"migrations\", \"0002_second\"))\n+        self.assertEqual(len(project_state.models), 2)\n+\n+        author_state = project_state.models[\"migrations\", \"author\"]\n+        self.assertEqual(\n+            [x for x, y in author_state.fields],\n+            [\"id\", \"name\", \"slug\", \"age\", \"rating\"]\n+        )\n+\n+        book_state = project_state.models[\"migrations\", \"book\"]\n+        self.assertEqual(\n+            [x for x, y in book_state.fields],\n+            [\"id\", \"author\"]\n+        )\n+\n+    @override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"})\n+    def test_name_match(self):\n+        \"Tests prefix name matching\"\n+        migration_loader = MigrationLoader(connection)\n+        self.assertEqual(\n+            migration_loader.get_migration_by_prefix(\"migrations\", \"0001\").name,\n+            \"0001_initial\",\n+        )\n+        with self.assertRaises(AmbiguityError):\n+            migration_loader.get_migration_by_prefix(\"migrations\", \"0\")\n+        with self.assertRaises(KeyError):\n+            migration_loader.get_migration_by_prefix(\"migrations\", \"blarg\")\ndiff --git a/tests/migrations/test_migrations/0001_initial.py b/tests/migrations/test_migrations/0001_initial.py\nnew file mode 100644\nindex 000000000000..f20bac8aeca9\n--- /dev/null\n+++ b/tests/migrations/test_migrations/0001_initial.py\n@@ -0,0 +1,27 @@\n+from django.db import migrations, models\n+\n+\n+class Migration(migrations.Migration):\n+    \n+    operations = [\n+\n+        migrations.CreateModel(\n+            \"Author\",\n+            [\n+                (\"id\", models.AutoField(primary_key=True)),\n+                (\"name\", models.CharField(max_length=255)),\n+                (\"slug\", models.SlugField(null=True)),\n+                (\"age\", models.IntegerField(default=0)),\n+                (\"silly_field\", models.BooleanField(default=False)),\n+            ],\n+        ),\n+\n+        migrations.CreateModel(\n+            \"Tribble\",\n+            [\n+                (\"id\", models.AutoField(primary_key=True)),\n+                (\"fluffy\", models.BooleanField(default=True)),\n+            ],\n+        )\n+        \n+    ]\ndiff --git a/tests/migrations/test_migrations/0002_second.py b/tests/migrations/test_migrations/0002_second.py\nnew file mode 100644\nindex 000000000000..ace9a8334705\n--- /dev/null\n+++ b/tests/migrations/test_migrations/0002_second.py\n@@ -0,0 +1,24 @@\n+from django.db import migrations, models\n+\n+\n+class Migration(migrations.Migration):\n+\n+    dependencies = [(\"migrations\", \"0001_initial\")]\n+\n+    operations = [\n+\n+        migrations.DeleteModel(\"Tribble\"),\n+\n+        migrations.RemoveField(\"Author\", \"silly_field\"),\n+\n+        migrations.AddField(\"Author\", \"rating\", models.IntegerField(default=0)),\n+\n+        migrations.CreateModel(\n+            \"Book\",\n+            [\n+                (\"id\", models.AutoField(primary_key=True)),\n+                (\"author\", models.ForeignKey(\"migrations.Author\", null=True)),\n+            ],\n+        )\n+\n+    ]\ndiff --git a/tests/migrations/test_migrations/__init__.py b/tests/migrations/test_migrations/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/migrations/test_migrations_2/0001_initial.py b/tests/migrations/test_migrations_2/0001_initial.py\nnew file mode 100644\nindex 000000000000..94c4bc074660\n--- /dev/null\n+++ b/tests/migrations/test_migrations_2/0001_initial.py\n@@ -0,0 +1,21 @@\n+from django.db import migrations, models\n+\n+\n+class Migration(migrations.Migration):\n+\n+    dependencies = [(\"migrations\", \"0002_second\")]\n+    \n+    operations = [\n+\n+        migrations.CreateModel(\n+            \"OtherAuthor\",\n+            [\n+                (\"id\", models.AutoField(primary_key=True)),\n+                (\"name\", models.CharField(max_length=255)),\n+                (\"slug\", models.SlugField(null=True)),\n+                (\"age\", models.IntegerField(default=0)),\n+                (\"silly_field\", models.BooleanField(default=False)),\n+            ],\n+        ),\n+        \n+    ]\ndiff --git a/tests/migrations/test_migrations_2/__init__.py b/tests/migrations/test_migrations_2/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/migrations/test_operations.py b/tests/migrations/test_operations.py\nnew file mode 100644\nindex 000000000000..1bc4a42d7eba\n--- /dev/null\n+++ b/tests/migrations/test_operations.py\n@@ -0,0 +1,327 @@\n+from django.db import connection, models, migrations, router\n+from django.db.transaction import atomic\n+from django.db.utils import IntegrityError\n+from django.db.migrations.state import ProjectState\n+from .test_base import MigrationTestBase\n+\n+\n+class OperationTests(MigrationTestBase):\n+    \"\"\"\n+    Tests running the operations and making sure they do what they say they do.\n+    Each test looks at their state changing, and then their database operation -\n+    both forwards and backwards.\n+    \"\"\"\n+\n+    def set_up_test_model(self, app_label, second_model=False):\n+        \"\"\"\n+        Creates a test model state and database table.\n+        \"\"\"\n+        # Make the \"current\" state\n+        operations = [migrations.CreateModel(\n+            \"Pony\",\n+            [\n+                (\"id\", models.AutoField(primary_key=True)),\n+                (\"pink\", models.IntegerField(default=3)),\n+                (\"weight\", models.FloatField()),\n+            ],\n+        )]\n+        if second_model:\n+            operations.append(migrations.CreateModel(\"Stable\", [(\"id\", models.AutoField(primary_key=True))]))\n+        project_state = ProjectState()\n+        for operation in operations:\n+            operation.state_forwards(app_label, project_state)\n+        # Set up the database\n+        with connection.schema_editor() as editor:\n+            for operation in operations:\n+                operation.database_forwards(app_label, editor, ProjectState(), project_state)\n+        return project_state\n+\n+    def test_create_model(self):\n+        \"\"\"\n+        Tests the CreateModel operation.\n+        Most other tests use this operation as part of setup, so check failures here first.\n+        \"\"\"\n+        operation = migrations.CreateModel(\n+            \"Pony\",\n+            [\n+                (\"id\", models.AutoField(primary_key=True)),\n+                (\"pink\", models.IntegerField(default=1)),\n+            ],\n+        )\n+        # Test the state alteration\n+        project_state = ProjectState()\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_crmo\", new_state)\n+        self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].name, \"Pony\")\n+        self.assertEqual(len(new_state.models[\"test_crmo\", \"pony\"].fields), 2)\n+        # Test the database alteration\n+        self.assertTableNotExists(\"test_crmo_pony\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_crmo\", editor, project_state, new_state)\n+        self.assertTableExists(\"test_crmo_pony\")\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_crmo\", editor, new_state, project_state)\n+        self.assertTableNotExists(\"test_crmo_pony\")\n+        # And deconstruction\n+        definition = operation.deconstruct()\n+        self.assertEqual(definition[0], \"CreateModel\")\n+        self.assertEqual(len(definition[1]), 2)\n+        self.assertEqual(len(definition[2]), 0)\n+        self.assertEqual(definition[1][0], \"Pony\")\n+\n+    def test_delete_model(self):\n+        \"\"\"\n+        Tests the DeleteModel operation.\n+        \"\"\"\n+        project_state = self.set_up_test_model(\"test_dlmo\")\n+        # Test the state alteration\n+        operation = migrations.DeleteModel(\"Pony\")\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_dlmo\", new_state)\n+        self.assertNotIn((\"test_dlmo\", \"pony\"), new_state.models)\n+        # Test the database alteration\n+        self.assertTableExists(\"test_dlmo_pony\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_dlmo\", editor, project_state, new_state)\n+        self.assertTableNotExists(\"test_dlmo_pony\")\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_dlmo\", editor, new_state, project_state)\n+        self.assertTableExists(\"test_dlmo_pony\")\n+\n+    def test_add_field(self):\n+        \"\"\"\n+        Tests the AddField operation.\n+        \"\"\"\n+        project_state = self.set_up_test_model(\"test_adfl\")\n+        # Test the state alteration\n+        operation = migrations.AddField(\"Pony\", \"height\", models.FloatField(null=True))\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_adfl\", new_state)\n+        self.assertEqual(len(new_state.models[\"test_adfl\", \"pony\"].fields), 4)\n+        # Test the database alteration\n+        self.assertColumnNotExists(\"test_adfl_pony\", \"height\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_adfl\", editor, project_state, new_state)\n+        self.assertColumnExists(\"test_adfl_pony\", \"height\")\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_adfl\", editor, new_state, project_state)\n+        self.assertColumnNotExists(\"test_adfl_pony\", \"height\")\n+\n+    def test_add_field_m2m(self):\n+        \"\"\"\n+        Tests the AddField operation with a ManyToManyField.\n+        \"\"\"\n+        project_state = self.set_up_test_model(\"test_adflmm\", second_model=True)\n+        # Test the state alteration\n+        operation = migrations.AddField(\"Pony\", \"stables\", models.ManyToManyField(\"Stable\", related_name=\"ponies\"))\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_adflmm\", new_state)\n+        self.assertEqual(len(new_state.models[\"test_adflmm\", \"pony\"].fields), 4)\n+        # Test the database alteration\n+        self.assertTableNotExists(\"test_adflmm_pony_stables\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_adflmm\", editor, project_state, new_state)\n+        self.assertTableExists(\"test_adflmm_pony_stables\")\n+        self.assertColumnNotExists(\"test_adflmm_pony\", \"stables\")\n+        # Make sure the M2M field actually works\n+        with atomic():\n+            app_cache = new_state.render()\n+            Pony = app_cache.get_model(\"test_adflmm\", \"Pony\")\n+            p = Pony.objects.create(pink=False, weight=4.55)\n+            p.stables.create()\n+            self.assertEqual(p.stables.count(), 1)\n+            p.stables.all().delete()\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_adflmm\", editor, new_state, project_state)\n+        self.assertTableNotExists(\"test_adflmm_pony_stables\")\n+\n+    def test_remove_field(self):\n+        \"\"\"\n+        Tests the RemoveField operation.\n+        \"\"\"\n+        project_state = self.set_up_test_model(\"test_rmfl\")\n+        # Test the state alteration\n+        operation = migrations.RemoveField(\"Pony\", \"pink\")\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_rmfl\", new_state)\n+        self.assertEqual(len(new_state.models[\"test_rmfl\", \"pony\"].fields), 2)\n+        # Test the database alteration\n+        self.assertColumnExists(\"test_rmfl_pony\", \"pink\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_rmfl\", editor, project_state, new_state)\n+        self.assertColumnNotExists(\"test_rmfl_pony\", \"pink\")\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_rmfl\", editor, new_state, project_state)\n+        self.assertColumnExists(\"test_rmfl_pony\", \"pink\")\n+\n+    def test_alter_model_table(self):\n+        \"\"\"\n+        Tests the AlterModelTable operation.\n+        \"\"\"\n+        project_state = self.set_up_test_model(\"test_almota\")\n+        # Test the state alteration\n+        operation = migrations.AlterModelTable(\"Pony\", \"test_almota_pony_2\")\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_almota\", new_state)\n+        self.assertEqual(new_state.models[\"test_almota\", \"pony\"].options[\"db_table\"], \"test_almota_pony_2\")\n+        # Test the database alteration\n+        self.assertTableExists(\"test_almota_pony\")\n+        self.assertTableNotExists(\"test_almota_pony_2\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_almota\", editor, project_state, new_state)\n+        self.assertTableNotExists(\"test_almota_pony\")\n+        self.assertTableExists(\"test_almota_pony_2\")\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_almota\", editor, new_state, project_state)\n+        self.assertTableExists(\"test_almota_pony\")\n+        self.assertTableNotExists(\"test_almota_pony_2\")\n+\n+    def test_alter_field(self):\n+        \"\"\"\n+        Tests the AlterField operation.\n+        \"\"\"\n+        project_state = self.set_up_test_model(\"test_alfl\")\n+        # Test the state alteration\n+        operation = migrations.AlterField(\"Pony\", \"pink\", models.IntegerField(null=True))\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_alfl\", new_state)\n+        self.assertEqual(project_state.models[\"test_alfl\", \"pony\"].get_field_by_name(\"pink\").null, False)\n+        self.assertEqual(new_state.models[\"test_alfl\", \"pony\"].get_field_by_name(\"pink\").null, True)\n+        # Test the database alteration\n+        self.assertColumnNotNull(\"test_alfl_pony\", \"pink\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_alfl\", editor, project_state, new_state)\n+        self.assertColumnNull(\"test_alfl_pony\", \"pink\")\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_alfl\", editor, new_state, project_state)\n+        self.assertColumnNotNull(\"test_alfl_pony\", \"pink\")\n+\n+    def test_rename_field(self):\n+        \"\"\"\n+        Tests the RenameField operation.\n+        \"\"\"\n+        project_state = self.set_up_test_model(\"test_rnfl\")\n+        # Test the state alteration\n+        operation = migrations.RenameField(\"Pony\", \"pink\", \"blue\")\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_rnfl\", new_state)\n+        self.assertIn(\"blue\", [n for n, f in new_state.models[\"test_rnfl\", \"pony\"].fields])\n+        self.assertNotIn(\"pink\", [n for n, f in new_state.models[\"test_rnfl\", \"pony\"].fields])\n+        # Test the database alteration\n+        self.assertColumnExists(\"test_rnfl_pony\", \"pink\")\n+        self.assertColumnNotExists(\"test_rnfl_pony\", \"blue\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_rnfl\", editor, project_state, new_state)\n+        self.assertColumnExists(\"test_rnfl_pony\", \"blue\")\n+        self.assertColumnNotExists(\"test_rnfl_pony\", \"pink\")\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_rnfl\", editor, new_state, project_state)\n+        self.assertColumnExists(\"test_rnfl_pony\", \"pink\")\n+        self.assertColumnNotExists(\"test_rnfl_pony\", \"blue\")\n+\n+    def test_alter_unique_together(self):\n+        \"\"\"\n+        Tests the AlterUniqueTogether operation.\n+        \"\"\"\n+        project_state = self.set_up_test_model(\"test_alunto\")\n+        # Test the state alteration\n+        operation = migrations.AlterUniqueTogether(\"Pony\", [(\"pink\", \"weight\")])\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_alunto\", new_state)\n+        self.assertEqual(len(project_state.models[\"test_alunto\", \"pony\"].options.get(\"unique_together\", set())), 0)\n+        self.assertEqual(len(new_state.models[\"test_alunto\", \"pony\"].options.get(\"unique_together\", set())), 1)\n+        # Make sure we can insert duplicate rows\n+        cursor = connection.cursor()\n+        cursor.execute(\"INSERT INTO test_alunto_pony (id, pink, weight) VALUES (1, 1, 1)\")\n+        cursor.execute(\"INSERT INTO test_alunto_pony (id, pink, weight) VALUES (2, 1, 1)\")\n+        cursor.execute(\"DELETE FROM test_alunto_pony\")\n+        # Test the database alteration\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_alunto\", editor, project_state, new_state)\n+        cursor.execute(\"INSERT INTO test_alunto_pony (id, pink, weight) VALUES (1, 1, 1)\")\n+        with self.assertRaises(IntegrityError):\n+            with atomic():\n+                cursor.execute(\"INSERT INTO test_alunto_pony (id, pink, weight) VALUES (2, 1, 1)\")\n+        cursor.execute(\"DELETE FROM test_alunto_pony\")\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_alunto\", editor, new_state, project_state)\n+        cursor.execute(\"INSERT INTO test_alunto_pony (id, pink, weight) VALUES (1, 1, 1)\")\n+        cursor.execute(\"INSERT INTO test_alunto_pony (id, pink, weight) VALUES (2, 1, 1)\")\n+        cursor.execute(\"DELETE FROM test_alunto_pony\")\n+\n+    def test_alter_index_together(self):\n+        \"\"\"\n+        Tests the AlterIndexTogether operation.\n+        \"\"\"\n+        project_state = self.set_up_test_model(\"test_alinto\")\n+        # Test the state alteration\n+        operation = migrations.AlterIndexTogether(\"Pony\", [(\"pink\", \"weight\")])\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_alinto\", new_state)\n+        self.assertEqual(len(project_state.models[\"test_alinto\", \"pony\"].options.get(\"index_together\", set())), 0)\n+        self.assertEqual(len(new_state.models[\"test_alinto\", \"pony\"].options.get(\"index_together\", set())), 1)\n+        # Make sure there's no matching index\n+        self.assertIndexNotExists(\"test_alinto_pony\", [\"pink\", \"weight\"])\n+        # Test the database alteration\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_alinto\", editor, project_state, new_state)\n+        self.assertIndexExists(\"test_alinto_pony\", [\"pink\", \"weight\"])\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_alinto\", editor, new_state, project_state)\n+        self.assertIndexNotExists(\"test_alinto_pony\", [\"pink\", \"weight\"])\n+\n+\n+class MigrateNothingRouter(object):\n+    \"\"\"\n+    A router that sends all writes to the other database.\n+    \"\"\"\n+    def allow_migrate(self, db, model):\n+        return False\n+\n+\n+class MultiDBOperationTests(MigrationTestBase):\n+    multi_db = True\n+\n+    def setUp(self):\n+        # Make the 'other' database appear to be a slave of the 'default'\n+        self.old_routers = router.routers\n+        router.routers = [MigrateNothingRouter()]\n+\n+    def tearDown(self):\n+        # Restore the 'other' database as an independent database\n+        router.routers = self.old_routers\n+\n+    def test_create_model(self):\n+        \"\"\"\n+        Tests that CreateModel honours multi-db settings.\n+        \"\"\"\n+        operation = migrations.CreateModel(\n+            \"Pony\",\n+            [\n+                (\"id\", models.AutoField(primary_key=True)),\n+                (\"pink\", models.IntegerField(default=1)),\n+            ],\n+        )\n+        # Test the state alteration\n+        project_state = ProjectState()\n+        new_state = project_state.clone()\n+        operation.state_forwards(\"test_crmo\", new_state)\n+        # Test the database alteration\n+        self.assertTableNotExists(\"test_crmo_pony\")\n+        with connection.schema_editor() as editor:\n+            operation.database_forwards(\"test_crmo\", editor, project_state, new_state)\n+        self.assertTableNotExists(\"test_crmo_pony\")\n+        # And test reversal\n+        with connection.schema_editor() as editor:\n+            operation.database_backwards(\"test_crmo\", editor, new_state, project_state)\n+        self.assertTableNotExists(\"test_crmo_pony\")\ndiff --git a/tests/migrations/test_state.py b/tests/migrations/test_state.py\nnew file mode 100644\nindex 000000000000..e5b3fbfa086e\n--- /dev/null\n+++ b/tests/migrations/test_state.py\n@@ -0,0 +1,77 @@\n+from django.test import TestCase\n+from django.db import models\n+from django.db.models.loading import BaseAppCache\n+from django.db.migrations.state import ProjectState, ModelState\n+\n+\n+class StateTests(TestCase):\n+    \"\"\"\n+    Tests state construction, rendering and modification by operations.\n+    \"\"\"\n+\n+    def test_create(self):\n+        \"\"\"\n+        Tests making a ProjectState from an AppCache\n+        \"\"\"\n+\n+        new_app_cache = BaseAppCache()\n+\n+        class Author(models.Model):\n+            name = models.CharField(max_length=255)\n+            bio = models.TextField()\n+            age = models.IntegerField(blank=True, null=True)\n+            class Meta:\n+                app_label = \"migrations\"\n+                app_cache = new_app_cache\n+                unique_together = [\"name\", \"bio\"]\n+\n+        class Book(models.Model):\n+            title = models.CharField(max_length=1000)\n+            author = models.ForeignKey(Author)\n+            class Meta:\n+                app_label = \"migrations\"\n+                app_cache = new_app_cache\n+                verbose_name = \"tome\"\n+                db_table = \"test_tome\"\n+\n+        project_state = ProjectState.from_app_cache(new_app_cache)\n+        author_state = project_state.models['migrations', 'author']\n+        book_state = project_state.models['migrations', 'book']\n+        \n+        self.assertEqual(author_state.app_label, \"migrations\")\n+        self.assertEqual(author_state.name, \"Author\")\n+        self.assertEqual([x for x, y in author_state.fields], [\"id\", \"name\", \"bio\", \"age\"])\n+        self.assertEqual(author_state.fields[1][1].max_length, 255)\n+        self.assertEqual(author_state.fields[2][1].null, False)\n+        self.assertEqual(author_state.fields[3][1].null, True)\n+        self.assertEqual(author_state.options, {\"unique_together\": set((\"name\", \"bio\"))})\n+        self.assertEqual(author_state.bases, (models.Model, ))\n+        \n+        self.assertEqual(book_state.app_label, \"migrations\")\n+        self.assertEqual(book_state.name, \"Book\")\n+        self.assertEqual([x for x, y in book_state.fields], [\"id\", \"title\", \"author\"])\n+        self.assertEqual(book_state.fields[1][1].max_length, 1000)\n+        self.assertEqual(book_state.fields[2][1].null, False)\n+        self.assertEqual(book_state.options, {\"verbose_name\": \"tome\", \"db_table\": \"test_tome\"})\n+        self.assertEqual(book_state.bases, (models.Model, ))\n+\n+    def test_render(self):\n+        \"\"\"\n+        Tests rendering a ProjectState into an AppCache.\n+        \"\"\"\n+        project_state = ProjectState()\n+        project_state.add_model_state(ModelState(\n+            \"migrations\",\n+            \"Tag\",\n+            [\n+                (\"id\", models.AutoField(primary_key=True)),\n+                (\"name\", models.CharField(max_length=100)),\n+                (\"hidden\", models.BooleanField()),\n+            ],\n+            {},\n+            None,\n+        ))\n+\n+        new_app_cache = project_state.render()\n+        self.assertEqual(new_app_cache.get_model(\"migrations\", \"Tag\")._meta.get_field_by_name(\"name\")[0].max_length, 100)\n+        self.assertEqual(new_app_cache.get_model(\"migrations\", \"Tag\")._meta.get_field_by_name(\"hidden\")[0].null, False)\ndiff --git a/tests/migrations/test_writer.py b/tests/migrations/test_writer.py\nnew file mode 100644\nindex 000000000000..22925fee9b37\n--- /dev/null\n+++ b/tests/migrations/test_writer.py\n@@ -0,0 +1,84 @@\n+# encoding: utf8\n+import datetime\n+from django.utils import six\n+from django.test import TestCase\n+from django.db.migrations.writer import MigrationWriter\n+from django.db import models, migrations\n+\n+\n+class WriterTests(TestCase):\n+    \"\"\"\n+    Tests the migration writer (makes migration files from Migration instances)\n+    \"\"\"\n+\n+    def safe_exec(self, string, value=None):\n+        l = {}\n+        try:\n+            exec(string, globals(), l)\n+        except Exception as e:\n+            if value:\n+                self.fail(\"Could not exec %r (from value %r): %s\" % (string.strip(), value, e))\n+            else:\n+                self.fail(\"Could not exec %r: %s\" % (string.strip(), e))\n+        return l\n+\n+    def serialize_round_trip(self, value):\n+        string, imports = MigrationWriter.serialize(value)\n+        return self.safe_exec(\"%s\\ntest_value_result = %s\" % (\"\\n\".join(imports), string), value)['test_value_result']\n+\n+    def assertSerializedEqual(self, value):\n+        self.assertEqual(self.serialize_round_trip(value), value)\n+\n+    def assertSerializedIs(self, value):\n+        self.assertIs(self.serialize_round_trip(value), value)\n+\n+    def assertSerializedFieldEqual(self, value):\n+        new_value = self.serialize_round_trip(value)\n+        self.assertEqual(value.__class__, new_value.__class__)\n+        self.assertEqual(value.max_length, new_value.max_length)\n+        self.assertEqual(value.null, new_value.null)\n+        self.assertEqual(value.unique, new_value.unique)\n+\n+    def test_serialize(self):\n+        \"\"\"\n+        Tests various different forms of the serializer.\n+        This does not care about formatting, just that the parsed result is\n+        correct, so we always exec() the result and check that.\n+        \"\"\"\n+        # Basic values\n+        self.assertSerializedEqual(1)\n+        self.assertSerializedEqual(None)\n+        self.assertSerializedEqual(\"foobar\")\n+        self.assertSerializedEqual(u\"fobr\")\n+        self.assertSerializedEqual({1: 2})\n+        self.assertSerializedEqual([\"a\", 2, True, None])\n+        self.assertSerializedEqual(set([2, 3, \"eighty\"]))\n+        self.assertSerializedEqual({\"lalalala\": [\"yeah\", \"no\", \"maybe\"]})\n+        # Datetime stuff\n+        self.assertSerializedEqual(datetime.datetime.utcnow())\n+        self.assertSerializedEqual(datetime.datetime.utcnow)\n+        self.assertSerializedEqual(datetime.date.today())\n+        self.assertSerializedEqual(datetime.date.today)\n+        # Django fields\n+        self.assertSerializedFieldEqual(models.CharField(max_length=255))\n+        self.assertSerializedFieldEqual(models.TextField(null=True, blank=True))\n+\n+    def test_simple_migration(self):\n+        \"\"\"\n+        Tests serializing a simple migration.\n+        \"\"\"\n+        migration = type(\"Migration\", (migrations.Migration,), {\n+            \"operations\": [\n+                migrations.DeleteModel(\"MyModel\"),\n+                migrations.AddField(\"OtherModel\", \"field_name\", models.DateTimeField(default=datetime.datetime.utcnow))\n+            ],\n+            \"dependencies\": [(\"testapp\", \"some_other_one\")],\n+        })\n+        writer = MigrationWriter(migration)\n+        output = writer.as_string()\n+        # It should NOT be unicode.\n+        self.assertIsInstance(output, six.binary_type, \"Migration as_string returned unicode\")\n+        # We don't test the output formatting - that's too fragile.\n+        # Just make sure it runs for now, and that things look alright.\n+        result = self.safe_exec(output)\n+        self.assertIn(\"Migration\", result)\ndiff --git a/tests/multiple_database/tests.py b/tests/multiple_database/tests.py\nindex 949679418c26..629cb1237cc5 100644\n--- a/tests/multiple_database/tests.py\n+++ b/tests/multiple_database/tests.py\n@@ -933,7 +933,7 @@ def db_for_write(self, model, **hints):\n     def allow_relation(self, obj1, obj2, **hints):\n         return obj1._state.db in ('default', 'other') and obj2._state.db in ('default', 'other')\n \n-    def allow_syncdb(self, db, model):\n+    def allow_migrate(self, db, model):\n         return True\n \n class AuthRouter(object):\n@@ -960,7 +960,7 @@ def allow_relation(self, obj1, obj2, **hints):\n             return True\n         return None\n \n-    def allow_syncdb(self, db, model):\n+    def allow_migrate(self, db, model):\n         \"Make sure the auth app only appears on the 'other' db\"\n         if db == 'other':\n             return model._meta.app_label == 'auth'\n@@ -1022,30 +1022,30 @@ def test_db_selection(self):\n     def test_syncdb_selection(self):\n         \"Synchronization behavior is predictable\"\n \n-        self.assertTrue(router.allow_syncdb('default', User))\n-        self.assertTrue(router.allow_syncdb('default', Book))\n+        self.assertTrue(router.allow_migrate('default', User))\n+        self.assertTrue(router.allow_migrate('default', Book))\n \n-        self.assertTrue(router.allow_syncdb('other', User))\n-        self.assertTrue(router.allow_syncdb('other', Book))\n+        self.assertTrue(router.allow_migrate('other', User))\n+        self.assertTrue(router.allow_migrate('other', Book))\n \n         # Add the auth router to the chain.\n         # TestRouter is a universal synchronizer, so it should have no effect.\n         router.routers = [TestRouter(), AuthRouter()]\n \n-        self.assertTrue(router.allow_syncdb('default', User))\n-        self.assertTrue(router.allow_syncdb('default', Book))\n+        self.assertTrue(router.allow_migrate('default', User))\n+        self.assertTrue(router.allow_migrate('default', Book))\n \n-        self.assertTrue(router.allow_syncdb('other', User))\n-        self.assertTrue(router.allow_syncdb('other', Book))\n+        self.assertTrue(router.allow_migrate('other', User))\n+        self.assertTrue(router.allow_migrate('other', Book))\n \n         # Now check what happens if the router order is the other way around\n         router.routers = [AuthRouter(), TestRouter()]\n \n-        self.assertFalse(router.allow_syncdb('default', User))\n-        self.assertTrue(router.allow_syncdb('default', Book))\n+        self.assertFalse(router.allow_migrate('default', User))\n+        self.assertTrue(router.allow_migrate('default', Book))\n \n-        self.assertTrue(router.allow_syncdb('other', User))\n-        self.assertFalse(router.allow_syncdb('other', Book))\n+        self.assertTrue(router.allow_migrate('other', User))\n+        self.assertFalse(router.allow_migrate('other', Book))\n \n     def test_partial_router(self):\n         \"A router can choose to implement a subset of methods\"\n@@ -1062,8 +1062,8 @@ def test_partial_router(self):\n \n         self.assertTrue(router.allow_relation(dive, dive))\n \n-        self.assertTrue(router.allow_syncdb('default', User))\n-        self.assertTrue(router.allow_syncdb('default', Book))\n+        self.assertTrue(router.allow_migrate('default', User))\n+        self.assertTrue(router.allow_migrate('default', Book))\n \n         router.routers = [WriteRouter(), AuthRouter(), TestRouter()]\n \n@@ -1075,8 +1075,8 @@ def test_partial_router(self):\n \n         self.assertTrue(router.allow_relation(dive, dive))\n \n-        self.assertFalse(router.allow_syncdb('default', User))\n-        self.assertTrue(router.allow_syncdb('default', Book))\n+        self.assertFalse(router.allow_migrate('default', User))\n+        self.assertTrue(router.allow_migrate('default', Book))\n \n \n     def test_database_routing(self):\n@@ -1607,12 +1607,12 @@ def test_auth_manager(self):\n         self.assertEqual(User.objects.using('other').count(), 1)\n \n     def test_dumpdata(self):\n-        \"Check that dumpdata honors allow_syncdb restrictions on the router\"\n+        \"Check that dumpdata honors allow_migrate restrictions on the router\"\n         User.objects.create_user('alice', 'alice@example.com')\n         User.objects.db_manager('default').create_user('bob', 'bob@example.com')\n \n         # Check that dumping the default database doesn't try to include auth\n-        # because allow_syncdb prohibits auth on default\n+        # because allow_migrate prohibits auth on default\n         new_io = StringIO()\n         management.call_command('dumpdata', 'auth', format='json', database='default', stdout=new_io)\n         command_output = new_io.getvalue().strip()\n@@ -1625,10 +1625,10 @@ def test_dumpdata(self):\n         self.assertTrue('\"email\": \"alice@example.com\"' in command_output)\n \n class AntiPetRouter(object):\n-    # A router that only expresses an opinion on syncdb,\n+    # A router that only expresses an opinion on migrate,\n     # passing pets to the 'other' database\n \n-    def allow_syncdb(self, db, model):\n+    def allow_migrate(self, db, model):\n         \"Make sure the auth app only appears on the 'other' db\"\n         if db == 'other':\n             return model._meta.object_name == 'Pet'\n@@ -1917,7 +1917,7 @@ def test_foreignkey_collection(self):\n \n \n class SyncOnlyDefaultDatabaseRouter(object):\n-    def allow_syncdb(self, db, model):\n+    def allow_migrate(self, db, model):\n         return db == DEFAULT_DB_ALIAS\n \n \ndiff --git a/tests/schema/__init__.py b/tests/schema/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/schema/models.py b/tests/schema/models.py\nnew file mode 100644\nindex 000000000000..dc717ec105ff\n--- /dev/null\n+++ b/tests/schema/models.py\n@@ -0,0 +1,97 @@\n+from django.db import models\n+from django.db.models.loading import BaseAppCache\n+\n+# Because we want to test creation and deletion of these as separate things,\n+# these models are all inserted into a separate AppCache so the main test\n+# runner doesn't syncdb them.\n+\n+new_app_cache = BaseAppCache()\n+\n+\n+class Author(models.Model):\n+    name = models.CharField(max_length=255)\n+    height = models.PositiveIntegerField(null=True, blank=True)\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+\n+\n+class AuthorWithM2M(models.Model):\n+    name = models.CharField(max_length=255)\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+\n+\n+class Book(models.Model):\n+    author = models.ForeignKey(Author)\n+    title = models.CharField(max_length=100, db_index=True)\n+    pub_date = models.DateTimeField()\n+    # tags = models.ManyToManyField(\"Tag\", related_name=\"books\")\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+\n+\n+class BookWithM2M(models.Model):\n+    author = models.ForeignKey(Author)\n+    title = models.CharField(max_length=100, db_index=True)\n+    pub_date = models.DateTimeField()\n+    tags = models.ManyToManyField(\"TagM2MTest\", related_name=\"books\")\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+\n+\n+class BookWithSlug(models.Model):\n+    author = models.ForeignKey(Author)\n+    title = models.CharField(max_length=100, db_index=True)\n+    pub_date = models.DateTimeField()\n+    slug = models.CharField(max_length=20, unique=True)\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+        db_table = \"schema_book\"\n+\n+\n+class Tag(models.Model):\n+    title = models.CharField(max_length=255)\n+    slug = models.SlugField(unique=True)\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+\n+\n+class TagM2MTest(models.Model):\n+    title = models.CharField(max_length=255)\n+    slug = models.SlugField(unique=True)\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+\n+\n+class TagIndexed(models.Model):\n+    title = models.CharField(max_length=255)\n+    slug = models.SlugField(unique=True)\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+        index_together = [[\"slug\", \"title\"]]\n+\n+\n+class TagUniqueRename(models.Model):\n+    title = models.CharField(max_length=255)\n+    slug2 = models.SlugField(unique=True)\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+        db_table = \"schema_tag\"\n+\n+\n+class UniqueTest(models.Model):\n+    year = models.IntegerField()\n+    slug = models.SlugField(unique=False)\n+\n+    class Meta:\n+        app_cache = new_app_cache\n+        unique_together = [\"year\", \"slug\"]\ndiff --git a/tests/schema/tests.py b/tests/schema/tests.py\nnew file mode 100644\nindex 000000000000..c3764979d6c0\n--- /dev/null\n+++ b/tests/schema/tests.py\n@@ -0,0 +1,650 @@\n+from __future__ import absolute_import\n+import datetime\n+from django.test import TransactionTestCase\n+from django.utils.unittest import skipUnless\n+from django.db import connection, DatabaseError, IntegrityError\n+from django.db.models.fields import IntegerField, TextField, CharField, SlugField\n+from django.db.models.fields.related import ManyToManyField, ForeignKey\n+from django.db.transaction import atomic\n+from .models import Author, AuthorWithM2M, Book, BookWithSlug, BookWithM2M, Tag, TagIndexed, TagM2MTest, TagUniqueRename, UniqueTest\n+\n+\n+class SchemaTests(TransactionTestCase):\n+    \"\"\"\n+    Tests that the schema-alteration code works correctly.\n+\n+    Be aware that these tests are more liable than most to false results,\n+    as sometimes the code to check if a test has worked is almost as complex\n+    as the code it is testing.\n+    \"\"\"\n+    \n+    available_apps = []\n+\n+    models = [Author, AuthorWithM2M, Book, BookWithSlug, BookWithM2M, Tag, TagIndexed, TagM2MTest, TagUniqueRename, UniqueTest]\n+    no_table_strings = [\"no such table\", \"unknown table\", \"does not exist\"]\n+\n+    # Utility functions\n+\n+    def tearDown(self):\n+        # Delete any tables made for our models\n+        self.delete_tables()\n+\n+    def delete_tables(self):\n+        \"Deletes all model tables for our models for a clean test environment\"\n+        cursor = connection.cursor()\n+        connection.disable_constraint_checking()\n+        for model in self.models:\n+            # Remove any M2M tables first\n+            for field in model._meta.local_many_to_many:\n+                with atomic():\n+                    try:\n+                        cursor.execute(connection.schema_editor().sql_delete_table % {\n+                            \"table\": connection.ops.quote_name(field.rel.through._meta.db_table),\n+                        })\n+                    except DatabaseError as e:\n+                        if any([s in str(e).lower() for s in self.no_table_strings]):\n+                            pass\n+                        else:\n+                            raise\n+            # Then remove the main tables\n+            with atomic():\n+                try:\n+                    cursor.execute(connection.schema_editor().sql_delete_table % {\n+                        \"table\": connection.ops.quote_name(model._meta.db_table),\n+                    })\n+                except DatabaseError as e:\n+                    if any([s in str(e).lower() for s in self.no_table_strings]):\n+                        pass\n+                    else:\n+                        raise\n+        connection.enable_constraint_checking()\n+\n+    def column_classes(self, model):\n+        cursor = connection.cursor()\n+        columns = dict(\n+            (d[0], (connection.introspection.get_field_type(d[1], d), d))\n+            for d in connection.introspection.get_table_description(\n+                cursor,\n+                model._meta.db_table,\n+            )\n+        )\n+        # SQLite has a different format for field_type\n+        for name, (type, desc) in columns.items():\n+            if isinstance(type, tuple):\n+                columns[name] = (type[0], desc)\n+        # SQLite also doesn't error properly\n+        if not columns:\n+            raise DatabaseError(\"Table does not exist (empty pragma)\")\n+        return columns\n+\n+    # Tests\n+\n+    def test_creation_deletion(self):\n+        \"\"\"\n+        Tries creating a model's table, and then deleting it.\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        # Check that it's there\n+        list(Author.objects.all())\n+        # Clean up that table\n+        with connection.schema_editor() as editor:\n+            editor.delete_model(Author)\n+        # Check that it's gone\n+        self.assertRaises(\n+            DatabaseError,\n+            lambda: list(Author.objects.all()),\n+        )\n+\n+    @skipUnless(connection.features.supports_foreign_keys, \"No FK support\")\n+    def test_fk(self):\n+        \"Tests that creating tables out of FK order, then repointing, works\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Book)\n+            editor.create_model(Author)\n+            editor.create_model(Tag)\n+        # Check that initial tables are there\n+        list(Author.objects.all())\n+        list(Book.objects.all())\n+        # Make sure the FK constraint is present\n+        with self.assertRaises(IntegrityError):\n+            Book.objects.create(\n+                author_id = 1,\n+                title = \"Much Ado About Foreign Keys\",\n+                pub_date = datetime.datetime.now(),\n+            )\n+        # Repoint the FK constraint\n+        new_field = ForeignKey(Tag)\n+        new_field.set_attributes_from_name(\"author\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Book,\n+                Book._meta.get_field_by_name(\"author\")[0],\n+                new_field,\n+                strict=True,\n+            )\n+        # Make sure the new FK constraint is present\n+        constraints = connection.introspection.get_constraints(connection.cursor(), Book._meta.db_table)\n+        for name, details in constraints.items():\n+            if details['columns'] == [\"author_id\"] and details['foreign_key']:\n+                self.assertEqual(details['foreign_key'], ('schema_tag', 'id'))\n+                break\n+        else:\n+            self.fail(\"No FK constraint for author_id found\")\n+\n+    def test_add_field(self):\n+        \"\"\"\n+        Tests adding fields to models\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        # Ensure there's no age field\n+        columns = self.column_classes(Author)\n+        self.assertNotIn(\"age\", columns)\n+        # Alter the name field to a TextField\n+        new_field = IntegerField(null=True)\n+        new_field.set_attributes_from_name(\"age\")\n+        with connection.schema_editor() as editor:\n+            editor.add_field(\n+                Author,\n+                new_field,\n+            )\n+        # Ensure the field is right afterwards\n+        columns = self.column_classes(Author)\n+        self.assertEqual(columns['age'][0], \"IntegerField\")\n+        self.assertEqual(columns['age'][1][6], True)\n+\n+    def test_alter(self):\n+        \"\"\"\n+        Tests simple altering of fields\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        # Ensure the field is right to begin with\n+        columns = self.column_classes(Author)\n+        self.assertEqual(columns['name'][0], \"CharField\")\n+        self.assertEqual(bool(columns['name'][1][6]), bool(connection.features.interprets_empty_strings_as_nulls))\n+        # Alter the name field to a TextField\n+        new_field = TextField(null=True)\n+        new_field.set_attributes_from_name(\"name\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Author,\n+                Author._meta.get_field_by_name(\"name\")[0],\n+                new_field,\n+                strict=True,\n+            )\n+        # Ensure the field is right afterwards\n+        columns = self.column_classes(Author)\n+        self.assertEqual(columns['name'][0], \"TextField\")\n+        self.assertEqual(columns['name'][1][6], True)\n+        # Change nullability again\n+        new_field2 = TextField(null=False)\n+        new_field2.set_attributes_from_name(\"name\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Author,\n+                new_field,\n+                new_field2,\n+                strict=True,\n+            )\n+        # Ensure the field is right afterwards\n+        columns = self.column_classes(Author)\n+        self.assertEqual(columns['name'][0], \"TextField\")\n+        self.assertEqual(bool(columns['name'][1][6]), False)\n+\n+    def test_rename(self):\n+        \"\"\"\n+        Tests simple altering of fields\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        # Ensure the field is right to begin with\n+        columns = self.column_classes(Author)\n+        self.assertEqual(columns['name'][0], \"CharField\")\n+        self.assertNotIn(\"display_name\", columns)\n+        # Alter the name field's name\n+        new_field = CharField(max_length=254)\n+        new_field.set_attributes_from_name(\"display_name\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Author,\n+                Author._meta.get_field_by_name(\"name\")[0],\n+                new_field,\n+                strict = True,\n+            )\n+        # Ensure the field is right afterwards\n+        columns = self.column_classes(Author)\n+        self.assertEqual(columns['display_name'][0], \"CharField\")\n+        self.assertNotIn(\"name\", columns)\n+\n+    def test_m2m_create(self):\n+        \"\"\"\n+        Tests M2M fields on models during creation\n+        \"\"\"\n+        # Create the tables\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+            editor.create_model(TagM2MTest)\n+            editor.create_model(BookWithM2M)\n+        # Ensure there is now an m2m table there\n+        columns = self.column_classes(BookWithM2M._meta.get_field_by_name(\"tags\")[0].rel.through)\n+        self.assertEqual(columns['tagm2mtest_id'][0], \"IntegerField\")\n+\n+    def test_m2m(self):\n+        \"\"\"\n+        Tests adding/removing M2M fields on models\n+        \"\"\"\n+        # Create the tables\n+        with connection.schema_editor() as editor:\n+            editor.create_model(AuthorWithM2M)\n+            editor.create_model(TagM2MTest)\n+        # Create an M2M field\n+        new_field = ManyToManyField(\"schema.TagM2MTest\", related_name=\"authors\")\n+        new_field.contribute_to_class(AuthorWithM2M, \"tags\")\n+        try:\n+            # Ensure there's no m2m table there\n+            self.assertRaises(DatabaseError, self.column_classes, new_field.rel.through)\n+            # Add the field\n+            with connection.schema_editor() as editor:\n+                editor.add_field(\n+                    Author,\n+                    new_field,\n+                )\n+            # Ensure there is now an m2m table there\n+            columns = self.column_classes(new_field.rel.through)\n+            self.assertEqual(columns['tagm2mtest_id'][0], \"IntegerField\")\n+            # Remove the M2M table again\n+            with connection.schema_editor() as editor:\n+                editor.remove_field(\n+                    Author,\n+                    new_field,\n+                )\n+            # Ensure there's no m2m table there\n+            self.assertRaises(DatabaseError, self.column_classes, new_field.rel.through)\n+        finally:\n+            # Cleanup model states\n+            AuthorWithM2M._meta.local_many_to_many.remove(new_field)\n+\n+    def test_m2m_repoint(self):\n+        \"\"\"\n+        Tests repointing M2M fields\n+        \"\"\"\n+        # Create the tables\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+            editor.create_model(BookWithM2M)\n+            editor.create_model(TagM2MTest)\n+            editor.create_model(UniqueTest)\n+        # Ensure the M2M exists and points to TagM2MTest\n+        constraints = connection.introspection.get_constraints(connection.cursor(), BookWithM2M._meta.get_field_by_name(\"tags\")[0].rel.through._meta.db_table)\n+        if connection.features.supports_foreign_keys:\n+            for name, details in constraints.items():\n+                if details['columns'] == [\"tagm2mtest_id\"] and details['foreign_key']:\n+                    self.assertEqual(details['foreign_key'], ('schema_tagm2mtest', 'id'))\n+                    break\n+            else:\n+                self.fail(\"No FK constraint for tagm2mtest_id found\")\n+        # Repoint the M2M\n+        new_field = ManyToManyField(UniqueTest)\n+        new_field.contribute_to_class(BookWithM2M, \"uniques\")\n+        try:\n+            with connection.schema_editor() as editor:\n+                editor.alter_field(\n+                    Author,\n+                    BookWithM2M._meta.get_field_by_name(\"tags\")[0],\n+                    new_field,\n+                )\n+            # Ensure old M2M is gone\n+            self.assertRaises(DatabaseError, self.column_classes, BookWithM2M._meta.get_field_by_name(\"tags\")[0].rel.through)\n+            # Ensure the new M2M exists and points to UniqueTest\n+            constraints = connection.introspection.get_constraints(connection.cursor(), new_field.rel.through._meta.db_table)\n+            if connection.features.supports_foreign_keys:\n+                for name, details in constraints.items():\n+                    if details['columns'] == [\"uniquetest_id\"] and details['foreign_key']:\n+                        self.assertEqual(details['foreign_key'], ('schema_uniquetest', 'id'))\n+                        break\n+                else:\n+                    self.fail(\"No FK constraint for uniquetest_id found\")\n+        finally:\n+            # Cleanup model states\n+            BookWithM2M._meta.local_many_to_many.remove(new_field)\n+            del BookWithM2M._meta._m2m_cache\n+\n+    @skipUnless(connection.features.supports_check_constraints, \"No check constraints\")\n+    def test_check_constraints(self):\n+        \"\"\"\n+        Tests creating/deleting CHECK constraints\n+        \"\"\"\n+        # Create the tables\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        # Ensure the constraint exists\n+        constraints = connection.introspection.get_constraints(connection.cursor(), Author._meta.db_table)\n+        for name, details in constraints.items():\n+            if details['columns'] == [\"height\"] and details['check']:\n+                break\n+        else:\n+            self.fail(\"No check constraint for height found\")\n+        # Alter the column to remove it\n+        new_field = IntegerField(null=True, blank=True)\n+        new_field.set_attributes_from_name(\"height\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Author,\n+                Author._meta.get_field_by_name(\"height\")[0],\n+                new_field,\n+                strict = True,\n+            )\n+        constraints = connection.introspection.get_constraints(connection.cursor(), Author._meta.db_table)\n+        for name, details in constraints.items():\n+            if details['columns'] == [\"height\"] and details['check']:\n+                self.fail(\"Check constraint for height found\")\n+        # Alter the column to re-add it\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Author,\n+                new_field,\n+                Author._meta.get_field_by_name(\"height\")[0],\n+                strict = True,\n+            )\n+        constraints = connection.introspection.get_constraints(connection.cursor(), Author._meta.db_table)\n+        for name, details in constraints.items():\n+            if details['columns'] == [\"height\"] and details['check']:\n+                break\n+        else:\n+            self.fail(\"No check constraint for height found\")\n+\n+    def test_unique(self):\n+        \"\"\"\n+        Tests removing and adding unique constraints to a single column.\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Tag)\n+        # Ensure the field is unique to begin with\n+        Tag.objects.create(title=\"foo\", slug=\"foo\")\n+        self.assertRaises(IntegrityError, Tag.objects.create, title=\"bar\", slug=\"foo\")\n+        Tag.objects.all().delete()\n+        # Alter the slug field to be non-unique\n+        new_field = SlugField(unique=False)\n+        new_field.set_attributes_from_name(\"slug\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Tag,\n+                Tag._meta.get_field_by_name(\"slug\")[0],\n+                new_field,\n+                strict = True,\n+            )\n+        # Ensure the field is no longer unique\n+        Tag.objects.create(title=\"foo\", slug=\"foo\")\n+        Tag.objects.create(title=\"bar\", slug=\"foo\")\n+        Tag.objects.all().delete()\n+        # Alter the slug field to be unique\n+        new_new_field = SlugField(unique=True)\n+        new_new_field.set_attributes_from_name(\"slug\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Tag,\n+                new_field,\n+                new_new_field,\n+                strict = True,\n+            )\n+        # Ensure the field is unique again\n+        Tag.objects.create(title=\"foo\", slug=\"foo\")\n+        self.assertRaises(IntegrityError, Tag.objects.create, title=\"bar\", slug=\"foo\")\n+        Tag.objects.all().delete()\n+        # Rename the field\n+        new_field = SlugField(unique=False)\n+        new_field.set_attributes_from_name(\"slug2\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Tag,\n+                Tag._meta.get_field_by_name(\"slug\")[0],\n+                TagUniqueRename._meta.get_field_by_name(\"slug2\")[0],\n+                strict = True,\n+            )\n+        # Ensure the field is still unique\n+        TagUniqueRename.objects.create(title=\"foo\", slug2=\"foo\")\n+        self.assertRaises(IntegrityError, TagUniqueRename.objects.create, title=\"bar\", slug2=\"foo\")\n+        Tag.objects.all().delete()\n+\n+    def test_unique_together(self):\n+        \"\"\"\n+        Tests removing and adding unique_together constraints on a model.\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(UniqueTest)\n+        # Ensure the fields are unique to begin with\n+        UniqueTest.objects.create(year=2012, slug=\"foo\")\n+        UniqueTest.objects.create(year=2011, slug=\"foo\")\n+        UniqueTest.objects.create(year=2011, slug=\"bar\")\n+        self.assertRaises(IntegrityError, UniqueTest.objects.create, year=2012, slug=\"foo\")\n+        UniqueTest.objects.all().delete()\n+        # Alter the model to it's non-unique-together companion\n+        with connection.schema_editor() as editor:\n+            editor.alter_unique_together(\n+                UniqueTest,\n+                UniqueTest._meta.unique_together,\n+                [],\n+            )\n+        # Ensure the fields are no longer unique\n+        UniqueTest.objects.create(year=2012, slug=\"foo\")\n+        UniqueTest.objects.create(year=2012, slug=\"foo\")\n+        UniqueTest.objects.all().delete()\n+        # Alter it back\n+        new_new_field = SlugField(unique=True)\n+        new_new_field.set_attributes_from_name(\"slug\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_unique_together(\n+                UniqueTest,\n+                [],\n+                UniqueTest._meta.unique_together,\n+            )\n+        # Ensure the fields are unique again\n+        UniqueTest.objects.create(year=2012, slug=\"foo\")\n+        self.assertRaises(IntegrityError, UniqueTest.objects.create, year=2012, slug=\"foo\")\n+        UniqueTest.objects.all().delete()\n+\n+    def test_index_together(self):\n+        \"\"\"\n+        Tests removing and adding index_together constraints on a model.\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Tag)\n+        # Ensure there's no index on the year/slug columns first\n+        self.assertEqual(\n+            False,\n+            any(\n+                c[\"index\"]\n+                for c in connection.introspection.get_constraints(connection.cursor(), \"schema_tag\").values()\n+                if c['columns'] == [\"slug\", \"title\"]\n+            ),\n+        )\n+        # Alter the model to add an index\n+        with connection.schema_editor() as editor:\n+            editor.alter_index_together(\n+                Tag,\n+                [],\n+                [(\"slug\", \"title\")],\n+            )\n+        # Ensure there is now an index\n+        self.assertEqual(\n+            True,\n+            any(\n+                c[\"index\"]\n+                for c in connection.introspection.get_constraints(connection.cursor(), \"schema_tag\").values()\n+                if c['columns'] == [\"slug\", \"title\"]\n+            ),\n+        )\n+        # Alter it back\n+        new_new_field = SlugField(unique=True)\n+        new_new_field.set_attributes_from_name(\"slug\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_index_together(\n+                Tag,\n+                [(\"slug\", \"title\")],\n+                [],\n+            )\n+        # Ensure there's no index\n+        self.assertEqual(\n+            False,\n+            any(\n+                c[\"index\"]\n+                for c in connection.introspection.get_constraints(connection.cursor(), \"schema_tag\").values()\n+                if c['columns'] == [\"slug\", \"title\"]\n+            ),\n+        )\n+\n+    def test_create_index_together(self):\n+        \"\"\"\n+        Tests creating models with index_together already defined\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(TagIndexed)\n+        # Ensure there is an index\n+        self.assertEqual(\n+            True,\n+            any(\n+                c[\"index\"]\n+                for c in connection.introspection.get_constraints(connection.cursor(), \"schema_tagindexed\").values()\n+                if c['columns'] == [\"slug\", \"title\"]\n+            ),\n+        )\n+\n+    def test_db_table(self):\n+        \"\"\"\n+        Tests renaming of the table\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+        # Ensure the table is there to begin with\n+        columns = self.column_classes(Author)\n+        self.assertEqual(columns['name'][0], \"CharField\")\n+        # Alter the table\n+        with connection.schema_editor() as editor:\n+            editor.alter_db_table(\n+                Author,\n+                \"schema_author\",\n+                \"schema_otherauthor\",\n+            )\n+        # Ensure the table is there afterwards\n+        Author._meta.db_table = \"schema_otherauthor\"\n+        columns = self.column_classes(Author)\n+        self.assertEqual(columns['name'][0], \"CharField\")\n+        # Alter the table again\n+        with connection.schema_editor() as editor:\n+            editor.alter_db_table(\n+                Author,\n+                \"schema_otherauthor\",\n+                \"schema_author\",\n+            )\n+        # Ensure the table is still there\n+        Author._meta.db_table = \"schema_author\"\n+        columns = self.column_classes(Author)\n+        self.assertEqual(columns['name'][0], \"CharField\")\n+\n+    def test_indexes(self):\n+        \"\"\"\n+        Tests creation/altering of indexes\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Author)\n+            editor.create_model(Book)\n+        # Ensure the table is there and has the right index\n+        self.assertIn(\n+            \"title\",\n+            connection.introspection.get_indexes(connection.cursor(), Book._meta.db_table),\n+        )\n+        # Alter to remove the index\n+        new_field = CharField(max_length=100, db_index=False)\n+        new_field.set_attributes_from_name(\"title\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Book,\n+                Book._meta.get_field_by_name(\"title\")[0],\n+                new_field,\n+                strict = True,\n+            )\n+        # Ensure the table is there and has no index\n+        self.assertNotIn(\n+            \"title\",\n+            connection.introspection.get_indexes(connection.cursor(), Book._meta.db_table),\n+        )\n+        # Alter to re-add the index\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                Book,\n+                new_field,\n+                Book._meta.get_field_by_name(\"title\")[0],\n+                strict = True,\n+            )\n+        # Ensure the table is there and has the index again\n+        self.assertIn(\n+            \"title\",\n+            connection.introspection.get_indexes(connection.cursor(), Book._meta.db_table),\n+        )\n+        # Add a unique column, verify that creates an implicit index\n+        with connection.schema_editor() as editor:\n+            editor.add_field(\n+                Book,\n+                BookWithSlug._meta.get_field_by_name(\"slug\")[0],\n+            )\n+        self.assertIn(\n+            \"slug\",\n+            connection.introspection.get_indexes(connection.cursor(), Book._meta.db_table),\n+        )\n+        # Remove the unique, check the index goes with it\n+        new_field2 = CharField(max_length=20, unique=False)\n+        new_field2.set_attributes_from_name(\"slug\")\n+        with connection.schema_editor() as editor:\n+            editor.alter_field(\n+                BookWithSlug,\n+                BookWithSlug._meta.get_field_by_name(\"slug\")[0],\n+                new_field2,\n+                strict = True,\n+            )\n+        self.assertNotIn(\n+            \"slug\",\n+            connection.introspection.get_indexes(connection.cursor(), Book._meta.db_table),\n+        )\n+\n+    def test_primary_key(self):\n+        \"\"\"\n+        Tests altering of the primary key\n+        \"\"\"\n+        # Create the table\n+        with connection.schema_editor() as editor:\n+            editor.create_model(Tag)\n+        # Ensure the table is there and has the right PK\n+        self.assertTrue(\n+            connection.introspection.get_indexes(connection.cursor(), Tag._meta.db_table)['id']['primary_key'],\n+        )\n+        # Alter to change the PK\n+        new_field = SlugField(primary_key=True)\n+        new_field.set_attributes_from_name(\"slug\")\n+        with connection.schema_editor() as editor:\n+            editor.remove_field(Tag, Tag._meta.get_field_by_name(\"id\")[0])\n+            editor.alter_field(\n+                Tag,\n+                Tag._meta.get_field_by_name(\"slug\")[0],\n+                new_field,\n+            )\n+        # Ensure the PK changed\n+        self.assertNotIn(\n+            'id',\n+            connection.introspection.get_indexes(connection.cursor(), Tag._meta.db_table),\n+        )\n+        self.assertTrue(\n+            connection.introspection.get_indexes(connection.cursor(), Tag._meta.db_table)['slug']['primary_key'],\n+        )\ndiff --git a/tests/utils_tests/test_functional.py b/tests/utils_tests/test_functional.py\nindex df377c6e06e1..66e051033e85 100644\n--- a/tests/utils_tests/test_functional.py\n+++ b/tests/utils_tests/test_functional.py\n@@ -65,3 +65,15 @@ def value(self):\n \n         # check that it behaves like a property when there's no instance\n         self.assertIsInstance(A.value, cached_property)\n+\n+    def test_lazy_equality(self):\n+        \"\"\"\n+        Tests that == and != work correctly for Promises.\n+        \"\"\"\n+\n+        lazy_a = lazy(lambda: 4, int)\n+        lazy_b = lazy(lambda: 4, int)\n+        lazy_c = lazy(lambda: 5, int)\n+\n+        self.assertEqual(lazy_a(), lazy_b())\n+        self.assertNotEqual(lazy_b(), lazy_c())\n"
  },
  {
    "index": 22,
    "filtered_comments": [
      "> > djangoci uses MariaDB 10.1.40. I can bump MariaDB version in the next few days. We need to remember that Django 3.0 supports MariaDB 10.1 and higher so a new db feature is required e.g. has_json_field.\r\n> \r\n> How about `supports_json` ? It's not really a separate data type on SQLite or MariaDB.\r\n\r\nIt is also not a separate field on Oracle, but a feature flag will determine if backend has JSON field or not, so ... :thinking:   ",
      "> @laymonage I updated MariaDB to 10.2.24 on Jenkins.\r\n\r\nThanks! As expected, the tests have passed now.\r\n\r\nI have added a `supports_json` feature (can be renamed if desired). Not sure if I should check the SQLite version, though. I don't think there's any way to check if the JSON1 extension is enabled (maybe we could try to do `SELECT json('\"test\"')`, but that's a bit hack-ish).\r\nThe JSON1 extension was introduced with the release of SQLite 3.9.0. However, since it's a loadable extension, it *might* work if it's loaded on older SQLite version(s). I haven't tried.\r\n\r\nAlso, I'm not sure if I should use `check` and extend the list returned by that method instead of raising a `NotSupportedError`. I've seen both examples in the existing codebase.",
      "> I have added a supports_json feature (can be renamed if desired). Not sure if I should check the SQLite version, though. I don't think there's any way to check if the JSON1 extension is enabled (maybe we could try to do SELECT json('\"test\"'), but that's a bit hack-ish).\r\n> The JSON1 extension was introduced with the release of SQLite 3.9.0. However, since it's a loadable extension, it might work if it's loaded on older SQLite version(s). I haven't tried.\r\n\r\nI think trying the `json` function and catching the error isn't so bad, as long as it won't break any transactions.\r\n\r\nThe other option is to use `PRAGMA compile_options` and check if the extension is in there, however I am not sure if it's possible to load the `json1` extension without it being built in at compile time...\r\n\r\n```\r\nsqlite> PRAGMA compile_options;\r\nBUG_COMPATIBLE_20160819\r\nCOMPILER=clang-10.0.1\r\nDEFAULT_CACHE_SIZE=2000\r\nDEFAULT_CKPTFULLFSYNC\r\nDEFAULT_JOURNAL_SIZE_LIMIT=32768\r\nDEFAULT_PAGE_SIZE=4096\r\nDEFAULT_SYNCHRONOUS=2\r\nDEFAULT_WAL_SYNCHRONOUS=1\r\nENABLE_API_ARMOR\r\nENABLE_COLUMN_METADATA\r\nENABLE_DBSTAT_VTAB\r\nENABLE_FTS3\r\nENABLE_FTS3_PARENTHESIS\r\nENABLE_FTS3_TOKENIZER\r\nENABLE_FTS4\r\nENABLE_FTS5\r\nENABLE_JSON1\r\nENABLE_LOCKING_STYLE=1\r\nENABLE_PREUPDATE_HOOK\r\nENABLE_RTREE\r\nENABLE_SESSION\r\nENABLE_SNAPSHOT\r\nENABLE_SQLLOG\r\nENABLE_UNKNOWN_SQL_FUNCTION\r\nENABLE_UPDATE_DELETE_LIMIT\r\nHAVE_ISNAN\r\nMAX_LENGTH=2147483645\r\nMAX_MMAP_SIZE=1073741824\r\nMAX_VARIABLE_NUMBER=500000\r\nOMIT_AUTORESET\r\nOMIT_LOAD_EXTENSION\r\nSTMTJRNL_SPILL=131072\r\nTHREADSAFE=2\r\nUSE_URI\r\n```",
      "> I think trying the json function and catching the error isn't so bad, as long as it won't break any transactions.\r\n\r\nI'm not sure where and how to properly put it in Django's source code, though.\r\n\r\n---\r\n\r\nI tried compiling SQLite 3.28.0 without JSON1, compiling JSON1 separately, and loading it with the `.load` command.\r\n`SELECT JSON('\"test\"');` works, but `ENABLE_JSON1` doesn't show up with `PRAGMA compile_options` (which is correct since I didn't build JSON1 along with SQLite).\r\n\r\nOn the other hand, I also tried loading JSON1 (compiled from SQLite 3.28.0 source code) on SQLite 3.8.7.1 (what's available on Debian Jessie). This SQLite version supports extension loading, but I got a segmentation fault when I tried to load JSON1. So, I guess it needs SQLite 3.9.0 and up.\r\n\r\nBy the way... JSON1 is also enabled by default if SQLite is compiled using `make` with the amalgamation and the given configurations.",
      "> > I'm not sure where and how to properly put it in Django's source code, though.\r\n> \r\n> You can use a `@cached_property` for the feature, for example https://github.com/django/django/blob/master/django/db/backends/mysql/features.py#L110\r\n\r\nYeah, I've used it in my `supports_json` DB feature. What I mean is, should I do something like this?\r\n\r\n```python\r\ntry:\r\n    with self.connection.cursor() as cursor:\r\n        cursor.execute(\"SELECT JSON('\\\"test\\\"')\r\nexcept DatabaseError:\r\n    return False\r\nelse:\r\n    return True\r\n```",
      "> No, `json.dumps` and `json.loads` take `None` as the default argument for the `cls` parameter. See https://docs.python.org/3/library/json.html#json.dumps.\r\n\r\nActually, I didn't test it but I saw [this part](https://github.com/django/django/blob/698df6a009cb1c4dbd55905264f24f6edf41066e/django/contrib/postgres/fields/jsonb.py#L25) in code.",
      "> Actually, I didn't test it but I saw [this part](https://github.com/django/django/blob/698df6a009cb1c4dbd55905264f24f6edf41066e/django/contrib/postgres/fields/jsonb.py#L25) in code.\r\n\r\nYes, but that's unnecessary since the default argument is `None`.\r\n\r\n> I would move under class as instance method.\r\n\r\nI don't think that would work since the first argument would be the `JSONField` instance, instead of the value?\r\n",
      "@laymonage Thanks for updates :+1: I think that we should currently move all PostgreSQL tests related with JSONField (e.g. `postgres_tests/test_json.py`) to all databases scope and start to work on failures. I would also recommend to remove current implementation from `contrib.postgres` and for backward compatibility leave it only as a reference to a new implementation (probably some workaround should be added to migrations):\r\n\r\n- `django.contrib.postgres.fields.JSONField` -> `django.db.models.JSONField`,\r\n- `django.contrib.postgres.forms.JSONField` -> `django.forms.JSONField`,\r\n\r\nFor example, `django/contrib/postgres/fields/jsonb.py`:\r\n```python\r\nfrom django.db.models import JSONField\r\n\r\n__all__ = ['JSONField']\r\n```\r\nall lookups should be moved from `contrib/postgres/fields/jsonb.py`  to `db/models/lookups.py`.\r\n\r\nWith these changes we will be able to find caveats for each database :male_detective: .",
      "@felixxm I remember some folks saying it'd be better to leave the current implementation in `contrib.postgres` as it is (and add a deprecation message). However, I see your idea is reasonable, as long as we can maintain all of the lookups and transforms. I guess I'll try going down that route and see if we can do that.\r\n\r\nMeanwhile, I've removed some tests in `postgres_tests` and incorporated them into `test_jsonfield.py`.\r\n\r\nSome updates:\r\n\r\n- `JSON_VALID(NULL)` returns `0` (false) on SQLite, while it returns true on MySQL and MariaDB (or maybe the check just doesn't occur). This makes it impossible to store SQL `NULL` even if we specify `blank=True, null=True`. I've updated the SQLite constraint with `OR \"%(column)s\" IS NULL` and now it works correctly.\r\n- Oracle Database stores SQL `NULL` as an empty string `''` on fields that support empty strings. I've updated `JSONField` to accommodate this behavior. Saving empty Python strings would still work, as they would be encoded as `'\"\"'`.\r\n- I've refactored the tests into different classes for cohesiveness.",
      "Apparently, it's not because the values aren't quoted.  \r\ncx_Oracle uses bind variables:  \r\nhttps://www.oracle.com/technetwork/articles/dsl/prez-python-queries-101587.html\r\nhttps://oracle.readthedocs.io/en/latest/plsql/bind/\r\n\r\nBasically, query parameters get passed using variables, so queries look like this on Oracle:\r\n\r\n```sql\r\nSELECT * FROM TABLE WHERE col1 = :arg1 AND col2 = :arg2 ...\r\n```\r\n\r\nand the arguments can be passed using a dictionary, kwargs, or sequence (list, tuple), e.g.\r\n```python\r\nparams = {'arg1': 'hello', 'arg2': 'world'}\r\n```\r\n\r\n(see also: https://github.com/django/django/blob/master/django/db/backends/oracle/base.py#L478)\r\n\r\nThe problem is, `JSON_EXISTS` function on Oracle [doesn't support bind variables](https://stackoverflow.com/questions/48913687/jdbc-prepared-statement-to-query-json-using-json-exists). We can format the arguments directly into the SQL string (which is what I've done), but this opens up the possibility of SQL injections.\r\n\r\nHowever, I do `json.dumps()` on the specified key before formatting it, so the key will be double-quoted. If someone were to execute an SQL injection, they should end the quote first, which I don't think is possible since `\"` will be escaped by `json.dumps()` into `\\\"`. I think the worst that could happen is a `DatabaseError`. I currently can't think of a key string that can be used to perform an SQL injection.\r\n\r\nShould we go through with it, or drop support for these lookups on Oracle?\r\n\r\n",
      "Thanks a lot for the feedback. I started working on the lookups and transforms on MySQL using @adamchainz's and @raphaelm's existing code. It turns out that the code doesn't pass all of the tests from `contrib.postgres`, so I still have to fix things up.\r\n\r\nI also try to simplify or find better ways to implement the lookups and transforms, but fixing one thing tends to break another. It's very confusing, to be honest. Not to mention debugging it isn't so easy since I have to inspect the queries most of the time... :grimacing: \r\n\r\nEdit: on the other hand, `TestQuerying` test cases aren't run by djangoci. What's up with that?",
      "I've implemented the transforms and lookups on Oracle. Some features aren't supported, so I skipped the tests for those on Oracle.\r\n\r\nSome notes:\r\n- I didn't choose to implement it using the simple dot-notation syntax.\r\n  It's mainly because it requires the tables to be given aliases in the query. I could not find an easy and clean way to do that. The [oracle_json_field](https://github.com/Exscientia/oracle-json-field/blob/master/oracle_json_field/managers.py) package uses a custom Queryset and Manager with forced self-join to make table aliases.\r\n- In effect, I had to use `JSON_QUERY` to retrieve JSON objects and arrays, and `JSON_VALUE` to retrieve scalar values. To combine this, I used `COALESCE`. I probably should use `models.functions.Coalesce` for this, but if that's the case, it would make sense to also write `JSON_QUERY` and `JSON_VALUE` functions. It would probably add a little overhead on the Python-side. I'm not sure if I should do this. If I should, I'll probably also write some JSON `Func`s for all database backends that support them. For now, I'm just writing `COALESCE` directly into the SQL.\r\n- On the upside, using `JSON_QUERY` and `JSON_VALUE` supports querying > 4 KBytes of data while using the dot-notation syntax does not.",
      "I've implemented the transforms and lookups on SQLite. It turns out I can reuse most of the code from MySQL implementation. I only had to handle the case for querying JSON `null` values in JSON objects to differentiate them from missing keys (by using the `JSON_TYPE` function).\r\nSurprisingly, the support is equivalent to MySQL, which is much better than on Oracle.\r\n\r\nI have also added tests for storing JSON `null` scalar values. It is possible to do so by using `Value` during object creation. However, the Python representation of SQL `NULL` and JSON `null` are the same, i.e. `None`.",
      "I think I've found a way to implement `contains` and `contained_by` on SQLite and Oracle. I'll see what I can do.",
      "I managed to get `contains` working on SQLite and Oracle, though it was a bit of a hack since they both don't include a function similar to `JSON_CONTAINS`. It seems to work fine for its intended use (a `dict` rhs, to be checked on the top level of the JSON document). I added more tests, but I cannot guarantee it to work uniformly across all backends, especially for scalar and array rhs.\r\n\r\nEdit: I cannot think of a way to implement `contained_by`. Without a `JSON_CONTAINS` function, one would need to enumerate the JSON document in the database, which I think is impossible to do in one query."
    ],
    "code_diff": "diff --git a/AUTHORS b/AUTHORS\nindex 4632c66a62e8..03ed3fb7d643 100644\n--- a/AUTHORS\n+++ b/AUTHORS\n@@ -784,6 +784,7 @@ answer newbie questions, and generally made Django that much better:\n     Ryan Rubin <ryanmrubin@gmail.com>\n     Ryno Mathee <rmathee@gmail.com>\n     Sachin Jat <sanch.jat@gmail.com>\n+    Sage M. Abdullah <https://github.com/laymonage>\n     Sam Newman <http://www.magpiebrain.com/>\n     Sander Dijkhuis <sander.dijkhuis@gmail.com>\n     Sanket Saurav <sanketsaurav@gmail.com>\ndiff --git a/django/contrib/postgres/aggregates/general.py b/django/contrib/postgres/aggregates/general.py\nindex 918373e926e7..8390747c096b 100644\n--- a/django/contrib/postgres/aggregates/general.py\n+++ b/django/contrib/postgres/aggregates/general.py\n@@ -1,4 +1,5 @@\n-from django.contrib.postgres.fields import ArrayField, JSONField\n+from django.contrib.postgres.fields import ArrayField\n+from django.db.models import JSONField\n from django.db.models.aggregates import Aggregate\n \n from .mixins import OrderableAggMixin\ndiff --git a/django/contrib/postgres/apps.py b/django/contrib/postgres/apps.py\nindex 97475de6f7a2..25cfa1a814ce 100644\n--- a/django/contrib/postgres/apps.py\n+++ b/django/contrib/postgres/apps.py\n@@ -47,7 +47,6 @@ def ready(self):\n         for conn in connections.all():\n             if conn.vendor == 'postgresql':\n                 conn.introspection.data_types_reverse.update({\n-                    3802: 'django.contrib.postgres.fields.JSONField',\n                     3904: 'django.contrib.postgres.fields.IntegerRangeField',\n                     3906: 'django.contrib.postgres.fields.DecimalRangeField',\n                     3910: 'django.contrib.postgres.fields.DateTimeRangeField',\ndiff --git a/django/contrib/postgres/fields/jsonb.py b/django/contrib/postgres/fields/jsonb.py\nindex c402dd19d8e4..15d4daf466f0 100644\n--- a/django/contrib/postgres/fields/jsonb.py\n+++ b/django/contrib/postgres/fields/jsonb.py\n@@ -1,185 +1,41 @@\n-import json\n+import warnings\n \n-from psycopg2.extras import Json\n-\n-from django.contrib.postgres import forms, lookups\n-from django.core import exceptions\n-from django.db.models import (\n-    Field, TextField, Transform, lookups as builtin_lookups,\n+from django.db.models import JSONField as BuiltinJSONField\n+from django.db.models.fields.json import (\n+    KeyTextTransform as BuiltinKeyTextTransform,\n+    KeyTransform as BuiltinKeyTransform,\n )\n-from django.db.models.fields.mixins import CheckFieldDefaultMixin\n-from django.utils.translation import gettext_lazy as _\n+from django.utils.deprecation import RemovedInDjango40Warning\n \n __all__ = ['JSONField']\n \n \n-class JsonAdapter(Json):\n-    \"\"\"\n-    Customized psycopg2.extras.Json to allow for a custom encoder.\n-    \"\"\"\n-    def __init__(self, adapted, dumps=None, encoder=None):\n-        self.encoder = encoder\n-        super().__init__(adapted, dumps=dumps)\n-\n-    def dumps(self, obj):\n-        options = {'cls': self.encoder} if self.encoder else {}\n-        return json.dumps(obj, **options)\n-\n-\n-class JSONField(CheckFieldDefaultMixin, Field):\n-    empty_strings_allowed = False\n-    description = _('A JSON object')\n-    default_error_messages = {\n-        'invalid': _(\"Value must be valid JSON.\"),\n+class JSONField(BuiltinJSONField):\n+    system_check_deprecated_details = {\n+        'msg': (\n+            'django.contrib.postgres.fields.JSONField is deprecated '\n+            'and will be removed in Django 4.0.'\n+        ),\n+        'hint': 'Use django.db.models.JSONField instead.',\n+        'id': 'fields.W903'\n     }\n-    _default_hint = ('dict', '{}')\n-\n-    def __init__(self, verbose_name=None, name=None, encoder=None, **kwargs):\n-        if encoder and not callable(encoder):\n-            raise ValueError(\"The encoder parameter must be a callable object.\")\n-        self.encoder = encoder\n-        super().__init__(verbose_name, name, **kwargs)\n-\n-    def db_type(self, connection):\n-        return 'jsonb'\n-\n-    def deconstruct(self):\n-        name, path, args, kwargs = super().deconstruct()\n-        if self.encoder is not None:\n-            kwargs['encoder'] = self.encoder\n-        return name, path, args, kwargs\n-\n-    def get_transform(self, name):\n-        transform = super().get_transform(name)\n-        if transform:\n-            return transform\n-        return KeyTransformFactory(name)\n-\n-    def get_prep_value(self, value):\n-        if value is not None:\n-            return JsonAdapter(value, encoder=self.encoder)\n-        return value\n-\n-    def validate(self, value, model_instance):\n-        super().validate(value, model_instance)\n-        options = {'cls': self.encoder} if self.encoder else {}\n-        try:\n-            json.dumps(value, **options)\n-        except TypeError:\n-            raise exceptions.ValidationError(\n-                self.error_messages['invalid'],\n-                code='invalid',\n-                params={'value': value},\n-            )\n-\n-    def value_to_string(self, obj):\n-        return self.value_from_object(obj)\n \n-    def formfield(self, **kwargs):\n-        return super().formfield(**{\n-            'form_class': forms.JSONField,\n-            **kwargs,\n-        })\n \n-\n-JSONField.register_lookup(lookups.DataContains)\n-JSONField.register_lookup(lookups.ContainedBy)\n-JSONField.register_lookup(lookups.HasKey)\n-JSONField.register_lookup(lookups.HasKeys)\n-JSONField.register_lookup(lookups.HasAnyKeys)\n-JSONField.register_lookup(lookups.JSONExact)\n-\n-\n-class KeyTransform(Transform):\n-    operator = '->'\n-    nested_operator = '#>'\n-\n-    def __init__(self, key_name, *args, **kwargs):\n+class KeyTransform(BuiltinKeyTransform):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            'django.contrib.postgres.fields.jsonb.KeyTransform is deprecated in favor of '\n+            'django.db.models.fields.json.KeyTransform',\n+            RemovedInDjango40Warning, stacklevel=2\n+        )\n         super().__init__(*args, **kwargs)\n-        self.key_name = key_name\n \n-    def as_sql(self, compiler, connection):\n-        key_transforms = [self.key_name]\n-        previous = self.lhs\n-        while isinstance(previous, KeyTransform):\n-            key_transforms.insert(0, previous.key_name)\n-            previous = previous.lhs\n-        lhs, params = compiler.compile(previous)\n-        if len(key_transforms) > 1:\n-            return '(%s %s %%s)' % (lhs, self.nested_operator), params + [key_transforms]\n-        try:\n-            lookup = int(self.key_name)\n-        except ValueError:\n-            lookup = self.key_name\n-        return '(%s %s %%s)' % (lhs, self.operator), tuple(params) + (lookup,)\n \n-\n-class KeyTextTransform(KeyTransform):\n-    operator = '->>'\n-    nested_operator = '#>>'\n-    output_field = TextField()\n-\n-\n-class KeyTransformTextLookupMixin:\n-    \"\"\"\n-    Mixin for combining with a lookup expecting a text lhs from a JSONField\n-    key lookup. Make use of the ->> operator instead of casting key values to\n-    text and performing the lookup on the resulting representation.\n-    \"\"\"\n-    def __init__(self, key_transform, *args, **kwargs):\n-        assert isinstance(key_transform, KeyTransform)\n-        key_text_transform = KeyTextTransform(\n-            key_transform.key_name, *key_transform.source_expressions, **key_transform.extra\n+class KeyTextTransform(BuiltinKeyTextTransform):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            'django.contrib.postgres.fields.jsonb.KeyTextTransform is deprecated in favor of '\n+            'django.db.models.fields.json.KeyTextTransform',\n+            RemovedInDjango40Warning, stacklevel=2\n         )\n-        super().__init__(key_text_transform, *args, **kwargs)\n-\n-\n-class KeyTransformIExact(KeyTransformTextLookupMixin, builtin_lookups.IExact):\n-    pass\n-\n-\n-class KeyTransformIContains(KeyTransformTextLookupMixin, builtin_lookups.IContains):\n-    pass\n-\n-\n-class KeyTransformStartsWith(KeyTransformTextLookupMixin, builtin_lookups.StartsWith):\n-    pass\n-\n-\n-class KeyTransformIStartsWith(KeyTransformTextLookupMixin, builtin_lookups.IStartsWith):\n-    pass\n-\n-\n-class KeyTransformEndsWith(KeyTransformTextLookupMixin, builtin_lookups.EndsWith):\n-    pass\n-\n-\n-class KeyTransformIEndsWith(KeyTransformTextLookupMixin, builtin_lookups.IEndsWith):\n-    pass\n-\n-\n-class KeyTransformRegex(KeyTransformTextLookupMixin, builtin_lookups.Regex):\n-    pass\n-\n-\n-class KeyTransformIRegex(KeyTransformTextLookupMixin, builtin_lookups.IRegex):\n-    pass\n-\n-\n-KeyTransform.register_lookup(KeyTransformIExact)\n-KeyTransform.register_lookup(KeyTransformIContains)\n-KeyTransform.register_lookup(KeyTransformStartsWith)\n-KeyTransform.register_lookup(KeyTransformIStartsWith)\n-KeyTransform.register_lookup(KeyTransformEndsWith)\n-KeyTransform.register_lookup(KeyTransformIEndsWith)\n-KeyTransform.register_lookup(KeyTransformRegex)\n-KeyTransform.register_lookup(KeyTransformIRegex)\n-\n-\n-class KeyTransformFactory:\n-\n-    def __init__(self, key_name):\n-        self.key_name = key_name\n-\n-    def __call__(self, *args, **kwargs):\n-        return KeyTransform(self.key_name, *args, **kwargs)\n+        super().__init__(*args, **kwargs)\ndiff --git a/django/contrib/postgres/forms/jsonb.py b/django/contrib/postgres/forms/jsonb.py\nindex 2865498d689a..39d248088a99 100644\n--- a/django/contrib/postgres/forms/jsonb.py\n+++ b/django/contrib/postgres/forms/jsonb.py\n@@ -1,62 +1,16 @@\n-import json\n+import warnings\n \n-from django import forms\n-from django.utils.translation import gettext_lazy as _\n+from django.forms import JSONField as BuiltinJSONField\n+from django.utils.deprecation import RemovedInDjango40Warning\n \n __all__ = ['JSONField']\n \n \n-class InvalidJSONInput(str):\n-    pass\n-\n-\n-class JSONString(str):\n-    pass\n-\n-\n-class JSONField(forms.CharField):\n-    default_error_messages = {\n-        'invalid': _('%(value)s value must be valid JSON.'),\n-    }\n-    widget = forms.Textarea\n-\n-    def to_python(self, value):\n-        if self.disabled:\n-            return value\n-        if value in self.empty_values:\n-            return None\n-        elif isinstance(value, (list, dict, int, float, JSONString)):\n-            return value\n-        try:\n-            converted = json.loads(value)\n-        except json.JSONDecodeError:\n-            raise forms.ValidationError(\n-                self.error_messages['invalid'],\n-                code='invalid',\n-                params={'value': value},\n-            )\n-        if isinstance(converted, str):\n-            return JSONString(converted)\n-        else:\n-            return converted\n-\n-    def bound_data(self, data, initial):\n-        if self.disabled:\n-            return initial\n-        try:\n-            return json.loads(data)\n-        except json.JSONDecodeError:\n-            return InvalidJSONInput(data)\n-\n-    def prepare_value(self, value):\n-        if isinstance(value, InvalidJSONInput):\n-            return value\n-        return json.dumps(value)\n-\n-    def has_changed(self, initial, data):\n-        if super().has_changed(initial, data):\n-            return True\n-        # For purposes of seeing whether something has changed, True isn't the\n-        # same as 1 and the order of keys doesn't matter.\n-        data = self.to_python(data)\n-        return json.dumps(initial, sort_keys=True) != json.dumps(data, sort_keys=True)\n+class JSONField(BuiltinJSONField):\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            'django.contrib.postgres.forms.JSONField is deprecated in favor of '\n+            'django.forms.JSONField',\n+            RemovedInDjango40Warning, stacklevel=2\n+        )\n+        super().__init__(*args, **kwargs)\ndiff --git a/django/db/backends/base/features.py b/django/db/backends/base/features.py\nindex 45c62f463b8f..f7bf241f590b 100644\n--- a/django/db/backends/base/features.py\n+++ b/django/db/backends/base/features.py\n@@ -295,6 +295,13 @@ class BaseDatabaseFeatures:\n     # Does the backend support boolean expressions in the SELECT clause?\n     supports_boolean_expr_in_select_clause = True\n \n+    # Does the backend support JSONField?\n+    supports_json_field = True\n+    # Does the backend support primities in JSONField?\n+    supports_primitives_in_json_field = True\n+    # Can the backend introspect JSONField?\n+    can_introspect_json_field = True\n+\n     def __init__(self, connection):\n         self.connection = connection\n \ndiff --git a/django/db/backends/mysql/base.py b/django/db/backends/mysql/base.py\nindex aeb935e91fa2..c2cd1ddb4fda 100644\n--- a/django/db/backends/mysql/base.py\n+++ b/django/db/backends/mysql/base.py\n@@ -118,6 +118,7 @@ class DatabaseWrapper(BaseDatabaseWrapper):\n         'BigIntegerField': 'bigint',\n         'IPAddressField': 'char(15)',\n         'GenericIPAddressField': 'char(39)',\n+        'JSONField': 'json',\n         'NullBooleanField': 'bool',\n         'OneToOneField': 'integer',\n         'PositiveBigIntegerField': 'bigint UNSIGNED',\n@@ -339,11 +340,14 @@ def display_name(self):\n     @cached_property\n     def data_type_check_constraints(self):\n         if self.features.supports_column_check_constraints:\n-            return {\n+            check_constraints = {\n                 'PositiveBigIntegerField': '`%(column)s` >= 0',\n                 'PositiveIntegerField': '`%(column)s` >= 0',\n                 'PositiveSmallIntegerField': '`%(column)s` >= 0',\n             }\n+            if self.mysql_is_mariadb and self.mysql_version < (10, 4, 3):\n+                check_constraints['JSONField'] = 'JSON_VALID(`%(column)s`)'\n+            return check_constraints\n         return {}\n \n     @cached_property\ndiff --git a/django/db/backends/mysql/features.py b/django/db/backends/mysql/features.py\nindex 1d0cd365dbc8..bff2e7b0bff6 100644\n--- a/django/db/backends/mysql/features.py\n+++ b/django/db/backends/mysql/features.py\n@@ -153,3 +153,15 @@ def ignores_table_name_case(self):\n     def supports_default_in_lead_lag(self):\n         # To be added in https://jira.mariadb.org/browse/MDEV-12981.\n         return not self.connection.mysql_is_mariadb\n+\n+    @cached_property\n+    def supports_json_field(self):\n+        if self.connection.mysql_is_mariadb:\n+            return self.connection.mysql_version >= (10, 2, 7)\n+        return self.connection.mysql_version >= (5, 7, 8)\n+\n+    @cached_property\n+    def can_introspect_json_field(self):\n+        if self.connection.mysql_is_mariadb:\n+            return self.supports_json_field and self.can_introspect_check_constraints\n+        return self.supports_json_field\ndiff --git a/django/db/backends/mysql/introspection.py b/django/db/backends/mysql/introspection.py\nindex 9334cc57481e..926665c99e9a 100644\n--- a/django/db/backends/mysql/introspection.py\n+++ b/django/db/backends/mysql/introspection.py\n@@ -9,7 +9,7 @@\n from django.db.models.indexes import Index\n from django.utils.datastructures import OrderedSet\n \n-FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('extra', 'is_unsigned'))\n+FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('extra', 'is_unsigned', 'has_json_constraint'))\n InfoLine = namedtuple('InfoLine', 'col_name data_type max_len num_prec num_scale extra column_default is_unsigned')\n \n \n@@ -24,6 +24,7 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         FIELD_TYPE.DOUBLE: 'FloatField',\n         FIELD_TYPE.FLOAT: 'FloatField',\n         FIELD_TYPE.INT24: 'IntegerField',\n+        FIELD_TYPE.JSON: 'JSONField',\n         FIELD_TYPE.LONG: 'IntegerField',\n         FIELD_TYPE.LONGLONG: 'BigIntegerField',\n         FIELD_TYPE.SHORT: 'SmallIntegerField',\n@@ -53,6 +54,10 @@ def get_field_type(self, data_type, description):\n                 return 'PositiveIntegerField'\n             elif field_type == 'SmallIntegerField':\n                 return 'PositiveSmallIntegerField'\n+        # JSON data type is an alias to LONGTEXT on MariaDB, use check\n+        # constraints clauses to introspect JSONField.\n+        if description.has_json_constraint:\n+            return 'JSONField'\n         return field_type\n \n     def get_table_list(self, cursor):\n@@ -66,6 +71,17 @@ def get_table_description(self, cursor, table_name):\n         Return a description of the table with the DB-API cursor.description\n         interface.\"\n         \"\"\"\n+        json_constraints = {}\n+        if self.connection.mysql_is_mariadb and self.connection.features.can_introspect_json_field:\n+            cursor.execute(\"\"\"\n+                SELECT c.constraint_name AS column_name\n+                FROM information_schema.check_constraints AS c\n+                WHERE\n+                    c.table_name = %s AND\n+                    LOWER(c.check_clause) = 'json_valid(`' + LOWER(c.constraint_name) + '`)' AND\n+                    c.constraint_schema = DATABASE()\n+            \"\"\", [table_name])\n+            json_constraints = {row[0] for row in cursor.fetchall()}\n         # information_schema database gives more accurate results for some figures:\n         # - varchar length returned by cursor.description is an internal length,\n         #   not visible length (#5725)\n@@ -100,6 +116,7 @@ def to_int(i):\n                 info.column_default,\n                 info.extra,\n                 info.is_unsigned,\n+                line[0] in json_constraints,\n             ))\n         return fields\n \ndiff --git a/django/db/backends/mysql/operations.py b/django/db/backends/mysql/operations.py\nindex 534295776720..6ce48df96982 100644\n--- a/django/db/backends/mysql/operations.py\n+++ b/django/db/backends/mysql/operations.py\n@@ -327,3 +327,13 @@ def regex_lookup(self, lookup_type):\n \n     def insert_statement(self, ignore_conflicts=False):\n         return 'INSERT IGNORE INTO' if ignore_conflicts else super().insert_statement(ignore_conflicts)\n+\n+    def lookup_cast(self, lookup_type, internal_type=None):\n+        lookup = '%s'\n+        if internal_type == 'JSONField':\n+            if self.connection.mysql_is_mariadb or lookup_type in (\n+                'iexact', 'contains', 'icontains', 'startswith', 'istartswith',\n+                'endswith', 'iendswith', 'regex', 'iregex',\n+            ):\n+                lookup = 'JSON_UNQUOTE(%s)'\n+        return lookup\ndiff --git a/django/db/backends/oracle/base.py b/django/db/backends/oracle/base.py\nindex e1c76ec058ab..39444dc04ed3 100644\n--- a/django/db/backends/oracle/base.py\n+++ b/django/db/backends/oracle/base.py\n@@ -115,6 +115,7 @@ class DatabaseWrapper(BaseDatabaseWrapper):\n         'FilePathField': 'NVARCHAR2(%(max_length)s)',\n         'FloatField': 'DOUBLE PRECISION',\n         'IntegerField': 'NUMBER(11)',\n+        'JSONField': 'NCLOB',\n         'BigIntegerField': 'NUMBER(19)',\n         'IPAddressField': 'VARCHAR2(15)',\n         'GenericIPAddressField': 'VARCHAR2(39)',\n@@ -133,6 +134,7 @@ class DatabaseWrapper(BaseDatabaseWrapper):\n     }\n     data_type_check_constraints = {\n         'BooleanField': '%(qn_column)s IN (0,1)',\n+        'JSONField': '%(qn_column)s IS JSON',\n         'NullBooleanField': '%(qn_column)s IN (0,1)',\n         'PositiveBigIntegerField': '%(qn_column)s >= 0',\n         'PositiveIntegerField': '%(qn_column)s >= 0',\ndiff --git a/django/db/backends/oracle/features.py b/django/db/backends/oracle/features.py\nindex 73a6e8668648..0ad193f77111 100644\n--- a/django/db/backends/oracle/features.py\n+++ b/django/db/backends/oracle/features.py\n@@ -59,3 +59,4 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n     supports_slicing_ordering_in_compound = True\n     allows_multiple_constraints_on_same_fields = False\n     supports_boolean_expr_in_select_clause = False\n+    supports_primitives_in_json_field = False\ndiff --git a/django/db/backends/oracle/introspection.py b/django/db/backends/oracle/introspection.py\nindex 2322ae0b5d6a..3fab497b2aec 100644\n--- a/django/db/backends/oracle/introspection.py\n+++ b/django/db/backends/oracle/introspection.py\n@@ -7,7 +7,7 @@\n     BaseDatabaseIntrospection, FieldInfo as BaseFieldInfo, TableInfo,\n )\n \n-FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('is_autofield',))\n+FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('is_autofield', 'is_json'))\n \n \n class DatabaseIntrospection(BaseDatabaseIntrospection):\n@@ -45,6 +45,8 @@ def get_field_type(self, data_type, description):\n                     return 'IntegerField'\n             elif scale == -127:\n                 return 'FloatField'\n+        elif data_type == cx_Oracle.NCLOB and description.is_json:\n+            return 'JSONField'\n \n         return super().get_field_type(data_type, description)\n \n@@ -83,12 +85,23 @@ def get_table_description(self, cursor, table_name):\n                 CASE\n                     WHEN identity_column = 'YES' THEN 1\n                     ELSE 0\n-                END as is_autofield\n+                END as is_autofield,\n+                CASE\n+                    WHEN EXISTS (\n+                        SELECT  1\n+                        FROM user_json_columns\n+                        WHERE\n+                            user_json_columns.table_name = user_tab_cols.table_name AND\n+                            user_json_columns.column_name = user_tab_cols.column_name\n+                    )\n+                    THEN 1\n+                    ELSE 0\n+                END as is_json\n             FROM user_tab_cols\n             WHERE table_name = UPPER(%s)\"\"\", [table_name])\n         field_map = {\n-            column: (internal_size, default if default != 'NULL' else None, is_autofield)\n-            for column, default, internal_size, is_autofield in cursor.fetchall()\n+            column: (internal_size, default if default != 'NULL' else None, is_autofield, is_json)\n+            for column, default, internal_size, is_autofield, is_json in cursor.fetchall()\n         }\n         self.cache_bust_counter += 1\n         cursor.execute(\"SELECT * FROM {} WHERE ROWNUM < 2 AND {} > 0\".format(\n@@ -97,11 +110,11 @@ def get_table_description(self, cursor, table_name):\n         description = []\n         for desc in cursor.description:\n             name = desc[0]\n-            internal_size, default, is_autofield = field_map[name]\n+            internal_size, default, is_autofield, is_json = field_map[name]\n             name = name % {}  # cx_Oracle, for some reason, doubles percent signs.\n             description.append(FieldInfo(\n                 self.identifier_converter(name), *desc[1:3], internal_size, desc[4] or 0,\n-                desc[5] or 0, *desc[6:], default, is_autofield,\n+                desc[5] or 0, *desc[6:], default, is_autofield, is_json,\n             ))\n         return description\n \ndiff --git a/django/db/backends/oracle/operations.py b/django/db/backends/oracle/operations.py\nindex 358c91505f66..59ed65510068 100644\n--- a/django/db/backends/oracle/operations.py\n+++ b/django/db/backends/oracle/operations.py\n@@ -175,7 +175,7 @@ def time_trunc_sql(self, lookup_type, field_name):\n     def get_db_converters(self, expression):\n         converters = super().get_db_converters(expression)\n         internal_type = expression.output_field.get_internal_type()\n-        if internal_type == 'TextField':\n+        if internal_type in ['JSONField', 'TextField']:\n             converters.append(self.convert_textfield_value)\n         elif internal_type == 'BinaryField':\n             converters.append(self.convert_binaryfield_value)\ndiff --git a/django/db/backends/postgresql/base.py b/django/db/backends/postgresql/base.py\nindex 0ab81ced74fd..3136367b63a6 100644\n--- a/django/db/backends/postgresql/base.py\n+++ b/django/db/backends/postgresql/base.py\n@@ -86,6 +86,7 @@ class DatabaseWrapper(BaseDatabaseWrapper):\n         'BigIntegerField': 'bigint',\n         'IPAddressField': 'inet',\n         'GenericIPAddressField': 'inet',\n+        'JSONField': 'jsonb',\n         'NullBooleanField': 'boolean',\n         'OneToOneField': 'integer',\n         'PositiveBigIntegerField': 'bigint',\ndiff --git a/django/db/backends/postgresql/introspection.py b/django/db/backends/postgresql/introspection.py\nindex b7bf18691c39..feb54643b9c7 100644\n--- a/django/db/backends/postgresql/introspection.py\n+++ b/django/db/backends/postgresql/introspection.py\n@@ -26,6 +26,7 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):\n         1266: 'TimeField',\n         1700: 'DecimalField',\n         2950: 'UUIDField',\n+        3802: 'JSONField',\n     }\n \n     ignored_tables = []\ndiff --git a/django/db/backends/sqlite3/base.py b/django/db/backends/sqlite3/base.py\nindex a3ae1f048ef3..b017b5f5c884 100644\n--- a/django/db/backends/sqlite3/base.py\n+++ b/django/db/backends/sqlite3/base.py\n@@ -5,6 +5,7 @@\n import decimal\n import functools\n import hashlib\n+import json\n import math\n import operator\n import re\n@@ -100,6 +101,7 @@ class DatabaseWrapper(BaseDatabaseWrapper):\n         'BigIntegerField': 'bigint',\n         'IPAddressField': 'char(15)',\n         'GenericIPAddressField': 'char(39)',\n+        'JSONField': 'text',\n         'NullBooleanField': 'bool',\n         'OneToOneField': 'integer',\n         'PositiveBigIntegerField': 'bigint unsigned',\n@@ -114,6 +116,7 @@ class DatabaseWrapper(BaseDatabaseWrapper):\n     }\n     data_type_check_constraints = {\n         'PositiveBigIntegerField': '\"%(column)s\" >= 0',\n+        'JSONField': '(JSON_VALID(\"%(column)s\") OR \"%(column)s\" IS NULL)',\n         'PositiveIntegerField': '\"%(column)s\" >= 0',\n         'PositiveSmallIntegerField': '\"%(column)s\" >= 0',\n     }\n@@ -213,6 +216,7 @@ def get_new_connection(self, conn_params):\n         conn.create_function(\"django_time_diff\", 2, _sqlite_time_diff)\n         conn.create_function(\"django_timestamp_diff\", 2, _sqlite_timestamp_diff)\n         conn.create_function(\"django_format_dtdelta\", 3, _sqlite_format_dtdelta)\n+        conn.create_function(\"django_json_contains\", 2, _sqlite_json_contains)\n         conn.create_function('regexp', 2, _sqlite_regexp)\n         conn.create_function('ACOS', 1, none_guard(math.acos))\n         conn.create_function('ASIN', 1, none_guard(math.asin))\n@@ -588,3 +592,12 @@ def _sqlite_lpad(text, length, fill_text):\n @none_guard\n def _sqlite_rpad(text, length, fill_text):\n     return (text + fill_text * length)[:length]\n+\n+\n+@none_guard\n+def _sqlite_json_contains(haystack, needle):\n+    target, candidate = json.loads(haystack), json.loads(needle)\n+    if isinstance(target, dict) and isinstance(candidate, dict):\n+        return target.items() >= candidate.items()\n+    else:\n+        return target == candidate\ndiff --git a/django/db/backends/sqlite3/features.py b/django/db/backends/sqlite3/features.py\nindex 6aebbc32627a..2301ed0b889e 100644\n--- a/django/db/backends/sqlite3/features.py\n+++ b/django/db/backends/sqlite3/features.py\n@@ -1,4 +1,9 @@\n+import operator\n+\n+from django.db import transaction\n from django.db.backends.base.features import BaseDatabaseFeatures\n+from django.db.utils import OperationalError\n+from django.utils.functional import cached_property\n \n from .base import Database\n \n@@ -45,3 +50,14 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n     supports_frame_range_fixed_distance = Database.sqlite_version_info >= (3, 28, 0)\n     supports_aggregate_filter_clause = Database.sqlite_version_info >= (3, 30, 1)\n     supports_order_by_nulls_modifier = Database.sqlite_version_info >= (3, 30, 0)\n+\n+    @cached_property\n+    def supports_json_field(self):\n+        try:\n+            with self.connection.cursor() as cursor, transaction.atomic():\n+                cursor.execute('SELECT JSON(\\'{\"a\": \"b\"}\\')')\n+        except OperationalError:\n+            return False\n+        return True\n+\n+    can_introspect_json_field = property(operator.attrgetter('supports_json_field'))\ndiff --git a/django/db/backends/sqlite3/introspection.py b/django/db/backends/sqlite3/introspection.py\nindex 2b5e732a4737..d87ff4633100 100644\n--- a/django/db/backends/sqlite3/introspection.py\n+++ b/django/db/backends/sqlite3/introspection.py\n@@ -9,7 +9,7 @@\n from django.db.models.indexes import Index\n from django.utils.regex_helper import _lazy_re_compile\n \n-FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('pk',))\n+FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('pk', 'has_json_constraint'))\n \n field_size_re = _lazy_re_compile(r'^\\s*(?:var)?char\\s*\\(\\s*(\\d+)\\s*\\)\\s*$')\n \n@@ -63,6 +63,8 @@ def get_field_type(self, data_type, description):\n             # No support for BigAutoField or SmallAutoField as SQLite treats\n             # all integer primary keys as signed 64-bit integers.\n             return 'AutoField'\n+        if description.has_json_constraint:\n+            return 'JSONField'\n         return field_type\n \n     def get_table_list(self, cursor):\n@@ -81,12 +83,24 @@ def get_table_description(self, cursor, table_name):\n         interface.\n         \"\"\"\n         cursor.execute('PRAGMA table_info(%s)' % self.connection.ops.quote_name(table_name))\n+        table_info = cursor.fetchall()\n+        json_columns = set()\n+        if self.connection.features.can_introspect_json_field:\n+            for line in table_info:\n+                column = line[1]\n+                has_json_constraint = cursor.execute(\n+                    \"SELECT sql FROM sqlite_master WHERE type='table' AND name=%s \"\n+                    \"AND sql LIKE '%%json_valid(%s)%%'\" %\n+                    (self.connection.ops.quote_name(table_name), self.connection.ops.quote_name(column))\n+                ).fetchone()\n+                if has_json_constraint:\n+                    json_columns.add(column)\n         return [\n             FieldInfo(\n                 name, data_type, None, get_field_size(data_type), None, None,\n-                not notnull, default, pk == 1,\n+                not notnull, default, pk == 1, name in json_columns\n             )\n-            for cid, name, data_type, notnull, default, pk in cursor.fetchall()\n+            for cid, name, data_type, notnull, default, pk in table_info\n         ]\n \n     def get_sequences(self, cursor, table_name, table_fields=()):\ndiff --git a/django/db/models/__init__.py b/django/db/models/__init__.py\nindex 06bd72f8b98e..b81482e3b6f1 100644\n--- a/django/db/models/__init__.py\n+++ b/django/db/models/__init__.py\n@@ -17,6 +17,7 @@\n from django.db.models.fields import *  # NOQA\n from django.db.models.fields import __all__ as fields_all\n from django.db.models.fields.files import FileField, ImageField\n+from django.db.models.fields.json import JSONField\n from django.db.models.fields.proxy import OrderWrt\n from django.db.models.indexes import *  # NOQA\n from django.db.models.indexes import __all__ as indexes_all\n@@ -43,9 +44,9 @@\n     'Case', 'Exists', 'Expression', 'ExpressionList', 'ExpressionWrapper', 'F',\n     'Func', 'OuterRef', 'RowRange', 'Subquery', 'Value', 'ValueRange', 'When',\n     'Window', 'WindowFrame',\n-    'FileField', 'ImageField', 'OrderWrt', 'Lookup', 'Transform', 'Manager',\n-    'Prefetch', 'Q', 'QuerySet', 'prefetch_related_objects', 'DEFERRED', 'Model',\n-    'FilteredRelation',\n+    'FileField', 'ImageField', 'JSONField', 'OrderWrt', 'Lookup', 'Transform',\n+    'Manager', 'Prefetch', 'Q', 'QuerySet', 'prefetch_related_objects',\n+    'DEFERRED', 'Model', 'FilteredRelation',\n     'ForeignKey', 'ForeignObject', 'OneToOneField', 'ManyToManyField',\n     'ManyToOneRel', 'ManyToManyRel', 'OneToOneRel',\n ]\ndiff --git a/django/db/models/base.py b/django/db/models/base.py\nindex 24453e218a43..60d77d5becf0 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -16,7 +16,7 @@\n     connections, router, transaction,\n )\n from django.db.models import (\n-    NOT_PROVIDED, ExpressionWrapper, IntegerField, Max, Value,\n+    NOT_PROVIDED, ExpressionWrapper, IntegerField, JSONField, Max, Value,\n )\n from django.db.models.constants import LOOKUP_SEP\n from django.db.models.constraints import CheckConstraint, UniqueConstraint\n@@ -1333,13 +1333,35 @@ def _check_managers(cls, **kwargs):\n     @classmethod\n     def _check_fields(cls, **kwargs):\n         \"\"\"Perform all field checks.\"\"\"\n-        errors = []\n+        errors = cls._check_json_fields(**kwargs)\n         for field in cls._meta.local_fields:\n             errors.extend(field.check(**kwargs))\n         for field in cls._meta.local_many_to_many:\n             errors.extend(field.check(from_model=cls, **kwargs))\n         return errors\n \n+    @classmethod\n+    def _check_json_fields(cls, **kwargs):\n+        errors = []\n+        for db in settings.DATABASES:\n+            if not router.allow_migrate_model(db, cls):\n+                continue\n+            connection = connections[db]\n+            if (\n+                'supports_json_field' in cls._meta.required_db_features or\n+                connection.features.supports_json_field\n+            ):\n+                continue\n+            if any(isinstance(field, JSONField) for field in cls._meta.local_fields):\n+                errors.append(\n+                    checks.Error(\n+                        '%s does not support JSONFields.' % connection.display_name,\n+                        obj=cls,\n+                        id='models.E036',\n+                    )\n+                )\n+        return errors\n+\n     @classmethod\n     def _check_m2m_through_same_relationship(cls):\n         \"\"\" Check if no relationship model is used by more than one m2m field.\ndiff --git a/django/db/models/fields/json.py b/django/db/models/fields/json.py\nnew file mode 100644\nindex 000000000000..a4d3d22f833e\n--- /dev/null\n+++ b/django/db/models/fields/json.py\n@@ -0,0 +1,527 @@\n+import json\n+\n+from django import forms\n+from django.core import exceptions\n+from django.db.models import lookups\n+from django.db.models.lookups import (\n+    FieldGetDbPrepValueMixin, Lookup, Transform,\n+)\n+from django.db.utils import NotSupportedError\n+from django.utils.translation import gettext_lazy as _\n+\n+from . import Field\n+from .mixins import CheckFieldDefaultMixin\n+\n+\n+class JSONField(CheckFieldDefaultMixin, Field):\n+    description = _('A JSON object')\n+    default_error_messages = {\n+        'invalid': _('Value must be valid JSON.'),\n+    }\n+    _default_hint = ('dict', '{}')\n+\n+    def __init__(\n+        self, verbose_name=None, name=None, encoder=None,\n+        decoder=None, *args, **kwargs\n+    ):\n+        self.encoder, self.decoder = encoder, decoder\n+        super().__init__(verbose_name, name, *args, **kwargs)\n+\n+    def deconstruct(self):\n+        name, path, args, kwargs = super().deconstruct()\n+        path = 'django.db.models.JSONField'\n+        if self.encoder is not None:\n+            kwargs['encoder'] = self.encoder\n+        if self.decoder is not None:\n+            kwargs['decoder'] = self.decoder\n+        return name, path, args, kwargs\n+\n+    def from_db_value(self, value, expression, connection):\n+        if value is None:\n+            return value\n+        elif connection.vendor == 'postgresql' and self.decoder is None:\n+            return value\n+        else:\n+            try:\n+                return json.loads(value, cls=self.decoder)\n+            except json.JSONDecodeError:\n+                return value\n+\n+    def get_internal_type(self):\n+        return 'JSONField'\n+\n+    def get_prep_value(self, value):\n+        if value is None:\n+            return value\n+        return json.dumps(value, cls=self.encoder)\n+\n+    def get_transform(self, name):\n+        transform = super().get_transform(name)\n+        if transform:\n+            return transform\n+        return KeyTransformFactory(name)\n+\n+    def select_format(self, compiler, sql, params):\n+        if compiler.connection.vendor == 'postgresql' and self.decoder is not None:\n+            # Avoid psycopg2's automatic decoding to allow custom decoder\n+            return '%s::text' % sql, params\n+        return super().select_format(compiler, sql, params)\n+\n+    def validate(self, value, model_instance):\n+        super().validate(value, model_instance)\n+        try:\n+            json.dumps(value, cls=self.encoder)\n+        except TypeError:\n+            raise exceptions.ValidationError(\n+                self.error_messages['invalid'],\n+                code='invalid',\n+                params={'value': value},\n+            )\n+\n+    def value_to_string(self, obj):\n+        return self.value_from_object(obj)\n+\n+    def formfield(self, **kwargs):\n+        return super().formfield(**{\n+            'form_class': forms.JSONField,\n+            'encoder': self.encoder,\n+            'decoder': self.decoder,\n+            **kwargs,\n+        })\n+\n+\n+def compile_json_path(key_transforms):\n+    path = ['$']\n+    for key_transform in key_transforms:\n+        try:\n+            num = int(key_transform)\n+        except ValueError:  # non-integer\n+            path.append('.')\n+            path.append(json.dumps(key_transform))\n+        else:\n+            path.append('[%s]' % num)\n+    return ''.join(path)\n+\n+\n+class SimpleFunctionOperatorMixin(FieldGetDbPrepValueMixin):\n+    def as_sql_function(self, compiler, connection, template, flipped=False):\n+        lhs, lhs_params = self.process_lhs(compiler, connection)\n+        rhs, rhs_params = self.process_rhs(compiler, connection)\n+        if flipped:\n+            return template % (rhs, lhs), tuple(rhs_params) + tuple(lhs_params)\n+        else:\n+            return template % (lhs, rhs), tuple(lhs_params) + tuple(rhs_params)\n+\n+    def as_sql_operator(self, compiler, connection, operator):\n+        lhs, lhs_params = self.process_lhs(compiler, connection)\n+        rhs, rhs_params = self.process_rhs(compiler, connection)\n+        params = tuple(lhs_params) + tuple(rhs_params)\n+        return '%s %s %s' % (lhs, operator, rhs), params\n+\n+    def as_postgresql(self, compiler, connection):\n+        return self.as_sql_operator(compiler, connection, self.postgres_operator)\n+\n+    def as_sql(self, compiler, connection):\n+        raise NotSupportedError(\n+            '%s lookup is not supported by this database backend.' % self.lookup_name\n+        )\n+\n+\n+class HasKeyLookup(SimpleFunctionOperatorMixin, Lookup):\n+    logical_operator = None\n+\n+    def as_sql(self, compiler, connection, template=None):\n+        if isinstance(self.lhs, KeyTransform):\n+            lhs, lhs_params, lhs_key_transforms = self.lhs.preprocess_lhs(compiler, connection)\n+        else:\n+            lhs, lhs_params, lhs_key_transforms = *compiler.compile(self.lhs), []\n+        rhs = [self.rhs] if not isinstance(self.rhs, (list, tuple)) else list(self.rhs)\n+        rhs_params = []\n+        for key in rhs:\n+            if isinstance(key, str):\n+                rhs_params.insert(0, compile_json_path(lhs_key_transforms + [key]))\n+            else:\n+                if isinstance(key, KeyTransform):\n+                    _, _, key_transforms = key.preprocess_lhs(compiler, connection)\n+                else:\n+                    key_transforms = []\n+                rhs_params.insert(0, compile_json_path(lhs_key_transforms + key_transforms))\n+        sql = template % lhs\n+        if self.logical_operator:\n+            # Add condition for each key.\n+            sql = '(%s)' % self.logical_operator.join([sql] * len(rhs_params))\n+        return sql, tuple(lhs_params) + tuple(rhs_params)\n+\n+    def as_mysql(self, compiler, connection):\n+        return self.as_sql(compiler, connection, template=\"JSON_CONTAINS_PATH(%s, 'one', %%s)\")\n+\n+    def as_oracle(self, compiler, connection):\n+        sql, params = self.as_sql(compiler, connection, template=\"JSON_EXISTS(%s, '%%s')\")\n+        # Add paths directly into SQL because path expressions cannot be passed\n+        # as bind variables on Oracle.\n+        return sql % tuple(params), []\n+\n+    def as_sqlite(self, compiler, connection):\n+        return self.as_sql(compiler, connection, template='JSON_TYPE(%s, %%s) IS NOT NULL')\n+\n+\n+@JSONField.register_lookup\n+class HasKey(HasKeyLookup):\n+    lookup_name = 'has_key'\n+    postgres_operator = '?'\n+    prepare_rhs = False\n+\n+\n+@JSONField.register_lookup\n+class HasAnyKeys(HasKeyLookup):\n+    lookup_name = 'has_any_keys'\n+    postgres_operator = '?|'\n+    logical_operator = ' OR '\n+\n+    def get_prep_lookup(self):\n+        return [\n+            str(item) if not isinstance(item, KeyTransform) else item\n+            for item in self.rhs\n+        ]\n+\n+\n+@JSONField.register_lookup\n+class HasKeys(HasAnyKeys):\n+    lookup_name = 'has_keys'\n+    postgres_operator = '?&'\n+    logical_operator = ' AND '\n+\n+\n+@JSONField.register_lookup\n+class DataContains(SimpleFunctionOperatorMixin, Lookup):\n+    lookup_name = 'contains'\n+    postgres_operator = '@>'\n+\n+    def as_mysql(self, compiler, connection, flipped=False):\n+        return super().as_sql_function(\n+            compiler, connection, template=\"JSON_CONTAINS(%s, %s, '$')\", flipped=flipped\n+        )\n+\n+    def as_oracle(self, compiler, connection):\n+        lhs, lhs_params = self.process_lhs(compiler, connection)\n+        if isinstance(self.rhs, KeyTransform):\n+            _, _, key_transforms = self.rhs.preprocess_lhs(compiler, connection)\n+            return \"JSON_EXISTS(%s, '%s')\" % (lhs, compile_json_path(key_transforms)), []\n+        else:\n+            rhs = json.loads(self.rhs)\n+        if isinstance(rhs, dict):\n+            if not rhs:\n+                return \"DBMS_LOB.SUBSTR(%s) LIKE '{%%%%}'\" % lhs, []\n+            conditions = []\n+            for key, value in rhs.items():\n+                k = json.dumps(key)\n+                if value is None:\n+                    conditions.append(\n+                        \"(JSON_EXISTS(%s, '$.%s') AND\"\n+                        \" COALESCE(JSON_QUERY(%s, '$.%s'), JSON_VALUE(%s, '$.%s')) IS NULL)\" % ((lhs, k) * 3)\n+                    )\n+                elif isinstance(value, (list, dict)):\n+                    conditions.append(\n+                        \"JSON_QUERY(%s, '$.%s') = JSON_QUERY('{\\\"val\\\": %s}', '$.val')\" % (lhs, k, json.dumps(value))\n+                    )\n+                else:\n+                    conditions.append(\n+                        \"JSON_VALUE(%s, '$.%s') = JSON_VALUE('{\\\"val\\\": %s}', '$.val')\" % (lhs, k, json.dumps(value))\n+                    )\n+            return ' AND '.join(conditions), []\n+        else:\n+            return 'DBMS_LOB.SUBSTR(%s) = %%s' % lhs, [self.rhs]\n+\n+    def as_sqlite(self, compiler, connection, flipped=False):\n+        return super().as_sql_function(\n+            compiler, connection, template='django_json_contains(%s, %s)', flipped=flipped\n+        )\n+\n+\n+@JSONField.register_lookup\n+class ContainedBy(DataContains):\n+    lookup_name = 'contained_by'\n+    postgres_operator = '<@'\n+\n+    def as_mysql(self, compiler, connection):\n+        return super().as_mysql(compiler, connection, flipped=True)\n+\n+    def as_oracle(self, compiler, connection):\n+        return super().as_sql(compiler, connection)\n+\n+    def as_sqlite(self, compiler, connection):\n+        return super().as_sqlite(compiler, connection, flipped=True)\n+\n+\n+@JSONField.register_lookup\n+class JSONExact(lookups.Exact):\n+    can_use_none_as_rhs = True\n+\n+    def process_lhs(self, compiler, connection):\n+        lhs, lhs_params = super().process_lhs(compiler, connection)\n+        if connection.vendor == 'sqlite':\n+            rhs, rhs_params = super().process_rhs(compiler, connection)\n+            if rhs == '%s' and rhs_params == [None]:\n+                # Need to use JSON_TYPE instead of JSON_EXTRACT\n+                # to determine JSON null values.\n+                lhs = \"JSON_TYPE(%s, '$')\" % lhs\n+        return lhs, lhs_params\n+\n+    def process_rhs(self, compiler, connection):\n+        rhs, rhs_params = super().process_rhs(compiler, connection)\n+        # Treat None lookup values as null.\n+        if rhs == '%s' and rhs_params == [None]:\n+            rhs, rhs_params = ('%s', ['null'])\n+        if connection.vendor == 'mysql':\n+            func = [\"JSON_EXTRACT(%s, '$')\" for value in rhs_params]\n+            rhs = rhs % tuple(func)\n+        return rhs, rhs_params\n+\n+\n+class KeyTransform(Transform):\n+    postgres_operator = '->'\n+    postgres_nested_operator = '#>'\n+\n+    def __init__(self, key_name, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.key_name = key_name\n+\n+    def preprocess_lhs(self, compiler, connection, lhs_only=False):\n+        if not lhs_only:\n+            key_transforms = [self.key_name]\n+        previous = self.lhs\n+        while isinstance(previous, KeyTransform):\n+            if not lhs_only:\n+                key_transforms.insert(0, previous.key_name)\n+            previous = previous.lhs\n+        lhs, params = compiler.compile(previous)\n+        return (lhs, params, key_transforms) if not lhs_only else (lhs, params)\n+\n+    def as_mysql(self, compiler, connection):\n+        lhs, params, key_transforms = self.preprocess_lhs(compiler, connection)\n+        json_path = compile_json_path(key_transforms)\n+        return 'JSON_EXTRACT(%s, %%s)' % lhs, tuple(params) + (json_path,)\n+\n+    def as_oracle(self, compiler, connection):\n+        lhs, params, key_transforms = self.preprocess_lhs(compiler, connection)\n+        json_path = compile_json_path(key_transforms)\n+        return \"COALESCE(JSON_QUERY(%s, '%s'), JSON_VALUE(%s, '%s'))\" % ((lhs, json_path) * 2), tuple(params)\n+\n+    def as_postgresql(self, compiler, connection):\n+        lhs, params, key_transforms = self.preprocess_lhs(compiler, connection)\n+        if len(key_transforms) > 1:\n+            return '(%s %s %%s)' % (lhs, self.postgres_nested_operator), params + [key_transforms]\n+        try:\n+            lookup = int(self.key_name)\n+        except ValueError:\n+            lookup = self.key_name\n+        return '(%s %s %%s)' % (lhs, self.postgres_operator), tuple(params) + (lookup,)\n+\n+    def as_sqlite(self, compiler, connection):\n+        lhs, params, key_transforms = self.preprocess_lhs(compiler, connection)\n+        json_path = compile_json_path(key_transforms)\n+        return 'JSON_EXTRACT(%s, %%s)' % lhs, tuple(params) + (json_path,)\n+\n+\n+class KeyTextTransform(KeyTransform):\n+    postgres_operator = '->>'\n+    postgres_nested_operator = '#>>'\n+\n+\n+class KeyTransformTextLookupMixin:\n+    \"\"\"\n+    Mixin for combining with a lookup expecting a text lhs from a JSONField\n+    key lookup. On PostgreSQL, make use of the ->> operator instead of casting\n+    key values to text and performing the lookup on the resulting representation.\n+    On MySQL, JSON_UNQUOTE is applied to the lhs.\n+    \"\"\"\n+    def __init__(self, key_transform, *args, **kwargs):\n+        if not isinstance(key_transform, KeyTransform):\n+            raise TypeError(\n+                'Transform should be an instance of KeyTransform in order to use this lookup.'\n+            )\n+        key_text_transform = KeyTextTransform(\n+            key_transform.key_name, *key_transform.source_expressions, **key_transform.extra\n+        )\n+        super().__init__(key_text_transform, *args, **kwargs)\n+\n+\n+class KeyTransformNumericLookupMixin:\n+    def process_rhs(self, compiler, connection):\n+        rhs, rhs_params = super().process_rhs(compiler, connection)\n+        if connection.vendor != 'postgresql':\n+            rhs_params = [json.loads(value) for value in rhs_params]\n+        return rhs, rhs_params\n+\n+\n+class CaseInsensitiveMixin:\n+    \"\"\"\n+    Mixin to allow case-insensitive comparison of JSON values on MySQL.\n+    MySQL handles strings used in JSON context using the utf8mb4_bin collation.\n+    Because utf8mb4_bin is a binary collation, comparison of JSON values is case-sensitive.\n+    \"\"\"\n+    def process_lhs(self, compiler, connection):\n+        lhs, lhs_params = super().process_lhs(compiler, connection)\n+        if connection.vendor == 'mysql':\n+            return 'LOWER(%s)' % lhs, lhs_params\n+        return lhs, lhs_params\n+\n+    def process_rhs(self, compiler, connection):\n+        rhs, rhs_params = super().process_rhs(compiler, connection)\n+        if connection.vendor == 'mysql':\n+            return 'LOWER(%s)' % rhs, rhs_params\n+        return rhs, rhs_params\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformIsNull(lookups.IsNull):\n+    def as_oracle(self, compiler, connection):\n+        if isinstance(self.lhs, KeyTransform):\n+            prev_lhs, prev_params, key_transforms = self.lhs.preprocess_lhs(compiler, connection)\n+        else:\n+            prev_lhs, prev_params, key_transforms = *compiler.compile(self.lhs), []\n+        json_path = compile_json_path(key_transforms)\n+        if self.rhs:\n+            return (\n+                \"(NOT JSON_EXISTS(%s, '%s') OR JSON_QUERY(%s, '$') IS NULL)\" % (prev_lhs, json_path, prev_lhs),\n+                prev_params\n+            )\n+        else:\n+            return \"JSON_EXISTS(%s, '%s')\" % (prev_lhs, json_path), prev_params\n+\n+    def as_sqlite(self, compiler, connection):\n+        lhs, lhs_params = super().process_lhs(compiler, connection)\n+        if isinstance(self.lhs, KeyTransform):\n+            prev_lhs, prev_params = self.lhs.preprocess_lhs(compiler, connection, lhs_only=True)\n+        else:\n+            prev_lhs, prev_params = compiler.compile(self.lhs)\n+        if self.rhs:\n+            return 'JSON_TYPE(%s, %%s) IS NULL' % prev_lhs, lhs_params\n+        else:\n+            return 'JSON_TYPE(%s, %%s) IS NOT NULL' % prev_lhs, lhs_params\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformExact(JSONExact):\n+    def process_lhs(self, compiler, connection):\n+        lhs, lhs_params = super().process_lhs(compiler, connection)\n+        if connection.vendor == 'sqlite':\n+            rhs, rhs_params = super().process_rhs(compiler, connection)\n+            if rhs == '%s' and rhs_params == ['null']:\n+                if isinstance(self.lhs, KeyTransform):\n+                    lhs, params = self.lhs.preprocess_lhs(compiler, connection, lhs_only=True)\n+                else:\n+                    lhs, params = compiler.compile(self.lhs)\n+                lhs = 'JSON_TYPE(%s, %%s)' % lhs\n+        return lhs, lhs_params\n+\n+    def process_rhs(self, compiler, connection):\n+        if isinstance(self.rhs, KeyTransform):\n+            rhs, rhs_params = super(lookups.Exact, self).process_rhs(compiler, connection)\n+            if connection.vendor == 'oracle':\n+                rhs_params *= 2\n+            return rhs, rhs_params\n+        else:\n+            rhs, rhs_params = super().process_rhs(compiler, connection)\n+        if connection.vendor == 'oracle':\n+            func = []\n+            for value in rhs_params:\n+                val = json.loads(value)\n+                if isinstance(val, (list, dict)):\n+                    func.append(\"JSON_QUERY('{\\\"val\\\": %s}', '$.val')\" % value)\n+                else:\n+                    func.append(\"JSON_VALUE('{\\\"val\\\": %s}', '$.val')\" % value)\n+            rhs = rhs % tuple(func)\n+            rhs_params = []\n+        elif connection.vendor == 'sqlite':\n+            func = [\"JSON_EXTRACT(%s, '$')\" if value != 'null' else '%s' for value in rhs_params]\n+            rhs = rhs % tuple(func)\n+        return rhs, rhs_params\n+\n+    def as_oracle(self, compiler, connection):\n+        rhs, rhs_params = super().process_rhs(compiler, connection)\n+        if rhs_params == ['null']:\n+            lhs, lhs_params = self.process_lhs(compiler, connection)\n+            if isinstance(self.lhs, KeyTransform):\n+                prev_lhs, _, key_transforms = self.lhs.preprocess_lhs(compiler, connection)\n+            else:\n+                prev_lhs, _, key_transforms = *compiler.compile(self.lhs), []\n+            json_path = compile_json_path(key_transforms)\n+            sql = \"(JSON_EXISTS(%s, '%s') AND %s IS NULL)\" % (prev_lhs, json_path, lhs)\n+            return sql, []\n+        else:\n+            return super().as_sql(compiler, connection)\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformIExact(CaseInsensitiveMixin, KeyTransformTextLookupMixin, lookups.IExact):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformIContains(CaseInsensitiveMixin, KeyTransformTextLookupMixin, lookups.IContains):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformContains(KeyTransformTextLookupMixin, lookups.Contains):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformStartsWith(KeyTransformTextLookupMixin, lookups.StartsWith):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformIStartsWith(CaseInsensitiveMixin, KeyTransformTextLookupMixin, lookups.IStartsWith):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformEndsWith(KeyTransformTextLookupMixin, lookups.EndsWith):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformIEndsWith(CaseInsensitiveMixin, KeyTransformTextLookupMixin, lookups.IEndsWith):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformRegex(KeyTransformTextLookupMixin, lookups.Regex):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformIRegex(CaseInsensitiveMixin, KeyTransformTextLookupMixin, lookups.IRegex):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformLte(KeyTransformNumericLookupMixin, lookups.LessThanOrEqual):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformLt(KeyTransformNumericLookupMixin, lookups.LessThan):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformGte(KeyTransformNumericLookupMixin, lookups.GreaterThanOrEqual):\n+    pass\n+\n+\n+@KeyTransform.register_lookup\n+class KeyTransformGt(KeyTransformNumericLookupMixin, lookups.GreaterThan):\n+    pass\n+\n+\n+class KeyTransformFactory:\n+\n+    def __init__(self, key_name):\n+        self.key_name = key_name\n+\n+    def __call__(self, *args, **kwargs):\n+        return KeyTransform(self.key_name, *args, **kwargs)\ndiff --git a/django/forms/fields.py b/django/forms/fields.py\nindex c5374c7e9dba..f26ca1fbe15f 100644\n--- a/django/forms/fields.py\n+++ b/django/forms/fields.py\n@@ -4,6 +4,7 @@\n \n import copy\n import datetime\n+import json\n import math\n import operator\n import os\n@@ -21,8 +22,8 @@\n     FILE_INPUT_CONTRADICTION, CheckboxInput, ClearableFileInput, DateInput,\n     DateTimeInput, EmailInput, FileInput, HiddenInput, MultipleHiddenInput,\n     NullBooleanSelect, NumberInput, Select, SelectMultiple,\n-    SplitDateTimeWidget, SplitHiddenDateTimeWidget, TextInput, TimeInput,\n-    URLInput,\n+    SplitDateTimeWidget, SplitHiddenDateTimeWidget, Textarea, TextInput,\n+    TimeInput, URLInput,\n )\n from django.utils import formats\n from django.utils.dateparse import parse_datetime, parse_duration\n@@ -37,7 +38,7 @@\n     'RegexField', 'EmailField', 'FileField', 'ImageField', 'URLField',\n     'BooleanField', 'NullBooleanField', 'ChoiceField', 'MultipleChoiceField',\n     'ComboField', 'MultiValueField', 'FloatField', 'DecimalField',\n-    'SplitDateTimeField', 'GenericIPAddressField', 'FilePathField',\n+    'SplitDateTimeField', 'GenericIPAddressField', 'JSONField', 'FilePathField',\n     'SlugField', 'TypedChoiceField', 'TypedMultipleChoiceField', 'UUIDField',\n )\n \n@@ -1211,3 +1212,65 @@ def to_python(self, value):\n             except ValueError:\n                 raise ValidationError(self.error_messages['invalid'], code='invalid')\n         return value\n+\n+\n+class InvalidJSONInput(str):\n+    pass\n+\n+\n+class JSONString(str):\n+    pass\n+\n+\n+class JSONField(CharField):\n+    default_error_messages = {\n+        'invalid': _('Enter a valid JSON value.'),\n+    }\n+    widget = Textarea\n+\n+    def __init__(self, encoder=None, decoder=None, **kwargs):\n+        self.encoder, self.decoder = encoder, decoder\n+        super().__init__(**kwargs)\n+\n+    def to_python(self, value):\n+        if self.disabled:\n+            return value\n+        if value in self.empty_values:\n+            return None\n+        elif isinstance(value, (list, dict, int, float, JSONString)):\n+            return value\n+        try:\n+            converted = json.loads(value, cls=self.decoder)\n+        except json.JSONDecodeError:\n+            raise ValidationError(\n+                self.error_messages['invalid'],\n+                code='invalid',\n+                params={'value': value},\n+            )\n+        if isinstance(converted, str):\n+            return JSONString(converted)\n+        else:\n+            return converted\n+\n+    def bound_data(self, data, initial):\n+        if self.disabled:\n+            return initial\n+        try:\n+            return json.loads(data, cls=self.decoder)\n+        except json.JSONDecodeError:\n+            return InvalidJSONInput(data)\n+\n+    def prepare_value(self, value):\n+        if isinstance(value, InvalidJSONInput):\n+            return value\n+        return json.dumps(value, cls=self.encoder)\n+\n+    def has_changed(self, initial, data):\n+        if super().has_changed(initial, data):\n+            return True\n+        # For purposes of seeing whether something has changed, True isn't the\n+        # same as 1 and the order of keys doesn't matter.\n+        return (\n+            json.dumps(initial, sort_keys=True, cls=self.encoder) !=\n+            json.dumps(self.to_python(data), sort_keys=True, cls=self.encoder)\n+        )\ndiff --git a/docs/internals/deprecation.txt b/docs/internals/deprecation.txt\nindex dc6923d48883..b35c9e57e12d 100644\n--- a/docs/internals/deprecation.txt\n+++ b/docs/internals/deprecation.txt\n@@ -46,6 +46,9 @@ details on these changes.\n \n * The ``HttpRequest.is_ajax()`` method will be removed.\n \n+* ``django.contrib.postgres.fields.JSONField`` and\n+  ``django.contrib.postgres.forms.JSONField`` will be removed.\n+\n See the :ref:`Django 3.1 release notes <deprecated-features-3.1>` for more\n details on these changes.\n \ndiff --git a/docs/ref/checks.txt b/docs/ref/checks.txt\nindex e7b0a5ec8ce8..22efbd7a4e71 100644\n--- a/docs/ref/checks.txt\n+++ b/docs/ref/checks.txt\n@@ -177,6 +177,8 @@ Model fields\n   in historical migrations.\n * **fields.W902**: ``FloatRangeField`` is deprecated and will be removed in\n   Django 3.1. *This check appeared in Django 2.2 and 3.0*.\n+* **fields.W903**: ``django.contrib.postgres.fields.JSONField`` is deprecated\n+  and will be removed in Django 4.0.\n \n File fields\n ~~~~~~~~~~~\n@@ -323,6 +325,7 @@ Models\n   ``<max_length>`` characters.\n * **models.W035**: ``db_table`` ``<db_table>`` is used by multiple models:\n   ``<model list>``.\n+* **models.E036**: ``<database>`` does not support ``JSONField``\\s.\n \n Security\n --------\ndiff --git a/docs/ref/contrib/postgres/fields.txt b/docs/ref/contrib/postgres/fields.txt\nindex e8fcef62158c..d7c40094111a 100644\n--- a/docs/ref/contrib/postgres/fields.txt\n+++ b/docs/ref/contrib/postgres/fields.txt\n@@ -517,6 +517,14 @@ using in conjunction with lookups on\n     of the JSON which allows indexing. The trade-off is a small additional cost\n     on writing to the ``jsonb`` field. ``JSONField`` uses ``jsonb``.\n \n+.. deprecated:: 3.1\n+\n+    A new :class:`~django.db.models.JSONField` that works on all supported\n+    database backends is available starting with Django 3.1. It is advised to\n+    migrate and use the new ``JSONField``. For now, this PostgreSQL-only\n+    ``JSONField`` is left as a reference to the new one and is deprecated as of\n+    this release.\n+\n Querying ``JSONField``\n ----------------------\n \ndiff --git a/docs/ref/contrib/postgres/forms.txt b/docs/ref/contrib/postgres/forms.txt\nindex f559ac75cb8a..9faf51c71eaa 100644\n--- a/docs/ref/contrib/postgres/forms.txt\n+++ b/docs/ref/contrib/postgres/forms.txt\n@@ -173,6 +173,14 @@ Fields\n         it is a useful way to format data from a client-side widget for\n         submission to the server.\n \n+.. deprecated:: 3.1\n+\n+    A new :class:`~django.forms.JSONField` that works on all supported\n+    database backends is available starting with Django 3.1. It is advised to\n+    migrate and use the new JSONField. For now, this PostgreSQL-only\n+    ``JSONField`` is left as a reference to the new one and is deprecated as of\n+    this release.\n+\n Range Fields\n ------------\n \ndiff --git a/docs/ref/databases.txt b/docs/ref/databases.txt\nindex b4c190bafddd..5610fdb7d707 100644\n--- a/docs/ref/databases.txt\n+++ b/docs/ref/databases.txt\n@@ -778,6 +778,95 @@ iterator. Your code must handle this.\n \n .. _`Isolation in SQLite`: https://sqlite.org/isolation.html\n \n+.. _sqlite-json1:\n+\n+Enabling JSON1 extension on SQLite\n+----------------------------------\n+\n+This document describes how to enable JSON1 extension on Python's\n+:py:mod:`sqlite3` library so that :class:`~django.db.models.JSONField` can be\n+used. If the extension is not enabled on your installation and there is no\n+other database connection that supports ``JSONField``, a system error will be\n+raised (\"``No database connection that supports JSONField is found.``\"). To\n+check if the extension is enabled on your installation, you can do a query with\n+one of the functions included in the extension, e.g. ``JSON()``. For example::\n+\n+    >>> import sqlite3\n+    >>> conn = sqlite3.connect(':memory:')\n+    >>> c = conn.cursor()\n+    >>> c.execute(\"SELECT JSON('123');\")\n+\n+If the query doesn't throw any errors, then the JSON1 extension should already\n+be enabled. Otherwise, follow the instructions below according to your\n+operating system to set it up correctly.\n+\n+Linux\n+~~~~~\n+\n+On most major Linux distributions, the JSON1 extension is included in their\n+SQLite and/or Python packages and enabled by default. If that's not the case on\n+your installation, then do the following:\n+\n+- Download the `SQLite amalgamation`_,\n+  with or without the configuration script.\n+- Extract the source code archive and enter the directory of the result.\n+- Compile the source code using the ``-DSQLITE_ENABLE_JSON1`` flag to enable\n+  the JSON1 extension. For example:\n+\n+  .. code-block:: console\n+\n+      gcc -DSQLITE_ENABLE_JSON1 -c -fPIC sqlite3.c\n+\n+  To enable other extensions, see the `compilation instructions`_.\n+- Create a shared library. For example:\n+\n+  .. code-block:: console\n+\n+      gcc -shared -o libsqlite3.so -fPIC sqlite3.o -ldl -lpthread\n+\n+- Place the resulting file (``libsqlite3.so``) in a desired directory, e.g.\n+  ``/usr/lib/sqlite3/``.\n+- Set the ``LD_PRELOAD`` environment variable to use your compiled SQLite every\n+  time you run Django. For example:\n+\n+  .. code-block:: console\n+\n+      export LD_PRELOAD=/usr/lib/sqlite3/libsqlite3.so\n+\n+- Now, the JSON1 extension should be ready to be used in Python and Django.\n+\n+.. _`SQLite amalgamation`: https://www.sqlite.org/download.html\n+.. _`compilation instructions`: https://www.sqlite.org/howtocompile.html\n+\n+Mac\n+~~~\n+\n+On Mac, follow the instructions for Linux above, but instead of setting the\n+``LD_PRELOAD`` environment variable, use ``DYLD_LIBRARY_PATH``. For example:\n+\n+  .. code-block:: console\n+\n+      export DYLD_LIBRARY_PATH=/usr/lib/sqlite3\n+\n+Windows\n+~~~~~~~\n+\n+- Download the `precompiled DLL`_ that matches your Python installation (32-bit\n+  or 64-bit).\n+- Locate your Python installation. By default, it should be in\n+  ``%localappdata%\\Programs\\Python\\PythonXX``, where ``XX`` is the Python\n+  version. For example, it's located in\n+  ``C:\\Users\\<username>\\AppData\\Local\\Programs\\Python\\Python37``. If you added\n+  Python installation directory to your ``PATH`` environment variable, you can\n+  run the command ``where python`` on a command prompt to locate it.\n+- Enter the ``DLLs`` directory in your Python installation.\n+- Rename (or delete) ``sqlite3.dll`` inside the ``DLLs`` directory.\n+- Extract ``sqlite3.dll`` from the downloaded DLL archive and put it in the\n+  ``DLLs`` directory.\n+- Now, the JSON1 extension should be ready to be used in Python and Django.\n+\n+.. _`precompiled DLL`: https://www.sqlite.org/download.html\n+\n .. _oracle-notes:\n \n Oracle notes\ndiff --git a/docs/ref/forms/fields.txt b/docs/ref/forms/fields.txt\nindex d2a0550a47e2..da05905af497 100644\n--- a/docs/ref/forms/fields.txt\n+++ b/docs/ref/forms/fields.txt\n@@ -809,6 +809,64 @@ For each field, we describe the default widget used if you don't specify\n         ``192.0.2.1``. Default is disabled. Can only be used\n         when ``protocol`` is set to ``'both'``.\n \n+``JSONField``\n+-------------\n+\n+.. class:: JSONField(encoder=None, decoder=None, **kwargs)\n+\n+    A field which accepts JSON encoded data for a\n+    :class:`~django.db.models.JSONField`.\n+\n+    * Default widget: :class:`Textarea`\n+    * Empty value: ``''`` (an empty string)\n+    * Normalizes to: Python representation of the JSON value (usually as a\n+      dictionary, list, string, number, boolean or ``None``), depending on\n+      ``decoder``.\n+    * Validates that the given value is a valid JSON value.\n+    * Error message keys: ``required``, ``invalid``\n+\n+    Takes two optional arguments:\n+\n+    .. attribute:: encoder\n+\n+        A :py:class:`json.JSONEncoder` subclass to serialize data types\n+        not supported by the standard JSON serializer (``datetime``, ``uuid``,\n+        etc.). For example, you can use the\n+        :class:`~django.core.serializers.json.DjangoJSONEncoder` class.\n+\n+        The ``encoder`` is used to serialize the data before it's rendered by the\n+        widget. It is also used in :meth:`Field.has_changed()`.\n+\n+        If ``None`` is specified, it defaults to ``json.JSONEncoder``.\n+\n+    .. attribute:: decoder\n+\n+        A :py:class:`json.JSONDecoder` subclass to deserialize the input.\n+        Your deserialization may need to account for the fact that you can't be\n+        certain of the input type. For example, you run the risk of returning a\n+        ``datetime`` that was actually a string that just happened to be in the\n+        same format chosen for ``datetime``\\s.\n+\n+        The ``decoder`` can be used to validate the input. If\n+        :py:class:`json.JSONDecodeError` is raised during the deserialization, a\n+        ``ValidationError`` will be raised.\n+\n+        If ``None`` is specified, it defaults to ``json.JSONDecoder``.\n+\n+    Specification for ``JSONEncoder`` and ``JSONDecoder`` subclasses can be found\n+    in the Python documentation.\n+\n+    .. note::\n+\n+        If you use a :class:`ModelForm <django.forms.ModelForm>`, the ``encoder`` and\n+        ``decoder`` from its :class:`~django.db.models.JSONField` will be used.\n+\n+    .. admonition:: User friendly forms\n+\n+        ``JSONField`` is not particularly user friendly in most cases. However,\n+        it is a useful way to format data from a client-side widget for\n+        submission to the server.\n+\n ``MultipleChoiceField``\n -----------------------\n \ndiff --git a/docs/ref/models/fields.txt b/docs/ref/models/fields.txt\nindex d321506d996a..3b4f88367fd8 100644\n--- a/docs/ref/models/fields.txt\n+++ b/docs/ref/models/fields.txt\n@@ -345,7 +345,7 @@ The default can't be a mutable object (model instance, ``list``, ``set``, etc.),\n as a reference to the same instance of that object would be used as the default\n value in all new model instances. Instead, wrap the desired default in a\n callable. For example, if you want to specify a default ``dict`` for\n-:class:`~django.contrib.postgres.fields.JSONField`, use a function::\n+:class:`~django.db.models.JSONField`, use a function::\n \n     def contact_default():\n         return {\"email\": \"to1@example.com\"}\n@@ -1169,6 +1169,84 @@ are converted to lowercase.\n If you allow for blank values, you have to allow for null values since blank\n values are stored as null.\n \n+``JSONField``\n+-------------\n+\n+.. class:: JSONField(encoder=None, decoder=None, **options)\n+\n+A field for storing JSON encoded data. In Python the data is represented in\n+its Python native format: dictionaries, lists, strings, numbers, booleans\n+and ``None``.\n+\n+.. attribute:: JSONField.encoder\n+\n+    An optional :py:class:`json.JSONEncoder` subclass to serialize data types\n+    not supported by the standard JSON serializer (``datetime``, ``uuid``,\n+    etc.). For example, you can use the\n+    :class:`~django.core.serializers.json.DjangoJSONEncoder` class.\n+\n+    If ``None`` is specified, it defaults to ``json.JSONEncoder``.\n+\n+.. attribute:: JSONField.decoder\n+\n+    An optional :py:class:`json.JSONDecoder` subclass to deserialize the value\n+    retrieved from the database. The value will be in the format chosen by the\n+    custom encoder (most often a string). Your deserialization may need to\n+    account for the fact that you can't be certain of the input type. For\n+    example, you run the risk of returning a ``datetime`` that was actually a\n+    string that just happened to be in the same format chosen for\n+    ``datetime``\\s.\n+\n+    If ``None`` is specified, it defaults to ``json.JSONDecoder``.\n+\n+Specification for ``JSONEncoder`` and ``JSONDecoder`` subclasses can be found\n+in the Python documentation.\n+\n+If you give the field a :attr:`~django.db.models.Field.default`, ensure\n+it's an immutable object, such as a ``str``, or a callable object that returns\n+a fresh mutable object each time, such as ``dict`` or a function. Providing a\n+mutable default object like ``default={}`` or ``default=[]`` shares the one\n+object between all model instances. There is a system check for this.\n+\n+To query ``JSONField`` in the database, see\n+:ref:`Querying JSONField <querying-jsonfield>`.\n+\n+.. note::\n+\n+    Oracle Database supports storing JSON data using SQL data types\n+    ``VARCHAR2``, ``CLOB``, and ``BLOB``. ``JSONField`` uses ``CLOB``.\n+\n+.. note::\n+\n+    PostgreSQL has two native JSON based data types: ``json`` and ``jsonb``.\n+    The main difference between them is how they are stored and how they can be\n+    queried. PostgreSQL's ``json`` field is stored as the original string\n+    representation of the JSON and must be decoded on the fly when queried\n+    based on keys. The ``jsonb`` field is stored based on the actual structure\n+    of the JSON which allows indexing. The trade-off is a small additional cost\n+    on writing to the ``jsonb`` field. ``JSONField`` uses ``jsonb``.\n+\n+.. admonition:: MariaDB users\n+\n+    Requires MariaDB 10.2.7 or later.\n+\n+.. admonition:: MySQL users\n+\n+    Requires MySQL 5.7.8 or later.\n+\n+.. admonition:: Oracle users\n+\n+    Oracle Database does not support storing JSON scalar values. Only JSON\n+    objects and arrays (represented in Python using :py:class:`dict` and\n+    :py:class:`list`) are supported.\n+\n+.. admonition:: SQLite users\n+\n+    Requires SQLite 3.9.0 or later with the JSON1_ extension enabled. See\n+    :ref:`sqlite-json1` for more details.\n+\n+.. _JSON1: https://www.sqlite.org/json1.html\n+\n ``NullBooleanField``\n --------------------\n \ndiff --git a/docs/releases/3.1.txt b/docs/releases/3.1.txt\nindex 221cf39dee0d..110a268bb247 100644\n--- a/docs/releases/3.1.txt\n+++ b/docs/releases/3.1.txt\n@@ -27,6 +27,20 @@ officially support the latest release of each series.\n What's new in Django 3.1\n ========================\n \n+JSONField for all supported database backends\n+---------------------------------------------\n+\n+Django now includes a :class:`models.JSONField <django.db.models.JSONField>`\n+and a :class:`forms.JSONField <django.forms.JSONField>` that work on all\n+supported database backends. Both fields support the use of custom JSON encoder\n+and decoder. The model field also supports lookups and transforms supported in\n+previously\n+:class:`PostgreSQL-only JSONField <django.contrib.postgres.fields.JSONField>`.\n+\n+If your project uses the PostgreSQL ``JSONField`` form and/or model fields, it\n+is advised to migrate and use the new fields. For now, the old fields are left\n+as a reference to the new ones and are deprecated as of this release.\n+\n Minor features\n --------------\n \n@@ -421,6 +435,11 @@ Miscellaneous\n \n * ``django.utils.decorators.classproperty()`` decorator is moved to\n   ``django.utils.functional.classproperty()``.\n+* Third party database backends must implement support for\n+  :class:`~django.db.models.JSONField` or set\n+  ``DatabaseFeatures.supports_json_field`` to ``False``. If ``JSONField``\n+  introspection is also implemented, set\n+  ``DatabaseFeatures.can_introspect_json_field`` to ``True``.\n \n * :tfilter:`floatformat` template filter now outputs (positive) ``0`` for\n   negative numbers which round to zero.\n@@ -454,6 +473,15 @@ Miscellaneous\n Features deprecated in 3.1\n ==========================\n \n+PostgreSQL ``JSONField``\n+------------------------\n+\n+:class:`django.contrib.postgres.fields.JSONField` and\n+:class:`django.contrib.postgres.forms.JSONField` are\n+deprecated in favor of the similar fields that work on all supported database\n+backends: :class:`models.JSONField <django.db.models.JSONField>` and\n+:class:`forms.JSONField <django.forms.JSONField>`.\n+\n Miscellaneous\n -------------\n \ndiff --git a/docs/topics/db/queries.txt b/docs/topics/db/queries.txt\nindex 79f38084fa55..fba275616178 100644\n--- a/docs/topics/db/queries.txt\n+++ b/docs/topics/db/queries.txt\n@@ -790,6 +790,286 @@ being evaluated and therefore populate the cache::\n     Simply printing the queryset will not populate the cache. This is because\n     the call to ``__repr__()`` only returns a slice of the entire queryset.\n \n+.. _querying-jsonfield:\n+\n+Querying ``JSONField``\n+======================\n+\n+Lookups implementation is different in :class:`~django.db.models.JSONField`,\n+mainly due to the existence of key transformations. To demonstrate, we will\n+use the following example model::\n+\n+    from django.db import models\n+\n+    class Dog(models.Model):\n+        name = models.CharField(max_length=200)\n+        data = models.JSONField()\n+\n+        def __str__(self):\n+            return self.name\n+\n+As with other fields, storing ``None`` as the field's value will store it as\n+SQL ``NULL``. While not recommended, it is possible to store JSON scalar\n+``null`` instead of SQL ``NULL`` by using\n+:meth:`Value('null') <django.db.models.Value>`. However, after retrieved from\n+the database, the Python representation of the data is the same as SQL\n+``NULL``, i.e. ``None``. Therefore, it can be hard to distinguish between the\n+two.\n+\n+Note that this only applies to ``None`` as the top-level value of the field.\n+If ``None`` is inside a :py:class:`list` or :py:class:`dict`, it will always\n+be interpreted as JSON ``null``.\n+\n+When querying, ``None`` value will always be interpreted as JSON ``null``.\n+To query for SQL ``NULL``, use :lookup:`isnull`.\n+\n+To illustrate::\n+\n+    >>> Dog.objects.create(name='Max', data=None)  # Stored as SQL NULL\n+    >>> Dog.objects.create(name='Archie', data=Value('null'))  # Stored as JSON null\n+\n+    >>> Dog.objects.filter(data=None)\n+    <QuerySet [<Dog: Archie>]>\n+    >>> Dog.objects.filter(data=Value('null'))\n+    <QuerySet [<Dog: Archie>]>\n+    >>> Dog.objects.filter(data__isnull=True)\n+    <QuerySet [<Dog: Max>]>\n+\n+.. note::\n+\n+    Storing JSON scalar ``null`` does not violate\n+    :attr:`null=False <django.db.models.Field.null>`.\n+\n+.. fieldlookup:: jsonfield.key\n+\n+Key, index, and path transforms\n+-------------------------------\n+\n+To query based on a given dictionary key, simply use that key as the lookup\n+name::\n+\n+    >>> Dog.objects.create(name='Rufus', data={\n+    ...     'breed': 'labrador',\n+    ...     'owner': {\n+    ...         'name': 'Bob',\n+    ...         'other_pets': [{\n+    ...             'name': 'Fishy',\n+    ...         }],\n+    ...     },\n+    ... })\n+    >>> Dog.objects.create(name='Meg', data={'breed': 'collie', 'owner': None})\n+\n+    >>> Dog.objects.filter(data__breed='collie')\n+    <QuerySet [<Dog: Meg>]>\n+\n+Multiple keys can be chained together to form a path transform::\n+\n+    >>> Dog.objects.filter(data__owner__name='Bob')\n+    <QuerySet [<Dog: Rufus>]>\n+\n+If the key is an integer, it will be interpreted as an index transform in an\n+array::\n+\n+    >>> Dog.objects.filter(data__owner__other_pets__0__name='Fishy')\n+    <QuerySet [<Dog: Rufus>]>\n+\n+If the key you wish to query by clashes with the name of another lookup, use\n+the :lookup:`contains <jsonfield.contains>` lookup instead.\n+\n+.. note::\n+\n+    The transform examples given above implicitly use the :lookup:`exact`\n+    lookup. Key, index, and path transforms can also be chained with\n+    :lookup:`iexact`, :lookup:`contains`, :lookup:`icontains`,\n+    :lookup:`startswith`, :lookup:`istartswith`, :lookup:`endswith`,\n+    :lookup:`iendswith`, :lookup:`regex`, :lookup:`iregex`, :lookup:`lt`,\n+    :lookup:`lte`, :lookup:`gt`, and :lookup:`gte` lookups.\n+\n+To query for ``null`` in JSON data, use ``None`` as a value::\n+\n+    >>> Dog.objects.filter(data__owner=None)\n+    <QuerySet [<Dog: Meg>]>\n+\n+To query for missing keys, use the ``isnull`` lookup::\n+\n+    >>> Dog.objects.create(name='Shep', data={'breed': 'collie'})\n+    >>> Dog.objects.filter(data__owner__isnull=True)\n+    <QuerySet [<Dog: Shep>]>\n+\n+.. warning::\n+\n+    Since any string could be a key in a JSON object, any lookup other than\n+    those listed below will be interpreted as a key lookup. No errors are\n+    raised. Be extra careful for typing mistakes, and always check your queries\n+    work as you intend.\n+\n+.. admonition:: MariaDB and Oracle users\n+\n+    Using :meth:`~django.db.models.query.QuerySet.order_by` on key, index,\n+    or path transforms will sort the objects using the string representation\n+    of the values. This is because MariaDB and Oracle Database do not provide\n+    a function that converts JSON values into their equivalent SQL values.\n+    Therefore, ordering by numerical JSON values does not work as expected.\n+\n+.. admonition:: Oracle users\n+\n+    On Oracle Database, using ``None`` as the lookup value in an\n+    :meth:`~django.db.models.query.QuerySet.exclude` query will return objects\n+    that do not have ``null`` as the value at the given path, including objects\n+    that do not have the path. On other database backends, the query will\n+    return objects that have the path and the value is not ``null``.\n+\n+.. note::\n+\n+    On PostgreSQL, if only one key or index is used, the SQL operator ``->`` is\n+    used. If multiple operators are used then the ``#>`` operator is used.\n+    On MySQL, MariaDB, and SQLite, the ``JSON_EXTRACT`` function is used.\n+    On Oracle Database, a combination of ``JSON_QUERY`` and ``JSON_VALUE``\n+    using ``COALESCE`` is used.\n+\n+Containment and key operations\n+------------------------------\n+\n+.. fieldlookup:: jsonfield.contains\n+\n+``contains``\n+~~~~~~~~~~~~\n+\n+The :lookup:`contains` lookup is overridden on ``JSONField``. The returned\n+objects are those where the given ``dict`` of key-value pairs are all\n+contained in the top-level of the field. For example::\n+\n+    >>> Dog.objects.create(name='Rufus', data={'breed': 'labrador', 'owner': 'Bob'})\n+    >>> Dog.objects.create(name='Meg', data={'breed': 'collie', 'owner': 'Bob'})\n+    >>> Dog.objects.create(name='Fred', data={})\n+\n+    >>> Dog.objects.filter(data__contains={'owner': 'Bob'})\n+    <QuerySet [<Dog: Rufus>, <Dog: Meg>]>\n+\n+    >>> Dog.objects.filter(data__contains={'breed': 'collie'})\n+    <QuerySet [<Dog: Meg>]>\n+\n+.. note::\n+\n+    This lookup is only overridden on ``JSONField``. It is not overridden on\n+    key, index, and path transforms.\n+\n+.. note::\n+\n+    On PostgreSQL, the implementation uses the SQL operator ``@>``. On MySQL\n+    and MariaDB, it uses the SQL function ``JSON_CONTAINS``. On Oracle\n+    Database, it uses the SQL function ``JSON_QUERY`` and ``JSON_VALUE``.\n+    On SQLite, it uses the SQL function ``JSON_EXTRACT`` and ``JSON_TYPE``.\n+\n+.. admonition:: Oracle and SQLite users\n+\n+    Since Oracle Database and SQLite don't provide a built-in function to\n+    support this lookup, the implementation is actually similar to key\n+    lookups chained together with the SQL operator ``AND``.\n+\n+.. fieldlookup:: jsonfield.contained_by\n+\n+``contained_by``\n+~~~~~~~~~~~~~~~~\n+\n+This is the inverse of the :lookup:`contains <jsonfield.contains>` lookup -\n+the objects returned will be those where the key-value pairs on the object are\n+a subset of those in the value passed. For\n+example::\n+\n+    >>> Dog.objects.create(name='Rufus', data={'breed': 'labrador', 'owner': 'Bob'})\n+    >>> Dog.objects.create(name='Meg', data={'breed': 'collie', 'owner': 'Bob'})\n+    >>> Dog.objects.create(name='Fred', data={})\n+\n+    >>> Dog.objects.filter(data__contained_by={'breed': 'collie', 'owner': 'Bob'})\n+    <QuerySet [<Dog: Meg>, <Dog: Fred>]>\n+\n+    >>> Dog.objects.filter(data__contained_by={'breed': 'collie'})\n+    <QuerySet [<Dog: Fred>]>\n+\n+.. note::\n+\n+    On PostgreSQL, the implementation uses the SQL operator ``<@``. On MySQL\n+    and MariaDB, it uses the SQL function ``JSON_CONTAINS`` like ``contains``\n+    lookup but with the arguments switched.\n+\n+.. admonition:: Oracle and SQLite users\n+\n+    Due to the limitation on Oracle and SQLite database backends, this lookup\n+    is not supported.\n+\n+.. fieldlookup:: jsonfield.has_key\n+\n+``has_key``\n+~~~~~~~~~~~\n+\n+Returns objects where the given key is in the top-level of the data. For\n+example::\n+\n+    >>> Dog.objects.create(name='Rufus', data={'breed': 'labrador'})\n+    >>> Dog.objects.create(name='Meg', data={'breed': 'collie', 'owner': 'Bob'})\n+\n+    >>> Dog.objects.filter(data__has_key='owner')\n+    <QuerySet [<Dog: Meg>]>\n+\n+.. note::\n+\n+    On PostgreSQL, the implementation uses the SQL operator ``?``. On MySQL\n+    and MariaDB, it uses the SQL function ``JSON_CONTAINS_PATH``. On Oracle\n+    Database, it uses the SQL function ``JSON_EXISTS``. On SQLite, it uses\n+    the SQL function ``JSON_TYPE``.\n+\n+.. admonition:: SQLite users\n+\n+    The implementation uses ``JSON_TYPE`` along with ``IS NOT NULL`` clause.\n+    The function ``JSON_EXTRACT`` is not used because it also returns SQL\n+    ``NULL`` for JSON value ``null``, which makes it impossible to\n+    differentiate between a missing key and an existing key with a ``null``\n+    value.\n+\n+.. fieldlookup:: jsonfield.has_any_keys\n+\n+``has_any_keys``\n+~~~~~~~~~~~~~~~~\n+\n+Returns objects where any of the given keys are in the top-level of the data.\n+For example::\n+\n+    >>> Dog.objects.create(name='Rufus', data={'breed': 'labrador'})\n+    >>> Dog.objects.create(name='Meg', data={'owner': 'Bob'})\n+    >>> Dog.objects.create(name='Fred', data={})\n+\n+    >>> Dog.objects.filter(data__has_any_keys=['owner', 'breed'])\n+    <QuerySet [<Dog: Rufus>, <Dog: Meg>]>\n+\n+.. note::\n+\n+    On PostgreSQL, the implementation uses the SQL operator ``?|``. On all\n+    other database backends, the implementation is similar to\n+    :lookup:`contains <jsonfield.contains>` lookup and each key is joined\n+    using the SQL ``OR`` operator.\n+\n+.. fieldlookup:: jsonfield.has_keys\n+\n+``has_keys``\n+~~~~~~~~~~~~\n+\n+Returns objects where all of the given keys are in the top-level of the data.\n+For example::\n+\n+    >>> Dog.objects.create(name='Rufus', data={})\n+    >>> Dog.objects.create(name='Meg', data={'breed': 'collie', 'owner': 'Bob'})\n+\n+    >>> Dog.objects.filter(data__has_keys=['breed', 'owner'])\n+    <QuerySet [<Dog: Meg>]>\n+\n+.. note::\n+\n+    On PostgreSQL, the implementation uses the SQL operator ``?&``. On all\n+    other database backends, the implementation is similar to\n+    :lookup:`contains <jsonfield.contains>` lookup and each key is joined\n+    using the SQL ``AND`` operator.\n+\n .. _complex-lookups-with-q:\n \n Complex lookups with ``Q`` objects\ndiff --git a/tests/forms_tests/field_tests/test_jsonfield.py b/tests/forms_tests/field_tests/test_jsonfield.py\nnew file mode 100644\nindex 000000000000..521b74e98c98\n--- /dev/null\n+++ b/tests/forms_tests/field_tests/test_jsonfield.py\n@@ -0,0 +1,110 @@\n+import json\n+import uuid\n+\n+from django import forms\n+from django.core import exceptions\n+from django.core.serializers.json import DjangoJSONEncoder\n+from django.test import SimpleTestCase\n+\n+\n+class CustomDecoder(json.JSONDecoder):\n+    def __init__(self, object_hook=None, *args, **kwargs):\n+        return super().__init__(object_hook=self.object_hook, *args, **kwargs)\n+\n+    def object_hook(self, dct):\n+        try:\n+            dct['uuid'] = uuid.UUID(dct['uuid'])\n+        except KeyError:\n+            pass\n+        return dct\n+\n+\n+class TestFormField(SimpleTestCase):\n+\n+    def test_valid(self):\n+        field = forms.JSONField()\n+        value = field.clean('{\"a\": \"b\"}')\n+        self.assertEqual(value, {'a': 'b'})\n+\n+    def test_valid_empty(self):\n+        field = forms.JSONField(required=False)\n+        value = field.clean('')\n+        self.assertIsNone(value)\n+\n+    def test_invalid(self):\n+        field = forms.JSONField()\n+        with self.assertRaisesMessage(exceptions.ValidationError, 'Enter a valid JSON value.'):\n+            field.clean('{some badly formed: json}')\n+\n+    def test_formfield_disabled(self):\n+        class JsonForm(forms.Form):\n+            name = forms.CharField()\n+            jfield = forms.JSONField(disabled=True)\n+\n+        form = JsonForm({'name': 'xyz', 'jfield': '[\"bar\"]'}, initial={'jfield': ['foo']})\n+        self.assertIn('[&quot;foo&quot;]</textarea>', form.as_p())\n+\n+    def test_prepare_value(self):\n+        field = forms.JSONField()\n+        self.assertEqual(field.prepare_value({'a': 'b'}), '{\"a\": \"b\"}')\n+        self.assertEqual(field.prepare_value(None), 'null')\n+        self.assertEqual(field.prepare_value('foo'), '\"foo\"')\n+\n+    def test_redisplay_wrong_input(self):\n+        \"\"\"\n+        When displaying a bound form (typically due to invalid input), the form\n+        should not overquote JSONField inputs.\n+        \"\"\"\n+        class JsonForm(forms.Form):\n+            name = forms.CharField(max_length=2)\n+            jfield = forms.JSONField()\n+\n+        # JSONField input is fine, name is too long\n+        form = JsonForm({'name': 'xyz', 'jfield': '[\"foo\"]'})\n+        self.assertNotIn('jfield', form.errors)\n+        self.assertIn('[&quot;foo&quot;]</textarea>', form.as_p())\n+\n+        # This time, the JSONField input is wrong\n+        form = JsonForm({'name': 'xy', 'jfield': '{\"foo\"}'})\n+        self.assertIn('jfield', form.errors)\n+        self.assertIn('{&quot;foo&quot;}</textarea>', form.as_p())\n+\n+    def test_widget(self):\n+        \"\"\"The default widget of a JSONField is a Textarea.\"\"\"\n+        field = forms.JSONField()\n+        self.assertIsInstance(field.widget, forms.widgets.Textarea)\n+\n+    def test_custom_widget_kwarg(self):\n+        \"\"\"The widget can be overridden with a kwarg.\"\"\"\n+        field = forms.JSONField(widget=forms.widgets.Input)\n+        self.assertIsInstance(field.widget, forms.widgets.Input)\n+\n+    def test_custom_widget_attribute(self):\n+        \"\"\"The widget can be overridden with an attribute.\"\"\"\n+        class CustomJSONField(forms.JSONField):\n+            widget = forms.widgets.Input\n+\n+        field = CustomJSONField()\n+        self.assertIsInstance(field.widget, forms.widgets.Input)\n+\n+    def test_already_converted_value(self):\n+        field = forms.JSONField(required=False)\n+        tests = [\n+            '[\"a\", \"b\", \"c\"]', '{\"a\": 1, \"b\": 2}', '1', '1.5', '\"foo\"',\n+            'true', 'false', 'null',\n+        ]\n+        for json_string in tests:\n+            with self.subTest(json_string=json_string):\n+                val = field.clean(json_string)\n+                self.assertEqual(field.clean(val), val)\n+\n+    def test_has_changed(self):\n+        field = forms.JSONField()\n+        self.assertIs(field.has_changed({'a': True}, '{\"a\": 1}'), True)\n+        self.assertIs(field.has_changed({'a': 1, 'b': 2}, '{\"b\": 2, \"a\": 1}'), False)\n+\n+    def test_custom_encoder_decoder(self):\n+        value = {'uuid': uuid.UUID('{12345678-1234-5678-1234-567812345678}')}\n+        field = forms.JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n+        self.assertEqual(field.prepare_value(value), '{\"uuid\": \"12345678-1234-5678-1234-567812345678\"}')\n+        self.assertEqual(field.clean('{\"uuid\": \"12345678-1234-5678-1234-567812345678\"}'), value)\ndiff --git a/tests/inspectdb/models.py b/tests/inspectdb/models.py\nindex 8a48031b2487..d0076ce94f99 100644\n--- a/tests/inspectdb/models.py\n+++ b/tests/inspectdb/models.py\n@@ -68,6 +68,17 @@ class ColumnTypes(models.Model):\n     uuid_field = models.UUIDField()\n \n \n+class JSONFieldColumnType(models.Model):\n+    json_field = models.JSONField()\n+    null_json_field = models.JSONField(blank=True, null=True)\n+\n+    class Meta:\n+        required_db_features = {\n+            'can_introspect_json_field',\n+            'supports_json_field',\n+        }\n+\n+\n class UniqueTogether(models.Model):\n     field1 = models.IntegerField()\n     field2 = models.CharField(max_length=10)\ndiff --git a/tests/inspectdb/tests.py b/tests/inspectdb/tests.py\nindex 6e3f4b8aa608..afe89e0dda6a 100644\n--- a/tests/inspectdb/tests.py\n+++ b/tests/inspectdb/tests.py\n@@ -85,6 +85,15 @@ def test_field_types(self):\n         elif not connection.features.interprets_empty_strings_as_nulls:\n             assertFieldType('uuid_field', \"models.CharField(max_length=32)\")\n \n+    @skipUnlessDBFeature('can_introspect_json_field', 'supports_json_field')\n+    def test_json_field(self):\n+        out = StringIO()\n+        call_command('inspectdb', 'inspectdb_jsonfieldcolumntype', stdout=out)\n+        output = out.getvalue()\n+        if not connection.features.interprets_empty_strings_as_nulls:\n+            self.assertIn('json_field = models.JSONField()', output)\n+        self.assertIn('null_json_field = models.JSONField(blank=True, null=True)', output)\n+\n     def test_number_field_types(self):\n         \"\"\"Test introspection of various Django field types\"\"\"\n         assertFieldType = self.make_field_type_asserter()\ndiff --git a/tests/invalid_models_tests/test_models.py b/tests/invalid_models_tests/test_models.py\nindex ec2d345d5a9c..e8a595f6e6dd 100644\n--- a/tests/invalid_models_tests/test_models.py\n+++ b/tests/invalid_models_tests/test_models.py\n@@ -1,4 +1,5 @@\n import unittest\n+from unittest import mock\n \n from django.conf import settings\n from django.core.checks import Error, Warning\n@@ -6,7 +7,7 @@\n from django.db import connection, connections, models\n from django.db.models.functions import Lower\n from django.db.models.signals import post_init\n-from django.test import SimpleTestCase\n+from django.test import SimpleTestCase, TestCase, skipUnlessDBFeature\n from django.test.utils import isolate_apps, override_settings, register_lookup\n \n \n@@ -1211,6 +1212,42 @@ def dummy_function(*args, **kwargs):\n         ])\n \n \n+@isolate_apps('invalid_models_tests')\n+class JSONFieldTests(TestCase):\n+\n+    @skipUnlessDBFeature('supports_json_field')\n+    def test_ordering_by_json_field_value(self):\n+        class Model(models.Model):\n+            field = models.JSONField()\n+\n+            class Meta:\n+                ordering = ['field__value']\n+\n+        self.assertEqual(Model.check(), [])\n+\n+    def test_check_json_fields(self):\n+        class Model(models.Model):\n+            field = models.JSONField()\n+\n+        error = Error(\n+            '%s does not support JSONFields.' % connection.display_name,\n+            obj=Model,\n+            id='models.E036',\n+        )\n+        expected = [] if connection.features.supports_json_field else [error, error]\n+        self.assertEqual(Model.check(), expected)\n+\n+    def test_json_fields_required_db_features(self):\n+        class Model(models.Model):\n+            age = models.JSONField()\n+\n+            class Meta:\n+                required_db_features = {'supports_json_field'}\n+\n+        with mock.patch.object(connection.features, 'supports_json_field', False):\n+            self.assertEqual(Model.check(), [])\n+\n+\n @isolate_apps('invalid_models_tests')\n class ConstraintsTests(SimpleTestCase):\n     def test_check_constraints(self):\ndiff --git a/tests/model_fields/models.py b/tests/model_fields/models.py\nindex 0fd5910339ea..18620c404c7d 100644\n--- a/tests/model_fields/models.py\n+++ b/tests/model_fields/models.py\n@@ -335,6 +335,20 @@ class PersonTwoImages(models.Model):\n                                   width_field='headshot_width')\n \n \n+class JSONModel(models.Model):\n+    value = models.JSONField()\n+\n+    class Meta:\n+        required_db_features = {'supports_json_field'}\n+\n+\n+class NullableJSONModel(models.Model):\n+    value = models.JSONField(blank=True, null=True)\n+\n+    class Meta:\n+        required_db_features = {'supports_json_field'}\n+\n+\n class AllFieldsModel(models.Model):\n     big_integer = models.BigIntegerField()\n     binary = models.BinaryField()\ndiff --git a/tests/model_fields/test_jsonfield.py b/tests/model_fields/test_jsonfield.py\nnew file mode 100644\nindex 000000000000..12bb95899ec9\n--- /dev/null\n+++ b/tests/model_fields/test_jsonfield.py\n@@ -0,0 +1,691 @@\n+import json\n+import operator\n+import uuid\n+from unittest import skipIf\n+\n+from tests.forms_tests.field_tests.test_jsonfield import CustomDecoder\n+\n+from django import forms\n+from django.core import checks, serializers\n+from django.core.exceptions import ValidationError\n+from django.core.serializers.json import DjangoJSONEncoder\n+from django.db import (\n+    DataError, IntegrityError, OperationalError, connection, models,\n+)\n+from django.db.models import Count, F, OuterRef, Q, Subquery, Transform, Value\n+from django.db.models.expressions import RawSQL\n+from django.db.models.fields.json import (\n+    KeyTextTransform, KeyTransform, KeyTransformFactory,\n+    KeyTransformTextLookupMixin,\n+)\n+from django.db.models.functions import Cast\n+from django.db.utils import DatabaseError\n+from django.test import SimpleTestCase, TestCase, skipUnlessDBFeature\n+from django.test.utils import CaptureQueriesContext, isolate_apps\n+from django.utils.version import PY37\n+\n+from .models import JSONModel, NullableJSONModel\n+\n+\n+class StrEncoder(json.JSONEncoder):\n+    def encode(self, obj):\n+        return str(obj)\n+\n+\n+class SetEncoderDecoderMixin:\n+    def _set_encoder_decoder(self, encoder, decoder):\n+        field = JSONModel._meta.get_field('value')\n+        field.encoder, field.decoder = encoder, decoder\n+        return field.check()\n+\n+    def tearDown(self):\n+        self._set_encoder_decoder(None, None)\n+        return super().tearDown()\n+\n+\n+class TestFieldMeta(SetEncoderDecoderMixin, TestCase):\n+    def test_deconstruction(self):\n+        field = models.JSONField(\n+            'JSON data', 'data', default=list, encoder=DjangoJSONEncoder, decoder=CustomDecoder\n+        )\n+        name, path, args, kwargs = field.deconstruct()\n+        self.assertEqual(name, 'data')\n+        self.assertEqual(path, 'django.db.models.JSONField')\n+        self.assertEqual(args, [])\n+        self.assertEqual(kwargs, {\n+            'verbose_name': 'JSON data', 'default': list,\n+            'encoder': DjangoJSONEncoder, 'decoder': CustomDecoder\n+        })\n+\n+    def test_get_transforms(self):\n+        @models.JSONField.register_lookup\n+        class MyTransform(Transform):\n+            lookup_name = 'my_transform'\n+        field = models.JSONField()\n+        transform = field.get_transform('my_transform')\n+        self.assertIs(transform, MyTransform)\n+        models.JSONField._unregister_lookup(MyTransform)\n+        models.JSONField._clear_cached_lookups()\n+        transform = field.get_transform('my_transform')\n+        self.assertIsInstance(transform, KeyTransformFactory)\n+\n+    def test_key_transform_text_lookup_mixin_non_key_transform(self):\n+        transform = Transform('test')\n+        with self.assertRaisesMessage(\n+            TypeError,\n+            'Transform should be an instance of KeyTransform in order to use this lookup.'\n+        ):\n+            KeyTransformTextLookupMixin(transform)\n+\n+\n+class TestValidation(SetEncoderDecoderMixin, TestCase):\n+\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.uuid_value = {'uuid': uuid.UUID('{12345678-1234-5678-1234-567812345678}')}\n+\n+    def test_validation_error(self):\n+        field = models.JSONField()\n+        with self.assertRaises(ValidationError) as err:\n+            field.clean(self.uuid_value, None)\n+        self.assertEqual(err.exception.code, 'invalid')\n+        self.assertEqual(err.exception.message % err.exception.params, 'Value must be valid JSON.')\n+\n+    def test_not_serializable(self):\n+        obj = JSONModel(value=self.uuid_value)\n+        if PY37:\n+            msg = 'Object of type UUID is not JSON serializable'\n+        else:\n+            msg = \"Object of type 'UUID' is not JSON serializable\"\n+        with self.assertRaisesMessage(TypeError, msg):\n+            obj.save()\n+\n+    @skipUnlessDBFeature('supports_json_field')\n+    def test_custom_encoder_decoder(self):\n+        self._set_encoder_decoder(DjangoJSONEncoder, CustomDecoder)\n+        obj = JSONModel(value=self.uuid_value)\n+        obj.clean_fields()\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.value, self.uuid_value)\n+\n+    @skipUnlessDBFeature('supports_json_field')\n+    def test_db_check_constraints(self):\n+        value = '{@!invalid json value 123 $!@#'\n+        self._set_encoder_decoder(StrEncoder, None)\n+        obj = JSONModel(value=value)\n+        with self.assertRaises((IntegrityError, DataError, OperationalError)):\n+            obj.save()\n+\n+\n+class TestModelFormField(SimpleTestCase):\n+    def test_formfield(self):\n+        model_field = models.JSONField()\n+        form_field = model_field.formfield()\n+        self.assertIsInstance(form_field, forms.JSONField)\n+\n+    def test_formfield_custom_encoder_decoder(self):\n+        model_field = models.JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n+        form_field = model_field.formfield()\n+        self.assertIs(form_field.encoder, DjangoJSONEncoder)\n+        self.assertIs(form_field.decoder, CustomDecoder)\n+\n+\n+@isolate_apps('model_fields.test_jsonfield')\n+@skipUnlessDBFeature('supports_json_field')\n+class TestChecks(TestCase):\n+    def test_invalid_default(self):\n+        class MyModel(models.Model):\n+            field = models.JSONField(default={})\n+\n+        model = MyModel()\n+        self.assertEqual(model.check(), [\n+            checks.Warning(\n+                msg=(\n+                    \"JSONField default should be a callable instead of an \"\n+                    \"instance so that it's not shared between all field \"\n+                    \"instances.\"\n+                ),\n+                hint='Use a callable instead, e.g., use `dict` instead of `{}`.',\n+                obj=MyModel._meta.get_field('field'),\n+                id='fields.E010',\n+            )\n+        ])\n+\n+    def test_valid_default(self):\n+        class MyModel(models.Model):\n+            field = models.JSONField(default=dict)\n+\n+        model = MyModel()\n+        self.assertEqual(model.check(), [])\n+\n+    def test_valid_default_none(self):\n+        class MyModel(models.Model):\n+            field = models.JSONField(default=None)\n+\n+        model = MyModel()\n+        self.assertEqual(model.check(), [])\n+\n+    def test_valid_callable_default(self):\n+        def callable_default():\n+            return {'it': 'works'}\n+\n+        class MyModel(models.Model):\n+            field = models.JSONField(default=callable_default)\n+\n+        model = MyModel()\n+        self.assertEqual(model.check(), [])\n+\n+\n+class TestSerialization(SimpleTestCase):\n+    test_data = (\n+        '[{\"fields\": {\"value\": %s}, '\n+        '\"model\": \"model_fields.jsonmodel\", \"pk\": null}]'\n+    )\n+    test_values = (\n+        # (Python value, serialized value),\n+        ({'a': 'b', 'c': None}, '{\"a\": \"b\", \"c\": null}'),\n+        ('abc', '\"abc\"'),\n+        ('{\"a\": \"a\"}', '\"{\\\\\"a\\\\\": \\\\\"a\\\\\"}\"'),\n+    )\n+\n+    def test_dumping(self):\n+        for value, serialized in self.test_values:\n+            with self.subTest(value=value):\n+                instance = JSONModel(value=value)\n+                data = serializers.serialize('json', [instance])\n+                self.assertJSONEqual(data, self.test_data % serialized)\n+\n+    def test_loading(self):\n+        for value, serialized in self.test_values:\n+            with self.subTest(value=value):\n+                instance = list(\n+                    serializers.deserialize('json', self.test_data % serialized)\n+                )[0].object\n+                self.assertEqual(instance.value, value)\n+\n+\n+@skipUnlessDBFeature('supports_json_field')\n+class TestSaveLoad(TestCase):\n+    def test_null(self):\n+        obj = NullableJSONModel(value=None)\n+        obj.save()\n+        obj.refresh_from_db()\n+        self.assertEqual(\n+            obj.value,\n+            '' if connection.features.interprets_empty_strings_as_nulls else None,\n+        )\n+\n+    @skipUnlessDBFeature('supports_primitives_in_json_field')\n+    def test_json_null_different_from_sql_null(self):\n+        json_null = NullableJSONModel.objects.create(value=Value('null'))\n+        json_null.refresh_from_db()\n+        sql_null = NullableJSONModel.objects.create(value=None)\n+        sql_null.refresh_from_db()\n+\n+        # They are different in the database ('null' vs NULL).\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value=Value('null')),\n+            [json_null]\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value=None),\n+            [json_null]\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True),\n+            [sql_null]\n+        )\n+        # They are equal in Python (None).\n+        self.assertEqual(json_null.value, sql_null.value)\n+\n+    @skipUnlessDBFeature('supports_primitives_in_json_field')\n+    def test_primitives(self):\n+        values = [\n+            True,\n+            1,\n+            1.45,\n+            'String',\n+            '',\n+        ]\n+        for value in values:\n+            with self.subTest(value=value):\n+                obj = JSONModel(value=value)\n+                obj.save()\n+                obj.refresh_from_db()\n+                if value == Value('null'):\n+                    value = None\n+                self.assertEqual(obj.value, value)\n+\n+    def test_dict(self):\n+        values = [\n+            {},\n+            {'name': 'John', 'age': 20, 'height': 180.3},\n+            {'a': True, 'b': {'b1': False, 'b2': None}},\n+        ]\n+        for value in values:\n+            with self.subTest(value=value):\n+                obj = JSONModel.objects.create(value=value)\n+                obj.refresh_from_db()\n+                self.assertEqual(obj.value, value)\n+\n+    def test_list(self):\n+        values = [\n+            [],\n+            ['John', 20, 180.3],\n+            [True, [False, None]],\n+        ]\n+        for value in values:\n+            with self.subTest(value=value):\n+                obj = JSONModel.objects.create(value=value)\n+                obj.refresh_from_db()\n+                self.assertEqual(obj.value, value)\n+\n+    def test_realistic_object(self):\n+        value = {\n+            'name': 'John',\n+            'age': 20,\n+            'pets': [\n+                {'name': 'Kit', 'type': 'cat', 'age': 2},\n+                {'name': 'Max', 'type': 'dog', 'age': 1},\n+            ],\n+            'courses': [\n+                ['A1', 'A2', 'A3'],\n+                ['B1', 'B2'],\n+                ['C1'],\n+            ],\n+        }\n+        obj = JSONModel.objects.create(value=value)\n+        obj.refresh_from_db()\n+        self.assertEqual(obj.value, value)\n+\n+\n+@skipUnlessDBFeature('supports_json_field')\n+class TestQuerying(TestCase):\n+    @classmethod\n+    def setUpTestData(cls):\n+        cls.primitives = [True, False, 'yes', 7, 9.6]\n+        values = [\n+            None,\n+            [],\n+            {},\n+            {'a': 'b', 'c': 14},\n+            {\n+                'a': 'b',\n+                'c': 14,\n+                'd': ['e', {'f': 'g'}],\n+                'h': True,\n+                'i': False,\n+                'j': None,\n+                'k': {'l': 'm'},\n+            },\n+            [1, [2]],\n+            {'k': True, 'l': False},\n+            {\n+                'foo': 'bar',\n+                'baz': {'a': 'b', 'c': 'd'},\n+                'bar': ['foo', 'bar'],\n+                'bax': {'foo': 'bar'},\n+            },\n+        ]\n+        cls.objs = [\n+            NullableJSONModel.objects.create(value=value)\n+            for value in values\n+        ]\n+        if connection.features.supports_primitives_in_json_field:\n+            cls.objs.extend([\n+                NullableJSONModel.objects.create(value=value)\n+                for value in cls.primitives\n+            ])\n+\n+    def test_has_key_with_null_value(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__has_key='j'),\n+            [self.objs[4]]\n+        )\n+\n+    def test_has_key(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__has_key='a'),\n+            [self.objs[3], self.objs[4]]\n+        )\n+\n+    def test_has_keys(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__has_keys=['a', 'c', 'h']),\n+            [self.objs[4]]\n+        )\n+\n+    def test_has_any_keys(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__has_any_keys=['c', 'l']),\n+            [self.objs[3], self.objs[4], self.objs[6]],\n+        )\n+\n+    @skipUnlessDBFeature('supports_primitives_in_json_field')\n+    def test_contains_primitives(self):\n+        for value in self.primitives:\n+            with self.subTest(value=value):\n+                self.assertTrue(\n+                    NullableJSONModel.objects.filter(value__contains=value).exists()\n+                )\n+\n+    def test_contains_empty_dict(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__contains={}),\n+            self.objs[2:5] + self.objs[6:8],\n+        )\n+\n+    def test_contains_multiple(self):\n+        query = NullableJSONModel.objects.filter(value__contains={'k': True, 'l': False})\n+        self.assertSequenceEqual(\n+            query,\n+            [self.objs[6]]\n+        )\n+\n+    def test_contains_complex(self):\n+        query = NullableJSONModel.objects.filter(value__contains={'d': ['e', {'f': 'g'}]})\n+        self.assertSequenceEqual(\n+            query,\n+            [self.objs[4]]\n+        )\n+\n+    def test_contains_array(self):\n+        query = NullableJSONModel.objects.filter(value__contains=[1, [2]])\n+        self.assertSequenceEqual(\n+            query,\n+            [self.objs[5]]\n+        )\n+\n+    def test_contains_null(self):\n+        query = NullableJSONModel.objects.filter(value__contains={'i': False, 'j': None})\n+        self.assertSequenceEqual(\n+            query,\n+            [self.objs[4]]\n+        )\n+\n+    @skipIf(connection.vendor == 'oracle', \"Oracle does not support 'contained_by' lookup.\")\n+    def test_contained_by(self):\n+        query = NullableJSONModel.objects.filter(value__contained_by={'a': 'b', 'c': 14, 'h': True})\n+        self.assertSequenceEqual(\n+            query,\n+            [self.objs[2], self.objs[3]]\n+        )\n+\n+    def test_exact(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__exact={}),\n+            [self.objs[2]]\n+        )\n+\n+    def test_exact_complex(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__exact={'a': 'b', 'c': 14}),\n+            [self.objs[3]]\n+        )\n+\n+    def test_isnull(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__isnull=True),\n+            [self.objs[0]]\n+        )\n+\n+    def test_ordering_by_transform(self):\n+        objs = [\n+            NullableJSONModel.objects.create(value={'ord': 93, 'name': 'bar'}),\n+            NullableJSONModel.objects.create(value={'ord': 22.1, 'name': 'foo'}),\n+            NullableJSONModel.objects.create(value={'ord': -1, 'name': 'baz'}),\n+            NullableJSONModel.objects.create(value={'ord': 21.931902, 'name': 'spam'}),\n+            NullableJSONModel.objects.create(value={'ord': -100291029, 'name': 'eggs'}),\n+        ]\n+        query = NullableJSONModel.objects.filter(value__name__isnull=False).order_by('value__ord')\n+        if connection.vendor == 'mysql' and connection.mysql_is_mariadb or connection.vendor == 'oracle':\n+            # MariaDB and Oracle use string representation of the JSON values to sort the objects.\n+            self.assertSequenceEqual(query, [objs[2], objs[4], objs[3], objs[1], objs[0]])\n+        else:\n+            self.assertSequenceEqual(query, [objs[4], objs[2], objs[3], objs[1], objs[0]])\n+\n+    def test_ordering_grouping_by_key_transform(self):\n+        base_qs = NullableJSONModel.objects.filter(value__d__0__isnull=False)\n+        for qs in (\n+            base_qs.order_by('value__d__0'),\n+            base_qs.annotate(key=KeyTransform('0', KeyTransform('d', 'value'))).order_by('key'),\n+        ):\n+            self.assertSequenceEqual(qs, [self.objs[4]])\n+        qs = NullableJSONModel.objects.filter(value__isnull=False)\n+        if connection.vendor != 'oracle':\n+            # Oracle doesn't support direct COUNT on LOB fields.\n+            self.assertQuerysetEqual(\n+                qs.values('value__d__0').annotate(count=Count('value__d__0')).order_by('count'),\n+                [1, 11],\n+                operator.itemgetter('count'),\n+            )\n+        expected = [(None, 0), ('g', 1)] if connection.vendor != 'oracle' else [('', 0), ('g', 1)]\n+        self.assertQuerysetEqual(\n+            qs.filter(value__isnull=False).annotate(\n+                key=KeyTextTransform('f', KeyTransform('1', KeyTransform('d', 'value'))),\n+            ).values('key').annotate(count=Count('key')).order_by('count'),\n+            expected,\n+            operator.itemgetter('key', 'count'),\n+        )\n+\n+    def test_key_transform_raw_expression(self):\n+        if connection.vendor == 'postgresql':\n+            expr = RawSQL('%s::jsonb', ['{\"x\": \"bar\"}'])\n+        else:\n+            expr = RawSQL('%s', ['{\"x\": \"bar\"}'])\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr)),\n+            [self.objs[7]],\n+        )\n+\n+    def test_key_transform_expression(self):\n+        if connection.vendor == 'oracle' or connection.vendor == 'mysql' and connection.mysql_is_mariadb:\n+            expr = 'key'\n+        else:\n+            expr = Cast('key', models.JSONField())\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__d__0__isnull=False).annotate(\n+                key=KeyTransform('d', 'value'),\n+                chain=KeyTransform('0', 'key'),\n+                expr=KeyTransform('0', expr),\n+            ).filter(chain=F('expr')),\n+            [self.objs[4]],\n+        )\n+\n+    def test_nested_key_transform_raw_expression(self):\n+        if connection.vendor == 'postgresql':\n+            expr = RawSQL('%s::jsonb', ['{\"x\": {\"y\": \"bar\"}}'])\n+        else:\n+            expr = RawSQL('%s', ['{\"x\": {\"y\": \"bar\"}}'])\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__foo=KeyTransform('y', KeyTransform('x', expr))),\n+            [self.objs[7]],\n+        )\n+\n+    def test_nested_key_transform_expression(self):\n+        if connection.vendor == 'oracle' or connection.vendor == 'mysql' and connection.mysql_is_mariadb:\n+            expr = 'key'\n+        else:\n+            expr = Cast('key', models.JSONField())\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__d__0__isnull=False).annotate(\n+                key=KeyTransform('d', 'value'),\n+                chain=KeyTransform('f', KeyTransform('1', 'key')),\n+                expr=KeyTransform('f', KeyTransform('1', expr)),\n+            ).filter(chain=F('expr')),\n+            [self.objs[4]],\n+        )\n+\n+    def test_deep_values(self):\n+        query = NullableJSONModel.objects.values_list('value__k__l')\n+        empty = ('',) if connection.features.interprets_empty_strings_as_nulls else (None,)\n+        expected_objs = [empty] * len(self.objs)\n+        expected_objs[4] = ('m',)\n+        self.assertSequenceEqual(query, expected_objs)\n+\n+    @skipUnlessDBFeature('can_distinct_on_fields')\n+    def test_deep_distinct(self):\n+        query = NullableJSONModel.objects.distinct('value__k__l').values_list('value__k__l')\n+        self.assertSequenceEqual(query, [('m',), (None,)])\n+\n+    def test_isnull_key(self):\n+        # key__isnull=False works the same as has_key='key'.\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__a__isnull=True),\n+            self.objs[:3] + self.objs[5:]\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__a__isnull=False),\n+            [self.objs[3], self.objs[4]]\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__j__isnull=False),\n+            [self.objs[4]]\n+        )\n+\n+    def test_none_key(self):\n+        self.assertSequenceEqual(NullableJSONModel.objects.filter(value__j=None), [self.objs[4]])\n+\n+    def test_none_key_exclude(self):\n+        obj = NullableJSONModel.objects.create(value={'j': 1})\n+        if connection.vendor == 'oracle':\n+            # On Oracle, the query returns JSON objects and arrays that do not have a 'null' value\n+            # at the specified path, including those that do not have the key.\n+            self.assertSequenceEqual(\n+                NullableJSONModel.objects.exclude(value__j=None),\n+                self.objs[1:4] + self.objs[5:] + [obj]\n+            )\n+        else:\n+            self.assertSequenceEqual(NullableJSONModel.objects.exclude(value__j=None), [obj])\n+\n+    def test_isnull_key_or_none(self):\n+        obj = NullableJSONModel.objects.create(value={'a': None})\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(Q(value__a__isnull=True) | Q(value__a=None)),\n+            self.objs[:3] + self.objs[5:] + [obj]\n+        )\n+\n+    def test_shallow_list_lookup(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__0=1),\n+            [self.objs[5]]\n+        )\n+\n+    def test_shallow_obj_lookup(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__a='b'),\n+            [self.objs[3], self.objs[4]]\n+        )\n+\n+    def test_obj_subquery_lookup(self):\n+        qs = NullableJSONModel.objects.annotate(\n+            field=Subquery(NullableJSONModel.objects.filter(pk=OuterRef('pk')).values('value')),\n+        ).filter(field__a='b')\n+        self.assertSequenceEqual(qs, [self.objs[3], self.objs[4]])\n+\n+    def test_deep_lookup_objs(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__k__l='m'),\n+            [self.objs[4]]\n+        )\n+\n+    def test_shallow_lookup_obj_target(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__k={'l': 'm'}),\n+            [self.objs[4]]\n+        )\n+\n+    def test_deep_lookup_array(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__1__0=2),\n+            [self.objs[5]]\n+        )\n+\n+    def test_deep_lookup_mixed(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__d__1__f='g'),\n+            [self.objs[4]]\n+        )\n+\n+    def test_deep_lookup_transform(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__c__gt=2),\n+            [self.objs[3], self.objs[4]]\n+        )\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(value__c__lt=5),\n+            []\n+        )\n+\n+    def test_usage_in_subquery(self):\n+        self.assertSequenceEqual(\n+            NullableJSONModel.objects.filter(id__in=NullableJSONModel.objects.filter(value__c=14)),\n+            self.objs[3:5]\n+        )\n+\n+    def test_iexact(self):\n+        self.assertTrue(NullableJSONModel.objects.filter(value__foo__iexact='BaR').exists())\n+        self.assertFalse(NullableJSONModel.objects.filter(value__foo__iexact='\"BaR\"').exists())\n+\n+    def test_contains(self):\n+        self.assertTrue(NullableJSONModel.objects.filter(value__foo__contains='ar').exists())\n+\n+    def test_icontains(self):\n+        self.assertTrue(NullableJSONModel.objects.filter(value__foo__icontains='A').exists())\n+\n+    def test_startswith(self):\n+        self.assertTrue(NullableJSONModel.objects.filter(value__foo__startswith='b').exists())\n+\n+    def test_istartswith(self):\n+        self.assertTrue(NullableJSONModel.objects.filter(value__foo__istartswith='B').exists())\n+\n+    def test_endswith(self):\n+        self.assertTrue(NullableJSONModel.objects.filter(value__foo__endswith='r').exists())\n+\n+    def test_iendswith(self):\n+        self.assertTrue(NullableJSONModel.objects.filter(value__foo__iendswith='R').exists())\n+\n+    def test_regex(self):\n+        self.assertTrue(NullableJSONModel.objects.filter(value__foo__regex=r'^bar$').exists())\n+\n+    def test_iregex(self):\n+        self.assertTrue(NullableJSONModel.objects.filter(value__foo__iregex=r'^bAr$').exists())\n+\n+    def test_key_sql_injection(self):\n+        with CaptureQueriesContext(connection) as queries:\n+            query = NullableJSONModel.objects.filter(**{\"\"\"value__test' = '\"a\"') OR 1 = 1 OR ('d\"\"\": 'x', })\n+            if connection.vendor == 'oracle':\n+                with self.assertRaises(DatabaseError):\n+                    query.exists()\n+            else:\n+                self.assertFalse(query.exists())\n+        if connection.vendor == 'postgresql':\n+            self.assertIn(\n+                \"\"\".\"value\" -> 'test'' = ''\"a\"'') OR 1 = 1 OR (''d') = '\"x\"' \"\"\",\n+                queries[0]['sql'],\n+            )\n+\n+    def test_lookups_with_key_transform(self):\n+        sql = '%s::jsonb' if connection.vendor == 'postgresql' else '%s'\n+        tests = (\n+            ('value__d__contains', 'e'),\n+            ('value__baz__contained_by', {'a': 'b', 'c': 'd', 'e': 'f'}),\n+            ('value__baz__has_key', 'c'),\n+            ('value__baz__has_keys', ['a', 'c']),\n+            ('value__baz__has_any_keys', ['a', 'x']),\n+            ('value__contains', KeyTransform('bax', 'value')),\n+            (\n+                'value__contained_by',\n+                KeyTransform('x', RawSQL(sql, ['{\"x\": {\"a\": \"b\", \"c\": 1, \"d\": \"e\"}}'])),\n+            ),\n+            ('value__has_key', KeyTextTransform('foo', 'value')),\n+        )\n+        if connection.vendor == 'oracle':\n+            # contained_by is not supported in Oracle.\n+            tests = tests[0:1] + tests[2:6] + tests[7:]\n+        for lookup, value in tests:\n+            with self.subTest(lookup=lookup):\n+                self.assertTrue(NullableJSONModel.objects.filter(\n+                    **{lookup: value},\n+                ).exists())\ndiff --git a/tests/postgres_tests/fields.py b/tests/postgres_tests/fields.py\nindex 4ebc0ce7dc1a..a36c10c750e1 100644\n--- a/tests/postgres_tests/fields.py\n+++ b/tests/postgres_tests/fields.py\n@@ -10,7 +10,7 @@\n     from django.contrib.postgres.fields import (\n         ArrayField, BigIntegerRangeField, CICharField, CIEmailField,\n         CITextField, DateRangeField, DateTimeRangeField, DecimalRangeField,\n-        HStoreField, IntegerRangeField, JSONField,\n+        HStoreField, IntegerRangeField,\n     )\n     from django.contrib.postgres.search import SearchVectorField\n except ImportError:\n@@ -26,10 +26,6 @@ def deconstruct(self):\n             })\n             return name, path, args, kwargs\n \n-    class DummyJSONField(models.Field):\n-        def __init__(self, encoder=None, **kwargs):\n-            super().__init__(**kwargs)\n-\n     ArrayField = DummyArrayField\n     BigIntegerRangeField = models.Field\n     CICharField = models.Field\n@@ -40,7 +36,6 @@ def __init__(self, encoder=None, **kwargs):\n     DecimalRangeField = models.Field\n     HStoreField = models.Field\n     IntegerRangeField = models.Field\n-    JSONField = DummyJSONField\n     SearchVectorField = models.Field\n \n \ndiff --git a/tests/postgres_tests/migrations/0002_create_test_models.py b/tests/postgres_tests/migrations/0002_create_test_models.py\nindex 12d94e348a21..98c5ea34c9ab 100644\n--- a/tests/postgres_tests/migrations/0002_create_test_models.py\n+++ b/tests/postgres_tests/migrations/0002_create_test_models.py\n@@ -1,10 +1,9 @@\n-from django.core.serializers.json import DjangoJSONEncoder\n from django.db import migrations, models\n \n from ..fields import (\n     ArrayField, BigIntegerRangeField, CICharField, CIEmailField, CITextField,\n     DateRangeField, DateTimeRangeField, DecimalRangeField, EnumField,\n-    HStoreField, IntegerRangeField, JSONField, SearchVectorField,\n+    HStoreField, IntegerRangeField, SearchVectorField,\n )\n from ..models import TagField\n \n@@ -60,7 +59,7 @@ class Migration(migrations.Migration):\n                 ('uuids', ArrayField(models.UUIDField(), size=None, default=list)),\n                 ('decimals', ArrayField(models.DecimalField(max_digits=5, decimal_places=2), size=None, default=list)),\n                 ('tags', ArrayField(TagField(), blank=True, null=True, size=None)),\n-                ('json', ArrayField(JSONField(default={}), default=[])),\n+                ('json', ArrayField(models.JSONField(default={}), default=[])),\n                 ('int_ranges', ArrayField(IntegerRangeField(), null=True, blank=True)),\n                 ('bigint_ranges', ArrayField(BigIntegerRangeField(), null=True, blank=True)),\n             ],\n@@ -259,18 +258,6 @@ class Migration(migrations.Migration):\n             },\n             bases=(models.Model,),\n         ),\n-        migrations.CreateModel(\n-            name='JSONModel',\n-            fields=[\n-                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n-                ('field', JSONField(null=True, blank=True)),\n-                ('field_custom', JSONField(null=True, blank=True, encoder=DjangoJSONEncoder)),\n-            ],\n-            options={\n-                'required_db_vendor': 'postgresql',\n-            },\n-            bases=(models.Model,),\n-        ),\n         migrations.CreateModel(\n             name='ArrayEnumModel',\n             fields=[\ndiff --git a/tests/postgres_tests/models.py b/tests/postgres_tests/models.py\nindex 8528c59da1c0..d8cb3e00970f 100644\n--- a/tests/postgres_tests/models.py\n+++ b/tests/postgres_tests/models.py\n@@ -1,10 +1,9 @@\n-from django.core.serializers.json import DjangoJSONEncoder\n from django.db import models\n \n from .fields import (\n     ArrayField, BigIntegerRangeField, CICharField, CIEmailField, CITextField,\n     DateRangeField, DateTimeRangeField, DecimalRangeField, EnumField,\n-    HStoreField, IntegerRangeField, JSONField, SearchVectorField,\n+    HStoreField, IntegerRangeField, SearchVectorField,\n )\n \n \n@@ -63,15 +62,19 @@ class NestedIntegerArrayModel(PostgreSQLModel):\n     field = ArrayField(ArrayField(models.IntegerField()))\n \n \n-class OtherTypesArrayModel(PostgreSQLModel):\n+class OtherTypesArrayModel(models.Model):\n     ips = ArrayField(models.GenericIPAddressField(), default=list)\n     uuids = ArrayField(models.UUIDField(), default=list)\n     decimals = ArrayField(models.DecimalField(max_digits=5, decimal_places=2), default=list)\n     tags = ArrayField(TagField(), blank=True, null=True)\n-    json = ArrayField(JSONField(default=dict), default=list)\n+    json = ArrayField(models.JSONField(default=dict), default=list)\n     int_ranges = ArrayField(IntegerRangeField(), blank=True, null=True)\n     bigint_ranges = ArrayField(BigIntegerRangeField(), blank=True, null=True)\n \n+    class Meta:\n+        required_db_features = {'supports_json_field'}\n+        required_db_vendor = 'postgresql'\n+\n \n class HStoreModel(PostgreSQLModel):\n     field = HStoreField(blank=True, null=True)\n@@ -160,11 +163,6 @@ class RangeLookupsModel(PostgreSQLModel):\n     decimal_field = models.DecimalField(max_digits=5, decimal_places=2, blank=True, null=True)\n \n \n-class JSONModel(PostgreSQLModel):\n-    field = JSONField(blank=True, null=True)\n-    field_custom = JSONField(blank=True, null=True, encoder=DjangoJSONEncoder)\n-\n-\n class ArrayFieldSubclass(ArrayField):\n     def __init__(self, *args, **kwargs):\n         super().__init__(models.IntegerField())\ndiff --git a/tests/postgres_tests/test_bulk_update.py b/tests/postgres_tests/test_bulk_update.py\nindex 6dd7036a9bf2..7fa2a6a7db77 100644\n--- a/tests/postgres_tests/test_bulk_update.py\n+++ b/tests/postgres_tests/test_bulk_update.py\n@@ -2,7 +2,7 @@\n \n from . import PostgreSQLTestCase\n from .models import (\n-    HStoreModel, IntegerArrayModel, JSONModel, NestedIntegerArrayModel,\n+    HStoreModel, IntegerArrayModel, NestedIntegerArrayModel,\n     NullableIntegerArrayModel, OtherTypesArrayModel, RangesModel,\n )\n \n@@ -17,7 +17,6 @@ def test_bulk_update(self):\n         test_data = [\n             (IntegerArrayModel, 'field', [], [1, 2, 3]),\n             (NullableIntegerArrayModel, 'field', [1, 2, 3], None),\n-            (JSONModel, 'field', {'a': 'b'}, {'c': 'd'}),\n             (NestedIntegerArrayModel, 'field', [], [[1, 2, 3]]),\n             (HStoreModel, 'field', {}, {1: 2}),\n             (RangesModel, 'ints', None, NumericRange(lower=1, upper=10)),\ndiff --git a/tests/postgres_tests/test_introspection.py b/tests/postgres_tests/test_introspection.py\nindex 8ae5b80da11f..50cb9b282806 100644\n--- a/tests/postgres_tests/test_introspection.py\n+++ b/tests/postgres_tests/test_introspection.py\n@@ -19,12 +19,6 @@ def assertFieldsInModel(self, model, field_outputs):\n         for field_output in field_outputs:\n             self.assertIn(field_output, output)\n \n-    def test_json_field(self):\n-        self.assertFieldsInModel(\n-            'postgres_tests_jsonmodel',\n-            ['field = django.contrib.postgres.fields.JSONField(blank=True, null=True)'],\n-        )\n-\n     def test_range_fields(self):\n         self.assertFieldsInModel(\n             'postgres_tests_rangesmodel',\ndiff --git a/tests/postgres_tests/test_json.py b/tests/postgres_tests/test_json.py\ndeleted file mode 100644\nindex 0ffa05f98144..000000000000\n--- a/tests/postgres_tests/test_json.py\n+++ /dev/null\n@@ -1,579 +0,0 @@\n-import datetime\n-import operator\n-import uuid\n-from decimal import Decimal\n-\n-from django.core import checks, exceptions, serializers\n-from django.core.serializers.json import DjangoJSONEncoder\n-from django.db import connection\n-from django.db.models import Count, F, OuterRef, Q, Subquery\n-from django.db.models.expressions import RawSQL\n-from django.db.models.functions import Cast\n-from django.forms import CharField, Form, widgets\n-from django.test.utils import CaptureQueriesContext, isolate_apps\n-from django.utils.html import escape\n-\n-from . import PostgreSQLSimpleTestCase, PostgreSQLTestCase\n-from .models import JSONModel, PostgreSQLModel\n-\n-try:\n-    from django.contrib.postgres import forms\n-    from django.contrib.postgres.fields import JSONField\n-    from django.contrib.postgres.fields.jsonb import KeyTextTransform, KeyTransform\n-except ImportError:\n-    pass\n-\n-\n-class TestModelMetaOrdering(PostgreSQLSimpleTestCase):\n-    def test_ordering_by_json_field_value(self):\n-        class TestJSONModel(JSONModel):\n-            class Meta:\n-                ordering = ['field__value']\n-\n-        self.assertEqual(TestJSONModel.check(), [])\n-\n-\n-class TestSaveLoad(PostgreSQLTestCase):\n-    def test_null(self):\n-        instance = JSONModel()\n-        instance.save()\n-        loaded = JSONModel.objects.get()\n-        self.assertIsNone(loaded.field)\n-\n-    def test_empty_object(self):\n-        instance = JSONModel(field={})\n-        instance.save()\n-        loaded = JSONModel.objects.get()\n-        self.assertEqual(loaded.field, {})\n-\n-    def test_empty_list(self):\n-        instance = JSONModel(field=[])\n-        instance.save()\n-        loaded = JSONModel.objects.get()\n-        self.assertEqual(loaded.field, [])\n-\n-    def test_boolean(self):\n-        instance = JSONModel(field=True)\n-        instance.save()\n-        loaded = JSONModel.objects.get()\n-        self.assertIs(loaded.field, True)\n-\n-    def test_string(self):\n-        instance = JSONModel(field='why?')\n-        instance.save()\n-        loaded = JSONModel.objects.get()\n-        self.assertEqual(loaded.field, 'why?')\n-\n-    def test_number(self):\n-        instance = JSONModel(field=1)\n-        instance.save()\n-        loaded = JSONModel.objects.get()\n-        self.assertEqual(loaded.field, 1)\n-\n-    def test_realistic_object(self):\n-        obj = {\n-            'a': 'b',\n-            'c': 1,\n-            'd': ['e', {'f': 'g'}],\n-            'h': True,\n-            'i': False,\n-            'j': None,\n-        }\n-        instance = JSONModel(field=obj)\n-        instance.save()\n-        loaded = JSONModel.objects.get()\n-        self.assertEqual(loaded.field, obj)\n-\n-    def test_custom_encoding(self):\n-        \"\"\"\n-        JSONModel.field_custom has a custom DjangoJSONEncoder.\n-        \"\"\"\n-        some_uuid = uuid.uuid4()\n-        obj_before = {\n-            'date': datetime.date(2016, 8, 12),\n-            'datetime': datetime.datetime(2016, 8, 12, 13, 44, 47, 575981),\n-            'decimal': Decimal('10.54'),\n-            'uuid': some_uuid,\n-        }\n-        obj_after = {\n-            'date': '2016-08-12',\n-            'datetime': '2016-08-12T13:44:47.575',\n-            'decimal': '10.54',\n-            'uuid': str(some_uuid),\n-        }\n-        JSONModel.objects.create(field_custom=obj_before)\n-        loaded = JSONModel.objects.get()\n-        self.assertEqual(loaded.field_custom, obj_after)\n-\n-\n-class TestQuerying(PostgreSQLTestCase):\n-    @classmethod\n-    def setUpTestData(cls):\n-        cls.objs = JSONModel.objects.bulk_create([\n-            JSONModel(field=None),\n-            JSONModel(field=True),\n-            JSONModel(field=False),\n-            JSONModel(field='yes'),\n-            JSONModel(field=7),\n-            JSONModel(field=[]),\n-            JSONModel(field={}),\n-            JSONModel(field={\n-                'a': 'b',\n-                'c': 1,\n-            }),\n-            JSONModel(field={\n-                'a': 'b',\n-                'c': 1,\n-                'd': ['e', {'f': 'g'}],\n-                'h': True,\n-                'i': False,\n-                'j': None,\n-                'k': {'l': 'm'},\n-            }),\n-            JSONModel(field=[1, [2]]),\n-            JSONModel(field={\n-                'k': True,\n-                'l': False,\n-            }),\n-            JSONModel(field={\n-                'foo': 'bar',\n-                'baz': {'a': 'b', 'c': 'd'},\n-                'bar': ['foo', 'bar'],\n-                'bax': {'foo': 'bar'},\n-            }),\n-        ])\n-\n-    def test_exact(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__exact={}),\n-            [self.objs[6]]\n-        )\n-\n-    def test_exact_complex(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__exact={'a': 'b', 'c': 1}),\n-            [self.objs[7]]\n-        )\n-\n-    def test_isnull(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__isnull=True),\n-            [self.objs[0]]\n-        )\n-\n-    def test_ordering_by_transform(self):\n-        objs = [\n-            JSONModel.objects.create(field={'ord': 93, 'name': 'bar'}),\n-            JSONModel.objects.create(field={'ord': 22.1, 'name': 'foo'}),\n-            JSONModel.objects.create(field={'ord': -1, 'name': 'baz'}),\n-            JSONModel.objects.create(field={'ord': 21.931902, 'name': 'spam'}),\n-            JSONModel.objects.create(field={'ord': -100291029, 'name': 'eggs'}),\n-        ]\n-        query = JSONModel.objects.filter(field__name__isnull=False).order_by('field__ord')\n-        self.assertSequenceEqual(query, [objs[4], objs[2], objs[3], objs[1], objs[0]])\n-\n-    def test_ordering_grouping_by_key_transform(self):\n-        base_qs = JSONModel.objects.filter(field__d__0__isnull=False)\n-        for qs in (\n-            base_qs.order_by('field__d__0'),\n-            base_qs.annotate(key=KeyTransform('0', KeyTransform('d', 'field'))).order_by('key'),\n-        ):\n-            self.assertSequenceEqual(qs, [self.objs[8]])\n-        qs = JSONModel.objects.filter(field__isnull=False)\n-        self.assertQuerysetEqual(\n-            qs.values('field__d__0').annotate(count=Count('field__d__0')).order_by('count'),\n-            [1, 10],\n-            operator.itemgetter('count'),\n-        )\n-        self.assertQuerysetEqual(\n-            qs.filter(field__isnull=False).annotate(\n-                key=KeyTextTransform('f', KeyTransform('1', KeyTransform('d', 'field'))),\n-            ).values('key').annotate(count=Count('key')).order_by('count'),\n-            [(None, 0), ('g', 1)],\n-            operator.itemgetter('key', 'count'),\n-        )\n-\n-    def test_key_transform_raw_expression(self):\n-        expr = RawSQL('%s::jsonb', ['{\"x\": \"bar\"}'])\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__foo=KeyTransform('x', expr)),\n-            [self.objs[-1]],\n-        )\n-\n-    def test_key_transform_expression(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__d__0__isnull=False).annotate(\n-                key=KeyTransform('d', 'field'),\n-                chain=KeyTransform('0', 'key'),\n-                expr=KeyTransform('0', Cast('key', JSONField())),\n-            ).filter(chain=F('expr')),\n-            [self.objs[8]],\n-        )\n-\n-    def test_nested_key_transform_raw_expression(self):\n-        expr = RawSQL('%s::jsonb', ['{\"x\": {\"y\": \"bar\"}}'])\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__foo=KeyTransform('y', KeyTransform('x', expr))),\n-            [self.objs[-1]],\n-        )\n-\n-    def test_nested_key_transform_expression(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__d__0__isnull=False).annotate(\n-                key=KeyTransform('d', 'field'),\n-                chain=KeyTransform('f', KeyTransform('1', 'key')),\n-                expr=KeyTransform('f', KeyTransform('1', Cast('key', JSONField()))),\n-            ).filter(chain=F('expr')),\n-            [self.objs[8]],\n-        )\n-\n-    def test_deep_values(self):\n-        query = JSONModel.objects.values_list('field__k__l')\n-        self.assertSequenceEqual(\n-            query,\n-            [\n-                (None,), (None,), (None,), (None,), (None,), (None,),\n-                (None,), (None,), ('m',), (None,), (None,), (None,),\n-            ]\n-        )\n-\n-    def test_deep_distinct(self):\n-        query = JSONModel.objects.distinct('field__k__l').values_list('field__k__l')\n-        self.assertSequenceEqual(query, [('m',), (None,)])\n-\n-    def test_isnull_key(self):\n-        # key__isnull works the same as has_key='key'.\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__a__isnull=True),\n-            self.objs[:7] + self.objs[9:]\n-        )\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__a__isnull=False),\n-            [self.objs[7], self.objs[8]]\n-        )\n-\n-    def test_none_key(self):\n-        self.assertSequenceEqual(JSONModel.objects.filter(field__j=None), [self.objs[8]])\n-\n-    def test_none_key_exclude(self):\n-        obj = JSONModel.objects.create(field={'j': 1})\n-        self.assertSequenceEqual(JSONModel.objects.exclude(field__j=None), [obj])\n-\n-    def test_isnull_key_or_none(self):\n-        obj = JSONModel.objects.create(field={'a': None})\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(Q(field__a__isnull=True) | Q(field__a=None)),\n-            self.objs[:7] + self.objs[9:] + [obj]\n-        )\n-\n-    def test_contains(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__contains={'a': 'b'}),\n-            [self.objs[7], self.objs[8]]\n-        )\n-\n-    def test_contained_by(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__contained_by={'a': 'b', 'c': 1, 'h': True}),\n-            [self.objs[6], self.objs[7]]\n-        )\n-\n-    def test_has_key(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__has_key='a'),\n-            [self.objs[7], self.objs[8]]\n-        )\n-\n-    def test_has_keys(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__has_keys=['a', 'c', 'h']),\n-            [self.objs[8]]\n-        )\n-\n-    def test_has_any_keys(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__has_any_keys=['c', 'l']),\n-            [self.objs[7], self.objs[8], self.objs[10]]\n-        )\n-\n-    def test_shallow_list_lookup(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__0=1),\n-            [self.objs[9]]\n-        )\n-\n-    def test_shallow_obj_lookup(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__a='b'),\n-            [self.objs[7], self.objs[8]]\n-        )\n-\n-    def test_obj_subquery_lookup(self):\n-        qs = JSONModel.objects.annotate(\n-            value=Subquery(JSONModel.objects.filter(pk=OuterRef('pk')).values('field')),\n-        ).filter(value__a='b')\n-        self.assertSequenceEqual(qs, [self.objs[7], self.objs[8]])\n-\n-    def test_deep_lookup_objs(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__k__l='m'),\n-            [self.objs[8]]\n-        )\n-\n-    def test_shallow_lookup_obj_target(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__k={'l': 'm'}),\n-            [self.objs[8]]\n-        )\n-\n-    def test_deep_lookup_array(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__1__0=2),\n-            [self.objs[9]]\n-        )\n-\n-    def test_deep_lookup_mixed(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__d__1__f='g'),\n-            [self.objs[8]]\n-        )\n-\n-    def test_deep_lookup_transform(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__c__gt=1),\n-            []\n-        )\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(field__c__lt=5),\n-            [self.objs[7], self.objs[8]]\n-        )\n-\n-    def test_usage_in_subquery(self):\n-        self.assertSequenceEqual(\n-            JSONModel.objects.filter(id__in=JSONModel.objects.filter(field__c=1)),\n-            self.objs[7:9]\n-        )\n-\n-    def test_iexact(self):\n-        self.assertTrue(JSONModel.objects.filter(field__foo__iexact='BaR').exists())\n-        self.assertFalse(JSONModel.objects.filter(field__foo__iexact='\"BaR\"').exists())\n-\n-    def test_icontains(self):\n-        self.assertFalse(JSONModel.objects.filter(field__foo__icontains='\"bar\"').exists())\n-\n-    def test_startswith(self):\n-        self.assertTrue(JSONModel.objects.filter(field__foo__startswith='b').exists())\n-\n-    def test_istartswith(self):\n-        self.assertTrue(JSONModel.objects.filter(field__foo__istartswith='B').exists())\n-\n-    def test_endswith(self):\n-        self.assertTrue(JSONModel.objects.filter(field__foo__endswith='r').exists())\n-\n-    def test_iendswith(self):\n-        self.assertTrue(JSONModel.objects.filter(field__foo__iendswith='R').exists())\n-\n-    def test_regex(self):\n-        self.assertTrue(JSONModel.objects.filter(field__foo__regex=r'^bar$').exists())\n-\n-    def test_iregex(self):\n-        self.assertTrue(JSONModel.objects.filter(field__foo__iregex=r'^bAr$').exists())\n-\n-    def test_key_sql_injection(self):\n-        with CaptureQueriesContext(connection) as queries:\n-            self.assertFalse(\n-                JSONModel.objects.filter(**{\n-                    \"\"\"field__test' = '\"a\"') OR 1 = 1 OR ('d\"\"\": 'x',\n-                }).exists()\n-            )\n-        self.assertIn(\n-            \"\"\".\"field\" -> 'test'' = ''\"a\"'') OR 1 = 1 OR (''d') = '\"x\"' \"\"\",\n-            queries[0]['sql'],\n-        )\n-\n-    def test_lookups_with_key_transform(self):\n-        tests = (\n-            ('field__d__contains', 'e'),\n-            ('field__baz__contained_by', {'a': 'b', 'c': 'd', 'e': 'f'}),\n-            ('field__baz__has_key', 'c'),\n-            ('field__baz__has_keys', ['a', 'c']),\n-            ('field__baz__has_any_keys', ['a', 'x']),\n-            ('field__contains', KeyTransform('bax', 'field')),\n-            (\n-                'field__contained_by',\n-                KeyTransform('x', RawSQL('%s::jsonb', ['{\"x\": {\"a\": \"b\", \"c\": 1, \"d\": \"e\"}}'])),\n-            ),\n-            ('field__has_key', KeyTextTransform('foo', 'field')),\n-        )\n-        for lookup, value in tests:\n-            with self.subTest(lookup=lookup):\n-                self.assertTrue(JSONModel.objects.filter(\n-                    **{lookup: value},\n-                ).exists())\n-\n-\n-@isolate_apps('postgres_tests')\n-class TestChecks(PostgreSQLSimpleTestCase):\n-\n-    def test_invalid_default(self):\n-        class MyModel(PostgreSQLModel):\n-            field = JSONField(default={})\n-\n-        model = MyModel()\n-        self.assertEqual(model.check(), [\n-            checks.Warning(\n-                msg=(\n-                    \"JSONField default should be a callable instead of an \"\n-                    \"instance so that it's not shared between all field \"\n-                    \"instances.\"\n-                ),\n-                hint='Use a callable instead, e.g., use `dict` instead of `{}`.',\n-                obj=MyModel._meta.get_field('field'),\n-                id='fields.E010',\n-            )\n-        ])\n-\n-    def test_valid_default(self):\n-        class MyModel(PostgreSQLModel):\n-            field = JSONField(default=dict)\n-\n-        model = MyModel()\n-        self.assertEqual(model.check(), [])\n-\n-    def test_valid_default_none(self):\n-        class MyModel(PostgreSQLModel):\n-            field = JSONField(default=None)\n-\n-        model = MyModel()\n-        self.assertEqual(model.check(), [])\n-\n-\n-class TestSerialization(PostgreSQLSimpleTestCase):\n-    test_data = (\n-        '[{\"fields\": {\"field\": %s, \"field_custom\": null}, '\n-        '\"model\": \"postgres_tests.jsonmodel\", \"pk\": null}]'\n-    )\n-    test_values = (\n-        # (Python value, serialized value),\n-        ({'a': 'b', 'c': None}, '{\"a\": \"b\", \"c\": null}'),\n-        ('abc', '\"abc\"'),\n-        ('{\"a\": \"a\"}', '\"{\\\\\"a\\\\\": \\\\\"a\\\\\"}\"'),\n-    )\n-\n-    def test_dumping(self):\n-        for value, serialized in self.test_values:\n-            with self.subTest(value=value):\n-                instance = JSONModel(field=value)\n-                data = serializers.serialize('json', [instance])\n-                self.assertJSONEqual(data, self.test_data % serialized)\n-\n-    def test_loading(self):\n-        for value, serialized in self.test_values:\n-            with self.subTest(value=value):\n-                instance = list(serializers.deserialize('json', self.test_data % serialized))[0].object\n-                self.assertEqual(instance.field, value)\n-\n-\n-class TestValidation(PostgreSQLSimpleTestCase):\n-\n-    def test_not_serializable(self):\n-        field = JSONField()\n-        with self.assertRaises(exceptions.ValidationError) as cm:\n-            field.clean(datetime.timedelta(days=1), None)\n-        self.assertEqual(cm.exception.code, 'invalid')\n-        self.assertEqual(cm.exception.message % cm.exception.params, \"Value must be valid JSON.\")\n-\n-    def test_custom_encoder(self):\n-        with self.assertRaisesMessage(ValueError, \"The encoder parameter must be a callable object.\"):\n-            field = JSONField(encoder=DjangoJSONEncoder())\n-        field = JSONField(encoder=DjangoJSONEncoder)\n-        self.assertEqual(field.clean(datetime.timedelta(days=1), None), datetime.timedelta(days=1))\n-\n-\n-class TestFormField(PostgreSQLSimpleTestCase):\n-\n-    def test_valid(self):\n-        field = forms.JSONField()\n-        value = field.clean('{\"a\": \"b\"}')\n-        self.assertEqual(value, {'a': 'b'})\n-\n-    def test_valid_empty(self):\n-        field = forms.JSONField(required=False)\n-        value = field.clean('')\n-        self.assertIsNone(value)\n-\n-    def test_invalid(self):\n-        field = forms.JSONField()\n-        with self.assertRaises(exceptions.ValidationError) as cm:\n-            field.clean('{some badly formed: json}')\n-        self.assertEqual(cm.exception.messages[0], '{some badly formed: json} value must be valid JSON.')\n-\n-    def test_formfield(self):\n-        model_field = JSONField()\n-        form_field = model_field.formfield()\n-        self.assertIsInstance(form_field, forms.JSONField)\n-\n-    def test_formfield_disabled(self):\n-        class JsonForm(Form):\n-            name = CharField()\n-            jfield = forms.JSONField(disabled=True)\n-\n-        form = JsonForm({'name': 'xyz', 'jfield': '[\"bar\"]'}, initial={'jfield': ['foo']})\n-        self.assertIn('[&quot;foo&quot;]</textarea>', form.as_p())\n-\n-    def test_prepare_value(self):\n-        field = forms.JSONField()\n-        self.assertEqual(field.prepare_value({'a': 'b'}), '{\"a\": \"b\"}')\n-        self.assertEqual(field.prepare_value(None), 'null')\n-        self.assertEqual(field.prepare_value('foo'), '\"foo\"')\n-\n-    def test_redisplay_wrong_input(self):\n-        \"\"\"\n-        When displaying a bound form (typically due to invalid input), the form\n-        should not overquote JSONField inputs.\n-        \"\"\"\n-        class JsonForm(Form):\n-            name = CharField(max_length=2)\n-            jfield = forms.JSONField()\n-\n-        # JSONField input is fine, name is too long\n-        form = JsonForm({'name': 'xyz', 'jfield': '[\"foo\"]'})\n-        self.assertIn('[&quot;foo&quot;]</textarea>', form.as_p())\n-\n-        # This time, the JSONField input is wrong\n-        form = JsonForm({'name': 'xy', 'jfield': '{\"foo\"}'})\n-        # Appears once in the textarea and once in the error message\n-        self.assertEqual(form.as_p().count(escape('{\"foo\"}')), 2)\n-\n-    def test_widget(self):\n-        \"\"\"The default widget of a JSONField is a Textarea.\"\"\"\n-        field = forms.JSONField()\n-        self.assertIsInstance(field.widget, widgets.Textarea)\n-\n-    def test_custom_widget_kwarg(self):\n-        \"\"\"The widget can be overridden with a kwarg.\"\"\"\n-        field = forms.JSONField(widget=widgets.Input)\n-        self.assertIsInstance(field.widget, widgets.Input)\n-\n-    def test_custom_widget_attribute(self):\n-        \"\"\"The widget can be overridden with an attribute.\"\"\"\n-        class CustomJSONField(forms.JSONField):\n-            widget = widgets.Input\n-\n-        field = CustomJSONField()\n-        self.assertIsInstance(field.widget, widgets.Input)\n-\n-    def test_already_converted_value(self):\n-        field = forms.JSONField(required=False)\n-        tests = [\n-            '[\"a\", \"b\", \"c\"]', '{\"a\": 1, \"b\": 2}', '1', '1.5', '\"foo\"',\n-            'true', 'false', 'null',\n-        ]\n-        for json_string in tests:\n-            val = field.clean(json_string)\n-            self.assertEqual(field.clean(val), val)\n-\n-    def test_has_changed(self):\n-        field = forms.JSONField()\n-        self.assertIs(field.has_changed({'a': True}, '{\"a\": 1}'), True)\n-        self.assertIs(field.has_changed({'a': 1, 'b': 2}, '{\"b\": 2, \"a\": 1}'), False)\ndiff --git a/tests/postgres_tests/test_json_deprecation.py b/tests/postgres_tests/test_json_deprecation.py\nnew file mode 100644\nindex 000000000000..aba1828c66eb\n--- /dev/null\n+++ b/tests/postgres_tests/test_json_deprecation.py\n@@ -0,0 +1,54 @@\n+try:\n+    from django.contrib.postgres.fields import JSONField as ModelJSONField\n+    from django.contrib.postgres.fields.jsonb import KeyTransform, KeyTextTransform\n+    from django.contrib.postgres.forms import JSONField as FormJSONField\n+except ImportError:\n+    pass\n+\n+from django.utils.deprecation import RemovedInDjango40Warning\n+\n+from . import PostgreSQLSimpleTestCase\n+from .models import PostgreSQLModel\n+\n+\n+class DeprecationTests(PostgreSQLSimpleTestCase):\n+    def test_model_field_deprecation_message(self):\n+        warning = {\n+            'msg': (\n+                'django.contrib.postgres.fields.JSONField is deprecated '\n+                'and will be removed in Django 4.0.'\n+            ),\n+            'hint': 'Use django.db.models.JSONField instead.',\n+            'id': 'fields.W903',\n+        }\n+\n+        class PostgreSQLJSONModel(PostgreSQLModel):\n+            field = ModelJSONField()\n+        warnings = PostgreSQLJSONModel().check()\n+        self.assertEqual(warnings[0].msg, warning['msg'])\n+        self.assertEqual(warnings[0].hint, warning['hint'])\n+        self.assertEqual(warnings[0].id, warning['id'])\n+\n+    def test_form_field_deprecation_message(self):\n+        msg = (\n+            'django.contrib.postgres.forms.JSONField is deprecated in favor of '\n+            'django.forms.JSONField'\n+        )\n+        with self.assertWarnsMessage(RemovedInDjango40Warning, msg):\n+            FormJSONField()\n+\n+    def test_key_transform_deprecation_message(self):\n+        msg = (\n+            'django.contrib.postgres.fields.jsonb.KeyTransform is deprecated in favor of '\n+            'django.db.models.fields.json.KeyTransform'\n+        )\n+        with self.assertWarnsMessage(RemovedInDjango40Warning, msg):\n+            KeyTransform('foo', 'bar')\n+\n+    def test_key_text_transform_deprecation_message(self):\n+        msg = (\n+            'django.contrib.postgres.fields.jsonb.KeyTextTransform is deprecated in favor of '\n+            'django.db.models.fields.json.KeyTextTransform'\n+        )\n+        with self.assertWarnsMessage(RemovedInDjango40Warning, msg):\n+            KeyTextTransform('foo', 'bar')\ndiff --git a/tests/queries/models.py b/tests/queries/models.py\nindex e9eec5718dde..4ecad9ee9cc7 100644\n--- a/tests/queries/models.py\n+++ b/tests/queries/models.py\n@@ -747,3 +747,10 @@ class ReturningModel(models.Model):\n \n class NonIntegerPKReturningModel(models.Model):\n     created = CreatedField(editable=False, primary_key=True)\n+\n+\n+class JSONFieldNullable(models.Model):\n+    json_field = models.JSONField(blank=True, null=True)\n+\n+    class Meta:\n+        required_db_features = {'supports_json_field'}\ndiff --git a/tests/queries/test_bulk_update.py b/tests/queries/test_bulk_update.py\nindex e2e9a6147a4f..ec43c866912b 100644\n--- a/tests/queries/test_bulk_update.py\n+++ b/tests/queries/test_bulk_update.py\n@@ -3,11 +3,11 @@\n from django.core.exceptions import FieldDoesNotExist\n from django.db.models import F\n from django.db.models.functions import Lower\n-from django.test import TestCase\n+from django.test import TestCase, skipUnlessDBFeature\n \n from .models import (\n-    Article, CustomDbColumn, CustomPk, Detail, Individual, Member, Note,\n-    Number, Order, Paragraph, SpecialCategory, Tag, Valid,\n+    Article, CustomDbColumn, CustomPk, Detail, Individual, JSONFieldNullable,\n+    Member, Note, Number, Order, Paragraph, SpecialCategory, Tag, Valid,\n )\n \n \n@@ -228,3 +228,14 @@ def test_datetime_field(self):\n             article.created = point_in_time\n         Article.objects.bulk_update(articles, ['created'])\n         self.assertCountEqual(Article.objects.filter(created=point_in_time), articles)\n+\n+    @skipUnlessDBFeature('supports_json_field')\n+    def test_json_field(self):\n+        JSONFieldNullable.objects.bulk_create([\n+            JSONFieldNullable(json_field={'a': i}) for i in range(10)\n+        ])\n+        objs = JSONFieldNullable.objects.all()\n+        for obj in objs:\n+            obj.json_field = {'c': obj.json_field['a'] + 1}\n+        JSONFieldNullable.objects.bulk_update(objs, ['json_field'])\n+        self.assertCountEqual(JSONFieldNullable.objects.filter(json_field__has_key='c'), objs)\n"
  },
  {
    "index": 23,
    "filtered_comments": [
      "I've made error message single-line so there is no short/long description separation. See https://github.com/chrismedrela/django/commit/1929a8c3565bdd6aa36b8ce3f578f34091105d59.\n",
      "It is great to see validation.py get replaced with something far more sane - and I think the overall approach is good.\n\nIn addition to my line level comments - here are some overall thoughts:\n\nThere are a number of places where related field checks are skipped if the the value is a string - I'm assuming for lazy resolution. Shouldn't we be in a position by the time checks are done to have all related fields connected? It seems that potential problems that are checked for are now deferred to runtime leaving users with \"why did this setup pass checks and now bombs?\"\n\nI wish there was a way to test this without the brittle problem of doing essentially string comparisons on the error messages. Any typo fixes or rewording means updating the docs. Unfortunately I don't have any bright ideas. When hitting a similar situation for SuspiciousOperation the solution was to create specific subclasses - but that seems like the wrong type of fix here.\n\nThere is a bit too much opaque use of **kwargs being passed around - it is fine for this, but if the design of this feature were to be much more complex than it is, it would be an ass-biting laying in wait.\n\nAs said in a comment, I think the \"check_all_models\" adds enough enhancement and exposes enough checking API for this feature without also adding the global \"registration\" of custom check functions.\n\nThe docs will need some more polish (I'm willing to help - left no comments yet), and actual deprecations need to be started.\n\nI'm NOT NOT NOT a coverage zealot but I did run my little diff coverage tool on the branch which found the following lines that were added/changed that are not tested:\n\nhttps://gist.github.com/ptone/fa491c101de3bc4fc5c7\nhttp://ptone.com/temp/checks-coverage/ (untested changes have block red line numbers)\n\n100% coverage should not be a blind objective, but it can be helpful for you to see any major untested areas, but overall the tests looked good.\n\nThanks for the tremendous amount of work during your GSOC.\n",
      "Thanks for the review, Preston -- much appreciated to have another set of eyes on the codebase.\n\nRegarding the string resolution of related field names -- that's mostly inherited from the old codebase -- Chris hasn't introduced anything new there. You're completely correct that at the point checks are performed, all the models _should_ be resolved. I'll stand corrected on this, but as I recall, the reason the string exclusions exist is so that when a bad model has been referenced, we can catch the fact that it hasn't been resolved, report that problem, and then perform any other checks that are appropriate. However, some checks will break hard if the foreign key reference hasn't been resolved, so you need to skip over those checks.\n\nRegarding the tests checking string content -- I agree that isn't ideal. A stretch goal for this project is to enable pyflakes-style warning/error suppression -- so you'll be able to register that you don't care about E115, and have errors of that type suppressed. This will also give us a simple constant against which we can perform tests. \n\nThe *_kwargs usage is a 'room for expansion' thing, much like the use of *_kwargs on save(). The idea is that you might be able to pass in specific qualifiers or modifiers to the checks; we don't know exactly what they will be -- one use at the moment is \"the app name\", but there could be others. Requiring **kwargs in the prototype for check methods means any future flags will be silently ignored, but can be specifically catered for when appropriate.\n\nAdding custom check functions was a specific goal for the project, with security checks being the use case validating the need for the feature. \n\nCompletely agreed on docs needing polish before this is merged -- that's true of any project, however. I'll certainly remember to call on you when we get to a merge point :-) \n\nThanks for the hit list on coverage, too. My validation to date has been a line-for-line comparison with the old validation checks; that means we should be at least as covered as we were previously, but doesn't account for previously existing testing holes.\n",
      "Am I correct that this issue implies that a default value should always be set, and that this could be a check added to BooleanField?\n\nhttps://github.com/django/django/pull/1466/files\n",
      "@loic, thank you for your input. I've had a look at these wiki pages, but I think that we will stay with \"system checks\" -- I cannot see any option that is _much_ better.\n",
      "One thing I'm curious about: how should we (whether that's Django or third-parties) decide what validation should be done by this checking process, and what should be done in `__init__()`? A number of fields do checks in `__init__()` and raise exceptions there - for example, `FileField` will raise a `TypeError` from `__init__()` if you try to pass it a `unique` argument, but it will not check `upload_to` until model checking.\n\nSince `__init__()` is always run, while model checking is generally only run in development, it would seem that this distinction matters most in a production environment. Since model checking is skipped there, under the assumption that problems have already been addressed in development, perhaps the distinction should be that `__init__()` only does the validation necessary to make the code actually run, while all correctness checks are done by the checking process.\n\nAny thoughts?\n",
      "@marfire I think you've found an unusual edge case of the old validation design.\n\nThe only examples of exceptions raised in `__init__()` that I can find are:\n- AutoField (rejecting `primary_key=False`), \n- FileField (rejecting `primary_key` and `unique` arguments). \n- ForeignKey/M2M (rejecting references to abstract models, and references that aren't a model or a string)\n\nHistorically, implementing these checks in validation.py would have meant extending the 'type specific' blocks in validation.py. Although these blocks already existed, it's not an especially good design pattern (putting all your validation logic in one place), so those three cases of localized validation have slipped in. \n\nI'm fairly certain that these checks could all be converted into system checks without any real change in behavior; and given that we're now moved to a 'check behavior stored on the field' archictecture, we can avoid the bad architecture. We also get slightly improved error reporting behavior as well -- under the current setup, if you have multiple ForeignKeys pointing to an abstract model, each one would be reported as an individual exception. Using a check-based approach, you'd get a summary of _all_ the bad references at once.\n\nAs for third party fields -- historically, they haven't had a choice. They've had to use assertions in `__init__`, because they didn't have access to validation.py. This is one of the reasons behind a move to a checking framework.\n\nSo - my advice for third parties (once this all lands in trunk) would be to use checks, rather than assertions in `__init__` checks -- and, for backwards compatibility, do both :-)\n",
      "@freakboy3742 Thanks for the clarification. That's good news - it's certainly nicer to do everything in system checks than it is to split the work with `__init__()`. \n",
      "I've rebased this branch. I've also improved documentation. I've also fixed the problem of compatibility checks -- I've added new `is_overridden` method to `Settings` and `UserSettingsHolder`. @ptone, do you have time to review documentation? This is the last thing we need to do in order to merge this branch.\n"
    ],
    "code_diff": "diff --git a/django/conf/__init__.py b/django/conf/__init__.py\nindex 7a915f1486bf..5a5a40519c01 100644\n--- a/django/conf/__init__.py\n+++ b/django/conf/__init__.py\n@@ -117,6 +117,7 @@ def __setattr__(self, name, value):\n \n class Settings(BaseSettings):\n     def __init__(self, settings_module):\n+\n         # update this dict from global settings (but only for ALL_CAPS settings)\n         for setting in dir(global_settings):\n             if setting == setting.upper():\n@@ -161,6 +162,9 @@ def __init__(self, settings_module):\n             os.environ['TZ'] = self.TIME_ZONE\n             time.tzset()\n \n+    def is_overridden(self, setting):\n+        return setting in self.__dict__\n+\n \n class UserSettingsHolder(BaseSettings):\n     \"\"\"\n@@ -194,4 +198,10 @@ def __delattr__(self, name):\n     def __dir__(self):\n         return list(self.__dict__) + dir(self.default_settings)\n \n+    def is_overridden(self, setting):\n+        if setting in self._deleted:\n+            return False\n+        else:\n+            return self.default_settings.is_overridden(setting)\n+\n settings = LazySettings()\ndiff --git a/django/conf/global_settings.py b/django/conf/global_settings.py\nindex 6dd25e18f9bb..0e0bddf911d3 100644\n--- a/django/conf/global_settings.py\n+++ b/django/conf/global_settings.py\n@@ -616,3 +616,13 @@\n \n # Migration module overrides for apps, by app label.\n MIGRATION_MODULES = {}\n+\n+#################\n+# SYSTEM CHECKS #\n+#################\n+\n+# List of all issues generated by system checks that should be silenced. Light\n+# issues like warnings, infos or debugs will not generate a message. Silencing\n+# serious issues like errors and criticals does not result in hiding the\n+# message, but Django will not stop you from e.g. running server.\n+SILENCED_SYSTEM_CHECKS = []\ndiff --git a/django/contrib/admin/__init__.py b/django/contrib/admin/__init__.py\nindex cba84cf80700..6f174e2538e1 100644\n--- a/django/contrib/admin/__init__.py\n+++ b/django/contrib/admin/__init__.py\n@@ -1,15 +1,19 @@\n # ACTION_CHECKBOX_NAME is unused, but should stay since its import from here\n # has been referenced in documentation.\n+from django.contrib.admin.checks import check_admin_app\n from django.contrib.admin.decorators import register\n from django.contrib.admin.helpers import ACTION_CHECKBOX_NAME\n from django.contrib.admin.options import ModelAdmin, HORIZONTAL, VERTICAL\n from django.contrib.admin.options import StackedInline, TabularInline\n-from django.contrib.admin.sites import AdminSite, site\n from django.contrib.admin.filters import (ListFilter, SimpleListFilter,\n     FieldListFilter, BooleanFieldListFilter, RelatedFieldListFilter,\n     ChoicesFieldListFilter, DateFieldListFilter, AllValuesFieldListFilter)\n+from django.contrib.admin.sites import AdminSite, site\n+from django.core import checks\n from django.utils.module_loading import autodiscover_modules\n \n \n def autodiscover():\n     autodiscover_modules('admin', register_to=site)\n+\n+checks.register(check_admin_app)\ndiff --git a/django/contrib/admin/checks.py b/django/contrib/admin/checks.py\nnew file mode 100644\nindex 000000000000..b70192ebce0b\n--- /dev/null\n+++ b/django/contrib/admin/checks.py\n@@ -0,0 +1,912 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from itertools import chain\n+\n+from django.contrib.admin.util import get_fields_from_path, NotRelationField\n+from django.core import checks\n+from django.db import models\n+from django.db.models.fields import FieldDoesNotExist\n+from django.forms.models import BaseModelForm, _get_foreign_key, BaseModelFormSet\n+\n+\n+@checks.register()\n+def check_admin_app(**kwargs):\n+    from django.contrib.admin.sites import site\n+\n+    return list(chain(*[\n+        model_admin.check(model, **kwargs)\n+        for model, model_admin in site._registry.items()\n+    ]))\n+\n+\n+class BaseModelAdminChecks(object):\n+\n+    def __init__(self):\n+        # Before we can introspect models, they need to be fully loaded so that\n+        # inter-relations are set up correctly. We force that here.\n+        models.get_apps()\n+\n+    def check(self, cls, model, **kwargs):\n+        errors = []\n+        errors.extend(self._check_raw_id_fields(cls, model))\n+        errors.extend(self._check_fields(cls, model))\n+        errors.extend(self._check_fieldsets(cls, model))\n+        errors.extend(self._check_exclude(cls, model))\n+        errors.extend(self._check_form(cls, model))\n+        errors.extend(self._check_filter_vertical(cls, model))\n+        errors.extend(self._check_filter_horizontal(cls, model))\n+        errors.extend(self._check_radio_fields(cls, model))\n+        errors.extend(self._check_prepopulated_fields(cls, model))\n+        errors.extend(self._check_ordering(cls, model))\n+        errors.extend(self._check_readonly_fields(cls, model))\n+        return errors\n+\n+    def _check_raw_id_fields(self, cls, model):\n+        \"\"\" Check that `raw_id_fields` only contains field names that are listed\n+        on the model. \"\"\"\n+\n+        if not isinstance(cls.raw_id_fields, (list, tuple)):\n+            return must_be('a list or tuple', option='raw_id_fields', obj=cls, id='admin.E001')\n+        else:\n+            return list(chain(*[\n+                self._check_raw_id_fields_item(cls, model, field_name, 'raw_id_fields[%d]' % index)\n+                for index, field_name in enumerate(cls.raw_id_fields)\n+            ]))\n+\n+    def _check_raw_id_fields_item(self, cls, model, field_name, label):\n+        \"\"\" Check an item of `raw_id_fields`, i.e. check that field named\n+        `field_name` exists in model `model` and is a ForeignKey or a\n+        ManyToManyField. \"\"\"\n+\n+        try:\n+            field = model._meta.get_field(field_name)\n+        except models.FieldDoesNotExist:\n+            return refer_to_missing_field(field=field_name, option=label,\n+                                          model=model, obj=cls, id='admin.E002')\n+        else:\n+            if not isinstance(field, (models.ForeignKey, models.ManyToManyField)):\n+                return must_be('a ForeignKey or ManyToManyField',\n+                               option=label, obj=cls, id='admin.E003')\n+            else:\n+                return []\n+\n+    def _check_fields(self, cls, model):\n+        \"\"\" Check that `fields` only refer to existing fields, doesn't contain\n+        duplicates. Check if at most one of `fields` and `fieldsets` is defined.\n+        \"\"\"\n+\n+        if cls.fields is None:\n+            return []\n+        elif not isinstance(cls.fields, (list, tuple)):\n+            return must_be('a list or tuple', option='fields', obj=cls, id='admin.E004')\n+        elif cls.fieldsets:\n+            return [\n+                checks.Error(\n+                    'Both \"fieldsets\" and \"fields\" are specified.',\n+                    hint=None,\n+                    obj=cls,\n+                    id='admin.E005',\n+                )\n+            ]\n+        elif len(cls.fields) != len(set(cls.fields)):\n+            return [\n+                checks.Error(\n+                    'There are duplicate field(s) in \"fields\".',\n+                    hint=None,\n+                    obj=cls,\n+                    id='admin.E006',\n+                )\n+            ]\n+        else:\n+            return list(chain(*[\n+                self._check_field_spec(cls, model, field_name, 'fields')\n+                for field_name in cls.fields\n+            ]))\n+\n+    def _check_fieldsets(self, cls, model):\n+        \"\"\" Check that fieldsets is properly formatted and doesn't contain\n+        duplicates. \"\"\"\n+\n+        if cls.fieldsets is None:\n+            return []\n+        elif not isinstance(cls.fieldsets, (list, tuple)):\n+            return must_be('a list or tuple', option='fieldsets', obj=cls, id='admin.E007')\n+        else:\n+            return list(chain(*[\n+                self._check_fieldsets_item(cls, model, fieldset, 'fieldsets[%d]' % index)\n+                for index, fieldset in enumerate(cls.fieldsets)\n+            ]))\n+\n+    def _check_fieldsets_item(self, cls, model, fieldset, label):\n+        \"\"\" Check an item of `fieldsets`, i.e. check that this is a pair of a\n+        set name and a dictionary containing \"fields\" key. \"\"\"\n+\n+        if not isinstance(fieldset, (list, tuple)):\n+            return must_be('a list or tuple', option=label, obj=cls, id='admin.E008')\n+        elif len(fieldset) != 2:\n+            return must_be('a pair', option=label, obj=cls, id='admin.E009')\n+        elif not isinstance(fieldset[1], dict):\n+            return must_be('a dictionary', option='%s[1]' % label, obj=cls, id='admin.E010')\n+        elif 'fields' not in fieldset[1]:\n+            return [\n+                checks.Error(\n+                    '\"%s[1]\" must contain \"fields\" key.' % label,\n+                    hint=None,\n+                    obj=cls,\n+                    id='admin.E011',\n+                )\n+            ]\n+        elif len(fieldset[1]['fields']) != len(set(fieldset[1]['fields'])):\n+            return [\n+                checks.Error(\n+                    'There are duplicate field(s) in \"%s[1]\".' % label,\n+                    hint=None,\n+                    obj=cls,\n+                    id='admin.E012',\n+                )\n+            ]\n+        else:\n+            return list(chain(*[\n+                self._check_field_spec(cls, model, fields, '%s[1][\\'fields\\']' % label)\n+                for fields in fieldset[1]['fields']\n+            ]))\n+\n+    def _check_field_spec(self, cls, model, fields, label):\n+        \"\"\" `fields` should be an item of `fields` or an item of\n+        fieldset[1]['fields'] for any `fieldset` in `fieldsets`. It should be a\n+        field name or a tuple of field names. \"\"\"\n+\n+        if isinstance(fields, tuple):\n+            return list(chain(*[\n+                self._check_field_spec_item(cls, model, field_name, \"%s[%d]\" % (label, index))\n+                for index, field_name in enumerate(fields)\n+            ]))\n+        else:\n+            return self._check_field_spec_item(cls, model, fields, label)\n+\n+    def _check_field_spec_item(self, cls, model, field_name, label):\n+        if field_name in cls.readonly_fields:\n+            # Stuff can be put in fields that isn't actually a model field if\n+            # it's in readonly_fields, readonly_fields will handle the\n+            # validation of such things.\n+            return []\n+        else:\n+            try:\n+                field = model._meta.get_field(field_name)\n+            except models.FieldDoesNotExist:\n+                # If we can't find a field on the model that matches, it could\n+                # be an extra field on the form.\n+                return []\n+            else:\n+                if (isinstance(field, models.ManyToManyField) and\n+                        not field.rel.through._meta.auto_created):\n+                    return [\n+                        checks.Error(\n+                            '\"%s\" cannot include the ManyToManyField \"%s\", '\n+                                'because \"%s\" manually specifies relationship model.'\n+                                % (label, field_name, field_name),\n+                            hint=None,\n+                            obj=cls,\n+                            id='admin.E013',\n+                        )\n+                    ]\n+                else:\n+                    return []\n+\n+    def _check_exclude(self, cls, model):\n+        \"\"\" Check that exclude is a sequence without duplicates. \"\"\"\n+\n+        if cls.exclude is None:  # default value is None\n+            return []\n+        elif not isinstance(cls.exclude, (list, tuple)):\n+            return must_be('a list or tuple', option='exclude', obj=cls, id='admin.E014')\n+        elif len(cls.exclude) > len(set(cls.exclude)):\n+            return [\n+                checks.Error(\n+                    '\"exclude\" contains duplicate field(s).',\n+                    hint=None,\n+                    obj=cls,\n+                    id='admin.E015',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_form(self, cls, model):\n+        \"\"\" Check that form subclasses BaseModelForm. \"\"\"\n+\n+        if hasattr(cls, 'form') and not issubclass(cls.form, BaseModelForm):\n+            return must_inherit_from(parent='BaseModelForm', option='form',\n+                                     obj=cls, id='admin.E016')\n+        else:\n+            return []\n+\n+    def _check_filter_vertical(self, cls, model):\n+        \"\"\" Check that filter_vertical is a sequence of field names. \"\"\"\n+\n+        if not hasattr(cls, 'filter_vertical'):\n+            return []\n+        elif not isinstance(cls.filter_vertical, (list, tuple)):\n+            return must_be('a list or tuple', option='filter_vertical', obj=cls, id='admin.E017')\n+        else:\n+            return list(chain(*[\n+                self._check_filter_item(cls, model, field_name, \"filter_vertical[%d]\" % index)\n+                for index, field_name in enumerate(cls.filter_vertical)\n+            ]))\n+\n+    def _check_filter_horizontal(self, cls, model):\n+        \"\"\" Check that filter_horizontal is a sequence of field names. \"\"\"\n+\n+        if not hasattr(cls, 'filter_horizontal'):\n+            return []\n+        elif not isinstance(cls.filter_horizontal, (list, tuple)):\n+            return must_be('a list or tuple', option='filter_horizontal', obj=cls, id='admin.E018')\n+        else:\n+            return list(chain(*[\n+                self._check_filter_item(cls, model, field_name, \"filter_horizontal[%d]\" % index)\n+                for index, field_name in enumerate(cls.filter_horizontal)\n+            ]))\n+\n+    def _check_filter_item(self, cls, model, field_name, label):\n+        \"\"\" Check one item of `filter_vertical` or `filter_horizontal`, i.e.\n+        check that given field exists and is a ManyToManyField. \"\"\"\n+\n+        try:\n+            field = model._meta.get_field(field_name)\n+        except models.FieldDoesNotExist:\n+            return refer_to_missing_field(field=field_name, option=label,\n+                                          model=model, obj=cls, id='admin.E019')\n+        else:\n+            if not isinstance(field, models.ManyToManyField):\n+                return must_be('a ManyToManyField', option=label, obj=cls, id='admin.E020')\n+            else:\n+                return []\n+\n+    def _check_radio_fields(self, cls, model):\n+        \"\"\" Check that `radio_fields` is a dictionary. \"\"\"\n+\n+        if not hasattr(cls, 'radio_fields'):\n+            return []\n+        elif not isinstance(cls.radio_fields, dict):\n+            return must_be('a dictionary', option='radio_fields', obj=cls, id='admin.E021')\n+        else:\n+            return list(chain(*[\n+                self._check_radio_fields_key(cls, model, field_name, 'radio_fields') +\n+                self._check_radio_fields_value(cls, model, val, 'radio_fields[\\'%s\\']' % field_name)\n+                for field_name, val in cls.radio_fields.items()\n+            ]))\n+\n+    def _check_radio_fields_key(self, cls, model, field_name, label):\n+        \"\"\" Check that a key of `radio_fields` dictionary is name of existing\n+        field and that the field is a ForeignKey or has `choices` defined. \"\"\"\n+\n+        try:\n+            field = model._meta.get_field(field_name)\n+        except models.FieldDoesNotExist:\n+            return refer_to_missing_field(field=field_name, option=label,\n+                                          model=model, obj=cls, id='admin.E022')\n+        else:\n+            if not (isinstance(field, models.ForeignKey) or field.choices):\n+                return [\n+                    checks.Error(\n+                        '\"%s\" refers to \"%s\", which is neither an instance of ForeignKey nor does have choices set.'\n+                            % (label, field_name),\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E023',\n+                    )\n+                ]\n+            else:\n+                return []\n+\n+    def _check_radio_fields_value(self, cls, model, val, label):\n+        \"\"\" Check type of a value of `radio_fields` dictionary. \"\"\"\n+\n+        from django.contrib.admin.options import HORIZONTAL, VERTICAL\n+\n+        if val not in (HORIZONTAL, VERTICAL):\n+            return [\n+                checks.Error(\n+                    '\"%s\" is neither admin.HORIZONTAL nor admin.VERTICAL.' % label,\n+                    hint=None,\n+                    obj=cls,\n+                    id='admin.E024',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_prepopulated_fields(self, cls, model):\n+        \"\"\" Check that `prepopulated_fields` is a dictionary containing allowed\n+        field types. \"\"\"\n+\n+        if not hasattr(cls, 'prepopulated_fields'):\n+            return []\n+        elif not isinstance(cls.prepopulated_fields, dict):\n+            return must_be('a dictionary', option='prepopulated_fields', obj=cls, id='admin.E025')\n+        else:\n+            return list(chain(*[\n+                self._check_prepopulated_fields_key(cls, model, field_name, 'prepopulated_fields') +\n+                self._check_prepopulated_fields_value(cls, model, val, 'prepopulated_fields[\\'%s\\']' % field_name)\n+                for field_name, val in cls.prepopulated_fields.items()\n+            ]))\n+\n+    def _check_prepopulated_fields_key(self, cls, model, field_name, label):\n+        \"\"\" Check a key of `prepopulated_fields` dictionary, i.e. check that it\n+        is a name of existing field and the field is one of the allowed types.\n+        \"\"\"\n+\n+        forbidden_field_types = (\n+            models.DateTimeField,\n+            models.ForeignKey,\n+            models.ManyToManyField\n+        )\n+\n+        try:\n+            field = model._meta.get_field(field_name)\n+        except models.FieldDoesNotExist:\n+            return refer_to_missing_field(field=field_name, option=label,\n+                                          model=model, obj=cls, id='admin.E026')\n+        else:\n+            if isinstance(field, forbidden_field_types):\n+                return [\n+                    checks.Error(\n+                        '\"%s\" refers to \"%s\", which must not be a DateTimeField, '\n+                            'ForeignKey or ManyToManyField.'\n+                            % (label, field_name),\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E027',\n+                    )\n+                ]\n+            else:\n+                return []\n+\n+    def _check_prepopulated_fields_value(self, cls, model, val, label):\n+        \"\"\" Check a value of `prepopulated_fields` dictionary, i.e. it's an\n+        iterable of existing fields. \"\"\"\n+\n+        if not isinstance(val, (list, tuple)):\n+            return must_be('a list or tuple', option=label, obj=cls, id='admin.E028')\n+        else:\n+            return list(chain(*[\n+                self._check_prepopulated_fields_value_item(cls, model, subfield_name, \"%s[%r]\" % (label, index))\n+                for index, subfield_name in enumerate(val)\n+            ]))\n+\n+    def _check_prepopulated_fields_value_item(self, cls, model, field_name, label):\n+        \"\"\" For `prepopulated_fields` equal to {\"slug\": (\"title\",)},\n+        `field_name` is \"title\". \"\"\"\n+\n+        try:\n+            model._meta.get_field(field_name)\n+        except models.FieldDoesNotExist:\n+            return refer_to_missing_field(field=field_name, option=label,\n+                                          model=model, obj=cls, id='admin.E029')\n+        else:\n+            return []\n+\n+    def _check_ordering(self, cls, model):\n+        \"\"\" Check that ordering refers to existing fields or is random. \"\"\"\n+\n+        # ordering = None\n+        if cls.ordering is None:  # The default value is None\n+            return []\n+        elif not isinstance(cls.ordering, (list, tuple)):\n+            return must_be('a list or tuple', option='ordering', obj=cls, id='admin.E030')\n+        else:\n+            return list(chain(*[\n+                self._check_ordering_item(cls, model, field_name, 'ordering[%d]' % index)\n+                for index, field_name in enumerate(cls.ordering)\n+            ]))\n+\n+    def _check_ordering_item(self, cls, model, field_name, label):\n+        \"\"\" Check that `ordering` refers to existing fields. \"\"\"\n+\n+        if field_name == '?' and len(cls.ordering) != 1:\n+            return [\n+                checks.Error(\n+                    '\"ordering\" has the random ordering marker \"?\", '\n+                        'but contains other fields as well.',\n+                    hint='Either remove the \"?\", or remove the other fields.',\n+                    obj=cls,\n+                    id='admin.E031',\n+                )\n+            ]\n+        elif field_name == '?':\n+            return []\n+        elif '__' in field_name:\n+            # Skip ordering in the format field1__field2 (FIXME: checking\n+            # this format would be nice, but it's a little fiddly).\n+            return []\n+        else:\n+            if field_name.startswith('-'):\n+                field_name = field_name[1:]\n+\n+            try:\n+                model._meta.get_field(field_name)\n+            except models.FieldDoesNotExist:\n+                return refer_to_missing_field(field=field_name, option=label,\n+                                              model=model, obj=cls, id='admin.E032')\n+            else:\n+                return []\n+\n+    def _check_readonly_fields(self, cls, model):\n+        \"\"\" Check that readonly_fields refers to proper attribute or field. \"\"\"\n+\n+        if cls.readonly_fields == ():\n+            return []\n+        elif not isinstance(cls.readonly_fields, (list, tuple)):\n+            return must_be('a list or tuple', option='readonly_fields', obj=cls, id='admin.E033')\n+        else:\n+            return list(chain(*[\n+                self._check_readonly_fields_item(cls, model, field_name, \"readonly_fields[%d]\" % index)\n+                for index, field_name in enumerate(cls.readonly_fields)\n+            ]))\n+\n+    def _check_readonly_fields_item(self, cls, model, field_name, label):\n+        if callable(field_name):\n+            return []\n+        elif hasattr(cls, field_name):\n+            return []\n+        elif hasattr(model, field_name):\n+            return []\n+        else:\n+            try:\n+                model._meta.get_field(field_name)\n+            except models.FieldDoesNotExist:\n+                return [\n+                    checks.Error(\n+                        '\"%s\" is neither a callable nor an attribute of \"%s\" nor found in the model %s.%s.'\n+                            % (label, cls.__name__, model._meta.app_label, model._meta.object_name),\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E034',\n+                    )\n+                ]\n+            else:\n+                return []\n+\n+\n+class ModelAdminChecks(BaseModelAdminChecks):\n+\n+    def check(self, cls, model, **kwargs):\n+        errors = super(ModelAdminChecks, self).check(cls, model=model, **kwargs)\n+        errors.extend(self._check_save_as(cls, model))\n+        errors.extend(self._check_save_on_top(cls, model))\n+        errors.extend(self._check_inlines(cls, model))\n+        errors.extend(self._check_list_display(cls, model))\n+        errors.extend(self._check_list_display_links(cls, model))\n+        errors.extend(self._check_list_filter(cls, model))\n+        errors.extend(self._check_list_select_related(cls, model))\n+        errors.extend(self._check_list_per_page(cls, model))\n+        errors.extend(self._check_list_max_show_all(cls, model))\n+        errors.extend(self._check_list_editable(cls, model))\n+        errors.extend(self._check_search_fields(cls, model))\n+        errors.extend(self._check_date_hierarchy(cls, model))\n+        return errors\n+\n+    def _check_save_as(self, cls, model):\n+        \"\"\" Check save_as is a boolean. \"\"\"\n+\n+        if not isinstance(cls.save_as, bool):\n+            return must_be('a boolean', option='save_as',\n+                           obj=cls, id='admin.E101')\n+        else:\n+            return []\n+\n+    def _check_save_on_top(self, cls, model):\n+        \"\"\" Check save_on_top is a boolean. \"\"\"\n+\n+        if not isinstance(cls.save_on_top, bool):\n+            return must_be('a boolean', option='save_on_top',\n+                           obj=cls, id='admin.E102')\n+        else:\n+            return []\n+\n+    def _check_inlines(self, cls, model):\n+        \"\"\" Check all inline model admin classes. \"\"\"\n+\n+        if not isinstance(cls.inlines, (list, tuple)):\n+            return must_be('a list or tuple', option='inlines', obj=cls, id='admin.E103')\n+        else:\n+            return list(chain(*[\n+                self._check_inlines_item(cls, model, item, \"inlines[%d]\" % index)\n+                for index, item in enumerate(cls.inlines)\n+            ]))\n+\n+    def _check_inlines_item(self, cls, model, inline, label):\n+        \"\"\" Check one inline model admin. \"\"\"\n+\n+        from django.contrib.admin.options import BaseModelAdmin\n+\n+        if not issubclass(inline, BaseModelAdmin):\n+            return must_inherit_from(parent='BaseModelAdmin', option=label,\n+                                     obj=cls, id='admin.E104')\n+        elif not inline.model:\n+            return [\n+                checks.Error(\n+                    '\"model\" is a required attribute of \"%s\".' % label,\n+                    hint=None,\n+                    obj=cls,\n+                    id='admin.E105',\n+                )\n+            ]\n+        elif not issubclass(inline.model, models.Model):\n+            return must_be('a Model', option='%s.model' % label,\n+                           obj=cls, id='admin.E106')\n+        else:\n+            return inline.check(model)\n+\n+    def _check_list_display(self, cls, model):\n+        \"\"\" Check that list_display only contains fields or usable attributes.\n+        \"\"\"\n+\n+        if not isinstance(cls.list_display, (list, tuple)):\n+            return must_be('a list or tuple', option='list_display', obj=cls, id='admin.E107')\n+        else:\n+            return list(chain(*[\n+                self._check_list_display_item(cls, model, item, \"list_display[%d]\" % index)\n+                for index, item in enumerate(cls.list_display)\n+            ]))\n+\n+    def _check_list_display_item(self, cls, model, item, label):\n+        if callable(item):\n+            return []\n+        elif hasattr(cls, item):\n+            return []\n+        elif hasattr(model, item):\n+            # getattr(model, item) could be an X_RelatedObjectsDescriptor\n+            try:\n+                field = model._meta.get_field(item)\n+            except models.FieldDoesNotExist:\n+                try:\n+                    field = getattr(model, item)\n+                except AttributeError:\n+                    field = None\n+\n+            if field is None:\n+                return [\n+                    checks.Error(\n+                        '\"%s\" refers to \"%s\" that is neither a field, method nor a property of model %s.%s.'\n+                            % label, item, model._meta.app_label, model._meta.object_name,\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E108',\n+                    )\n+                ]\n+            elif isinstance(field, models.ManyToManyField):\n+                return [\n+                    checks.Error(\n+                        '\"%s\" must not be a ManyToManyField.' % label,\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E109',\n+                    )\n+                ]\n+            else:\n+                return []\n+        else:\n+            try:\n+                model._meta.get_field(item)\n+            except models.FieldDoesNotExist:\n+                return [\n+                    checks.Error(\n+                        '\"%s\" is neither a callable nor an attribute of \"%s\" nor found in model %s.%s.'\n+                            % (label, cls.__name__, model._meta.app_label, model._meta.object_name),\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E110',\n+                    )\n+                ]\n+            else:\n+                return []\n+\n+    def _check_list_display_links(self, cls, model):\n+        \"\"\" Check that list_display_links is a unique subset of list_display.\n+        \"\"\"\n+\n+        if cls.list_display_links is None:\n+            return []\n+        elif not isinstance(cls.list_display_links, (list, tuple)):\n+            return must_be('a list or tuple or None', option='list_display_links', obj=cls, id='admin.E111')\n+        else:\n+            return list(chain(*[\n+                self._check_list_display_links_item(cls, model, field_name, \"list_display_links[%d]\" % index)\n+                for index, field_name in enumerate(cls.list_display_links)\n+            ]))\n+\n+    def _check_list_display_links_item(self, cls, model, field_name, label):\n+        if field_name not in cls.list_display:\n+            return [\n+                checks.Error(\n+                    '\"%s\" refers to \"%s\", which is not defined in \"list_display\".'\n+                        % (label, field_name),\n+                    hint=None,\n+                    obj=cls,\n+                    id='admin.E112',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_list_filter(self, cls, model):\n+        if not isinstance(cls.list_filter, (list, tuple)):\n+            return must_be('a list or tuple', option='list_filter', obj=cls, id='admin.E113')\n+        else:\n+            return list(chain(*[\n+                self._check_list_filter_item(cls, model, item, \"list_filter[%d]\" % index)\n+                for index, item in enumerate(cls.list_filter)\n+            ]))\n+\n+    def _check_list_filter_item(self, cls, model, item, label):\n+        \"\"\"\n+        Check one item of `list_filter`, i.e. check if it is one of three options:\n+        1. 'field' -- a basic field filter, possibly w/ relationships (e.g.\n+           'field__rel')\n+        2. ('field', SomeFieldListFilter) - a field-based list filter class\n+        3. SomeListFilter - a non-field list filter class\n+        \"\"\"\n+\n+        from django.contrib.admin import ListFilter, FieldListFilter\n+\n+        if callable(item) and not isinstance(item, models.Field):\n+            # If item is option 3, it should be a ListFilter...\n+            if not issubclass(item, ListFilter):\n+                return must_inherit_from(parent='ListFilter', option=label,\n+                                         obj=cls, id='admin.E114')\n+            # ...  but not a FieldListFilter.\n+            elif issubclass(item, FieldListFilter):\n+                return [\n+                    checks.Error(\n+                        '\"%s\" must not inherit from FieldListFilter.' % label,\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E115',\n+                    )\n+                ]\n+            else:\n+                return []\n+        elif isinstance(item, (tuple, list)):\n+            # item is option #2\n+            field, list_filter_class = item\n+            if not issubclass(list_filter_class, FieldListFilter):\n+                return must_inherit_from(parent='FieldListFilter', option='%s[1]' % label,\n+                                         obj=cls, id='admin.E116')\n+            else:\n+                return []\n+        else:\n+            # item is option #1\n+            field = item\n+\n+            # Validate the field string\n+            try:\n+                get_fields_from_path(model, field)\n+            except (NotRelationField, FieldDoesNotExist):\n+                return [\n+                    checks.Error(\n+                        '\"%s\" refers to \"%s\", which does not refer to a Field.' % (label, field),\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E117',\n+                    )\n+                ]\n+            else:\n+                return []\n+\n+    def _check_list_select_related(self, cls, model):\n+        \"\"\" Check that list_select_related is a boolean, a list or a tuple. \"\"\"\n+\n+        if not isinstance(cls.list_select_related, (bool, list, tuple)):\n+            return must_be('a boolean, tuple or list', option='list_select_related',\n+                           obj=cls, id='admin.E118')\n+        else:\n+            return []\n+\n+    def _check_list_per_page(self, cls, model):\n+        \"\"\" Check that list_per_page is an integer. \"\"\"\n+\n+        if not isinstance(cls.list_per_page, int):\n+            return must_be('an integer', option='list_per_page', obj=cls, id='admin.E119')\n+        else:\n+            return []\n+\n+    def _check_list_max_show_all(self, cls, model):\n+        \"\"\" Check that list_max_show_all is an integer. \"\"\"\n+\n+        if not isinstance(cls.list_max_show_all, int):\n+            return must_be('an integer', option='list_max_show_all', obj=cls, id='admin.E120')\n+        else:\n+            return []\n+\n+    def _check_list_editable(self, cls, model):\n+        \"\"\" Check that list_editable is a sequence of editable fields from\n+        list_display without first element. \"\"\"\n+\n+        if not isinstance(cls.list_editable, (list, tuple)):\n+            return must_be('a list or tuple', option='list_editable', obj=cls, id='admin.E121')\n+        else:\n+            return list(chain(*[\n+                self._check_list_editable_item(cls, model, item, \"list_editable[%d]\" % index)\n+                for index, item in enumerate(cls.list_editable)\n+            ]))\n+\n+    def _check_list_editable_item(self, cls, model, field_name, label):\n+        try:\n+            field = model._meta.get_field_by_name(field_name)[0]\n+        except models.FieldDoesNotExist:\n+            return refer_to_missing_field(field=field_name, option=label,\n+                                          model=model, obj=cls, id='admin.E122')\n+        else:\n+            if field_name not in cls.list_display:\n+                return refer_to_missing_field(field=field_name, option=label,\n+                                              model=model, obj=cls, id='admin.E123')\n+            elif field_name in cls.list_display_links:\n+                return [\n+                    checks.Error(\n+                        '\"%s\" cannot be in both \"list_editable\" and \"list_display_links\".'\n+                            % field_name,\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E124',\n+                    )\n+                ]\n+            elif not cls.list_display_links and cls.list_display[0] in cls.list_editable:\n+                return [\n+                    checks.Error(\n+                        '\"%s\" refers to the first field in list_display (\"%s\"), '\n+                            'which cannot be used unless list_display_links is set.'\n+                            % (label, cls.list_display[0]),\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E125',\n+                    )\n+                ]\n+            elif not field.editable:\n+                return [\n+                    checks.Error(\n+                        '\"%s\" refers to field \"%s\", whih is not editable through the admin.'\n+                            % (label, field_name),\n+                        hint=None,\n+                        obj=cls,\n+                        id='admin.E126',\n+                    )\n+                ]\n+\n+    def _check_search_fields(self, cls, model):\n+        \"\"\" Check search_fields is a sequence. \"\"\"\n+\n+        if not isinstance(cls.search_fields, (list, tuple)):\n+            return must_be('a list or tuple', option='search_fields', obj=cls, id='admin.E127')\n+        else:\n+            return []\n+\n+    def _check_date_hierarchy(self, cls, model):\n+        \"\"\" Check that date_hierarchy refers to DateField or DateTimeField. \"\"\"\n+\n+        if cls.date_hierarchy is None:\n+            return []\n+        else:\n+            try:\n+                field = model._meta.get_field(cls.date_hierarchy)\n+            except models.FieldDoesNotExist:\n+                return refer_to_missing_field(option='date_hierarchy',\n+                                              field=cls.date_hierarchy,\n+                                              model=model, obj=cls, id='admin.E128')\n+            else:\n+                if not isinstance(field, (models.DateField, models.DateTimeField)):\n+                    return must_be('a DateField or DateTimeField', option='date_hierarchy',\n+                                   obj=cls, id='admin.E129')\n+                else:\n+                    return []\n+\n+\n+class InlineModelAdminChecks(BaseModelAdminChecks):\n+\n+    def check(self, cls, parent_model, **kwargs):\n+        errors = super(InlineModelAdminChecks, self).check(cls, model=cls.model, **kwargs)\n+        errors.extend(self._check_fk_name(cls, parent_model))\n+        errors.extend(self._check_exclude_of_parent_model(cls, parent_model))\n+        errors.extend(self._check_extra(cls))\n+        errors.extend(self._check_max_num(cls))\n+        errors.extend(self._check_formset(cls))\n+        return errors\n+\n+    def _check_exclude_of_parent_model(self, cls, parent_model):\n+        # Do not perform more specific checks if the base checks result in an\n+        # error.\n+        errors = super(InlineModelAdminChecks, self)._check_exclude(cls, parent_model)\n+        if errors:\n+            return []\n+\n+        # Skip if `fk_name` is invalid.\n+        if self._check_fk_name(cls, parent_model):\n+            return []\n+\n+        if cls.exclude is None:\n+            return []\n+\n+        fk = _get_foreign_key(parent_model, cls.model, fk_name=cls.fk_name)\n+        if fk.name in cls.exclude:\n+            return [\n+                checks.Error(\n+                    'Cannot exclude the field \"%s\", because it is the foreign key '\n+                        'to the parent model %s.%s.'\n+                        % (fk.name, parent_model._meta.app_label, parent_model._meta.object_name),\n+                    hint=None,\n+                    obj=cls,\n+                    id='admin.E201',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_fk_name(self, cls, parent_model):\n+        try:\n+            _get_foreign_key(parent_model, cls.model, fk_name=cls.fk_name)\n+        except ValueError as e:\n+            return [checks.Error(e.args[0], hint=None, obj=cls, id='admin.E202')]\n+        else:\n+            return []\n+\n+    def _check_extra(self, cls):\n+        \"\"\" Check that extra is an integer. \"\"\"\n+\n+        if not isinstance(cls.extra, int):\n+            return must_be('an integer', option='extra', obj=cls, id='admin.E203')\n+        else:\n+            return []\n+\n+    def _check_max_num(self, cls):\n+        \"\"\" Check that max_num is an integer. \"\"\"\n+\n+        if cls.max_num is None:\n+            return []\n+        elif not isinstance(cls.max_num, int):\n+            return must_be('an integer', option='max_num', obj=cls, id='admin.E204')\n+        else:\n+            return []\n+\n+    def _check_formset(self, cls):\n+        \"\"\" Check formset is a subclass of BaseModelFormSet. \"\"\"\n+\n+        if not issubclass(cls.formset, BaseModelFormSet):\n+            return must_inherit_from(parent='BaseModelFormSet', option='formset',\n+                                     obj=cls, id='admin.E205')\n+        else:\n+            return []\n+\n+\n+def must_be(type, option, obj, id):\n+    return [\n+        checks.Error(\n+            '\"%s\" must be %s.' % (option, type),\n+            hint=None,\n+            obj=obj,\n+            id=id,\n+        ),\n+    ]\n+\n+\n+def must_inherit_from(parent, option, obj, id):\n+    return [\n+        checks.Error(\n+            '\"%s\" must inherit from %s.' % (option, parent),\n+            hint=None,\n+            obj=obj,\n+            id=id,\n+        ),\n+    ]\n+\n+\n+def refer_to_missing_field(field, option, model, obj, id):\n+    return [\n+        checks.Error(\n+            '\"%s\" refers to field \"%s\", which is missing from model %s.%s.'\n+                % (option, field, model._meta.app_label, model._meta.object_name),\n+            hint=None,\n+            obj=obj,\n+            id=id,\n+        ),\n+    ]\ndiff --git a/django/contrib/admin/options.py b/django/contrib/admin/options.py\nindex 3b02ac020c9c..428a00f2c0ad 100644\n--- a/django/contrib/admin/options.py\n+++ b/django/contrib/admin/options.py\n@@ -8,14 +8,17 @@\n from django.conf import settings\n from django.contrib import messages\n from django.contrib.admin import widgets, helpers\n-from django.contrib.admin.utils import (unquote, flatten_fieldsets, get_deleted_objects,\n-    model_format_dict, NestedObjects, lookup_needs_distinct)\n-from django.contrib.admin import validation\n+from django.contrib.admin.checks import (BaseModelAdminChecks, ModelAdminChecks,\n+    InlineModelAdminChecks)\n+from django.contrib.admin.utils import (unquote, flatten_fieldsets,\n+    get_deleted_objects, model_format_dict, NestedObjects,\n+    lookup_needs_distinct)\n from django.contrib.admin.templatetags.admin_static import static\n from django.contrib.admin.templatetags.admin_urls import add_preserved_filters\n from django.contrib.auth import get_permission_codename\n from django.contrib.contenttypes.models import ContentType\n-from django.core.exceptions import PermissionDenied, ValidationError, FieldError\n+from django.core import checks\n+from django.core.exceptions import PermissionDenied, ValidationError, FieldError, ImproperlyConfigured\n from django.core.paginator import Paginator\n from django.core.urlresolvers import reverse\n from django.db import models, transaction, router\n@@ -30,16 +33,17 @@\n from django.http.response import HttpResponseBase\n from django.shortcuts import get_object_or_404\n from django.template.response import SimpleTemplateResponse, TemplateResponse\n-from django.utils.decorators import method_decorator\n-from django.utils.html import escape, escapejs\n-from django.utils.safestring import mark_safe\n from django.utils import six\n+from django.utils.decorators import method_decorator\n from django.utils.deprecation import RenameMethodsBase\n+from django.utils.encoding import force_text\n+from django.utils.encoding import python_2_unicode_compatible\n+from django.utils.html import escape, escapejs\n from django.utils.http import urlencode\n from django.utils.text import capfirst, get_text_list\n from django.utils.translation import ugettext as _\n from django.utils.translation import ungettext\n-from django.utils.encoding import force_text\n+from django.utils.safestring import mark_safe\n from django.views.decorators.csrf import csrf_protect\n \n \n@@ -77,13 +81,18 @@ class IncorrectLookupParameters(Exception):\n csrf_protect_m = method_decorator(csrf_protect)\n \n \n+@python_2_unicode_compatible\n class RenameBaseModelAdminMethods(forms.MediaDefiningClass, RenameMethodsBase):\n     renamed_methods = (\n         ('queryset', 'get_queryset', DeprecationWarning),\n     )\n \n+    def __str__(cls):\n+        return '%s.%s' % (cls.__module__, cls.__name__)\n+\n \n-class BaseModelAdmin(six.with_metaclass(RenameBaseModelAdminMethods)):\n+class BaseModelAdmin(six.with_metaclass(RenameBaseModelAdminMethods),\n+                     BaseModelAdminChecks):\n     \"\"\"Functionality common to both ModelAdmin and InlineAdmin.\"\"\"\n \n     raw_id_fields = ()\n@@ -99,13 +108,7 @@ class BaseModelAdmin(six.with_metaclass(RenameBaseModelAdminMethods)):\n     readonly_fields = ()\n     ordering = None\n \n-    # validation\n-    validator_class = validation.BaseValidator\n-\n-    @classmethod\n-    def validate(cls, model):\n-        validator = cls.validator_class()\n-        validator.validate(cls, model)\n+    checks = BaseModelAdminChecks\n \n     def __init__(self):\n         self._orig_formfield_overrides = self.formfield_overrides\n@@ -416,7 +419,30 @@ def has_delete_permission(self, request, obj=None):\n         codename = get_permission_codename('delete', opts)\n         return request.user.has_perm(\"%s.%s\" % (opts.app_label, codename))\n \n+    @classmethod\n+    def check(cls, model, **kwargs):\n+        if hasattr(cls, 'validator'):\n+            warnings.warn(\n+                'ModelAdmin.validator is deprecated. Use \"checks\" instead.',\n+                PendingDeprecationWarning)\n+            validator = cls.validator()\n+            try:\n+                validator.validate(cls, model)\n+            except ImproperlyConfigured as e:\n+                return [\n+                    checks.Error(\n+                        e.args[0],\n+                        hint=None,\n+                        obj=cls,\n+                    )\n+                ]\n+            else:\n+                return []\n+        else:\n+            return cls.checks().check(cls, model, **kwargs)\n \n+\n+@python_2_unicode_compatible\n class ModelAdmin(BaseModelAdmin):\n     \"Encapsulates all admin options and functionality for a given model.\"\n \n@@ -450,8 +476,7 @@ class ModelAdmin(BaseModelAdmin):\n     actions_on_bottom = False\n     actions_selection_counter = True\n \n-    # validation\n-    validator_class = validation.ModelAdminValidator\n+    checks = ModelAdminChecks\n \n     def __init__(self, model, admin_site):\n         self.model = model\n@@ -1643,7 +1668,11 @@ def _create_formsets(self, request, obj):\n             inline_instances.append(inline)\n         return formsets, inline_instances\n \n+    def __str__(self):\n+        return \"%s.%s\" % (self.model._meta.app_label, self.__class__.__name__)\n+\n \n+@python_2_unicode_compatible\n class InlineModelAdmin(BaseModelAdmin):\n     \"\"\"\n     Options for inline editing of ``model`` instances.\n@@ -1662,8 +1691,7 @@ class InlineModelAdmin(BaseModelAdmin):\n     verbose_name_plural = None\n     can_delete = True\n \n-    # validation\n-    validator_class = validation.InlineValidator\n+    checks = InlineModelAdminChecks\n \n     def __init__(self, parent_model, admin_site):\n         self.admin_site = admin_site\ndiff --git a/django/contrib/admin/sites.py b/django/contrib/admin/sites.py\nindex 2dac947fbce2..3f213a79e873 100644\n--- a/django/contrib/admin/sites.py\n+++ b/django/contrib/admin/sites.py\n@@ -98,9 +98,6 @@ def register(self, model_or_iterable, admin_class=None, **options):\n                     options['__module__'] = __name__\n                     admin_class = type(\"%sAdmin\" % model.__name__, (admin_class,), options)\n \n-                if admin_class is not ModelAdmin and settings.DEBUG:\n-                    admin_class.validate(model)\n-\n                 # Instantiate the admin class to save in the registry\n                 self._registry[model] = admin_class(model, self)\n \ndiff --git a/django/contrib/admin/validation.py b/django/contrib/admin/validation.py\nindex e4a12211b52b..0d758fac4202 100644\n--- a/django/contrib/admin/validation.py\n+++ b/django/contrib/admin/validation.py\n@@ -1,8 +1,11 @@\n+# -*- encoding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n from django.core.exceptions import ImproperlyConfigured\n from django.db import models\n from django.db.models.fields import FieldDoesNotExist\n from django.forms.models import BaseModelForm, BaseModelFormSet, _get_foreign_key\n-from django.contrib.admin.utils import get_fields_from_path, NotRelationField\n+from django.contrib.admin.util import get_fields_from_path, NotRelationField\n \n \"\"\"\n Does basic ModelAdmin option validation. Calls custom validation\n@@ -257,10 +260,8 @@ def validate_list_display(self, cls, model):\n                                     % (cls.__name__, idx, field))\n \n     def validate_list_display_links(self, cls, model):\n-        \" Validate that list_display_links either is None or a unique subset of list_display.\"\n+        \" Validate that list_display_links is a unique subset of list_display. \"\n         if hasattr(cls, 'list_display_links'):\n-            if cls.list_display_links is None:\n-                return\n             check_isseq(cls, 'list_display_links', cls.list_display_links)\n             for idx, field in enumerate(cls.list_display_links):\n                 if field not in cls.list_display:\n@@ -346,16 +347,15 @@ def validate_list_editable(self, cls, model):\n                     raise ImproperlyConfigured(\"'%s.list_editable[%d]' refers to \"\n                         \"'%s' which is not defined in 'list_display'.\"\n                         % (cls.__name__, idx, field_name))\n-                if cls.list_display_links is not None:\n-                    if field_name in cls.list_display_links:\n-                        raise ImproperlyConfigured(\"'%s' cannot be in both '%s.list_editable'\"\n-                            \" and '%s.list_display_links'\"\n-                            % (field_name, cls.__name__, cls.__name__))\n-                    if not cls.list_display_links and cls.list_display[0] in cls.list_editable:\n-                        raise ImproperlyConfigured(\"'%s.list_editable[%d]' refers to\"\n-                            \" the first field in list_display, '%s', which can't be\"\n-                            \" used unless list_display_links is set.\"\n-                            % (cls.__name__, idx, cls.list_display[0]))\n+                if field_name in cls.list_display_links:\n+                    raise ImproperlyConfigured(\"'%s' cannot be in both '%s.list_editable'\"\n+                        \" and '%s.list_display_links'\"\n+                        % (field_name, cls.__name__, cls.__name__))\n+                if not cls.list_display_links and cls.list_display[0] in cls.list_editable:\n+                    raise ImproperlyConfigured(\"'%s.list_editable[%d]' refers to\"\n+                        \" the first field in list_display, '%s', which can't be\"\n+                        \" used unless list_display_links is set.\"\n+                        % (cls.__name__, idx, cls.list_display[0]))\n                 if not field.editable:\n                     raise ImproperlyConfigured(\"'%s.list_editable[%d]' refers to a \"\n                         \"field, '%s', which isn't editable through the admin.\"\n@@ -405,14 +405,17 @@ def check_type(cls, attr, type_):\n         raise ImproperlyConfigured(\"'%s.%s' should be a %s.\"\n                 % (cls.__name__, attr, type_.__name__ ))\n \n+\n def check_isseq(cls, label, obj):\n     if not isinstance(obj, (list, tuple)):\n         raise ImproperlyConfigured(\"'%s.%s' must be a list or tuple.\" % (cls.__name__, label))\n \n+\n def check_isdict(cls, label, obj):\n     if not isinstance(obj, dict):\n         raise ImproperlyConfigured(\"'%s.%s' must be a dictionary.\" % (cls.__name__, label))\n \n+\n def get_field(cls, model, label, field):\n     try:\n         return model._meta.get_field(field)\n@@ -420,6 +423,7 @@ def get_field(cls, model, label, field):\n         raise ImproperlyConfigured(\"'%s.%s' refers to field '%s' that is missing from model '%s.%s'.\"\n                 % (cls.__name__, label, field, model._meta.app_label, model.__name__))\n \n+\n def fetch_attr(cls, model, label, field):\n     try:\n         return model._meta.get_field(field)\ndiff --git a/django/contrib/auth/__init__.py b/django/contrib/auth/__init__.py\nindex 4ef5c0b2cd30..5c18c794fef4 100644\n--- a/django/contrib/auth/__init__.py\n+++ b/django/contrib/auth/__init__.py\n@@ -3,6 +3,7 @@\n \n from django.conf import settings\n from django.core.exceptions import ImproperlyConfigured, PermissionDenied\n+import django.contrib.auth.checks\n from django.utils.module_loading import import_by_path\n from django.middleware.csrf import rotate_token\n \ndiff --git a/django/contrib/auth/checks.py b/django/contrib/auth/checks.py\nnew file mode 100644\nindex 000000000000..1d5f022d85fb\n--- /dev/null\n+++ b/django/contrib/auth/checks.py\n@@ -0,0 +1,69 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from django.core import checks\n+\n+\n+@checks.register()\n+def check_user_model(**kwargs):\n+    from django.conf import settings\n+    from django.db import models\n+\n+    errors = []\n+    app_name, model_name = settings.AUTH_USER_MODEL.split('.')\n+\n+    cls = models.get_model(app_name, model_name)\n+\n+    # Check that REQUIRED_FIELDS is a list\n+    if not isinstance(cls.REQUIRED_FIELDS, (list, tuple)):\n+        errors.append(\n+            checks.Error(\n+                'The REQUIRED_FIELDS must be a list or tuple.',\n+                hint=None,\n+                obj=cls,\n+                id='auth.E001',\n+            )\n+        )\n+\n+    # Check that the USERNAME FIELD isn't included in REQUIRED_FIELDS.\n+    if cls.USERNAME_FIELD in cls.REQUIRED_FIELDS:\n+        errors.append(\n+            checks.Error(\n+                'The field named as the USERNAME_FIELD '\n+                    'must not be included in REQUIRED_FIELDS '\n+                    'on a custom user model.',\n+                hint=None,\n+                obj=cls,\n+                id='auth.E002',\n+            )\n+        )\n+\n+\n+    # Check that the username field is unique\n+    if not cls._meta.get_field(cls.USERNAME_FIELD).unique:\n+        if settings.AUTHENTICATION_BACKENDS == \\\n+                ('django.contrib.auth.backends.ModelBackend',):\n+            errors.append(\n+                checks.Error(\n+                    'The %s.%s field must be unique because it is '\n+                        'pointed to by USERNAME_FIELD.'\n+                        % (cls._meta.object_name, cls.USERNAME_FIELD),\n+                    hint=None,\n+                    obj=cls,\n+                    id='auth.E003',\n+                )\n+            )\n+        else:\n+            errors.append(\n+                checks.Warning(\n+                    'The %s.%s field is pointed to by USERNAME_FIELD, '\n+                        'but it is not unique.'\n+                        % (cls._meta.object_name, cls.USERNAME_FIELD),\n+                    hint='Ensure that your authentication backend can handle '\n+                        'non-unique usernames.',\n+                    obj=cls,\n+                    id='auth.W004',\n+                )\n+            )\n+\n+    return errors\ndiff --git a/django/contrib/auth/management/commands/changepassword.py b/django/contrib/auth/management/commands/changepassword.py\nindex 3240b0f99225..5b47cefd9a01 100644\n--- a/django/contrib/auth/management/commands/changepassword.py\n+++ b/django/contrib/auth/management/commands/changepassword.py\n@@ -13,7 +13,7 @@ class Command(BaseCommand):\n     )\n     help = \"Change a user's password for django.contrib.auth.\"\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def _get_pass(self, prompt=\"Password: \"):\n         p = getpass.getpass(prompt=prompt)\ndiff --git a/django/contrib/auth/tests/test_management.py b/django/contrib/auth/tests/test_management.py\nindex e56df0676b58..1eedc3eee033 100644\n--- a/django/contrib/auth/tests/test_management.py\n+++ b/django/contrib/auth/tests/test_management.py\n@@ -2,18 +2,18 @@\n from datetime import date\n \n from django.contrib.auth import models, management\n+from django.contrib.auth.checks import check_user_model\n from django.contrib.auth.management import create_permissions\n from django.contrib.auth.management.commands import changepassword\n from django.contrib.auth.models import User\n from django.contrib.auth.tests.custom_user import CustomUser\n from django.contrib.auth.tests.utils import skipIfCustomUser\n from django.contrib.contenttypes.models import ContentType\n+from django.core import checks\n from django.core.management import call_command\n from django.core.management.base import CommandError\n-from django.core.management.validation import get_validation_errors\n-from django.db.models.loading import get_app\n from django.test import TestCase\n-from django.test.utils import override_settings\n+from django.test.utils import override_settings, override_system_checks\n from django.utils import six\n from django.utils.six import StringIO\n \n@@ -144,7 +144,7 @@ def test_swappable_user(self):\n             email=\"joe@somewhere.org\",\n             date_of_birth=\"1976-04-01\",\n             stdout=new_io,\n-            skip_validation=True\n+            skip_checks=True\n         )\n         command_output = new_io.getvalue().strip()\n         self.assertEqual(command_output, 'Superuser created successfully.')\n@@ -167,7 +167,7 @@ def test_swappable_user_missing_required_field(self):\n                 username=\"joe@somewhere.org\",\n                 stdout=new_io,\n                 stderr=new_io,\n-                skip_validation=True\n+                skip_checks=True\n             )\n \n         self.assertEqual(CustomUser._default_manager.count(), 0)\n@@ -175,25 +175,81 @@ def test_swappable_user_missing_required_field(self):\n \n class CustomUserModelValidationTestCase(TestCase):\n     @override_settings(AUTH_USER_MODEL='auth.CustomUserNonListRequiredFields')\n+    @override_system_checks([check_user_model])\n     def test_required_fields_is_list(self):\n-        \"REQUIRED_FIELDS should be a list.\"\n-        new_io = StringIO()\n-        get_validation_errors(new_io, get_app('auth'))\n-        self.assertIn(\"The REQUIRED_FIELDS must be a list or tuple.\", new_io.getvalue())\n+        \"\"\" REQUIRED_FIELDS should be a list. \"\"\"\n+\n+        from .test_custom_user import CustomUserNonListRequiredFields\n+        errors = checks.run_checks()\n+        expected = [\n+            checks.Error(\n+                'The REQUIRED_FIELDS must be a list or tuple.',\n+                hint=None,\n+                obj=CustomUserNonListRequiredFields,\n+                id='auth.E001',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n \n     @override_settings(AUTH_USER_MODEL='auth.CustomUserBadRequiredFields')\n+    @override_system_checks([check_user_model])\n     def test_username_not_in_required_fields(self):\n-        \"USERNAME_FIELD should not appear in REQUIRED_FIELDS.\"\n-        new_io = StringIO()\n-        get_validation_errors(new_io, get_app('auth'))\n-        self.assertIn(\"The field named as the USERNAME_FIELD should not be included in REQUIRED_FIELDS on a swappable User model.\", new_io.getvalue())\n+        \"\"\" USERNAME_FIELD should not appear in REQUIRED_FIELDS. \"\"\"\n+\n+        from .test_custom_user import CustomUserBadRequiredFields\n+        errors = checks.run_checks()\n+        expected = [\n+            checks.Error(\n+                'The field named as the USERNAME_FIELD must not be included '\n+                    'in REQUIRED_FIELDS on a custom user model.',\n+                hint=None,\n+                obj=CustomUserBadRequiredFields,\n+                id='auth.E002',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n \n     @override_settings(AUTH_USER_MODEL='auth.CustomUserNonUniqueUsername')\n+    @override_system_checks([check_user_model])\n     def test_username_non_unique(self):\n-        \"A non-unique USERNAME_FIELD should raise a model validation error.\"\n-        new_io = StringIO()\n-        get_validation_errors(new_io, get_app('auth'))\n-        self.assertIn(\"The USERNAME_FIELD must be unique. Add unique=True to the field parameters.\", new_io.getvalue())\n+        \"\"\" A non-unique USERNAME_FIELD should raise an error. \"\"\"\n+\n+        from .test_custom_user import CustomUserNonUniqueUsername\n+        errors = checks.run_checks()\n+        expected = [\n+            checks.Error(\n+                'The CustomUserNonUniqueUsername.username field must be '\n+                    'unique because it is pointed to by USERNAME_FIELD.',\n+                hint=None,\n+                obj=CustomUserNonUniqueUsername,\n+                id='auth.E003',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    @override_settings(AUTH_USER_MODEL='auth.CustomUserNonUniqueUsername',\n+                       AUTHENTICATION_BACKENDS=[\n+                           'my.custom.backend',\n+                       ])\n+    @override_system_checks([check_user_model])\n+    def test_username_non_unique_with_custom_backend(self):\n+        \"\"\" A non-unique USERNAME_FIELD should raise an error only if we use the\n+        default authentication backend. Otherwise, an warning should be raised.\n+        \"\"\"\n+\n+        from .test_custom_user import CustomUserNonUniqueUsername\n+        errors = checks.run_checks()\n+        expected = [\n+            checks.Warning(\n+                'The CustomUserNonUniqueUsername.username field is pointed to '\n+                    'by USERNAME_FIELD, but it is not unique.',\n+                hint='Ensure that your authentication backend can handle '\n+                    'non-unique usernames.',\n+                obj=CustomUserNonUniqueUsername,\n+                id='auth.W004',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n \n \n class PermissionTestCase(TestCase):\ndiff --git a/django/contrib/contenttypes/__init__.py b/django/contrib/contenttypes/__init__.py\nindex e69de29bb2d1..0e47d28912b8 100644\n--- a/django/contrib/contenttypes/__init__.py\n+++ b/django/contrib/contenttypes/__init__.py\n@@ -0,0 +1,4 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+import django.contrib.contenttypes.checks\ndiff --git a/django/contrib/contenttypes/checks.py b/django/contrib/contenttypes/checks.py\nnew file mode 100644\nindex 000000000000..78454510aa5a\n--- /dev/null\n+++ b/django/contrib/contenttypes/checks.py\n@@ -0,0 +1,19 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from django.core import checks\n+\n+\n+@checks.register('models')\n+def check_generic_foreign_keys(**kwargs):\n+    from .generic import GenericForeignKey\n+    from django.db import models\n+\n+    errors = []\n+    fields = (obj\n+        for cls in models.get_models()\n+        for obj in vars(cls).itervalues()\n+        if isinstance(obj, GenericForeignKey))\n+    for field in fields:\n+        errors.extend(field.check())\n+    return errors\ndiff --git a/django/contrib/contenttypes/generic.py b/django/contrib/contenttypes/generic.py\nindex 51eed3f28bfb..b8b07555704f 100644\n--- a/django/contrib/contenttypes/generic.py\n+++ b/django/contrib/contenttypes/generic.py\n@@ -6,10 +6,12 @@\n from collections import defaultdict\n from functools import partial\n \n+from django.core import checks\n from django.core.exceptions import ObjectDoesNotExist\n from django.db import connection\n from django.db import models, router, DEFAULT_DB_ALIAS\n-from django.db.models import signals\n+from django.db.models import signals, FieldDoesNotExist\n+from django.db.models.base import ModelBase\n from django.db.models.fields.related import ForeignObject, ForeignObjectRel\n from django.db.models.related import PathInfo\n from django.db.models.sql.where import Constraint\n@@ -20,7 +22,7 @@\n from django.contrib.contenttypes.models import ContentType\n from django.utils import six\n from django.utils.deprecation import RenameMethodsBase\n-from django.utils.encoding import smart_text\n+from django.utils.encoding import smart_text, python_2_unicode_compatible\n \n \n class RenameGenericForeignKeyMethods(RenameMethodsBase):\n@@ -29,6 +31,7 @@ class RenameGenericForeignKeyMethods(RenameMethodsBase):\n     )\n \n \n+@python_2_unicode_compatible\n class GenericForeignKey(six.with_metaclass(RenameGenericForeignKeyMethods)):\n     \"\"\"\n     Provides a generic relation to any object through content-type/object-id\n@@ -150,6 +153,54 @@ def __set__(self, instance, value):\n         setattr(instance, self.fk_field, fk)\n         setattr(instance, self.cache_attr, value)\n \n+    def __str__(self):\n+        model = self.model\n+        app = model._meta.app_label\n+        return '%s.%s.%s' % (app, model._meta.object_name, self.name)\n+\n+    def check(self, **kwargs):\n+        errors = []\n+        errors.extend(self._check_content_type_field())\n+        errors.extend(self._check_object_id_field())\n+        errors.extend(self._check_field_name())\n+        return errors\n+\n+    def _check_content_type_field(self):\n+        return _check_content_type_field(\n+            model=self.model,\n+            field_name=self.ct_field,\n+            checked_object=self)\n+\n+    def _check_object_id_field(self):\n+        try:\n+            self.model._meta.get_field(self.fk_field)\n+        except FieldDoesNotExist:\n+            return [\n+                checks.Error(\n+                    'The field refers to \"%s\" field which is missing.'\n+                        % self.fk_field,\n+                    hint=None,\n+                    obj=self,\n+                    id='contenttypes.E001',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_field_name(self):\n+        if self.name.endswith(\"_\"):\n+            return [\n+                checks.Error(\n+                    'Field names must not end with underscores.',\n+                    hint=None,\n+                    obj=self,\n+                    id='contenttypes.E002',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+\n class GenericRelation(ForeignObject):\n     \"\"\"Provides an accessor to generic related objects (e.g. comments)\"\"\"\n \n@@ -243,6 +294,114 @@ def bulk_related_objects(self, objs, using=DEFAULT_DB_ALIAS):\n                     [obj.pk for obj in objs]\n                 })\n \n+    def check(self, **kwargs):\n+        errors = super(GenericRelation, self).check(**kwargs)\n+        errors.extend(self._check_content_type_field())\n+        errors.extend(self._check_object_id_field())\n+        errors.extend(self._check_generic_foreign_key_existence())\n+        return errors\n+\n+    def _check_content_type_field(self):\n+        target = self.rel.to\n+        if isinstance(target, ModelBase):\n+            return _check_content_type_field(\n+                model=target,\n+                field_name=self.content_type_field_name,\n+                checked_object=self)\n+        else:\n+            return []\n+\n+    def _check_object_id_field(self):\n+        target = self.rel.to\n+        if isinstance(target, ModelBase):\n+            opts = target._meta\n+            try:\n+                opts.get_field(self.object_id_field_name)\n+            except FieldDoesNotExist:\n+                return [\n+                    checks.Error(\n+                        'The field refers to %s.%s field which is missing.'\n+                            % (opts.object_name, self.object_id_field_name),\n+                        hint=None,\n+                        obj=self,\n+                        id='contenttypes.E003',\n+                    )\n+                ]\n+            else:\n+                return []\n+        else:\n+            return []\n+\n+    def _check_generic_foreign_key_existence(self):\n+        target = self.rel.to\n+        if isinstance(target, ModelBase):\n+            # Using `vars` is very ugly approach, but there is no better one,\n+            # because GenericForeignKeys are not considered as fields and,\n+            # therefore, are not included in `target._meta.local_fields`.\n+            fields = target._meta.virtual_fields\n+            if any(isinstance(field, GenericForeignKey) and\n+                    field.ct_field == self.content_type_field_name and\n+                    field.fk_field == self.object_id_field_name\n+                    for field in fields):\n+                return []\n+            else:\n+                return [\n+                    checks.Warning(\n+                        'The field defines a generic relation with the model '\n+                            '%s.%s, but the model lacks GenericForeignKey.'\n+                            % (target._meta.app_label, target._meta.object_name),\n+                        hint=None,\n+                        obj=self,\n+                        id='contenttypes.E004',\n+                    )\n+                ]\n+        else:\n+            return []\n+\n+\n+def _check_content_type_field(model, field_name, checked_object):\n+    \"\"\" Check if field named `field_name` in model `model` exists and is\n+    valid content_type field (is a ForeignKey to ContentType). \"\"\"\n+\n+    try:\n+        field = model._meta.get_field(field_name)\n+    except FieldDoesNotExist:\n+        return [\n+            checks.Error(\n+                'The field refers to %s.%s field which is missing.'\n+                    % (model._meta.object_name, field_name),\n+                hint=None,\n+                obj=checked_object,\n+                id='contenttypes.E005',\n+            )\n+        ]\n+    else:\n+        if not isinstance(field, models.ForeignKey):\n+            return [\n+                checks.Error(\n+                    '\"%s\" field is used by a %s '\n+                        'as content type field and therefore it must be '\n+                        'a ForeignKey.'\n+                        % (field_name, checked_object.__class__.__name__),\n+                    hint=None,\n+                    obj=checked_object,\n+                    id='contenttypes.E006',\n+                )\n+            ]\n+        elif field.rel.to != ContentType:\n+            return [\n+                checks.Error(\n+                    '\"%s\" field is used by a %s '\n+                        'as content type field and therefore it must be '\n+                        'a ForeignKey to ContentType.'\n+                        % (field_name, checked_object.__class__.__name__),\n+                    hint=None,\n+                    obj=checked_object,\n+                    id='contenttypes.E007',\n+                )\n+            ]\n+        else:\n+            return []\n \n class ReverseGenericRelatedObjectsDescriptor(object):\n     \"\"\"\ndiff --git a/django/contrib/gis/management/commands/ogrinspect.py b/django/contrib/gis/management/commands/ogrinspect.py\nindex 238209123e1b..0b9f16a92243 100644\n--- a/django/contrib/gis/management/commands/ogrinspect.py\n+++ b/django/contrib/gis/management/commands/ogrinspect.py\n@@ -69,7 +69,7 @@ class Command(LabelCommand):\n                     help='Generate mapping dictionary for use with `LayerMapping`.')\n         )\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def handle(self, *args, **options):\n         try:\ndiff --git a/django/contrib/sites/managers.py b/django/contrib/sites/managers.py\nindex becb35b404cb..c792423a06e9 100644\n--- a/django/contrib/sites/managers.py\n+++ b/django/contrib/sites/managers.py\n@@ -1,41 +1,66 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n from django.conf import settings\n+from django.core import checks\n from django.db import models\n from django.db.models.fields import FieldDoesNotExist\n \n+\n class CurrentSiteManager(models.Manager):\n     \"Use this to limit objects to those associated with the current site.\"\n+\n     def __init__(self, field_name=None):\n         super(CurrentSiteManager, self).__init__()\n         self.__field_name = field_name\n-        self.__is_validated = False\n-        \n-    def _validate_field_name(self):\n-        field_names = self.model._meta.get_all_field_names()\n-        \n-        # If a custom name is provided, make sure the field exists on the model\n-        if self.__field_name is not None and self.__field_name not in field_names:\n-            raise ValueError(\"%s couldn't find a field named %s in %s.\" % \\\n-                (self.__class__.__name__, self.__field_name, self.model._meta.object_name))\n-        \n-        # Otherwise, see if there is a field called either 'site' or 'sites'\n-        else:\n-            for potential_name in ['site', 'sites']:\n-                if potential_name in field_names:\n-                    self.__field_name = potential_name\n-                    self.__is_validated = True\n-                    break\n-        \n-        # Now do a type check on the field (FK or M2M only)\n+\n+    def _get_field_name(self):\n+        \"\"\" Return self.__field_name or 'site' or 'sites'. \"\"\"\n+\n+        if not self.__field_name:\n+            try:\n+                self.model._meta.get_field('site')\n+            except FieldDoesNotExist:\n+                self.__field_name = 'sites'\n+            else:\n+                self.__field_name = 'site'\n+        return self.__field_name\n+\n+    def get_queryset(self):\n+        return super(CurrentSiteManager, self).get_queryset().filter(\n+            **{self._get_field_name() + '__id__exact': settings.SITE_ID})\n+\n+    def check(self, **kwargs):\n+        errors = super(CurrentSiteManager, self).check(**kwargs)\n+        errors.extend(self._check_field_name())\n+        return errors\n+\n+    def _check_field_name(self):\n+        field_name = self._get_field_name()\n         try:\n-            field = self.model._meta.get_field(self.__field_name)\n-            if not isinstance(field, (models.ForeignKey, models.ManyToManyField)):\n-                raise TypeError(\"%s must be a ForeignKey or ManyToManyField.\" %self.__field_name)\n+            field = self.model._meta.get_field(field_name)\n         except FieldDoesNotExist:\n-            raise ValueError(\"%s couldn't find a field named %s in %s.\" % \\\n-                    (self.__class__.__name__, self.__field_name, self.model._meta.object_name))\n-        self.__is_validated = True\n-    \n-    def get_queryset(self):\n-        if not self.__is_validated:\n-            self._validate_field_name()\n-        return super(CurrentSiteManager, self).get_queryset().filter(**{self.__field_name + '__id__exact': settings.SITE_ID})\n+            return [\n+                checks.Error(\n+                    'CurrentSiteManager could not find a field named \"%s\".'\n+                        % field_name,\n+                    hint='Ensure that you did not misspell the field name. '\n+                        'Does the field exist?',\n+                    obj=self,\n+                    id='sites.E001',\n+                )\n+            ]\n+\n+        if not isinstance(field, (models.ForeignKey, models.ManyToManyField)):\n+            return [\n+                checks.Error(\n+                    '%s.%s is used by a CurrentSiteManager and must be '\n+                        'a ForeignKey or ManyToManyField.'\n+                        % (self.model._meta.object_name, field_name),\n+                    hint=None,\n+                    obj=self,\n+                    id='sites.E002',\n+                )\n+            ]\n+\n+        return []\ndiff --git a/django/contrib/staticfiles/management/commands/collectstatic.py b/django/contrib/staticfiles/management/commands/collectstatic.py\nindex c1e9fa811ba9..d0412f038f3a 100644\n--- a/django/contrib/staticfiles/management/commands/collectstatic.py\n+++ b/django/contrib/staticfiles/management/commands/collectstatic.py\n@@ -45,7 +45,7 @@ class Command(NoArgsCommand):\n                 \"'.*' and '*~'.\"),\n     )\n     help = \"Collect static files in a single location.\"\n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def __init__(self, *args, **kwargs):\n         super(NoArgsCommand, self).__init__(*args, **kwargs)\ndiff --git a/django/core/checks/__init__.py b/django/core/checks/__init__.py\nindex e69de29bb2d1..ecb10940d403 100644\n--- a/django/core/checks/__init__.py\n+++ b/django/core/checks/__init__.py\n@@ -0,0 +1,9 @@\n+# -*- coding: utf8 -*-\n+from __future__ import unicode_literals\n+\n+from .messages import (CheckMessage,\n+        Debug, Info, Warning, Error, Critical,\n+        DEBUG, INFO, WARNING, ERROR, CRITICAL)\n+from .registration import register, run_checks, tag_exists\n+import django.core.checks.compatibility.django_1_6_0\n+import django.core.checks.model_checks\ndiff --git a/django/core/checks/compatibility/base.py b/django/core/checks/compatibility/base.py\ndeleted file mode 100644\nindex 7fe52d2af965..000000000000\n--- a/django/core/checks/compatibility/base.py\n+++ /dev/null\n@@ -1,39 +0,0 @@\n-from __future__ import unicode_literals\n-import warnings\n-\n-from django.core.checks.compatibility import django_1_6_0\n-\n-\n-COMPAT_CHECKS = [\n-    # Add new modules at the top, so we keep things in descending order.\n-    # After two-three minor releases, old versions should get dropped.\n-    django_1_6_0,\n-]\n-\n-\n-def check_compatibility():\n-    \"\"\"\n-    Runs through compatibility checks to warn the user with an existing install\n-    about changes in an up-to-date Django.\n-\n-    Modules should be located in ``django.core.compat_checks`` (typically one\n-    per release of Django) & must have a ``run_checks`` function that runs\n-    all the checks.\n-\n-    Returns a list of informational messages about incompatibilities.\n-    \"\"\"\n-    messages = []\n-\n-    for check_module in COMPAT_CHECKS:\n-        check = getattr(check_module, 'run_checks', None)\n-\n-        if check is None:\n-            warnings.warn(\n-                \"The '%s' module lacks a \" % check_module.__name__ +\n-                \"'run_checks' method, which is needed to verify compatibility.\"\n-            )\n-            continue\n-\n-        messages.extend(check())\n-\n-    return messages\ndiff --git a/django/core/checks/compatibility/django_1_6_0.py b/django/core/checks/compatibility/django_1_6_0.py\nindex 96b29d6b77d9..098ee6091b2c 100644\n--- a/django/core/checks/compatibility/django_1_6_0.py\n+++ b/django/core/checks/compatibility/django_1_6_0.py\n@@ -1,62 +1,67 @@\n+# -*- encoding: utf-8 -*-\n from __future__ import unicode_literals\n \n-from django.db import models\n+from .. import Warning, register\n \n-def check_test_runner():\n-    \"\"\"\n-    Checks if the user has *not* overridden the ``TEST_RUNNER`` setting &\n-    warns them about the default behavior changes.\n \n-    If the user has overridden that setting, we presume they know what they're\n-    doing & avoid generating a message.\n-    \"\"\"\n+@register('compatibility')\n+def check_1_6_compatibility(**kwargs):\n+    errors = []\n+    errors.extend(_check_test_runner(**kwargs))\n+    errors.extend(_check_boolean_field_default_value(**kwargs))\n+    return errors\n+\n+\n+def _check_test_runner(apps=None, **kwargs):\n+    \"\"\" Warn an user if the user has *not* set explicitly the ``TEST_RUNNER``\n+    setting. \"\"\"\n+\n+    if apps is not None:\n+        return []\n+\n     from django.conf import settings\n-    new_default = 'django.test.runner.DiscoverRunner'\n-    test_runner_setting = getattr(settings, 'TEST_RUNNER', new_default)\n-\n-    if test_runner_setting == new_default:\n-        message = [\n-            \"Django 1.6 introduced a new default test runner ('%s')\" % new_default,\n-            \"You should ensure your tests are all running & behaving as expected. See\",\n-            \"https://docs.djangoproject.com/en/dev/releases/1.6/#discovery-of-tests-in-any-test-module\",\n-            \"for more information.\",\n+\n+    if not settings.is_overridden(\"TEST_RUNNER\"):\n+        return [\n+            Warning(\n+                'You have not explicitly set \"TEST_RUNNER\". In Django 1.6, '\n+                    'there is a new test runner (\"django.test.runner.DiscoverRunner\") '\n+                    'by default. You should ensure your tests are still all '\n+                    'running & behaving as expected. See '\n+                    'https://docs.djangoproject.com/en/dev/releases/1.6/#discovery-of-tests-in-any-test-module '\n+                    'for more information.',\n+                hint=None,\n+                obj=None,\n+                id='W047',\n+            )\n         ]\n-        return ' '.join(message)\n+    else:\n+        return []\n+\n \n-def check_boolean_field_default_value():\n+def _check_boolean_field_default_value(apps=None, **kwargs):\n     \"\"\"\n     Checks if there are any BooleanFields without a default value, &\n     warns the user that the default has changed from False to Null.\n     \"\"\"\n-    fields = []\n-    for cls in models.get_models():\n-        opts = cls._meta\n-        for f in opts.local_fields:\n-            if isinstance(f, models.BooleanField) and not f.has_default():\n-                fields.append(\n-                    '%s.%s: \"%s\"' % (opts.app_label, opts.object_name, f.name)\n-                )\n-    if fields:\n-        fieldnames = \", \".join(fields)\n-        message = [\n-            \"You have not set a default value for one or more BooleanFields:\",\n-            \"%s.\" % fieldnames,\n-            \"In Django 1.6 the default value of BooleanField was changed from\",\n-            \"False to Null when Field.default isn't defined. See\",\n-            \"https://docs.djangoproject.com/en/1.6/ref/models/fields/#booleanfield\"\n-            \"for more information.\"\n-        ]\n-        return ' '.join(message)\n \n+    from django.db import models\n \n-def run_checks():\n-    \"\"\"\n-    Required by the ``check`` management command, this returns a list of\n-    messages from all the relevant check functions for this version of Django.\n-    \"\"\"\n-    checks = [\n-        check_test_runner(),\n-        check_boolean_field_default_value(),\n-    ]\n-    # Filter out the ``None`` or empty strings.\n-    return [output for output in checks if output]\n+    invalid_fields = [field\n+        for model in models.get_models(**kwargs)\n+        if apps is None or model._meta.app_label in apps\n+        for field in model._meta.local_fields\n+        if isinstance(field, models.BooleanField) and not field.has_default()]\n+\n+    return [\n+        Warning(\n+            'The field has not set a default value. In Django 1.6 '\n+                'the default value of BooleanField was changed from '\n+                'False to Null when Field.default is not defined. '\n+                'See https://docs.djangoproject.com/en/1.6/ref/models/fields/#booleanfield '\n+                'for more information.',\n+            hint=None,\n+            obj=field,\n+            id='W048',\n+        )\n+        for field in invalid_fields]\ndiff --git a/django/core/checks/messages.py b/django/core/checks/messages.py\nnew file mode 100644\nindex 000000000000..c3a4428a7b18\n--- /dev/null\n+++ b/django/core/checks/messages.py\n@@ -0,0 +1,86 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from functools import partial\n+\n+from django.utils.encoding import python_2_unicode_compatible, force_str\n+\n+\n+# Levels\n+DEBUG = 10\n+INFO = 20\n+WARNING = 30\n+ERROR = 40\n+CRITICAL = 50\n+\n+\n+@python_2_unicode_compatible\n+class CheckMessage(object):\n+\n+    def __init__(self, level, msg, hint, obj=None, id=None):\n+        assert isinstance(level, int), \"The first argument should be level.\"\n+        self.level = level\n+        self.msg = msg\n+        self.hint = hint\n+        self.obj = obj\n+        self.id = id\n+\n+    def __eq__(self, other):\n+        return all(getattr(self, attr) == getattr(other, attr)\n+                   for attr in ['level', 'msg', 'hint', 'obj', 'id'])\n+\n+    def __ne__(self, other):\n+        return not (self == other)\n+\n+    def __str__(self):\n+        from django.db import models\n+\n+        if self.obj is None:\n+            obj = \"?\"\n+        elif isinstance(self.obj, models.base.ModelBase):\n+            # We need to hardcode ModelBase and Field cases because its __str__\n+            # method doesn't return \"applabel.modellabel\" and cannot be changed.\n+            model = self.obj\n+            app = model._meta.app_label\n+            obj = '%s.%s' % (app, model._meta.object_name)\n+        else:\n+            obj = force_str(self.obj)\n+        id = \"(%s) \" % self.id if self.id else \"\"\n+        hint = \"\\n\\tHINT: %s\" % self.hint if self.hint else ''\n+        return \"%s: %s%s%s\" % (obj, id, self.msg, hint)\n+\n+    def __repr__(self):\n+        return \"<%s: level=%r, msg=%r, hint=%r, obj=%r, id=%r>\" % \\\n+            (self.__class__.__name__, self.level, self.msg, self.hint, self.obj, self.id)\n+\n+    def is_serious(self):\n+        return self.level >= ERROR\n+\n+    def is_silenced(self):\n+        from django.conf import settings\n+        return self.id in settings.SILENCED_SYSTEM_CHECKS\n+\n+\n+class Debug(CheckMessage):\n+    def __init__(self, *args, **kwargs):\n+        return super(Debug, self).__init__(DEBUG, *args, **kwargs)\n+\n+\n+class Info(CheckMessage):\n+    def __init__(self, *args, **kwargs):\n+        return super(Info, self).__init__(INFO, *args, **kwargs)\n+\n+\n+class Warning(CheckMessage):\n+    def __init__(self, *args, **kwargs):\n+        return super(Warning, self).__init__(WARNING, *args, **kwargs)\n+\n+\n+class Error(CheckMessage):\n+    def __init__(self, *args, **kwargs):\n+        return super(Error, self).__init__(ERROR, *args, **kwargs)\n+\n+\n+class Critical(CheckMessage):\n+    def __init__(self, *args, **kwargs):\n+        return super(Critical, self).__init__(CRITICAL, *args, **kwargs)\ndiff --git a/django/core/checks/model_checks.py b/django/core/checks/model_checks.py\nnew file mode 100644\nindex 000000000000..8f55da8b4daa\n--- /dev/null\n+++ b/django/core/checks/model_checks.py\n@@ -0,0 +1,16 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from itertools import chain\n+\n+from . import Warning, register\n+\n+\n+@register('models')\n+def check_all_models(apps=None, **kwargs):\n+    from django.db import models\n+\n+    errors = [model.check(**kwargs)\n+        for model in models.get_models(**kwargs)\n+        if apps is None or model._meta.app_label in apps]\n+    return list(chain(*errors))\ndiff --git a/django/core/checks/registration.py b/django/core/checks/registration.py\nnew file mode 100644\nindex 000000000000..f053e5b74c40\n--- /dev/null\n+++ b/django/core/checks/registration.py\n@@ -0,0 +1,63 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from itertools import chain\n+\n+from django.utils.itercompat import is_iterable\n+\n+\n+class CheckFramework(object):\n+\n+    def __init__(self):\n+        self.registered_checks = []\n+\n+    def register(self, *tags):\n+        \"\"\"\n+        Decorator. Register given function `f` labeled with given `tags`. The\n+        function should receive **kwargs and return list of Errors and\n+        Warnings.\n+\n+        Example::\n+\n+            framework = CheckFramework()\n+            @framework.register('mytag', 'anothertag')\n+            def my_check(apps, **kwargs):\n+                # ... perform checks and collect `errors` ...\n+                return errors\n+\n+        \"\"\"\n+\n+        def inner(f):\n+            f.tags = tags\n+            self.registered_checks.append(f)\n+            return f\n+\n+        return inner\n+\n+    def run_checks(self, apps=None, tags=None):\n+        \"\"\" Run all registered checks and return list of Errors and Warnings.\n+        \"\"\"\n+        errors = []\n+        if tags is not None:\n+            checks = [check for check in self.registered_checks\n+                      if hasattr(check, 'tags') and set(check.tags) & set(tags)]\n+        else:\n+            checks = self.registered_checks\n+\n+        for f in checks:\n+            new_errors = f(apps=apps)\n+            assert is_iterable(new_errors), (\n+                \"The function %r did not return a list. All functions registered \"\n+                \"in checking framework must return a list.\" % f)\n+            errors.extend(new_errors)\n+        return errors\n+\n+    def tag_exists(self, tag):\n+        tags = chain(*[check.tags for check in self.registered_checks if hasattr(check, 'tags')])\n+        return tag in tags\n+\n+\n+framework = CheckFramework()\n+register = framework.register\n+run_checks = framework.run_checks\n+tag_exists = framework.tag_exists\ndiff --git a/django/core/management/base.py b/django/core/management/base.py\nindex 1aba53dd0148..e214ef2aac4b 100644\n--- a/django/core/management/base.py\n+++ b/django/core/management/base.py\n@@ -1,3 +1,6 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n \"\"\"\n Base classes for writing management commands (named commands which can\n be executed through ``django-admin.py`` or ``manage.py``).\n@@ -5,14 +8,15 @@\n \"\"\"\n import os\n import sys\n+import warnings\n \n from optparse import make_option, OptionParser\n \n import django\n+from django.core import checks\n from django.core.exceptions import ImproperlyConfigured\n from django.core.management.color import color_style, no_style\n from django.utils.encoding import force_str\n-from django.utils.six import StringIO\n \n \n class CommandError(Exception):\n@@ -134,13 +138,18 @@ class BaseCommand(object):\n         wrapped with ``BEGIN;`` and ``COMMIT;``. Default value is\n         ``False``.\n \n+    ``requires_system_checks``\n+        A boolean; if ``True``, entire Django project will be checked for errors\n+        prior to executing the command. If it's missing, the value of\n+        ``requires_model_validation`` is used. If the latter flag is missing\n+        too, the default value (``True``) is used. Defining both\n+        ``requires_system_checks`` and ``requires_model_validation`` results in\n+        an error.\n+\n     ``requires_model_validation``\n-        A boolean; if ``True``, validation of installed models will be\n-        performed prior to executing the command. Default value is\n-        ``True``. To validate an individual application's models\n-        rather than all applications' models, call\n-        ``self.validate(app)`` from ``handle()``, where ``app`` is the\n-        application's Python module.\n+        A deprecated boolean. If ``requires_system_checks`` is missing, then\n+        this value is used. Defining both ``requires_system_checks`` and\n+        ``requires_model_validation`` results in an error.\n \n     ``leave_locale_alone``\n         A boolean indicating whether the locale set in settings should be\n@@ -179,13 +188,40 @@ class BaseCommand(object):\n \n     # Configuration shortcuts that alter various logic.\n     can_import_settings = True\n-    requires_model_validation = True\n     output_transaction = False  # Whether to wrap the output in a \"BEGIN; COMMIT;\"\n     leave_locale_alone = False\n \n+    # Uncomment the following line of code after deprecation plan for\n+    # requires_model_validation comes to completion:\n+    #\n+    # requires_system_checks = True\n+\n     def __init__(self):\n         self.style = color_style()\n \n+        # `requires_model_validation` is deprecated in favour of\n+        # `requires_system_checks`. If both options are present, an error is\n+        # raised. Otherwise the present option is used. If none of them is\n+        # defined, the default value (True) is used.\n+        has_old_option = hasattr(self, 'requires_model_validation')\n+        has_new_option = hasattr(self, 'requires_system_checks')\n+\n+        if has_old_option:\n+            warnings.warn(\n+                '\"requires_model_validation\" is deprecated '\n+                'in favour of \"requires_system_checks\".',\n+                PendingDeprecationWarning)\n+        if has_old_option and has_new_option:\n+            raise ImproperlyConfigured(\n+                'Command %s defines both \"requires_model_validation\" '\n+                'and \"requires_system_checks\", which is illegal. Use only '\n+                '\"requires_system_checks\".' % self.__class__.__name__)\n+\n+        self.requires_system_checks = (\n+            self.requires_system_checks if has_new_option else\n+            self.requires_model_validation if has_old_option else\n+            True)\n+\n     def get_version(self):\n         \"\"\"\n         Return the Django version, which should be correct for all\n@@ -251,8 +287,8 @@ def run_from_argv(self, argv):\n \n     def execute(self, *args, **options):\n         \"\"\"\n-        Try to execute this command, performing model validation if\n-        needed (as controlled by the attribute\n+        Try to execute this command, performing system checks if needed (as\n+        controlled by attributes ``self.requires_system_checks`` and\n         ``self.requires_model_validation``, except if force-skipped).\n         \"\"\"\n         self.stdout = OutputWrapper(options.get('stdout', sys.stdout))\n@@ -284,8 +320,10 @@ def execute(self, *args, **options):\n             translation.activate('en-us')\n \n         try:\n-            if self.requires_model_validation and not options.get('skip_validation'):\n-                self.validate()\n+            if (self.requires_system_checks and\n+                    not options.get('skip_validation') and  # This will be removed at the end of deprecation proccess for `skip_validation`.\n+                    not options.get('skip_checks')):\n+                self.check()\n             output = self.handle(*args, **options)\n             if output:\n                 if self.output_transaction:\n@@ -303,21 +341,63 @@ def execute(self, *args, **options):\n                 translation.activate(saved_locale)\n \n     def validate(self, app=None, display_num_errors=False):\n-        \"\"\"\n-        Validates the given app, raising CommandError for any errors.\n+        \"\"\" Deprecated. Delegates to ``check``. ``app`` argument is ignored. \"\"\"\n \n-        If app is None, then this will validate all installed apps.\n+        return self.check(display_num_errors)\n \n+    def check(self, apps=None, tags=None, display_num_errors=False):\n         \"\"\"\n-        from django.core.management.validation import get_validation_errors\n-        s = StringIO()\n-        num_errors = get_validation_errors(s, app)\n-        if num_errors:\n-            s.seek(0)\n-            error_text = s.read()\n-            raise CommandError(\"One or more models did not validate:\\n%s\" % error_text)\n+        Uses the system check framework to validate entire Django project.\n+        Raises CommandError for any serious message (error or critical errors).\n+        If there are only light messages (like warnings), they are printed to\n+        stderr and no exception is raised.\n+\n+        \"\"\"\n+\n+        all_issues = checks.run_checks(apps=apps, tags=tags)\n+\n+        msg = \"\"\n+        if all_issues:\n+            debugs = [e for e in all_issues if e.level < checks.INFO and not e.is_silenced()]\n+            infos = [e for e in all_issues if checks.INFO <= e.level < checks.WARNING and not e.is_silenced()]\n+            warnings = [e for e in all_issues if checks.WARNING <= e.level < checks.ERROR and not e.is_silenced()]\n+            errors = [e for e in all_issues if checks.ERROR <= e.level < checks.CRITICAL]\n+            criticals = [e for e in all_issues if checks.CRITICAL <= e.level]\n+            sorted_issues = [\n+                (criticals, 'CRITICALS'),\n+                (errors, 'ERRORS'),\n+                (warnings, 'WARNINGS'),\n+                (infos, 'INFOS'),\n+                (debugs, 'DEBUGS'),\n+            ]\n+\n+            for issues, group_name in sorted_issues:\n+                if issues:\n+                    formatted = (\n+                        color_style().ERROR(force_str(e))\n+                        if e.is_serious()\n+                        else color_style().WARNING(force_str(e))\n+                        for e in issues)\n+                    formatted = \"\\n\".join(sorted(formatted))\n+                    msg += '\\n%s:\\n%s\\n' % (group_name, formatted)\n+\n+            msg = \"There are some issues:\\n%s\" % msg\n+\n         if display_num_errors:\n-            self.stdout.write(\"%s error%s found\" % (num_errors, '' if num_errors == 1 else 's'))\n+            if msg:\n+                msg += '\\n'\n+            msg += \"System check identified %s.\" % (\n+                \"no problems\" if len(all_issues) == 0 else\n+                \"1 issue\" if len(all_issues) == 1 else\n+                \"%s issues\" % len(all_issues)\n+            )\n+\n+        if any(e.is_serious() and not e.is_silenced() for e in all_issues):\n+            raise CommandError(msg)\n+        elif msg and all_issues:\n+            self.stderr.write(msg)\n+        elif msg:\n+            self.stdout.write(msg)\n \n     def handle(self, *args, **options):\n         \"\"\"\ndiff --git a/django/core/management/commands/check.py b/django/core/management/commands/check.py\nindex 05f48c82bc48..9d181dc20aee 100644\n--- a/django/core/management/commands/check.py\n+++ b/django/core/management/commands/check.py\n@@ -1,14 +1,26 @@\n+# -*- coding: utf-8 -*-\n from __future__ import unicode_literals\n-import warnings\n \n-from django.core.checks.compatibility.base import check_compatibility\n-from django.core.management.base import NoArgsCommand\n+from optparse import make_option\n \n+from django.core import checks\n+from django.core.management.base import BaseCommand, CommandError\n \n-class Command(NoArgsCommand):\n-    help = \"Checks your configuration's compatibility with this version \" + \\\n-           \"of Django.\"\n \n-    def handle_noargs(self, **options):\n-        for message in check_compatibility():\n-            warnings.warn(message)\n+class Command(BaseCommand):\n+    help = \"Uses the system check framework to validate entire Django project.\"\n+\n+    requires_system_checks = False\n+\n+    option_list = BaseCommand.option_list + (\n+        make_option('--tag', '-t', action='append', dest='tags',\n+            help='Run only checks labeled with given tag.'),\n+    )\n+\n+    def handle(self, *apps, **options):\n+        apps = apps or None  # If apps is an empty list, replace with None\n+        tags = options.get('tags', None)\n+        if tags and any(not checks.tag_exists(tag) for tag in tags):\n+            invalid_tag = next(tag for tag in tags if not checks.tag_exists(tag))\n+            raise CommandError('There is no system check labeled with \"%s\" tag.' % invalid_tag)\n+        self.check(apps=apps, tags=tags, display_num_errors=True)\ndiff --git a/django/core/management/commands/compilemessages.py b/django/core/management/commands/compilemessages.py\nindex cb4149200412..16c6499c525a 100644\n--- a/django/core/management/commands/compilemessages.py\n+++ b/django/core/management/commands/compilemessages.py\n@@ -63,7 +63,7 @@ class Command(BaseCommand):\n     )\n     help = 'Compiles .po files to .mo files for use with builtin gettext support.'\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n     leave_locale_alone = True\n \n     def handle(self, **options):\ndiff --git a/django/core/management/commands/createcachetable.py b/django/core/management/commands/createcachetable.py\nindex 27668f272d29..d945c95fe252 100644\n--- a/django/core/management/commands/createcachetable.py\n+++ b/django/core/management/commands/createcachetable.py\n@@ -19,7 +19,7 @@ class Command(LabelCommand):\n                 'Defaults to the \"default\" database.'),\n     )\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def handle_label(self, tablename, **options):\n         db = options.get('database')\ndiff --git a/django/core/management/commands/dbshell.py b/django/core/management/commands/dbshell.py\nindex 74659208d4fb..20047fb78f8a 100644\n--- a/django/core/management/commands/dbshell.py\n+++ b/django/core/management/commands/dbshell.py\n@@ -13,7 +13,7 @@ class Command(BaseCommand):\n                 'open a shell.  Defaults to the \"default\" database.'),\n     )\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def handle(self, **options):\n         connection = connections[options.get('database')]\ndiff --git a/django/core/management/commands/diffsettings.py b/django/core/management/commands/diffsettings.py\nindex 9e70e9ad8fdf..3155e6e648cd 100644\n--- a/django/core/management/commands/diffsettings.py\n+++ b/django/core/management/commands/diffsettings.py\n@@ -17,7 +17,7 @@ class Command(NoArgsCommand):\n                          'Default values are prefixed by \"###\".'),\n     )\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def handle_noargs(self, **options):\n         # Inspired by Postfix's \"postconf -n\".\ndiff --git a/django/core/management/commands/inspectdb.py b/django/core/management/commands/inspectdb.py\nindex 89549dc3c1bd..af560ae6e774 100644\n--- a/django/core/management/commands/inspectdb.py\n+++ b/django/core/management/commands/inspectdb.py\n@@ -18,7 +18,7 @@ class Command(NoArgsCommand):\n                 'introspect.  Defaults to using the \"default\" database.'),\n     )\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     db_module = 'django.db'\n \ndiff --git a/django/core/management/commands/makemessages.py b/django/core/management/commands/makemessages.py\nindex 7ad1a8f3aacd..91629541856b 100644\n--- a/django/core/management/commands/makemessages.py\n+++ b/django/core/management/commands/makemessages.py\n@@ -195,7 +195,7 @@ class Command(NoArgsCommand):\n \"applications) directory.\\n\\nYou must run this command with one of either the \"\n \"--locale or --all options.\")\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n     leave_locale_alone = True\n \n     def handle_noargs(self, *args, **options):\ndiff --git a/django/core/management/commands/runserver.py b/django/core/management/commands/runserver.py\nindex 402b3d342c1f..80fc5196e540 100644\n--- a/django/core/management/commands/runserver.py\n+++ b/django/core/management/commands/runserver.py\n@@ -32,7 +32,7 @@ class Command(BaseCommand):\n     args = '[optional port number, or ipaddr:port]'\n \n     # Validation is called explicitly each time the server is reloaded.\n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def get_handler(self, *args, **options):\n         \"\"\"\n@@ -94,8 +94,8 @@ def inner_run(self, *args, **options):\n         shutdown_message = options.get('shutdown_message', '')\n         quit_command = 'CTRL-BREAK' if sys.platform == 'win32' else 'CONTROL-C'\n \n-        self.stdout.write(\"Validating models...\\n\\n\")\n-        self.validate(display_num_errors=True)\n+        self.stdout.write(\"Performing system checks...\\n\\n\")\n+        self.check(display_num_errors=True)\n         self.stdout.write((\n             \"%(started_at)s\\n\"\n             \"Django version %(version)s, using settings %(settings)r\\n\"\ndiff --git a/django/core/management/commands/shell.py b/django/core/management/commands/shell.py\nindex 00a6602c0bb7..0edc0e2826ef 100644\n--- a/django/core/management/commands/shell.py\n+++ b/django/core/management/commands/shell.py\n@@ -17,7 +17,7 @@ class Command(NoArgsCommand):\n \n     )\n     help = \"Runs a Python interactive interpreter. Tries to use IPython or bpython, if one of them is available.\"\n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def _ipython_pre_011(self):\n         \"\"\"Start IPython pre-0.11\"\"\"\ndiff --git a/django/core/management/commands/test.py b/django/core/management/commands/test.py\nindex 5232c376467c..36dca9c27bee 100644\n--- a/django/core/management/commands/test.py\n+++ b/django/core/management/commands/test.py\n@@ -30,7 +30,7 @@ class Command(BaseCommand):\n     help = ('Discover and run tests in the specified modules or the current directory.')\n     args = '[path.to.modulename|path.to.modulename.TestCase|path.to.modulename.TestCase.test_method]...'\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def __init__(self):\n         self.test_runner = None\ndiff --git a/django/core/management/commands/testserver.py b/django/core/management/commands/testserver.py\nindex 97fc8ea8e1b9..b6d6da28f363 100644\n--- a/django/core/management/commands/testserver.py\n+++ b/django/core/management/commands/testserver.py\n@@ -15,7 +15,7 @@ class Command(BaseCommand):\n     help = 'Runs a development server with data from the given fixture(s).'\n     args = '[fixture ...]'\n \n-    requires_model_validation = False\n+    requires_system_checks = False\n \n     def handle(self, *fixture_labels, **options):\n         from django.core.management import call_command\ndiff --git a/django/core/management/commands/validate.py b/django/core/management/commands/validate.py\nindex 0dec3ea8b927..60fd2ce09318 100644\n--- a/django/core/management/commands/validate.py\n+++ b/django/core/management/commands/validate.py\n@@ -1,10 +1,17 @@\n-from django.core.management.base import NoArgsCommand\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n \n+import warnings\n \n-class Command(NoArgsCommand):\n-    help = \"Validates all installed models.\"\n+from django.core.management.commands.check import Command as CheckCommand\n \n-    requires_model_validation = False\n+\n+class Command(CheckCommand):\n+    help = 'Deprecated. Use \"check\" command instead. ' + CheckCommand.help\n \n     def handle_noargs(self, **options):\n-        self.validate(display_num_errors=True)\n+        warnings.warn('\"validate\" command is deprecated in favour of \"check\" '\n+            'command, which performs all system checks, including model '\n+            'validation and compatibility checks.',\n+            PendingDeprecationWarning)\n+        super(Command, self).handle_noargs(**options)\ndiff --git a/django/core/management/templates.py b/django/core/management/templates.py\nindex 164fd071e509..17a4d9cb0bcf 100644\n--- a/django/core/management/templates.py\n+++ b/django/core/management/templates.py\n@@ -52,7 +52,7 @@ class TemplateCommand(BaseCommand):\n                          'Separate multiple extensions with commas, or use '\n                          '-n multiple times.')\n         )\n-    requires_model_validation = False\n+    requires_system_checks = False\n     # Can't import settings during this command, because they haven't\n     # necessarily been created.\n     can_import_settings = False\ndiff --git a/django/core/management/validation.py b/django/core/management/validation.py\ndeleted file mode 100644\nindex 85897ee9e285..000000000000\n--- a/django/core/management/validation.py\n+++ /dev/null\n@@ -1,386 +0,0 @@\n-import collections\n-import sys\n-\n-from django.conf import settings\n-from django.core.management.color import color_style\n-from django.utils.encoding import force_str\n-from django.utils.itercompat import is_iterable\n-from django.utils import six\n-\n-\n-class ModelErrorCollection:\n-    def __init__(self, outfile=sys.stdout):\n-        self.errors = []\n-        self.outfile = outfile\n-        self.style = color_style()\n-\n-    def add(self, context, error):\n-        self.errors.append((context, error))\n-        self.outfile.write(self.style.ERROR(force_str(\"%s: %s\\n\" % (context, error))))\n-\n-\n-def get_validation_errors(outfile, app=None):\n-    \"\"\"\n-    Validates all models that are part of the specified app. If no app name is provided,\n-    validates all models of all installed apps. Writes errors, if any, to outfile.\n-    Returns number of errors.\n-    \"\"\"\n-    from django.db import models, connection\n-    from django.db.models.loading import get_app_errors\n-    from django.db.models.deletion import SET_NULL, SET_DEFAULT\n-\n-    e = ModelErrorCollection(outfile)\n-\n-    for (app_name, error) in get_app_errors().items():\n-        e.add(app_name, error)\n-\n-    for cls in models.get_models(app, include_swapped=True):\n-        opts = cls._meta\n-\n-        # Check swappable attribute.\n-        if opts.swapped:\n-            try:\n-                app_label, model_name = opts.swapped.split('.')\n-            except ValueError:\n-                e.add(opts, \"%s is not of the form 'app_label.app_name'.\" % opts.swappable)\n-                continue\n-            if not models.get_model(app_label, model_name):\n-                e.add(opts, \"Model has been swapped out for '%s' which has not been installed or is abstract.\" % opts.swapped)\n-            # No need to perform any other validation checks on a swapped model.\n-            continue\n-\n-        # If this is the current User model, check known validation problems with User models\n-        if settings.AUTH_USER_MODEL == '%s.%s' % (opts.app_label, opts.object_name):\n-            # Check that REQUIRED_FIELDS is a list\n-            if not isinstance(cls.REQUIRED_FIELDS, (list, tuple)):\n-                e.add(opts, 'The REQUIRED_FIELDS must be a list or tuple.')\n-\n-            # Check that the USERNAME FIELD isn't included in REQUIRED_FIELDS.\n-            if cls.USERNAME_FIELD in cls.REQUIRED_FIELDS:\n-                e.add(opts, 'The field named as the USERNAME_FIELD should not be included in REQUIRED_FIELDS on a swappable User model.')\n-\n-            # Check that the username field is unique\n-            if not opts.get_field(cls.USERNAME_FIELD).unique:\n-                e.add(opts, 'The USERNAME_FIELD must be unique. Add unique=True to the field parameters.')\n-\n-        # Store a list of column names which have already been used by other fields.\n-        used_column_names = []\n-\n-        # Model isn't swapped; do field-specific validation.\n-        for f in opts.local_fields:\n-            if f.name == 'id' and not f.primary_key and opts.pk.name == 'id':\n-                e.add(opts, '\"%s\": You can\\'t use \"id\" as a field name, because each model automatically gets an \"id\" field if none of the fields have primary_key=True. You need to either remove/rename your \"id\" field or add primary_key=True to a field.' % f.name)\n-            if f.name.endswith('_'):\n-                e.add(opts, '\"%s\": Field names cannot end with underscores, because this would lead to ambiguous queryset filters.' % f.name)\n-            if (f.primary_key and f.null and\n-                    not connection.features.interprets_empty_strings_as_nulls):\n-                # We cannot reliably check this for backends like Oracle which\n-                # consider NULL and '' to be equal (and thus set up\n-                # character-based fields a little differently).\n-                e.add(opts, '\"%s\": Primary key fields cannot have null=True.' % f.name)\n-\n-            # Column name validation.\n-            # Determine which column name this field wants to use.\n-            _, column_name = f.get_attname_column()\n-\n-            # Ensure the column name is not already in use.\n-            if column_name and column_name in used_column_names:\n-                e.add(opts, \"Field '%s' has column name '%s' that is already used.\" % (f.name, column_name))\n-            else:\n-                used_column_names.append(column_name)\n-\n-            if isinstance(f, models.CharField):\n-                try:\n-                    max_length = int(f.max_length)\n-                    if max_length <= 0:\n-                        e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\n-                except (ValueError, TypeError):\n-                    e.add(opts, '\"%s\": CharFields require a \"max_length\" attribute that is a positive integer.' % f.name)\n-            if isinstance(f, models.DecimalField):\n-                decimalp_ok, mdigits_ok = False, False\n-                decimalp_msg = '\"%s\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.'\n-                try:\n-                    decimal_places = int(f.decimal_places)\n-                    if decimal_places < 0:\n-                        e.add(opts, decimalp_msg % f.name)\n-                    else:\n-                        decimalp_ok = True\n-                except (ValueError, TypeError):\n-                    e.add(opts, decimalp_msg % f.name)\n-                mdigits_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute that is a positive integer.'\n-                try:\n-                    max_digits = int(f.max_digits)\n-                    if max_digits <= 0:\n-                        e.add(opts,  mdigits_msg % f.name)\n-                    else:\n-                        mdigits_ok = True\n-                except (ValueError, TypeError):\n-                    e.add(opts, mdigits_msg % f.name)\n-                invalid_values_msg = '\"%s\": DecimalFields require a \"max_digits\" attribute value that is greater than or equal to the value of the \"decimal_places\" attribute.'\n-                if decimalp_ok and mdigits_ok:\n-                    if decimal_places > max_digits:\n-                        e.add(opts, invalid_values_msg % f.name)\n-            if isinstance(f, models.FileField) and not f.upload_to:\n-                e.add(opts, '\"%s\": FileFields require an \"upload_to\" attribute.' % f.name)\n-            if isinstance(f, models.ImageField):\n-                try:\n-                    from django.utils.image import Image\n-                except ImportError:\n-                    e.add(opts, '\"%s\": To use ImageFields, you need to install Pillow. Get it at https://pypi.python.org/pypi/Pillow.' % f.name)\n-            if isinstance(f, models.BooleanField) and getattr(f, 'null', False):\n-                e.add(opts, '\"%s\": BooleanFields do not accept null values. Use a NullBooleanField instead.' % f.name)\n-            if isinstance(f, models.FilePathField) and not (f.allow_files or f.allow_folders):\n-                e.add(opts, '\"%s\": FilePathFields must have either allow_files or allow_folders set to True.' % f.name)\n-            if isinstance(f, models.GenericIPAddressField) and not getattr(f, 'null', False) and getattr(f, 'blank', False):\n-                e.add(opts, '\"%s\": GenericIPAddressField can not accept blank values if null values are not allowed, as blank values are stored as null.' % f.name)\n-            if f.choices:\n-                if isinstance(f.choices, six.string_types) or not is_iterable(f.choices):\n-                    e.add(opts, '\"%s\": \"choices\" should be iterable (e.g., a tuple or list).' % f.name)\n-                else:\n-                    for c in f.choices:\n-                        if isinstance(c, six.string_types) or not is_iterable(c) or len(c) != 2:\n-                            e.add(opts, '\"%s\": \"choices\" should be a sequence of two-item iterables (e.g. list of 2 item tuples).' % f.name)\n-            if f.db_index not in (None, True, False):\n-                e.add(opts, '\"%s\": \"db_index\" should be either None, True or False.' % f.name)\n-\n-            # Perform any backend-specific field validation.\n-            connection.validation.validate_field(e, opts, f)\n-\n-            # Check if the on_delete behavior is sane\n-            if f.rel and hasattr(f.rel, 'on_delete'):\n-                if f.rel.on_delete == SET_NULL and not f.null:\n-                    e.add(opts, \"'%s' specifies on_delete=SET_NULL, but cannot be null.\" % f.name)\n-                elif f.rel.on_delete == SET_DEFAULT and not f.has_default():\n-                    e.add(opts, \"'%s' specifies on_delete=SET_DEFAULT, but has no default value.\" % f.name)\n-\n-            # Check to see if the related field will clash with any existing\n-            # fields, m2m fields, m2m related objects or related objects\n-            if f.rel:\n-                if f.rel.to not in models.get_models():\n-                    # If the related model is swapped, provide a hint;\n-                    # otherwise, the model just hasn't been installed.\n-                    if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\n-                        e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\n-                    else:\n-                        e.add(opts, \"'%s' has a relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\n-                # it is a string and we could not find the model it refers to\n-                # so skip the next section\n-                if isinstance(f.rel.to, six.string_types):\n-                    continue\n-\n-                # Make sure the related field specified by a ForeignKey is unique\n-                if f.requires_unique_target:\n-                    if len(f.foreign_related_fields) > 1:\n-                        has_unique_field = False\n-                        for rel_field in f.foreign_related_fields:\n-                            has_unique_field = has_unique_field or rel_field.unique\n-                        if not has_unique_field:\n-                            e.add(opts, \"Field combination '%s' under model '%s' must have a unique=True constraint\" % (','.join(rel_field.name for rel_field in f.foreign_related_fields), f.rel.to.__name__))\n-                    else:\n-                        if not f.foreign_related_fields[0].unique:\n-                            e.add(opts, \"Field '%s' under model '%s' must have a unique=True constraint.\" % (f.foreign_related_fields[0].name, f.rel.to.__name__))\n-\n-                rel_opts = f.rel.to._meta\n-                rel_name = f.related.get_accessor_name()\n-                rel_query_name = f.related_query_name()\n-                if not f.rel.is_hidden():\n-                    for r in rel_opts.fields:\n-                        if r.name == rel_name:\n-                            e.add(opts, \"Accessor for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\n-                        if r.name == rel_query_name:\n-                            e.add(opts, \"Reverse query name for field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\n-                    for r in rel_opts.local_many_to_many:\n-                        if r.name == rel_name:\n-                            e.add(opts, \"Accessor for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\n-                        if r.name == rel_query_name:\n-                            e.add(opts, \"Reverse query name for field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\n-                    for r in rel_opts.get_all_related_many_to_many_objects():\n-                        if r.get_accessor_name() == rel_name:\n-                            e.add(opts, \"Accessor for field '%s' clashes with accessor for field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, r.model._meta.object_name, r.field.name, f.name))\n-                        if r.get_accessor_name() == rel_query_name:\n-                            e.add(opts, \"Reverse query name for field '%s' clashes with accessor for field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, r.model._meta.object_name, r.field.name, f.name))\n-                    for r in rel_opts.get_all_related_objects():\n-                        if r.field is not f:\n-                            if r.get_accessor_name() == rel_name:\n-                                e.add(opts, \"Accessor for field '%s' clashes with accessor for field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, r.model._meta.object_name, r.field.name, f.name))\n-                            if r.get_accessor_name() == rel_query_name:\n-                                e.add(opts, \"Reverse query name for field '%s' clashes with accessor for field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, r.model._meta.object_name, r.field.name, f.name))\n-\n-        seen_intermediary_signatures = []\n-        for i, f in enumerate(opts.local_many_to_many):\n-            # Check to see if the related m2m field will clash with any\n-            # existing fields, m2m fields, m2m related objects or related\n-            # objects\n-            if f.rel.to not in models.get_models():\n-                # If the related model is swapped, provide a hint;\n-                # otherwise, the model just hasn't been installed.\n-                if not isinstance(f.rel.to, six.string_types) and f.rel.to._meta.swapped:\n-                    e.add(opts, \"'%s' defines a relation with the model '%s.%s', which has been swapped out. Update the relation to point at settings.%s.\" % (f.name, f.rel.to._meta.app_label, f.rel.to._meta.object_name, f.rel.to._meta.swappable))\n-                else:\n-                    e.add(opts, \"'%s' has an m2m relation with model %s, which has either not been installed or is abstract.\" % (f.name, f.rel.to))\n-\n-                # it is a string and we could not find the model it refers to\n-                # so skip the next section\n-                if isinstance(f.rel.to, six.string_types):\n-                    continue\n-\n-            # Check that the field is not set to unique.  ManyToManyFields do not support unique.\n-            if f.unique:\n-                e.add(opts, \"ManyToManyFields cannot be unique.  Remove the unique argument on '%s'.\" % f.name)\n-\n-            if f.rel.through is not None and not isinstance(f.rel.through, six.string_types):\n-                from_model, to_model = cls, f.rel.to\n-                if from_model == to_model and f.rel.symmetrical and not f.rel.through._meta.auto_created:\n-                    e.add(opts, \"Many-to-many fields with intermediate tables cannot be symmetrical.\")\n-                seen_from, seen_to, seen_self = False, False, 0\n-                for inter_field in f.rel.through._meta.fields:\n-                    rel_to = getattr(inter_field.rel, 'to', None)\n-                    if from_model == to_model:  # relation to self\n-                        if rel_to == from_model:\n-                            seen_self += 1\n-                        if seen_self > 2:\n-                            e.add(opts, \"Intermediary model %s has more than \"\n-                                \"two foreign keys to %s, which is ambiguous \"\n-                                \"and is not permitted.\" % (\n-                                    f.rel.through._meta.object_name,\n-                                    from_model._meta.object_name\n-                                )\n-                            )\n-                    else:\n-                        if rel_to == from_model:\n-                            if seen_from:\n-                                e.add(opts, \"Intermediary model %s has more \"\n-                                    \"than one foreign key to %s, which is \"\n-                                    \"ambiguous and is not permitted.\" % (\n-                                        f.rel.through._meta.object_name,\n-                                         from_model._meta.object_name\n-                                     )\n-                                 )\n-                            else:\n-                                seen_from = True\n-                        elif rel_to == to_model:\n-                            if seen_to:\n-                                e.add(opts, \"Intermediary model %s has more \"\n-                                    \"than one foreign key to %s, which is \"\n-                                    \"ambiguous and is not permitted.\" % (\n-                                        f.rel.through._meta.object_name,\n-                                        rel_to._meta.object_name\n-                                    )\n-                                )\n-                            else:\n-                                seen_to = True\n-                if f.rel.through not in models.get_models(include_auto_created=True):\n-                    e.add(opts, \"'%s' specifies an m2m relation through model \"\n-                        \"%s, which has not been installed.\" % (f.name, f.rel.through)\n-                    )\n-                signature = (f.rel.to, cls, f.rel.through)\n-                if signature in seen_intermediary_signatures:\n-                    e.add(opts, \"The model %s has two manually-defined m2m \"\n-                        \"relations through the model %s, which is not \"\n-                        \"permitted. Please consider using an extra field on \"\n-                        \"your intermediary model instead.\" % (\n-                            cls._meta.object_name,\n-                            f.rel.through._meta.object_name\n-                        )\n-                    )\n-                else:\n-                    seen_intermediary_signatures.append(signature)\n-                if not f.rel.through._meta.auto_created:\n-                    seen_related_fk, seen_this_fk = False, False\n-                    for field in f.rel.through._meta.fields:\n-                        if field.rel:\n-                            if not seen_related_fk and field.rel.to == f.rel.to:\n-                                seen_related_fk = True\n-                            elif field.rel.to == cls:\n-                                seen_this_fk = True\n-                    if not seen_related_fk or not seen_this_fk:\n-                        e.add(opts, \"'%s' is a manually-defined m2m relation \"\n-                            \"through model %s, which does not have foreign keys \"\n-                            \"to %s and %s\" % (f.name, f.rel.through._meta.object_name,\n-                                f.rel.to._meta.object_name, cls._meta.object_name)\n-                        )\n-            elif isinstance(f.rel.through, six.string_types):\n-                e.add(opts, \"'%s' specifies an m2m relation through model %s, \"\n-                    \"which has not been installed\" % (f.name, f.rel.through)\n-                )\n-\n-            rel_opts = f.rel.to._meta\n-            rel_name = f.related.get_accessor_name()\n-            rel_query_name = f.related_query_name()\n-            # If rel_name is none, there is no reverse accessor (this only\n-            # occurs for symmetrical m2m relations to self). If this is the\n-            # case, there are no clashes to check for this field, as there are\n-            # no reverse descriptors for this field.\n-            if rel_name is not None:\n-                for r in rel_opts.fields:\n-                    if r.name == rel_name:\n-                        e.add(opts, \"Accessor for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\n-                    if r.name == rel_query_name:\n-                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\n-                for r in rel_opts.local_many_to_many:\n-                    if r.name == rel_name:\n-                        e.add(opts, \"Accessor for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\n-                    if r.name == rel_query_name:\n-                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, rel_opts.object_name, r.name, f.name))\n-                for r in rel_opts.get_all_related_many_to_many_objects():\n-                    if r.field is not f:\n-                        if r.get_accessor_name() == rel_name:\n-                            e.add(opts, \"Accessor for m2m field '%s' clashes with accessor for m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, r.model._meta.object_name, r.field.name, f.name))\n-                        if r.get_accessor_name() == rel_query_name:\n-                            e.add(opts, \"Reverse query name for m2m field '%s' clashes with accessor for m2m field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, r.model._meta.object_name, r.field.name, f.name))\n-                for r in rel_opts.get_all_related_objects():\n-                    if r.get_accessor_name() == rel_name:\n-                        e.add(opts, \"Accessor for m2m field '%s' clashes with accessor for field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, r.model._meta.object_name, r.field.name, f.name))\n-                    if r.get_accessor_name() == rel_query_name:\n-                        e.add(opts, \"Reverse query name for m2m field '%s' clashes with accessor for field '%s.%s'. Add a related_name argument to the definition for '%s'.\" % (f.name, r.model._meta.object_name, r.field.name, f.name))\n-\n-        # Check ordering attribute.\n-        if opts.ordering:\n-            for field_name in opts.ordering:\n-                if field_name == '?':\n-                    continue\n-                if field_name.startswith('-'):\n-                    field_name = field_name[1:]\n-                if opts.order_with_respect_to and field_name == '_order':\n-                    continue\n-                # Skip ordering in the format field1__field2 (FIXME: checking\n-                # this format would be nice, but it's a little fiddly).\n-                if '__' in field_name:\n-                    continue\n-                # Skip ordering on pk. This is always a valid order_by field\n-                # but is an alias and therefore won't be found by opts.get_field.\n-                if field_name == 'pk':\n-                    continue\n-                try:\n-                    opts.get_field(field_name, many_to_many=False)\n-                except models.FieldDoesNotExist:\n-                    e.add(opts, '\"ordering\" refers to \"%s\", a field that doesn\\'t exist.' % field_name)\n-\n-        # Check unique_together.\n-        for ut in opts.unique_together:\n-            validate_local_fields(e, opts, \"unique_together\", ut)\n-        if not isinstance(opts.index_together, collections.Sequence):\n-            e.add(opts, '\"index_together\" must a sequence')\n-        else:\n-            for it in opts.index_together:\n-                validate_local_fields(e, opts, \"index_together\", it)\n-\n-    return len(e.errors)\n-\n-\n-def validate_local_fields(e, opts, field_name, fields):\n-    from django.db import models\n-\n-    if not isinstance(fields, collections.Sequence):\n-        e.add(opts, 'all %s elements must be sequences' % field_name)\n-    else:\n-        for field in fields:\n-            try:\n-                f = opts.get_field(field, many_to_many=True)\n-            except models.FieldDoesNotExist:\n-                e.add(opts, '\"%s\" refers to %s, a field that doesn\\'t exist.' % (field_name, field))\n-            else:\n-                if isinstance(f.rel, models.ManyToManyRel):\n-                    e.add(opts, '\"%s\" refers to %s. ManyToManyFields are not supported in %s.' % (field_name, f.name, field_name))\n-                if f not in opts.local_fields:\n-                    e.add(opts, '\"%s\" refers to %s. This is not in the same model as the %s statement.' % (field_name, f.name, field_name))\ndiff --git a/django/db/backends/__init__.py b/django/db/backends/__init__.py\nindex 8dd15dfee133..07813b0c45fd 100644\n--- a/django/db/backends/__init__.py\n+++ b/django/db/backends/__init__.py\n@@ -1,5 +1,6 @@\n import datetime\n import time\n+from types import MethodType\n \n from django.db.utils import DatabaseError, ProgrammingError\n \n@@ -12,6 +13,7 @@\n from importlib import import_module\n \n from django.conf import settings\n+from django.core import checks\n from django.db import DEFAULT_DB_ALIAS\n from django.db.backends.signals import connection_created\n from django.db.backends import utils\n@@ -1405,5 +1407,24 @@ def __init__(self, connection):\n         self.connection = connection\n \n     def validate_field(self, errors, opts, f):\n-        \"By default, there is no backend-specific validation\"\n+        \"\"\"\n+        By default, there is no backend-specific validation.\n+\n+        This method has been deprecated by the new checks framework. New\n+        backends should implement check_field instead.\n+\n+        \"\"\"\n+\n         pass\n+\n+    def check_field(self, field, **kwargs):\n+        class ErrorList(list):\n+            def add(self, opts, error_message):\n+                self.append(checks.Error(error_message, hint=None, obj=field))\n+\n+        errors = ErrorList()\n+        # Some tests create fields in isolation -- the fields are not attached\n+        # to any model, so they have no `model` attribute.\n+        opts = field.model._meta if hasattr(field, 'model') else None\n+        self.validate_field(errors, field, opts)\n+        return list(errors)\ndiff --git a/django/db/backends/mysql/validation.py b/django/db/backends/mysql/validation.py\nindex 17b7cde75661..d4a4a0685be7 100644\n--- a/django/db/backends/mysql/validation.py\n+++ b/django/db/backends/mysql/validation.py\n@@ -1,17 +1,34 @@\n+from django.core import checks\n from django.db.backends import BaseDatabaseValidation\n \n \n class DatabaseValidation(BaseDatabaseValidation):\n-    def validate_field(self, errors, opts, f):\n+    def check_field(self, field, **kwargs):\n         \"\"\"\n         MySQL has the following field length restriction:\n         No character (varchar) fields can have a length exceeding 255\n         characters if they have a unique index on them.\n         \"\"\"\n-        from django.db import models\n-        varchar_fields = (models.CharField, models.CommaSeparatedIntegerField,\n-                models.SlugField)\n-        if (isinstance(f, varchar_fields) and f.unique\n-                and (f.max_length is None or int(f.max_length) > 255)):\n-            msg = '\"%(name)s\": %(cls)s cannot have a \"max_length\" greater than 255 when using \"unique=True\".'\n-            errors.add(opts, msg % {'name': f.name, 'cls': f.__class__.__name__})\n+        from django.db import connection\n+\n+        errors = super(DatabaseValidation, self).check_field(field, **kwargs)\n+        try:\n+            field_type = field.db_type(connection)\n+        except AttributeError:\n+            # If the field is a relative field and the target model is\n+            # missing, then field.rel.to is not a model and doesn't have\n+            # `_meta` attribute.\n+            field_type = ''\n+\n+        if (field_type.startswith('varchar') and field.unique\n+            and (field.max_length is None or int(field.max_length) > 255)):\n+            errors.append(\n+                checks.Error(\n+                    'Under mysql backend, the field cannot have a \"max_length\" '\n+                        'greated than 255 when it is unique.',\n+                    hint=None,\n+                    obj=field,\n+                    id='E047',\n+                )\n+            )\n+        return errors\ndiff --git a/django/db/models/base.py b/django/db/models/base.py\nindex a9d8695f211c..fadc0d850c02 100644\n--- a/django/db/models/base.py\n+++ b/django/db/models/base.py\n@@ -7,6 +7,7 @@\n \n import django.db.models.manager  # Imported to register signal handler.\n from django.conf import settings\n+from django.core import checks\n from django.core.exceptions import (ObjectDoesNotExist,\n     MultipleObjectsReturned, FieldError, ValidationError, NON_FIELD_ERRORS)\n from django.db.models.fields import AutoField, FieldDoesNotExist\n@@ -1011,6 +1012,308 @@ def clean_fields(self, exclude=None):\n         if errors:\n             raise ValidationError(errors)\n \n+    @classmethod\n+    def check(cls, **kwargs):\n+        errors = []\n+        errors.extend(cls._check_swappable())\n+        errors.extend(cls._check_managers(**kwargs))\n+        if not cls._meta.swapped:\n+            errors.extend(cls._check_fields(**kwargs))\n+            errors.extend(cls._check_m2m_through_same_relationship())\n+            errors.extend(cls._check_id_field())\n+            errors.extend(cls._check_column_name_clashes())\n+            errors.extend(cls._check_index_together())\n+            errors.extend(cls._check_unique_together())\n+            errors.extend(cls._check_ordering())\n+        return errors\n+\n+    @classmethod\n+    def _check_swappable(cls):\n+        \"\"\" Check if the swapped model exists. \"\"\"\n+\n+        errors = []\n+        if cls._meta.swapped:\n+            try:\n+                app_label, model_name = cls._meta.swapped.split('.')\n+            except ValueError:\n+                errors.append(\n+                    checks.Error(\n+                        '\"%s\" is not of the form \"app_label.app_name\".'\n+                            % cls._meta.swappable,\n+                        hint=None,\n+                        obj=cls,\n+                        id='E002',\n+                    )\n+                )\n+            else:\n+                if not get_model(app_label, model_name):\n+                    errors.append(\n+                        checks.Error(\n+                            'The model has been swapped out for %s.%s '\n+                                'which has not been installed or is abstract.'\n+                                % (app_label, model_name),\n+                            hint='Ensure that you did not misspell the model '\n+                                'name and the app name as well as the model '\n+                                'is not abstract. Does your INSTALLED_APPS '\n+                                'setting contain the \"%s\" app?'\n+                                % app_label,\n+                            obj=cls,\n+                            id='E003',\n+                        )\n+                    )\n+        return errors\n+\n+    @classmethod\n+    def _check_managers(cls, **kwargs):\n+        \"\"\" Perform all manager checks. \"\"\"\n+\n+        errors = []\n+        managers = cls._meta.concrete_managers + cls._meta.abstract_managers\n+        for (_, _, manager) in managers:\n+            errors.extend(manager.check(**kwargs))\n+        return errors\n+\n+    @classmethod\n+    def _check_fields(cls, **kwargs):\n+        \"\"\" Perform all field checks. \"\"\"\n+\n+        errors = []\n+        for field in cls._meta.local_fields:\n+            errors.extend(field.check(**kwargs))\n+        for field in cls._meta.local_many_to_many:\n+            errors.extend(field.check(from_model=cls, **kwargs))\n+        return errors\n+\n+    @classmethod\n+    def _check_m2m_through_same_relationship(cls):\n+        \"\"\" Check if no relationship model is used by more than one m2m field.\n+        \"\"\"\n+\n+        errors = []\n+        seen_intermediary_signatures = []\n+\n+        fields = cls._meta.local_many_to_many\n+\n+        # Skip when the target model wasn't found.\n+        fields = (f for f in fields if isinstance(f.rel.to, ModelBase))\n+\n+        # Skip when the relationship model wasn't found.\n+        fields = (f for f in fields if isinstance(f.rel.through, ModelBase))\n+\n+        for f in fields:\n+            signature = (f.rel.to, cls, f.rel.through)\n+            if signature in seen_intermediary_signatures:\n+                errors.append(\n+                    checks.Error(\n+                        'The model has two many-to-many relations through '\n+                            'the intermediary %s model, which is not permitted.'\n+                            % f.rel.through._meta.object_name,\n+                        hint=None,\n+                        obj=cls,\n+                        id='E004',\n+                    )\n+                )\n+            else:\n+                seen_intermediary_signatures.append(signature)\n+        return errors\n+\n+    @classmethod\n+    def _check_id_field(cls):\n+        \"\"\" Check if `id` field is a primary key. \"\"\"\n+\n+        fields = list(f for f in cls._meta.local_fields\n+            if f.name == 'id' and f != cls._meta.pk)\n+        # fields is empty or consists of the invalid \"id\" field\n+        if fields and not fields[0].primary_key and cls._meta.pk.name == 'id':\n+            return [\n+                checks.Error(\n+                    'You cannot use \"id\" as a field name, because each model '\n+                        'automatically gets an \"id\" field if none '\n+                        'of the fields have primary_key=True.',\n+                    hint='Remove or rename \"id\" field '\n+                        'or add primary_key=True to a field.',\n+                    obj=cls,\n+                    id='E005',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    @classmethod\n+    def _check_column_name_clashes(cls):\n+        # Store a list of column names which have already been used by other fields.\n+        used_column_names = []\n+        errors = []\n+\n+        for f in cls._meta.local_fields:\n+            _, column_name = f.get_attname_column()\n+\n+            # Ensure the column name is not already in use.\n+            if column_name and column_name in used_column_names:\n+                errors.append(\n+                    checks.Error(\n+                        'Field \"%s\" has column name \"%s\" that is already used.'\n+                            % (f.name, column_name),\n+                        hint=None,\n+                        obj=cls,\n+                    )\n+                )\n+            else:\n+                used_column_names.append(column_name)\n+\n+        return errors\n+\n+    @classmethod\n+    def _check_index_together(cls):\n+        \"\"\" Check the value of \"index_together\" option. \"\"\"\n+\n+        if not isinstance(cls._meta.index_together, (tuple, list)):\n+            return [\n+                checks.Error(\n+                    '\"index_together\" must be a list or tuple.',\n+                    hint=None,\n+                    obj=cls,\n+                    id='E006',\n+                )\n+            ]\n+\n+        elif any(not isinstance(fields, (tuple, list))\n+                for fields in cls._meta.index_together):\n+            return [\n+                checks.Error(\n+                    'All \"index_together\" elements must be lists or tuples.',\n+                    hint=None,\n+                    obj=cls,\n+                    id='E007',\n+                )\n+            ]\n+\n+        else:\n+            errors = []\n+            for fields in cls._meta.index_together:\n+                errors.extend(cls._check_local_fields(fields, \"index_together\"))\n+            return errors\n+\n+    @classmethod\n+    def _check_unique_together(cls):\n+        \"\"\" Check the value of \"unique_together\" option. \"\"\"\n+\n+        if not isinstance(cls._meta.unique_together, (tuple, list)):\n+            return [\n+                checks.Error(\n+                    '\"unique_together\" must be a list or tuple.',\n+                    hint=None,\n+                    obj=cls,\n+                    id='E008',\n+                )\n+            ]\n+\n+        elif any(not isinstance(fields, (tuple, list))\n+                for fields in cls._meta.unique_together):\n+            return [\n+                checks.Error(\n+                    'All \"unique_together\" elements must be lists or tuples.',\n+                    hint=None,\n+                    obj=cls,\n+                    id='E009',\n+                )\n+            ]\n+\n+        else:\n+            errors = []\n+            for fields in cls._meta.unique_together:\n+                errors.extend(cls._check_local_fields(fields, \"unique_together\"))\n+            return errors\n+\n+    @classmethod\n+    def _check_local_fields(cls, fields, option):\n+        from django.db import models\n+\n+        errors = []\n+        for field_name in fields:\n+            try:\n+                field = cls._meta.get_field(field_name,\n+                    many_to_many=True)\n+            except models.FieldDoesNotExist:\n+                errors.append(\n+                    checks.Error(\n+                        '\"%s\" points to a missing field named \"%s\".'\n+                            % (option, field_name),\n+                        hint='Ensure that you did not misspell the field name.',\n+                        obj=cls,\n+                        id='E010',\n+                    )\n+                )\n+            else:\n+                if isinstance(field.rel, models.ManyToManyRel):\n+                    errors.append(\n+                        checks.Error(\n+                            '\"%s\" refers to a m2m \"%s\" field, but '\n+                                'ManyToManyFields are not supported in \"%s\".'\n+                                % (option, field_name, option),\n+                            hint=None,\n+                            obj=cls,\n+                            id='E011',\n+                        )\n+                    )\n+        return errors\n+\n+    @classmethod\n+    def _check_ordering(cls):\n+        \"\"\" Check \"ordering\" option -- is it a list of lists and do all fields\n+        exist? \"\"\"\n+\n+        from django.db.models import FieldDoesNotExist\n+\n+        if not cls._meta.ordering:\n+            return []\n+\n+        if not isinstance(cls._meta.ordering, (list, tuple)):\n+            return [\n+                checks.Error(\n+                    '\"ordering\" must be a tuple or list '\n+                        '(even if you want to order by only one field).',\n+                    hint=None,\n+                    obj=cls,\n+                    id='E012',\n+                )\n+            ]\n+\n+        errors = []\n+\n+        fields = cls._meta.ordering\n+\n+        # Skip '?' fields.\n+        fields = (f for f in fields if f != '?')\n+\n+        # Convert \"-field\" to \"field\".\n+        fields = ((f[1:] if f.startswith('-') else f) for f in fields)\n+\n+        fields = (f for f in fields if\n+            f != '_order' or not cls._meta.order_with_respect_to)\n+\n+        # Skip ordering in the format field1__field2 (FIXME: checking\n+        # this format would be nice, but it's a little fiddly).\n+        fields = (f for f in fields if '__' not in f)\n+\n+        # Skip ordering on pk. This is always a valid order_by field\n+        # but is an alias and therefore won't be found by opts.get_field.\n+        fields = (f for f in fields if f != 'pk')\n+\n+        for field_name in fields:\n+            try:\n+                cls._meta.get_field(field_name, many_to_many=False)\n+            except FieldDoesNotExist:\n+                errors.append(\n+                    checks.Error(\n+                        '\"ordering\" pointing to a missing \"%s\" field.' % field_name,\n+                        hint='Ensure that you did not misspell the field name.',\n+                        obj=cls,\n+                        id='E013',\n+                    )\n+                )\n+        return errors\n+\n \n ############################################\n # HELPER FUNCTIONS (CURRIED MODEL METHODS) #\ndiff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex 254e6854dfce..b7fd565a88f2 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -1,3 +1,4 @@\n+# -*- coding: utf-8 -*-\n from __future__ import unicode_literals\n \n import collections\n@@ -14,16 +15,18 @@\n from django.db.models.query_utils import QueryWrapper\n from django.conf import settings\n from django import forms\n-from django.core import exceptions, validators\n+from django.core import exceptions, validators, checks\n from django.utils.datastructures import DictWrapper\n from django.utils.dateparse import parse_date, parse_datetime, parse_time\n from django.utils.functional import curry, total_ordering, Promise\n from django.utils.text import capfirst\n from django.utils import timezone\n from django.utils.translation import ugettext_lazy as _\n-from django.utils.encoding import smart_text, force_text, force_bytes\n+from django.utils.encoding import (smart_text, force_text, force_bytes,\n+    python_2_unicode_compatible)\n from django.utils.ipv6 import clean_ipv6_address\n from django.utils import six\n+from django.utils.itercompat import is_iterable\n \n \n class Empty(object):\n@@ -68,6 +71,7 @@ def _empty(of_cls):\n \n \n @total_ordering\n+@python_2_unicode_compatible\n class Field(object):\n     \"\"\"Base class for all field types\"\"\"\n \n@@ -683,6 +687,101 @@ def value_from_object(self, obj):\n         \"\"\"\n         return getattr(obj, self.attname)\n \n+    def check(self, **kwargs):\n+        errors = []\n+        errors.extend(self._check_field_name())\n+        errors.extend(self._check_choices())\n+        errors.extend(self._check_db_index())\n+        errors.extend(self._check_null_allowed_for_primary_keys())\n+        errors.extend(self._check_backend_specific_checks(**kwargs))\n+        return errors\n+\n+    def _check_field_name(self):\n+        \"\"\" Check if field name is valid (i. e. not ending with an underscore).\n+        \"\"\"\n+        if self.name.endswith('_'):\n+            return [\n+                checks.Error(\n+                    'Field names must not end with underscores.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E001',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_choices(self):\n+        if self.choices:\n+            if (isinstance(self.choices, six.string_types) or\n+                    not is_iterable(self.choices)):\n+                return [\n+                    checks.Error(\n+                        '\"choices\" must be an iterable (e.g., a list or tuple).',\n+                        hint=None,\n+                        obj=self,\n+                        id='E033',\n+                    )\n+                ]\n+            elif any(isinstance(choice, six.string_types) or\n+                     not is_iterable(choice) or len(choice) != 2\n+                     for choice in self.choices):\n+                return [\n+                    checks.Error(\n+                        'All \"choices\" elements must be a tuple of two '\n+                            'elements (the first one is the actual value '\n+                            'to be stored and the second element is '\n+                            'the human-readable name).',\n+                        hint=None,\n+                        obj=self,\n+                        id='E034',\n+                    )\n+                ]\n+            else:\n+                return []\n+        else:\n+            return []\n+\n+    def _check_db_index(self):\n+        if self.db_index not in (None, True, False):\n+            return [\n+                checks.Error(\n+                    '\"db_index\" must be either None, True or False.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E035',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_null_allowed_for_primary_keys(self):\n+        if (self.primary_key and self.null and\n+                not connection.features.interprets_empty_strings_as_nulls):\n+            # We cannot reliably check this for backends like Oracle which\n+            # consider NULL and '' to be equal (and thus set up\n+            # character-based fields a little differently).\n+            return [\n+                checks.Error(\n+                    'Primary keys must not have null=True.',\n+                    hint='Set null=False on the field or '\n+                        'remove primary_key=True argument.',\n+                    obj=self,\n+                    id='E036',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_backend_specific_checks(self, **kwargs):\n+        return connection.validation.check_field(self, **kwargs)\n+\n+    def __str__(self):\n+        \"\"\" Return \"app_label.model_label.field_name\". \"\"\"\n+        model = self.model\n+        app = model._meta.app_label\n+        return '%s.%s.%s' % (app, model._meta.object_name, self.name)\n+\n     def __repr__(self):\n         \"\"\"\n         Displays the module, class and name of the field.\n@@ -703,8 +802,6 @@ class AutoField(Field):\n     }\n \n     def __init__(self, *args, **kwargs):\n-        assert kwargs.get('primary_key', False) is True, \\\n-               \"%ss must have primary_key=True.\" % self.__class__.__name__\n         kwargs['blank'] = True\n         Field.__init__(self, *args, **kwargs)\n \n@@ -754,6 +851,24 @@ def contribute_to_class(self, cls, name):\n     def formfield(self, **kwargs):\n         return None\n \n+    def check(self, **kwargs):\n+        errors = super(AutoField, self).check(**kwargs)\n+        errors.extend(self._check_primary_key())\n+        return errors\n+\n+    def _check_primary_key(self):\n+        if not self.primary_key:\n+            return [\n+                checks.Error(\n+                    'The field must have primary_key=True, because it is an AutoField.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E048',\n+                ),\n+            ]\n+        else:\n+            return []\n+\n \n class BooleanField(Field):\n     empty_strings_allowed = False\n@@ -816,6 +931,24 @@ def formfield(self, **kwargs):\n         defaults.update(kwargs)\n         return super(BooleanField, self).formfield(**defaults)\n \n+    def check(self, **kwargs):\n+        errors = super(BooleanField, self).check(**kwargs)\n+        errors.extend(self._check_null(**kwargs))\n+        return errors\n+\n+    def _check_null(self, **kwargs):\n+        if getattr(self, 'null', False):\n+            return [\n+                checks.Error(\n+                    'BooleanFields do not acceps null values.',\n+                    hint='Use a NullBooleanField instead.',\n+                    obj=self,\n+                    id='E037',\n+                )\n+            ]\n+        else:\n+            return []\n+\n \n class CharField(Field):\n     description = _(\"String (up to %(max_length)s)\")\n@@ -844,6 +977,37 @@ def formfield(self, **kwargs):\n         defaults.update(kwargs)\n         return super(CharField, self).formfield(**defaults)\n \n+    def check(self, **kwargs):\n+        errors = super(CharField, self).check(**kwargs)\n+        errors.extend(self._check_max_length_attibute(**kwargs))\n+        return errors\n+\n+    def _check_max_length_attibute(self, **kwargs):\n+        try:\n+            max_length = int(self.max_length)\n+            if max_length <= 0:\n+                raise ValueError()\n+        except TypeError:\n+            return [\n+                checks.Error(\n+                    'The field must have \"max_length\" attribute.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E038',\n+                )\n+            ]\n+        except ValueError:\n+            return [\n+                checks.Error(\n+                    '\"max_length\" must be a positive integer.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E039',\n+                )\n+            ]\n+        else:\n+            return []\n+\n \n # TODO: Maybe move this into contrib, because it's specialized.\n class CommaSeparatedIntegerField(CharField):\n@@ -1148,6 +1312,77 @@ def formfield(self, **kwargs):\n         defaults.update(kwargs)\n         return super(DecimalField, self).formfield(**defaults)\n \n+    def check(self, **kwargs):\n+        errors = super(DecimalField, self).check(**kwargs)\n+        errors.extend(self._check_decimal_places_and_max_digits(**kwargs))\n+        return errors\n+\n+    def _check_decimal_places_and_max_digits(self, **kwargs):\n+        errors = self.__check_decimal_places()\n+        errors += self.__check_max_digits()\n+        if not errors and int(self.decimal_places) > int(self.max_digits):\n+            errors.append(\n+                checks.Error(\n+                    '\"max_digits\" must be greater or equal to \"decimal_places\".',\n+                    hint=None,\n+                    obj=self,\n+                    id='E040',\n+                )\n+            )\n+        return errors\n+\n+    def __check_decimal_places(self):\n+        try:\n+            decimal_places = int(self.decimal_places)\n+            if decimal_places < 0:\n+                raise ValueError()\n+        except TypeError:\n+            return [\n+                checks.Error(\n+                    'The field requires a \"decimal_places\" attribute.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E041',\n+                )\n+            ]\n+        except ValueError:\n+            return [\n+                checks.Error(\n+                    '\"decimal_places\" attribute must be a non-negative integer.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E042',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def __check_max_digits(self):\n+        try:\n+            max_digits = int(self.max_digits)\n+            if max_digits <= 0:\n+                raise ValueError()\n+        except TypeError:\n+            return [\n+                checks.Error(\n+                    'The field requires a \"max_digits\" attribute.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E043',\n+                )\n+            ]\n+        except ValueError:\n+            return [\n+                checks.Error(\n+                    '\"max_digits\" attribute must be a positive integer.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E044',\n+                )\n+            ]\n+        else:\n+            return []\n+\n \n class EmailField(CharField):\n     default_validators = [validators.validate_email]\n@@ -1217,6 +1452,23 @@ def formfield(self, **kwargs):\n     def get_internal_type(self):\n         return \"FilePathField\"\n \n+    def check(self, **kwargs):\n+        errors = super(FilePathField, self).check(**kwargs)\n+        errors.extend(self._check_allowing_files_or_folders(**kwargs))\n+        return errors\n+\n+    def _check_allowing_files_or_folders(self, **kwargs):\n+        if not self.allow_files and not self.allow_folders:\n+            return [\n+                checks.Error(\n+                    'The field must have either \"allow_files\" or \"allow_folders\" set to True.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E045',\n+                )\n+            ]\n+        return []\n+\n \n class FloatField(Field):\n     empty_strings_allowed = False\n@@ -1387,6 +1639,24 @@ def formfield(self, **kwargs):\n         defaults.update(kwargs)\n         return super(GenericIPAddressField, self).formfield(**defaults)\n \n+    def check(self, **kwargs):\n+        errors = super(GenericIPAddressField, self).check(**kwargs)\n+        errors.extend(self._check_blank_and_null_values(**kwargs))\n+        return errors\n+\n+    def _check_blank_and_null_values(self, **kwargs):\n+        if not getattr(self, 'null', False) and getattr(self, 'blank', False):\n+            return [\n+                checks.Error(\n+                    'The field cannot accept blank values if null values '\n+                        'are not allowed, as blank values are stored as null.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E046',\n+                )\n+            ]\n+        return []\n+\n \n class NullBooleanField(Field):\n     empty_strings_allowed = False\ndiff --git a/django/db/models/fields/files.py b/django/db/models/fields/files.py\nindex 557ec6ec8a2d..d4c18401b996 100644\n--- a/django/db/models/fields/files.py\n+++ b/django/db/models/fields/files.py\n@@ -3,6 +3,8 @@\n \n from django import forms\n from django.db.models.fields import Field\n+from django.core import checks\n+from django.core.exceptions import ImproperlyConfigured\n from django.core.files.base import File\n from django.core.files.storage import default_storage\n from django.core.files.images import ImageFile\n@@ -220,9 +222,8 @@ class FileField(Field):\n     description = _(\"File\")\n \n     def __init__(self, verbose_name=None, name=None, upload_to='', storage=None, **kwargs):\n-        for arg in ('primary_key', 'unique'):\n-            if arg in kwargs:\n-                raise TypeError(\"'%s' is not a valid argument for %s.\" % (arg, self.__class__))\n+        self._primary_key_set_explicitly = 'primary_key' in kwargs\n+        self._unique_set_explicitly = 'unique' in kwargs\n \n         self.storage = storage or default_storage\n         self.upload_to = upload_to\n@@ -304,6 +305,54 @@ def formfield(self, **kwargs):\n         defaults.update(kwargs)\n         return super(FileField, self).formfield(**defaults)\n \n+    def check(self, **kwargs):\n+        errors = super(FileField, self).check(**kwargs)\n+        errors.extend(self._check_upload_to())\n+        errors.extend(self._check_unique())\n+        errors.extend(self._check_primary_key())\n+        return errors\n+\n+    def _check_upload_to(self):\n+        if not self.upload_to:\n+            return [\n+                checks.Error(\n+                    'The field requires an \"upload_to\" attribute.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E031',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_unique(self):\n+        if self._unique_set_explicitly:\n+            return [\n+                checks.Error(\n+                    '\"unique\" is not a valid argument for %s.'\n+                        % self.__class__.__name__,\n+                    hint=None,\n+                    obj=self,\n+                    id='E049',\n+                )\n+            ]\n+        else:\n+            return []\n+\n+    def _check_primary_key(self):\n+        if self._primary_key_set_explicitly:\n+            return [\n+                checks.Error(\n+                    '\"primary_key\" is not a valid argument for %s.'\n+                        % self.__class__.__name__,\n+                    hint=None,\n+                    obj=self,\n+                    id='E050',\n+                )\n+            ]\n+        else:\n+            return []\n+\n \n class ImageFileDescriptor(FileDescriptor):\n     \"\"\"\n@@ -422,3 +471,25 @@ def formfield(self, **kwargs):\n         defaults = {'form_class': forms.ImageField}\n         defaults.update(kwargs)\n         return super(ImageField, self).formfield(**defaults)\n+\n+    def check(self, **kwargs):\n+        errors = super(ImageField, self).check(**kwargs)\n+        errors.extend(self._check_image_library_installed())\n+        return errors\n+\n+    def _check_image_library_installed(self):\n+        try:\n+            from django.utils.image import Image\n+        except ImproperlyConfigured:\n+            return [\n+                checks.Error(\n+                    'To use ImageFields, Pillow must be installed.',\n+                    hint='Get Pillow at https://pypi.python.org/pypi/Pillow '\n+                        'or run command \"pip install pillow\".',\n+                    obj=self,\n+                    id='E032',\n+                )\n+            ]\n+        else:\n+            return []\n+\ndiff --git a/django/db/models/fields/related.py b/django/db/models/fields/related.py\nindex 8252cce6726e..d757bc9d82f7 100644\n--- a/django/db/models/fields/related.py\n+++ b/django/db/models/fields/related.py\n@@ -1,13 +1,15 @@\n from operator import attrgetter\n \n+from django.core import checks\n from django.db import connection, connections, router, transaction\n from django.db.backends import utils\n from django.db.models import signals\n+from django.db.models.deletion import SET_NULL, SET_DEFAULT, CASCADE\n from django.db.models.fields import (AutoField, Field, IntegerField,\n     PositiveIntegerField, PositiveSmallIntegerField, FieldDoesNotExist)\n-from django.db.models.related import RelatedObject, PathInfo\n+from django.db.models.loading import get_models\n from django.db.models.query import QuerySet\n-from django.db.models.deletion import CASCADE\n+from django.db.models.related import RelatedObject, PathInfo\n from django.utils.encoding import smart_text\n from django.utils import six\n from django.utils.deprecation import RenameMethodsBase\n@@ -16,6 +18,7 @@\n from django.core import exceptions\n from django import forms\n \n+\n RECURSIVE_RELATIONSHIP_CONSTANT = 'self'\n \n def add_lazy_relation(cls, field, relation, operation):\n@@ -137,6 +140,167 @@ def related_query_name(self):\n         # \"related_name\" option.\n         return self.rel.related_query_name or self.rel.related_name or self.opts.model_name\n \n+    def check(self, **kwargs):\n+        errors = super(RelatedField, self).check(**kwargs)\n+        errors.extend(self._check_relation_model_exists())\n+        errors.extend(self._check_referencing_to_swapped_model())\n+        errors.extend(self._check_clashes())\n+        return errors\n+\n+    def _check_relation_model_exists(self):\n+        rel_is_missing = self.rel.to not in get_models()\n+        rel_is_string = isinstance(self.rel.to, six.string_types)\n+        model_name = self.rel.to if rel_is_string else self.rel.to._meta.object_name\n+        if rel_is_missing and (rel_is_string or not self.rel.to._meta.swapped):\n+            return [\n+                checks.Error(\n+                    'The field has a relation with model %s, which '\n+                        'has either not been installed or is abstract.'\n+                        % model_name,\n+                    hint='Ensure that you did not misspell the model name and '\n+                        'the model is not abstract. Does your INSTALLED_APPS '\n+                        'setting contain the app where %s is defined?'\n+                        % model_name,\n+                    obj=self,\n+                    id='E030',\n+                )\n+            ]\n+        return []\n+\n+    def _check_referencing_to_swapped_model(self):\n+        if (self.rel.to not in get_models() and\n+                not isinstance(self.rel.to, six.string_types) and\n+                self.rel.to._meta.swapped):\n+            model = \"%s.%s\" % (\n+                self.rel.to._meta.app_label,\n+                self.rel.to._meta.object_name\n+            )\n+            return [\n+                checks.Error(\n+                    'The field defines a relation with the model %s, '\n+                        'which has been swapped out.' % model,\n+                    hint='Update the relation to point at settings.%s'\n+                        % self.rel.to._meta.swappable,\n+                    obj=self,\n+                    id='E029',\n+                )\n+            ]\n+        return []\n+\n+    def _check_clashes(self):\n+        \"\"\" Check accessor and reverse query name clashes. \"\"\"\n+\n+        from django.db.models.base import ModelBase\n+\n+        errors = []\n+        opts = self.model._meta\n+\n+        # `f.rel.to` may be a string instead of a model. Skip if model name is\n+        # not resolved.\n+        if not isinstance(self.rel.to, ModelBase):\n+            return []\n+\n+        # If the field doesn't install backward relation on the target model (so\n+        # `is_hidden` returns True), then there are no clashes to check and we\n+        # can skip these fields.\n+        if self.rel.is_hidden():\n+            return []\n+\n+        try:\n+            self.related\n+        except AttributeError:\n+            return []\n+\n+        # Consider that we are checking field `Model.foreign` and the models\n+        # are:\n+        #\n+        #     class Target(models.Model):\n+        #         model = models.IntegerField()\n+        #         model_set = models.IntegerField()\n+        #\n+        #     class Model(models.Model):\n+        #         foreign = models.ForeignKey(Target)\n+        #         m2m = models.ManyToManyField(Target)\n+\n+        rel_opts = self.rel.to._meta\n+        # rel_opts.object_name == \"Target\"\n+        rel_name = self.related.get_accessor_name()  # i. e. \"model_set\"\n+        rel_query_name = self.related_query_name()  # i. e. \"model\"\n+        field_name = \"%s.%s\" % (opts.object_name,\n+            self.name)  # i. e. \"Model.field\"\n+\n+        # Check clashes between accessor or reverse query name of `field`\n+        # and any other field name -- i. e. accessor for Model.foreign is\n+        # model_set and it clashes with Target.model_set.\n+        potential_clashes = rel_opts.fields + rel_opts.local_many_to_many\n+        for clash_field in potential_clashes:\n+            clash_name = \"%s.%s\" % (rel_opts.object_name,\n+                clash_field.name)  # i. e. \"Target.model_set\"\n+            if clash_field.name == rel_name:\n+                errors.append(\n+                    checks.Error(\n+                        'Accessor for field %s clashes with field %s.'\n+                            % (field_name, clash_name),\n+                        hint='Rename field %s or add/change a related_name '\n+                            'argument to the definition for field %s.'\n+                            % (clash_name, field_name),\n+                        obj=self,\n+                        id='E014',\n+                    )\n+                )\n+\n+            if clash_field.name == rel_query_name:\n+                errors.append(\n+                    checks.Error(\n+                        'Reverse query name for field %s clashes with field %s.'\n+                            % (field_name, clash_name),\n+                        hint='Rename field %s or add/change a related_name '\n+                            'argument to the definition for field %s.'\n+                            % (clash_name, field_name),\n+                        obj=self,\n+                        id='E015',\n+                    )\n+                )\n+\n+        # Check clashes between accessors/reverse query names of `field` and\n+        # any other field accessor -- i. e. Model.foreign accessor clashes with\n+        # Model.m2m accessor.\n+        potential_clashes = rel_opts.get_all_related_many_to_many_objects()\n+        potential_clashes += rel_opts.get_all_related_objects()\n+        potential_clashes = (r for r in potential_clashes\n+            if r.field is not self)\n+        for clash_field in potential_clashes:\n+            clash_name = \"%s.%s\" % (  # i. e. \"Model.m2m\"\n+                clash_field.model._meta.object_name,\n+                clash_field.field.name)\n+            if clash_field.get_accessor_name() == rel_name:\n+                errors.append(\n+                    checks.Error(\n+                        'Clash between accessors for %s and %s.'\n+                            % (field_name, clash_name),\n+                        hint='Add or change a related_name argument '\n+                            'to the definition for %s or %s.'\n+                            % (field_name, clash_name),\n+                        obj=self,\n+                        id='E016',\n+                    )\n+                )\n+\n+            if clash_field.get_accessor_name() == rel_query_name:\n+                errors.append(\n+                    checks.Error(\n+                        'Clash between reverse query names for %s and %s.'\n+                            % (field_name, clash_name),\n+                        hint='Add or change a related_name argument '\n+                            'to the definition for %s or %s.'\n+                            % (field_name, clash_name),\n+                        obj=self,\n+                        id='E017',\n+                    )\n+                )\n+\n+        return errors\n+\n \n class RenameRelatedObjectDescriptorMethods(RenameMethodsBase):\n     renamed_methods = (\n@@ -1169,6 +1333,61 @@ def contribute_to_related_class(self, cls, related):\n             if self.rel.limit_choices_to:\n                 cls._meta.related_fkey_lookups.append(self.rel.limit_choices_to)\n \n+    def check(self, **kwargs):\n+        errors = super(ForeignObject, self).check(**kwargs)\n+        errors.extend(self._check_unique_target())\n+        return errors\n+\n+    def _check_unique_target(self):\n+        rel_is_string = isinstance(self.rel.to, six.string_types)\n+        if rel_is_string or not self.requires_unique_target:\n+            return []\n+\n+        # Skip if the\n+        try:\n+            self.foreign_related_fields\n+        except FieldDoesNotExist:\n+            return []\n+\n+        try:\n+            self.related\n+        except AttributeError:\n+            return []\n+\n+        has_unique_field = any(rel_field.unique\n+            for rel_field in self.foreign_related_fields)\n+        if not has_unique_field and len(self.foreign_related_fields) > 1:\n+            field_combination = ','.join(rel_field.name\n+                for rel_field in self.foreign_related_fields)\n+            model_name = self.rel.to.__name__\n+            return [\n+                checks.Error(\n+                    'No unique=True constraint '\n+                        'on field combination \"%s\" under model %s.'\n+                        % (field_combination, model_name),\n+                    hint='Set unique=True argument on any of the fields '\n+                        '\"%s\" under model %s.'\n+                        % (field_combination, model_name),\n+                    obj=self,\n+                    id='E018',\n+                )\n+            ]\n+        elif not has_unique_field:\n+            field_name = self.foreign_related_fields[0].name\n+            model_name = self.rel.to.__name__\n+            return [\n+                checks.Error(\n+                    '%s.%s must have unique=True '\n+                        'because it is referenced by a foreign key.'\n+                        % (model_name, field_name),\n+                    hint=None,\n+                    obj=self,\n+                    id='E019',\n+                )\n+            ]\n+        else:\n+            return []\n+\n \n class ForeignKey(ForeignObject):\n     empty_strings_allowed = False\n@@ -1184,7 +1403,6 @@ def __init__(self, to, to_field=None, rel_class=ManyToOneRel,\n         except AttributeError:  # to._meta doesn't exist, so it must be RECURSIVE_RELATIONSHIP_CONSTANT\n             assert isinstance(to, six.string_types), \"%s(%r) is invalid. First parameter to ForeignKey must be either a model, a model name, or the string %r\" % (self.__class__.__name__, to, RECURSIVE_RELATIONSHIP_CONSTANT)\n         else:\n-            assert not to._meta.abstract, \"%s cannot define a relation with abstract class %s\" % (self.__class__.__name__, to._meta.object_name)\n             # For backwards compatibility purposes, we need to *try* and set\n             # the to_field during FK construction. It won't be guaranteed to\n             # be correct until contribute_to_class is called. Refs #12190.\n@@ -1331,6 +1549,34 @@ def db_type(self, connection):\n     def db_parameters(self, connection):\n         return {\"type\": self.db_type(connection), \"check\": []}\n \n+    def check(self, **kwargs):\n+        errors = super(ForeignKey, self).check(**kwargs)\n+        errors.extend(self._check_on_delete())\n+        return errors\n+\n+    def _check_on_delete(self):\n+        on_delete = getattr(self.rel, 'on_delete', None)\n+        if on_delete == SET_NULL and not self.null:\n+            return [\n+                checks.Error(\n+                    'The field specifies on_delete=SET_NULL, but cannot be null.',\n+                    hint='Set null=True argument on the field.',\n+                    obj=self,\n+                    id='E020',\n+                )\n+            ]\n+        elif on_delete == SET_DEFAULT and not self.has_default():\n+            return [\n+                checks.Error(\n+                    'The field specifies on_delete=SET_DEFAULT, but has no default value.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E021',\n+                )\n+            ]\n+        else:\n+            return []\n+\n \n class OneToOneField(ForeignKey):\n     \"\"\"\n@@ -1414,7 +1660,7 @@ class ManyToManyField(RelatedField):\n \n     def __init__(self, to, db_constraint=True, **kwargs):\n         try:\n-            assert not to._meta.abstract, \"%s cannot define a relation with abstract class %s\" % (self.__class__.__name__, to._meta.object_name)\n+            to._meta\n         except AttributeError:  # to._meta doesn't exist, so it must be RECURSIVE_RELATIONSHIP_CONSTANT\n             assert isinstance(to, six.string_types), \"%s(%r) is invalid. First parameter to ManyToManyField must be either a model, a model name, or the string %r\" % (self.__class__.__name__, to, RECURSIVE_RELATIONSHIP_CONSTANT)\n             # Class names must be ASCII in Python 2.x, so we forcibly coerce it here to break early if there's a problem.\n@@ -1617,3 +1863,139 @@ def db_type(self, connection):\n \n     def db_parameters(self, connection):\n         return {\"type\": None, \"check\": None}\n+\n+    def check(self, **kwargs):\n+        errors = super(ManyToManyField, self).check(**kwargs)\n+        errors.extend(self._check_unique(**kwargs))\n+        errors.extend(self._check_relationship_model(**kwargs))\n+        return errors\n+\n+    def _check_unique(self, **kwargs):\n+        if self.unique:\n+            return [\n+                checks.Error(\n+                    'ManyToManyFields must not be unique.',\n+                    hint=None,\n+                    obj=self,\n+                    id='E022',\n+                )\n+            ]\n+        return []\n+\n+    def _check_relationship_model(self, from_model=None, **kwargs):\n+        errors = []\n+\n+        if self.rel.through not in get_models(include_auto_created=True):\n+            # The relationship model is not installed.\n+            errors.append(\n+                checks.Error(\n+                    'The field specifies a many-to-many relation through model '\n+                        '%s, which has not been installed.'\n+                        % self.rel.through,\n+                    hint='Ensure that you did not misspell the model name and '\n+                        'the model is not abstract. Does your INSTALLED_APPS '\n+                        'setting contain the app where %s is defined?'\n+                        % self.rel.through,\n+                    obj=self,\n+                    id='E023',\n+                )\n+            )\n+\n+        elif not isinstance(self.rel.through, six.string_types):\n+\n+            assert from_model is not None, \\\n+                \"ManyToManyField with intermediate \" \\\n+                \"tables cannot be checked if you don't pass the model \" \\\n+                \"where the field is attached to.\"\n+\n+            # Set some useful local variables\n+            to_model = self.rel.to\n+            from_model_name = from_model._meta.object_name\n+            if isinstance(to_model, six.string_types):\n+                to_model_name = to_model\n+            else:\n+                to_model_name = to_model._meta.object_name\n+            relationship_model_name = self.rel.through._meta.object_name\n+            self_referential = from_model == to_model\n+\n+            # Check symmetrical attribute.\n+            if (self_referential and self.rel.symmetrical and\n+                    not self.rel.through._meta.auto_created):\n+                errors.append(\n+                    checks.Error(\n+                        'Many-to-many fields with intermediate tables must not be symmetrical.',\n+                        hint=None,\n+                        obj=self,\n+                        id='E024',\n+                    )\n+                )\n+\n+            # Count foreign keys in intermediate model\n+            if self_referential:\n+                seen_self = sum(from_model == getattr(field.rel, 'to', None)\n+                    for field in self.rel.through._meta.fields)\n+\n+                if seen_self > 2:\n+                    errors.append(\n+                        checks.Error(\n+                            'The model is used as an intermediary model by '\n+                                '%s, but it has more than two foreign keys '\n+                                'to %s, which is ambiguous and is not permitted.'\n+                                % (self, from_model_name),\n+                            hint=None,\n+                            obj=self.rel.through,\n+                            id='E025',\n+                        )\n+                    )\n+\n+            else:\n+                # Count foreign keys in relationship model\n+                seen_from = sum(from_model == getattr(field.rel, 'to', None)\n+                    for field in self.rel.through._meta.fields)\n+                seen_to = sum(to_model == getattr(field.rel, 'to', None)\n+                    for field in self.rel.through._meta.fields)\n+\n+                if seen_from > 1:\n+                    errors.append(\n+                        checks.Error(\n+                            'The model is used as an intermediary model by '\n+                                '%s, but it has more than one foreign key '\n+                                'to %s, which is ambiguous and is not permitted.'\n+                                % (self, from_model_name),\n+                            hint='If you want to create a recursive relationship, '\n+                                'use ForeignKey(\"self\", symmetrical=False, '\n+                                'through=\"%s\").'\n+                                % relationship_model_name,\n+                            obj=self,\n+                            id='E026',\n+                        )\n+                    )\n+\n+                if seen_to > 1:\n+                    errors.append(\n+                        checks.Error(\n+                            'The model is used as an intermediary model by '\n+                                '%s, but it has more than one foreign key '\n+                                'to %s, which is ambiguous and is not permitted.'\n+                                % (self, to_model_name),\n+                            hint='If you want to create a recursive '\n+                                'relationship, use ForeignKey(\"self\", '\n+                                'symmetrical=False, through=\"%s\").'\n+                                % relationship_model_name,\n+                            obj=self,\n+                            id='E027',\n+                        )\n+                    )\n+\n+                if seen_from == 0 or seen_to == 0:\n+                    errors.append(\n+                        checks.Error(\n+                            'The model is used as an intermediary model by '\n+                                '%s, but it misses a foreign key to %s or %s.'\n+                                % (self, from_model_name, to_model_name),\n+                            hint=None,\n+                            obj=self.rel.through,\n+                            id='E028',\n+                        )\n+                    )\n+        return errors\ndiff --git a/django/db/models/loading.py b/django/db/models/loading.py\nindex 2858b8b6992d..d022964b6745 100644\n--- a/django/db/models/loading.py\n+++ b/django/db/models/loading.py\n@@ -47,9 +47,6 @@ def _initialize():\n         # May contain apps that are not installed.\n         app_models=ModelDict(),\n \n-        # Mapping of app_labels to errors raised when trying to import the app.\n-        app_errors = {},\n-\n         # Pending lookups for lazy relations\n         pending_lookups = {},\n \n@@ -233,11 +230,6 @@ def get_app(self, app_label, emptyOK=False):\n         finally:\n             imp.release_lock()\n \n-    def get_app_errors(self):\n-        \"Returns the map of known problems with the INSTALLED_APPS.\"\n-        self._populate()\n-        return self.app_errors\n-\n     def get_models(self, app_mod=None,\n                    include_auto_created=False, include_deferred=False,\n                    only_installed=True, include_swapped=False):\n@@ -383,7 +375,6 @@ def __init__(self):\n get_app_path = cache.get_app_path\n get_app_paths = cache.get_app_paths\n get_app = cache.get_app\n-get_app_errors = cache.get_app_errors\n get_models = cache.get_models\n get_model = cache.get_model\n register_models = cache.register_models\ndiff --git a/django/db/models/manager.py b/django/db/models/manager.py\nindex 48ad3db9cc04..140d71e4f1ec 100644\n--- a/django/db/models/manager.py\n+++ b/django/db/models/manager.py\n@@ -7,6 +7,7 @@\n from django.db.models.fields import FieldDoesNotExist\n from django.utils import six\n from django.utils.deprecation import RenameMethodsBase\n+from django.utils.encoding import python_2_unicode_compatible\n \n \n def ensure_default_manager(sender, **kwargs):\n@@ -58,6 +59,7 @@ class RenameManagerMethods(RenameMethodsBase):\n     )\n \n \n+@python_2_unicode_compatible\n class BaseManager(six.with_metaclass(RenameManagerMethods)):\n     # Tracks each time a Manager instance is created. Used to retain order.\n     creation_counter = 0\n@@ -155,6 +157,23 @@ def db_manager(self, using=None, hints=None):\n     def db(self):\n         return self._db or router.db_for_read(self.model, **self._hints)\n \n+    def check(self, **kwargs):\n+        return []\n+\n+    def __str__(self):\n+        \"\"\" Return \"app_label.model_label.manager_name\". \"\"\"\n+        model = self.model\n+        opts = model._meta\n+        app = model._meta.app_label\n+        manager_name = next(name for (_, name, manager)\n+            in opts.concrete_managers + opts.abstract_managers\n+            if manager == self)\n+        return '%s.%s.%s' % (app, model._meta.object_name, manager_name)\n+\n+    #######################\n+    # PROXIES TO QUERYSET #\n+    #######################\n+\n     def get_queryset(self):\n         \"\"\"\n         Returns a new QuerySet object.  Subclasses can override this method to\n@@ -171,6 +190,7 @@ def all(self):\n         # understanding of how this comes into play.\n         return self.get_queryset()\n \n+\n Manager = BaseManager.from_queryset(QuerySet, class_name='Manager')\n \n \ndiff --git a/django/forms/models.py b/django/forms/models.py\nindex c05c45238362..92077f15469a 100644\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -901,13 +901,12 @@ def get_unique_error_message(self, unique_check):\n         return super(BaseInlineFormSet, self).get_unique_error_message(unique_check)\n \n \n-def _get_foreign_key(parent_model, model, fk_name=None, can_fail=False):\n+def _get_foreign_key(parent_model, model, fk_name=None):\n     \"\"\"\n     Finds and returns the ForeignKey from model to parent if there is one\n     (returns None if can_fail is True and no such field exists). If fk_name is\n-    provided, assume it is the name of the ForeignKey field. Unles can_fail is\n-    True, an exception is raised if there is no ForeignKey from model to\n-    parent_model.\n+    provided, assume it is the name of the ForeignKey field. An exception is\n+    raised if there is no ForeignKey from model to parent_model.\n     \"\"\"\n     # avoid circular import\n     from django.db.models import ForeignKey\n@@ -919,9 +918,13 @@ def _get_foreign_key(parent_model, model, fk_name=None, can_fail=False):\n             if not isinstance(fk, ForeignKey) or \\\n                     (fk.rel.to != parent_model and\n                      fk.rel.to not in parent_model._meta.get_parent_list()):\n-                raise Exception(\"fk_name '%s' is not a ForeignKey to %s\" % (fk_name, parent_model))\n+                raise ValueError(\n+                    '\"fk_name\" refers to \"%s\" field, which is not a ForeignKey to %s.%s.'\n+                    % (fk_name, parent_model._meta.app_label, parent_model._meta.object_name))\n         elif len(fks_to_parent) == 0:\n-            raise Exception(\"%s has no field named '%s'\" % (model, fk_name))\n+            raise ValueError(\n+                '\"fk_name\" refers to \"%s\" field, which is missing from model %s.%s.'\n+                % (fk_name, model._meta.app_label, model._meta.object_name))\n     else:\n         # Try to discover what the ForeignKey from model to parent_model is\n         fks_to_parent = [\n@@ -933,11 +936,13 @@ def _get_foreign_key(parent_model, model, fk_name=None, can_fail=False):\n         if len(fks_to_parent) == 1:\n             fk = fks_to_parent[0]\n         elif len(fks_to_parent) == 0:\n-            if can_fail:\n-                return\n-            raise Exception(\"%s has no ForeignKey to %s\" % (model, parent_model))\n+            raise ValueError(\n+                '\"fk_name\" must be explicitly defined, because there are no ForeignKey from %s.%s to %s.%s.'\n+                % (model._meta.app_label, model._meta.object_name, parent_model._meta.app_label, parent_model._meta.object_name))\n         else:\n-            raise Exception(\"%s has more than 1 ForeignKey to %s\" % (model, parent_model))\n+            raise ValueError(\n+                '\"fk_name\" must be explicitly defined, because %s.%s has more than one ForeignKey to %s.%s.'\n+                % (model._meta.app_label, model._meta.object_name, parent_model._meta.app_label, parent_model._meta.object_name))\n     return fk\n \n \ndiff --git a/django/test/testcases.py b/django/test/testcases.py\nindex 3f0046314e2e..7886e52aa411 100644\n--- a/django/test/testcases.py\n+++ b/django/test/testcases.py\n@@ -752,7 +752,7 @@ def _fixture_setup(self):\n                 # We have to use this slightly awkward syntax due to the fact\n                 # that we're using *args and **kwargs together.\n                 call_command('loaddata', *self.fixtures,\n-                             **{'verbosity': 0, 'database': db_name, 'skip_validation': True})\n+                             **{'verbosity': 0, 'database': db_name, 'skip_checks': True})\n \n     def _post_teardown(self):\n         \"\"\"Performs any post-test things. This includes:\n@@ -780,7 +780,7 @@ def _fixture_teardown(self):\n         # when flushing only a subset of the apps\n         for db_name in self._databases_names(include_mirrors=False):\n             call_command('flush', verbosity=0, interactive=False,\n-                         database=db_name, skip_validation=True,\n+                         database=db_name, skip_checks=True,\n                          reset_sequences=False,\n                          allow_cascade=self.available_apps is not None,\n                          inhibit_post_migrate=self.available_apps is not None)\n@@ -847,7 +847,7 @@ def _fixture_setup(self):\n                                     'verbosity': 0,\n                                     'commit': False,\n                                     'database': db_name,\n-                                    'skip_validation': True,\n+                                    'skip_checks': True,\n                                  })\n                 except Exception:\n                     self._fixture_teardown()\ndiff --git a/django/test/utils.py b/django/test/utils.py\nindex cd3e99d58b52..162341a9db56 100644\n--- a/django/test/utils.py\n+++ b/django/test/utils.py\n@@ -242,6 +242,26 @@ def disable(self):\n                                  setting=key, value=new_value, enter=False)\n \n \n+def override_system_checks(new_checks):\n+    \"\"\" Acts as a decorator. Overrides list of registered system checks.\n+    Useful when you override `INSTALLED_APPS`, e.g. if you exclude `auth` app,\n+    you also need to exclude its system checks. \"\"\"\n+\n+    from django.core.checks.registration import framework\n+\n+    def outer(test_func):\n+        @wraps(test_func)\n+        def inner(*args, **kwargs):\n+            old_checks = framework.registered_checks\n+            framework.registered_checks = new_checks\n+            try:\n+                return test_func(*args, **kwargs)\n+            finally:\n+                framework.registered_checks = old_checks\n+        return inner\n+    return outer\n+\n+\n def compare_xml(want, got):\n     \"\"\"Tries to do a 'xml-comparison' of want and got.  Plain string\n     comparison doesn't always work because, for example, attribute\ndiff --git a/django/utils/termcolors.py b/django/utils/termcolors.py\nindex 8c66e338de9a..acb1a7ff4a56 100644\n--- a/django/utils/termcolors.py\n+++ b/django/utils/termcolors.py\n@@ -74,6 +74,7 @@ def make_style(opts=(), **kwargs):\n PALETTES = {\n     NOCOLOR_PALETTE: {\n         'ERROR':        {},\n+        'WARNING':      {},\n         'NOTICE':       {},\n         'SQL_FIELD':    {},\n         'SQL_COLTYPE':  {},\n@@ -93,6 +94,7 @@ def make_style(opts=(), **kwargs):\n     },\n     DARK_PALETTE: {\n         'ERROR':        { 'fg': 'red', 'opts': ('bold',) },\n+        'WARNING':      { 'fg': 'yellow', 'opts': ('bold',) },\n         'NOTICE':       { 'fg': 'red' },\n         'SQL_FIELD':    { 'fg': 'green', 'opts': ('bold',) },\n         'SQL_COLTYPE':  { 'fg': 'green' },\n@@ -112,6 +114,7 @@ def make_style(opts=(), **kwargs):\n     },\n     LIGHT_PALETTE: {\n         'ERROR':        { 'fg': 'red', 'opts': ('bold',) },\n+        'WARNING':      { 'fg': 'yellow', 'opts':('bold',) },\n         'NOTICE':       { 'fg': 'red' },\n         'SQL_FIELD':    { 'fg': 'green', 'opts': ('bold',) },\n         'SQL_COLTYPE':  { 'fg': 'green' },\ndiff --git a/docs/howto/custom-management-commands.txt b/docs/howto/custom-management-commands.txt\nindex 2325e32ac2f3..918e048870ae 100644\n--- a/docs/howto/custom-management-commands.txt\n+++ b/docs/howto/custom-management-commands.txt\n@@ -227,13 +227,21 @@ All attributes can be set in your derived class and can be used in\n   wrapped with ``BEGIN;`` and ``COMMIT;``. Default value is\n   ``False``.\n \n+.. attribute:: BaseCommand.requires_system_checks\n+\n+  A boolean; if ``True``, entire Django project will be checked for errors\n+  prior to executing the command. If it's missing, the value of\n+  ``requires_model_validation`` is used. If the latter flag is missing too,\n+  the default value (``True``) is used. Defining both\n+  ``requires_system_checks`` and ``requires_model_validation`` results in an\n+  error.\n+\n .. attribute:: BaseCommand.requires_model_validation\n \n-  A boolean; if ``True``, validation of installed models will be\n-  performed prior to executing the command. Default value is\n-  ``True``. To validate an individual application's models\n-  rather than all applications' models, call\n-  :meth:`~BaseCommand.validate` from :meth:`~BaseCommand.handle`.\n+  A deprecated boolean. Use ``requires_system_checks`` instead. If\n+  ``requires_system_checks`` is missing, then this value is used. Defining\n+  both ``requires_system_checks`` and ``requires_model_validation`` results in\n+  an error.\n \n .. attribute:: BaseCommand.leave_locale_alone\n \n@@ -299,12 +307,22 @@ the :meth:`~BaseCommand.handle` method must be implemented.\n \n     The actual logic of the command. Subclasses must implement this method.\n \n-.. method:: BaseCommand.validate(app=None, display_num_errors=False)\n+.. method:: BaseCommand.check(apps=None, tags=None, display_num_errors=False)\n \n-    Validates the given app, raising :class:`CommandError` for any errors.\n+    Uses the system check framework to validate entire Django project. Yes,\n+    it's includes model validation and compatibility checks. For any serious\n+    issue (like an error or critical error), raises :class:`CommandError`. If\n+    there are only light issues (like warnings), does not raise any exception.\n+    In both cases, prints to stderr. When there are no issues, prints to\n+    stdout.\n \n-    If ``app`` is None, then all installed apps are validated.\n+    If `apps` and `tags` are both None, all system checks are performed.\n+    `tags` can be a list of check tags, like ``compatibility`` or ``models``.\n+\n+.. method:: BaseCommand.validate(app=None, display_num_errors=False)\n \n+    Deprecated. Use \"check\" instead. Delegates to \"check\". If ``app`` is None,\n+    then all installed apps are checked for errors.\n \n .. _ref-basecommand-subclasses:\n \ndiff --git a/docs/index.txt b/docs/index.txt\nindex df6861aa1c65..56c6c6d6418a 100644\n--- a/docs/index.txt\n+++ b/docs/index.txt\n@@ -286,6 +286,7 @@ Learn about some other core functionalities of the Django framework:\n * :doc:`Flatpages <ref/contrib/flatpages>`\n * :doc:`Redirects <ref/contrib/redirects>`\n * :doc:`Signals <topics/signals>`\n+* :doc:`System check framework <ref/checks>`\n * :doc:`The sites framework <ref/contrib/sites>`\n * :doc:`Unicode in Django <ref/unicode>`\n \ndiff --git a/docs/internals/deprecation.txt b/docs/internals/deprecation.txt\nindex 498f303daf72..595df65960d6 100644\n--- a/docs/internals/deprecation.txt\n+++ b/docs/internals/deprecation.txt\n@@ -461,6 +461,16 @@ these changes.\n   ``BaseMemcachedCache._get_memcache_timeout()`` method to\n   ``get_backend_timeout()``.\n \n+* ``BaseCommand.requires_model_validation`` will be removed in favour of\n+  ``requires_system_checks``. Admin validators will be replaced by admin\n+  checks.\n+\n+* ``ModelAdmin.validator`` will be removed in favour of new ``checks``\n+  attribute.\n+\n+* ``django.db.backends.DatabaseValidation.validate_field`` will be removed in\n+  favour of ``check_field`` method.\n+\n 2.0\n ---\n \ndiff --git a/docs/intro/tutorial01.txt b/docs/intro/tutorial01.txt\nindex 1c301c0cc213..5f28424843d1 100644\n--- a/docs/intro/tutorial01.txt\n+++ b/docs/intro/tutorial01.txt\n@@ -136,7 +136,7 @@ see the following output on the command line:\n \n .. parsed-literal::\n \n-    Validating models...\n+    Performing system checks...\n \n     0 errors found\n     |today| - 15:50:53\n@@ -475,8 +475,8 @@ Note the following:\n \n If you're interested, also run the following commands:\n \n-* :djadmin:`python manage.py validate <validate>` -- Checks for any errors\n-  in the construction of your models.\n+* :djadmin:`python manage.py check <check>` -- Checks for any errors\n+  in your project.\n \n * :djadmin:`python manage.py sqlcustom polls <sqlcustom>` -- Outputs any\n   :ref:`custom SQL statements <initial-sql>` (such as table modifications or\ndiff --git a/docs/ref/checks.txt b/docs/ref/checks.txt\nnew file mode 100644\nindex 000000000000..cfc09e3e8ddc\n--- /dev/null\n+++ b/docs/ref/checks.txt\n@@ -0,0 +1,204 @@\n+.. module:: django.core.checks\n+\n+.. _`system-check-framework`:\n+\n+======================\n+System check framework\n+======================\n+\n+System check framework is a set of static checks validating mainly apps and\n+models. It detects common problems and report them providing hints. The\n+framework is open-ended, so you can easily add your own checks.\n+\n+Checks can be triggered explicitly via :djadmin:`check` command. It's also\n+triggered implicitly before most command, including :djadmin:`runserver` and\n+:djadmin:`migrate`. For performance issues, the checks are not performed if\n+``DEBUG`` is set to False.\n+\n+Django will not allow you to run server if there is any serious message like\n+an error or critical error. If there are light messages like warnings, Django\n+won't stop you, but will issue the message. You can hide unwanted warnings or\n+force Django to run even if there are serious messages bu overriding\n+:setting:`SILENCED_SYSTEM_CHECKS` setting.\n+\n+Writing your own checks\n+=======================\n+\n+The framework is flexible and allow you to write functions (or other callable\n+objects like methods) performing any kind of checks. The function has to\n+receive ``**kwargs`` and return list of messages. If there are no message, you\n+still have to return an empty list. The function need to be `registered`_ or\n+called from another check.\n+\n+.. _`registered`: `registering-checks`_\n+\n+.. class:: django.core.checks.CheckMessage(level, msg, hint, obj=None, id=None)\n+\n+:class:`~django.core.checks.CheckMessage` is the key concept of\n+:mod:`django.core.checks` module and represents a single message, i.e. a\n+warning or an error. The concept is very similar to messages from `message\n+framework` or `logging framework`_. Messages are tagged with ``level``\n+indicating how serious the message is. They can have an unique identifier.\n+\n+.. _`logging framework`: http://docs.python.org/2/library/logging.html\n+\n+Constructor arguments are:\n+\n+``level``\n+    A positive integer. Determines how serious the message is. Use one of the\n+    predefined values: ``DEBUG``, ``INFO``, ``WARNING``, ``ERROR``,\n+    ``CRITICAL``. If the level is greater or equal to ``ERROR``, then Django\n+    stops everything and doesn't allow to i.e. run a server. Messages with\n+    level lower than ``ERROR`` (i. e. warnings) are still reported, but Django\n+    allows will not stop you.\n+\n+``msg``\n+    A required single-line string.\n+\n+``hint``\n+    A single-line string or ``None`` if you cannot provide any hint. Note that\n+    this argument is required even if you pass ``None``::\n+\n+        Error('error message') # bad\n+        Error('error message', None) # good\n+        Error('error message', hint=None) # good (preferable)\n+\n+``obj``\n+    Optional. Point to the invalid object. The object should be a model, field\n+    or manager or any other object that defines ``__unicode__`` method (on\n+    Python 3 you need to define ``__str__`` method). The method is used while\n+    reporting all messages and its result precedes the message.\n+\n+``id``\n+    Optional string. Short and unique name of an issue. Follow \"applabel.X001\"\n+    style, where ``X`` is one of ``CEWID`` letters depending on the message\n+    level (``C`` for criticals, ``E`` for errors and so on), i.e. a warning\n+    may have ``applabel.W001`` id and an error can be ``applabel.E002``. The\n+    message number must be unique amount an app, so ``applabel.E001`` and\n+    ``anotherapp.E001`` are OK. Do not reuse one number for more than one\n+    message, e.g. avoid ``applabel.E001`` and ``applabel.W001``.\n+\n+There are shortcuts to make creating messages with common levels easier. You can\n+omit ``level`` argument because the level is indicated by the class name.\n+\n+.. class:: django.core.checks.Debug(msg, hint, obj=None, id=None)\n+.. class:: django.core.checks.Info(msg, hint, obj=None, id=None)\n+.. class:: django.core.checks.Warning(msg, hint, obj=None, id=None)\n+.. class:: django.core.checks.Error(msg, hint, obj=None, id=None)\n+.. class:: django.core.checks.Critical(msg, hint, obj=None, id=None)\n+\n+Messages are comparable. That allows you to easily write tests::\n+\n+    from django.core.checks import Error\n+    errors = checked_object.check()\n+    expected_errors = [\n+        Error(\n+            'an error',\n+            hint=None,\n+            obj=checked_object,\n+            id='myapp.E001',\n+        )\n+    ]\n+    self.assertEqual(errors, expected_errors)\n+\n+.. _`registering-checks`:\n+\n+Registering and labeling checks\n+-------------------------------\n+\n+Your check function need to be registered explicitly in system check\n+framework.\n+\n+.. method:: register(*tags)(function)\n+\n+Note that checks are performed just after loading apps and you need to\n+register them earlier. It's recommended to use ``register`` function as a\n+decorator so your function will be registered at the the time of importing the\n+module.\n+\n+You can pass as many tags to ``register`` as you want in order to label your\n+check. Tagging checks is useful since it allows you to run only a certain\n+group of checks, i.e. compatibility checks::\n+\n+    from checks import register\n+\n+    @register('compatibility')\n+    def my_check(apps, **kwargs):\n+        # ... perform compatibility checks and collect errors\n+        return errors\n+\n+Note that registered checks not only have to take ``**kwargs`` and return list\n+of messages, they also accept ``apps`` argument. If ``apps`` is not ``None``,\n+only the given set of apps should be validated otherwise all apps should be\n+checked.\n+\n+.. _field-checking:\n+\n+Field checks\n+------------\n+\n+You do not need to register check function if it's called from another check\n+function. It's easy in the case of fields, managers and models checks.\n+\n+You need to override ``check`` method: it should call the superclass (1),\n+perform its own checks which may end in new messages (2) and eventually return\n+list of all errors and warnings (3). It's recommended to delegate checks to\n+separated methods named i.e. ``_check_min_max_values``.\n+\n+Consider an example where you are implementing ``RangedIntegerField``. It adds\n+``min`` and ``max`` arguments to the constructor. You want to check if min\n+value is smaller or equal to max value. Here is a snippet of code showing how\n+you can implement this check::\n+\n+    from django.core import checks\n+    from django.db import models\n+\n+    class RangedIntegerField(models.IntegerField):\n+        def __init__(self, min=None, max=None, **kwargs):\n+            super(RangedIntegerField, self).__init__(**kwargs)\n+            self.min = min\n+            self.max = max\n+\n+        def check(self, **kwargs):\n+            errors = super(RangedIntegerField, self).check(**kwargs) # (1) call the superclass\n+            # (2) Do some custom checks and add messages to `errors`:\n+            errors.extend(self._check_min_max_values(**kwargs))\n+            return errors # (3) return all errors and warnings\n+\n+        def _check_min_max_values(self, **kwargs):\n+            if (self.min is not None and\n+                    self.max is not None and\n+                    self.min > self.max):\n+                return [\n+                    checks.Error(\n+                        'min greated than max.',\n+                        hint='Lower min or upper max.',\n+                        obj=self,\n+                        id='myapp.E001',\n+                    )\n+                ]\n+            return [] # When no error, return an empty list\n+\n+Database backend specific checks are performed in\n+``DatabaseValidation.check_field`` method. The API of ``DatabaseValidation``\n+is not formally stable and may change in future.\n+\n+Manager checks\n+--------------\n+\n+Manager checks are very similar to :ref:`field checking <field-checking>`. The\n+only difference is that your class needs to inherit from ``models.Manager``.\n+\n+Model checks\n+------------\n+\n+It's possible to check Model classes. Note that the framework verifies the\n+class, not it's instances. It works exactly the same as field checks except that\n+you need to use classmethods::\n+\n+    class MyModel(models.Model):\n+        @classmethod\n+        def check(cls, **kwargs):\n+            errors = super(MyModel, cls).check(**kwargs)\n+            # ... your own checks ...\n+            return errors\ndiff --git a/docs/ref/django-admin.txt b/docs/ref/django-admin.txt\nindex 5bc9a2b83ec8..8aed77cd8142 100644\n--- a/docs/ref/django-admin.txt\n+++ b/docs/ref/django-admin.txt\n@@ -89,18 +89,36 @@ documentation for the :djadminopt:`--verbosity` option.\n Available commands\n ==================\n \n-check\n------\n+check <appname appname ...>\n+---------------------------\n \n .. django-admin:: check\n \n .. versionadded:: 1.6\n \n-Performs a series of checks to verify a given setup (settings/application code)\n-is compatible with the current version of Django.\n+Uses the :ref:`system check framework <system-check-framework>` to validate\n+entire Django project and prints any messages (mostly errors and warnings).\n+It's an extended version of old ``validate`` and ``check`` commands.\n+\n+System checks framework validate all installed models (according to the\n+:setting:`INSTALLED_APPS` setting) and perform compatibility checks (is your\n+setup and application code compatible with the current Django version?) as\n+well as custom checks.\n+\n+You can check some apps by listing them::\n+\n+    python manage.py auth admin myapp\n+\n+If you do not specify any app, all apps will be checked.\n \n-Upon finding things that are incompatible or require notifying the user, it\n-issues a series of warnings.\n+.. django-admin-option:: --tag <tagname>\n+\n+:ref:`System check framework <system-check-framework>` performs many different\n+types of checks, i.e. model checks and compatibility checks. If you want to\n+perform only a certain kind of checks, follow ``--tag`` (``-t``) keyword\n+argument with the name of the check group (``models`` or ``compatibility``)::\n+\n+    python manage.py --tag security -t compatibility\n \n compilemessages\n ---------------\n@@ -765,9 +783,9 @@ The development server automatically reloads Python code for each request, as\n needed. You don't need to restart the server for code changes to take effect.\n \n When you start the server, and each time you change Python code while the\n-server is running, the server will validate all of your installed models. (See\n-the ``validate`` command below.) If the validator finds errors, it will print\n-them to standard output, but it won't stop the server.\n+server is running, the server will check entire Django project for errors (see\n+the ``check`` command below). If there will be any error, it will be printed\n+to standard output, but it won't stop the server.\n \n You can run as many servers as you want, as long as they're on separate ports.\n Just execute ``django-admin.py runserver`` more than once.\n@@ -1259,14 +1277,6 @@ To run on 1.2.3.4:7000 with a ``test`` fixture::\n The :djadminopt:`--noinput` option may be provided to suppress all user\n prompts.\n \n-validate\n---------\n-\n-.. django-admin:: validate\n-\n-Validates all installed models (according to the :setting:`INSTALLED_APPS`\n-setting) and prints validation errors to standard output.\n-\n Commands provided by applications\n =================================\n \ndiff --git a/docs/ref/index.txt b/docs/ref/index.txt\nindex 1d71b62f41d1..94881b9778f4 100644\n--- a/docs/ref/index.txt\n+++ b/docs/ref/index.txt\n@@ -5,6 +5,7 @@ API Reference\n .. toctree::\n    :maxdepth: 1\n \n+   checks\n    class-based-views/index\n    clickjacking\n    contrib/index\ndiff --git a/docs/ref/settings.txt b/docs/ref/settings.txt\nindex 4c46f3a8cdf9..52f457514f87 100644\n--- a/docs/ref/settings.txt\n+++ b/docs/ref/settings.txt\n@@ -1733,6 +1733,20 @@ The backend used for signing cookies and other data.\n \n See also the :doc:`/topics/signing` documentation.\n \n+.. setting:: SILENCED_SYSTEM_CHECKS\n+\n+SILENCED_SYSTEM_CHECKS\n+----------------------\n+\n+Default: '[]'\n+\n+List of identificators of messages generated by the system check framework\n+(i.e. ``[\"E001\"]``). Silencing light messages like warnings results in hiding\n+them. Silencing serious messages like errors or criticals means that Django\n+will let you i.e. run server, but the messages will be still printed.\n+\n+See also the :doc:`/ref/checks` documentation.\n+\n .. setting:: TEMPLATE_CONTEXT_PROCESSORS\n \n TEMPLATE_CONTEXT_PROCESSORS\n@@ -2725,6 +2739,7 @@ Error reporting\n * :setting:`IGNORABLE_404_URLS`\n * :setting:`MANAGERS`\n * :setting:`SEND_BROKEN_LINK_EMAILS`\n+* :setting:`SILENCED_SYSTEM_CHECKS`\n \n File uploads\n ------------\ndiff --git a/docs/releases/1.7.txt b/docs/releases/1.7.txt\nindex 0f1a5de073e7..6ca98cb50daf 100644\n--- a/docs/releases/1.7.txt\n+++ b/docs/releases/1.7.txt\n@@ -92,11 +92,29 @@ The :meth:`QuerySet.as_manager() <django.db.models.query.QuerySet.as_manager>`\n class method has been added to :ref:`create Manager with QuerySet methods\n <create-manager-with-queryset-methods>`.\n \n+<<<<<<< HEAD\n Using a custom manager when traversing reverse relations\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n It is now possible to :ref:`specify a custom manager\n <using-custom-reverse-manager>` when traversing a reverse relationship.\n+=======\n+New system check framework\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:doc:`System check framework </ref/checks>` is a set of static checks\n+<<<<<<< HEAD\n+validating mainly apps and models. It detects common problems and report\n+them providing hints. The framework is extensible and it's possible to\n+write custom checks.\n+>>>>>>> deprecated BaseCommand.{requires_model_validation, validate}\n+=======\n+detecting common problems (like invalid models) and reporting them providing\n+hints. The framework is extensible and it's possible to write custom checks.\n+>>>>>>> improved docs\n+\n+To perform system checks, you need to run ``check`` command. ``validate``\n+command is deprecated now.\n \n Admin shortcuts support time zones\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ -584,3 +602,27 @@ The :class:`django.db.models.IPAddressField` and\n The ``BaseMemcachedCache._get_memcache_timeout()`` method has been renamed to\n ``get_backend_timeout()``. Despite being a private API, it will go through the\n normal deprecation.\n+\n+``validate``\n+------------\n+\n+``validate`` command is deprecated in favour of ``check`` command.\n+\n+``django.core.management.BaseCommand``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+``requires_model_validation`` is deprecated in favour of a new\n+``requires_system_checks`` flag. If the latter flag is missing, then the\n+former flag value is used. Defining both ``requires_system_checks`` and\n+``requires_model_validation`` results in an error.\n+\n+``check`` method has replaced old ``validate`` method.\n+\n+``ModelAdmin.validator``\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+``ModelAdmin.validator`` is deprecated in favour of new ``checks`` attribute.\n+\n+``django.db.backends.DatabaseValidation.validate_field``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+This method is deprecated in favour of a new ``check_field`` method.\ndiff --git a/docs/topics/db/models.txt b/docs/topics/db/models.txt\nindex b0011e1098ed..5c52f9fd174a 100644\n--- a/docs/topics/db/models.txt\n+++ b/docs/topics/db/models.txt\n@@ -153,8 +153,8 @@ ones:\n     <Field.blank>`, the field will be required.\n \n :attr:`~Field.choices`\n-    An iterable (e.g., a list or tuple) of 2-tuples to use as choices for\n-    this field. If this is given, the default form widget will be a select box\n+    An iterable (e.g., a list or tuple) of 2-tuples to use as choices for this\n+    field. If this is given, the default form widget will be a select box\n     instead of the standard text field and will limit choices to the choices\n     given.\n \n@@ -957,7 +957,7 @@ The reverse name of the ``common.ChildA.m2m`` field will be\n reverse name of the ``rare.ChildB.m2m`` field will be ``rare_childb_related``.\n It is up to you how you use the ``'%(class)s'`` and ``'%(app_label)s`` portion\n to construct your related name, but if you forget to use it, Django will raise\n-errors when you validate your models (or run :djadmin:`migrate`).\n+errors when you perform system checks (or run :djadmin:`migrate`).\n \n If you don't specify a :attr:`~django.db.models.ForeignKey.related_name`\n attribute for a field in an abstract base class, the default reverse name will\n@@ -1050,7 +1050,7 @@ are putting those types of relations on a subclass of another model,\n you **must** specify the\n :attr:`~django.db.models.ForeignKey.related_name` attribute on each\n such field. If you forget, Django will raise an error when you run\n-:djadmin:`validate` or :djadmin:`migrate`.\n+:djadmin:`check` or :djadmin:`migrate`.\n \n For example, using the above ``Place`` class again, let's create another\n subclass with a :class:`~django.db.models.ManyToManyField`::\ndiff --git a/tests/admin_validation/__init__.py b/tests/admin_checks/__init__.py\nsimilarity index 100%\nrename from tests/admin_validation/__init__.py\nrename to tests/admin_checks/__init__.py\ndiff --git a/tests/admin_validation/models.py b/tests/admin_checks/models.py\nsimilarity index 96%\nrename from tests/admin_validation/models.py\nrename to tests/admin_checks/models.py\nindex d23849a2a85f..5db1747a642e 100644\n--- a/tests/admin_validation/models.py\n+++ b/tests/admin_checks/models.py\n@@ -1,5 +1,5 @@\n \"\"\"\n-Tests of ModelAdmin validation logic.\n+Tests of ModelAdmin system checks logic.\n \"\"\"\n \n from django.db import models\ndiff --git a/tests/admin_checks/tests.py b/tests/admin_checks/tests.py\nnew file mode 100644\nindex 000000000000..bcbc928d5c00\n--- /dev/null\n+++ b/tests/admin_checks/tests.py\n@@ -0,0 +1,439 @@\n+from __future__ import unicode_literals\n+\n+from django import forms\n+from django.contrib import admin\n+from django.core import checks\n+from django.core.exceptions import ImproperlyConfigured\n+from django.test import TestCase\n+from django.test.utils import str_prefix\n+\n+from .models import Song, Book, Album, TwoAlbumFKAndAnE, City, State\n+\n+\n+class SongForm(forms.ModelForm):\n+    pass\n+\n+\n+class ValidFields(admin.ModelAdmin):\n+    form = SongForm\n+    fields = ['title']\n+\n+\n+class ValidFormFieldsets(admin.ModelAdmin):\n+    def get_form(self, request, obj=None, **kwargs):\n+        class ExtraFieldForm(SongForm):\n+            name = forms.CharField(max_length=50)\n+        return ExtraFieldForm\n+\n+    fieldsets = (\n+        (None, {\n+            'fields': ('name',),\n+        }),\n+    )\n+\n+\n+class SystemChecksTestCase(TestCase):\n+\n+    def test_checks_are_performed(self):\n+        class MyAdmin(admin.ModelAdmin):\n+            @classmethod\n+            def check(self, model, **kwargs):\n+                return ['error!']\n+\n+        admin.site.register(Song, MyAdmin)\n+        try:\n+            errors = checks.run_checks()\n+            expected = ['error!']\n+            self.assertEqual(errors, expected)\n+        finally:\n+            admin.site.unregister(Song)\n+\n+    def test_readonly_and_editable(self):\n+        class SongAdmin(admin.ModelAdmin):\n+            readonly_fields = [\"original_release\"]\n+            fieldsets = [\n+                (None, {\n+                    \"fields\": [\"title\", \"original_release\"],\n+                }),\n+            ]\n+\n+        errors = SongAdmin.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_custom_modelforms_with_fields_fieldsets(self):\n+        \"\"\"\n+        # Regression test for #8027: custom ModelForms with fields/fieldsets\n+        \"\"\"\n+\n+        errors = ValidFields.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_custom_get_form_with_fieldsets(self):\n+        \"\"\"\n+        Ensure that the fieldsets checks are skipped when the ModelAdmin.get_form() method\n+        is overridden.\n+        Refs #19445.\n+        \"\"\"\n+\n+        errors = ValidFormFieldsets.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_exclude_values(self):\n+        \"\"\"\n+        Tests for basic system checks of 'exclude' option values (#12689)\n+        \"\"\"\n+\n+        class ExcludedFields1(admin.ModelAdmin):\n+            exclude = 'foo'\n+\n+        errors = ExcludedFields1.check(model=Book)\n+        expected = [\n+            checks.Error(\n+                '\"exclude\" must be a list or tuple.',\n+                hint=None,\n+                obj=ExcludedFields1,\n+                id='admin.E014',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_exclude_duplicate_values(self):\n+        class ExcludedFields2(admin.ModelAdmin):\n+            exclude = ('name', 'name')\n+\n+        errors = ExcludedFields2.check(model=Book)\n+        expected = [\n+            checks.Error(\n+                '\"exclude\" contains duplicate field(s).',\n+                hint=None,\n+                obj=ExcludedFields2,\n+                id='admin.E015',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_exclude_in_inline(self):\n+        class ExcludedFieldsInline(admin.TabularInline):\n+            model = Song\n+            exclude = 'foo'\n+\n+        class ExcludedFieldsAlbumAdmin(admin.ModelAdmin):\n+            model = Album\n+            inlines = [ExcludedFieldsInline]\n+\n+        errors = ExcludedFieldsAlbumAdmin.check(model=Album)\n+        expected = [\n+            checks.Error(\n+                '\"exclude\" must be a list or tuple.',\n+                hint=None,\n+                obj=ExcludedFieldsInline,\n+                id='admin.E014',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_exclude_inline_model_admin(self):\n+        \"\"\"\n+        Regression test for #9932 - exclude in InlineModelAdmin should not\n+        contain the ForeignKey field used in ModelAdmin.model\n+        \"\"\"\n+\n+        class SongInline(admin.StackedInline):\n+            model = Song\n+            exclude = ['album']\n+\n+        class AlbumAdmin(admin.ModelAdmin):\n+            model = Album\n+            inlines = [SongInline]\n+\n+        errors = AlbumAdmin.check(model=Album)\n+        expected = [\n+            checks.Error(\n+                'Cannot exclude the field \"album\", because it is the foreign key '\n+                    'to the parent model admin_checks.Album.',\n+                hint=None,\n+                obj=SongInline,\n+                id='admin.E201',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_app_label_in_admin_checks(self):\n+        \"\"\"\n+        Regression test for #15669 - Include app label in admin system check messages\n+        \"\"\"\n+\n+        class RawIdNonexistingAdmin(admin.ModelAdmin):\n+            raw_id_fields = ('nonexisting',)\n+\n+        errors = RawIdNonexistingAdmin.check(model=Album)\n+        expected = [\n+            checks.Error(\n+                '\"raw_id_fields[0]\" refers to field \"nonexisting\", which is '\n+                    'missing from model admin_checks.Album.',\n+                hint=None,\n+                obj=RawIdNonexistingAdmin,\n+                id='admin.E002',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_fk_exclusion(self):\n+        \"\"\"\n+        Regression test for #11709 - when testing for fk excluding (when exclude is\n+        given) make sure fk_name is honored or things blow up when there is more\n+        than one fk to the parent model.\n+        \"\"\"\n+\n+        class TwoAlbumFKAndAnEInline(admin.TabularInline):\n+            model = TwoAlbumFKAndAnE\n+            exclude = (\"e\",)\n+            fk_name = \"album1\"\n+\n+        class MyAdmin(admin.ModelAdmin):\n+            inlines = [TwoAlbumFKAndAnEInline]\n+\n+        errors = MyAdmin.check(model=Album)\n+        self.assertEqual(errors, [])\n+\n+    def test_inline_self_check(self):\n+        class TwoAlbumFKAndAnEInline(admin.TabularInline):\n+            model = TwoAlbumFKAndAnE\n+\n+        class MyAdmin(admin.ModelAdmin):\n+            inlines = [TwoAlbumFKAndAnEInline]\n+\n+        errors = MyAdmin.check(model=Album)\n+        expected = [\n+            checks.Error(\n+                '\"fk_name\" must be explicitly defined, because '\n+                    'admin_checks.TwoAlbumFKAndAnE has more than one '\n+                    'ForeignKey to admin_checks.Album.',\n+                hint=None,\n+                obj=TwoAlbumFKAndAnEInline,\n+                id='admin.E202',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_inline_with_specified(self):\n+        class TwoAlbumFKAndAnEInline(admin.TabularInline):\n+            model = TwoAlbumFKAndAnE\n+            fk_name = \"album1\"\n+\n+        class MyAdmin(admin.ModelAdmin):\n+            inlines = [TwoAlbumFKAndAnEInline]\n+\n+        errors = MyAdmin.check(model=Album)\n+        self.assertEqual(errors, [])\n+\n+    def test_readonly(self):\n+        class SongAdmin(admin.ModelAdmin):\n+            readonly_fields = (\"title\",)\n+\n+        errors = SongAdmin.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_readonly_on_method(self):\n+        def my_function(obj):\n+            pass\n+\n+        class SongAdmin(admin.ModelAdmin):\n+            readonly_fields = (my_function,)\n+\n+        errors = SongAdmin.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_readonly_on_modeladmin(self):\n+        class SongAdmin(admin.ModelAdmin):\n+            readonly_fields = (\"readonly_method_on_modeladmin\",)\n+\n+            def readonly_method_on_modeladmin(self, obj):\n+                pass\n+\n+        errors = SongAdmin.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_readonly_method_on_model(self):\n+        class SongAdmin(admin.ModelAdmin):\n+            readonly_fields = (\"readonly_method_on_model\",)\n+\n+        errors = SongAdmin.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_nonexistant_field(self):\n+        class SongAdmin(admin.ModelAdmin):\n+            readonly_fields = (\"title\", \"nonexistant\")\n+\n+        errors = SongAdmin.check(model=Song)\n+        expected = [\n+            checks.Error(\n+                '\"readonly_fields[1]\" is neither a callable nor an attribute '\n+                    'of \"SongAdmin\" nor found in the model admin_checks.Song.',\n+                hint=None,\n+                obj=SongAdmin,\n+                id='admin.E034',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_nonexistant_field_on_inline(self):\n+        class CityInline(admin.TabularInline):\n+            model = City\n+            readonly_fields=['i_dont_exist']  # Missing attribute\n+\n+        errors = CityInline.check(State)\n+        expected = [\n+            checks.Error(\n+                '\"readonly_fields[0]\" is neither a callable nor an attribute '\n+                    'of \"CityInline\" nor found in the model admin_checks.City.',\n+                hint=None,\n+                obj=CityInline,\n+                id='admin.E034',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_extra(self):\n+        class SongAdmin(admin.ModelAdmin):\n+            def awesome_song(self, instance):\n+                if instance.title == \"Born to Run\":\n+                    return \"Best Ever!\"\n+                return \"Status unknown.\"\n+\n+        errors = SongAdmin.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_readonly_lambda(self):\n+        class SongAdmin(admin.ModelAdmin):\n+            readonly_fields = (lambda obj: \"test\",)\n+\n+        errors = SongAdmin.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_graceful_m2m_fail(self):\n+        \"\"\"\n+        Regression test for #12203/#12237 - Fail more gracefully when a M2M field that\n+        specifies the 'through' option is included in the 'fields' or the 'fieldsets'\n+        ModelAdmin options.\n+        \"\"\"\n+\n+        class BookAdmin(admin.ModelAdmin):\n+            fields = ['authors']\n+\n+        errors = BookAdmin.check(model=Book)\n+        expected = [\n+            checks.Error(\n+                '\"fields\" cannot include the ManyToManyField \"authors\", '\n+                    'because \"authors\" manually specifies relationship model.',\n+                hint=None,\n+                obj=BookAdmin,\n+                id='admin.E013',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_cannot_include_through(self):\n+        class FieldsetBookAdmin(admin.ModelAdmin):\n+            fieldsets = (\n+                ('Header 1', {'fields': ('name',)}),\n+                ('Header 2', {'fields': ('authors',)}),\n+            )\n+\n+        errors = FieldsetBookAdmin.check(model=Book)\n+        expected = [\n+            checks.Error(\n+                '\"fieldsets[1][1][\\'fields\\']\" cannot include the ManyToManyField '\n+                    '\"authors\", because \"authors\" manually specifies relationship model.',\n+                hint=None,\n+                obj=FieldsetBookAdmin,\n+                id='admin.E013',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_nested_fields(self):\n+        class NestedFieldsAdmin(admin.ModelAdmin):\n+            fields = ('price', ('name', 'subtitle'))\n+\n+        errors = NestedFieldsAdmin.check(model=Book)\n+        self.assertEqual(errors, [])\n+\n+    def test_nested_fieldsets(self):\n+        class NestedFieldsetAdmin(admin.ModelAdmin):\n+            fieldsets = (\n+                ('Main', {'fields': ('price', ('name', 'subtitle'))}),\n+            )\n+\n+        errors = NestedFieldsetAdmin.check(model=Book)\n+        self.assertEqual(errors, [])\n+\n+    def test_explicit_through_override(self):\n+        \"\"\"\n+        Regression test for #12209 -- If the explicitly provided through model\n+        is specified as a string, the admin should still be able use\n+        Model.m2m_field.through\n+        \"\"\"\n+\n+        class AuthorsInline(admin.TabularInline):\n+            model = Book.authors.through\n+\n+        class BookAdmin(admin.ModelAdmin):\n+            inlines = [AuthorsInline]\n+\n+        errors = BookAdmin.check(model=Book)\n+        self.assertEqual(errors, [])\n+\n+    def test_non_model_fields(self):\n+        \"\"\"\n+        Regression for ensuring ModelAdmin.fields can contain non-model fields\n+        that broke with r11737\n+        \"\"\"\n+\n+        class SongForm(forms.ModelForm):\n+            extra_data = forms.CharField()\n+\n+        class FieldsOnFormOnlyAdmin(admin.ModelAdmin):\n+            form = SongForm\n+            fields = ['title', 'extra_data']\n+\n+        errors = FieldsOnFormOnlyAdmin.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_non_model_first_field(self):\n+        \"\"\"\n+        Regression for ensuring ModelAdmin.field can handle first elem being a\n+        non-model field (test fix for UnboundLocalError introduced with r16225).\n+        \"\"\"\n+\n+        class SongForm(forms.ModelForm):\n+            extra_data = forms.CharField()\n+            class Meta:\n+                model = Song\n+                fields = '__all__'\n+\n+\n+        class FieldsOnFormOnlyAdmin(admin.ModelAdmin):\n+            form = SongForm\n+            fields = ['extra_data', 'title']\n+\n+        errors = FieldsOnFormOnlyAdmin.check(model=Song)\n+        self.assertEqual(errors, [])\n+\n+    def test_validator_compatibility(self):\n+        class MyValidator(object):\n+            def validate(self, cls, model):\n+                raise ImproperlyConfigured(\"error!\")\n+\n+        class MyModelAdmin(admin.ModelAdmin):\n+            validator = MyValidator\n+\n+        errors = MyModelAdmin.check(model=Song)\n+        expected = [\n+            checks.Error(\n+                'error!',\n+                hint=None,\n+                obj=MyModelAdmin,\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\ndiff --git a/tests/check/__init__.py b/tests/admin_scripts/app_raising_messages/__init__.py\nsimilarity index 100%\nrename from tests/check/__init__.py\nrename to tests/admin_scripts/app_raising_messages/__init__.py\ndiff --git a/tests/admin_scripts/app_raising_messages/models.py b/tests/admin_scripts/app_raising_messages/models.py\nnew file mode 100644\nindex 000000000000..aece8a81765a\n--- /dev/null\n+++ b/tests/admin_scripts/app_raising_messages/models.py\n@@ -0,0 +1,27 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from django.core import checks\n+from django.db import models\n+\n+\n+class ModelRaisingMessages(models.Model):\n+    @classmethod\n+    def check(self, **kwargs):\n+        return [\n+            checks.Warning(\n+                'First warning',\n+                hint='Hint',\n+                obj='obj'\n+            ),\n+            checks.Warning(\n+                'Second warning',\n+                hint=None,\n+                obj='a'\n+            ),\n+            checks.Error(\n+                'An error',\n+                hint='Error hint',\n+                obj=None,\n+            )\n+        ]\ndiff --git a/tests/invalid_models/invalid_models/__init__.py b/tests/admin_scripts/app_raising_warning/__init__.py\nsimilarity index 100%\nrename from tests/invalid_models/invalid_models/__init__.py\nrename to tests/admin_scripts/app_raising_warning/__init__.py\ndiff --git a/tests/admin_scripts/app_raising_warning/models.py b/tests/admin_scripts/app_raising_warning/models.py\nnew file mode 100644\nindex 000000000000..8f58abe12773\n--- /dev/null\n+++ b/tests/admin_scripts/app_raising_warning/models.py\n@@ -0,0 +1,16 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from django.core import checks\n+from django.db import models\n+\n+\n+class ModelRaisingMessages(models.Model):\n+    @classmethod\n+    def check(self, **kwargs):\n+        return [\n+            checks.Warning(\n+                'A warning',\n+                hint=None,\n+            ),\n+        ]\ndiff --git a/tests/admin_scripts/management/commands/app_command.py b/tests/admin_scripts/management/commands/app_command.py\nindex d26df1264297..ce97578f1981 100644\n--- a/tests/admin_scripts/management/commands/app_command.py\n+++ b/tests/admin_scripts/management/commands/app_command.py\n@@ -3,7 +3,7 @@\n \n class Command(AppCommand):\n     help = 'Test Application-based commands'\n-    requires_model_validation = False\n+    requires_system_checks = False\n     args = '[appname ...]'\n \n     def handle_app(self, app, **options):\ndiff --git a/tests/admin_scripts/management/commands/base_command.py b/tests/admin_scripts/management/commands/base_command.py\nindex 2701f32a056f..568a8c1521f4 100644\n--- a/tests/admin_scripts/management/commands/base_command.py\n+++ b/tests/admin_scripts/management/commands/base_command.py\n@@ -10,7 +10,7 @@ class Command(BaseCommand):\n         make_option('--option_c','-c', action='store', dest='option_c', default='3'),\n     )\n     help = 'Test basic commands'\n-    requires_model_validation = False\n+    requires_system_checks = False\n     args = '[labels ...]'\n \n     def handle(self, *labels, **options):\ndiff --git a/tests/admin_scripts/management/commands/label_command.py b/tests/admin_scripts/management/commands/label_command.py\nindex 3bce1305bcda..9bba413ff3d3 100644\n--- a/tests/admin_scripts/management/commands/label_command.py\n+++ b/tests/admin_scripts/management/commands/label_command.py\n@@ -3,7 +3,7 @@\n \n class Command(LabelCommand):\n     help = \"Test Label-based commands\"\n-    requires_model_validation = False\n+    requires_system_checks = False\n     args = '<label>'\n \n     def handle_label(self, label, **options):\ndiff --git a/tests/admin_scripts/management/commands/noargs_command.py b/tests/admin_scripts/management/commands/noargs_command.py\nindex 9652099f9bac..ecc394554f52 100644\n--- a/tests/admin_scripts/management/commands/noargs_command.py\n+++ b/tests/admin_scripts/management/commands/noargs_command.py\n@@ -3,7 +3,7 @@\n \n class Command(NoArgsCommand):\n     help = \"Test No-args commands\"\n-    requires_model_validation = False\n+    requires_system_checks = False\n \n \n     def handle_noargs(self, **options):\ndiff --git a/tests/admin_scripts/management/commands/validation_command.py b/tests/admin_scripts/management/commands/validation_command.py\nnew file mode 100644\nindex 000000000000..e9ba86dc6cb9\n--- /dev/null\n+++ b/tests/admin_scripts/management/commands/validation_command.py\n@@ -0,0 +1,11 @@\n+from django.core.management.base import NoArgsCommand\n+\n+\n+class InvalidCommand(NoArgsCommand):\n+    help = (\"Test raising an error if both requires_system_checks \"\n+            \"and requires_model_validation are defined.\")\n+    requires_system_checks = True\n+    requires_model_validation = True\n+\n+    def handle_noargs(self, **options):\n+        pass\ndiff --git a/tests/admin_scripts/tests.py b/tests/admin_scripts/tests.py\nindex 9f7c8d7b5422..ba23b38086d1 100644\n--- a/tests/admin_scripts/tests.py\n+++ b/tests/admin_scripts/tests.py\n@@ -1,4 +1,6 @@\n # -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n \"\"\"\n A series of tests to establish that the command-line managment tools work as\n advertised - especially with regards to the handling of the DJANGO_SETTINGS_MODULE\n@@ -18,13 +20,15 @@\n import django\n from django import conf, get_version\n from django.conf import settings\n+from django.core.exceptions import ImproperlyConfigured\n from django.core.management import BaseCommand, CommandError, call_command\n from django.db import connection\n-from django.test.runner import DiscoverRunner\n from django.utils.encoding import force_text\n from django.utils._os import upath\n+from django.utils import six\n from django.utils.six import StringIO\n from django.test import LiveServerTestCase\n+from django.test.runner import DiscoverRunner\n \n \n test_dir = os.path.realpath(os.path.join(os.environ['DJANGO_TEST_TEMP_DIR'], 'test_project'))\n@@ -51,6 +55,7 @@ def write_settings(self, filename, apps=None, is_dir=False, sdict=None):\n                 'DATABASES',\n                 'ROOT_URLCONF',\n                 'SECRET_KEY',\n+                'TEST_RUNNER',  # We need to include TEST_RUNNER, otherwise we get a compatibility warning.\n             ]\n             for s in exports:\n                 if hasattr(settings, s):\n@@ -921,21 +926,36 @@ def test_custom_command_with_settings(self):\n         \"alternate: manage.py can execute user commands if settings are provided as argument\"\n         args = ['noargs_command', '--settings=alternate_settings']\n         out, err = self.run_manage(args)\n-        self.assertOutput(out, \"EXECUTE:NoArgsCommand options=[('no_color', False), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]\")\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n+        expected_out = (\"EXECUTE:NoArgsCommand \"\n+            \"options=[('no_color', False), ('pythonpath', None), \"\n+            \"('settings', 'alternate_settings'), ('traceback', None), \"\n+            \"('verbosity', %s)]\" % verbosity)\n+        self.assertOutput(out, expected_out)\n         self.assertNoOutput(err)\n \n     def test_custom_command_with_environment(self):\n         \"alternate: manage.py can execute user commands if settings are provided in environment\"\n         args = ['noargs_command']\n         out, err = self.run_manage(args, 'alternate_settings')\n-        self.assertOutput(out, \"EXECUTE:NoArgsCommand options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n+        expected_out = (\"EXECUTE:NoArgsCommand \"\n+            \"options=[('no_color', False), ('pythonpath', None), \"\n+            \"('settings', None), ('traceback', None), \"\n+            \"('verbosity', %s)]\" % verbosity)\n+        self.assertOutput(out, expected_out)\n         self.assertNoOutput(err)\n \n     def test_custom_command_output_color(self):\n         \"alternate: manage.py output syntax color can be deactivated with the `--no-color` option\"\n         args = ['noargs_command', '--no-color', '--settings=alternate_settings']\n         out, err = self.run_manage(args)\n-        self.assertOutput(out, \"EXECUTE:NoArgsCommand options=[('no_color', True), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]\")\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n+        expected_out = (\"EXECUTE:NoArgsCommand \"\n+            \"options=[('no_color', True), ('pythonpath', None), \"\n+            \"('settings', 'alternate_settings'), ('traceback', None), \"\n+            \"('verbosity', %s)]\" % verbosity)\n+        self.assertOutput(out, expected_out)\n         self.assertNoOutput(err)\n \n \n@@ -1051,49 +1071,122 @@ def test_builtin_command_with_attribute_error(self):\n         self.assertOutput(err, \"AttributeError: 'list' object has no attribute 'crash'\")\n \n \n-class ManageValidate(AdminScriptTestCase):\n+class ManageCheck(AdminScriptTestCase):\n+\n     def tearDown(self):\n         self.remove_settings('settings.py')\n \n     def test_nonexistent_app(self):\n-        \"manage.py validate reports an error on a non-existent app in INSTALLED_APPS\"\n-        self.write_settings('settings.py', apps=['admin_scriptz.broken_app'], sdict={'USE_I18N': False})\n-        args = ['validate']\n+        \"\"\" manage.py check reports an error on a non-existent app in\n+        INSTALLED_APPS \"\"\"\n+\n+        self.write_settings('settings.py',\n+            apps=['admin_scriptz.broken_app'],\n+            sdict={'USE_I18N': False})\n+        args = ['check']\n         out, err = self.run_manage(args)\n         self.assertNoOutput(out)\n+        self.assertOutput(err, 'ImportError')\n         self.assertOutput(err, 'No module named')\n         self.assertOutput(err, 'admin_scriptz')\n \n     def test_broken_app(self):\n-        \"manage.py validate reports an ImportError if an app's models.py raises one on import\"\n+        \"\"\" manage.py check reports an ImportError if an app's models.py\n+        raises one on import \"\"\"\n+\n         self.write_settings('settings.py', apps=['admin_scripts.broken_app'])\n-        args = ['validate']\n+        args = ['check']\n         out, err = self.run_manage(args)\n         self.assertNoOutput(out)\n         self.assertOutput(err, 'ImportError')\n \n     def test_complex_app(self):\n-        \"manage.py validate does not raise an ImportError validating a complex app with nested calls to load_app\"\n-        self.write_settings('settings.py',\n-            apps=['admin_scripts.complex_app', 'admin_scripts.simple_app'],\n-            sdict={'DEBUG': True})\n-        args = ['validate']\n+        \"\"\" manage.py check does not raise an ImportError validating a\n+        complex app with nested calls to load_app \"\"\"\n+\n+        self.write_settings(\n+            'settings.py',\n+            apps=[\n+                'admin_scripts.complex_app',\n+                'admin_scripts.simple_app',\n+                'django.contrib.admin',\n+                'django.contrib.auth',\n+                'django.contrib.contenttypes',\n+            ],\n+            sdict={\n+                'DEBUG': True\n+            }\n+        )\n+        args = ['check']\n         out, err = self.run_manage(args)\n         self.assertNoOutput(err)\n-        self.assertOutput(out, '0 errors found')\n+        self.assertEqual(out, 'System check identified no problems.\\n')\n \n     def test_app_with_import(self):\n-        \"manage.py validate does not raise errors when an app imports a base class that itself has an abstract base\"\n+        \"\"\" manage.py check does not raise errors when an app imports a base\n+        class that itself has an abstract base. \"\"\"\n+\n         self.write_settings('settings.py',\n             apps=['admin_scripts.app_with_import',\n                   'django.contrib.auth',\n                   'django.contrib.contenttypes',\n                   'django.contrib.sites'],\n             sdict={'DEBUG': True})\n-        args = ['validate']\n+        args = ['check']\n         out, err = self.run_manage(args)\n         self.assertNoOutput(err)\n-        self.assertOutput(out, '0 errors found')\n+        self.assertEqual(out, 'System check identified no problems.\\n')\n+\n+    def test_output_format(self):\n+        \"\"\" All errors/warnings should be sorted by level and by message. \"\"\"\n+\n+        self.write_settings('settings.py',\n+            apps=['admin_scripts.app_raising_messages'],\n+            sdict={'DEBUG': True})\n+        args = ['check']\n+        out, err = self.run_manage(args)\n+        expected_err = (\n+            \"CommandError: There are some issues:\\n\"\n+            \"\\n\"\n+            \"ERRORS:\\n\"\n+            \"?: An error\\n\"\n+            \"\\tHINT: Error hint\\n\"\n+            \"\\n\"\n+            \"WARNINGS:\\n\"\n+            \"a: Second warning\\n\"\n+            \"obj: First warning\\n\"\n+            \"\\tHINT: Hint\\n\"\n+            \"\\n\"\n+            \"System check identified 3 issues.\\n\"\n+        )\n+        self.assertEqual(err, expected_err)\n+        self.assertNoOutput(out)\n+\n+    def test_warning_does_not_halt(self):\n+        \"\"\"\n+        When there are only warnings or less serious messages, then Django\n+        should not prevent user from launching their project, so `check`\n+        command should not raise `CommandError` exception.\n+\n+        In this test we also test output format.\n+\n+        \"\"\"\n+\n+        self.write_settings('settings.py',\n+            apps=['admin_scripts.app_raising_warning'],\n+            sdict={'DEBUG': True})\n+        args = ['check']\n+        out, err = self.run_manage(args)\n+        expected_err = (\n+            \"There are some issues:\\n\"  # No \"CommandError: \" part\n+            \"\\n\"\n+            \"WARNINGS:\\n\"\n+            \"?: A warning\\n\"\n+            \"\\n\"\n+            \"System check identified 1 issue.\\n\"\n+        )\n+        self.assertEqual(err, expected_err)\n+        self.assertNoOutput(out)\n \n \n class CustomTestRunner(DiscoverRunner):\n@@ -1288,37 +1381,44 @@ def test_no_color(self):\n     def test_base_command(self):\n         \"User BaseCommands can execute when a label is provided\"\n         args = ['base_command', 'testlabel']\n-        out, err = self.run_manage(args)\n-        self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=('testlabel',), options=[('no_color', False), ('option_a', '1'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        expected_labels = \"('testlabel',)\"\n+        self._test_base_command(args, expected_labels)\n \n     def test_base_command_no_label(self):\n         \"User BaseCommands can execute when no labels are provided\"\n         args = ['base_command']\n-        out, err = self.run_manage(args)\n-        self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=(), options=[('no_color', False), ('option_a', '1'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        expected_labels = \"()\"\n+        self._test_base_command(args, expected_labels)\n \n     def test_base_command_multiple_label(self):\n         \"User BaseCommands can execute when no labels are provided\"\n         args = ['base_command', 'testlabel', 'anotherlabel']\n-        out, err = self.run_manage(args)\n-        self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=('testlabel', 'anotherlabel'), options=[('no_color', False), ('option_a', '1'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        expected_labels = \"('testlabel', 'anotherlabel')\"\n+        self._test_base_command(args, expected_labels)\n \n     def test_base_command_with_option(self):\n         \"User BaseCommands can execute with options when a label is provided\"\n         args = ['base_command', 'testlabel', '--option_a=x']\n-        out, err = self.run_manage(args)\n-        self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=('testlabel',), options=[('no_color', False), ('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        expected_labels = \"('testlabel',)\"\n+        self._test_base_command(args, expected_labels, option_a=\"'x'\")\n \n     def test_base_command_with_options(self):\n         \"User BaseCommands can execute with multiple options when a label is provided\"\n         args = ['base_command', 'testlabel', '-a', 'x', '--option_b=y']\n+        expected_labels = \"('testlabel',)\"\n+        self._test_base_command(args, expected_labels, option_a=\"'x'\", option_b=\"'y'\")\n+\n+    def _test_base_command(self, args, labels, option_a=\"'1'\", option_b=\"'2'\"):\n         out, err = self.run_manage(args)\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n+\n+        expected_out = (\"EXECUTE:BaseCommand labels=%s, \"\n+            \"options=[('no_color', False), ('option_a', %s), ('option_b', %s), \"\n+            \"('option_c', '3'), ('pythonpath', None), ('settings', None), \"\n+            \"('traceback', None), ('verbosity', %s)]\"\n+            % (labels, option_a, option_b, verbosity))\n         self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=('testlabel',), options=[('no_color', False), ('option_a', 'x'), ('option_b', 'y'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        self.assertOutput(out, expected_out)\n \n     def test_base_run_from_argv(self):\n         \"\"\"\n@@ -1363,8 +1463,9 @@ def test_noargs(self):\n         \"NoArg Commands can be executed\"\n         args = ['noargs_command']\n         out, err = self.run_manage(args)\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n         self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:NoArgsCommand options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        self.assertOutput(out, \"EXECUTE:NoArgsCommand options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', %s)]\" % verbosity)\n \n     def test_noargs_with_args(self):\n         \"NoArg Commands raise an error if an argument is provided\"\n@@ -1376,10 +1477,11 @@ def test_app_command(self):\n         \"User AppCommands can execute when a single app name is provided\"\n         args = ['app_command', 'auth']\n         out, err = self.run_manage(args)\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n         self.assertNoOutput(err)\n         self.assertOutput(out, \"EXECUTE:AppCommand app=<module 'django.contrib.auth.models'\")\n         self.assertOutput(out, os.sep.join(['django', 'contrib', 'auth', 'models.py']))\n-        self.assertOutput(out, \"'>, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        self.assertOutput(out, \"'>, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', %s)]\" % verbosity)\n \n     def test_app_command_no_apps(self):\n         \"User AppCommands raise an error when no app name is provided\"\n@@ -1391,13 +1493,14 @@ def test_app_command_multiple_apps(self):\n         \"User AppCommands raise an error when multiple app names are provided\"\n         args = ['app_command', 'auth', 'contenttypes']\n         out, err = self.run_manage(args)\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n         self.assertNoOutput(err)\n         self.assertOutput(out, \"EXECUTE:AppCommand app=<module 'django.contrib.auth.models'\")\n         self.assertOutput(out, os.sep.join(['django', 'contrib', 'auth', 'models.py']))\n-        self.assertOutput(out, \"'>, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        self.assertOutput(out, \"'>, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', %s)]\" % verbosity)\n         self.assertOutput(out, \"EXECUTE:AppCommand app=<module 'django.contrib.contenttypes.models'\")\n         self.assertOutput(out, os.sep.join(['django', 'contrib', 'contenttypes', 'models.py']))\n-        self.assertOutput(out, \"'>, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        self.assertOutput(out, \"'>, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', %s)]\" % verbosity)\n \n     def test_app_command_invalid_appname(self):\n         \"User AppCommands can execute when a single app name is provided\"\n@@ -1415,8 +1518,9 @@ def test_label_command(self):\n         \"User LabelCommands can execute when a label is provided\"\n         args = ['label_command', 'testlabel']\n         out, err = self.run_manage(args)\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n         self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:LabelCommand label=testlabel, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        self.assertOutput(out, \"EXECUTE:LabelCommand label=testlabel, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', %s)]\" % verbosity)\n \n     def test_label_command_no_label(self):\n         \"User LabelCommands raise an error if no label is provided\"\n@@ -1428,9 +1532,15 @@ def test_label_command_multiple_label(self):\n         \"User LabelCommands are executed multiple times if multiple labels are provided\"\n         args = ['label_command', 'testlabel', 'anotherlabel']\n         out, err = self.run_manage(args)\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n         self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:LabelCommand label=testlabel, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n-        self.assertOutput(out, \"EXECUTE:LabelCommand label=anotherlabel, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]\")\n+        self.assertOutput(out, \"EXECUTE:LabelCommand label=testlabel, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', %s)]\" % verbosity)\n+        self.assertOutput(out, \"EXECUTE:LabelCommand label=anotherlabel, options=[('no_color', False), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', %s)]\" % verbosity)\n+\n+    def test_requires_model_validation_and_requires_system_checks_both_defined(self):\n+        from .management.commands.validation_command import InvalidCommand\n+        self.assertRaises(ImproperlyConfigured, InvalidCommand)\n+\n \n class ArgumentOrder(AdminScriptTestCase):\n     \"\"\"Tests for 2-stage argument parsing scheme.\n@@ -1450,39 +1560,40 @@ def tearDown(self):\n         self.remove_settings('alternate_settings.py')\n \n     def test_setting_then_option(self):\n-        \"Options passed after settings are correctly handled\"\n+        \"\"\" Options passed after settings are correctly handled. \"\"\"\n         args = ['base_command', 'testlabel', '--settings=alternate_settings', '--option_a=x']\n-        out, err = self.run_manage(args)\n-        self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=('testlabel',), options=[('no_color', False), ('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]\")\n+        self._test(args)\n \n     def test_setting_then_short_option(self):\n-        \"Short options passed after settings are correctly handled\"\n-        args = ['base_command', 'testlabel', '--settings=alternate_settings', '--option_a=x']\n-        out, err = self.run_manage(args)\n-        self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=('testlabel',), options=[('no_color', False), ('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]\")\n+        \"\"\" Short options passed after settings are correctly handled. \"\"\"\n+        args = ['base_command', 'testlabel', '--settings=alternate_settings', '-a', 'x']\n+        self._test(args)\n \n     def test_option_then_setting(self):\n-        \"Options passed before settings are correctly handled\"\n+        \"\"\" Options passed before settings are correctly handled. \"\"\"\n         args = ['base_command', 'testlabel', '--option_a=x', '--settings=alternate_settings']\n-        out, err = self.run_manage(args)\n-        self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=('testlabel',), options=[('no_color', False), ('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]\")\n+        self._test(args)\n \n     def test_short_option_then_setting(self):\n-        \"Short options passed before settings are correctly handled\"\n+        \"\"\" Short options passed before settings are correctly handled. \"\"\"\n         args = ['base_command', 'testlabel', '-a', 'x', '--settings=alternate_settings']\n-        out, err = self.run_manage(args)\n-        self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=('testlabel',), options=[('no_color', False), ('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]\")\n+        self._test(args)\n \n     def test_option_then_setting_then_option(self):\n-        \"Options are correctly handled when they are passed before and after a setting\"\n+        \"\"\" Options are correctly handled when they are passed before and after\n+        a setting. \"\"\"\n         args = ['base_command', 'testlabel', '--option_a=x', '--settings=alternate_settings', '--option_b=y']\n+        self._test(args, option_b=\"'y'\")\n+\n+    def _test(self, args, option_b=\"'2'\"):\n         out, err = self.run_manage(args)\n+        verbosity = \"'1'\" if six.PY3 else \"u'1'\"\n+        expected_out = (\"EXECUTE:BaseCommand labels=('testlabel',), \"\n+            \"options=[('no_color', False), ('option_a', 'x'), ('option_b', %s), \"\n+            \"('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), \"\n+            \"('traceback', None), ('verbosity', %s)]\" % (option_b, verbosity))\n         self.assertNoOutput(err)\n-        self.assertOutput(out, \"EXECUTE:BaseCommand labels=('testlabel',), options=[('no_color', False), ('option_a', 'x'), ('option_b', 'y'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]\")\n+        self.assertOutput(out, expected_out)\n \n \n class StartProject(LiveServerTestCase, AdminScriptTestCase):\ndiff --git a/tests/admin_validation/tests.py b/tests/admin_validation/tests.py\ndeleted file mode 100644\nindex 8096d486d38a..000000000000\n--- a/tests/admin_validation/tests.py\n+++ /dev/null\n@@ -1,304 +0,0 @@\n-from __future__ import unicode_literals\n-\n-from django import forms\n-from django.contrib import admin\n-from django.core.exceptions import ImproperlyConfigured\n-from django.test import TestCase\n-from django.test.utils import str_prefix\n-\n-from .models import Song, Book, Album, TwoAlbumFKAndAnE, State, City\n-\n-\n-class SongForm(forms.ModelForm):\n-    pass\n-\n-class ValidFields(admin.ModelAdmin):\n-    form = SongForm\n-    fields = ['title']\n-\n-class ValidFormFieldsets(admin.ModelAdmin):\n-    def get_form(self, request, obj=None, **kwargs):\n-        class ExtraFieldForm(SongForm):\n-            name = forms.CharField(max_length=50)\n-        return ExtraFieldForm\n-\n-    fieldsets = (\n-        (None, {\n-            'fields': ('name',),\n-        }),\n-    )\n-\n-class ValidationTestCase(TestCase):\n-\n-    def test_readonly_and_editable(self):\n-        class SongAdmin(admin.ModelAdmin):\n-            readonly_fields = [\"original_release\"]\n-            fieldsets = [\n-                (None, {\n-                    \"fields\": [\"title\", \"original_release\"],\n-                }),\n-            ]\n-        SongAdmin.validate(Song)\n-\n-    def test_custom_modelforms_with_fields_fieldsets(self):\n-        \"\"\"\n-        # Regression test for #8027: custom ModelForms with fields/fieldsets\n-        \"\"\"\n-        ValidFields.validate(Song)\n-\n-    def test_custom_get_form_with_fieldsets(self):\n-        \"\"\"\n-        Ensure that the fieldsets validation is skipped when the ModelAdmin.get_form() method\n-        is overridden.\n-        Refs #19445.\n-        \"\"\"\n-        ValidFormFieldsets.validate(Song)\n-\n-    def test_exclude_values(self):\n-        \"\"\"\n-        Tests for basic validation of 'exclude' option values (#12689)\n-        \"\"\"\n-        class ExcludedFields1(admin.ModelAdmin):\n-            exclude = ('foo')\n-        self.assertRaisesMessage(ImproperlyConfigured,\n-            \"'ExcludedFields1.exclude' must be a list or tuple.\",\n-            ExcludedFields1.validate,\n-            Book)\n-\n-    def test_exclude_duplicate_values(self):\n-        class ExcludedFields2(admin.ModelAdmin):\n-            exclude = ('name', 'name')\n-        self.assertRaisesMessage(ImproperlyConfigured,\n-            \"There are duplicate field(s) in ExcludedFields2.exclude\",\n-            ExcludedFields2.validate,\n-            Book)\n-\n-    def test_exclude_in_inline(self):\n-        class ExcludedFieldsInline(admin.TabularInline):\n-            model = Song\n-            exclude = ('foo')\n-\n-        class ExcludedFieldsAlbumAdmin(admin.ModelAdmin):\n-            model = Album\n-            inlines = [ExcludedFieldsInline]\n-\n-        self.assertRaisesMessage(ImproperlyConfigured,\n-            \"'ExcludedFieldsInline.exclude' must be a list or tuple.\",\n-            ExcludedFieldsAlbumAdmin.validate,\n-            Album)\n-\n-    def test_exclude_inline_model_admin(self):\n-        \"\"\"\n-        # Regression test for #9932 - exclude in InlineModelAdmin\n-        # should not contain the ForeignKey field used in ModelAdmin.model\n-        \"\"\"\n-        class SongInline(admin.StackedInline):\n-            model = Song\n-            exclude = ['album']\n-\n-        class AlbumAdmin(admin.ModelAdmin):\n-            model = Album\n-            inlines = [SongInline]\n-\n-        self.assertRaisesMessage(ImproperlyConfigured,\n-            \"SongInline cannot exclude the field 'album' - this is the foreign key to the parent model admin_validation.Album.\",\n-            AlbumAdmin.validate,\n-            Album)\n-\n-    def test_app_label_in_admin_validation(self):\n-        \"\"\"\n-        Regression test for #15669 - Include app label in admin validation messages\n-        \"\"\"\n-        class RawIdNonexistingAdmin(admin.ModelAdmin):\n-            raw_id_fields = ('nonexisting',)\n-\n-        self.assertRaisesMessage(ImproperlyConfigured,\n-            \"'RawIdNonexistingAdmin.raw_id_fields' refers to field 'nonexisting' that is missing from model 'admin_validation.Album'.\",\n-            RawIdNonexistingAdmin.validate,\n-            Album)\n-\n-    def test_fk_exclusion(self):\n-        \"\"\"\n-        Regression test for #11709 - when testing for fk excluding (when exclude is\n-        given) make sure fk_name is honored or things blow up when there is more\n-        than one fk to the parent model.\n-        \"\"\"\n-        class TwoAlbumFKAndAnEInline(admin.TabularInline):\n-            model = TwoAlbumFKAndAnE\n-            exclude = (\"e\",)\n-            fk_name = \"album1\"\n-        class MyAdmin(admin.ModelAdmin):\n-            inlines = [TwoAlbumFKAndAnEInline]\n-        MyAdmin.validate(Album)\n-\n-\n-    def test_inline_self_validation(self):\n-        class TwoAlbumFKAndAnEInline(admin.TabularInline):\n-            model = TwoAlbumFKAndAnE\n-        class MyAdmin(admin.ModelAdmin):\n-            inlines = [TwoAlbumFKAndAnEInline]\n-\n-        self.assertRaisesMessage(Exception,\n-            \"<class 'admin_validation.models.TwoAlbumFKAndAnE'> has more than 1 ForeignKey to <class 'admin_validation.models.Album'>\",\n-            MyAdmin.validate, Album)\n-\n-    def test_inline_with_specified(self):\n-        class TwoAlbumFKAndAnEInline(admin.TabularInline):\n-            model = TwoAlbumFKAndAnE\n-            fk_name = \"album1\"\n-\n-        class MyAdmin(admin.ModelAdmin):\n-            inlines = [TwoAlbumFKAndAnEInline]\n-        MyAdmin.validate(Album)\n-\n-    def test_readonly(self):\n-        class SongAdmin(admin.ModelAdmin):\n-            readonly_fields = (\"title\",)\n-\n-        SongAdmin.validate(Song)\n-\n-    def test_readonly_on_method(self):\n-        def my_function(obj):\n-            pass\n-\n-        class SongAdmin(admin.ModelAdmin):\n-            readonly_fields = (my_function,)\n-\n-        SongAdmin.validate(Song)\n-\n-    def test_readonly_on_modeladmin(self):\n-        class SongAdmin(admin.ModelAdmin):\n-            readonly_fields = (\"readonly_method_on_modeladmin\",)\n-\n-            def readonly_method_on_modeladmin(self, obj):\n-                pass\n-\n-        SongAdmin.validate(Song)\n-\n-    def test_readonly_method_on_model(self):\n-        class SongAdmin(admin.ModelAdmin):\n-            readonly_fields = (\"readonly_method_on_model\",)\n-\n-        SongAdmin.validate(Song)\n-\n-    def test_nonexistant_field(self):\n-        class SongAdmin(admin.ModelAdmin):\n-            readonly_fields = (\"title\", \"nonexistant\")\n-\n-        self.assertRaisesMessage(ImproperlyConfigured,\n-            str_prefix(\"SongAdmin.readonly_fields[1], %(_)s'nonexistant' is not a callable \"\n-                       \"or an attribute of 'SongAdmin' or found in the model 'Song'.\"),\n-            SongAdmin.validate,\n-            Song)\n-\n-    def test_nonexistant_field_on_inline(self):\n-        class CityInline(admin.TabularInline):\n-            model = City\n-            readonly_fields=['i_dont_exist'] # Missing attribute\n-\n-        self.assertRaisesMessage(ImproperlyConfigured,\n-            str_prefix(\"CityInline.readonly_fields[0], %(_)s'i_dont_exist' is not a callable \"\n-                       \"or an attribute of 'CityInline' or found in the model 'City'.\"),\n-            CityInline.validate,\n-            City)\n-\n-    def test_extra(self):\n-        class SongAdmin(admin.ModelAdmin):\n-            def awesome_song(self, instance):\n-                if instance.title == \"Born to Run\":\n-                    return \"Best Ever!\"\n-                return \"Status unknown.\"\n-        SongAdmin.validate(Song)\n-\n-    def test_readonly_lambda(self):\n-        class SongAdmin(admin.ModelAdmin):\n-            readonly_fields = (lambda obj: \"test\",)\n-\n-        SongAdmin.validate(Song)\n-\n-    def test_graceful_m2m_fail(self):\n-        \"\"\"\n-        Regression test for #12203/#12237 - Fail more gracefully when a M2M field that\n-        specifies the 'through' option is included in the 'fields' or the 'fieldsets'\n-        ModelAdmin options.\n-        \"\"\"\n-\n-        class BookAdmin(admin.ModelAdmin):\n-            fields = ['authors']\n-\n-        self.assertRaisesMessage(ImproperlyConfigured,\n-            \"'BookAdmin.fields' can't include the ManyToManyField field 'authors' because 'authors' manually specifies a 'through' model.\",\n-            BookAdmin.validate,\n-            Book)\n-\n-    def test_cannot_include_through(self):\n-        class FieldsetBookAdmin(admin.ModelAdmin):\n-            fieldsets = (\n-                ('Header 1', {'fields': ('name',)}),\n-                ('Header 2', {'fields': ('authors',)}),\n-            )\n-        self.assertRaisesMessage(ImproperlyConfigured,\n-            \"'FieldsetBookAdmin.fieldsets[1][1]['fields']' can't include the ManyToManyField field 'authors' because 'authors' manually specifies a 'through' model.\",\n-            FieldsetBookAdmin.validate,\n-            Book)\n-\n-    def test_nested_fields(self):\n-        class NestedFieldsAdmin(admin.ModelAdmin):\n-            fields = ('price', ('name', 'subtitle'))\n-        NestedFieldsAdmin.validate(Book)\n-\n-    def test_nested_fieldsets(self):\n-        class NestedFieldsetAdmin(admin.ModelAdmin):\n-            fieldsets = (\n-                ('Main', {'fields': ('price', ('name', 'subtitle'))}),\n-            )\n-        NestedFieldsetAdmin.validate(Book)\n-\n-    def test_explicit_through_override(self):\n-        \"\"\"\n-        Regression test for #12209 -- If the explicitly provided through model\n-        is specified as a string, the admin should still be able use\n-        Model.m2m_field.through\n-        \"\"\"\n-\n-        class AuthorsInline(admin.TabularInline):\n-            model = Book.authors.through\n-\n-        class BookAdmin(admin.ModelAdmin):\n-            inlines = [AuthorsInline]\n-\n-        # If the through model is still a string (and hasn't been resolved to a model)\n-        # the validation will fail.\n-        BookAdmin.validate(Book)\n-\n-    def test_non_model_fields(self):\n-        \"\"\"\n-        Regression for ensuring ModelAdmin.fields can contain non-model fields\n-        that broke with r11737\n-        \"\"\"\n-        class SongForm(forms.ModelForm):\n-            extra_data = forms.CharField()\n-\n-        class FieldsOnFormOnlyAdmin(admin.ModelAdmin):\n-            form = SongForm\n-            fields = ['title', 'extra_data']\n-\n-        FieldsOnFormOnlyAdmin.validate(Song)\n-\n-    def test_non_model_first_field(self):\n-        \"\"\"\n-        Regression for ensuring ModelAdmin.field can handle first elem being a\n-        non-model field (test fix for UnboundLocalError introduced with r16225).\n-        \"\"\"\n-        class SongForm(forms.ModelForm):\n-            extra_data = forms.CharField()\n-            class Meta:\n-                model = Song\n-                fields = '__all__'\n-\n-\n-        class FieldsOnFormOnlyAdmin(admin.ModelAdmin):\n-            form = SongForm\n-            fields = ['extra_data', 'title']\n-\n-        FieldsOnFormOnlyAdmin.validate(Song)\ndiff --git a/tests/check/tests.py b/tests/check/tests.py\ndeleted file mode 100644\nindex 19b3840a9aaf..000000000000\n--- a/tests/check/tests.py\n+++ /dev/null\n@@ -1,127 +0,0 @@\n-from django.core.checks.compatibility import base\n-from django.core.checks.compatibility import django_1_6_0\n-from django.core.management.commands import check\n-from django.core.management import call_command\n-from django.db.models.fields import NOT_PROVIDED\n-from django.test import TestCase\n-\n-from .models import Book\n-\n-class StubCheckModule(object):\n-    # Has no ``run_checks`` attribute & will trigger a warning.\n-    __name__ = 'StubCheckModule'\n-\n-\n-class FakeWarnings(object):\n-    def __init__(self):\n-        self._warnings = []\n-\n-    def warn(self, message):\n-        self._warnings.append(message)\n-\n-\n-class CompatChecksTestCase(TestCase):\n-    def setUp(self):\n-        super(CompatChecksTestCase, self).setUp()\n-\n-        # We're going to override the list of checks to perform for test\n-        # consistency in the future.\n-        self.old_compat_checks = base.COMPAT_CHECKS\n-        base.COMPAT_CHECKS = [\n-            django_1_6_0,\n-        ]\n-\n-    def tearDown(self):\n-        # Restore what's supposed to be in ``COMPAT_CHECKS``.\n-        base.COMPAT_CHECKS = self.old_compat_checks\n-        super(CompatChecksTestCase, self).tearDown()\n-\n-    def test_check_test_runner_new_default(self):\n-        with self.settings(TEST_RUNNER='django.test.runner.DiscoverRunner'):\n-            result = django_1_6_0.check_test_runner()\n-            self.assertTrue(\"Django 1.6 introduced a new default test runner\" in result)\n-\n-    def test_check_test_runner_overridden(self):\n-        with self.settings(TEST_RUNNER='myapp.test.CustomRunnner'):\n-            self.assertEqual(django_1_6_0.check_test_runner(), None)\n-\n-    def test_run_checks_new_default(self):\n-        with self.settings(TEST_RUNNER='django.test.runner.DiscoverRunner'):\n-            result = django_1_6_0.run_checks()\n-            self.assertEqual(len(result), 1)\n-            self.assertTrue(\"Django 1.6 introduced a new default test runner\" in result[0])\n-\n-    def test_run_checks_overridden(self):\n-        with self.settings(TEST_RUNNER='myapp.test.CustomRunnner'):\n-            self.assertEqual(len(django_1_6_0.run_checks()), 0)\n-\n-    def test_boolean_field_default_value(self):\n-        with self.settings(TEST_RUNNER='myapp.test.CustomRunnner'):\n-            # We patch the field's default value to trigger the warning\n-            boolean_field = Book._meta.get_field('is_published')\n-            old_default = boolean_field.default\n-            try:\n-                boolean_field.default = NOT_PROVIDED\n-                result = django_1_6_0.run_checks()\n-                self.assertEqual(len(result), 1)\n-                self.assertTrue(\"You have not set a default value for one or more BooleanFields\" in result[0])\n-                self.assertTrue('check.Book: \"is_published\"' in result[0])\n-                # We did not patch the BlogPost.is_published field so\n-                # there should not be a warning about it\n-                self.assertFalse('check.BlogPost' in result[0])\n-            finally:\n-                # Restore the ``default``\n-                boolean_field.default = old_default\n-\n-    def test_check_compatibility(self):\n-        with self.settings(TEST_RUNNER='django.test.runner.DiscoverRunner'):\n-            result = base.check_compatibility()\n-            self.assertEqual(len(result), 1)\n-            self.assertTrue(\"Django 1.6 introduced a new default test runner\" in result[0])\n-\n-        with self.settings(TEST_RUNNER='myapp.test.CustomRunnner'):\n-            self.assertEqual(len(base.check_compatibility()), 0)\n-\n-    def test_check_compatibility_warning(self):\n-        # First, we're patching over the ``COMPAT_CHECKS`` with a stub which\n-        # will trigger the warning.\n-        base.COMPAT_CHECKS = [\n-            StubCheckModule(),\n-        ]\n-\n-        # Next, we unfortunately have to patch out ``warnings``.\n-        old_warnings = base.warnings\n-        base.warnings = FakeWarnings()\n-\n-        self.assertEqual(len(base.warnings._warnings), 0)\n-\n-        with self.settings(TEST_RUNNER='myapp.test.CustomRunnner'):\n-            self.assertEqual(len(base.check_compatibility()), 0)\n-\n-        self.assertEqual(len(base.warnings._warnings), 1)\n-        self.assertTrue(\"The 'StubCheckModule' module lacks a 'run_checks'\" in base.warnings._warnings[0])\n-\n-        # Restore the ``warnings``.\n-        base.warnings = old_warnings\n-\n-    def test_management_command(self):\n-        # Again, we unfortunately have to patch out ``warnings``. Different\n-        old_warnings = check.warnings\n-        check.warnings = FakeWarnings()\n-\n-        self.assertEqual(len(check.warnings._warnings), 0)\n-\n-        # Should not produce any warnings.\n-        with self.settings(TEST_RUNNER='myapp.test.CustomRunnner'):\n-            call_command('check')\n-\n-        self.assertEqual(len(check.warnings._warnings), 0)\n-\n-        with self.settings(TEST_RUNNER='django.test.runner.DiscoverRunner'):\n-            call_command('check')\n-\n-        self.assertEqual(len(check.warnings._warnings), 1)\n-        self.assertTrue(\"Django 1.6 introduced a new default test runner\" in check.warnings._warnings[0])\n-\n-        # Restore the ``warnings``.\n-        base.warnings = old_warnings\ndiff --git a/tests/check_framework/__init__.py b/tests/check_framework/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/check/models.py b/tests/check_framework/models.py\nsimilarity index 61%\nrename from tests/check/models.py\nrename to tests/check_framework/models.py\nindex 212b01bdd2b9..da2bf0c58d23 100644\n--- a/tests/check/models.py\n+++ b/tests/check_framework/models.py\n@@ -1,9 +1,19 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n from django.db import models\n \n+\n+class SimpleModel(models.Model):\n+    field = models.IntegerField()\n+    manager = models.manager.Manager()\n+\n+\n class Book(models.Model):\n     title = models.CharField(max_length=250)\n     is_published = models.BooleanField(default=False)\n \n+\n class BlogPost(models.Model):\n     title = models.CharField(max_length=250)\n     is_published = models.BooleanField(default=False)\ndiff --git a/tests/check_framework/tests.py b/tests/check_framework/tests.py\nnew file mode 100644\nindex 000000000000..65e445015bff\n--- /dev/null\n+++ b/tests/check_framework/tests.py\n@@ -0,0 +1,216 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from StringIO import StringIO\n+import sys\n+\n+from django.conf import settings\n+from django.core import checks\n+from django.core.checks import Error\n+from django.core.checks.messages import CheckMessage\n+from django.core.checks.registration import CheckFramework\n+from django.core.checks.compatibility.django_1_6_0 import check_1_6_compatibility\n+from django.core.management.base import CommandError\n+from django.core.management import call_command\n+from django.db.models.fields import NOT_PROVIDED\n+from django.test import TestCase\n+from django.test.utils import override_settings, override_system_checks\n+from django.utils.encoding import force_text\n+\n+from .models import SimpleModel, Book\n+\n+\n+class DummyObj(object):\n+    def __repr__(self):\n+        return \"obj\"\n+\n+\n+class SystemCheckFrameworkTests(TestCase):\n+\n+    def test_register_and_run_checks(self):\n+        calls = [0]\n+\n+        framework = CheckFramework()\n+        @framework.register()\n+        def f(**kwargs):\n+            calls[0] += 1\n+            return [1, 2, 3]\n+        errors = framework.run_checks()\n+        self.assertEqual(errors, [1, 2, 3])\n+        self.assertEqual(calls[0], 1)\n+\n+\n+class MessageTests(TestCase):\n+\n+    def test_printing(self):\n+        e = Error(\"Message\", hint=\"Hint\", obj=DummyObj())\n+        expected = \"obj: Message\\n\\tHINT: Hint\"\n+        self.assertEqual(force_text(e), expected)\n+\n+    def test_printing_no_hint(self):\n+        e = Error(\"Message\", hint=None, obj=DummyObj())\n+        expected = \"obj: Message\"\n+        self.assertEqual(force_text(e), expected)\n+\n+    def test_printing_no_object(self):\n+        e = Error(\"Message\", hint=\"Hint\", obj=None)\n+        expected = \"?: Message\\n\\tHINT: Hint\"\n+        self.assertEqual(force_text(e), expected)\n+\n+    def test_printing_with_given_id(self):\n+        e = Error(\"Message\", hint=\"Hint\", obj=DummyObj(), id=\"ID\")\n+        expected = \"obj: (ID) Message\\n\\tHINT: Hint\"\n+        self.assertEqual(force_text(e), expected)\n+\n+    def test_printing_field_error(self):\n+        field = SimpleModel._meta.get_field('field')\n+        e = Error(\"Error\", hint=None, obj=field)\n+        expected = \"check_framework.SimpleModel.field: Error\"\n+        self.assertEqual(force_text(e), expected)\n+\n+    def test_printing_model_error(self):\n+        e = Error(\"Error\", hint=None, obj=SimpleModel)\n+        expected = \"check_framework.SimpleModel: Error\"\n+        self.assertEqual(force_text(e), expected)\n+\n+    def test_printing_manager_error(self):\n+        manager = SimpleModel.manager\n+        e = Error(\"Error\", hint=None, obj=manager)\n+        expected = \"check_framework.SimpleModel.manager: Error\"\n+        self.assertEqual(force_text(e), expected)\n+\n+\n+class Django_1_6_0_CompatibilityChecks(TestCase):\n+\n+    @override_settings(TEST_RUNNER='django.test.runner.DiscoverRunner')\n+    def test_test_runner_new_default(self):\n+        errors = check_1_6_compatibility()\n+        self.assertEqual(errors, [])\n+\n+    @override_settings(TEST_RUNNER='myapp.test.CustomRunner')\n+    def test_test_runner_overriden(self):\n+        errors = check_1_6_compatibility()\n+        self.assertEqual(errors, [])\n+\n+    def test_test_runner_not_set_explicitly(self):\n+        # We remove the TEST_RUNNER attribute from custom settings module.\n+        old_test_runner = settings._wrapped.TEST_RUNNER\n+        del settings._wrapped.TEST_RUNNER\n+\n+        try:\n+            errors = check_1_6_compatibility()\n+            expected = [\n+                checks.Warning(\n+                   'You have not explicitly set \"TEST_RUNNER\". In Django 1.6, '\n+                        'there is a new test runner (\"django.test.runner.DiscoverRunner\") '\n+                        'by default. You should ensure your tests are still all '\n+                        'running & behaving as expected. See '\n+                        'https://docs.djangoproject.com/en/dev/releases/1.6/#discovery-of-tests-in-any-test-module '\n+                        'for more information.',\n+                    hint=None,\n+                    obj=None,\n+                    id='W047',\n+                )\n+            ]\n+            self.assertEqual(errors, expected)\n+        finally:\n+            # Restore TEST_RUNNER value\n+            settings._wrapped.TEST_RUNNER = old_test_runner\n+\n+    def test_boolean_field_default_value(self):\n+        with self.settings(TEST_RUNNER='myapp.test.CustomRunnner'):\n+            # We patch the field's default value to trigger the warning\n+            boolean_field = Book._meta.get_field('is_published')\n+            old_default = boolean_field.default\n+            try:\n+                boolean_field.default = NOT_PROVIDED\n+                errors = check_1_6_compatibility()\n+                expected = [\n+                    checks.Warning(\n+                        'The field has not set a default value. In Django 1.6 '\n+                            'the default value of BooleanField was changed from '\n+                            'False to Null when Field.default is not defined. '\n+                            'See https://docs.djangoproject.com/en/1.6/ref/models/fields/#booleanfield '\n+                            'for more information.',\n+                        hint=None,\n+                        obj=boolean_field,\n+                        id='W048',\n+                    )\n+                ]\n+                self.assertEqual(errors, expected)\n+            finally:\n+                # Restore the ``default``\n+                boolean_field.default = old_default\n+\n+\n+def simple_system_check(**kwargs):\n+    simple_system_check.kwargs = kwargs\n+    return []\n+\n+\n+def tagged_system_check(**kwargs):\n+    tagged_system_check.kwargs = kwargs\n+    return []\n+tagged_system_check.tags = ['simpletag']\n+\n+\n+class CheckCommandTests(TestCase):\n+\n+    def setUp(self):\n+        simple_system_check.kwargs = None\n+        tagged_system_check.kwargs = None\n+        self.old_stdout, self.old_stderr = sys.stdout, sys.stderr\n+        sys.stdout, sys.stderr = StringIO(), StringIO()\n+\n+    def tearDown(self):\n+        sys.stdout, sys.stderr = self.old_stdout, self.old_stderr\n+\n+    @override_system_checks([simple_system_check, tagged_system_check])\n+    def test_simple_call(self):\n+        call_command('check')\n+        self.assertEqual(simple_system_check.kwargs, {'apps': None})\n+        self.assertEqual(tagged_system_check.kwargs, {'apps': None})\n+\n+    @override_system_checks([simple_system_check, tagged_system_check])\n+    def test_given_app(self):\n+        call_command('check', 'auth', 'admin')\n+        self.assertEqual(simple_system_check.kwargs, {'apps': ('auth', 'admin')})\n+        self.assertEqual(tagged_system_check.kwargs, {'apps': ('auth', 'admin')})\n+\n+    @override_system_checks([simple_system_check, tagged_system_check])\n+    def test_given_tag(self):\n+        call_command('check', tags=['simpletag'])\n+        self.assertEqual(simple_system_check.kwargs, None)\n+        self.assertEqual(tagged_system_check.kwargs, {'apps': None})\n+\n+    @override_system_checks([simple_system_check, tagged_system_check])\n+    def test_invalid_tag(self):\n+        self.assertRaises(CommandError, call_command, 'check', tags=['missingtag'])\n+\n+\n+def custom_system_check(apps, **kwargs):\n+    return [\n+        Error(\n+            'Error',\n+            hint=None,\n+            id='mycheck.E001',\n+        )\n+    ]\n+\n+\n+class SilencingCheckTests(TestCase):\n+\n+    def setUp(self):\n+        self.old_stdout, self.old_stderr = sys.stdout, sys.stderr\n+        sys.stdout, sys.stderr = StringIO(), StringIO()\n+\n+    def tearDown(self):\n+        sys.stdout, sys.stderr = self.old_stdout, self.old_stderr\n+\n+    @override_settings(SILENCED_SYSTEM_CHECKS=['mycheck.E001'])\n+    @override_system_checks([custom_system_check])\n+    def test_simple(self):\n+        try:\n+            call_command('check')\n+        except CommandError:\n+            self.fail(\"The mycheck.E001 check should be silenced.\")\ndiff --git a/tests/contenttypes_tests/tests.py b/tests/contenttypes_tests/tests.py\nindex 63f02697df1c..bffde1e25ef5 100644\n--- a/tests/contenttypes_tests/tests.py\n+++ b/tests/contenttypes_tests/tests.py\n@@ -1,10 +1,18 @@\n-from __future__ import unicode_literals\n+# -*- coding: utf-8 -*-\n+from __future__ import absolute_import, unicode_literals\n \n+from django.contrib.contenttypes import generic\n from django.contrib.contenttypes.models import ContentType\n+from django.core import checks\n+from django.db import models\n+from django.db.models.loading import cache\n from django.test import TestCase\n+from django.test.utils import override_settings\n+from django.utils.encoding import force_str\n \n from .models import Author, Article\n \n+\n class ContentTypesViewsTests(TestCase):\n     fixtures = ['testdata.json']\n     urls = 'contenttypes_tests.urls'\n@@ -45,3 +53,301 @@ def test_bad_content_type(self):\n         short_url = '/shortcut/%s/%s/' % (42424242, an_author.pk)\n         response = self.client.get(short_url)\n         self.assertEqual(response.status_code, 404)\n+\n+\n+class IsolatedModelsTestCase(TestCase):\n+\n+    def setUp(self):\n+        # If you create a model in a test, the model is accessible in other\n+        # tests. To avoid this, we need to mock list of all models created in\n+        # `contenttypes_tests` module.\n+        self._old_app_models_cache = cache.app_models['contenttypes_tests']\n+        cache.app_models['contenttypes_tests'] = {}\n+        self._old_models_cache = cache._get_models_cache\n+        cache._get_models_cache = {}\n+\n+    def tearDown(self):\n+        cache.app_models['contenttypes_tests'] = self._old_app_models_cache\n+        cache._get_models_cache = self._old_models_cache\n+\n+\n+class GenericForeignKeyTests(IsolatedModelsTestCase):\n+\n+    def test_str(self):\n+        class Model(models.Model):\n+            field = generic.GenericForeignKey()\n+        expected = \"contenttypes_tests.Model.field\"\n+        actual = force_str(Model.field)\n+        self.assertEqual(expected, actual)\n+\n+    def test_missing_content_type_field(self):\n+        class TaggedItem(models.Model):\n+            # no content_type field\n+            object_id = models.PositiveIntegerField()\n+            content_object = generic.GenericForeignKey()\n+\n+        errors = TaggedItem.content_object.check()\n+        expected = [\n+            checks.Error(\n+                'The field refers to TaggedItem.content_type field which is missing.',\n+                hint=None,\n+                obj=TaggedItem.content_object,\n+                id='contenttypes.E005',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_invalid_content_type_field(self):\n+        class Model(models.Model):\n+            content_type = models.IntegerField()  # should be ForeignKey\n+            object_id = models.PositiveIntegerField()\n+            content_object = generic.GenericForeignKey(\n+                'content_type', 'object_id')\n+\n+        errors = Model.content_object.check()\n+        expected = [\n+            checks.Error(\n+                '\"content_type\" field is used by a GenericForeignKey '\n+                    'as content type field and therefore it must be '\n+                    'a ForeignKey.',\n+                hint=None,\n+                obj=Model.content_object,\n+                id='contenttypes.E006',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_content_type_field_pointing_to_wrong_model(self):\n+        class Model(models.Model):\n+            content_type = models.ForeignKey('self')  # should point to ContentType\n+            object_id = models.PositiveIntegerField()\n+            content_object = generic.GenericForeignKey(\n+                'content_type', 'object_id')\n+\n+        errors = Model.content_object.check()\n+        expected = [\n+            checks.Error(\n+                '\"content_type\" field is used by a GenericForeignKey '\n+                    'as content type field and therefore it must be '\n+                    'a ForeignKey to ContentType.',\n+                hint=None,\n+                obj=Model.content_object,\n+                id='contenttypes.E007',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_missing_object_id_field(self):\n+        class TaggedItem(models.Model):\n+            content_type = models.ForeignKey(ContentType)\n+            # missing object_id field\n+            content_object = generic.GenericForeignKey()\n+\n+        errors = TaggedItem.content_object.check()\n+        expected = [\n+            checks.Error(\n+                'The field refers to \"object_id\" field which is missing.',\n+                hint=None,\n+                obj=TaggedItem.content_object,\n+                id='contenttypes.E001',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_field_name_ending_with_underscore(self):\n+        class Model(models.Model):\n+            content_type = models.ForeignKey(ContentType)\n+            object_id = models.PositiveIntegerField()\n+            content_object_ = generic.GenericForeignKey(\n+                'content_type', 'object_id')\n+\n+        errors = Model.content_object_.check()\n+        expected = [\n+            checks.Error(\n+                'Field names must not end with underscores.',\n+                hint=None,\n+                obj=Model.content_object_,\n+                id='contenttypes.E002',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_generic_foreign_key_checks_are_performed(self):\n+        class MyGenericForeignKey(generic.GenericForeignKey):\n+            def check(self, **kwargs):\n+                return ['performed!']\n+\n+        class Model(models.Model):\n+            content_object = MyGenericForeignKey()\n+\n+        errors = checks.run_checks()\n+        self.assertEqual(errors, ['performed!'])\n+\n+\n+class GenericRelationshipTests(IsolatedModelsTestCase):\n+\n+    def test_valid_generic_relationship(self):\n+        class TaggedItem(models.Model):\n+            content_type = models.ForeignKey(ContentType)\n+            object_id = models.PositiveIntegerField()\n+            content_object = generic.GenericForeignKey()\n+\n+        class Bookmark(models.Model):\n+            tags = generic.GenericRelation('TaggedItem')\n+\n+        errors = Bookmark.tags.field.check()\n+        self.assertEqual(errors, [])\n+\n+    def test_valid_generic_relationship_with_explicit_fields(self):\n+        class TaggedItem(models.Model):\n+            custom_content_type = models.ForeignKey(ContentType)\n+            custom_object_id = models.PositiveIntegerField()\n+            content_object = generic.GenericForeignKey(\n+                'custom_content_type', 'custom_object_id')\n+\n+        class Bookmark(models.Model):\n+            tags = generic.GenericRelation('TaggedItem',\n+                content_type_field='custom_content_type',\n+                object_id_field='custom_object_id')\n+\n+        errors = Bookmark.tags.field.check()\n+        self.assertEqual(errors, [])\n+\n+    def test_pointing_to_missing_model(self):\n+        class Model(models.Model):\n+            rel = generic.GenericRelation('MissingModel')\n+\n+        errors = Model.rel.field.check()\n+        expected = [\n+            checks.Error(\n+                'The field has a relation with model MissingModel, '\n+                    'which has either not been installed or is abstract.',\n+                hint='Ensure that you did not misspell the model name and '\n+                    'the model is not abstract. Does your INSTALLED_APPS '\n+                    'setting contain the app where MissingModel is defined?',\n+                obj=Model.rel.field,\n+                id='E030',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_valid_self_referential_generic_relationship(self):\n+        class Model(models.Model):\n+            rel = generic.GenericRelation('Model')\n+            content_type = models.ForeignKey(ContentType)\n+            object_id = models.PositiveIntegerField()\n+            content_object = generic.GenericForeignKey(\n+                'content_type', 'object_id')\n+\n+        errors = Model.rel.field.check()\n+        self.assertEqual(errors, [])\n+\n+    def test_missing_content_type_field(self):\n+        class TaggedItem(models.Model):\n+            # no content_type field\n+            object_id = models.PositiveIntegerField()\n+            content_object = generic.GenericForeignKey()\n+\n+        class Bookmark(models.Model):\n+            tags = generic.GenericRelation('TaggedItem')\n+\n+        errors = Bookmark.tags.field.check()\n+        expected = [\n+            checks.Error(\n+                'The field refers to TaggedItem.content_type field which is missing.',\n+                hint=None,\n+                obj=Bookmark.tags.field,\n+                id='contenttypes.E005',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_missing_object_id_field(self):\n+        class TaggedItem(models.Model):\n+            content_type = models.ForeignKey(ContentType)\n+            # missing object_id field\n+            content_object = generic.GenericForeignKey()\n+\n+        class Bookmark(models.Model):\n+            tags = generic.GenericRelation('TaggedItem')\n+\n+        errors = Bookmark.tags.field.check()\n+        expected = [\n+            checks.Error(\n+                'The field refers to TaggedItem.object_id field which is missing.',\n+                hint=None,\n+                obj=Bookmark.tags.field,\n+                id='contenttypes.E003',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_missing_generic_foreign_key(self):\n+        class TaggedItem(models.Model):\n+            content_type = models.ForeignKey(ContentType)\n+            object_id = models.PositiveIntegerField()\n+\n+        class Bookmark(models.Model):\n+            tags = generic.GenericRelation('TaggedItem')\n+\n+        errors = Bookmark.tags.field.check()\n+        expected = [\n+            checks.Warning(\n+                'The field defines a generic relation with the model '\n+                    'contenttypes_tests.TaggedItem, but the model lacks '\n+                    'GenericForeignKey.',\n+                hint=None,\n+                obj=Bookmark.tags.field,\n+                id='contenttypes.E004',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    @override_settings(TEST_SWAPPED_MODEL='contenttypes_tests.Replacement')\n+    def test_pointing_to_swapped_model(self):\n+        class Replacement(models.Model):\n+            pass\n+\n+        class SwappedModel(models.Model):\n+            content_type = models.ForeignKey(ContentType)\n+            object_id = models.PositiveIntegerField()\n+            content_object = generic.GenericForeignKey()\n+\n+            class Meta:\n+                swappable = 'TEST_SWAPPED_MODEL'\n+\n+        class Model(models.Model):\n+            rel = generic.GenericRelation('SwappedModel')\n+\n+        errors = Model.rel.field.check()\n+        expected = [\n+            checks.Error(\n+                'The field defines a relation with the model '\n+                    'contenttypes_tests.SwappedModel, '\n+                    'which has been swapped out.',\n+                hint='Update the relation to point at settings.TEST_SWAPPED_MODEL',\n+                obj=Model.rel.field,\n+                id='E029',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_field_name_ending_with_underscore(self):\n+        class TaggedItem(models.Model):\n+            content_type = models.ForeignKey(ContentType)\n+            object_id = models.PositiveIntegerField()\n+            content_object = generic.GenericForeignKey()\n+\n+        class InvalidBookmark(models.Model):\n+            tags_ = generic.GenericRelation('TaggedItem')\n+\n+        errors = InvalidBookmark.tags_.field.check()\n+        expected = [\n+            checks.Error(\n+                'Field names must not end with underscores.',\n+                hint=None,\n+                obj=InvalidBookmark.tags_.field,\n+                id='E001',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\ndiff --git a/tests/fixtures_model_package/tests.py b/tests/fixtures_model_package/tests.py\nindex 2fa1fd10ef10..6b95833f9aa8 100644\n--- a/tests/fixtures_model_package/tests.py\n+++ b/tests/fixtures_model_package/tests.py\n@@ -5,6 +5,7 @@\n from django.core import management\n from django.db import transaction\n from django.test import TestCase, TransactionTestCase\n+from django.test.utils import override_system_checks\n from django.utils.six import StringIO\n \n from .models import Article, Book\n@@ -30,6 +31,7 @@ class TestNoInitialDataLoading(TransactionTestCase):\n \n     available_apps = ['fixtures_model_package']\n \n+    @override_system_checks([])\n     def test_migrate(self):\n         with transaction.atomic():\n             Book.objects.all().delete()\n@@ -42,6 +44,7 @@ def test_migrate(self):\n             self.assertQuerysetEqual(Book.objects.all(), [])\n \n \n+    @override_system_checks([])\n     def test_flush(self):\n         # Test presence of fixture (flush called by TransactionTestCase)\n         self.assertQuerysetEqual(\ndiff --git a/tests/inline_formsets/tests.py b/tests/inline_formsets/tests.py\nindex a16488dc7930..0143d7ed9528 100644\n--- a/tests/inline_formsets/tests.py\n+++ b/tests/inline_formsets/tests.py\n@@ -123,8 +123,9 @@ def test_exception_on_unspecified_foreign_key(self):\n         Child has two ForeignKeys to Parent, so if we don't specify which one\n         to use for the inline formset, we should get an exception.\n         \"\"\"\n-        six.assertRaisesRegex(self, Exception,\n-            \"<class 'inline_formsets.models.Child'> has more than 1 ForeignKey to <class 'inline_formsets.models.Parent'>\",\n+        six.assertRaisesRegex(self, ValueError,\n+            '\"fk_name\" must be explicitly defined, because inline_formsets.Child '\n+                'has more than one ForeignKey to inline_formsets.Parent.',\n             inlineformset_factory, Parent, Child\n         )\n \n@@ -143,8 +144,8 @@ def test_non_foreign_key_field(self):\n         If the field specified in fk_name is not a ForeignKey, we should get an\n         exception.\n         \"\"\"\n-        six.assertRaisesRegex(self, Exception,\n-            \"<class 'inline_formsets.models.Child'> has no field named 'test'\",\n+        six.assertRaisesRegex(self, ValueError,\n+            '\"fk_name\" refers to \"test\" field, which is missing from model inline_formsets.Child.',\n             inlineformset_factory, Parent, Child, fk_name='test'\n         )\n \ndiff --git a/tests/invalid_models/__init__.py b/tests/invalid_models/__init__.py\nindex e69de29bb2d1..934589e0364a 100644\n--- a/tests/invalid_models/__init__.py\n+++ b/tests/invalid_models/__init__.py\n@@ -0,0 +1,17 @@\n+# -*- encoding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from django.db.models.loading import cache\n+from django.test import TestCase\n+\n+\n+class IsolatedModelsTestCase(TestCase):\n+\n+    def setUp(self):\n+        # If you create a model in a test, the model is accessible in other\n+        # tests. To avoid this, we need to clear list of all models created in\n+        # `invalid_models` module.\n+        cache.app_models['invalid_models'] = {}\n+        cache._get_models_cache = {}\n+\n+    tearDown = setUp\ndiff --git a/tests/invalid_models/invalid_models/models.py b/tests/invalid_models/invalid_models/models.py\ndeleted file mode 100644\nindex 1113c0c05690..000000000000\n--- a/tests/invalid_models/invalid_models/models.py\n+++ /dev/null\n@@ -1,537 +0,0 @@\n-#encoding=utf-8\n-\"\"\"\n-26. Invalid models\n-\n-This example exists purely to point out errors in models.\n-\"\"\"\n-\n-from __future__ import unicode_literals\n-\n-from django.db import connection, models\n-\n-\n-class FieldErrors(models.Model):\n-    charfield = models.CharField()\n-    charfield2 = models.CharField(max_length=-1)\n-    charfield3 = models.CharField(max_length=\"bad\")\n-    decimalfield = models.DecimalField()\n-    decimalfield2 = models.DecimalField(max_digits=-1, decimal_places=-1)\n-    decimalfield3 = models.DecimalField(max_digits=\"bad\", decimal_places=\"bad\")\n-    decimalfield4 = models.DecimalField(max_digits=9, decimal_places=10)\n-    decimalfield5 = models.DecimalField(max_digits=10, decimal_places=10)\n-    filefield = models.FileField()\n-    choices = models.CharField(max_length=10, choices='bad')\n-    choices2 = models.CharField(max_length=10, choices=[(1, 2, 3), (1, 2, 3)])\n-    index = models.CharField(max_length=10, db_index='bad')\n-    field_ = models.CharField(max_length=10)\n-    nullbool = models.BooleanField(null=True)\n-    generic_ip_notnull_blank = models.GenericIPAddressField(null=False, blank=True)\n-\n-\n-class Target(models.Model):\n-    tgt_safe = models.CharField(max_length=10)\n-    clash1 = models.CharField(max_length=10)\n-    clash2 = models.CharField(max_length=10)\n-\n-    clash1_set = models.CharField(max_length=10)\n-\n-\n-class Clash1(models.Model):\n-    src_safe = models.CharField(max_length=10)\n-\n-    foreign = models.ForeignKey(Target)\n-    m2m = models.ManyToManyField(Target)\n-\n-\n-class Clash2(models.Model):\n-    src_safe = models.CharField(max_length=10)\n-\n-    foreign_1 = models.ForeignKey(Target, related_name='id')\n-    foreign_2 = models.ForeignKey(Target, related_name='src_safe')\n-\n-    m2m_1 = models.ManyToManyField(Target, related_name='id')\n-    m2m_2 = models.ManyToManyField(Target, related_name='src_safe')\n-\n-\n-class Target2(models.Model):\n-    clash3 = models.CharField(max_length=10)\n-    foreign_tgt = models.ForeignKey(Target)\n-    clashforeign_set = models.ForeignKey(Target)\n-\n-    m2m_tgt = models.ManyToManyField(Target)\n-    clashm2m_set = models.ManyToManyField(Target)\n-\n-\n-class Clash3(models.Model):\n-    src_safe = models.CharField(max_length=10)\n-\n-    foreign_1 = models.ForeignKey(Target2, related_name='foreign_tgt')\n-    foreign_2 = models.ForeignKey(Target2, related_name='m2m_tgt')\n-\n-    m2m_1 = models.ManyToManyField(Target2, related_name='foreign_tgt')\n-    m2m_2 = models.ManyToManyField(Target2, related_name='m2m_tgt')\n-\n-\n-class ClashForeign(models.Model):\n-    foreign = models.ForeignKey(Target2)\n-\n-\n-class ClashM2M(models.Model):\n-    m2m = models.ManyToManyField(Target2)\n-\n-\n-class SelfClashForeign(models.Model):\n-    src_safe = models.CharField(max_length=10)\n-    selfclashforeign = models.CharField(max_length=10)\n-\n-    selfclashforeign_set = models.ForeignKey(\"SelfClashForeign\")\n-    foreign_1 = models.ForeignKey(\"SelfClashForeign\", related_name='id')\n-    foreign_2 = models.ForeignKey(\"SelfClashForeign\", related_name='src_safe')\n-\n-\n-class ValidM2M(models.Model):\n-    src_safe = models.CharField(max_length=10)\n-    validm2m = models.CharField(max_length=10)\n-\n-    # M2M fields are symmetrical by default. Symmetrical M2M fields\n-    # on self don't require a related accessor, so many potential\n-    # clashes are avoided.\n-    validm2m_set = models.ManyToManyField(\"self\")\n-\n-    m2m_1 = models.ManyToManyField(\"self\", related_name='id')\n-    m2m_2 = models.ManyToManyField(\"self\", related_name='src_safe')\n-\n-    m2m_3 = models.ManyToManyField('self')\n-    m2m_4 = models.ManyToManyField('self')\n-\n-\n-class SelfClashM2M(models.Model):\n-    src_safe = models.CharField(max_length=10)\n-    selfclashm2m = models.CharField(max_length=10)\n-\n-    # Non-symmetrical M2M fields _do_ have related accessors, so\n-    # there is potential for clashes.\n-    selfclashm2m_set = models.ManyToManyField(\"self\", symmetrical=False)\n-\n-    m2m_1 = models.ManyToManyField(\"self\", related_name='id', symmetrical=False)\n-    m2m_2 = models.ManyToManyField(\"self\", related_name='src_safe', symmetrical=False)\n-\n-    m2m_3 = models.ManyToManyField('self', symmetrical=False)\n-    m2m_4 = models.ManyToManyField('self', symmetrical=False)\n-\n-\n-class Model(models.Model):\n-    \"But it's valid to call a model Model.\"\n-    year = models.PositiveIntegerField()  # 1960\n-    make = models.CharField(max_length=10)  # Aston Martin\n-    name = models.CharField(max_length=10)  # DB 4 GT\n-\n-\n-class Car(models.Model):\n-    colour = models.CharField(max_length=5)\n-    model = models.ForeignKey(Model)\n-\n-\n-class MissingRelations(models.Model):\n-    rel1 = models.ForeignKey(\"Rel1\")\n-    rel2 = models.ManyToManyField(\"Rel2\")\n-\n-\n-class MissingManualM2MModel(models.Model):\n-    name = models.CharField(max_length=5)\n-    missing_m2m = models.ManyToManyField(Model, through=\"MissingM2MModel\")\n-\n-\n-class Person(models.Model):\n-    name = models.CharField(max_length=5)\n-\n-\n-class Group(models.Model):\n-    name = models.CharField(max_length=5)\n-    primary = models.ManyToManyField(Person, through=\"Membership\", related_name=\"primary\")\n-    secondary = models.ManyToManyField(Person, through=\"Membership\", related_name=\"secondary\")\n-    tertiary = models.ManyToManyField(Person, through=\"RelationshipDoubleFK\", related_name=\"tertiary\")\n-\n-\n-class GroupTwo(models.Model):\n-    name = models.CharField(max_length=5)\n-    primary = models.ManyToManyField(Person, through=\"Membership\")\n-    secondary = models.ManyToManyField(Group, through=\"MembershipMissingFK\")\n-\n-\n-class Membership(models.Model):\n-    person = models.ForeignKey(Person)\n-    group = models.ForeignKey(Group)\n-    not_default_or_null = models.CharField(max_length=5)\n-\n-\n-class MembershipMissingFK(models.Model):\n-    person = models.ForeignKey(Person)\n-\n-\n-class PersonSelfRefM2M(models.Model):\n-    name = models.CharField(max_length=5)\n-    friends = models.ManyToManyField('self', through=\"Relationship\")\n-    too_many_friends = models.ManyToManyField('self', through=\"RelationshipTripleFK\")\n-\n-\n-class PersonSelfRefM2MExplicit(models.Model):\n-    name = models.CharField(max_length=5)\n-    friends = models.ManyToManyField('self', through=\"ExplicitRelationship\", symmetrical=True)\n-\n-\n-class Relationship(models.Model):\n-    first = models.ForeignKey(PersonSelfRefM2M, related_name=\"rel_from_set\")\n-    second = models.ForeignKey(PersonSelfRefM2M, related_name=\"rel_to_set\")\n-    date_added = models.DateTimeField()\n-\n-\n-class ExplicitRelationship(models.Model):\n-    first = models.ForeignKey(PersonSelfRefM2MExplicit, related_name=\"rel_from_set\")\n-    second = models.ForeignKey(PersonSelfRefM2MExplicit, related_name=\"rel_to_set\")\n-    date_added = models.DateTimeField()\n-\n-\n-class RelationshipTripleFK(models.Model):\n-    first = models.ForeignKey(PersonSelfRefM2M, related_name=\"rel_from_set_2\")\n-    second = models.ForeignKey(PersonSelfRefM2M, related_name=\"rel_to_set_2\")\n-    third = models.ForeignKey(PersonSelfRefM2M, related_name=\"too_many_by_far\")\n-    date_added = models.DateTimeField()\n-\n-\n-class RelationshipDoubleFK(models.Model):\n-    first = models.ForeignKey(Person, related_name=\"first_related_name\")\n-    second = models.ForeignKey(Person, related_name=\"second_related_name\")\n-    third = models.ForeignKey(Group, related_name=\"rel_to_set\")\n-    date_added = models.DateTimeField()\n-\n-\n-class AbstractModel(models.Model):\n-    name = models.CharField(max_length=10)\n-\n-    class Meta:\n-        abstract = True\n-\n-\n-class AbstractRelationModel(models.Model):\n-    fk1 = models.ForeignKey('AbstractModel')\n-    fk2 = models.ManyToManyField('AbstractModel')\n-\n-\n-class UniqueM2M(models.Model):\n-    \"\"\" Model to test for unique ManyToManyFields, which are invalid. \"\"\"\n-    unique_people = models.ManyToManyField(Person, unique=True)\n-\n-\n-class NonUniqueFKTarget1(models.Model):\n-    \"\"\" Model to test for non-unique FK target in yet-to-be-defined model: expect an error \"\"\"\n-    tgt = models.ForeignKey('FKTarget', to_field='bad')\n-\n-\n-class UniqueFKTarget1(models.Model):\n-    \"\"\" Model to test for unique FK target in yet-to-be-defined model: expect no error \"\"\"\n-    tgt = models.ForeignKey('FKTarget', to_field='good')\n-\n-\n-class FKTarget(models.Model):\n-    bad = models.IntegerField()\n-    good = models.IntegerField(unique=True)\n-\n-\n-class NonUniqueFKTarget2(models.Model):\n-    \"\"\" Model to test for non-unique FK target in previously seen model: expect an error \"\"\"\n-    tgt = models.ForeignKey(FKTarget, to_field='bad')\n-\n-\n-class UniqueFKTarget2(models.Model):\n-    \"\"\" Model to test for unique FK target in previously seen model: expect no error \"\"\"\n-    tgt = models.ForeignKey(FKTarget, to_field='good')\n-\n-\n-class NonExistingOrderingWithSingleUnderscore(models.Model):\n-    class Meta:\n-        ordering = (\"does_not_exist\",)\n-\n-\n-class InvalidSetNull(models.Model):\n-    fk = models.ForeignKey('self', on_delete=models.SET_NULL)\n-\n-\n-class InvalidSetDefault(models.Model):\n-    fk = models.ForeignKey('self', on_delete=models.SET_DEFAULT)\n-\n-\n-class UnicodeForeignKeys(models.Model):\n-    \"\"\"Foreign keys which can translate to ascii should be OK, but fail if\n-    they're not.\"\"\"\n-    good = models.ForeignKey('FKTarget')\n-    also_good = models.ManyToManyField('FKTarget', related_name='unicode2')\n-\n-    # In Python 3 this should become legal, but currently causes unicode errors\n-    # when adding the errors in core/management/validation.py\n-    #bad = models.ForeignKey('')\n-\n-\n-class PrimaryKeyNull(models.Model):\n-    my_pk_field = models.IntegerField(primary_key=True, null=True)\n-\n-\n-class OrderByPKModel(models.Model):\n-    \"\"\"\n-    Model to test that ordering by pk passes validation.\n-    Refs #8291\n-    \"\"\"\n-    name = models.CharField(max_length=100, blank=True)\n-\n-    class Meta:\n-        ordering = ('pk',)\n-\n-\n-class SwappableModel(models.Model):\n-    \"\"\"A model that can be, but isn't swapped out.\n-\n-    References to this model *shoudln't* raise any validation error.\n-    \"\"\"\n-    name = models.CharField(max_length=100)\n-\n-    class Meta:\n-        swappable = 'TEST_SWAPPABLE_MODEL'\n-\n-\n-class SwappedModel(models.Model):\n-    \"\"\"A model that is swapped out.\n-\n-    References to this model *should* raise a validation error.\n-    Requires TEST_SWAPPED_MODEL to be defined in the test environment;\n-    this is guaranteed by the test runner using @override_settings.\n-\n-    The foreign keys and m2m relations on this model *shouldn't*\n-    install related accessors, so there shouldn't be clashes with\n-    the equivalent names on the replacement.\n-    \"\"\"\n-    name = models.CharField(max_length=100)\n-\n-    foreign = models.ForeignKey(Target, related_name='swappable_fk_set')\n-    m2m = models.ManyToManyField(Target, related_name='swappable_m2m_set')\n-\n-    class Meta:\n-        swappable = 'TEST_SWAPPED_MODEL'\n-\n-\n-class ReplacementModel(models.Model):\n-    \"\"\"A replacement model for swapping purposes.\"\"\"\n-    name = models.CharField(max_length=100)\n-\n-    foreign = models.ForeignKey(Target, related_name='swappable_fk_set')\n-    m2m = models.ManyToManyField(Target, related_name='swappable_m2m_set')\n-\n-\n-class BadSwappableValue(models.Model):\n-    \"\"\"A model that can be swapped out; during testing, the swappable\n-    value is not of the format app.model\n-    \"\"\"\n-    name = models.CharField(max_length=100)\n-\n-    class Meta:\n-        swappable = 'TEST_SWAPPED_MODEL_BAD_VALUE'\n-\n-\n-class BadSwappableModel(models.Model):\n-    \"\"\"A model that can be swapped out; during testing, the swappable\n-    value references an unknown model.\n-    \"\"\"\n-    name = models.CharField(max_length=100)\n-\n-    class Meta:\n-        swappable = 'TEST_SWAPPED_MODEL_BAD_MODEL'\n-\n-\n-class HardReferenceModel(models.Model):\n-    fk_1 = models.ForeignKey(SwappableModel, related_name='fk_hardref1')\n-    fk_2 = models.ForeignKey('invalid_models.SwappableModel', related_name='fk_hardref2')\n-    fk_3 = models.ForeignKey(SwappedModel, related_name='fk_hardref3')\n-    fk_4 = models.ForeignKey('invalid_models.SwappedModel', related_name='fk_hardref4')\n-    m2m_1 = models.ManyToManyField(SwappableModel, related_name='m2m_hardref1')\n-    m2m_2 = models.ManyToManyField('invalid_models.SwappableModel', related_name='m2m_hardref2')\n-    m2m_3 = models.ManyToManyField(SwappedModel, related_name='m2m_hardref3')\n-    m2m_4 = models.ManyToManyField('invalid_models.SwappedModel', related_name='m2m_hardref4')\n-\n-\n-class BadIndexTogether1(models.Model):\n-    class Meta:\n-        index_together = [\n-            [\"field_that_does_not_exist\"],\n-        ]\n-\n-\n-class DuplicateColumnNameModel1(models.Model):\n-    \"\"\"\n-    A field (bar) attempts to use a column name which is already auto-assigned\n-    earlier in the class. This should raise a validation error.\n-    \"\"\"\n-    foo = models.IntegerField()\n-    bar = models.IntegerField(db_column='foo')\n-\n-    class Meta:\n-        db_table = 'foobar'\n-\n-\n-class DuplicateColumnNameModel2(models.Model):\n-    \"\"\"\n-    A field (foo) attempts to use a column name which is already auto-assigned\n-    later in the class. This should raise a validation error.\n-    \"\"\"\n-    foo = models.IntegerField(db_column='bar')\n-    bar = models.IntegerField()\n-\n-    class Meta:\n-        db_table = 'foobar'\n-\n-\n-class DuplicateColumnNameModel3(models.Model):\n-    \"\"\"Two fields attempt to use each others' names.\n-\n-    This is not a desirable scenario but valid nonetheless.\n-\n-    It should not raise a validation error.\n-    \"\"\"\n-    foo = models.IntegerField(db_column='bar')\n-    bar = models.IntegerField(db_column='foo')\n-\n-    class Meta:\n-        db_table = 'foobar3'\n-\n-\n-class DuplicateColumnNameModel4(models.Model):\n-    \"\"\"Two fields attempt to use the same db_column value.\n-\n-    This should raise a validation error.\n-    \"\"\"\n-    foo = models.IntegerField(db_column='baz')\n-    bar = models.IntegerField(db_column='baz')\n-\n-    class Meta:\n-        db_table = 'foobar'\n-\n-\n-model_errors = \"\"\"invalid_models.fielderrors: \"charfield\": CharFields require a \"max_length\" attribute that is a positive integer.\n-invalid_models.fielderrors: \"charfield2\": CharFields require a \"max_length\" attribute that is a positive integer.\n-invalid_models.fielderrors: \"charfield3\": CharFields require a \"max_length\" attribute that is a positive integer.\n-invalid_models.fielderrors: \"decimalfield\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.\n-invalid_models.fielderrors: \"decimalfield\": DecimalFields require a \"max_digits\" attribute that is a positive integer.\n-invalid_models.fielderrors: \"decimalfield2\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.\n-invalid_models.fielderrors: \"decimalfield2\": DecimalFields require a \"max_digits\" attribute that is a positive integer.\n-invalid_models.fielderrors: \"decimalfield3\": DecimalFields require a \"decimal_places\" attribute that is a non-negative integer.\n-invalid_models.fielderrors: \"decimalfield3\": DecimalFields require a \"max_digits\" attribute that is a positive integer.\n-invalid_models.fielderrors: \"decimalfield4\": DecimalFields require a \"max_digits\" attribute value that is greater than or equal to the value of the \"decimal_places\" attribute.\n-invalid_models.fielderrors: \"filefield\": FileFields require an \"upload_to\" attribute.\n-invalid_models.fielderrors: \"choices\": \"choices\" should be iterable (e.g., a tuple or list).\n-invalid_models.fielderrors: \"choices2\": \"choices\" should be a sequence of two-item iterables (e.g. list of 2 item tuples).\n-invalid_models.fielderrors: \"choices2\": \"choices\" should be a sequence of two-item iterables (e.g. list of 2 item tuples).\n-invalid_models.fielderrors: \"index\": \"db_index\" should be either None, True or False.\n-invalid_models.fielderrors: \"field_\": Field names cannot end with underscores, because this would lead to ambiguous queryset filters.\n-invalid_models.fielderrors: \"nullbool\": BooleanFields do not accept null values. Use a NullBooleanField instead.\n-invalid_models.fielderrors: \"generic_ip_notnull_blank\": GenericIPAddressField can not accept blank values if null values are not allowed, as blank values are stored as null.\n-invalid_models.clash1: Accessor for field 'foreign' clashes with field 'Target.clash1_set'. Add a related_name argument to the definition for 'foreign'.\n-invalid_models.clash1: Accessor for field 'foreign' clashes with accessor for field 'Clash1.m2m'. Add a related_name argument to the definition for 'foreign'.\n-invalid_models.clash1: Reverse query name for field 'foreign' clashes with field 'Target.clash1'. Add a related_name argument to the definition for 'foreign'.\n-invalid_models.clash1: Accessor for m2m field 'm2m' clashes with field 'Target.clash1_set'. Add a related_name argument to the definition for 'm2m'.\n-invalid_models.clash1: Accessor for m2m field 'm2m' clashes with accessor for field 'Clash1.foreign'. Add a related_name argument to the definition for 'm2m'.\n-invalid_models.clash1: Reverse query name for m2m field 'm2m' clashes with field 'Target.clash1'. Add a related_name argument to the definition for 'm2m'.\n-invalid_models.clash2: Accessor for field 'foreign_1' clashes with field 'Target.id'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.clash2: Accessor for field 'foreign_1' clashes with accessor for field 'Clash2.m2m_1'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.clash2: Reverse query name for field 'foreign_1' clashes with field 'Target.id'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.clash2: Reverse query name for field 'foreign_1' clashes with accessor for field 'Clash2.m2m_1'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.clash2: Accessor for field 'foreign_2' clashes with accessor for field 'Clash2.m2m_2'. Add a related_name argument to the definition for 'foreign_2'.\n-invalid_models.clash2: Reverse query name for field 'foreign_2' clashes with accessor for field 'Clash2.m2m_2'. Add a related_name argument to the definition for 'foreign_2'.\n-invalid_models.clash2: Accessor for m2m field 'm2m_1' clashes with field 'Target.id'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.clash2: Accessor for m2m field 'm2m_1' clashes with accessor for field 'Clash2.foreign_1'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.clash2: Reverse query name for m2m field 'm2m_1' clashes with field 'Target.id'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.clash2: Reverse query name for m2m field 'm2m_1' clashes with accessor for field 'Clash2.foreign_1'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.clash2: Accessor for m2m field 'm2m_2' clashes with accessor for field 'Clash2.foreign_2'. Add a related_name argument to the definition for 'm2m_2'.\n-invalid_models.clash2: Reverse query name for m2m field 'm2m_2' clashes with accessor for field 'Clash2.foreign_2'. Add a related_name argument to the definition for 'm2m_2'.\n-invalid_models.clash3: Accessor for field 'foreign_1' clashes with field 'Target2.foreign_tgt'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.clash3: Accessor for field 'foreign_1' clashes with accessor for field 'Clash3.m2m_1'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.clash3: Reverse query name for field 'foreign_1' clashes with field 'Target2.foreign_tgt'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.clash3: Reverse query name for field 'foreign_1' clashes with accessor for field 'Clash3.m2m_1'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.clash3: Accessor for field 'foreign_2' clashes with m2m field 'Target2.m2m_tgt'. Add a related_name argument to the definition for 'foreign_2'.\n-invalid_models.clash3: Accessor for field 'foreign_2' clashes with accessor for field 'Clash3.m2m_2'. Add a related_name argument to the definition for 'foreign_2'.\n-invalid_models.clash3: Reverse query name for field 'foreign_2' clashes with m2m field 'Target2.m2m_tgt'. Add a related_name argument to the definition for 'foreign_2'.\n-invalid_models.clash3: Reverse query name for field 'foreign_2' clashes with accessor for field 'Clash3.m2m_2'. Add a related_name argument to the definition for 'foreign_2'.\n-invalid_models.clash3: Accessor for m2m field 'm2m_1' clashes with field 'Target2.foreign_tgt'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.clash3: Accessor for m2m field 'm2m_1' clashes with accessor for field 'Clash3.foreign_1'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.clash3: Reverse query name for m2m field 'm2m_1' clashes with field 'Target2.foreign_tgt'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.clash3: Reverse query name for m2m field 'm2m_1' clashes with accessor for field 'Clash3.foreign_1'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.clash3: Accessor for m2m field 'm2m_2' clashes with m2m field 'Target2.m2m_tgt'. Add a related_name argument to the definition for 'm2m_2'.\n-invalid_models.clash3: Accessor for m2m field 'm2m_2' clashes with accessor for field 'Clash3.foreign_2'. Add a related_name argument to the definition for 'm2m_2'.\n-invalid_models.clash3: Reverse query name for m2m field 'm2m_2' clashes with m2m field 'Target2.m2m_tgt'. Add a related_name argument to the definition for 'm2m_2'.\n-invalid_models.clash3: Reverse query name for m2m field 'm2m_2' clashes with accessor for field 'Clash3.foreign_2'. Add a related_name argument to the definition for 'm2m_2'.\n-invalid_models.clashforeign: Accessor for field 'foreign' clashes with field 'Target2.clashforeign_set'. Add a related_name argument to the definition for 'foreign'.\n-invalid_models.clashm2m: Accessor for m2m field 'm2m' clashes with m2m field 'Target2.clashm2m_set'. Add a related_name argument to the definition for 'm2m'.\n-invalid_models.target2: Accessor for field 'foreign_tgt' clashes with accessor for field 'Target2.m2m_tgt'. Add a related_name argument to the definition for 'foreign_tgt'.\n-invalid_models.target2: Accessor for field 'foreign_tgt' clashes with accessor for field 'Target2.clashm2m_set'. Add a related_name argument to the definition for 'foreign_tgt'.\n-invalid_models.target2: Accessor for field 'foreign_tgt' clashes with accessor for field 'Target2.clashforeign_set'. Add a related_name argument to the definition for 'foreign_tgt'.\n-invalid_models.target2: Accessor for field 'clashforeign_set' clashes with accessor for field 'Target2.m2m_tgt'. Add a related_name argument to the definition for 'clashforeign_set'.\n-invalid_models.target2: Accessor for field 'clashforeign_set' clashes with accessor for field 'Target2.clashm2m_set'. Add a related_name argument to the definition for 'clashforeign_set'.\n-invalid_models.target2: Accessor for field 'clashforeign_set' clashes with accessor for field 'Target2.foreign_tgt'. Add a related_name argument to the definition for 'clashforeign_set'.\n-invalid_models.target2: Accessor for m2m field 'm2m_tgt' clashes with accessor for m2m field 'Target2.clashm2m_set'. Add a related_name argument to the definition for 'm2m_tgt'.\n-invalid_models.target2: Accessor for m2m field 'm2m_tgt' clashes with accessor for field 'Target2.foreign_tgt'. Add a related_name argument to the definition for 'm2m_tgt'.\n-invalid_models.target2: Accessor for m2m field 'm2m_tgt' clashes with accessor for field 'Target2.clashforeign_set'. Add a related_name argument to the definition for 'm2m_tgt'.\n-invalid_models.target2: Accessor for m2m field 'clashm2m_set' clashes with accessor for m2m field 'Target2.m2m_tgt'. Add a related_name argument to the definition for 'clashm2m_set'.\n-invalid_models.target2: Accessor for m2m field 'clashm2m_set' clashes with accessor for field 'Target2.foreign_tgt'. Add a related_name argument to the definition for 'clashm2m_set'.\n-invalid_models.target2: Accessor for m2m field 'clashm2m_set' clashes with accessor for field 'Target2.clashforeign_set'. Add a related_name argument to the definition for 'clashm2m_set'.\n-invalid_models.selfclashforeign: Accessor for field 'selfclashforeign_set' clashes with field 'SelfClashForeign.selfclashforeign_set'. Add a related_name argument to the definition for 'selfclashforeign_set'.\n-invalid_models.selfclashforeign: Reverse query name for field 'selfclashforeign_set' clashes with field 'SelfClashForeign.selfclashforeign'. Add a related_name argument to the definition for 'selfclashforeign_set'.\n-invalid_models.selfclashforeign: Accessor for field 'foreign_1' clashes with field 'SelfClashForeign.id'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.selfclashforeign: Reverse query name for field 'foreign_1' clashes with field 'SelfClashForeign.id'. Add a related_name argument to the definition for 'foreign_1'.\n-invalid_models.selfclashforeign: Accessor for field 'foreign_2' clashes with field 'SelfClashForeign.src_safe'. Add a related_name argument to the definition for 'foreign_2'.\n-invalid_models.selfclashforeign: Reverse query name for field 'foreign_2' clashes with field 'SelfClashForeign.src_safe'. Add a related_name argument to the definition for 'foreign_2'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'selfclashm2m_set' clashes with m2m field 'SelfClashM2M.selfclashm2m_set'. Add a related_name argument to the definition for 'selfclashm2m_set'.\n-invalid_models.selfclashm2m: Reverse query name for m2m field 'selfclashm2m_set' clashes with field 'SelfClashM2M.selfclashm2m'. Add a related_name argument to the definition for 'selfclashm2m_set'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'selfclashm2m_set' clashes with accessor for m2m field 'SelfClashM2M.m2m_3'. Add a related_name argument to the definition for 'selfclashm2m_set'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'selfclashm2m_set' clashes with accessor for m2m field 'SelfClashM2M.m2m_4'. Add a related_name argument to the definition for 'selfclashm2m_set'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'm2m_1' clashes with field 'SelfClashM2M.id'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'm2m_2' clashes with field 'SelfClashM2M.src_safe'. Add a related_name argument to the definition for 'm2m_2'.\n-invalid_models.selfclashm2m: Reverse query name for m2m field 'm2m_1' clashes with field 'SelfClashM2M.id'. Add a related_name argument to the definition for 'm2m_1'.\n-invalid_models.selfclashm2m: Reverse query name for m2m field 'm2m_2' clashes with field 'SelfClashM2M.src_safe'. Add a related_name argument to the definition for 'm2m_2'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'm2m_3' clashes with m2m field 'SelfClashM2M.selfclashm2m_set'. Add a related_name argument to the definition for 'm2m_3'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'm2m_3' clashes with accessor for m2m field 'SelfClashM2M.selfclashm2m_set'. Add a related_name argument to the definition for 'm2m_3'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'm2m_3' clashes with accessor for m2m field 'SelfClashM2M.m2m_4'. Add a related_name argument to the definition for 'm2m_3'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'm2m_4' clashes with m2m field 'SelfClashM2M.selfclashm2m_set'. Add a related_name argument to the definition for 'm2m_4'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'm2m_4' clashes with accessor for m2m field 'SelfClashM2M.selfclashm2m_set'. Add a related_name argument to the definition for 'm2m_4'.\n-invalid_models.selfclashm2m: Accessor for m2m field 'm2m_4' clashes with accessor for m2m field 'SelfClashM2M.m2m_3'. Add a related_name argument to the definition for 'm2m_4'.\n-invalid_models.selfclashm2m: Reverse query name for m2m field 'm2m_3' clashes with field 'SelfClashM2M.selfclashm2m'. Add a related_name argument to the definition for 'm2m_3'.\n-invalid_models.selfclashm2m: Reverse query name for m2m field 'm2m_4' clashes with field 'SelfClashM2M.selfclashm2m'. Add a related_name argument to the definition for 'm2m_4'.\n-invalid_models.missingrelations: 'rel1' has a relation with model Rel1, which has either not been installed or is abstract.\n-invalid_models.missingrelations: 'rel2' has an m2m relation with model Rel2, which has either not been installed or is abstract.\n-invalid_models.grouptwo: 'primary' is a manually-defined m2m relation through model Membership, which does not have foreign keys to Person and GroupTwo\n-invalid_models.grouptwo: 'secondary' is a manually-defined m2m relation through model MembershipMissingFK, which does not have foreign keys to Group and GroupTwo\n-invalid_models.missingmanualm2mmodel: 'missing_m2m' specifies an m2m relation through model MissingM2MModel, which has not been installed\n-invalid_models.group: The model Group has two manually-defined m2m relations through the model Membership, which is not permitted. Please consider using an extra field on your intermediary model instead.\n-invalid_models.group: Intermediary model RelationshipDoubleFK has more than one foreign key to Person, which is ambiguous and is not permitted.\n-invalid_models.personselfrefm2m: Many-to-many fields with intermediate tables cannot be symmetrical.\n-invalid_models.personselfrefm2m: Intermediary model RelationshipTripleFK has more than two foreign keys to PersonSelfRefM2M, which is ambiguous and is not permitted.\n-invalid_models.personselfrefm2mexplicit: Many-to-many fields with intermediate tables cannot be symmetrical.\n-invalid_models.abstractrelationmodel: 'fk1' has a relation with model AbstractModel, which has either not been installed or is abstract.\n-invalid_models.abstractrelationmodel: 'fk2' has an m2m relation with model AbstractModel, which has either not been installed or is abstract.\n-invalid_models.uniquem2m: ManyToManyFields cannot be unique.  Remove the unique argument on 'unique_people'.\n-invalid_models.nonuniquefktarget1: Field 'bad' under model 'FKTarget' must have a unique=True constraint.\n-invalid_models.nonuniquefktarget2: Field 'bad' under model 'FKTarget' must have a unique=True constraint.\n-invalid_models.nonexistingorderingwithsingleunderscore: \"ordering\" refers to \"does_not_exist\", a field that doesn't exist.\n-invalid_models.invalidsetnull: 'fk' specifies on_delete=SET_NULL, but cannot be null.\n-invalid_models.invalidsetdefault: 'fk' specifies on_delete=SET_DEFAULT, but has no default value.\n-invalid_models.hardreferencemodel: 'fk_3' defines a relation with the model 'invalid_models.SwappedModel', which has been swapped out. Update the relation to point at settings.TEST_SWAPPED_MODEL.\n-invalid_models.hardreferencemodel: 'fk_4' defines a relation with the model 'invalid_models.SwappedModel', which has been swapped out. Update the relation to point at settings.TEST_SWAPPED_MODEL.\n-invalid_models.hardreferencemodel: 'm2m_3' defines a relation with the model 'invalid_models.SwappedModel', which has been swapped out. Update the relation to point at settings.TEST_SWAPPED_MODEL.\n-invalid_models.hardreferencemodel: 'm2m_4' defines a relation with the model 'invalid_models.SwappedModel', which has been swapped out. Update the relation to point at settings.TEST_SWAPPED_MODEL.\n-invalid_models.badswappablevalue: TEST_SWAPPED_MODEL_BAD_VALUE is not of the form 'app_label.app_name'.\n-invalid_models.badswappablemodel: Model has been swapped out for 'not_an_app.Target' which has not been installed or is abstract.\n-invalid_models.badindextogether1: \"index_together\" refers to field_that_does_not_exist, a field that doesn't exist.\n-invalid_models.duplicatecolumnnamemodel1: Field 'bar' has column name 'foo' that is already used.\n-invalid_models.duplicatecolumnnamemodel2: Field 'bar' has column name 'bar' that is already used.\n-invalid_models.duplicatecolumnnamemodel4: Field 'bar' has column name 'baz' that is already used.\n-\"\"\"\n-\n-if not connection.features.interprets_empty_strings_as_nulls:\n-    model_errors += \"\"\"invalid_models.primarykeynull: \"my_pk_field\": Primary key fields cannot have null=True.\n-\"\"\"\ndiff --git a/tests/invalid_models/old_invalid_models/models.py b/tests/invalid_models/old_invalid_models/models.py\nnew file mode 100644\nindex 000000000000..c784591b6d17\n--- /dev/null\n+++ b/tests/invalid_models/old_invalid_models/models.py\n@@ -0,0 +1,141 @@\n+# -*- coding:utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from django.conf import settings\n+from django.db import models\n+\n+\n+class Target(models.Model):\n+    tgt_safe = models.CharField(max_length=10)\n+    clash1 = models.CharField(max_length=10)\n+    clash2 = models.CharField(max_length=10)\n+\n+    clash1_set = models.CharField(max_length=10)\n+\n+class ValidM2M(models.Model):\n+    src_safe = models.CharField(max_length=10)\n+    validm2m = models.CharField(max_length=10)\n+\n+    # M2M fields are symmetrical by default. Symmetrical M2M fields\n+    # on self don't require a related accessor, so many potential\n+    # clashes are avoided.\n+    validm2m_set = models.ManyToManyField(\"self\")\n+\n+    m2m_1 = models.ManyToManyField(\"self\", related_name='id')\n+    m2m_2 = models.ManyToManyField(\"self\", related_name='src_safe')\n+\n+    m2m_3 = models.ManyToManyField('self')\n+    m2m_4 = models.ManyToManyField('self')\n+\n+\n+class Model(models.Model):\n+    \"But it's valid to call a model Model.\"\n+    year = models.PositiveIntegerField()  # 1960\n+    make = models.CharField(max_length=10)  # Aston Martin\n+    name = models.CharField(max_length=10)  # DB 4 GT\n+\n+\n+class Person(models.Model):\n+    name = models.CharField(max_length=5)\n+\n+\n+class Group(models.Model):\n+    name = models.CharField(max_length=5)\n+    primary = models.ManyToManyField(Person, through=\"Membership\", related_name=\"primary\")\n+    secondary = models.ManyToManyField(Person, through=\"Membership\", related_name=\"secondary\")\n+\n+\n+class Membership(models.Model):\n+    person = models.ForeignKey(Person)\n+    group = models.ForeignKey(Group)\n+    not_default_or_null = models.CharField(max_length=5)\n+\n+\n+class UniqueFKTarget1(models.Model):\n+    \"\"\" Model to test for unique FK target in yet-to-be-defined model: expect no error \"\"\"\n+    tgt = models.ForeignKey('FKTarget', to_field='good')\n+\n+\n+class FKTarget(models.Model):\n+    good = models.IntegerField(unique=True)\n+\n+\n+class UniqueFKTarget2(models.Model):\n+    \"\"\" Model to test for unique FK target in previously seen model: expect no error \"\"\"\n+    tgt = models.ForeignKey(FKTarget, to_field='good')\n+\n+\n+class UnicodeForeignKeys(models.Model):\n+    \"\"\"Foreign keys which can translate to ascii should be OK, but fail if\n+    they're not.\"\"\"\n+    good = models.ForeignKey('FKTarget')\n+    also_good = models.ManyToManyField('FKTarget', related_name='unicode2')\n+\n+    # In Python 3 this should become legal, but currently causes unicode errors\n+    # when adding the errors in core/management/validation.py\n+    #bad = models.ForeignKey('')\n+\n+\n+class OrderByPKModel(models.Model):\n+    \"\"\"\n+    Model to test that ordering by pk passes validation.\n+    Refs #8291\n+    \"\"\"\n+    name = models.CharField(max_length=100, blank=True)\n+\n+    class Meta:\n+        ordering = ('pk',)\n+\n+\n+class SwappedModel(models.Model):\n+    \"\"\"A model that is swapped out.\n+\n+    The foreign keys and m2m relations on this model *shouldn't*\n+    install related accessors, so there shouldn't be clashes with\n+    the equivalent names on the replacement.\n+    \"\"\"\n+    name = models.CharField(max_length=100)\n+\n+    foreign = models.ForeignKey(Target, related_name='swappable_fk_set')\n+    m2m = models.ManyToManyField(Target, related_name='swappable_m2m_set')\n+\n+    class Meta:\n+        swappable = 'TEST_SWAPPED_MODEL'\n+\n+\n+class ReplacementModel(models.Model):\n+    \"\"\"A replacement model for swapping purposes.\"\"\"\n+    name = models.CharField(max_length=100)\n+\n+    foreign = models.ForeignKey(Target, related_name='swappable_fk_set')\n+    m2m = models.ManyToManyField(Target, related_name='swappable_m2m_set')\n+\n+\n+class SwappingModel(models.Model):\n+    \"\"\" Uses SwappedModel. \"\"\"\n+\n+    foreign_key = models.ForeignKey(settings.TEST_SWAPPED_MODEL,\n+        related_name='swapping_foreign_key')\n+    m2m = models.ManyToManyField(settings.TEST_SWAPPED_MODEL,\n+        related_name='swapping_m2m')\n+\n+\n+model_errors = \"\"\"\n+old_invalid_models.group: The model Group has two manually-defined m2m relations through the model Membership, which is not permitted. Please consider using an extra field on your intermediary model instead.\n+old_invalid_models.duplicatecolumnnamemodel1: Field 'bar' has column name 'foo' that is already used.\n+old_invalid_models.duplicatecolumnnamemodel2: Field 'bar' has column name 'bar' that is already used.\n+old_invalid_models.duplicatecolumnnamemodel4: Field 'bar' has column name 'baz' that is already used.\n+\"\"\"\n+\n+# Space\n+\n+\"\"\"\n+# Error messages predated by a character:\n+# - 'x' -- the test was rewritten\n+# - 'm' -- the test is actually a model test, not a field test; not rewritten\n+\n+m invalid_models.group: The model Group has two manually-defined m2m relations through the model Membership, which is not permitted. Please consider using an extra field on your intermediary model instead.\n+m invalid_models.duplicatecolumnnamemodel1: Field 'bar' has column name 'foo' that is already used.\n+m invalid_models.duplicatecolumnnamemodel2: Field 'bar' has column name 'bar' that is already used.\n+m invalid_models.duplicatecolumnnamemodel4: Field 'bar' has column name 'baz' that is already used.\n+\"\"\"\ndiff --git a/tests/invalid_models/test_backend_specific.py b/tests/invalid_models/test_backend_specific.py\nnew file mode 100644\nindex 000000000000..54f173ea2aed\n--- /dev/null\n+++ b/tests/invalid_models/test_backend_specific.py\n@@ -0,0 +1,68 @@\n+# -*- encoding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from types import MethodType\n+\n+from django.core.checks import Error\n+from django.db import connection, models\n+\n+from . import IsolatedModelsTestCase\n+\n+\n+class BackendSpecificChecksTests(IsolatedModelsTestCase):\n+\n+    def test_check_field(self):\n+        \"\"\" Test if backend specific checks are performed. \"\"\"\n+\n+        error = Error('an error', hint=None)\n+\n+        def mock(self, field, **kwargs):\n+            return [error]\n+\n+        class Model(models.Model):\n+            field = models.IntegerField()\n+\n+        field = Model._meta.get_field('field')\n+\n+        # Mock connection.validation.check_field method.\n+        v = connection.validation\n+        old_check_field = v.check_field\n+        v.check_field = MethodType(mock, v)\n+        try:\n+            errors = field.check()\n+        finally:\n+            # Unmock connection.validation.check_field method.\n+            v.check_field = old_check_field\n+\n+        self.assertEqual(errors, [error])\n+\n+    def test_validate_field(self):\n+        \"\"\" Errors raised by deprecated `validate_field` method should be\n+        collected. \"\"\"\n+\n+        def mock(self, errors, opts, field):\n+            errors.add(opts, \"An error!\")\n+\n+        class Model(models.Model):\n+            field = models.IntegerField()\n+\n+        field = Model._meta.get_field('field')\n+        expected = [\n+            Error(\n+                \"An error!\",\n+                hint=None,\n+                obj=field,\n+            )\n+        ]\n+\n+        # Mock connection.validation.validate_field method.\n+        v = connection.validation\n+        old_validate_field = v.validate_field\n+        v.validate_field = MethodType(mock, v)\n+        try:\n+            errors = field.check()\n+        finally:\n+            # Unmock connection.validation.validate_field method.\n+            v.validate_field = old_validate_field\n+\n+        self.assertEqual(errors, expected)\ndiff --git a/tests/invalid_models/test_models.py b/tests/invalid_models/test_models.py\nnew file mode 100644\nindex 000000000000..48a441e91477\n--- /dev/null\n+++ b/tests/invalid_models/test_models.py\n@@ -0,0 +1,319 @@\n+# -*- encoding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from django.core.checks import Error\n+from django.db import models\n+from django.test.utils import override_settings\n+\n+from . import IsolatedModelsTestCase\n+\n+\n+class IndexTogetherTests(IsolatedModelsTestCase):\n+\n+    def test_non_iterable(self):\n+        class Model(models.Model):\n+            class Meta:\n+                index_together = 'not-a-list'\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                '\"index_together\" must be a list or tuple.',\n+                hint=None,\n+                obj=Model,\n+                id='E006',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_list_containing_non_iterable(self):\n+        class Model(models.Model):\n+            class Meta:\n+                index_together = [\n+                    'non-iterable',\n+                    'second-non-iterable',\n+                ]\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'All \"index_together\" elements must be lists or tuples.',\n+                hint=None,\n+                obj=Model,\n+                id='E007',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_pointing_to_missing_field(self):\n+        class Model(models.Model):\n+            class Meta:\n+                index_together = [\n+                    [\"missing_field\"],\n+                ]\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                '\"index_together\" points to a missing field named \"missing_field\".',\n+                hint='Ensure that you did not misspell the field name.',\n+                obj=Model,\n+                id='E010',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_pointing_to_m2m_field(self):\n+        class Model(models.Model):\n+            m2m = models.ManyToManyField('self')\n+\n+            class Meta:\n+                index_together = [\n+                    [\"m2m\"],\n+                ]\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                '\"index_together\" refers to a m2m \"m2m\" field, but '\n+                    'ManyToManyFields are not supported in \"index_together\".',\n+                hint=None,\n+                obj=Model,\n+                id='E011',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+# unique_together tests are very similar to index_together tests.\n+class UniqueTogetherTests(IsolatedModelsTestCase):\n+\n+    def test_non_iterable(self):\n+        class Model(models.Model):\n+            class Meta:\n+                unique_together = 0\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                '\"unique_together\" must be a list or tuple.',\n+                hint=None,\n+                obj=Model,\n+                id='E008',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_list_containing_non_iterable(self):\n+        class Model(models.Model):\n+            one = models.IntegerField()\n+            two = models.IntegerField()\n+\n+            class Meta:\n+                unique_together = [('a', 'b'), 'not-a-list']\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'All \"unique_together\" elements must be lists or tuples.',\n+                hint=None,\n+                obj=Model,\n+                id='E009',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_valid_model(self):\n+        class Model(models.Model):\n+            one = models.IntegerField()\n+            two = models.IntegerField()\n+\n+            class Meta:\n+                # unique_together can be a simple tuple\n+                unique_together = ('one', 'two')\n+\n+        errors = Model.check()\n+        self.assertEqual(errors, [])\n+\n+    def test_pointing_to_missing_field(self):\n+        class Model(models.Model):\n+            class Meta:\n+                unique_together = [\n+                    [\"missing_field\"],\n+                ]\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                '\"unique_together\" points to a missing field named \"missing_field\".',\n+                hint='Ensure that you did not misspell the field name.',\n+                obj=Model,\n+                id='E010',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_pointing_to_m2m(self):\n+        class Model(models.Model):\n+            m2m = models.ManyToManyField('self')\n+\n+            class Meta:\n+                unique_together = [\n+                    [\"m2m\"],\n+                ]\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                '\"unique_together\" refers to a m2m \"m2m\" field, but '\n+                'ManyToManyFields are not supported in \"unique_together\".',\n+                hint=None,\n+                obj=Model,\n+                id='E011',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class OtherModelTests(IsolatedModelsTestCase):\n+\n+    def test_unique_primary_key(self):\n+        class Model(models.Model):\n+            id = models.IntegerField(primary_key=False)\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'You cannot use \"id\" as a field name, because each model '\n+                    'automatically gets an \"id\" field if none of the fields '\n+                    'have primary_key=True.',\n+                hint='Remove or rename \"id\" field or '\n+                    'add primary_key=True to a field.',\n+                obj=Model,\n+                id='E005',\n+            ),\n+            Error(\n+                'Field \"id\" has column name \"id\" that is already used.',\n+                hint=None,\n+                obj=Model,\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_field_names_ending_with_underscore(self):\n+        class Model(models.Model):\n+            field_ = models.CharField(max_length=10)\n+            m2m_ = models.ManyToManyField('self')\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Field names must not end with underscores.',\n+                hint=None,\n+                obj=Model._meta.get_field('field_'),\n+                id='E001',\n+            ),\n+            Error(\n+                'Field names must not end with underscores.',\n+                hint=None,\n+                obj=Model._meta.get_field('m2m_'),\n+                id='E001',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_ordering_non_iterable(self):\n+        class Model(models.Model):\n+            class Meta:\n+                ordering = \"missing_field\"\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                '\"ordering\" must be a tuple or list '\n+                    '(even if you want to order by only one field).',\n+                hint=None,\n+                obj=Model,\n+                id='E012',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_ordering_pointing_to_missing_field(self):\n+        class Model(models.Model):\n+            class Meta:\n+                ordering = (\"missing_field\",)\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                '\"ordering\" pointing to a missing \"missing_field\" field.',\n+                hint='Ensure that you did not misspell the field name.',\n+                obj=Model,\n+                id='E013',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    @override_settings(TEST_SWAPPED_MODEL_BAD_VALUE='not-a-model')\n+    def test_swappable_missing_app_name(self):\n+        class Model(models.Model):\n+            class Meta:\n+                swappable = 'TEST_SWAPPED_MODEL_BAD_VALUE'\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                '\"TEST_SWAPPED_MODEL_BAD_VALUE\" is not of the form \"app_label.app_name\".',\n+                hint=None,\n+                obj=Model,\n+                id='E002',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    @override_settings(TEST_SWAPPED_MODEL_BAD_MODEL='not_an_app.Target')\n+    def test_swappable_missing_app(self):\n+        class Model(models.Model):\n+            class Meta:\n+                swappable = 'TEST_SWAPPED_MODEL_BAD_MODEL'\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'The model has been swapped out for not_an_app.Target '\n+                    'which has not been installed or is abstract.',\n+                hint='Ensure that you did not misspell the model name and '\n+                    'the app name as well as the model is not abstract. Does '\n+                    'your INSTALLED_APPS setting contain the \"not_an_app\" app?',\n+                obj=Model,\n+                id='E003',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_two_m2m_through_same_relationship(self):\n+        class Person(models.Model):\n+            pass\n+\n+        class Group(models.Model):\n+            primary = models.ManyToManyField(Person,\n+                through=\"Membership\", related_name=\"primary\")\n+            secondary = models.ManyToManyField(Person, through=\"Membership\",\n+                related_name=\"secondary\")\n+\n+        class Membership(models.Model):\n+            person = models.ForeignKey(Person)\n+            group = models.ForeignKey(Group)\n+\n+        errors = Group.check()\n+        expected = [\n+            Error(\n+                'The model has two many-to-many relations through '\n+                    'the intermediary Membership model, which is not permitted.',\n+                hint=None,\n+                obj=Group,\n+                id='E004',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\ndiff --git a/tests/invalid_models/test_ordinary_fields.py b/tests/invalid_models/test_ordinary_fields.py\nnew file mode 100644\nindex 000000000000..09248832bc51\n--- /dev/null\n+++ b/tests/invalid_models/test_ordinary_fields.py\n@@ -0,0 +1,414 @@\n+# -*- encoding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from django.core.checks import Error\n+from django.db import models\n+\n+from . import IsolatedModelsTestCase\n+\n+\n+class AutoFieldTests(IsolatedModelsTestCase):\n+\n+    def test_valid_case(self):\n+        class Model(models.Model):\n+            id = models.AutoField(primary_key=True)\n+\n+        field = Model._meta.get_field('id')\n+        errors = field.check()\n+        expected = []\n+        self.assertEqual(errors, expected)\n+\n+    def test_primary_key(self):\n+        class Model(models.Model):\n+            field = models.AutoField(primary_key=False)\n+\n+            # Prevent Django from autocreating `id` AutoField, which would\n+            # result in an error, because a model must have exactly one\n+            # AutoField.\n+            another = models.IntegerField(primary_key=True)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field must have primary_key=True, because it is an AutoField.',\n+                hint=None,\n+                obj=field,\n+                id='E048',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class BooleanFieldTests(IsolatedModelsTestCase):\n+\n+    def test_nullable_boolean_field(self):\n+        class Model(models.Model):\n+            field = models.BooleanField(null=True)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'BooleanFields do not acceps null values.',\n+                hint='Use a NullBooleanField instead.',\n+                obj=field,\n+                id='E037',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class CharFieldTests(IsolatedModelsTestCase):\n+\n+    def test_valid_field(self):\n+        class Model(models.Model):\n+            field = models.CharField(\n+                max_length=255,\n+                choices=[\n+                    ('1', 'item1'),\n+                    ('2', 'item2'),\n+                ],\n+                db_index=True)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = []\n+        self.assertEqual(errors, expected)\n+\n+    def test_missing_max_length(self):\n+        class Model(models.Model):\n+            field = models.CharField()\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field must have \"max_length\" attribute.',\n+                hint=None,\n+                obj=field,\n+                id='E038',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_negative_max_length(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=-1)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                '\"max_length\" must be a positive integer.',\n+                hint=None,\n+                obj=field,\n+                id='E039',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_bad_max_length_value(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=\"bad\")\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                '\"max_length\" must be a positive integer.',\n+                hint=None,\n+                obj=field,\n+                id='E039',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_non_iterable_choices(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, choices='bad')\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                '\"choices\" must be an iterable (e.g., a list or tuple).',\n+                hint=None,\n+                obj=field,\n+                id='E033',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_choices_containing_non_pairs(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, choices=[(1, 2, 3), (1, 2, 3)])\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'All \"choices\" elements must be a tuple of two elements '\n+                    '(the first one is the actual value to be stored '\n+                    'and the second element is the human-readable name).',\n+                hint=None,\n+                obj=field,\n+                id='E034',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_bad_db_index_value(self):\n+        class Model(models.Model):\n+            field = models.CharField(max_length=10, db_index='bad')\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                '\"db_index\" must be either None, True or False.',\n+                hint=None,\n+                obj=field,\n+                id='E035',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_too_long_char_field_under_mysql(self):\n+        from django.db.backends.mysql.validation import DatabaseValidation\n+\n+        class Model(models.Model):\n+            field = models.CharField(unique=True, max_length=256)\n+\n+        field = Model._meta.get_field('field')\n+        validator = DatabaseValidation(connection=None)\n+        errors = validator.check_field(field)\n+        expected = [\n+            Error(\n+                'Under mysql backend, the field cannot have a \"max_length\" '\n+                    'greated than 255 when it is unique.',\n+                hint=None,\n+                obj=field,\n+                id='E047',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class DecimalFieldTests(IsolatedModelsTestCase):\n+\n+    def test_required_attributes(self):\n+        class Model(models.Model):\n+            field = models.DecimalField()\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field requires a \"decimal_places\" attribute.',\n+                hint=None,\n+                obj=field,\n+                id='E041',\n+            ),\n+            Error(\n+                'The field requires a \"max_digits\" attribute.',\n+                hint=None,\n+                obj=field,\n+                id='E043',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_negative_max_digits_and_decimal_places(self):\n+        class Model(models.Model):\n+            field = models.DecimalField(max_digits=-1, decimal_places=-1)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                '\"decimal_places\" attribute must be a non-negative integer.',\n+                hint=None,\n+                obj=field,\n+                id='E042',\n+            ),\n+            Error(\n+                '\"max_digits\" attribute must be a positive integer.',\n+                hint=None,\n+                obj=field,\n+                id='E044',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_bad_values_of_max_digits_and_decimal_places(self):\n+        class Model(models.Model):\n+            field = models.DecimalField(max_digits=\"bad\", decimal_places=\"bad\")\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                '\"decimal_places\" attribute must be a non-negative integer.',\n+                hint=None,\n+                obj=field,\n+                id='E042',\n+            ),\n+            Error(\n+                '\"max_digits\" attribute must be a positive integer.',\n+                hint=None,\n+                obj=field,\n+                id='E044',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_decimal_places_greater_than_max_digits(self):\n+        class Model(models.Model):\n+            field = models.DecimalField(max_digits=9, decimal_places=10)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                '\"max_digits\" must be greater or equal to \"decimal_places\".',\n+                hint=None,\n+                obj=field,\n+                id='E040',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_valid_field(self):\n+        class Model(models.Model):\n+            field = models.DecimalField(max_digits=10, decimal_places=10)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = []\n+        self.assertEqual(errors, expected)\n+\n+\n+class FileFieldTests(IsolatedModelsTestCase):\n+\n+    def test_valid_case(self):\n+        class Model(models.Model):\n+            field = models.FileField(upload_to='somewhere')\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = []\n+        self.assertEqual(errors, expected)\n+\n+    def test_missing_upload_to(self):\n+        class Model(models.Model):\n+            field = models.FileField()\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field requires an \"upload_to\" attribute.',\n+                hint=None,\n+                obj=field,\n+                id='E031',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_unique(self):\n+        class Model(models.Model):\n+            field = models.FileField(unique=False, upload_to='somewhere')\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                '\"unique\" is not a valid argument for FileField.',\n+                hint=None,\n+                obj=field,\n+                id='E049',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_primary_key(self):\n+        class Model(models.Model):\n+            field = models.FileField(primary_key=False, upload_to='somewhere')\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                '\"primary_key\" is not a valid argument for FileField.',\n+                hint=None,\n+                obj=field,\n+                id='E050',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class FilePathFieldTests(IsolatedModelsTestCase):\n+\n+    def test_forbidden_files_and_folders(self):\n+        class Model(models.Model):\n+            field = models.FilePathField(allow_files=False, allow_folders=False)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field must have either \"allow_files\" or \"allow_folders\" set to True.',\n+                hint=None,\n+                obj=field,\n+                id='E045',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class GenericIPAddressFieldTests(IsolatedModelsTestCase):\n+\n+    def test_non_nullable_blank(self):\n+        class Model(models.Model):\n+            field = models.GenericIPAddressField(null=False, blank=True)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field cannot accept blank values if null values '\n+                    'are not allowed, as blank values are stored as null.',\n+                hint=None,\n+                obj=field,\n+                id='E046',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+class ImageFieldTests(IsolatedModelsTestCase):\n+\n+    def test_pillow_installed(self):\n+        try:\n+            import django.utils.image\n+        except ImproperlyConfigured:\n+            pillow_installed = False\n+        else:\n+            pillow_installed = True\n+\n+        class Model(models.Model):\n+            field = models.ImageField(upload_to='somewhere')\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [] if pillow_installed else [\n+            Error(\n+                'To use ImageFields, Pillow must be installed.',\n+                hint='Get Pillow at https://pypi.python.org/pypi/Pillow '\n+                    'or run command \"pip install pillow\".',\n+                obj=field,\n+                id='E032',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\ndiff --git a/tests/invalid_models/test_relative_fields.py b/tests/invalid_models/test_relative_fields.py\nnew file mode 100644\nindex 000000000000..7f3ea6cf0e60\n--- /dev/null\n+++ b/tests/invalid_models/test_relative_fields.py\n@@ -0,0 +1,1036 @@\n+# -*- encoding: utf-8 -*-\n+from __future__ import unicode_literals\n+\n+from django.core.checks import Error\n+from django.db import models\n+from django.test.utils import override_settings\n+from django.test.testcases import skipIfDBFeature\n+\n+from . import IsolatedModelsTestCase\n+\n+\n+class RelativeFieldTests(IsolatedModelsTestCase):\n+\n+    def test_valid_foreign_key_without_accessor(self):\n+        class Target(models.Model):\n+            # There would be a clash if Model.field installed an accessor.\n+            model = models.IntegerField()\n+\n+        class Model(models.Model):\n+            field = models.ForeignKey(Target, related_name='+')\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        self.assertEqual(errors, [])\n+\n+    def test_foreign_key_to_missing_model(self):\n+        # Model names are resolved when a model is being created, so we cannot\n+        # test relative fields in isolation and we need to attach them to a\n+        # model.\n+        class Model(models.Model):\n+            foreign_key = models.ForeignKey('Rel1')\n+\n+        field = Model._meta.get_field('foreign_key')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field has a relation with model Rel1, '\n+                    'which has either not been installed or is abstract.',\n+                hint='Ensure that you did not misspell the model name and '\n+                    'the model is not abstract. Does your INSTALLED_APPS '\n+                    'setting contain the app where Rel1 is defined?',\n+                obj=field,\n+                id='E030',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_many_to_many_to_missing_model(self):\n+        class Model(models.Model):\n+            m2m = models.ManyToManyField(\"Rel2\")\n+\n+        field = Model._meta.get_field('m2m')\n+        errors = field.check(from_model=Model)\n+        expected = [\n+            Error(\n+                'The field has a relation with model Rel2, '\n+                    'which has either not been installed or is abstract.',\n+                hint='Ensure that you did not misspell the model name and '\n+                    'the model is not abstract. Does your INSTALLED_APPS '\n+                    'setting contain the app where Rel2 is defined?',\n+                obj=field,\n+                id='E030',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_ambiguous_relationship_model(self):\n+\n+        class Person(models.Model):\n+            pass\n+\n+        class Group(models.Model):\n+            field = models.ManyToManyField('Person',\n+                through=\"AmbiguousRelationship\", related_name='tertiary')\n+\n+        class AmbiguousRelationship(models.Model):\n+            # Too much foreign keys to Person.\n+            first_person = models.ForeignKey(Person, related_name=\"first\")\n+            second_person = models.ForeignKey(Person, related_name=\"second\")\n+            second_model = models.ForeignKey(Group)\n+\n+        field = Group._meta.get_field('field')\n+        errors = field.check(from_model=Group)\n+        expected = [\n+            Error(\n+                'The model is used as an intermediary model by '\n+                    'invalid_models.Group.field, but it has more than one '\n+                    'foreign key to Person, '\n+                    'which is ambiguous and is not permitted.',\n+                hint='If you want to create a recursive relationship, use '\n+                    'ForeignKey(\"self\", symmetrical=False, '\n+                    'through=\"AmbiguousRelationship\").',\n+                obj=field,\n+                id='E027',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_relationship_model_with_foreign_key_to_wrong_model(self):\n+        class WrongModel(models.Model):\n+            pass\n+\n+        class Person(models.Model):\n+            pass\n+\n+        class Group(models.Model):\n+            members = models.ManyToManyField('Person',\n+                through=\"InvalidRelationship\")\n+\n+        class InvalidRelationship(models.Model):\n+            person = models.ForeignKey(Person)\n+            wrong_foreign_key = models.ForeignKey(WrongModel)\n+            # The last foreign key should point to Group model.\n+\n+        field = Group._meta.get_field('members')\n+        errors = field.check(from_model=Group)\n+        expected = [\n+            Error(\n+                'The model is used as an intermediary model by '\n+                    'invalid_models.Group.members, but it misses '\n+                    'a foreign key to Group or Person.',\n+                hint=None,\n+                obj=InvalidRelationship,\n+                id='E028',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_relationship_model_missing_foreign_key(self):\n+        class Person(models.Model):\n+            pass\n+\n+        class Group(models.Model):\n+            members = models.ManyToManyField('Person',\n+                through=\"InvalidRelationship\")\n+\n+        class InvalidRelationship(models.Model):\n+            group = models.ForeignKey(Group)\n+            # No foreign key to Person\n+\n+        field = Group._meta.get_field('members')\n+        errors = field.check(from_model=Group)\n+        expected = [\n+            Error(\n+                'The model is used as an intermediary model by '\n+                    'invalid_models.Group.members, but it misses '\n+                    'a foreign key to Group or Person.',\n+                hint=None,\n+                obj=InvalidRelationship,\n+                id='E028',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_missing_relationship_model(self):\n+        class Person(models.Model):\n+            pass\n+\n+        class Group(models.Model):\n+            members = models.ManyToManyField('Person',\n+                through=\"MissingM2MModel\")\n+\n+        field = Group._meta.get_field('members')\n+        errors = field.check(from_model=Group)\n+        expected = [\n+            Error(\n+                'The field specifies a many-to-many relation through model '\n+                    'MissingM2MModel, which has not been installed.',\n+                hint='Ensure that you did not misspell the model name and '\n+                    'the model is not abstract. Does your INSTALLED_APPS '\n+                    'setting contain the app where MissingM2MModel is defined?',\n+                obj=field,\n+                id='E023',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_symmetrical_self_referential_field(self):\n+        class Person(models.Model):\n+            # Implicit symmetrical=False.\n+            friends = models.ManyToManyField('self', through=\"Relationship\")\n+\n+        class Relationship(models.Model):\n+            first = models.ForeignKey(Person, related_name=\"rel_from_set\")\n+            second = models.ForeignKey(Person, related_name=\"rel_to_set\")\n+\n+        field = Person._meta.get_field('friends')\n+        errors = field.check(from_model=Person)\n+        expected = [\n+            Error(\n+                'Many-to-many fields with intermediate tables must not be symmetrical.',\n+                hint=None,\n+                obj=field,\n+                id='E024',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_too_many_foreign_keys_in_self_referential_model(self):\n+        class Person(models.Model):\n+            friends = models.ManyToManyField('self',\n+                through=\"InvalidRelationship\", symmetrical=False)\n+\n+        class InvalidRelationship(models.Model):\n+            first = models.ForeignKey(Person, related_name=\"rel_from_set_2\")\n+            second = models.ForeignKey(Person, related_name=\"rel_to_set_2\")\n+            third = models.ForeignKey(Person, related_name=\"too_many_by_far\")\n+\n+        field = Person._meta.get_field('friends')\n+        errors = field.check(from_model=Person)\n+        expected = [\n+            Error(\n+                'The model is used as an intermediary model by '\n+                    'invalid_models.Person.friends, but it has more than two '\n+                    'foreign keys to Person, which is ambiguous and '\n+                    'is not permitted.',\n+                hint=None,\n+                obj=InvalidRelationship,\n+                id='E025',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_symmetric_self_reference_with_intermediate_table(self):\n+        class Person(models.Model):\n+            # Explicit symmetrical=True.\n+            friends = models.ManyToManyField('self',\n+                through=\"Relationship\", symmetrical=True)\n+\n+        class Relationship(models.Model):\n+            first = models.ForeignKey(Person, related_name=\"rel_from_set\")\n+            second = models.ForeignKey(Person, related_name=\"rel_to_set\")\n+\n+        field = Person._meta.get_field('friends')\n+        errors = field.check(from_model=Person)\n+        expected = [\n+            Error(\n+                'Many-to-many fields with intermediate tables must not be symmetrical.',\n+                hint=None,\n+                obj=field,\n+                id='E024',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_foreign_key_to_abstract_model(self):\n+        class Model(models.Model):\n+            foreign_key = models.ForeignKey('AbstractModel')\n+\n+        class AbstractModel(models.Model):\n+            class Meta:\n+                abstract = True\n+\n+        field = Model._meta.get_field('foreign_key')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field has a relation with model AbstractModel, '\n+                    'which has either not been installed or is abstract.',\n+                hint='Ensure that you did not misspell the model name and '\n+                    'the model is not abstract. Does your INSTALLED_APPS '\n+                    'setting contain the app where AbstractModel is defined?',\n+                obj=field,\n+                id='E030',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_m2m_to_abstract_model(self):\n+        class AbstractModel(models.Model):\n+            class Meta:\n+                abstract = True\n+\n+        class Model(models.Model):\n+            m2m = models.ManyToManyField('AbstractModel')\n+\n+        field = Model._meta.get_field('m2m')\n+        errors = field.check(from_model=Model)\n+        expected = [\n+            Error(\n+                'The field has a relation with model AbstractModel, '\n+                    'which has either not been installed or is abstract.',\n+                hint='Ensure that you did not misspell the model name and '\n+                    'the model is not abstract. Does your INSTALLED_APPS '\n+                    'setting contain the app where AbstractModel is defined?',\n+                obj=field,\n+                id='E030',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_unique_m2m(self):\n+        class Person(models.Model):\n+            name = models.CharField(max_length=5)\n+\n+        class Group(models.Model):\n+            members = models.ManyToManyField('Person', unique=True)\n+\n+        field = Group._meta.get_field('members')\n+        errors = field.check(from_model=Group)\n+        expected = [\n+            Error(\n+                'ManyToManyFields must not be unique.',\n+                hint=None,\n+                obj=field,\n+                id='E022',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_foreign_key_to_non_unique_field(self):\n+        class Target(models.Model):\n+            bad = models.IntegerField()  # No unique=True\n+\n+        class Model(models.Model):\n+            foreign_key = models.ForeignKey('Target', to_field='bad')\n+\n+        field = Model._meta.get_field('foreign_key')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'Target.bad must have unique=True because it is referenced by a foreign key.',\n+                hint=None,\n+                obj=field,\n+                id='E019',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_foreign_key_to_non_unique_field_under_explicit_model(self):\n+        class Target(models.Model):\n+            bad = models.IntegerField()\n+\n+        class Model(models.Model):\n+            field = models.ForeignKey(Target, to_field='bad')\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'Target.bad must have unique=True because it is referenced by a foreign key.',\n+                hint=None,\n+                obj=field,\n+                id='E019',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_foreign_object_to_non_unique_fields(self):\n+        class Person(models.Model):\n+            # Note that both fields are not unique.\n+            country_id = models.IntegerField()\n+            city_id = models.IntegerField()\n+\n+        class MMembership(models.Model):\n+            person_country_id = models.IntegerField()\n+            person_city_id = models.IntegerField()\n+\n+            person = models.ForeignObject(Person,\n+                from_fields=['person_country_id', 'person_city_id'],\n+                to_fields=['country_id', 'city_id'])\n+\n+        field = MMembership._meta.get_field('person')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'No unique=True constraint on field combination '\n+                    '\"country_id,city_id\" under model Person.',\n+                hint='Set unique=True argument on any of the fields '\n+                    '\"country_id,city_id\" under model Person.',\n+                obj=field,\n+                id='E018',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_on_delete_set_null_on_non_nullable_field(self):\n+        class Person(models.Model):\n+            pass\n+\n+        class Model(models.Model):\n+            foreign_key = models.ForeignKey('Person',\n+                on_delete=models.SET_NULL)\n+\n+        field = Model._meta.get_field('foreign_key')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field specifies on_delete=SET_NULL, but cannot be null.',\n+                hint='Set null=True argument on the field.',\n+                obj=field,\n+                id='E020',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_on_delete_set_default_without_default_value(self):\n+        class Person(models.Model):\n+            pass\n+\n+        class Model(models.Model):\n+            foreign_key = models.ForeignKey('Person',\n+                on_delete=models.SET_DEFAULT)\n+\n+        field = Model._meta.get_field('foreign_key')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'The field specifies on_delete=SET_DEFAULT, but has no default value.',\n+                hint=None,\n+                obj=field,\n+                id='E021',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    @skipIfDBFeature('interprets_empty_strings_as_nulls')\n+    def test_nullable_primary_key(self):\n+        class Model(models.Model):\n+            field = models.IntegerField(primary_key=True, null=True)\n+\n+        field = Model._meta.get_field('field')\n+        errors = field.check()\n+        expected = [\n+            Error(\n+                'Primary keys must not have null=True.',\n+                hint='Set null=False on the field or remove primary_key=True argument.',\n+                obj=field,\n+                id='E036',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_not_swapped_model(self):\n+        class SwappableModel(models.Model):\n+            # A model that can be, but isn't swapped out. References to this\n+            # model should *not* raise any validation error.\n+            class Meta:\n+                swappable = 'TEST_SWAPPABLE_MODEL'\n+\n+        class Model(models.Model):\n+            explicit_fk = models.ForeignKey(SwappableModel,\n+                related_name='explicit_fk')\n+            implicit_fk = models.ForeignKey('invalid_models.SwappableModel',\n+                related_name='implicit_fk')\n+            explicit_m2m = models.ManyToManyField(SwappableModel,\n+                related_name='explicit_m2m')\n+            implicit_m2m = models.ManyToManyField(\n+                'invalid_models.SwappableModel',\n+                related_name='implicit_m2m')\n+\n+        explicit_fk = Model._meta.get_field('explicit_fk')\n+        self.assertEqual(explicit_fk.check(), [])\n+\n+        implicit_fk = Model._meta.get_field('implicit_fk')\n+        self.assertEqual(implicit_fk.check(), [])\n+\n+        explicit_m2m = Model._meta.get_field('explicit_m2m')\n+        self.assertEqual(explicit_m2m.check(from_model=Model), [])\n+\n+        implicit_m2m = Model._meta.get_field('implicit_m2m')\n+        self.assertEqual(implicit_m2m.check(from_model=Model), [])\n+\n+    @override_settings(TEST_SWAPPED_MODEL='invalid_models.Replacement')\n+    def test_referencing_to_swapped_model(self):\n+        class Replacement(models.Model):\n+            pass\n+\n+        class SwappedModel(models.Model):\n+            class Meta:\n+                swappable = 'TEST_SWAPPED_MODEL'\n+\n+        class Model(models.Model):\n+            explicit_fk = models.ForeignKey(SwappedModel,\n+                related_name='explicit_fk')\n+            implicit_fk = models.ForeignKey('invalid_models.SwappedModel',\n+                related_name='implicit_fk')\n+            explicit_m2m = models.ManyToManyField(SwappedModel,\n+                related_name='explicit_m2m')\n+            implicit_m2m = models.ManyToManyField(\n+                'invalid_models.SwappedModel',\n+                related_name='implicit_m2m')\n+\n+        fields = [\n+            Model._meta.get_field('explicit_fk'),\n+            Model._meta.get_field('implicit_fk'),\n+            Model._meta.get_field('explicit_m2m'),\n+            Model._meta.get_field('implicit_m2m'),\n+        ]\n+\n+        expected_error = Error(\n+            'The field defines a relation with the model '\n+                'invalid_models.SwappedModel, which has been swapped out.',\n+            hint='Update the relation to point at settings.TEST_SWAPPED_MODEL',\n+            id='E029',\n+        )\n+\n+        for field in fields:\n+            expected_error.obj = field\n+            errors = field.check(from_model=Model)\n+            self.assertEqual(errors, [expected_error])\n+\n+\n+class AccessorClashTests(IsolatedModelsTestCase):\n+\n+    def test_fk_to_integer(self):\n+        self._test_accessor_clash(\n+            target=models.IntegerField(),\n+            relative=models.ForeignKey('Target'))\n+\n+    def test_fk_to_fk(self):\n+        self._test_accessor_clash(\n+            target=models.ForeignKey('Another'),\n+            relative=models.ForeignKey('Target'))\n+\n+    def test_fk_to_m2m(self):\n+        self._test_accessor_clash(\n+            target=models.ManyToManyField('Another'),\n+            relative=models.ForeignKey('Target'))\n+\n+    def test_m2m_to_integer(self):\n+        self._test_accessor_clash(\n+            target=models.IntegerField(),\n+            relative=models.ManyToManyField('Target'))\n+\n+    def test_m2m_to_fk(self):\n+        self._test_accessor_clash(\n+            target=models.ForeignKey('Another'),\n+            relative=models.ManyToManyField('Target'))\n+\n+    def test_m2m_to_m2m(self):\n+        self._test_accessor_clash(\n+            target=models.ManyToManyField('Another'),\n+            relative=models.ManyToManyField('Target'))\n+\n+    def _test_accessor_clash(self, target, relative):\n+        class Another(models.Model):\n+            pass\n+\n+        class Target(models.Model):\n+            model_set = target\n+\n+        class Model(models.Model):\n+            rel = relative\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Accessor for field Model.rel clashes with field Target.model_set.',\n+                hint='Rename field Target.model_set or add/change '\n+                    'a related_name argument to the definition '\n+                    'for field Model.rel.',\n+                obj=Model._meta.get_field('rel'),\n+                id='E014',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_clash_between_accessors(self):\n+        class Target(models.Model):\n+            pass\n+\n+        class Model(models.Model):\n+            foreign = models.ForeignKey(Target)\n+            m2m = models.ManyToManyField(Target)\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Clash between accessors for Model.foreign and Model.m2m.',\n+                hint='Add or change a related_name argument to the definition '\n+                    'for Model.foreign or Model.m2m.',\n+                obj=Model._meta.get_field('foreign'),\n+                id='E016',\n+            ),\n+            Error(\n+                'Clash between accessors for Model.m2m and Model.foreign.',\n+                hint='Add or change a related_name argument to the definition '\n+                    'for Model.m2m or Model.foreign.',\n+                obj=Model._meta.get_field('m2m'),\n+                id='E016',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class ReverseQueryNameClashTests(IsolatedModelsTestCase):\n+\n+    def test_fk_to_integer(self):\n+        self._test_reverse_query_name_clash(\n+            target=models.IntegerField(),\n+            relative=models.ForeignKey('Target'))\n+\n+    def test_fk_to_fk(self):\n+        self._test_reverse_query_name_clash(\n+            target=models.ForeignKey('Another'),\n+            relative=models.ForeignKey('Target'))\n+\n+    def test_fk_to_m2m(self):\n+        self._test_reverse_query_name_clash(\n+            target=models.ManyToManyField('Another'),\n+            relative=models.ForeignKey('Target'))\n+\n+    def test_m2m_to_integer(self):\n+        self._test_reverse_query_name_clash(\n+            target=models.IntegerField(),\n+            relative=models.ManyToManyField('Target'))\n+\n+    def test_m2m_to_fk(self):\n+        self._test_reverse_query_name_clash(\n+            target=models.ForeignKey('Another'),\n+            relative=models.ManyToManyField('Target'))\n+\n+    def test_m2m_to_m2m(self):\n+        self._test_reverse_query_name_clash(\n+            target=models.ManyToManyField('Another'),\n+            relative=models.ManyToManyField('Target'))\n+\n+    def _test_reverse_query_name_clash(self, target, relative):\n+        class Another(models.Model):\n+            pass\n+\n+        class Target(models.Model):\n+            model = target\n+\n+        class Model(models.Model):\n+            rel = relative\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Reverse query name for field Model.rel clashes with field Target.model.',\n+                hint='Rename field Target.model or add/change '\n+                    'a related_name argument to the definition '\n+                    'for field Model.rel.',\n+                obj=Model._meta.get_field('rel'),\n+                id='E015',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class ExplicitRelatedNameClashTests(IsolatedModelsTestCase):\n+\n+    def test_fk_to_integer(self):\n+        self._test_explicit_related_name_clash(\n+            target=models.IntegerField(),\n+            relative=models.ForeignKey('Target', related_name='clash'))\n+\n+    def test_fk_to_fk(self):\n+        self._test_explicit_related_name_clash(\n+            target=models.ForeignKey('Another'),\n+            relative=models.ForeignKey('Target', related_name='clash'))\n+\n+    def test_fk_to_m2m(self):\n+        self._test_explicit_related_name_clash(\n+            target=models.ManyToManyField('Another'),\n+            relative=models.ForeignKey('Target', related_name='clash'))\n+\n+    def test_m2m_to_integer(self):\n+        self._test_explicit_related_name_clash(\n+            target=models.IntegerField(),\n+            relative=models.ManyToManyField('Target', related_name='clash'))\n+\n+    def test_m2m_to_fk(self):\n+        self._test_explicit_related_name_clash(\n+            target=models.ForeignKey('Another'),\n+            relative=models.ManyToManyField('Target', related_name='clash'))\n+\n+    def test_m2m_to_m2m(self):\n+        self._test_explicit_related_name_clash(\n+            target=models.ManyToManyField('Another'),\n+            relative=models.ManyToManyField('Target', related_name='clash'))\n+\n+    def _test_explicit_related_name_clash(self, target, relative):\n+        class Another(models.Model):\n+            pass\n+\n+        class Target(models.Model):\n+            clash = target\n+\n+        class Model(models.Model):\n+            rel = relative\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Accessor for field Model.rel clashes with field Target.clash.',\n+                hint='Rename field Target.clash or add/change '\n+                    'a related_name argument to the definition '\n+                    'for field Model.rel.',\n+                obj=Model._meta.get_field('rel'),\n+                id='E014',\n+            ),\n+            Error('Reverse query name for field Model.rel clashes with field Target.clash.',\n+                hint='Rename field Target.clash or add/change '\n+                    'a related_name argument to the definition '\n+                    'for field Model.rel.',\n+                obj=Model._meta.get_field('rel'),\n+                id='E015',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class ExplicitRelatedQueryNameClashTests(IsolatedModelsTestCase):\n+\n+    def test_fk_to_integer(self):\n+        self._test_explicit_related_query_name_clash(\n+            target=models.IntegerField(),\n+            relative=models.ForeignKey('Target',\n+                related_query_name='clash'))\n+\n+    def test_fk_to_fk(self):\n+        self._test_explicit_related_query_name_clash(\n+            target=models.ForeignKey('Another'),\n+            relative=models.ForeignKey('Target',\n+                related_query_name='clash'))\n+\n+    def test_fk_to_m2m(self):\n+        self._test_explicit_related_query_name_clash(\n+            target=models.ManyToManyField('Another'),\n+            relative=models.ForeignKey('Target',\n+                related_query_name='clash'))\n+\n+    def test_m2m_to_integer(self):\n+        self._test_explicit_related_query_name_clash(\n+            target=models.IntegerField(),\n+            relative=models.ManyToManyField('Target',\n+                related_query_name='clash'))\n+\n+    def test_m2m_to_fk(self):\n+        self._test_explicit_related_query_name_clash(\n+            target=models.ForeignKey('Another'),\n+            relative=models.ManyToManyField('Target',\n+                related_query_name='clash'))\n+\n+    def test_m2m_to_m2m(self):\n+        self._test_explicit_related_query_name_clash(\n+            target=models.ManyToManyField('Another'),\n+            relative=models.ManyToManyField('Target',\n+                related_query_name='clash'))\n+\n+    def _test_explicit_related_query_name_clash(self, target, relative):\n+        class Another(models.Model):\n+            pass\n+\n+        class Target(models.Model):\n+            clash = target\n+\n+        class Model(models.Model):\n+            rel = relative\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Reverse query name for field Model.rel clashes with field Target.clash.',\n+                hint='Rename field Target.clash or add/change a related_name '\n+                    'argument to the definition for field Model.rel.',\n+                obj=Model._meta.get_field('rel'),\n+                id='E015',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class SelfReferentialM2MClashTests(IsolatedModelsTestCase):\n+\n+    def test_clash_between_accessors(self):\n+        class Model(models.Model):\n+            first_m2m = models.ManyToManyField('self', symmetrical=False)\n+            second_m2m = models.ManyToManyField('self', symmetrical=False)\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Clash between accessors for Model.first_m2m and Model.second_m2m.',\n+                hint=u'Add or change a related_name argument to the definition '\n+                    'for Model.first_m2m or Model.second_m2m.',\n+                obj=Model._meta.get_field('first_m2m'),\n+                id='E016',\n+            ),\n+            Error(\n+                'Clash between accessors for Model.second_m2m and Model.first_m2m.',\n+                hint=u'Add or change a related_name argument to the definition '\n+                    'for Model.second_m2m or Model.first_m2m.',\n+                obj=Model._meta.get_field('second_m2m'),\n+                id='E016',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_accessor_clash(self):\n+        class Model(models.Model):\n+            model_set = models.ManyToManyField(\"self\", symmetrical=False)\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Accessor for field Model.model_set clashes with field Model.model_set.',\n+                hint='Rename field Model.model_set or add/change '\n+                    'a related_name argument to the definition '\n+                    'for field Model.model_set.',\n+                obj=Model._meta.get_field('model_set'),\n+                id='E014',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_reverse_query_name_clash(self):\n+        class Model(models.Model):\n+            model = models.ManyToManyField(\"self\", symmetrical=False)\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Reverse query name for field Model.model clashes with field Model.model.',\n+                hint='Rename field Model.model or add/change a related_name '\n+                    'argument to the definition for field Model.model.',\n+                obj=Model._meta.get_field('model'),\n+                id='E015',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_clash_under_explicit_related_name(self):\n+        class Model(models.Model):\n+            clash = models.IntegerField()\n+            m2m = models.ManyToManyField(\"self\",\n+                symmetrical=False, related_name='clash')\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Accessor for field Model.m2m clashes with field Model.clash.',\n+                hint='Rename field Model.clash or add/change a related_name '\n+                    'argument to the definition for field Model.m2m.',\n+                obj=Model._meta.get_field('m2m'),\n+                id='E014',\n+            ),\n+            Error(\n+                'Reverse query name for field Model.m2m clashes with field Model.clash.',\n+                hint='Rename field Model.clash or add/change a related_name '\n+                    'argument to the definition for field Model.m2m.',\n+                obj=Model._meta.get_field('m2m'),\n+                id='E015',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_valid_model(self):\n+        class Model(models.Model):\n+            first = models.ManyToManyField(\"self\",\n+                symmetrical=False, related_name='first_accessor')\n+            second = models.ManyToManyField(\"self\",\n+                symmetrical=False, related_name='second_accessor')\n+\n+        errors = Model.check()\n+        self.assertEqual(errors, [])\n+\n+\n+class SelfReferentialFKClashTests(IsolatedModelsTestCase):\n+\n+    def test_accessor_clash(self):\n+        class Model(models.Model):\n+            model_set = models.ForeignKey(\"Model\")\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Accessor for field Model.model_set clashes with field Model.model_set.',\n+                hint='Rename field Model.model_set or add/change '\n+                    'a related_name argument to the definition '\n+                    'for field Model.model_set.',\n+                obj=Model._meta.get_field('model_set'),\n+                id='E014',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_reverse_query_name_clash(self):\n+        class Model(models.Model):\n+            model = models.ForeignKey(\"Model\")\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Reverse query name for field Model.model clashes with field Model.model.',\n+                hint='Rename field Model.model or add/change '\n+                    'a related_name argument to the definition '\n+                    'for field Model.model.',\n+                obj=Model._meta.get_field('model'),\n+                id='E015',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def test_clash_under_explicit_related_name(self):\n+        class Model(models.Model):\n+            clash = models.CharField(max_length=10)\n+            foreign = models.ForeignKey(\"Model\", related_name='clash')\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Accessor for field Model.foreign clashes with field Model.clash.',\n+                hint='Rename field Model.clash or add/change '\n+                    'a related_name argument to the definition '\n+                    'for field Model.foreign.',\n+                obj=Model._meta.get_field('foreign'),\n+                id='E014',\n+            ),\n+            Error(\n+                'Reverse query name for field Model.foreign clashes with field Model.clash.',\n+                hint='Rename field Model.clash or add/change '\n+                    'a related_name argument to the definition '\n+                    'for field Model.foreign.',\n+                obj=Model._meta.get_field('foreign'),\n+                id='E015',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+\n+class ComplexClashTests(IsolatedModelsTestCase):\n+\n+    # New tests should not be included here, because this is a single,\n+    # self-contained sanity check, not a test of everything.\n+    def test_complex_clash(self):\n+        class Target(models.Model):\n+            tgt_safe = models.CharField(max_length=10)\n+            clash = models.CharField(max_length=10)\n+            model = models.CharField(max_length=10)\n+\n+            clash1_set = models.CharField(max_length=10)\n+\n+        class Model(models.Model):\n+            src_safe = models.CharField(max_length=10)\n+\n+            foreign_1 = models.ForeignKey(Target, related_name='id')\n+            foreign_2 = models.ForeignKey(Target, related_name='src_safe')\n+\n+            m2m_1 = models.ManyToManyField(Target, related_name='id')\n+            m2m_2 = models.ManyToManyField(Target, related_name='src_safe')\n+\n+        errors = Model.check()\n+        expected = [\n+            Error(\n+                'Accessor for field Model.foreign_1 clashes with field Target.id.',\n+                hint='Rename field Target.id or add/change a related_name '\n+                    'argument to the definition for field Model.foreign_1.',\n+                obj=Model._meta.get_field('foreign_1'),\n+                id='E014',\n+            ),\n+            Error(\n+                'Reverse query name for field Model.foreign_1 clashes with field Target.id.',\n+                hint='Rename field Target.id or add/change a related_name '\n+                    'argument to the definition for field Model.foreign_1.',\n+                obj=Model._meta.get_field('foreign_1'),\n+                id='E015',\n+            ),\n+            Error(\n+                'Clash between accessors for Model.foreign_1 and Model.m2m_1.',\n+                hint='Add or change a related_name argument to '\n+                    'the definition for Model.foreign_1 or Model.m2m_1.',\n+                obj=Model._meta.get_field('foreign_1'),\n+                id='E016',\n+            ),\n+            Error(\n+                'Clash between reverse query names for Model.foreign_1 and Model.m2m_1.',\n+                hint='Add or change a related_name argument to '\n+                    'the definition for Model.foreign_1 or Model.m2m_1.',\n+                obj=Model._meta.get_field('foreign_1'),\n+                id='E017',\n+            ),\n+\n+            Error(\n+                'Clash between accessors for Model.foreign_2 and Model.m2m_2.',\n+                hint='Add or change a related_name argument '\n+                    'to the definition for Model.foreign_2 or Model.m2m_2.',\n+                obj=Model._meta.get_field('foreign_2'),\n+                id='E016',\n+            ),\n+            Error(\n+                'Clash between reverse query names for Model.foreign_2 and Model.m2m_2.',\n+                hint='Add or change a related_name argument to '\n+                    'the definition for Model.foreign_2 or Model.m2m_2.',\n+                obj=Model._meta.get_field('foreign_2'),\n+                id='E017',\n+            ),\n+\n+            Error(\n+                'Accessor for field Model.m2m_1 clashes with field Target.id.',\n+                hint='Rename field Target.id or add/change a related_name '\n+                    'argument to the definition for field Model.m2m_1.',\n+                obj=Model._meta.get_field('m2m_1'),\n+                id='E014',\n+            ),\n+            Error(\n+                'Reverse query name for field Model.m2m_1 clashes with field Target.id.',\n+                hint='Rename field Target.id or add/change a related_name '\n+                    'argument to the definition for field Model.m2m_1.',\n+                obj=Model._meta.get_field('m2m_1'),\n+                id='E015',\n+            ),\n+            Error(\n+                'Clash between accessors for Model.m2m_1 and Model.foreign_1.',\n+                hint='Add or change a related_name argument to the definition '\n+                    'for Model.m2m_1 or Model.foreign_1.',\n+                obj=Model._meta.get_field('m2m_1'),\n+                id='E016',\n+            ),\n+            Error(\n+                'Clash between reverse query names for Model.m2m_1 and Model.foreign_1.',\n+                hint='Add or change a related_name argument to '\n+                    'the definition for Model.m2m_1 or Model.foreign_1.',\n+                obj=Model._meta.get_field('m2m_1'),\n+                id='E017',\n+            ),\n+\n+            Error(\n+                'Clash between accessors for Model.m2m_2 and Model.foreign_2.',\n+                hint='Add or change a related_name argument to the definition '\n+                    'for Model.m2m_2 or Model.foreign_2.',\n+                obj=Model._meta.get_field('m2m_2'),\n+                id='E016',\n+            ),\n+            Error(\n+                'Clash between reverse query names for Model.m2m_2 and Model.foreign_2.',\n+                hint='Add or change a related_name argument to the definition '\n+                    'for Model.m2m_2 or Model.foreign_2.',\n+                obj=Model._meta.get_field('m2m_2'),\n+                id='E017',\n+            ),\n+        ]\n+        self.assertEqual(errors, expected)\ndiff --git a/tests/invalid_models/tests.py b/tests/invalid_models/tests.py\ndeleted file mode 100644\nindex 9c9db91da9dd..000000000000\n--- a/tests/invalid_models/tests.py\n+++ /dev/null\n@@ -1,58 +0,0 @@\n-import copy\n-import sys\n-import unittest\n-\n-from django.core.management.validation import get_validation_errors\n-from django.db.models.loading import cache, load_app\n-from django.test.utils import override_settings\n-from django.utils.six import StringIO\n-\n-\n-class InvalidModelTestCase(unittest.TestCase):\n-    \"\"\"Import an appliation with invalid models and test the exceptions.\"\"\"\n-\n-    def setUp(self):\n-        # Make sure sys.stdout is not a tty so that we get errors without\n-        # coloring attached (makes matching the results easier). We restore\n-        # sys.stderr afterwards.\n-        self.old_stdout = sys.stdout\n-        self.stdout = StringIO()\n-        sys.stdout = self.stdout\n-\n-        # This test adds dummy applications to the app cache. These\n-        # need to be removed in order to prevent bad interactions\n-        # with the flush operation in other tests.\n-        self.old_app_models = copy.deepcopy(cache.app_models)\n-        self.old_app_store = copy.deepcopy(cache.app_store)\n-\n-    def tearDown(self):\n-        cache.app_models = self.old_app_models\n-        cache.app_store = self.old_app_store\n-        cache._get_models_cache = {}\n-        sys.stdout = self.old_stdout\n-\n-    # Technically, this isn't an override -- TEST_SWAPPED_MODEL must be\n-    # set to *something* in order for the test to work. However, it's\n-    # easier to set this up as an override than to require every developer\n-    # to specify a value in their test settings.\n-    @override_settings(\n-        TEST_SWAPPED_MODEL='invalid_models.ReplacementModel',\n-        TEST_SWAPPED_MODEL_BAD_VALUE='not-a-model',\n-        TEST_SWAPPED_MODEL_BAD_MODEL='not_an_app.Target',\n-    )\n-    def test_invalid_models(self):\n-        try:\n-            module = load_app(\"invalid_models.invalid_models\")\n-        except Exception:\n-            self.fail('Unable to load invalid model module')\n-\n-        get_validation_errors(self.stdout, module)\n-        self.stdout.seek(0)\n-        error_log = self.stdout.read()\n-        actual = error_log.split('\\n')\n-        expected = module.model_errors.split('\\n')\n-\n-        unexpected = [err for err in actual if err not in expected]\n-        missing = [err for err in expected if err not in actual]\n-        self.assertFalse(unexpected, \"Unexpected Errors: \" + '\\n'.join(unexpected))\n-        self.assertFalse(missing, \"Missing Errors: \" + '\\n'.join(missing))\ndiff --git a/tests/logging_tests/tests.py b/tests/logging_tests/tests.py\nindex ef9f979e7eea..02b84f01e4e7 100644\n--- a/tests/logging_tests/tests.py\n+++ b/tests/logging_tests/tests.py\n@@ -335,7 +335,7 @@ def test_circular_dependency(self):\n         # validate is just an example command to trigger settings configuration\n         out, err = self.run_manage(['validate'])\n         self.assertNoOutput(err)\n-        self.assertOutput(out, \"0 errors found\")\n+        self.assertOutput(out, \"System check identified no problems.\")\n \n \n def dictConfig(config):\ndiff --git a/tests/migrate_signals/tests.py b/tests/migrate_signals/tests.py\nindex 9daa7ae3766a..8b5f5bd56e9e 100644\n--- a/tests/migrate_signals/tests.py\n+++ b/tests/migrate_signals/tests.py\n@@ -1,6 +1,7 @@\n+from django.core import management\n from django.db.models import signals\n from django.test import TestCase\n-from django.core import management\n+from django.test.utils import override_system_checks\n from django.utils import six\n \n from . import models\n@@ -63,6 +64,9 @@ class MigrateSignalTests(TestCase):\n     def test_pre_migrate_call_time(self):\n         self.assertEqual(pre_migrate_receiver.call_counter, 1)\n \n+    # `auth` app is imported, but not installed in this test, so we need to\n+    # exclude checks registered by this app.\n+    @override_system_checks([])\n     def test_pre_migrate_args(self):\n         r = PreMigrateReceiver()\n         signals.pre_migrate.connect(r, sender=models)\ndiff --git a/tests/migrations/test_commands.py b/tests/migrations/test_commands.py\nindex 01d11e2c0c71..9baae683bd44 100644\n--- a/tests/migrations/test_commands.py\n+++ b/tests/migrations/test_commands.py\n@@ -6,7 +6,7 @@\n \n from django.core.management import call_command\n from django.db.models.loading import cache\n-from django.test.utils import override_settings\n+from django.test.utils import override_settings, override_system_checks\n from django.utils import six\n from django.utils._os import upath\n from django.utils.encoding import force_text\n@@ -20,11 +20,13 @@ class MigrateTests(MigrationTestBase):\n     Tests running the migrate command.\n     \"\"\"\n \n+    # `auth` app is imported, but not installed in these tests (thanks to\n+    # MigrationTestBase), so we need to exclude checks registered by this app.\n+\n+    @override_system_checks([])\n     @override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"})\n     def test_migrate(self):\n-        \"\"\"\n-        Tests basic usage of the migrate command.\n-        \"\"\"\n+        \"\"\" Tests basic usage of the migrate command. \"\"\"\n         # Make sure no tables are created\n         self.assertTableNotExists(\"migrations_author\")\n         self.assertTableNotExists(\"migrations_tribble\")\n@@ -48,6 +50,7 @@ def test_migrate(self):\n         self.assertTableNotExists(\"migrations_tribble\")\n         self.assertTableNotExists(\"migrations_book\")\n \n+    @override_system_checks([])\n     @override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"})\n     def test_migrate_list(self):\n         \"\"\"\n@@ -70,6 +73,7 @@ def test_migrate_list(self):\n         # Cleanup by unmigrating everything\n         call_command(\"migrate\", \"migrations\", \"zero\", verbosity=0)\n \n+    @override_system_checks([])\n     @override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"})\n     def test_sqlmigrate(self):\n         \"\"\"\n@@ -108,6 +112,9 @@ def _rmrf(self, dname):\n             return\n         shutil.rmtree(dname)\n \n+    # `auth` app is imported, but not installed in this test (thanks to\n+    # MigrationTestBase), so we need to exclude checks registered by this app.\n+    @override_system_checks([])\n     def test_files_content(self):\n         self.assertTableNotExists(\"migrations_unicodemodel\")\n         cache.register_models('migrations', UnicodeModel)\ndiff --git a/tests/model_fields/tests.py b/tests/model_fields/tests.py\nindex ac7dd0252f73..9d266f529566 100644\n--- a/tests/model_fields/tests.py\n+++ b/tests/model_fields/tests.py\n@@ -85,6 +85,11 @@ def test_choices_form_class(self):\n         klass = forms.TypedMultipleChoiceField\n         self.assertIsInstance(field.formfield(choices_form_class=klass), klass)\n \n+    def test_field_str(self):\n+        from django.utils.encoding import force_str\n+        f = Foo._meta.get_field('a')\n+        self.assertEqual(force_str(f), \"model_fields.Foo.a\")\n+\n \n class DecimalFieldTests(test.TestCase):\n     def test_to_python(self):\ndiff --git a/tests/modeladmin/tests.py b/tests/modeladmin/tests.py\nindex f7d78677c334..2042976bb030 100644\n--- a/tests/modeladmin/tests.py\n+++ b/tests/modeladmin/tests.py\n@@ -1,22 +1,22 @@\n from __future__ import unicode_literals\n \n from datetime import date\n-import unittest\n \n from django import forms\n-from django.conf import settings\n from django.contrib.admin.options import (ModelAdmin, TabularInline,\n      HORIZONTAL, VERTICAL)\n from django.contrib.admin.sites import AdminSite\n from django.contrib.admin.widgets import AdminDateWidget, AdminRadioSelect\n+from django.contrib.admin.validation import ModelAdminValidator\n from django.contrib.admin import (SimpleListFilter,\n      BooleanFieldListFilter)\n+from django.core.checks import Error\n from django.core.exceptions import ImproperlyConfigured\n from django.forms.models import BaseModelFormSet\n from django.forms.widgets import Select\n from django.test import TestCase\n-from django.test.utils import str_prefix\n-from django.utils import six\n+from django.utils.encoding import force_text\n+\n \n from .models import Band, Concert, ValidationTestModel, ValidationTestInlineModel\n \n@@ -43,6 +43,19 @@ def setUp(self):\n         )\n         self.site = AdminSite()\n \n+    def test_str_on_model_admin_class(self):\n+        got = force_text(ModelAdmin)\n+        expected = \"django.contrib.admin.options.ModelAdmin\"\n+        self.assertEqual(got, expected)\n+\n+    def test_str_on_inline_model_admin_class(self):\n+        class MyInline(TabularInline):\n+            model = ValidationTestInlineModel\n+\n+        got = force_text(MyInline)\n+        expected = \"modeladmin.tests.MyInline\"\n+        self.assertEqual(got, expected)\n+\n     # form/fields/fieldsets interaction ##############################\n \n     def test_default_fields(self):\n@@ -537,181 +550,190 @@ class BandAdmin(ModelAdmin):\n             ['extra', 'transport', 'id', 'DELETE', 'main_band'])\n \n \n-class ValidationTests(unittest.TestCase):\n-    def test_validation_only_runs_in_debug(self):\n-        # Ensure validation only runs when DEBUG = True\n-        try:\n-            settings.DEBUG = True\n+class CheckTestCase(TestCase):\n \n-            class ValidationTestModelAdmin(ModelAdmin):\n-                raw_id_fields = 10\n-\n-            site = AdminSite()\n-\n-            six.assertRaisesRegex(self,\n-                ImproperlyConfigured,\n-                \"'ValidationTestModelAdmin.raw_id_fields' must be a list or tuple.\",\n-                site.register,\n-                ValidationTestModel,\n-                ValidationTestModelAdmin,\n+    def assertIsInvalid(self, model_admin, model, msg,\n+            id=None, hint=None, invalid_obj=None):\n+        invalid_obj = invalid_obj or model_admin\n+        errors = model_admin.check(model=model)\n+        expected = [\n+            Error(\n+                msg,\n+                hint=hint,\n+                obj=invalid_obj,\n+                id=id,\n             )\n-        finally:\n-            settings.DEBUG = False\n+        ]\n+        self.assertEqual(errors, expected)\n+\n+    def assertIsValid(self, model_admin, model):\n+        errors = model_admin.check(model=model)\n+        expected = []\n+        self.assertEqual(errors, expected)\n \n-        site = AdminSite()\n-        site.register(ValidationTestModel, ValidationTestModelAdmin)\n \n-    def test_raw_id_fields_validation(self):\n+class RawIdCheckTests(CheckTestCase):\n \n+    def test_not_iterable(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             raw_id_fields = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.raw_id_fields' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"raw_id_fields\" must be a list or tuple.',\n+            'admin.E001')\n \n+    def test_missing_field(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             raw_id_fields = ('non_existent_field',)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.raw_id_fields' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"raw_id_fields[0]\" refers to field \"non_existent_field\", '\n+                'which is missing from model modeladmin.ValidationTestModel.',\n+            'admin.E002')\n \n+    def test_invalid_field_type(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             raw_id_fields = ('name',)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.raw_id_fields\\[0\\]', 'name' must be either a ForeignKey or ManyToManyField.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"raw_id_fields[0]\" must be a ForeignKey or ManyToManyField.',\n+            'admin.E003')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             raw_id_fields = ('users',)\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n+\n+class FieldsetsCheckTests(CheckTestCase):\n+\n+    def test_valid_case(self):\n+        class ValidationTestModelAdmin(ModelAdmin):\n+            fieldsets = ((\"General\", {\"fields\": (\"name\",)}),)\n+\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_fieldsets_validation(self):\n+    def test_not_iterable(self):\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             fieldsets = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.fieldsets' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"fieldsets\" must be a list or tuple.',\n+            'admin.E007')\n \n+    def test_non_iterable_item(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             fieldsets = ({},)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.fieldsets\\[0\\]' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"fieldsets[0]\" must be a list or tuple.',\n+            'admin.E008')\n \n+    def test_item_not_a_pair(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             fieldsets = ((),)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.fieldsets\\[0\\]' does not have exactly two elements.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"fieldsets[0]\" must be a pair.',\n+            'admin.E009')\n \n+    def test_second_element_of_item_not_a_dict(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             fieldsets = ((\"General\", ()),)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.fieldsets\\[0\\]\\[1\\]' must be a dictionary.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"fieldsets[0][1]\" must be a dictionary.',\n+            'admin.E010')\n \n+    def test_missing_fields_key(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             fieldsets = ((\"General\", {}),)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'fields' key is required in ValidationTestModelAdmin.fieldsets\\[0\\]\\[1\\] field options dict.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n-\n-        class ValidationTestModelAdmin(ModelAdmin):\n-            fieldsets = ((\"General\", {\"fields\": (\"name\",)}),)\n-\n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"fieldsets[0][1]\" must contain \"fields\" key.',\n+            'admin.E011')\n \n+    def test_specified_both_fields_and_fieldsets(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             fieldsets = ((\"General\", {\"fields\": (\"name\",)}),)\n             fields = [\"name\",]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"Both fieldsets and fields are specified in ValidationTestModelAdmin.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+             'Both \"fieldsets\" and \"fields\" are specified.',\n+             'admin.E005')\n \n+    def test_duplicate_fields(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             fieldsets = [(None, {'fields': ['name', 'name']})]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"There are duplicate field\\(s\\) in ValidationTestModelAdmin.fieldsets\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+             'There are duplicate field(s) in \"fieldsets[0][1]\".',\n+             'admin.E012')\n+\n+    def test_fieldsets_with_custom_form_validation(self):\n+        class BandAdmin(ModelAdmin):\n+            fieldsets = (\n+                ('Band', {\n+                    'fields': ('name',)\n+                }),\n+            )\n+\n+        self.assertIsValid(BandAdmin, Band)\n+\n \n+class FieldsCheckTests(CheckTestCase):\n+\n+    def test_duplicate_fields_in_fields(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             fields = [\"name\", \"name\"]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"There are duplicate field\\(s\\) in ValidationTestModelAdmin.fields\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            'There are duplicate field(s) in \"fields\".',\n+            'admin.E006')\n+\n+    def test_inline(self):\n+        class ValidationTestInline(TabularInline):\n+            model = ValidationTestInlineModel\n+            fields = 10\n+\n+        class ValidationTestModelAdmin(ModelAdmin):\n+            inlines = [ValidationTestInline]\n+\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"fields\" must be a list or tuple.',\n+            'admin.E004',\n+            invalid_obj=ValidationTestInline)\n \n-    def test_form_validation(self):\n \n+class FormCheckTests(CheckTestCase):\n+\n+    def test_invalid_type(self):\n         class FakeForm(object):\n             pass\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             form = FakeForm\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"ValidationTestModelAdmin.form does not inherit from BaseModelForm.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n-\n-    def test_fieldsets_with_custom_form_validation(self):\n-\n-        class BandAdmin(ModelAdmin):\n-            fieldsets = (\n-                ('Band', {\n-                    'fields': ('name',)\n-                }),\n-            )\n-\n-        BandAdmin.validate(Band)\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"form\" must inherit from BaseModelForm.',\n+            'admin.E016')\n \n+    def test_valid_case(self):\n         class AdminBandForm(forms.ModelForm):\n             delete = forms.BooleanField()\n \n@@ -724,208 +746,212 @@ class BandAdmin(ModelAdmin):\n                 }),\n             )\n \n-        BandAdmin.validate(Band)\n+        self.assertIsValid(BandAdmin, Band)\n \n-    def test_filter_vertical_validation(self):\n \n+class FilterVerticalCheckTests(CheckTestCase):\n+\n+    def test_not_iterable(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             filter_vertical = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.filter_vertical' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"filter_vertical\" must be a list or tuple.',\n+            'admin.E017')\n \n+    def test_missing_field(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             filter_vertical = (\"non_existent_field\",)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.filter_vertical' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"filter_vertical[0]\" refers to field \"non_existent_field\", '\n+                'which is missing from model modeladmin.ValidationTestModel.',\n+            'admin.E019')\n \n+    def test_invalid_field_type(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             filter_vertical = (\"name\",)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.filter_vertical\\[0\\]' must be a ManyToManyField.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"filter_vertical[0]\" must be a ManyToManyField.',\n+            'admin.E020')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             filter_vertical = (\"users\",)\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n \n-    def test_filter_horizontal_validation(self):\n+class FilterHorizontalCheckTests(CheckTestCase):\n \n+    def test_not_iterable(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             filter_horizontal = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.filter_horizontal' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"filter_horizontal\" must be a list or tuple.',\n+            'admin.E018')\n \n+    def test_missing_field(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             filter_horizontal = (\"non_existent_field\",)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.filter_horizontal' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"filter_horizontal[0]\" refers to field \"non_existent_field\", '\n+                'which is missing from model modeladmin.ValidationTestModel.',\n+            'admin.E019')\n \n+    def test_invalid_field_type(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             filter_horizontal = (\"name\",)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.filter_horizontal\\[0\\]' must be a ManyToManyField.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"filter_horizontal[0]\" must be a ManyToManyField.',\n+            'admin.E020')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             filter_horizontal = (\"users\",)\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n+\n+class RadioFieldsCheckTests(CheckTestCase):\n \n-    def test_radio_fields_validation(self):\n+    def test_not_dictionary(self):\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             radio_fields = ()\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.radio_fields' must be a dictionary.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"radio_fields\" must be a dictionary.',\n+            'admin.E021')\n \n+    def test_missing_field(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n-            radio_fields = {\"non_existent_field\": None}\n+            radio_fields = {\"non_existent_field\": VERTICAL}\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.radio_fields' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"radio_fields\" refers to field \"non_existent_field\", '\n+                'which is missing from model modeladmin.ValidationTestModel.',\n+            'admin.E022')\n \n+    def test_invalid_field_type(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n-            radio_fields = {\"name\": None}\n+            radio_fields = {\"name\": VERTICAL}\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.radio_fields\\['name'\\]' is neither an instance of ForeignKey nor does have choices set.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"radio_fields\" refers to \"name\", which is neither an instance '\n+                'of ForeignKey nor does have choices set.',\n+            'admin.E023')\n \n+    def test_invalid_value(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             radio_fields = {\"state\": None}\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.radio_fields\\['state'\\]' is neither admin.HORIZONTAL nor admin.VERTICAL.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"radio_fields[\\'state\\']\" is neither admin.HORIZONTAL nor admin.VERTICAL.',\n+            'admin.E024')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             radio_fields = {\"state\": VERTICAL}\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n \n-    def test_prepopulated_fields_validation(self):\n+class PrepopulatedFieldsCheckTests(CheckTestCase):\n+\n+    def test_not_dictionary(self):\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             prepopulated_fields = ()\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.prepopulated_fields' must be a dictionary.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"prepopulated_fields\" must be a dictionary.',\n+            'admin.E025')\n \n+    def test_missing_field(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n-            prepopulated_fields = {\"non_existent_field\": None}\n+            prepopulated_fields = {\"non_existent_field\": (\"slug\",)}\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.prepopulated_fields' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"prepopulated_fields\" refers to field \"non_existent_field\", '\n+                'which is missing from model modeladmin.ValidationTestModel.',\n+            'admin.E026')\n \n+    def test_missing_field_again(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             prepopulated_fields = {\"slug\": (\"non_existent_field\",)}\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.prepopulated_fields\\['slug'\\]\\[0\\]' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"prepopulated_fields[\\'slug\\'][0]\" refers to field \"non_existent_field\", '\n+                'which is missing from model modeladmin.ValidationTestModel.',\n+            'admin.E029')\n \n+    def test_invalid_field_type(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             prepopulated_fields = {\"users\": (\"name\",)}\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.prepopulated_fields\\['users'\\]' is either a DateTimeField, ForeignKey or ManyToManyField. This isn't allowed.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"prepopulated_fields\" refers to \"users\", which must not be '\n+                'a DateTimeField, ForeignKey or ManyToManyField.',\n+            'admin.E027')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             prepopulated_fields = {\"slug\": (\"name\",)}\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_list_display_validation(self):\n+\n+class ListDisplayTests(CheckTestCase):\n+\n+    def test_not_iterable(self):\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             list_display = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_display' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_display\" must be a list or tuple.',\n+            'admin.E107')\n \n+    def test_missing_field(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_display = ('non_existent_field',)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            str_prefix(\"ValidationTestModelAdmin.list_display\\[0\\], %(_)s'non_existent_field' is not a callable or an attribute of 'ValidationTestModelAdmin' or found in the model 'ValidationTestModel'.\"),\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_display[0]\" is neither a callable nor an attribute '\n+                'of \"ValidationTestModelAdmin\" nor found in model modeladmin.ValidationTestModel.',\n+            'admin.E110')\n \n+    def test_invalid_field_type(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_display = ('users',)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_display\\[0\\]', 'users' is a ManyToManyField which is not supported.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_display[0]\" must not be a ManyToManyField.',\n+            'admin.E109')\n \n+    def test_valid_case(self):\n         def a_callable(obj):\n             pass\n \n@@ -934,40 +960,39 @@ def a_method(self, obj):\n                 pass\n             list_display = ('name', 'decade_published_in', 'a_method', a_callable)\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_list_display_links_validation(self):\n \n+class ListDisplayLinksCheckTests(CheckTestCase):\n+\n+    def test_not_iterable(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_display_links = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_display_links' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_display_links\" must be a list or tuple or None.',\n+            'admin.E111')\n \n+    def test_missing_field(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_display_links = ('non_existent_field',)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_display_links\\[0\\]' refers to 'non_existent_field' which is not defined in 'list_display'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_display_links[0]\" refers to \"non_existent_field\", which is not defined in \"list_display\".',\n+            'admin.E112')\n \n+    def test_missing_in_list_display(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_display_links = ('name',)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_display_links\\[0\\]' refers to 'name' which is not defined in 'list_display'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_display_links[0]\" refers to \"name\", which is not defined in \"list_display\".',\n+            'admin.E112')\n \n+    def test_valid_case(self):\n         def a_callable(obj):\n             pass\n \n@@ -977,58 +1002,60 @@ def a_method(self, obj):\n             list_display = ('name', 'decade_published_in', 'a_method', a_callable)\n             list_display_links = ('name', 'decade_published_in', 'a_method', a_callable)\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n+    def test_None_is_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_display_links = None\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_list_filter_validation(self):\n \n+class ListFilterTests(CheckTestCase):\n+\n+    def test_list_filter_validation(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_filter = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_filter' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_filter\" must be a list or tuple.',\n+            'admin.E113')\n \n+    def test_missing_field(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_filter = ('non_existent_field',)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_filter\\[0\\]' refers to 'non_existent_field' which does not refer to a Field.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_filter[0]\" refers to \"non_existent_field\", which does not refer to a Field.',\n+            'admin.E117')\n \n+    def test_not_filter(self):\n         class RandomClass(object):\n             pass\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             list_filter = (RandomClass,)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_filter\\[0\\]' is 'RandomClass' which is not a descendant of ListFilter.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_filter[0]\" must inherit from ListFilter.',\n+            'admin.E114')\n+\n+    def test_not_filter_again(self):\n+        class RandomClass(object):\n+            pass\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             list_filter = (('is_active', RandomClass),)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_filter\\[0\\]\\[1\\]' is 'RandomClass' which is not of type FieldListFilter.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_filter[0][1]\" must inherit from FieldListFilter.',\n+            'admin.E116')\n \n+    def test_not_filter_again_again(self):\n         class AwesomeFilter(SimpleListFilter):\n             def get_title(self):\n                 return 'awesomeness'\n@@ -1040,240 +1067,255 @@ def get_queryset(self, cl, qs):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_filter = (('is_active', AwesomeFilter),)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_filter\\[0\\]\\[1\\]' is 'AwesomeFilter' which is not of type FieldListFilter.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_filter[0][1]\" must inherit from FieldListFilter.',\n+            'admin.E116')\n \n+    def test_not_associated_with_field_name(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_filter = (BooleanFieldListFilter,)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_filter\\[0\\]' is 'BooleanFieldListFilter' which is of type FieldListFilter but is not associated with a field name.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_filter[0]\" must not inherit from FieldListFilter.',\n+            'admin.E115')\n \n-        # Valid declarations below -----------\n+    def test_valid_case(self):\n+        class AwesomeFilter(SimpleListFilter):\n+            def get_title(self):\n+                return 'awesomeness'\n+            def get_choices(self, request):\n+                return (('bit', 'A bit awesome'), ('very', 'Very awesome'), )\n+            def get_queryset(self, cl, qs):\n+                return qs\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             list_filter = ('is_active', AwesomeFilter, ('is_active', BooleanFieldListFilter), 'no')\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_list_per_page_validation(self):\n \n+class ListPerPageCheckTests(CheckTestCase):\n+\n+    def test_not_integer(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_per_page = 'hello'\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_per_page' should be a int.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_per_page\" must be an integer.',\n+            'admin.E119')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_per_page = 100\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_max_show_all_allowed_validation(self):\n \n+class ListMaxShowAllCheckTests(CheckTestCase):\n+\n+    def test_not_integer(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_max_show_all = 'hello'\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_max_show_all' should be a int.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_max_show_all\" must be an integer.',\n+            'admin.E120')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_max_show_all = 200\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n \n-    def test_search_fields_validation(self):\n+class SearchFieldsCheckTests(CheckTestCase):\n+\n+    def test_not_iterable(self):\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             search_fields = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.search_fields' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"search_fields\" must be a list or tuple.',\n+            'admin.E127')\n \n-    def test_date_hierarchy_validation(self):\n+\n+class DateHierarchyCheckTests(CheckTestCase):\n+\n+    def test_missing_field(self):\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             date_hierarchy = 'non_existent_field'\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.date_hierarchy' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"date_hierarchy\" refers to field \"non_existent_field\", which '\n+                'is missing from model modeladmin.ValidationTestModel.',\n+            'admin.E128')\n \n+    def test_invalid_field_type(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             date_hierarchy = 'name'\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.date_hierarchy is neither an instance of DateField nor DateTimeField.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"date_hierarchy\" must be a DateField or DateTimeField.',\n+            'admin.E129')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             date_hierarchy = 'pub_date'\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n \n-    def test_ordering_validation(self):\n+class OrderingCheckTests(CheckTestCase):\n \n+    def test_not_iterable(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             ordering = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.ordering' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"ordering\" must be a list or tuple.',\n+            'admin.E030')\n \n+    def test_missing_field(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             ordering = ('non_existent_field',)\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.ordering\\[0\\]' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"ordering[0]\" refers to field \"non_existent_field\", '\n+                'which is missing from model modeladmin.ValidationTestModel.',\n+            'admin.E032')\n \n+    def test_random_marker_not_alone(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             ordering = ('?', 'name')\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.ordering' has the random ordering marker '\\?', but contains other fields as well. Please either remove '\\?' or the other fields.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"ordering\" has the random ordering marker \"?\", but contains '\n+                'other fields as well.',\n+            'admin.E031',\n+            hint='Either remove the \"?\", or remove the other fields.')\n \n+    def test_valid_random_marker_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             ordering = ('?',)\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n+    def test_valid_complex_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             ordering = ('band__name',)\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             ordering = ('name',)\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_list_select_related_validation(self):\n \n+class ListSelectRelatedCheckTests(CheckTestCase):\n+\n+    def test_invalid_type(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_select_related = 1\n \n-        six.assertRaisesRegex(\n-            self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.list_select_related' should be either a \"\n-            \"bool, a tuple or a list\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(ValidationTestModelAdmin, ValidationTestModel,\n+            '\"list_select_related\" must be a boolean, tuple or list.',\n+            'admin.E118')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             list_select_related = False\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_save_as_validation(self):\n \n+class SaveAsCheckTests(CheckTestCase):\n+\n+    def test_not_boolean(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             save_as = 1\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.save_as' should be a bool.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"save_as\" must be a boolean.',\n+            'admin.E101')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             save_as = True\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n \n-    def test_save_on_top_validation(self):\n+class SaveOnTopCheckTests(CheckTestCase):\n \n+    def test_not_boolean(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             save_on_top = 1\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.save_on_top' should be a bool.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"save_on_top\" must be a boolean.',\n+            'admin.E102')\n \n+    def test_valid_case(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             save_on_top = True\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n \n-    def test_inlines_validation(self):\n+class InlinesCheckTests(CheckTestCase):\n \n+    def test_not_iterable(self):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = 10\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.inlines' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"inlines\" must be a list or tuple.',\n+            'admin.E103')\n \n+    def test_not_model_admin(self):\n         class ValidationTestInline(object):\n             pass\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.inlines\\[0\\]' does not inherit from BaseModelAdmin.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"inlines[0]\" must inherit from BaseModelAdmin.',\n+            'admin.E104')\n \n+    def test_missing_model_field(self):\n         class ValidationTestInline(TabularInline):\n             pass\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'model' is a required attribute of 'ValidationTestModelAdmin.inlines\\[0\\]'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"model\" is a required attribute of \"inlines[0]\".',\n+            'admin.E105')\n+\n+    def test_invalid_model_type(self):\n+        \"\"\" Test if `model` attribute on inline model admin is a models.Model.\n+        \"\"\"\n \n         class SomethingBad(object):\n             pass\n@@ -1284,39 +1326,24 @@ class ValidationTestInline(TabularInline):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestModelAdmin.inlines\\[0\\].model' does not inherit from models.Model.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"inlines[0].model\" must be a Model.',\n+            'admin.E106')\n \n+    def test_valid_case(self):\n         class ValidationTestInline(TabularInline):\n             model = ValidationTestInlineModel\n \n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_fields_validation(self):\n \n-        class ValidationTestInline(TabularInline):\n-            model = ValidationTestInlineModel\n-            fields = 10\n-\n-        class ValidationTestModelAdmin(ModelAdmin):\n-            inlines = [ValidationTestInline]\n-\n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestInline.fields' must be a list or tuple.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n-\n-    def test_fk_name_validation(self):\n+class FkNameCheckTests(CheckTestCase):\n \n+    def test_missing_field(self):\n         class ValidationTestInline(TabularInline):\n             model = ValidationTestInlineModel\n             fk_name = \"non_existent_field\"\n@@ -1324,13 +1351,14 @@ class ValidationTestInline(TabularInline):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestInline.fk_name' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestInlineModel'.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"fk_name\" refers to \"non_existent_field\" field, which '\n+                'is missing from model modeladmin.ValidationTestInlineModel.',\n+            'admin.E202',\n+            invalid_obj=ValidationTestInline)\n \n+    def test_valid_case(self):\n         class ValidationTestInline(TabularInline):\n             model = ValidationTestInlineModel\n             fk_name = \"parent\"\n@@ -1338,10 +1366,12 @@ class ValidationTestInline(TabularInline):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n \n-    def test_extra_validation(self):\n+class ExtraCheckTests(CheckTestCase):\n \n+    def test_not_integer(self):\n         class ValidationTestInline(TabularInline):\n             model = ValidationTestInlineModel\n             extra = \"hello\"\n@@ -1349,13 +1379,13 @@ class ValidationTestInline(TabularInline):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestInline.extra' should be a int.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"extra\" must be an integer.',\n+            'admin.E203',\n+            invalid_obj=ValidationTestInline)\n \n+    def test_valid_case(self):\n         class ValidationTestInline(TabularInline):\n             model = ValidationTestInlineModel\n             extra = 2\n@@ -1363,10 +1393,12 @@ class ValidationTestInline(TabularInline):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_max_num_validation(self):\n \n+class MaxNumCheckTests(CheckTestCase):\n+\n+    def test_not_integer(self):\n         class ValidationTestInline(TabularInline):\n             model = ValidationTestInlineModel\n             max_num = \"hello\"\n@@ -1374,13 +1406,13 @@ class ValidationTestInline(TabularInline):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestInline.max_num' should be a int.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"max_num\" must be an integer.',\n+            'admin.E204',\n+            invalid_obj=ValidationTestInline)\n \n+    def test_valid_case(self):\n         class ValidationTestInline(TabularInline):\n             model = ValidationTestInlineModel\n             max_num = 2\n@@ -1388,9 +1420,12 @@ class ValidationTestInline(TabularInline):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n \n-    def test_formset_validation(self):\n+\n+class FormsetCheckTests(CheckTestCase):\n+\n+    def test_invalid_type(self):\n \n         class FakeFormSet(object):\n             pass\n@@ -1402,13 +1437,13 @@ class ValidationTestInline(TabularInline):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        six.assertRaisesRegex(self,\n-            ImproperlyConfigured,\n-            \"'ValidationTestInline.formset' does not inherit from BaseModelFormSet.\",\n-            ValidationTestModelAdmin.validate,\n-            ValidationTestModel,\n-        )\n+        self.assertIsInvalid(\n+            ValidationTestModelAdmin, ValidationTestModel,\n+            '\"formset\" must inherit from BaseModelFormSet.',\n+            'admin.E205',\n+            invalid_obj=ValidationTestInline)\n \n+    def test_valid_case(self):\n         class RealModelFormSet(BaseModelFormSet):\n             pass\n \n@@ -1419,4 +1454,18 @@ class ValidationTestInline(TabularInline):\n         class ValidationTestModelAdmin(ModelAdmin):\n             inlines = [ValidationTestInline]\n \n-        ValidationTestModelAdmin.validate(ValidationTestModel)\n+        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n+\n+\n+class CustomModelAdminTests(CheckTestCase):\n+\n+    def test_deprecation(self):\n+\n+        class CustomValidator(ModelAdminValidator):\n+            def validate_me(self, model_admin, model):\n+                raise ImproperlyConfigured('error!')\n+\n+        class CustomModelAdmin(ModelAdmin):\n+            validator = CustomValidator\n+\n+        self.assertIsInvalid(CustomModelAdmin, ValidationTestModel, 'error!')\ndiff --git a/tests/proxy_model_inheritance/tests.py b/tests/proxy_model_inheritance/tests.py\nindex 9941506303b4..edbf7ffbf810 100644\n--- a/tests/proxy_model_inheritance/tests.py\n+++ b/tests/proxy_model_inheritance/tests.py\n@@ -4,10 +4,11 @@\n import sys\n \n from django.conf import settings\n+from django.core.checks.registration import framework as check_framework\n from django.core.management import call_command\n from django.db.models.loading import cache, load_app\n from django.test import TestCase, TransactionTestCase\n-from django.test.utils import override_settings\n+from django.test.utils import override_settings, override_system_checks\n from django.utils._os import upath\n \n from .models import (ConcreteModel, ConcreteModelSubclass,\n@@ -39,6 +40,9 @@ def tearDown(self):\n         del cache.app_models['app1']\n         del cache.app_models['app2']\n \n+    # `auth` app is imported, but not installed in this test, so we need to\n+    # exclude checks registered by this app.\n+    @override_system_checks([])\n     def test_table_exists(self):\n         try:\n             cache.set_available_apps(settings.INSTALLED_APPS)\ndiff --git a/tests/sites_framework/models.py b/tests/sites_framework/models.py\nindex 55c4f4992e2a..334acc6b5c7c 100644\n--- a/tests/sites_framework/models.py\n+++ b/tests/sites_framework/models.py\n@@ -27,12 +27,3 @@ class CustomArticle(AbstractArticle):\n \n     objects = models.Manager()\n     on_site = CurrentSiteManager(\"places_this_article_should_appear\")\n-\n-class InvalidArticle(AbstractArticle):\n-    site = models.ForeignKey(Site)\n-\n-    objects = models.Manager()\n-    on_site = CurrentSiteManager(\"places_this_article_should_appear\")\n-\n-class ConfusedArticle(AbstractArticle):\n-    site = models.IntegerField()\ndiff --git a/tests/sites_framework/tests.py b/tests/sites_framework/tests.py\nindex 7860394aa2c7..6e030752fd37 100644\n--- a/tests/sites_framework/tests.py\n+++ b/tests/sites_framework/tests.py\n@@ -1,18 +1,35 @@\n from django.conf import settings\n+from django.contrib.sites.managers import CurrentSiteManager\n from django.contrib.sites.models import Site\n+from django.core import checks\n+from django.db.models.loading import cache\n from django.test import TestCase\n \n from .models import (SyndicatedArticle, ExclusiveArticle, CustomArticle,\n-    InvalidArticle, ConfusedArticle)\n+    AbstractArticle)\n \n \n class SitesFrameworkTestCase(TestCase):\n     def setUp(self):\n-        Site.objects.get_or_create(id=settings.SITE_ID, domain=\"example.com\", name=\"example.com\")\n-        Site.objects.create(id=settings.SITE_ID+1, domain=\"example2.com\", name=\"example2.com\")\n+        Site.objects.get_or_create(id=settings.SITE_ID,\n+            domain=\"example.com\", name=\"example.com\")\n+        Site.objects.create(id=settings.SITE_ID+1,\n+            domain=\"example2.com\", name=\"example2.com\")\n+        self._clear_app_cache()\n+\n+    def tearDown(self):\n+        self._clear_app_cache()\n+\n+    def _clear_app_cache(self):\n+        # If you create a model in a test, the model is accessible in other\n+        # tests. To avoid this, we need to clear list of all models created in\n+        # `sites_framework` module.\n+        cache.app_models['sites_framework'] = {}\n+        cache._get_models_cache = {}\n \n     def test_site_fk(self):\n-        article = ExclusiveArticle.objects.create(title=\"Breaking News!\", site_id=settings.SITE_ID)\n+        article = ExclusiveArticle.objects.create(title=\"Breaking News!\",\n+            site_id=settings.SITE_ID)\n         self.assertEqual(ExclusiveArticle.on_site.all().get(), article)\n \n     def test_sites_m2m(self):\n@@ -24,13 +41,46 @@ def test_sites_m2m(self):\n         self.assertEqual(SyndicatedArticle.on_site.all().get(), article)\n \n     def test_custom_named_field(self):\n-        article = CustomArticle.objects.create(title=\"Tantalizing News!\", places_this_article_should_appear_id=settings.SITE_ID)\n+        article = CustomArticle.objects.create(title=\"Tantalizing News!\",\n+            places_this_article_should_appear_id=settings.SITE_ID)\n         self.assertEqual(CustomArticle.on_site.all().get(), article)\n \n     def test_invalid_name(self):\n-        article = InvalidArticle.objects.create(title=\"Bad News!\", site_id=settings.SITE_ID)\n-        self.assertRaises(ValueError, InvalidArticle.on_site.all)\n+        from django.db import models\n+\n+        class InvalidArticle(AbstractArticle):\n+            site = models.ForeignKey(Site)\n+\n+            objects = models.Manager()\n+            on_site = CurrentSiteManager(\"places_this_article_should_appear\")\n+\n+        errors = InvalidArticle.check()\n+        expected = [\n+            checks.Error(\n+                'CurrentSiteManager could not find a field named '\n+                    '\"places_this_article_should_appear\".',\n+                hint='Ensure that you did not misspell the field name. '\n+                    'Does the field exist?',\n+                obj=InvalidArticle.on_site,\n+                id='sites.E001',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\n \n     def test_invalid_field_type(self):\n-        article = ConfusedArticle.objects.create(title=\"More Bad News!\", site=settings.SITE_ID)\n-        self.assertRaises(TypeError, ConfusedArticle.on_site.all)\n+        from django.db import models\n+\n+        class ConfusedArticle(AbstractArticle):\n+            site = models.IntegerField()\n+\n+        errors = ConfusedArticle.check()\n+        expected = [\n+            checks.Error(\n+                'ConfusedArticle.site is used by a CurrentSiteManager '\n+                    'and must be a ForeignKey or ManyToManyField.',\n+                hint=None,\n+                obj=ConfusedArticle.on_site,\n+                id='sites.E002',\n+            )\n+        ]\n+        self.assertEqual(errors, expected)\ndiff --git a/tests/test_runner/tests.py b/tests/test_runner/tests.py\nindex 00b5e9c978ce..d7f784a4e45d 100644\n--- a/tests/test_runner/tests.py\n+++ b/tests/test_runner/tests.py\n@@ -5,7 +5,6 @@\n \n from importlib import import_module\n from optparse import make_option\n-import sys\n import unittest\n \n from django.core.exceptions import ImproperlyConfigured\n@@ -13,7 +12,7 @@\n from django import db\n from django.test import runner, TestCase, TransactionTestCase, skipUnlessDBFeature\n from django.test.testcases import connections_support_transactions\n-from django.test.utils import IgnoreAllDeprecationWarningsMixin\n+from django.test.utils import IgnoreAllDeprecationWarningsMixin, override_system_checks\n \n from admin_scripts.tests import AdminScriptTestCase\n from .models import Person\n@@ -247,6 +246,9 @@ class Sqlite3InMemoryTestDbs(TestCase):\n \n     @unittest.skipUnless(all(db.connections[conn].vendor == 'sqlite' for conn in db.connections),\n                          \"This is an sqlite-specific issue\")\n+    # `setup_databases` triggers system check framework, but we do not want to\n+    # perform checks.\n+    @override_system_checks([])\n     def test_transaction_support(self):\n         \"\"\"Ticket #16329: sqlite3 in-memory test databases\"\"\"\n         old_db_connections = db.connections\ndiff --git a/tests/test_sqlite.py b/tests/test_sqlite.py\nindex 6bce452c79c0..3e846376f918 100644\n--- a/tests/test_sqlite.py\n+++ b/tests/test_sqlite.py\n@@ -27,3 +27,5 @@\n PASSWORD_HASHERS = (\n     'django.contrib.auth.hashers.MD5PasswordHasher',\n )\n+\n+TEST_RUNNER = \"django.test.runner.DiscoverRunner\"\ndiff --git a/tests/user_commands/management/commands/dance.py b/tests/user_commands/management/commands/dance.py\nindex 911530d223d8..7297568f0603 100644\n--- a/tests/user_commands/management/commands/dance.py\n+++ b/tests/user_commands/management/commands/dance.py\n@@ -6,7 +6,7 @@\n class Command(BaseCommand):\n     help = \"Dance around like a madman.\"\n     args = ''\n-    requires_model_validation = True\n+    requires_system_checks = True\n \n     option_list = BaseCommand.option_list + (\n         make_option(\"-s\", \"--style\", default=\"Rock'n'Roll\"),\ndiff --git a/tests/validation/test_error_messages.py b/tests/validation/test_error_messages.py\nindex aa01db6007a6..be41cdd07a2d 100644\n--- a/tests/validation/test_error_messages.py\n+++ b/tests/validation/test_error_messages.py\n@@ -19,10 +19,6 @@ def test_autofield_field_raises_error_message(self):\n         f = models.AutoField(primary_key=True)\n         self._test_validation_messages(f, 'fo',\n             [\"'fo' value must be an integer.\"])\n-        # primary_key must be True. Refs #12467.\n-        with six.assertRaisesRegex(self, AssertionError,\n-                \"AutoFields must have primary_key=True.\"):\n-            models.AutoField(primary_key=False)\n \n     def test_integer_field_raises_error_message(self):\n         f = models.IntegerField()\n"
  },
  {
    "index": 24,
    "filtered_comments": [
      "Thank you for your patience, @JunyiJ. This is coming together well now.\r\n\r\nThere are a couple of things that I can think of outstanding.\r\n\r\n1. As suggested by @felixxm, we can now remove `django_power`:\r\n    ```diff\r\n    diff --git a/django/db/backends/sqlite3/base.py b/django/db/backends/sqlite3/base.py\r\n    index a9753bb094..07b153cd22 100644\r\n    --- a/django/db/backends/sqlite3/base.py\r\n    +++ b/django/db/backends/sqlite3/base.py\r\n    @@ -6,2 +6,3 @@ import decimal\r\n     import math\r\n    +import operator\r\n     import re\r\n    @@ -170,3 +171,2 @@ class DatabaseWrapper(BaseDatabaseWrapper):\r\n             conn.create_function(\"django_format_dtdelta\", 3, _sqlite_format_dtdelta)\r\n    -        conn.create_function(\"django_power\", 2, _sqlite_power)\r\n             conn.create_function('LPAD', 3, _sqlite_lpad)\r\n    @@ -187,3 +187,3 @@ class DatabaseWrapper(BaseDatabaseWrapper):\r\n             conn.create_function('PI', 0, lambda: math.pi)\r\n    -        conn.create_function('POWER', 2, _sqlite_power)\r\n    +        conn.create_function('POWER', 2, operator.pow)\r\n             conn.create_function('RADIANS', 1, math.radians)\r\n    @@ -498,5 +498 @@ def _sqlite_rpad(text, length, fill_text):\r\n         return (text + fill_text * length)[:length]\r\n    -\r\n    -\r\n    -def _sqlite_power(x, y):\r\n    -    return x ** y\r\n    diff --git a/django/db/backends/sqlite3/operations.py b/django/db/backends/sqlite3/operations.py\r\n    index 10b064d966..65796ea8a2 100644\r\n    --- a/django/db/backends/sqlite3/operations.py\r\n    +++ b/django/db/backends/sqlite3/operations.py\r\n    @@ -274,6 +274,6 @@ class DatabaseOperations(BaseDatabaseOperations):\r\n         def combine_expression(self, connector, sub_expressions):\r\n    -        # SQLite doesn't have a power function, so we fake it with a\r\n    -        # user-defined function django_power that's registered in connect().\r\n    +        # SQLite doesn't have a POWER function, so we fake it with a\r\n    +        # user-defined function that's registered in connect().\r\n             if connector == '^':\r\n    -            return 'django_power(%s)' % ','.join(sub_expressions)\r\n    +            return 'POWER(%s)' % ','.join(sub_expressions)\r\n             return super().combine_expression(connector, sub_expressions)\r\n    ```\r\n\r\n2. I raised the issue of input values being `DecimalField` and coming out as `FloatField`.\r\n\r\n    The functions `ACos`, `ASin`, `ATan`, `ATan2`, `Cos`, `Cot`, `Degrees`, `Log`, `Mod`, `Pi`, `Power`, `Radians`, `Sin`, `Sqrt` and `Tan` all declare `output_field = FloatField()`.\r\n\r\n    Firstly, this doesn't seem consistent as some functions are missing this, e.g. `Ln`. It makes sense that `IntegerField` becomes `FloatField`, but I'd expect `DecimalField` to stay `DecimalField`.\r\n\r\n    One way this could be achieved would be via a hack similar to this:\r\n   https://github.com/django/django/blob/d549b8805053d4b064bf492ba90e90db5d7e2a6b/django/db/models/functions/window.py#L49-L51\r\n    But as a mixin so that it can be added easily to each of the math functions that require it:\r\n    ```python\r\n    class MathOutputFieldMixin:\r\n        def _resolve_output_field(self):\r\n            sources = self.get_source_expressions()\r\n            if any(isinstance(s.output_field, DecimalField) for s in sources):\r\n                return DecimalField()\r\n            else:\r\n                return FloatField()\r\n    ```\r\n    I'd like some opinion on this from @felixxm and @timgraham before you spend time implementing it.",
      "Hi @pope1ni, I revised the code based on your suggestions except the output_field(Though I think it is a great idea to use output_field_reolved, I will wait to see whether there is different opinion). There are several issues that I noticed:\r\n1) For the log(a,b)function, different database behaves differently. Some use log(base, num) and some use log(num, base).\r\n2) Some data base doesn't support mod(double precision, double precision). I guess that is the reason why I didn't include the tests on float for the mod function.\r\n3) Some data base doesn't support log(double precision, double precision). I guess that is the reason why I didn't include the tests on float for the mod function.\r\n\r\nFor cases 2) and 3) Do you think it is OK to remove the tests  on float?\r\n\r\nThanks,\r\nJunyi",
      "Hi @JunyiJ,\r\n\r\nI've made a number of fixes that you can pull into your branch @ https://github.com/JunyiJ/django/pull/1:\r\n\r\n- Simplified the type casting for `Log()` and `Mod()` on PostgreSQL.\r\n  *(The issue was that a value is needed for `max_digits` and `decimal_places`.)*\r\n- We no longer cast `DecimalField` to `FloatField` for `output_field`.\r\n- Added type assertions to tests to complement the `output_field` stuff.\r\n- Added missing tests for `Round()`.\r\n- Fixed the problem with `Log()` on the SpatiaLite backend.\r\n- Various other bits of tidying in the tests.\r\n\r\nI've noted that `ATAN2` was added to SpatiaLite v4.3.0 which also seems to exhibit problems but I think Django's Jenkins currently uses < v4.3.0, so `math.atan2` is still being used.\r\n\r\nSee http://www.gaia-gis.it/gaia-sins/spatialite-sql-4.3.0.html#math\r\n\r\nBug filed for Log() on SpatiaLite: https://www.gaia-gis.it/fossil/libspatialite/tktview?name=8f59ddebf0",
      "@pope1ni In that case, we can either loose the test case criteria to pass the tests or change the Round function in math.py as what you did to Log. What do you think is a better way?",
      "> In that case, we can either loose the test case criteria to pass the tests...\r\n\r\nNo, we shouldn't do this. The behaviour of spatialite is incorrect. Tests should test correct behaviour, not be made lax to satisfy broken implementations. Could you revert that change?\r\n\r\nI'll do some more digging over the next couple of days and try and come up with a correct solution. I have a few ideas."
    ],
    "code_diff": "diff --git a/django/db/backends/sqlite3/base.py b/django/db/backends/sqlite3/base.py\nindex 7951143fd097..20905c83b85f 100644\n--- a/django/db/backends/sqlite3/base.py\n+++ b/django/db/backends/sqlite3/base.py\n@@ -173,10 +173,28 @@ def get_new_connection(self, conn_params):\n         conn.create_function(\"django_timestamp_diff\", 2, _sqlite_timestamp_diff)\n         conn.create_function(\"regexp\", 2, _sqlite_regexp)\n         conn.create_function(\"django_format_dtdelta\", 3, _sqlite_format_dtdelta)\n-        conn.create_function(\"django_power\", 2, _sqlite_power)\n         conn.create_function('LPAD', 3, _sqlite_lpad)\n         conn.create_function('REPEAT', 2, operator.mul)\n         conn.create_function('RPAD', 3, _sqlite_rpad)\n+        conn.create_function('ACOS', 1, math.acos)\n+        conn.create_function('ASIN', 1, math.asin)\n+        conn.create_function('ATAN', 1, math.atan)\n+        conn.create_function('ATAN2', 2, math.atan2)\n+        conn.create_function('CEILING', 1, math.ceil)\n+        conn.create_function('COS', 1, math.cos)\n+        conn.create_function('COT', 1, lambda x: 1 / math.tan(x))\n+        conn.create_function('DEGREES', 1, math.degrees)\n+        conn.create_function('EXP', 1, math.exp)\n+        conn.create_function('FLOOR', 1, math.floor)\n+        conn.create_function('LN', 1, math.log)\n+        conn.create_function('LOG', 2, lambda x, y: math.log(y, x))\n+        conn.create_function('MOD', 2, math.fmod)\n+        conn.create_function('PI', 0, lambda: math.pi)\n+        conn.create_function('POWER', 2, operator.pow)\n+        conn.create_function('RADIANS', 1, math.radians)\n+        conn.create_function('SIN', 1, math.sin)\n+        conn.create_function('SQRT', 1, math.sqrt)\n+        conn.create_function('TAN', 1, math.tan)\n         conn.execute('PRAGMA foreign_keys = ON')\n         return conn\n \n@@ -483,7 +501,3 @@ def _sqlite_lpad(text, length, fill_text):\n \n def _sqlite_rpad(text, length, fill_text):\n     return (text + fill_text * length)[:length]\n-\n-\n-def _sqlite_power(x, y):\n-    return x ** y\ndiff --git a/django/db/backends/sqlite3/operations.py b/django/db/backends/sqlite3/operations.py\nindex ee197d34b4c3..0ad95eb479cf 100644\n--- a/django/db/backends/sqlite3/operations.py\n+++ b/django/db/backends/sqlite3/operations.py\n@@ -273,10 +273,10 @@ def bulk_insert_sql(self, fields, placeholder_rows):\n         )\n \n     def combine_expression(self, connector, sub_expressions):\n-        # SQLite doesn't have a power function, so we fake it with a\n-        # user-defined function django_power that's registered in connect().\n+        # SQLite doesn't have a ^ operator, so use the user-defined POWER\n+        # function that's registered in connect().\n         if connector == '^':\n-            return 'django_power(%s)' % ','.join(sub_expressions)\n+            return 'POWER(%s)' % ','.join(sub_expressions)\n         return super().combine_expression(connector, sub_expressions)\n \n     def combine_duration_expression(self, connector, sub_expressions):\ndiff --git a/django/db/models/functions/__init__.py b/django/db/models/functions/__init__.py\nindex 7515cc67d902..5f5a8bd1dc5f 100644\n--- a/django/db/models/functions/__init__.py\n+++ b/django/db/models/functions/__init__.py\n@@ -5,6 +5,10 @@\n     Now, Trunc, TruncDate, TruncDay, TruncHour, TruncMinute, TruncMonth,\n     TruncQuarter, TruncSecond, TruncTime, TruncWeek, TruncYear,\n )\n+from .math import (\n+    Abs, ACos, ASin, ATan, ATan2, Ceil, Cos, Cot, Degrees, Exp, Floor, Ln, Log,\n+    Mod, Pi, Power, Radians, Round, Sin, Sqrt, Tan,\n+)\n from .text import (\n     Chr, Concat, ConcatPair, Left, Length, Lower, LPad, LTrim, Ord, Repeat,\n     Replace, Right, RPad, RTrim, StrIndex, Substr, Trim, Upper,\n@@ -23,6 +27,10 @@\n     'ExtractYear', 'Now', 'Trunc', 'TruncDate', 'TruncDay', 'TruncHour',\n     'TruncMinute', 'TruncMonth', 'TruncQuarter', 'TruncSecond', 'TruncTime',\n     'TruncWeek', 'TruncYear',\n+    # math\n+    'Abs', 'ACos', 'ASin', 'ATan', 'ATan2', 'Ceil', 'Cos', 'Cot', 'Degrees',\n+    'Exp', 'Floor', 'Ln', 'Log', 'Mod', 'Pi', 'Power', 'Radians', 'Round',\n+    'Sin', 'Sqrt', 'Tan',\n     # text\n     'Chr', 'Concat', 'ConcatPair', 'Left', 'Length', 'Lower', 'LPad', 'LTrim',\n     'Ord', 'Repeat', 'Replace', 'Right', 'RPad', 'RTrim', 'StrIndex', 'Substr',\ndiff --git a/django/db/models/functions/math.py b/django/db/models/functions/math.py\nnew file mode 100644\nindex 000000000000..6341e101a6a4\n--- /dev/null\n+++ b/django/db/models/functions/math.py\n@@ -0,0 +1,174 @@\n+import math\n+import sys\n+\n+from django.db.models import (\n+    DecimalField, FloatField, Func, IntegerField, Transform,\n+)\n+from django.db.models.functions import Cast\n+\n+\n+class DecimalInputMixin:\n+\n+    def as_postgresql(self, compiler, connection):\n+        # Cast FloatField to DecimalField as PostgreSQL doesn't support the\n+        # following function signatures:\n+        # - LOG(double, double)\n+        # - MOD(double, double)\n+        output_field = DecimalField(decimal_places=sys.float_info.dig, max_digits=1000)\n+        clone = self.copy()\n+        clone.set_source_expressions([\n+            Cast(expression, output_field) if isinstance(expression.output_field, FloatField)\n+            else expression for expression in self.get_source_expressions()\n+        ])\n+        return clone.as_sql(compiler, connection)\n+\n+\n+class OutputFieldMixin:\n+\n+    def _resolve_output_field(self):\n+        has_decimals = any(isinstance(s.output_field, DecimalField) for s in self.get_source_expressions())\n+        return DecimalField() if has_decimals else FloatField()\n+\n+\n+class Abs(Transform):\n+    function = 'ABS'\n+    lookup_name = 'abs'\n+\n+\n+class ACos(OutputFieldMixin, Transform):\n+    function = 'ACOS'\n+    lookup_name = 'acos'\n+\n+\n+class ASin(OutputFieldMixin, Transform):\n+    function = 'ASIN'\n+    lookup_name = 'asin'\n+\n+\n+class ATan(OutputFieldMixin, Transform):\n+    function = 'ATAN'\n+    lookup_name = 'atan'\n+\n+\n+class ATan2(OutputFieldMixin, Func):\n+    function = 'ATAN2'\n+    arity = 2\n+\n+    def as_sqlite(self, compiler, connection):\n+        if not getattr(connection.ops, 'spatialite', False) or connection.ops.spatial_version < (4, 3, 0):\n+            return self.as_sql(compiler, connection)\n+        # This function is usually ATan2(y, x), returning the inverse tangent\n+        # of y / x, but it's ATan2(x, y) on SpatiaLite 4.3+.\n+        # Cast integers to float to avoid inconsistent/buggy behavior if the\n+        # arguments are mixed between integer and float or decimal.\n+        # https://www.gaia-gis.it/fossil/libspatialite/tktview?name=0f72cca3a2\n+        clone = self.copy()\n+        clone.set_source_expressions([\n+            Cast(expression, FloatField()) if isinstance(expression.output_field, IntegerField)\n+            else expression for expression in self.get_source_expressions()[::-1]\n+        ])\n+        return clone.as_sql(compiler, connection)\n+\n+\n+class Ceil(Transform):\n+    function = 'CEILING'\n+    lookup_name = 'ceil'\n+\n+    def as_oracle(self, compiler, connection):\n+        return super().as_sql(compiler, connection, function='CEIL')\n+\n+\n+class Cos(OutputFieldMixin, Transform):\n+    function = 'COS'\n+    lookup_name = 'cos'\n+\n+\n+class Cot(OutputFieldMixin, Transform):\n+    function = 'COT'\n+    lookup_name = 'cot'\n+\n+    def as_oracle(self, compiler, connection):\n+        return super().as_sql(compiler, connection, template='(1 / TAN(%(expressions)s))')\n+\n+\n+class Degrees(OutputFieldMixin, Transform):\n+    function = 'DEGREES'\n+    lookup_name = 'degrees'\n+\n+    def as_oracle(self, compiler, connection):\n+        return super().as_sql(compiler, connection, template='((%%(expressions)s) * 180 / %s)' % math.pi)\n+\n+\n+class Exp(OutputFieldMixin, Transform):\n+    function = 'EXP'\n+    lookup_name = 'exp'\n+\n+\n+class Floor(Transform):\n+    function = 'FLOOR'\n+    lookup_name = 'floor'\n+\n+\n+class Ln(OutputFieldMixin, Transform):\n+    function = 'LN'\n+    lookup_name = 'ln'\n+\n+\n+class Log(DecimalInputMixin, OutputFieldMixin, Func):\n+    function = 'LOG'\n+    arity = 2\n+\n+    def as_sqlite(self, compiler, connection):\n+        if not getattr(connection.ops, 'spatialite', False):\n+            return self.as_sql(compiler, connection)\n+        # This function is usually Log(b, x) returning the logarithm of x to\n+        # the base b, but on SpatiaLite it's Log(x, b).\n+        clone = self.copy()\n+        clone.set_source_expressions(self.get_source_expressions()[::-1])\n+        return clone.as_sql(compiler, connection)\n+\n+\n+class Mod(DecimalInputMixin, OutputFieldMixin, Func):\n+    function = 'MOD'\n+    arity = 2\n+\n+\n+class Pi(OutputFieldMixin, Func):\n+    function = 'PI'\n+    arity = 0\n+\n+    def as_oracle(self, compiler, connection):\n+        return super().as_sql(compiler, connection, template=str(math.pi))\n+\n+\n+class Power(OutputFieldMixin, Func):\n+    function = 'POWER'\n+    arity = 2\n+\n+\n+class Radians(OutputFieldMixin, Transform):\n+    function = 'RADIANS'\n+    lookup_name = 'radians'\n+\n+    def as_oracle(self, compiler, connection):\n+        return super().as_sql(compiler, connection, template='((%%(expressions)s) * %s / 180)' % math.pi)\n+\n+\n+class Round(Transform):\n+    function = 'ROUND'\n+    lookup_name = 'round'\n+\n+\n+class Sin(OutputFieldMixin, Transform):\n+    function = 'SIN'\n+    lookup_name = 'sin'\n+\n+\n+class Sqrt(OutputFieldMixin, Transform):\n+    function = 'SQRT'\n+    lookup_name = 'sqrt'\n+\n+\n+class Tan(OutputFieldMixin, Transform):\n+    function = 'TAN'\n+    lookup_name = 'tan'\ndiff --git a/docs/ref/models/database-functions.txt b/docs/ref/models/database-functions.txt\nindex d39ed5001b2e..be5769148051 100644\n--- a/docs/ref/models/database-functions.txt\n+++ b/docs/ref/models/database-functions.txt\n@@ -673,6 +673,463 @@ that deal with time-parts can be used with ``TimeField``::\n     2014-06-16 00:00:00+10:00 2\n     2016-01-01 04:00:00+11:00 1\n \n+.. _math-functions:\n+\n+Math Functions\n+==============\n+\n+.. versionadded:: 2.2\n+\n+We'll be using the following model in math function examples::\n+\n+    class Vector(models.Model):\n+        x = models.FloatField()\n+        y = models.FloatField()\n+\n+``Abs``\n+-------\n+\n+.. class:: Abs(expression, **extra)\n+\n+Returns the absolute value of a numeric field or expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Abs\n+    >>> Vector.objects.create(x=-0.5, y=1.1)\n+    >>> vector = Vector.objects.annotate(x_abs=Abs('x'), y_abs=Abs('y')).get()\n+    >>> vector.x_abs, vector.y_abs\n+    (0.5, 1.1)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Abs\n+    >>> FloatField.register_lookup(Abs)\n+    >>> # Get vectors inside the unit cube\n+    >>> vectors = Vector.objects.filter(x__abs__lt=1, y__abs__lt=1)\n+\n+``ACos``\n+--------\n+\n+.. class:: ACos(expression, **extra)\n+\n+Returns the arccosine of a numeric field or expression. The expression value\n+must be within the range -1 to 1.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import ACos\n+    >>> Vector.objects.create(x=0.5, y=-0.9)\n+    >>> vector = Vector.objects.annotate(x_acos=ACos('x'), y_acos=ACos('y')).get()\n+    >>> vector.x_acos, vector.y_acos\n+    (1.0471975511965979, 2.6905658417935308)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import ACos\n+    >>> FloatField.register_lookup(ACos)\n+    >>> # Get vectors whose arccosine is less than 1\n+    >>> vectors = Vector.objects.filter(x__acos__lt=1, y__acos__lt=1)\n+\n+``ASin``\n+--------\n+\n+.. class:: ASin(expression, **extra)\n+\n+Returns the arcsine of a numeric field or expression. The expression value must\n+be in the range -1 to 1.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import ASin\n+    >>> Vector.objects.create(x=0, y=1)\n+    >>> vector = Vector.objects.annotate(x_asin=ASin('x'), y_asin=ASin('y')).get()\n+    >>> vector.x_asin, vector.y_asin\n+    (0.0, 1.5707963267948966)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import ASin\n+    >>> FloatField.register_lookup(ASin)\n+    >>> # Get vectors whose arcsine is less than 1\n+    >>> vectors = Vector.objects.filter(x__asin__lt=1, y__asin__lt=1)\n+\n+``ATan``\n+--------\n+\n+.. class:: ATan(expression, **extra)\n+\n+Returns the arctangent of a numeric field or expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import ATan\n+    >>> Vector.objects.create(x=3.12, y=6.987)\n+    >>> vector = Vector.objects.annotate(x_atan=ATan('x'), y_atan=ATan('y')).get()\n+    >>> vector.x_atan, vector.y_atan\n+    (1.2606282660069106, 1.428638798133829)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import ATan\n+    >>> FloatField.register_lookup(ATan)\n+    >>> # Get vectors whose arctangent is less than 2\n+    >>> vectors = Vector.objects.filter(x__atan__lt=2, y__atan__lt=2)\n+\n+``ATan2``\n+---------\n+\n+.. class:: ATan2(expression1, expression2, **extra)\n+\n+Returns the arctangent of ``expression1 / expression2``.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import ATan2\n+    >>> Vector.objects.create(x=2.5, y=1.9)\n+    >>> vector = Vector.objects.annotate(atan2=ATan2('x', 'y')).get()\n+    >>> vector.atan2\n+    0.9209258773829491\n+\n+``Ceil``\n+--------\n+\n+.. class:: Ceil(expression, **extra)\n+\n+Returns the smallest integer greater than or equal to a numeric field or\n+expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Ceil\n+    >>> Vector.objects.create(x=3.12, y=7.0)\n+    >>> vector = Vector.objects.annotate(x_ceil=Ceil('x'), y_ceil=Ceil('y')).get()\n+    >>> vector.x_ceil, vector.y_ceil\n+    (4.0, 7.0)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Ceil\n+    >>> FloatField.register_lookup(Ceil)\n+    >>> # Get vectors whose ceil is less than 10\n+    >>> vectors = Vector.objects.filter(x__ceil__lt=10, y__ceil__lt=10)\n+\n+``Cos``\n+-------\n+\n+.. class:: Cos(expression, **extra)\n+\n+Returns the cosine  of a numeric field or expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Cos\n+    >>> Vector.objects.create(x=-8.0, y=3.1415926)\n+    >>> vector = Vector.objects.annotate(x_cos=Cos('x'), y_cos=Cos('y')).get()\n+    >>> vector.x_cos, vector.y_cos\n+    (-0.14550003380861354, -0.9999999999999986)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Cos\n+    >>> FloatField.register_lookup(Cos)\n+    >>> # Get vectors whose cosine is less than 0.5\n+    >>> vectors = Vector.objects.filter(x__cos__lt=0.5, y__cos__lt=0.5)\n+\n+``Cot``\n+-------\n+\n+.. class:: Cot(expression, **extra)\n+\n+Returns the cotangent of a numeric field or expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Cot\n+    >>> Vector.objects.create(x=12.0, y=1.0)\n+    >>> vector = Vector.objects.annotate(x_cot=Cot('x'), y_cot=Cot('y')).get()\n+    >>> vector.x_cot, vector.y_cot\n+    (-1.5726734063976826, 0.642092615934331)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Cot\n+    >>> FloatField.register_lookup(Cot)\n+    >>> # Get vectors whose cotangent is less than 1\n+    >>> vectors = Vector.objects.filter(x__cot__lt=1, y__cot__lt=1)\n+\n+``Degrees``\n+-----------\n+\n+.. class:: Degrees(expression, **extra)\n+\n+Converts a numeric field or expression from radians to degrees.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Degrees\n+    >>> Vector.objects.create(x=-1.57, y=3.14)\n+    >>> vector = Vector.objects.annotate(x_d=Degrees('x'), y_d=Degrees('y')).get()\n+    >>> vector.x_d, vector.y_d\n+    (-89.95437383553924, 179.9087476710785)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Degrees\n+    >>> FloatField.register_lookup(Degrees)\n+    >>> # Get vectors whose degrees are less than 360\n+    >>> vectors = Vector.objects.filter(x__degrees__lt=360, y__degrees__lt=360)\n+\n+``Exp``\n+-------\n+\n+.. class:: Exp(expression, **extra)\n+\n+Returns the value of ``e`` (the natural logarithm base) raised to the power of\n+a numeric field or expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Exp\n+    >>> Vector.objects.create(x=5.4, y=-2.0)\n+    >>> vector = Vector.objects.annotate(x_exp=Exp('x'), y_exp=Exp('y')).get()\n+    >>> vector.x_exp, vector.y_exp\n+    (221.40641620418717, 0.1353352832366127)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Exp\n+    >>> FloatField.register_lookup(Exp)\n+    >>> # Get vectors whose exp() is greater than 10\n+    >>> vectors = Vector.objects.filter(x__exp__gt=10, y__exp__gt=10)\n+\n+``Floor``\n+---------\n+\n+.. class:: Floor(expression, **extra)\n+\n+Returns the largest integer value not greater than a numeric field or\n+expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Floor\n+    >>> Vector.objects.create(x=5.4, y=-2.3)\n+    >>> vector = Vector.objects.annotate(x_floor=Floor('x'), y_floor=Floor('y')).get()\n+    >>> vector.x_floor, vector.y_floor\n+    (5.0, -3.0)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Floor\n+    >>> FloatField.register_lookup(Floor)\n+    >>> # Get vectors whose floor() is greater than 10\n+    >>> vectors = Vector.objects.filter(x__floor__gt=10, y__floor__gt=10)\n+\n+``Ln``\n+------\n+\n+.. class:: Ln(expression, **extra)\n+\n+Returns the natural logarithm a numeric field or expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Ln\n+    >>> Vector.objects.create(x=5.4, y=233.0)\n+    >>> vector = Vector.objects.annotate(x_ln=Ln('x'), y_ln=Ln('y')).get()\n+    >>> vector.x_ln, vector.y_ln\n+    (1.6863989535702288, 5.4510384535657)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Ln\n+    >>> FloatField.register_lookup(Ln)\n+    >>> # Get vectors whose value greater than e\n+    >>> vectors = Vector.objects.filter(x__ln__gt=1, y__ln__gt=1)\n+\n+``Log``\n+-------\n+\n+.. class:: Log(expression1, expression2, **extra)\n+\n+Accepts two numeric fields or expressions and returns the logarithm of\n+the first to base of the second.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Log\n+    >>> Vector.objects.create(x=2.0, y=4.0)\n+    >>> vector = Vector.objects.annotate(log=Log('x', 'y')).get()\n+    >>> vector.log\n+    2.0\n+\n+``Mod``\n+-------\n+\n+.. class:: Mod(expression1, expression2, **extra)\n+\n+Accepts two numeric fields or expressions and returns the remainder of\n+the first divided by the second (modulo operation).\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Mod\n+    >>> Vector.objects.create(x=5.4, y=2.3)\n+    >>> vector = Vector.objects.annotate(mod=Mod('x', 'y')).get()\n+    >>> vector.mod\n+    0.8\n+\n+``Pi``\n+------\n+\n+.. class:: Pi(**extra)\n+\n+Returns the value of the mathematical constant ````.\n+\n+``Power``\n+---------\n+\n+.. class:: Power(expression1, expression2, **extra)\n+\n+Accepts two numeric fields or expressions and returns the value of the first\n+raised to the power of the second.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Power\n+    >>> Vector.objects.create(x=2, y=-2)\n+    >>> vector = Vector.objects.annotate(power=Power('x', 'y')).get()\n+    >>> vector.power\n+    0.25\n+\n+``Radians``\n+-----------\n+\n+.. class:: Radians(expression, **extra)\n+\n+Converts a numeric field or expression from degrees to radians.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Radians\n+    >>> Vector.objects.create(x=-90, y=180)\n+    >>> vector = Vector.objects.annotate(x_r=Radians('x'), y_r=Radians('y')).get()\n+    >>> vector.x_r, vector.y_r\n+    (-1.5707963267948966, 3.141592653589793)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Radians\n+    >>> FloatField.register_lookup(Radians)\n+    >>> # Get vectors whose radians are less than 1\n+    >>> vectors = Vector.objects.filter(x__radians__lt=1, y__radians__lt=1)\n+\n+``Round``\n+---------\n+\n+.. class:: Round(expression, **extra)\n+\n+Rounds a numeric field or expression to the nearest integer. Whether half\n+values are rounded up or down depends on the database.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Round\n+    >>> Vector.objects.create(x=5.4, y=-2.3)\n+    >>> vector = Vector.objects.annotate(x_r=Round('x'), y_r=Round('y')).get()\n+    >>> vector.x_r, vector.y_r\n+    (5.0, -2.0)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Round\n+    >>> FloatField.register_lookup(Round)\n+    >>> # Get vectors whose round() is less than 20\n+    >>> vectors = Vector.objects.filter(x__round__lt=20, y__round__lt=20)\n+\n+``Sin``\n+-------\n+\n+.. class:: Sin(expression, **extra)\n+\n+Returns the sine of a numeric field or expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Sin\n+    >>> Vector.objects.create(x=5.4, y=-2.3)\n+    >>> vector = Vector.objects.annotate(x_sin=Sin('x'), y_sin=Sin('y')).get()\n+    >>> vector.x_sin, vector.y_sin\n+    (-0.7727644875559871, -0.7457052121767203)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Sin\n+    >>> FloatField.register_lookup(Sin)\n+    >>> # Get vectors whose sin() is less than 0\n+    >>> vectors = Vector.objects.filter(x__sin__lt=0, y__sin__lt=0)\n+\n+``Sqrt``\n+--------\n+\n+.. class:: Sqrt(expression, **extra)\n+\n+Returns the square root of a nonnegative numeric field or expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Sqrt\n+    >>> Vector.objects.create(x=4.0, y=12.0)\n+    >>> vector = Vector.objects.annotate(x_sqrt=Sqrt('x'), y_sqrt=Sqrt('y')).get()\n+    >>> vector.x_sqrt, vector.y_sqrt\n+    (2.0, 3.46410)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Sqrt\n+    >>> FloatField.register_lookup(Sqrt)\n+    >>> # Get vectors whose sqrt() is less than 5\n+    >>> vectors = Vector.objects.filter(x__sqrt__lt=5, y__sqrt__lt=5)\n+\n+``Tan``\n+-------\n+\n+.. class:: Tan(expression, **extra)\n+\n+Returns the tangent of a numeric field or expression.\n+\n+Usage example::\n+\n+    >>> from django.db.models.functions import Tan\n+    >>> Vector.objects.create(x=0, y=12)\n+    >>> vector = Vector.objects.annotate(x_tan=Tan('x'), y_tan=Tan('y')).get()\n+    >>> vector.x_tan, vector.y_tan\n+    (0.0, -0.6358599286615808)\n+\n+It can also be registered as a transform. For example::\n+\n+    >>> from django.db.models import FloatField\n+    >>> from django.db.models.functions import Tan\n+    >>> FloatField.register_lookup(Tan)\n+    >>> # Get vectors whose tangent is less than 0\n+    >>> vectors = Vector.objects.filter(x__tan__lt=0, y__tan__lt=0)\n+\n .. _text-functions:\n \n Text functions\ndiff --git a/docs/releases/2.2.txt b/docs/releases/2.2.txt\nindex a68f3135e4df..86d9ee46c0d3 100644\n--- a/docs/releases/2.2.txt\n+++ b/docs/releases/2.2.txt\n@@ -166,6 +166,8 @@ Models\n \n * Added support for PostgreSQL operator classes (:attr:`.Index.opclasses`).\n \n+* Added many :ref:`math database functions <math-functions>`.\n+\n Requests and Responses\n ~~~~~~~~~~~~~~~~~~~~~~\n \ndiff --git a/docs/spelling_wordlist b/docs/spelling_wordlist\nindex e02548cfd7c2..8dae90993169 100644\n--- a/docs/spelling_wordlist\n+++ b/docs/spelling_wordlist\n@@ -19,6 +19,7 @@ app\n appname\n apps\n architected\n+arccosine\n arg\n args\n assistive\ndiff --git a/tests/db_functions/math/__init__.py b/tests/db_functions/math/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/tests/db_functions/math/test_abs.py b/tests/db_functions/math/test_abs.py\nnew file mode 100644\nindex 000000000000..99f2a336f715\n--- /dev/null\n+++ b/tests/db_functions/math/test_abs.py\n@@ -0,0 +1,50 @@\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Abs\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class AbsTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-0.8'), n2=Decimal('1.2'))\n+        obj = DecimalModel.objects.annotate(n1_abs=Abs('n1'), n2_abs=Abs('n2')).first()\n+        self.assertIsInstance(obj.n1_abs, Decimal)\n+        self.assertIsInstance(obj.n2_abs, Decimal)\n+        self.assertEqual(obj.n1, -obj.n1_abs)\n+        self.assertEqual(obj.n2, obj.n2_abs)\n+\n+    def test_float(self):\n+        obj = FloatModel.objects.create(f1=-0.5, f2=12)\n+        obj = FloatModel.objects.annotate(f1_abs=Abs('f1'), f2_abs=Abs('f2')).first()\n+        self.assertIsInstance(obj.f1_abs, float)\n+        self.assertIsInstance(obj.f2_abs, float)\n+        self.assertEqual(obj.f1, -obj.f1_abs)\n+        self.assertEqual(obj.f2, obj.f2_abs)\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=12, normal=0, big=-45)\n+        obj = IntegerModel.objects.annotate(\n+            small_abs=Abs('small'),\n+            normal_abs=Abs('normal'),\n+            big_abs=Abs('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_abs, int)\n+        self.assertIsInstance(obj.normal_abs, int)\n+        self.assertIsInstance(obj.big_abs, int)\n+        self.assertEqual(obj.small, obj.small_abs)\n+        self.assertEqual(obj.normal, obj.normal_abs)\n+        self.assertEqual(obj.big, -obj.big_abs)\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Abs)\n+            DecimalModel.objects.create(n1=Decimal('-1.5'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('-0.5'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__abs__gt=1)\n+            self.assertQuerysetEqual(objs, [Decimal('-1.5')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Abs)\ndiff --git a/tests/db_functions/math/test_acos.py b/tests/db_functions/math/test_acos.py\nnew file mode 100644\nindex 000000000000..041412123b5b\n--- /dev/null\n+++ b/tests/db_functions/math/test_acos.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import ACos\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class ACosTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-0.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_acos=ACos('n1'), n2_acos=ACos('n2')).first()\n+        self.assertIsInstance(obj.n1_acos, Decimal)\n+        self.assertIsInstance(obj.n2_acos, Decimal)\n+        self.assertAlmostEqual(obj.n1_acos, Decimal(math.acos(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_acos, Decimal(math.acos(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-0.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_acos=ACos('f1'), f2_acos=ACos('f2')).first()\n+        self.assertIsInstance(obj.f1_acos, float)\n+        self.assertIsInstance(obj.f2_acos, float)\n+        self.assertAlmostEqual(obj.f1_acos, math.acos(obj.f1))\n+        self.assertAlmostEqual(obj.f2_acos, math.acos(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=0, normal=1, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_acos=ACos('small'),\n+            normal_acos=ACos('normal'),\n+            big_acos=ACos('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_acos, float)\n+        self.assertIsInstance(obj.normal_acos, float)\n+        self.assertIsInstance(obj.big_acos, float)\n+        self.assertAlmostEqual(obj.small_acos, math.acos(obj.small))\n+        self.assertAlmostEqual(obj.normal_acos, math.acos(obj.normal))\n+        self.assertAlmostEqual(obj.big_acos, math.acos(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(ACos)\n+            DecimalModel.objects.create(n1=Decimal('0.5'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('-0.9'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__acos__lt=2)\n+            self.assertQuerysetEqual(objs, [Decimal('0.5')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(ACos)\ndiff --git a/tests/db_functions/math/test_asin.py b/tests/db_functions/math/test_asin.py\nnew file mode 100644\nindex 000000000000..55724498342d\n--- /dev/null\n+++ b/tests/db_functions/math/test_asin.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import ASin\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class ASinTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('0.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_asin=ASin('n1'), n2_asin=ASin('n2')).first()\n+        self.assertIsInstance(obj.n1_asin, Decimal)\n+        self.assertIsInstance(obj.n2_asin, Decimal)\n+        self.assertAlmostEqual(obj.n1_asin, Decimal(math.asin(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_asin, Decimal(math.asin(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-0.5, f2=0.87)\n+        obj = FloatModel.objects.annotate(f1_asin=ASin('f1'), f2_asin=ASin('f2')).first()\n+        self.assertIsInstance(obj.f1_asin, float)\n+        self.assertIsInstance(obj.f2_asin, float)\n+        self.assertAlmostEqual(obj.f1_asin, math.asin(obj.f1))\n+        self.assertAlmostEqual(obj.f2_asin, math.asin(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=0, normal=1, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_asin=ASin('small'),\n+            normal_asin=ASin('normal'),\n+            big_asin=ASin('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_asin, float)\n+        self.assertIsInstance(obj.normal_asin, float)\n+        self.assertIsInstance(obj.big_asin, float)\n+        self.assertAlmostEqual(obj.small_asin, math.asin(obj.small))\n+        self.assertAlmostEqual(obj.normal_asin, math.asin(obj.normal))\n+        self.assertAlmostEqual(obj.big_asin, math.asin(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(ASin)\n+            DecimalModel.objects.create(n1=Decimal('0.1'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('1.0'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__asin__gt=1)\n+            self.assertQuerysetEqual(objs, [Decimal('1.0')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(ASin)\ndiff --git a/tests/db_functions/math/test_atan.py b/tests/db_functions/math/test_atan.py\nnew file mode 100644\nindex 000000000000..62af5d158710\n--- /dev/null\n+++ b/tests/db_functions/math/test_atan.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import ATan\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class ATanTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_atan=ATan('n1'), n2_atan=ATan('n2')).first()\n+        self.assertIsInstance(obj.n1_atan, Decimal)\n+        self.assertIsInstance(obj.n2_atan, Decimal)\n+        self.assertAlmostEqual(obj.n1_atan, Decimal(math.atan(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_atan, Decimal(math.atan(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_atan=ATan('f1'), f2_atan=ATan('f2')).first()\n+        self.assertIsInstance(obj.f1_atan, float)\n+        self.assertIsInstance(obj.f2_atan, float)\n+        self.assertAlmostEqual(obj.f1_atan, math.atan(obj.f1))\n+        self.assertAlmostEqual(obj.f2_atan, math.atan(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-20, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_atan=ATan('small'),\n+            normal_atan=ATan('normal'),\n+            big_atan=ATan('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_atan, float)\n+        self.assertIsInstance(obj.normal_atan, float)\n+        self.assertIsInstance(obj.big_atan, float)\n+        self.assertAlmostEqual(obj.small_atan, math.atan(obj.small))\n+        self.assertAlmostEqual(obj.normal_atan, math.atan(obj.normal))\n+        self.assertAlmostEqual(obj.big_atan, math.atan(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(ATan)\n+            DecimalModel.objects.create(n1=Decimal('3.12'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('-5'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__atan__gt=0)\n+            self.assertQuerysetEqual(objs, [Decimal('3.12')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(ATan)\ndiff --git a/tests/db_functions/math/test_atan2.py b/tests/db_functions/math/test_atan2.py\nnew file mode 100644\nindex 000000000000..195892dfdd34\n--- /dev/null\n+++ b/tests/db_functions/math/test_atan2.py\n@@ -0,0 +1,33 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models.functions import ATan2\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class ATan2Tests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-9.9'), n2=Decimal('4.6'))\n+        obj = DecimalModel.objects.annotate(n_atan2=ATan2('n1', 'n2')).first()\n+        self.assertIsInstance(obj.n_atan2, Decimal)\n+        self.assertAlmostEqual(obj.n_atan2, Decimal(math.atan2(obj.n1, obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-25, f2=0.33)\n+        obj = FloatModel.objects.annotate(f_atan2=ATan2('f1', 'f2')).first()\n+        self.assertIsInstance(obj.f_atan2, float)\n+        self.assertAlmostEqual(obj.f_atan2, math.atan2(obj.f1, obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=0, normal=1, big=10)\n+        obj = IntegerModel.objects.annotate(\n+            atan2_sn=ATan2('small', 'normal'),\n+            atan2_nb=ATan2('normal', 'big'),\n+        ).first()\n+        self.assertIsInstance(obj.atan2_sn, float)\n+        self.assertIsInstance(obj.atan2_nb, float)\n+        self.assertAlmostEqual(obj.atan2_sn, math.atan2(obj.small, obj.normal))\n+        self.assertAlmostEqual(obj.atan2_nb, math.atan2(obj.normal, obj.big))\ndiff --git a/tests/db_functions/math/test_ceil.py b/tests/db_functions/math/test_ceil.py\nnew file mode 100644\nindex 000000000000..93e9713eb281\n--- /dev/null\n+++ b/tests/db_functions/math/test_ceil.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Ceil\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class CeilTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_ceil=Ceil('n1'), n2_ceil=Ceil('n2')).first()\n+        self.assertIsInstance(obj.n1_ceil, Decimal)\n+        self.assertIsInstance(obj.n2_ceil, Decimal)\n+        self.assertEqual(obj.n1_ceil, Decimal(math.ceil(obj.n1)))\n+        self.assertEqual(obj.n2_ceil, Decimal(math.ceil(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-12.5, f2=21.33)\n+        obj = FloatModel.objects.annotate(f1_ceil=Ceil('f1'), f2_ceil=Ceil('f2')).first()\n+        self.assertIsInstance(obj.f1_ceil, float)\n+        self.assertIsInstance(obj.f2_ceil, float)\n+        self.assertEqual(obj.f1_ceil, math.ceil(obj.f1))\n+        self.assertEqual(obj.f2_ceil, math.ceil(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-11, normal=0, big=-100)\n+        obj = IntegerModel.objects.annotate(\n+            small_ceil=Ceil('small'),\n+            normal_ceil=Ceil('normal'),\n+            big_ceil=Ceil('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_ceil, int)\n+        self.assertIsInstance(obj.normal_ceil, int)\n+        self.assertIsInstance(obj.big_ceil, int)\n+        self.assertEqual(obj.small_ceil, math.ceil(obj.small))\n+        self.assertEqual(obj.normal_ceil, math.ceil(obj.normal))\n+        self.assertEqual(obj.big_ceil, math.ceil(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Ceil)\n+            DecimalModel.objects.create(n1=Decimal('3.12'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('1.25'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__ceil__gt=3)\n+            self.assertQuerysetEqual(objs, [Decimal('3.12')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Ceil)\ndiff --git a/tests/db_functions/math/test_cos.py b/tests/db_functions/math/test_cos.py\nnew file mode 100644\nindex 000000000000..4d6a16f5fcb2\n--- /dev/null\n+++ b/tests/db_functions/math/test_cos.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Cos\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class CosTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_cos=Cos('n1'), n2_cos=Cos('n2')).first()\n+        self.assertIsInstance(obj.n1_cos, Decimal)\n+        self.assertIsInstance(obj.n2_cos, Decimal)\n+        self.assertAlmostEqual(obj.n1_cos, Decimal(math.cos(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_cos, Decimal(math.cos(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_cos=Cos('f1'), f2_cos=Cos('f2')).first()\n+        self.assertIsInstance(obj.f1_cos, float)\n+        self.assertIsInstance(obj.f2_cos, float)\n+        self.assertAlmostEqual(obj.f1_cos, math.cos(obj.f1))\n+        self.assertAlmostEqual(obj.f2_cos, math.cos(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-20, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_cos=Cos('small'),\n+            normal_cos=Cos('normal'),\n+            big_cos=Cos('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_cos, float)\n+        self.assertIsInstance(obj.normal_cos, float)\n+        self.assertIsInstance(obj.big_cos, float)\n+        self.assertAlmostEqual(obj.small_cos, math.cos(obj.small))\n+        self.assertAlmostEqual(obj.normal_cos, math.cos(obj.normal))\n+        self.assertAlmostEqual(obj.big_cos, math.cos(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Cos)\n+            DecimalModel.objects.create(n1=Decimal('-8.0'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('3.14'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__cos__gt=-0.2)\n+            self.assertQuerysetEqual(objs, [Decimal('-8.0')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Cos)\ndiff --git a/tests/db_functions/math/test_cot.py b/tests/db_functions/math/test_cot.py\nnew file mode 100644\nindex 000000000000..a1a13c9ee529\n--- /dev/null\n+++ b/tests/db_functions/math/test_cot.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Cot\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class CotTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_cot=Cot('n1'), n2_cot=Cot('n2')).first()\n+        self.assertIsInstance(obj.n1_cot, Decimal)\n+        self.assertIsInstance(obj.n2_cot, Decimal)\n+        self.assertAlmostEqual(obj.n1_cot, Decimal(1 / math.tan(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_cot, Decimal(1 / math.tan(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_cot=Cot('f1'), f2_cot=Cot('f2')).first()\n+        self.assertIsInstance(obj.f1_cot, float)\n+        self.assertIsInstance(obj.f2_cot, float)\n+        self.assertAlmostEqual(obj.f1_cot, 1 / math.tan(obj.f1))\n+        self.assertAlmostEqual(obj.f2_cot, 1 / math.tan(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-5, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_cot=Cot('small'),\n+            normal_cot=Cot('normal'),\n+            big_cot=Cot('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_cot, float)\n+        self.assertIsInstance(obj.normal_cot, float)\n+        self.assertIsInstance(obj.big_cot, float)\n+        self.assertAlmostEqual(obj.small_cot, 1 / math.tan(obj.small))\n+        self.assertAlmostEqual(obj.normal_cot, 1 / math.tan(obj.normal))\n+        self.assertAlmostEqual(obj.big_cot, 1 / math.tan(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Cot)\n+            DecimalModel.objects.create(n1=Decimal('12.0'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('1.0'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__cot__gt=0)\n+            self.assertQuerysetEqual(objs, [Decimal('1.0')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Cot)\ndiff --git a/tests/db_functions/math/test_degrees.py b/tests/db_functions/math/test_degrees.py\nnew file mode 100644\nindex 000000000000..d3c70fcbee7d\n--- /dev/null\n+++ b/tests/db_functions/math/test_degrees.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Degrees\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class DegreesTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_degrees=Degrees('n1'), n2_degrees=Degrees('n2')).first()\n+        self.assertIsInstance(obj.n1_degrees, Decimal)\n+        self.assertIsInstance(obj.n2_degrees, Decimal)\n+        self.assertAlmostEqual(obj.n1_degrees, Decimal(math.degrees(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_degrees, Decimal(math.degrees(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_degrees=Degrees('f1'), f2_degrees=Degrees('f2')).first()\n+        self.assertIsInstance(obj.f1_degrees, float)\n+        self.assertIsInstance(obj.f2_degrees, float)\n+        self.assertAlmostEqual(obj.f1_degrees, math.degrees(obj.f1))\n+        self.assertAlmostEqual(obj.f2_degrees, math.degrees(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-20, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_degrees=Degrees('small'),\n+            normal_degrees=Degrees('normal'),\n+            big_degrees=Degrees('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_degrees, float)\n+        self.assertIsInstance(obj.normal_degrees, float)\n+        self.assertIsInstance(obj.big_degrees, float)\n+        self.assertAlmostEqual(obj.small_degrees, math.degrees(obj.small))\n+        self.assertAlmostEqual(obj.normal_degrees, math.degrees(obj.normal))\n+        self.assertAlmostEqual(obj.big_degrees, math.degrees(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Degrees)\n+            DecimalModel.objects.create(n1=Decimal('5.4'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('-30'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__degrees__gt=0)\n+            self.assertQuerysetEqual(objs, [Decimal('5.4')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Degrees)\ndiff --git a/tests/db_functions/math/test_exp.py b/tests/db_functions/math/test_exp.py\nnew file mode 100644\nindex 000000000000..c44f7adefe3b\n--- /dev/null\n+++ b/tests/db_functions/math/test_exp.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Exp\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class ExpTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_exp=Exp('n1'), n2_exp=Exp('n2')).first()\n+        self.assertIsInstance(obj.n1_exp, Decimal)\n+        self.assertIsInstance(obj.n2_exp, Decimal)\n+        self.assertAlmostEqual(obj.n1_exp, Decimal(math.exp(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_exp, Decimal(math.exp(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_exp=Exp('f1'), f2_exp=Exp('f2')).first()\n+        self.assertIsInstance(obj.f1_exp, float)\n+        self.assertIsInstance(obj.f2_exp, float)\n+        self.assertAlmostEqual(obj.f1_exp, math.exp(obj.f1))\n+        self.assertAlmostEqual(obj.f2_exp, math.exp(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-20, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_exp=Exp('small'),\n+            normal_exp=Exp('normal'),\n+            big_exp=Exp('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_exp, float)\n+        self.assertIsInstance(obj.normal_exp, float)\n+        self.assertIsInstance(obj.big_exp, float)\n+        self.assertAlmostEqual(obj.small_exp, math.exp(obj.small))\n+        self.assertAlmostEqual(obj.normal_exp, math.exp(obj.normal))\n+        self.assertAlmostEqual(obj.big_exp, math.exp(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Exp)\n+            DecimalModel.objects.create(n1=Decimal('12.0'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('-1.0'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__exp__gt=10)\n+            self.assertQuerysetEqual(objs, [Decimal('12.0')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Exp)\ndiff --git a/tests/db_functions/math/test_floor.py b/tests/db_functions/math/test_floor.py\nnew file mode 100644\nindex 000000000000..8404184c3eda\n--- /dev/null\n+++ b/tests/db_functions/math/test_floor.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Floor\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class FloorTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_floor=Floor('n1'), n2_floor=Floor('n2')).first()\n+        self.assertIsInstance(obj.n1_floor, Decimal)\n+        self.assertIsInstance(obj.n2_floor, Decimal)\n+        self.assertEqual(obj.n1_floor, Decimal(math.floor(obj.n1)))\n+        self.assertEqual(obj.n2_floor, Decimal(math.floor(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_floor=Floor('f1'), f2_floor=Floor('f2')).first()\n+        self.assertIsInstance(obj.f1_floor, float)\n+        self.assertIsInstance(obj.f2_floor, float)\n+        self.assertEqual(obj.f1_floor, math.floor(obj.f1))\n+        self.assertEqual(obj.f2_floor, math.floor(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-20, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_floor=Floor('small'),\n+            normal_floor=Floor('normal'),\n+            big_floor=Floor('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_floor, int)\n+        self.assertIsInstance(obj.normal_floor, int)\n+        self.assertIsInstance(obj.big_floor, int)\n+        self.assertEqual(obj.small_floor, math.floor(obj.small))\n+        self.assertEqual(obj.normal_floor, math.floor(obj.normal))\n+        self.assertEqual(obj.big_floor, math.floor(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Floor)\n+            DecimalModel.objects.create(n1=Decimal('5.4'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('3.4'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__floor__gt=4)\n+            self.assertQuerysetEqual(objs, [Decimal('5.4')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Floor)\ndiff --git a/tests/db_functions/math/test_ln.py b/tests/db_functions/math/test_ln.py\nnew file mode 100644\nindex 000000000000..f30cf8e84efa\n--- /dev/null\n+++ b/tests/db_functions/math/test_ln.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Ln\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class LnTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_ln=Ln('n1'), n2_ln=Ln('n2')).first()\n+        self.assertIsInstance(obj.n1_ln, Decimal)\n+        self.assertIsInstance(obj.n2_ln, Decimal)\n+        self.assertAlmostEqual(obj.n1_ln, Decimal(math.log(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_ln, Decimal(math.log(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_ln=Ln('f1'), f2_ln=Ln('f2')).first()\n+        self.assertIsInstance(obj.f1_ln, float)\n+        self.assertIsInstance(obj.f2_ln, float)\n+        self.assertAlmostEqual(obj.f1_ln, math.log(obj.f1))\n+        self.assertAlmostEqual(obj.f2_ln, math.log(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=20, normal=15, big=1)\n+        obj = IntegerModel.objects.annotate(\n+            small_ln=Ln('small'),\n+            normal_ln=Ln('normal'),\n+            big_ln=Ln('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_ln, float)\n+        self.assertIsInstance(obj.normal_ln, float)\n+        self.assertIsInstance(obj.big_ln, float)\n+        self.assertAlmostEqual(obj.small_ln, math.log(obj.small))\n+        self.assertAlmostEqual(obj.normal_ln, math.log(obj.normal))\n+        self.assertAlmostEqual(obj.big_ln, math.log(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Ln)\n+            DecimalModel.objects.create(n1=Decimal('12.0'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('1.0'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__ln__gt=0)\n+            self.assertQuerysetEqual(objs, [Decimal('12.0')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Ln)\ndiff --git a/tests/db_functions/math/test_log.py b/tests/db_functions/math/test_log.py\nnew file mode 100644\nindex 000000000000..02cbe084d3ce\n--- /dev/null\n+++ b/tests/db_functions/math/test_log.py\n@@ -0,0 +1,36 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models.functions import Log\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class LogTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('12.9'), n2=Decimal('3.6'))\n+        obj = DecimalModel.objects.annotate(n_log=Log('n1', 'n2')).first()\n+        self.assertIsInstance(obj.n_log, Decimal)\n+        self.assertAlmostEqual(obj.n_log, Decimal(math.log(obj.n2, obj.n1)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=2.0, f2=4.0)\n+        obj = FloatModel.objects.annotate(f_log=Log('f1', 'f2')).first()\n+        self.assertIsInstance(obj.f_log, float)\n+        self.assertAlmostEqual(obj.f_log, math.log(obj.f2, obj.f1))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=4, normal=8, big=2)\n+        obj = IntegerModel.objects.annotate(\n+            small_log=Log('small', 'big'),\n+            normal_log=Log('normal', 'big'),\n+            big_log=Log('big', 'big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_log, float)\n+        self.assertIsInstance(obj.normal_log, float)\n+        self.assertIsInstance(obj.big_log, float)\n+        self.assertAlmostEqual(obj.small_log, math.log(obj.big, obj.small))\n+        self.assertAlmostEqual(obj.normal_log, math.log(obj.big, obj.normal))\n+        self.assertAlmostEqual(obj.big_log, math.log(obj.big, obj.big))\ndiff --git a/tests/db_functions/math/test_mod.py b/tests/db_functions/math/test_mod.py\nnew file mode 100644\nindex 000000000000..0e90175ddc1d\n--- /dev/null\n+++ b/tests/db_functions/math/test_mod.py\n@@ -0,0 +1,36 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models.functions import Mod\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class ModTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-9.9'), n2=Decimal('4.6'))\n+        obj = DecimalModel.objects.annotate(n_mod=Mod('n1', 'n2')).first()\n+        self.assertIsInstance(obj.n_mod, Decimal)\n+        self.assertAlmostEqual(obj.n_mod, Decimal(math.fmod(obj.n1, obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-25, f2=0.33)\n+        obj = FloatModel.objects.annotate(f_mod=Mod('f1', 'f2')).first()\n+        self.assertIsInstance(obj.f_mod, float)\n+        self.assertAlmostEqual(obj.f_mod, math.fmod(obj.f1, obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=20, normal=15, big=1)\n+        obj = IntegerModel.objects.annotate(\n+            small_mod=Mod('small', 'normal'),\n+            normal_mod=Mod('normal', 'big'),\n+            big_mod=Mod('big', 'small'),\n+        ).first()\n+        self.assertIsInstance(obj.small_mod, float)\n+        self.assertIsInstance(obj.normal_mod, float)\n+        self.assertIsInstance(obj.big_mod, float)\n+        self.assertEqual(obj.small_mod, math.fmod(obj.small, obj.normal))\n+        self.assertEqual(obj.normal_mod, math.fmod(obj.normal, obj.big))\n+        self.assertEqual(obj.big_mod, math.fmod(obj.big, obj.small))\ndiff --git a/tests/db_functions/math/test_pi.py b/tests/db_functions/math/test_pi.py\nnew file mode 100644\nindex 000000000000..2446420fd33a\n--- /dev/null\n+++ b/tests/db_functions/math/test_pi.py\n@@ -0,0 +1,15 @@\n+import math\n+\n+from django.db.models.functions import Pi\n+from django.test import TestCase\n+\n+from ..models import FloatModel\n+\n+\n+class PiTests(TestCase):\n+\n+    def test(self):\n+        FloatModel.objects.create(f1=2.5, f2=15.9)\n+        obj = FloatModel.objects.annotate(pi=Pi()).first()\n+        self.assertIsInstance(obj.pi, float)\n+        self.assertAlmostEqual(obj.pi, math.pi, places=5)\ndiff --git a/tests/db_functions/math/test_power.py b/tests/db_functions/math/test_power.py\nnew file mode 100644\nindex 000000000000..01ca2b34d9a8\n--- /dev/null\n+++ b/tests/db_functions/math/test_power.py\n@@ -0,0 +1,35 @@\n+from decimal import Decimal\n+\n+from django.db.models.functions import Power\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class PowerTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('1.0'), n2=Decimal('-0.6'))\n+        obj = DecimalModel.objects.annotate(n_power=Power('n1', 'n2')).first()\n+        self.assertIsInstance(obj.n_power, Decimal)\n+        self.assertAlmostEqual(obj.n_power, Decimal(obj.n1 ** obj.n2))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=2.3, f2=1.1)\n+        obj = FloatModel.objects.annotate(f_power=Power('f1', 'f2')).first()\n+        self.assertIsInstance(obj.f_power, float)\n+        self.assertAlmostEqual(obj.f_power, obj.f1 ** obj.f2)\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-1, normal=20, big=3)\n+        obj = IntegerModel.objects.annotate(\n+            small_power=Power('small', 'normal'),\n+            normal_power=Power('normal', 'big'),\n+            big_power=Power('big', 'small'),\n+        ).first()\n+        self.assertIsInstance(obj.small_power, float)\n+        self.assertIsInstance(obj.normal_power, float)\n+        self.assertIsInstance(obj.big_power, float)\n+        self.assertAlmostEqual(obj.small_power, obj.small ** obj.normal)\n+        self.assertAlmostEqual(obj.normal_power, obj.normal ** obj.big)\n+        self.assertAlmostEqual(obj.big_power, obj.big ** obj.small)\ndiff --git a/tests/db_functions/math/test_radians.py b/tests/db_functions/math/test_radians.py\nnew file mode 100644\nindex 000000000000..296c21e692f5\n--- /dev/null\n+++ b/tests/db_functions/math/test_radians.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Radians\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class RadiansTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_radians=Radians('n1'), n2_radians=Radians('n2')).first()\n+        self.assertIsInstance(obj.n1_radians, Decimal)\n+        self.assertIsInstance(obj.n2_radians, Decimal)\n+        self.assertAlmostEqual(obj.n1_radians, Decimal(math.radians(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_radians, Decimal(math.radians(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_radians=Radians('f1'), f2_radians=Radians('f2')).first()\n+        self.assertIsInstance(obj.f1_radians, float)\n+        self.assertIsInstance(obj.f2_radians, float)\n+        self.assertAlmostEqual(obj.f1_radians, math.radians(obj.f1))\n+        self.assertAlmostEqual(obj.f2_radians, math.radians(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-20, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_radians=Radians('small'),\n+            normal_radians=Radians('normal'),\n+            big_radians=Radians('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_radians, float)\n+        self.assertIsInstance(obj.normal_radians, float)\n+        self.assertIsInstance(obj.big_radians, float)\n+        self.assertAlmostEqual(obj.small_radians, math.radians(obj.small))\n+        self.assertAlmostEqual(obj.normal_radians, math.radians(obj.normal))\n+        self.assertAlmostEqual(obj.big_radians, math.radians(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Radians)\n+            DecimalModel.objects.create(n1=Decimal('2.0'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('-1.0'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__radians__gt=0)\n+            self.assertQuerysetEqual(objs, [Decimal('2.0')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Radians)\ndiff --git a/tests/db_functions/math/test_round.py b/tests/db_functions/math/test_round.py\nnew file mode 100644\nindex 000000000000..9ad390bd2410\n--- /dev/null\n+++ b/tests/db_functions/math/test_round.py\n@@ -0,0 +1,50 @@\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Round\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class RoundTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_round=Round('n1'), n2_round=Round('n2')).first()\n+        self.assertIsInstance(obj.n1_round, Decimal)\n+        self.assertIsInstance(obj.n2_round, Decimal)\n+        self.assertAlmostEqual(obj.n1_round, round(obj.n1))\n+        self.assertAlmostEqual(obj.n2_round, round(obj.n2))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.55, f2=0.55)\n+        obj = FloatModel.objects.annotate(f1_round=Round('f1'), f2_round=Round('f2')).first()\n+        self.assertIsInstance(obj.f1_round, float)\n+        self.assertIsInstance(obj.f2_round, float)\n+        self.assertAlmostEqual(obj.f1_round, round(obj.f1))\n+        self.assertAlmostEqual(obj.f2_round, round(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-20, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_round=Round('small'),\n+            normal_round=Round('normal'),\n+            big_round=Round('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_round, int)\n+        self.assertIsInstance(obj.normal_round, int)\n+        self.assertIsInstance(obj.big_round, int)\n+        self.assertEqual(obj.small_round, round(obj.small))\n+        self.assertEqual(obj.normal_round, round(obj.normal))\n+        self.assertEqual(obj.big_round, round(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Round)\n+            DecimalModel.objects.create(n1=Decimal('2.0'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('-1.0'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__round__gt=0)\n+            self.assertQuerysetEqual(objs, [Decimal('2.0')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Round)\ndiff --git a/tests/db_functions/math/test_sin.py b/tests/db_functions/math/test_sin.py\nnew file mode 100644\nindex 000000000000..efcf3319f072\n--- /dev/null\n+++ b/tests/db_functions/math/test_sin.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Sin\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class SinTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_sin=Sin('n1'), n2_sin=Sin('n2')).first()\n+        self.assertIsInstance(obj.n1_sin, Decimal)\n+        self.assertIsInstance(obj.n2_sin, Decimal)\n+        self.assertAlmostEqual(obj.n1_sin, Decimal(math.sin(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_sin, Decimal(math.sin(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_sin=Sin('f1'), f2_sin=Sin('f2')).first()\n+        self.assertIsInstance(obj.f1_sin, float)\n+        self.assertIsInstance(obj.f2_sin, float)\n+        self.assertAlmostEqual(obj.f1_sin, math.sin(obj.f1))\n+        self.assertAlmostEqual(obj.f2_sin, math.sin(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-20, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_sin=Sin('small'),\n+            normal_sin=Sin('normal'),\n+            big_sin=Sin('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_sin, float)\n+        self.assertIsInstance(obj.normal_sin, float)\n+        self.assertIsInstance(obj.big_sin, float)\n+        self.assertAlmostEqual(obj.small_sin, math.sin(obj.small))\n+        self.assertAlmostEqual(obj.normal_sin, math.sin(obj.normal))\n+        self.assertAlmostEqual(obj.big_sin, math.sin(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Sin)\n+            DecimalModel.objects.create(n1=Decimal('5.4'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('0.1'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__sin__lt=0)\n+            self.assertQuerysetEqual(objs, [Decimal('5.4')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Sin)\ndiff --git a/tests/db_functions/math/test_sqrt.py b/tests/db_functions/math/test_sqrt.py\nnew file mode 100644\nindex 000000000000..c391a6f5a9ca\n--- /dev/null\n+++ b/tests/db_functions/math/test_sqrt.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Sqrt\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class SqrtTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_sqrt=Sqrt('n1'), n2_sqrt=Sqrt('n2')).first()\n+        self.assertIsInstance(obj.n1_sqrt, Decimal)\n+        self.assertIsInstance(obj.n2_sqrt, Decimal)\n+        self.assertAlmostEqual(obj.n1_sqrt, Decimal(math.sqrt(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_sqrt, Decimal(math.sqrt(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_sqrt=Sqrt('f1'), f2_sqrt=Sqrt('f2')).first()\n+        self.assertIsInstance(obj.f1_sqrt, float)\n+        self.assertIsInstance(obj.f2_sqrt, float)\n+        self.assertAlmostEqual(obj.f1_sqrt, math.sqrt(obj.f1))\n+        self.assertAlmostEqual(obj.f2_sqrt, math.sqrt(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=20, normal=15, big=1)\n+        obj = IntegerModel.objects.annotate(\n+            small_sqrt=Sqrt('small'),\n+            normal_sqrt=Sqrt('normal'),\n+            big_sqrt=Sqrt('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_sqrt, float)\n+        self.assertIsInstance(obj.normal_sqrt, float)\n+        self.assertIsInstance(obj.big_sqrt, float)\n+        self.assertAlmostEqual(obj.small_sqrt, math.sqrt(obj.small))\n+        self.assertAlmostEqual(obj.normal_sqrt, math.sqrt(obj.normal))\n+        self.assertAlmostEqual(obj.big_sqrt, math.sqrt(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Sqrt)\n+            DecimalModel.objects.create(n1=Decimal('6.0'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('1.0'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__sqrt__gt=2)\n+            self.assertQuerysetEqual(objs, [Decimal('6.0')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Sqrt)\ndiff --git a/tests/db_functions/math/test_tan.py b/tests/db_functions/math/test_tan.py\nnew file mode 100644\nindex 000000000000..1df294150f5a\n--- /dev/null\n+++ b/tests/db_functions/math/test_tan.py\n@@ -0,0 +1,51 @@\n+import math\n+from decimal import Decimal\n+\n+from django.db.models import DecimalField\n+from django.db.models.functions import Tan\n+from django.test import TestCase\n+\n+from ..models import DecimalModel, FloatModel, IntegerModel\n+\n+\n+class TanTests(TestCase):\n+\n+    def test_decimal(self):\n+        DecimalModel.objects.create(n1=Decimal('-12.9'), n2=Decimal('0.6'))\n+        obj = DecimalModel.objects.annotate(n1_tan=Tan('n1'), n2_tan=Tan('n2')).first()\n+        self.assertIsInstance(obj.n1_tan, Decimal)\n+        self.assertIsInstance(obj.n2_tan, Decimal)\n+        self.assertAlmostEqual(obj.n1_tan, Decimal(math.tan(obj.n1)))\n+        self.assertAlmostEqual(obj.n2_tan, Decimal(math.tan(obj.n2)))\n+\n+    def test_float(self):\n+        FloatModel.objects.create(f1=-27.5, f2=0.33)\n+        obj = FloatModel.objects.annotate(f1_tan=Tan('f1'), f2_tan=Tan('f2')).first()\n+        self.assertIsInstance(obj.f1_tan, float)\n+        self.assertIsInstance(obj.f2_tan, float)\n+        self.assertAlmostEqual(obj.f1_tan, math.tan(obj.f1))\n+        self.assertAlmostEqual(obj.f2_tan, math.tan(obj.f2))\n+\n+    def test_integer(self):\n+        IntegerModel.objects.create(small=-20, normal=15, big=-1)\n+        obj = IntegerModel.objects.annotate(\n+            small_tan=Tan('small'),\n+            normal_tan=Tan('normal'),\n+            big_tan=Tan('big'),\n+        ).first()\n+        self.assertIsInstance(obj.small_tan, float)\n+        self.assertIsInstance(obj.normal_tan, float)\n+        self.assertIsInstance(obj.big_tan, float)\n+        self.assertAlmostEqual(obj.small_tan, math.tan(obj.small))\n+        self.assertAlmostEqual(obj.normal_tan, math.tan(obj.normal))\n+        self.assertAlmostEqual(obj.big_tan, math.tan(obj.big))\n+\n+    def test_transform(self):\n+        try:\n+            DecimalField.register_lookup(Tan)\n+            DecimalModel.objects.create(n1=Decimal('0.0'), n2=Decimal('0'))\n+            DecimalModel.objects.create(n1=Decimal('12.0'), n2=Decimal('0'))\n+            objs = DecimalModel.objects.filter(n1__tan__lt=0)\n+            self.assertQuerysetEqual(objs, [Decimal('12.0')], lambda a: a.n1)\n+        finally:\n+            DecimalField._unregister_lookup(Tan)\ndiff --git a/tests/db_functions/models.py b/tests/db_functions/models.py\nindex 24ffba78fc06..c31de39b85ef 100644\n--- a/tests/db_functions/models.py\n+++ b/tests/db_functions/models.py\n@@ -54,3 +54,14 @@ def __str__(self):\n class DecimalModel(models.Model):\n     n1 = models.DecimalField(decimal_places=2, max_digits=6)\n     n2 = models.DecimalField(decimal_places=2, max_digits=6)\n+\n+\n+class IntegerModel(models.Model):\n+    big = models.BigIntegerField(null=True, blank=True)\n+    normal = models.IntegerField(null=True, blank=True)\n+    small = models.SmallIntegerField(null=True, blank=True)\n+\n+\n+class FloatModel(models.Model):\n+    f1 = models.FloatField(null=True, blank=True)\n+    f2 = models.FloatField(null=True, blank=True)\n"
  },
  {
    "index": 25,
    "filtered_comments": [
      "Looking through the code base there are quite a few areas where it would probably be easier if we just assumed that if psycopg3 is installed that we want to use it; this might get a bit more fun for testing (extra environment, but realistically speaking we want to be on psycopg3 only in the longrun anways)",
      "Besides the issue in `SchemaLoggerTests`, there's one other regression with psycopg2. (I'll work through the failures with psycopg3 later).\r\n```\r\n======================================================================\r\nFAIL: test_orders_nulls_first_on_filtered_subquery (ordering.tests.OrderingTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/tim/code/django/tests/ordering/tests.py\", line 199, in test_orders_nulls_first_on_filtered_subquery\r\n    self.assertQuerysetEqualReversible(\r\n  File \"/home/tim/code/django/tests/ordering/tests.py\", line 129, in assertQuerysetEqualReversible\r\n    self.assertSequenceEqual(queryset.reverse(), list(reversed(sequence)))\r\nAssertionError: Sequences differ: <QuerySet [<Author: Name 3>, <Author: Name 1>, <Author: Name 2>]> != [<Author: Name 2>, <Author: Name 1>, <Author: Name 3>]\r\n\r\nFirst differing element 0:\r\n<Author: Name 3>\r\n<Author: Name 2>\r\n\r\n- <QuerySet [<Author: Name 3>, <Author: Name 1>, <Author: Name 2>]>\r\n? ----------               ^                                   ^  -\r\n\r\n+ [<Author: Name 2>, <Author: Name 1>, <Author: Name 3>]\r\n?                ^                                   ^\r\n```\r\nThe old SQL:\r\n```sql\r\n SELECT DISTINCT \"ordering_author\".\"id\",\r\n                \"ordering_author\".\"name\",\r\n                \"ordering_author\".\"editor_id\",\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\") AS \"last_date\",\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\") IS NULL,\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\")\r\nFROM   \"ordering_author\"\r\nORDER  BY (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n           FROM   \"ordering_article\" U0\r\n           WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                    AND Upper(U0.\"headline\" :: text) LIKE Upper('%Article%') )\r\n           GROUP  BY U0.\"author_id\") IS NULL,\r\n          (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n           FROM   \"ordering_article\" U0\r\n           WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                    AND Upper(U0.\"headline\" :: text) LIKE Upper('%Article%') )\r\n           GROUP  BY U0.\"author_id\") DESC  \r\n```\r\n\r\n\r\nThe new SQL: \r\n```sql\r\n SELECT DISTINCT \"ordering_author\".\"id\",\r\n                \"ordering_author\".\"name\",\r\n                \"ordering_author\".\"editor_id\",\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\") AS \"last_date\",\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\") IS NULL,\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\")\r\nFROM   \"ordering_author\"\r\nORDER  BY 5 DESC  \r\n```\r\nIt might be that because CockroachDB has `DatabaseFeatures.nulls_order_largest = False` (unlike PostgreSQL), the loss of the second subquery in the `ORDER BY` is problematic.",
      "Hi folks,\r\n\r\nI have just rebased this PR to latest master. Now that all prerequisites for this PR are in further rebasing should get easier and not require manual intervention. I think the PR is starting to become solid, there are some areas though where I'd highly appreciate some help:\r\n\r\n * PostGIS in general. The adapter registration there is imo still rather ugly, I am open to creative ideas :)\r\n * Is my new handling of registering extension sound? \r\n * Do the new `as_postgresql` make sense everywhere?\r\n * Ideas on https://github.com/django/django/pull/15687/files#diff-530111bb812ef292d6db3ab0285e128bb585992d02d47dcca872d2e8c9b592daR586 \r\n * Is `compose` used correctly and consistently or are there other variants still around in the codebase?",
      "@charettes Uff that is great to hear. Btw I do have a custom branch where I have enabled github actions to test all combinations: https://github.com/apollo13/django/tree/psycopg-dev So once you have something I'll reapply it there to run the tests against all combinations.",
      "After digging in, many of the errors about 'could not determine type of placeholder' are actually string arguments like:\r\n```\r\nSELECT \"ordering_article\".\"id\",\r\n       \"ordering_article\".\"author_id\",\r\n       \"ordering_article\".\"second_author_id\",\r\n       \"ordering_article\".\"headline\",\r\n       \"ordering_article\".\"pub_date\",\r\n       '1' AS \"constant\"\r\nFROM \"ordering_article\"\r\nORDER BY \"constant\" ASC,\r\n         \"ordering_article\".\"headline\" DESC;\r\n\r\nargs=('1',);\r\n```\r\nfails with:\r\n```\r\npsycopg.errors.IndeterminateDatatype: could not determine data type of placeholder $1\r\nHINT:  consider adding explicit type casts to the placeholder arguments\r\n```\r\nAs far as I can tell, the same query is generated when running against PostgreSQL. Am I missing some initialization code or something? Could it be a problem with CockroachDB?",
      "@timgraham if you want this to work properly for CockroachDB you'd likely want to override `Value.as_cockroachdb` to add explicit casts based of `self.output_field` so annotating `Value(\"1\")` turns into `('%s::text', '1')`.\r\n\r\nThis might not work properly until a form of https://github.com/apollo13/django/pull/3 lands here though as we've been supporting `Value(json.dumps(1))` and expect the JSON type inference to just work and I believe we'll have to deprecate that."
    ],
    "code_diff": "diff --git a/django/contrib/gis/db/backends/postgis/adapter.py b/django/contrib/gis/db/backends/postgis/adapter.py\nindex 9161e25f1635..c95f9032538a 100644\n--- a/django/contrib/gis/db/backends/postgis/adapter.py\n+++ b/django/contrib/gis/db/backends/postgis/adapter.py\n@@ -1,8 +1,6 @@\n \"\"\"\n  This object provides quoting for GEOS geometries into PostgreSQL/PostGIS.\n \"\"\"\n-from psycopg2.extensions import ISQLQuote\n-\n from django.contrib.gis.db.backends.postgis.pgraster import to_pgraster\n from django.contrib.gis.geos import GEOSGeometry\n from django.db.backends.postgresql.psycopg_any import sql\n@@ -27,6 +25,8 @@ def __init__(self, obj, geography=False):\n \n     def __conform__(self, proto):\n         \"\"\"Does the given protocol conform to what Psycopg2 expects?\"\"\"\n+        from psycopg2.extensions import ISQLQuote\n+\n         if proto == ISQLQuote:\n             return self\n         else:\ndiff --git a/django/contrib/gis/db/backends/postgis/base.py b/django/contrib/gis/db/backends/postgis/base.py\nindex 98c2813aa284..23ec0553f824 100644\n--- a/django/contrib/gis/db/backends/postgis/base.py\n+++ b/django/contrib/gis/db/backends/postgis/base.py\n@@ -1,17 +1,93 @@\n+from functools import lru_cache\n+\n from django.db.backends.base.base import NO_DB_ALIAS\n-from django.db.backends.postgresql.base import (\n-    DatabaseWrapper as Psycopg2DatabaseWrapper,\n-)\n+from django.db.backends.postgresql.base import DatabaseWrapper as PsycopgDatabaseWrapper\n+from django.db.backends.postgresql.psycopg_any import is_psycopg3\n \n+from .adapter import PostGISAdapter\n from .features import DatabaseFeatures\n from .introspection import PostGISIntrospection\n from .operations import PostGISOperations\n from .schema import PostGISSchemaEditor\n \n+if is_psycopg3:\n+    from psycopg.adapt import Dumper\n+    from psycopg.pq import Format\n+    from psycopg.types import TypeInfo\n+    from psycopg.types.string import TextBinaryLoader, TextLoader\n+\n+    class GeometryType:\n+        pass\n+\n+    class GeographyType:\n+        pass\n+\n+    class RasterType:\n+        pass\n+\n+    class BaseTextDumper(Dumper):\n+        def dump(self, obj):\n+            # Return bytes as hex for text formatting\n+            return obj.ewkb.hex().encode()\n+\n+    class BaseBinaryDumper(Dumper):\n+        format = Format.BINARY\n+\n+        def dump(self, obj):\n+            return obj.ewkb\n+\n+    @lru_cache\n+    def postgis_adapters(geo_oid, geog_oid, raster_oid):\n+        class BaseDumper(Dumper):\n+            def __init_subclass__(cls, base_dumper):\n+                super().__init_subclass__()\n+\n+                cls.GeometryDumper = type(\n+                    \"GeometryDumper\", (base_dumper,), {\"oid\": geo_oid}\n+                )\n+                cls.GeographyDumper = type(\n+                    \"GeographyDumper\", (base_dumper,), {\"oid\": geog_oid}\n+                )\n+                cls.RasterDumper = type(\n+                    \"RasterDumper\", (BaseTextDumper,), {\"oid\": raster_oid}\n+                )\n+\n+            def get_key(self, obj, format):\n+                if obj.is_geometry:\n+                    return GeographyType if obj.geography else GeometryType\n+                else:\n+                    return RasterType\n+\n+            def upgrade(self, obj, format):\n+                if obj.is_geometry:\n+                    if obj.geography:\n+                        return self.GeographyDumper(GeographyType)\n+                    else:\n+                        return self.GeometryDumper(GeometryType)\n+                else:\n+                    return self.RasterDumper(RasterType)\n+\n+            def dump(self, obj):\n+                raise NotImplementedError\n+\n+        class PostGISTextDumper(BaseDumper, base_dumper=BaseTextDumper):\n+            pass\n \n-class DatabaseWrapper(Psycopg2DatabaseWrapper):\n+        class PostGISBinaryDumper(BaseDumper, base_dumper=BaseBinaryDumper):\n+            format = Format.BINARY\n+\n+        return PostGISTextDumper, PostGISBinaryDumper\n+\n+\n+class DatabaseWrapper(PsycopgDatabaseWrapper):\n     SchemaEditorClass = PostGISSchemaEditor\n \n+    _type_infos = {\n+        \"geometry\": {},\n+        \"geography\": {},\n+        \"raster\": {},\n+    }\n+\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         if kwargs.get(\"alias\", \"\") != NO_DB_ALIAS:\n@@ -27,3 +103,45 @@ def prepare_database(self):\n             if bool(cursor.fetchone()):\n                 return\n             cursor.execute(\"CREATE EXTENSION IF NOT EXISTS postgis\")\n+            if is_psycopg3:\n+                # Ensure adapters are registers if PostGIS is used within this\n+                # connection.\n+                self.register_geometry_adapters(self.connection, True)\n+\n+    def get_new_connection(self, conn_params):\n+        connection = super().get_new_connection(conn_params)\n+        if is_psycopg3:\n+            self.register_geometry_adapters(connection)\n+        return connection\n+\n+    if is_psycopg3:\n+\n+        def _register_type(self, pg_connection, typename):\n+            registry = self._type_infos[typename]\n+            try:\n+                info = registry[self.alias]\n+            except KeyError:\n+                info = TypeInfo.fetch(pg_connection, typename)\n+                registry[self.alias] = info\n+\n+            if info:  # Can be None if the type does not exist (yet).\n+                info.register(pg_connection)\n+                pg_connection.adapters.register_loader(info.oid, TextLoader)\n+                pg_connection.adapters.register_loader(info.oid, TextBinaryLoader)\n+\n+            return info.oid if info else None\n+\n+        def register_geometry_adapters(self, pg_connection, clear_caches=False):\n+            if clear_caches:\n+                for typename in self._type_infos:\n+                    self._type_infos[typename].pop(self.alias, None)\n+\n+            geo_oid = self._register_type(pg_connection, \"geometry\")\n+            geog_oid = self._register_type(pg_connection, \"geography\")\n+            raster_oid = self._register_type(pg_connection, \"raster\")\n+\n+            PostGISTextDumper, PostGISBinaryDumper = postgis_adapters(\n+                geo_oid, geog_oid, raster_oid\n+            )\n+            pg_connection.adapters.register_dumper(PostGISAdapter, PostGISTextDumper)\n+            pg_connection.adapters.register_dumper(PostGISAdapter, PostGISBinaryDumper)\ndiff --git a/django/contrib/gis/db/backends/postgis/features.py b/django/contrib/gis/db/backends/postgis/features.py\nindex 29a107963108..d96e939db3d7 100644\n--- a/django/contrib/gis/db/backends/postgis/features.py\n+++ b/django/contrib/gis/db/backends/postgis/features.py\n@@ -1,10 +1,10 @@\n from django.contrib.gis.db.backends.base.features import BaseSpatialFeatures\n from django.db.backends.postgresql.features import (\n-    DatabaseFeatures as Psycopg2DatabaseFeatures,\n+    DatabaseFeatures as PsycopgDatabaseFeatures,\n )\n \n \n-class DatabaseFeatures(BaseSpatialFeatures, Psycopg2DatabaseFeatures):\n+class DatabaseFeatures(BaseSpatialFeatures, PsycopgDatabaseFeatures):\n     supports_geography = True\n     supports_3d_storage = True\n     supports_3d_functions = True\ndiff --git a/django/contrib/gis/db/backends/postgis/operations.py b/django/contrib/gis/db/backends/postgis/operations.py\nindex 31ad31e2e584..070f670a0b52 100644\n--- a/django/contrib/gis/db/backends/postgis/operations.py\n+++ b/django/contrib/gis/db/backends/postgis/operations.py\n@@ -11,6 +11,7 @@\n from django.core.exceptions import ImproperlyConfigured\n from django.db import NotSupportedError, ProgrammingError\n from django.db.backends.postgresql.operations import DatabaseOperations\n+from django.db.backends.postgresql.psycopg_any import is_psycopg3\n from django.db.models import Func, Value\n from django.utils.functional import cached_property\n from django.utils.version import get_version_tuple\n@@ -161,7 +162,8 @@ class PostGISOperations(BaseSpatialOperations, DatabaseOperations):\n \n     unsupported_functions = set()\n \n-    select = \"%s::bytea\"\n+    select = \"%s\" if is_psycopg3 else \"%s::bytea\"\n+\n     select_extent = None\n \n     @cached_property\n@@ -407,6 +409,8 @@ def get_geometry_converter(self, expression):\n         geom_class = expression.output_field.geom_class\n \n         def converter(value, expression, connection):\n+            if isinstance(value, str):  # Coming from hex strings.\n+                value = value.encode(\"ascii\")\n             return None if value is None else GEOSGeometryBase(read(value), geom_class)\n \n         return converter\ndiff --git a/django/contrib/postgres/fields/array.py b/django/contrib/postgres/fields/array.py\nindex eaff032465bf..8477dd9fff2f 100644\n--- a/django/contrib/postgres/fields/array.py\n+++ b/django/contrib/postgres/fields/array.py\n@@ -237,7 +237,7 @@ def formfield(self, **kwargs):\n \n class ArrayRHSMixin:\n     def __init__(self, lhs, rhs):\n-        # Don't wrap arrays that contains only None values, psycopg2 doesn't\n+        # Don't wrap arrays that contains only None values, psycopg doesn't\n         # allow this.\n         if isinstance(rhs, (tuple, list)) and any(self._rhs_not_none_values(rhs)):\n             expressions = []\ndiff --git a/django/contrib/postgres/fields/ranges.py b/django/contrib/postgres/fields/ranges.py\nindex d5c438dbdcb8..fbb601266090 100644\n--- a/django/contrib/postgres/fields/ranges.py\n+++ b/django/contrib/postgres/fields/ranges.py\n@@ -9,6 +9,7 @@\n     NumericRange,\n     Range,\n )\n+from django.db.models.functions import Cast\n from django.db.models.lookups import PostgresOperatorLookup\n \n from .utils import AttributeSetter\n@@ -208,7 +209,14 @@ def db_type(self, connection):\n         return \"daterange\"\n \n \n-RangeField.register_lookup(lookups.DataContains)\n+class RangeContains(lookups.DataContains):\n+    def get_prep_lookup(self):\n+        if not isinstance(self.rhs, (list, tuple, Range)):\n+            return Cast(self.rhs, self.lhs.field.base_field)\n+        return super().get_prep_lookup()\n+\n+\n+RangeField.register_lookup(RangeContains)\n RangeField.register_lookup(lookups.ContainedBy)\n RangeField.register_lookup(lookups.Overlap)\n \ndiff --git a/django/contrib/postgres/operations.py b/django/contrib/postgres/operations.py\nindex 9dbd49177329..5ac396bedf38 100644\n--- a/django/contrib/postgres/operations.py\n+++ b/django/contrib/postgres/operations.py\n@@ -35,6 +35,10 @@ def database_forwards(self, app_label, schema_editor, from_state, to_state):\n         # installed, otherwise a subsequent data migration would use the same\n         # connection.\n         register_type_handlers(schema_editor.connection)\n+        if hasattr(schema_editor.connection, \"register_geometry_adapters\"):\n+            schema_editor.connection.register_geometry_adapters(\n+                schema_editor.connection.connection, True\n+            )\n \n     def database_backwards(self, app_label, schema_editor, from_state, to_state):\n         if not router.allow_migrate(schema_editor.connection.alias, app_label):\ndiff --git a/django/contrib/postgres/search.py b/django/contrib/postgres/search.py\nindex 05c8f72f6fb1..4e370aa16774 100644\n--- a/django/contrib/postgres/search.py\n+++ b/django/contrib/postgres/search.py\n@@ -39,6 +39,11 @@ def db_type(self, connection):\n         return \"tsquery\"\n \n \n+class _Float4Field(Field):\n+    def db_type(self, connection):\n+        return \"float4\"\n+\n+\n class SearchConfig(Expression):\n     def __init__(self, config):\n         super().__init__()\n@@ -138,7 +143,11 @@ def as_sql(self, compiler, connection, function=None, template=None):\n         if clone.weight:\n             weight_sql, extra_params = compiler.compile(clone.weight)\n             sql = \"setweight({}, {})\".format(sql, weight_sql)\n-        return sql, config_params + params + extra_params\n+\n+        # These parameters must be bound on the client side because we may\n+        # want to create an index on this expression.\n+        sql = connection.ops.compose_sql(sql, config_params + params + extra_params)\n+        return sql, []\n \n \n class CombinedSearchVector(SearchVectorCombinable, CombinedExpression):\n@@ -244,6 +253,8 @@ def __init__(\n         normalization=None,\n         cover_density=False,\n     ):\n+        from .fields.array import ArrayField\n+\n         if not hasattr(vector, \"resolve_expression\"):\n             vector = SearchVector(vector)\n         if not hasattr(query, \"resolve_expression\"):\n@@ -252,6 +263,7 @@ def __init__(\n         if weights is not None:\n             if not hasattr(weights, \"resolve_expression\"):\n                 weights = Value(weights)\n+            weights = Cast(weights, ArrayField(_Float4Field()))\n             expressions = (weights,) + expressions\n         if normalization is not None:\n             if not hasattr(normalization, \"resolve_expression\"):\ndiff --git a/django/contrib/postgres/signals.py b/django/contrib/postgres/signals.py\nindex 5c6ca3687ab8..a3816d3d305d 100644\n--- a/django/contrib/postgres/signals.py\n+++ b/django/contrib/postgres/signals.py\n@@ -1,10 +1,8 @@\n import functools\n \n-import psycopg2\n-from psycopg2.extras import register_hstore\n-\n from django.db import connections\n from django.db.backends.base.base import NO_DB_ALIAS\n+from django.db.backends.postgresql.psycopg_any import is_psycopg3\n \n \n def get_type_oids(connection_alias, type_name):\n@@ -32,30 +30,51 @@ def get_citext_oids(connection_alias):\n     return get_type_oids(connection_alias, \"citext\")\n \n \n-def register_type_handlers(connection, **kwargs):\n-    if connection.vendor != \"postgresql\" or connection.alias == NO_DB_ALIAS:\n-        return\n-\n-    oids, array_oids = get_hstore_oids(connection.alias)\n-    # Don't register handlers when hstore is not available on the database.\n-    #\n-    # If someone tries to create an hstore field it will error there. This is\n-    # necessary as someone may be using PSQL without extensions installed but\n-    # be using other features of contrib.postgres.\n-    #\n-    # This is also needed in order to create the connection in order to install\n-    # the hstore extension.\n-    if oids:\n-        register_hstore(\n-            connection.connection, globally=True, oid=oids, array_oid=array_oids\n-        )\n+if is_psycopg3:\n+    from psycopg.types import TypeInfo, hstore\n \n-    oids, citext_oids = get_citext_oids(connection.alias)\n-    # Don't register handlers when citext is not available on the database.\n-    #\n-    # The same comments in the above call to register_hstore() also apply here.\n-    if oids:\n-        array_type = psycopg2.extensions.new_array_type(\n-            citext_oids, \"citext[]\", psycopg2.STRING\n-        )\n-        psycopg2.extensions.register_type(array_type, None)\n+    def register_type_handlers(connection, **kwargs):\n+        if connection.vendor != \"postgresql\" or connection.alias == NO_DB_ALIAS:\n+            return\n+\n+        oids, array_oids = get_hstore_oids(connection.alias)\n+        for oid, array_oid in zip(oids, array_oids):\n+            ti = TypeInfo(\"hstore\", oid, array_oid)\n+            hstore.register_hstore(ti, connection.connection)\n+\n+        _, citext_oids = get_citext_oids(connection.alias)\n+        for array_oid in citext_oids:\n+            ti = TypeInfo(\"citext\", 0, array_oid)\n+            ti.register(connection.connection)\n+\n+else:\n+    import psycopg2\n+    from psycopg2.extras import register_hstore\n+\n+    def register_type_handlers(connection, **kwargs):\n+        if connection.vendor != \"postgresql\" or connection.alias == NO_DB_ALIAS:\n+            return\n+\n+        oids, array_oids = get_hstore_oids(connection.alias)\n+        # Don't register handlers when hstore is not available on the database.\n+        #\n+        # If someone tries to create an hstore field it will error there. This is\n+        # necessary as someone may be using PSQL without extensions installed but\n+        # be using other features of contrib.postgres.\n+        #\n+        # This is also needed in order to create the connection in order to install\n+        # the hstore extension.\n+        if oids:\n+            register_hstore(\n+                connection.connection, globally=True, oid=oids, array_oid=array_oids\n+            )\n+\n+        oids, citext_oids = get_citext_oids(connection.alias)\n+        # Don't register handlers when citext is not available on the database.\n+        #\n+        # The same comments in the above call to register_hstore() also apply here.\n+        if oids:\n+            array_type = psycopg2.extensions.new_array_type(\n+                citext_oids, \"citext[]\", psycopg2.STRING\n+            )\n+            psycopg2.extensions.register_type(array_type, None)\ndiff --git a/django/core/management/commands/loaddata.py b/django/core/management/commands/loaddata.py\nindex 6fae25540748..618c6c29b614 100644\n--- a/django/core/management/commands/loaddata.py\n+++ b/django/core/management/commands/loaddata.py\n@@ -207,7 +207,7 @@ def save_obj(self, obj):\n             self.models.add(obj.object.__class__)\n             try:\n                 obj.save(using=self.using)\n-            # psycopg2 raises ValueError if data contains NUL chars.\n+            # psycopg raises ValueError if data contains NUL chars.\n             except (DatabaseError, IntegrityError, ValueError) as e:\n                 e.args = (\n                     \"Could not load %(object_label)s(pk=%(pk)s): %(error_msg)s\"\ndiff --git a/django/db/backends/base/features.py b/django/db/backends/base/features.py\nindex a1d38d3530fe..190f728bba2a 100644\n--- a/django/db/backends/base/features.py\n+++ b/django/db/backends/base/features.py\n@@ -164,6 +164,8 @@ class BaseDatabaseFeatures:\n     # Can we roll back DDL in a transaction?\n     can_rollback_ddl = False\n \n+    schema_editor_uses_clientside_param_binding = False\n+\n     # Does it support operations requiring references rename in a transaction?\n     supports_atomic_references_rename = True\n \n@@ -335,6 +337,9 @@ class BaseDatabaseFeatures:\n     # Does the backend support the logical XOR operator?\n     supports_logical_xor = False\n \n+    # Set to (exception, message) if null characters in text are disallowed.\n+    prohibits_null_characters_in_text_exception = None\n+\n     # Collation names for use by the Django test suite.\n     test_collations = {\n         \"ci\": None,  # Case-insensitive.\ndiff --git a/django/db/backends/base/operations.py b/django/db/backends/base/operations.py\nindex 407681a41830..4ee73c073479 100644\n--- a/django/db/backends/base/operations.py\n+++ b/django/db/backends/base/operations.py\n@@ -525,6 +525,9 @@ def adapt_unknown_value(self, value):\n         else:\n             return value\n \n+    def adapt_integerfield_value(self, value, internal_type):\n+        return value\n+\n     def adapt_datefield_value(self, value):\n         \"\"\"\n         Transform a date value to an object compatible with what is expected\ndiff --git a/django/db/backends/postgresql/base.py b/django/db/backends/postgresql/base.py\nindex 0aee39aa5c06..ceea1bebade3 100644\n--- a/django/db/backends/postgresql/base.py\n+++ b/django/db/backends/postgresql/base.py\n@@ -1,7 +1,7 @@\n \"\"\"\n PostgreSQL database backend for Django.\n \n-Requires psycopg 2: https://www.psycopg.org/\n+Requires psycopg2 >= 2.8.4 or psycopg >= 3.1\n \"\"\"\n \n import asyncio\n@@ -21,48 +21,63 @@\n from django.utils.version import get_version_tuple\n \n try:\n-    import psycopg2 as Database\n-    import psycopg2.extensions\n-    import psycopg2.extras\n-except ImportError as e:\n-    raise ImproperlyConfigured(\"Error loading psycopg2 module: %s\" % e)\n+    try:\n+        import psycopg as Database\n+    except ImportError:\n+        import psycopg2 as Database\n+except ImportError:\n+    raise ImproperlyConfigured(\"Error loading psycopg2 or psycopg module\")\n \n \n-def psycopg2_version():\n-    version = psycopg2.__version__.split(\" \", 1)[0]\n+def psycopg_version():\n+    version = Database.__version__.split(\" \", 1)[0]\n     return get_version_tuple(version)\n \n \n-PSYCOPG2_VERSION = psycopg2_version()\n-\n-if PSYCOPG2_VERSION < (2, 8, 4):\n+if psycopg_version() < (2, 8, 4):\n+    raise ImproperlyConfigured(\n+        f\"psycopg2 version 2.8.4 or newer is required; you have {Database.__version__}\"\n+    )\n+if (3,) <= psycopg_version() < (3, 1):\n     raise ImproperlyConfigured(\n-        \"psycopg2 version 2.8.4 or newer is required; you have %s\"\n-        % psycopg2.__version__\n+        f\"psycopg version 3.1 or newer is required; you have {Database.__version__}\"\n     )\n \n \n-# Some of these import psycopg2, so import them after checking if it's installed.\n-from .client import DatabaseClient  # NOQA\n-from .creation import DatabaseCreation  # NOQA\n-from .features import DatabaseFeatures  # NOQA\n-from .introspection import DatabaseIntrospection  # NOQA\n-from .operations import DatabaseOperations  # NOQA\n-from .psycopg_any import IsolationLevel  # NOQA\n-from .schema import DatabaseSchemaEditor  # NOQA\n+from .psycopg_any import IsolationLevel, is_psycopg3  # NOQA isort:skip\n+\n+if is_psycopg3:\n+    from psycopg import adapters, sql\n+    from psycopg.pq import Format\n \n-psycopg2.extensions.register_adapter(SafeString, psycopg2.extensions.QuotedString)\n-psycopg2.extras.register_uuid()\n+    from .psycopg_any import get_adapters_template, register_tzloader\n \n-# Register support for inet[] manually so we don't have to handle the Inet()\n-# object on load all the time.\n-INETARRAY_OID = 1041\n-INETARRAY = psycopg2.extensions.new_array_type(\n-    (INETARRAY_OID,),\n-    \"INETARRAY\",\n-    psycopg2.extensions.UNICODE,\n-)\n-psycopg2.extensions.register_type(INETARRAY)\n+    TIMESTAMPTZ_OID = adapters.types[\"timestamptz\"].oid\n+\n+else:\n+    import psycopg2.extensions\n+    import psycopg2.extras\n+\n+    psycopg2.extensions.register_adapter(SafeString, psycopg2.extensions.QuotedString)\n+    psycopg2.extras.register_uuid()\n+\n+    # Register support for inet[] manually so we don't have to handle the Inet()\n+    # object on load all the time.\n+    INETARRAY_OID = 1041\n+    INETARRAY = psycopg2.extensions.new_array_type(\n+        (INETARRAY_OID,),\n+        \"INETARRAY\",\n+        psycopg2.extensions.UNICODE,\n+    )\n+    psycopg2.extensions.register_type(INETARRAY)\n+\n+# Some of these import psycopg, so import them after checking if it's installed.\n+from .client import DatabaseClient  # NOQA isort:skip\n+from .creation import DatabaseCreation  # NOQA isort:skip\n+from .features import DatabaseFeatures  # NOQA isort:skip\n+from .introspection import DatabaseIntrospection  # NOQA isort:skip\n+from .operations import DatabaseOperations  # NOQA isort:skip\n+from .schema import DatabaseSchemaEditor  # NOQA isort:skip\n \n \n class DatabaseWrapper(BaseDatabaseWrapper):\n@@ -209,6 +224,15 @@ def get_connection_params(self):\n             conn_params[\"host\"] = settings_dict[\"HOST\"]\n         if settings_dict[\"PORT\"]:\n             conn_params[\"port\"] = settings_dict[\"PORT\"]\n+        if is_psycopg3:\n+            conn_params[\"context\"] = get_adapters_template(\n+                settings.USE_TZ, self.timezone\n+            )\n+            # Disable prepared statements by default to keep connection poolers\n+            # working. Can be reenabled via OPTIONS in the settings dict.\n+            conn_params[\"prepare_threshold\"] = conn_params.pop(\n+                \"prepare_threshold\", None\n+            )\n         return conn_params\n \n     @async_unsafe\n@@ -232,17 +256,19 @@ def get_new_connection(self, conn_params):\n             except ValueError:\n                 raise ImproperlyConfigured(\n                     f\"Invalid transaction isolation level {isolation_level_value} \"\n-                    f\"specified. Use one of the IsolationLevel values.\"\n+                    f\"specified. Use one of the psycopg.IsolationLevel values.\"\n                 )\n-        connection = Database.connect(**conn_params)\n+        connection = self.Database.connect(**conn_params)\n         if set_isolation_level:\n             connection.isolation_level = self.isolation_level\n-        # Register dummy loads() to avoid a round trip from psycopg2's decode\n-        # to json.dumps() to json.loads(), when using a custom decoder in\n-        # JSONField.\n-        psycopg2.extras.register_default_jsonb(\n-            conn_or_curs=connection, loads=lambda x: x\n-        )\n+        if not is_psycopg3:\n+            # Register dummy loads() to avoid a round trip from psycopg2's\n+            # decode to json.dumps() to json.loads(), when using a custom\n+            # decoder in JSONField.\n+            psycopg2.extras.register_default_jsonb(\n+                conn_or_curs=connection, loads=lambda x: x\n+            )\n+        connection.cursor_factory = Cursor\n         return connection\n \n     def ensure_timezone(self):\n@@ -275,7 +301,15 @@ def create_cursor(self, name=None):\n             )\n         else:\n             cursor = self.connection.cursor()\n-        cursor.tzinfo_factory = self.tzinfo_factory if settings.USE_TZ else None\n+\n+        if is_psycopg3:\n+            # Register the cursor timezone only if the connection disagrees, to\n+            # avoid copying the adapter map.\n+            tzloader = self.connection.adapters.get_loader(TIMESTAMPTZ_OID, Format.TEXT)\n+            if self.timezone != tzloader.timezone:\n+                register_tzloader(self.timezone, cursor)\n+        else:\n+            cursor.tzinfo_factory = self.tzinfo_factory if settings.USE_TZ else None\n         return cursor\n \n     def tzinfo_factory(self, offset):\n@@ -379,11 +413,43 @@ def make_debug_cursor(self, cursor):\n         return CursorDebugWrapper(cursor, self)\n \n \n-class CursorDebugWrapper(BaseCursorDebugWrapper):\n-    def copy_expert(self, sql, file, *args):\n-        with self.debug_sql(sql):\n-            return self.cursor.copy_expert(sql, file, *args)\n+if is_psycopg3:\n+\n+    class Cursor(Database.Cursor):\n+        \"\"\"\n+        A subclass of psycopg cursor implementing callproc.\n+        \"\"\"\n+\n+        def callproc(self, name, args=None):\n+            if not isinstance(name, sql.Identifier):\n+                name = sql.Identifier(name)\n+\n+            qparts = [sql.SQL(\"SELECT * FROM \"), name, sql.SQL(\"(\")]\n+            if args:\n+                for item in args:\n+                    qparts.append(sql.Literal(item))\n+                    qparts.append(sql.SQL(\",\"))\n+                del qparts[-1]\n+\n+            qparts.append(sql.SQL(\")\"))\n+            stmt = sql.Composed(qparts)\n+            self.execute(stmt)\n+            return args\n+\n+    class CursorDebugWrapper(BaseCursorDebugWrapper):\n+        def copy(self, statement):\n+            with self.debug_sql(statement):\n+                return self.cursor.copy(statement)\n+\n+else:\n+\n+    Cursor = psycopg2.extensions.cursor\n+\n+    class CursorDebugWrapper(BaseCursorDebugWrapper):\n+        def copy_expert(self, sql, file, *args):\n+            with self.debug_sql(sql):\n+                return self.cursor.copy_expert(sql, file, *args)\n \n-    def copy_to(self, file, table, *args, **kwargs):\n-        with self.debug_sql(sql=\"COPY %s TO STDOUT\" % table):\n-            return self.cursor.copy_to(file, table, *args, **kwargs)\n+        def copy_to(self, file, table, *args, **kwargs):\n+            with self.debug_sql(sql=\"COPY %s TO STDOUT\" % table):\n+                return self.cursor.copy_to(file, table, *args, **kwargs)\ndiff --git a/django/db/backends/postgresql/features.py b/django/db/backends/postgresql/features.py\nindex 0eed8c8d63a4..fd5b05aad42a 100644\n--- a/django/db/backends/postgresql/features.py\n+++ b/django/db/backends/postgresql/features.py\n@@ -1,7 +1,8 @@\n import operator\n \n-from django.db import InterfaceError\n+from django.db import DataError, InterfaceError\n from django.db.backends.base.features import BaseDatabaseFeatures\n+from django.db.backends.postgresql.psycopg_any import is_psycopg3\n from django.utils.functional import cached_property\n \n \n@@ -26,6 +27,7 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n     can_introspect_materialized_views = True\n     can_distinct_on_fields = True\n     can_rollback_ddl = True\n+    schema_editor_uses_clientside_param_binding = True\n     supports_combined_alters = True\n     nulls_order_largest = True\n     closed_cursor_error_class = InterfaceError\n@@ -81,6 +83,13 @@ class DatabaseFeatures(BaseDatabaseFeatures):\n         },\n     }\n \n+    @cached_property\n+    def prohibits_null_characters_in_text_exception(self):\n+        if is_psycopg3:\n+            return DataError, \"PostgreSQL text fields cannot contain NUL (0x00) bytes\"\n+        else:\n+            return ValueError, \"A string literal cannot contain NUL (0x00) characters.\"\n+\n     @cached_property\n     def introspected_field_types(self):\n         return {\ndiff --git a/django/db/backends/postgresql/operations.py b/django/db/backends/postgresql/operations.py\nindex 824e0c3e4bcf..18cfcb29cbc5 100644\n--- a/django/db/backends/postgresql/operations.py\n+++ b/django/db/backends/postgresql/operations.py\n@@ -3,9 +3,16 @@\n \n from django.conf import settings\n from django.db.backends.base.operations import BaseDatabaseOperations\n-from django.db.backends.postgresql.psycopg_any import Inet, Jsonb, mogrify\n+from django.db.backends.postgresql.psycopg_any import (\n+    Inet,\n+    Jsonb,\n+    errors,\n+    is_psycopg3,\n+    mogrify,\n+)\n from django.db.backends.utils import split_tzname_delta\n from django.db.models.constants import OnConflict\n+from django.utils.regex_helper import _lazy_re_compile\n \n \n @lru_cache\n@@ -36,6 +43,18 @@ class DatabaseOperations(BaseDatabaseOperations):\n         \"SmallAutoField\": \"smallint\",\n     }\n \n+    if is_psycopg3:\n+        from psycopg.types import numeric\n+\n+        integerfield_type_map = {\n+            \"SmallIntegerField\": numeric.Int2,\n+            \"IntegerField\": numeric.Int4,\n+            \"BigIntegerField\": numeric.Int8,\n+            \"PositiveSmallIntegerField\": numeric.Int2,\n+            \"PositiveIntegerField\": numeric.Int4,\n+            \"PositiveBigIntegerField\": numeric.Int8,\n+        }\n+\n     def unification_cast_sql(self, output_field):\n         internal_type = output_field.get_internal_type()\n         if internal_type in (\n@@ -56,19 +75,23 @@ def unification_cast_sql(self, output_field):\n             )\n         return \"%s\"\n \n+    # EXTRACT format cannot be passed in parameters.\n+    _extract_format_re = _lazy_re_compile(r\"[A-Z_]+\")\n+\n     def date_extract_sql(self, lookup_type, sql, params):\n         # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT\n-        extract_sql = f\"EXTRACT(%s FROM {sql})\"\n-        extract_param = lookup_type\n         if lookup_type == \"week_day\":\n             # For consistency across backends, we return Sunday=1, Saturday=7.\n-            extract_sql = f\"EXTRACT(%s FROM {sql}) + 1\"\n-            extract_param = \"dow\"\n+            return f\"EXTRACT(DOW FROM {sql}) + 1\", params\n         elif lookup_type == \"iso_week_day\":\n-            extract_param = \"isodow\"\n+            return f\"EXTRACT(ISODOW FROM {sql})\", params\n         elif lookup_type == \"iso_year\":\n-            extract_param = \"isoyear\"\n-        return extract_sql, (extract_param, *params)\n+            return f\"EXTRACT(ISOYEAR FROM {sql})\", params\n+\n+        lookup_type = lookup_type.upper()\n+        if not self._extract_format_re.fullmatch(lookup_type):\n+            raise ValueError(f\"Invalid lookup type: {lookup_type!r}\")\n+        return f\"EXTRACT({lookup_type} FROM {sql})\", params\n \n     def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\n         sql, params = self._convert_sql_to_tz(sql, params, tzname)\n@@ -100,10 +123,7 @@ def datetime_extract_sql(self, lookup_type, sql, params, tzname):\n         sql, params = self._convert_sql_to_tz(sql, params, tzname)\n         if lookup_type == \"second\":\n             # Truncate fractional seconds.\n-            return (\n-                f\"EXTRACT(%s FROM DATE_TRUNC(%s, {sql}))\",\n-                (\"second\", \"second\", *params),\n-            )\n+            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n         return self.date_extract_sql(lookup_type, sql, params)\n \n     def datetime_trunc_sql(self, lookup_type, sql, params, tzname):\n@@ -114,10 +134,7 @@ def datetime_trunc_sql(self, lookup_type, sql, params, tzname):\n     def time_extract_sql(self, lookup_type, sql, params):\n         if lookup_type == \"second\":\n             # Truncate fractional seconds.\n-            return (\n-                f\"EXTRACT(%s FROM DATE_TRUNC(%s, {sql}))\",\n-                (\"second\", \"second\", *params),\n-            )\n+            return f\"EXTRACT(SECOND FROM DATE_TRUNC(%s, {sql}))\", (\"second\", *params)\n         return self.date_extract_sql(lookup_type, sql, params)\n \n     def time_trunc_sql(self, lookup_type, sql, params, tzname=None):\n@@ -137,6 +154,16 @@ def fetch_returned_insert_rows(self, cursor):\n     def lookup_cast(self, lookup_type, internal_type=None):\n         lookup = \"%s\"\n \n+        if lookup_type == \"isnull\" and internal_type in (\n+            \"CharField\",\n+            \"EmailField\",\n+            \"TextField\",\n+            \"CICharField\",\n+            \"CIEmailField\",\n+            \"CITextField\",\n+        ):\n+            return \"%s::text\"\n+\n         # Cast text lookups to text to allow things like filter(x__contains=4)\n         if lookup_type in (\n             \"iexact\",\n@@ -178,7 +205,7 @@ def compose_sql(self, sql, params):\n         return mogrify(sql, params, self.connection)\n \n     def set_time_zone_sql(self):\n-        return \"SET TIME ZONE %s\"\n+        return \"SELECT set_config('TimeZone', %s, false)\"\n \n     def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):\n         if not tables:\n@@ -278,12 +305,22 @@ def distinct_sql(self, fields, params):\n         else:\n             return [\"DISTINCT\"], []\n \n-    def last_executed_query(self, cursor, sql, params):\n-        # https://www.psycopg.org/docs/cursor.html#cursor.query\n-        # The query attribute is a Psycopg extension to the DB API 2.0.\n-        if cursor.query is not None:\n-            return cursor.query.decode()\n-        return None\n+    if is_psycopg3:\n+\n+        def last_executed_query(self, cursor, sql, params):\n+            try:\n+                return self.compose_sql(sql, params)\n+            except errors.DataError:\n+                return None\n+\n+    else:\n+\n+        def last_executed_query(self, cursor, sql, params):\n+            # https://www.psycopg.org/docs/cursor.html#cursor.query\n+            # The query attribute is a Psycopg extension to the DB API 2.0.\n+            if cursor.query is not None:\n+                return cursor.query.decode()\n+            return None\n \n     def return_insert_columns(self, fields):\n         if not fields:\n@@ -303,6 +340,13 @@ def bulk_insert_sql(self, fields, placeholder_rows):\n         values_sql = \", \".join(\"(%s)\" % sql for sql in placeholder_rows_sql)\n         return \"VALUES \" + values_sql\n \n+    if is_psycopg3:\n+\n+        def adapt_integerfield_value(self, value, internal_type):\n+            if value is None or hasattr(value, \"resolve_expression\"):\n+                return value\n+            return self.integerfield_type_map[internal_type](value)\n+\n     def adapt_datefield_value(self, value):\n         return value\n \ndiff --git a/django/db/backends/postgresql/psycopg_any.py b/django/db/backends/postgresql/psycopg_any.py\nindex e9bb84f31396..579104deadbf 100644\n--- a/django/db/backends/postgresql/psycopg_any.py\n+++ b/django/db/backends/postgresql/psycopg_any.py\n@@ -1,31 +1,102 @@\n-from enum import IntEnum\n+import ipaddress\n+from functools import lru_cache\n \n-from psycopg2 import errors, extensions, sql  # NOQA\n-from psycopg2.extras import DateRange, DateTimeRange, DateTimeTZRange, Inet  # NOQA\n-from psycopg2.extras import Json as Jsonb  # NOQA\n-from psycopg2.extras import NumericRange, Range  # NOQA\n+try:\n+    from psycopg import ClientCursor, IsolationLevel, adapt, adapters, errors, sql\n+    from psycopg.postgres import types\n+    from psycopg.types.datetime import TimestamptzLoader\n+    from psycopg.types.json import Jsonb\n+    from psycopg.types.range import Range, RangeDumper\n+    from psycopg.types.string import TextLoader\n \n-RANGE_TYPES = (DateRange, DateTimeRange, DateTimeTZRange, NumericRange)\n+    Inet = ipaddress.ip_address\n \n+    DateRange = DateTimeRange = DateTimeTZRange = NumericRange = Range\n+    RANGE_TYPES = (Range,)\n \n-class IsolationLevel(IntEnum):\n-    READ_UNCOMMITTED = extensions.ISOLATION_LEVEL_READ_UNCOMMITTED\n-    READ_COMMITTED = extensions.ISOLATION_LEVEL_READ_COMMITTED\n-    REPEATABLE_READ = extensions.ISOLATION_LEVEL_REPEATABLE_READ\n-    SERIALIZABLE = extensions.ISOLATION_LEVEL_SERIALIZABLE\n+    TSRANGE_OID = types[\"tsrange\"].oid\n+    TSTZRANGE_OID = types[\"tstzrange\"].oid\n \n+    def mogrify(sql, params, connection):\n+        return ClientCursor(connection.connection).mogrify(sql, params)\n \n-def _quote(value, connection=None):\n-    adapted = extensions.adapt(value)\n-    if hasattr(adapted, \"encoding\"):\n-        adapted.encoding = \"utf8\"\n-    # getquoted() returns a quoted bytestring of the adapted value.\n-    return adapted.getquoted().decode()\n+    # Adapters.\n+    class BaseTzLoader(TimestamptzLoader):\n+        \"\"\"\n+        Load a PostgreSQL timestamptz using the a specific timezone.\n+        The timezone can be None too, in which case it will be chopped.\n+        \"\"\"\n \n+        timezone = None\n \n-sql.quote = _quote\n+        def load(self, data):\n+            res = super().load(data)\n+            return res.replace(tzinfo=self.timezone)\n \n+    def register_tzloader(tz, context):\n+        class SpecificTzLoader(BaseTzLoader):\n+            timezone = tz\n \n-def mogrify(sql, params, connection):\n-    with connection.cursor() as cursor:\n-        return cursor.mogrify(sql, params).decode()\n+        context.adapters.register_loader(\"timestamptz\", SpecificTzLoader)\n+\n+    class DjangoRangeDumper(RangeDumper):\n+        \"\"\"A Range dumper customized for Django.\"\"\"\n+\n+        def upgrade(self, obj, format):\n+            # Dump ranges containing naive datetimes as tstzrange, because\n+            # Django doesn't use tz-aware ones.\n+            dumper = super().upgrade(obj, format)\n+            if dumper is not self and dumper.oid == TSRANGE_OID:\n+                dumper.oid = TSTZRANGE_OID\n+            return dumper\n+\n+    @lru_cache\n+    def get_adapters_template(use_tz, timezone):\n+        # Create at adapters map extending the base one.\n+        ctx = adapt.AdaptersMap(adapters)\n+        # Register a no-op dumper to avoid a round trip from psycopg version 3\n+        # decode to json.dumps() to json.loads(), when using a custom decoder\n+        # in JSONField.\n+        ctx.register_loader(\"jsonb\", TextLoader)\n+        # Don't convert automatically from PostgreSQL network types to Python\n+        # ipaddress.\n+        ctx.register_loader(\"inet\", TextLoader)\n+        ctx.register_loader(\"cidr\", TextLoader)\n+        ctx.register_dumper(Range, DjangoRangeDumper)\n+        # Register a timestamptz loader configured on self.timezone.\n+        # This, however, can be overridden by create_cursor.\n+        register_tzloader(timezone, ctx)\n+        return ctx\n+\n+    is_psycopg3 = True\n+\n+except ImportError:\n+    from enum import IntEnum\n+\n+    from psycopg2 import errors, extensions, sql  # NOQA\n+    from psycopg2.extras import DateRange, DateTimeRange, DateTimeTZRange, Inet  # NOQA\n+    from psycopg2.extras import Json as Jsonb  # NOQA\n+    from psycopg2.extras import NumericRange, Range  # NOQA\n+\n+    RANGE_TYPES = (DateRange, DateTimeRange, DateTimeTZRange, NumericRange)\n+\n+    class IsolationLevel(IntEnum):\n+        READ_UNCOMMITTED = extensions.ISOLATION_LEVEL_READ_UNCOMMITTED\n+        READ_COMMITTED = extensions.ISOLATION_LEVEL_READ_COMMITTED\n+        REPEATABLE_READ = extensions.ISOLATION_LEVEL_REPEATABLE_READ\n+        SERIALIZABLE = extensions.ISOLATION_LEVEL_SERIALIZABLE\n+\n+    def _quote(value, connection=None):\n+        adapted = extensions.adapt(value)\n+        if hasattr(adapted, \"encoding\"):\n+            adapted.encoding = \"utf8\"\n+        # getquoted() returns a quoted bytestring of the adapted value.\n+        return adapted.getquoted().decode()\n+\n+    sql.quote = _quote\n+\n+    def mogrify(sql, params, connection):\n+        with connection.cursor() as cursor:\n+            return cursor.mogrify(sql, params).decode()\n+\n+    is_psycopg3 = False\ndiff --git a/django/db/backends/postgresql/schema.py b/django/db/backends/postgresql/schema.py\nindex cc0da8581746..1bd72bc0cb0b 100644\n--- a/django/db/backends/postgresql/schema.py\n+++ b/django/db/backends/postgresql/schema.py\n@@ -40,6 +40,14 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n     )\n     sql_delete_procedure = \"DROP FUNCTION %(procedure)s(%(param_types)s)\"\n \n+    def execute(self, sql, params=()):\n+        # Merge the query client-side, as PostgreSQL won't do it server-side.\n+        if params is None:\n+            return super().execute(sql, params)\n+        sql = self.connection.ops.compose_sql(str(sql), params)\n+        # Don't let the superclass touch anything.\n+        return super().execute(sql, None)\n+\n     sql_add_identity = (\n         \"ALTER TABLE %(table)s ALTER COLUMN %(column)s ADD \"\n         \"GENERATED BY DEFAULT AS IDENTITY\"\ndiff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex cd995de80fb6..991db8d707c1 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -2019,6 +2019,10 @@ def get_prep_value(self, value):\n                 \"Field '%s' expected a number but got %r.\" % (self.name, value),\n             ) from e\n \n+    def get_db_prep_value(self, value, connection, prepared=False):\n+        value = super().get_db_prep_value(value, connection, prepared)\n+        return connection.ops.adapt_integerfield_value(value, self.get_internal_type())\n+\n     def get_internal_type(self):\n         return \"IntegerField\"\n \ndiff --git a/django/db/models/functions/comparison.py b/django/db/models/functions/comparison.py\nindex eb1f20a77cf5..de7eef4cdce6 100644\n--- a/django/db/models/functions/comparison.py\n+++ b/django/db/models/functions/comparison.py\n@@ -1,6 +1,7 @@\n \"\"\"Database functions that do comparisons or type conversions.\"\"\"\n from django.db import NotSupportedError\n from django.db.models.expressions import Func, Value\n+from django.db.models.fields import TextField\n from django.db.models.fields.json import JSONField\n from django.utils.regex_helper import _lazy_re_compile\n \n@@ -158,7 +159,14 @@ def as_sql(self, compiler, connection, **extra_context):\n         return super().as_sql(compiler, connection, **extra_context)\n \n     def as_postgresql(self, compiler, connection, **extra_context):\n-        return self.as_sql(\n+        copy = self.copy()\n+        copy.set_source_expressions(\n+            [\n+                Cast(expression, TextField()) if index % 2 == 0 else expression\n+                for index, expression in enumerate(copy.get_source_expressions())\n+            ]\n+        )\n+        return super(JSONObject, copy).as_sql(\n             compiler,\n             connection,\n             function=\"JSONB_BUILD_OBJECT\",\ndiff --git a/django/db/models/functions/text.py b/django/db/models/functions/text.py\nindex a54ce8f19b27..34a1e81982cd 100644\n--- a/django/db/models/functions/text.py\n+++ b/django/db/models/functions/text.py\n@@ -1,7 +1,7 @@\n from django.db import NotSupportedError\n from django.db.models.expressions import Func, Value\n-from django.db.models.fields import CharField, IntegerField\n-from django.db.models.functions import Coalesce\n+from django.db.models.fields import CharField, IntegerField, TextField\n+from django.db.models.functions import Cast, Coalesce\n from django.db.models.lookups import Transform\n \n \n@@ -82,6 +82,20 @@ def as_sqlite(self, compiler, connection, **extra_context):\n             **extra_context,\n         )\n \n+    def as_postgresql(self, compiler, connection, **extra_context):\n+        copy = self.copy()\n+        copy.set_source_expressions(\n+            [\n+                Cast(expression, TextField())\n+                for expression in copy.get_source_expressions()\n+            ]\n+        )\n+        return super(ConcatPair, copy).as_sql(\n+            compiler,\n+            connection,\n+            **extra_context,\n+        )\n+\n     def as_mysql(self, compiler, connection, **extra_context):\n         # Use CONCAT_WS with an empty separator so that NULLs are ignored.\n         return super().as_sql(\ndiff --git a/django/db/models/lookups.py b/django/db/models/lookups.py\nindex dded81da82f5..9e2d9373e643 100644\n--- a/django/db/models/lookups.py\n+++ b/django/db/models/lookups.py\n@@ -568,7 +568,7 @@ def as_sql(self, compiler, connection):\n             raise ValueError(\n                 \"The QuerySet value for an isnull lookup must be True or False.\"\n             )\n-        sql, params = compiler.compile(self.lhs)\n+        sql, params = self.process_lhs(compiler, connection)\n         if self.rhs:\n             return \"%s IS NULL\" % sql, params\n         else:\ndiff --git a/docs/conf.py b/docs/conf.py\nindex 3e3750933795..8bb6530b75e4 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -174,7 +174,7 @@ def django_release():\n intersphinx_mapping = {\n     \"python\": (\"https://docs.python.org/3/\", None),\n     \"sphinx\": (\"https://www.sphinx-doc.org/en/master/\", None),\n-    \"psycopg2\": (\"https://www.psycopg.org/docs/\", None),\n+    \"psycopg\": (\"https://www.psycopg.org/psycopg3/docs/\", None),\n }\n \n # Python's docs don't change every week.\ndiff --git a/docs/ref/contrib/gis/install/index.txt b/docs/ref/contrib/gis/install/index.txt\nindex 51b3235279e9..026c64ccd103 100644\n--- a/docs/ref/contrib/gis/install/index.txt\n+++ b/docs/ref/contrib/gis/install/index.txt\n@@ -429,14 +429,14 @@ Install Django and set up database\n recommended that you create a :doc:`virtual environment\n <python:tutorial/venv>` for each project you create.\n \n-psycopg2\n-~~~~~~~~\n+psycopg\n+~~~~~~~\n \n-The ``psycopg2`` Python module provides the interface between Python and the\n-PostgreSQL database. ``psycopg2`` can be installed via pip within your Python\n+The ``psycopg`` Python module provides the interface between Python and the\n+PostgreSQL database. ``psycopg`` can be installed via pip within your Python\n virtual environment::\n \n-    ...\\> py -m pip install psycopg2\n+    ...\\> py -m pip install psycopg\n \n .. rubric:: Footnotes\n .. [#] GeoDjango uses the :func:`~ctypes.util.find_library` routine from\ndiff --git a/docs/ref/contrib/gis/install/postgis.txt b/docs/ref/contrib/gis/install/postgis.txt\nindex 0c7d20ae8483..e469ede4d0c1 100644\n--- a/docs/ref/contrib/gis/install/postgis.txt\n+++ b/docs/ref/contrib/gis/install/postgis.txt\n@@ -7,20 +7,25 @@ into a spatial database. :ref:`geosbuild`, :ref:`proj4` and\n :ref:`gdalbuild` should be installed prior to building PostGIS. You\n might also need additional libraries, see `PostGIS requirements`_.\n \n-The `psycopg2`_ module is required for use as the database adapter when using\n-GeoDjango with PostGIS.\n+The `psycopg`_ or `psycopg2`_ module is required for use as the database\n+adapter when using GeoDjango with PostGIS.\n \n On Debian/Ubuntu, you are advised to install the following packages:\n ``postgresql-x``, ``postgresql-x-postgis-3``, ``postgresql-server-dev-x``,\n-and ``python3-psycopg2`` (x matching the PostgreSQL version you want to\n+and ``python3-psycopg3`` (x matching the PostgreSQL version you want to\n install). Alternately, you can `build from source`_. Consult the\n platform-specific instructions if you are on :ref:`macos` or :ref:`windows`.\n \n .. _PostGIS: https://postgis.net/\n+.. _psycopg: https://www.psycopg.org/psycopg3/\n .. _psycopg2: https://www.psycopg.org/\n .. _PostGIS requirements: https://postgis.net/docs/postgis_installation.html#install_requirements\n .. _build from source: https://postgis.net/docs/postgis_installation.html#install_short_version\n \n+.. versionchanged:: 4.2\n+\n+    Support for ``psycopg`` 3.1+ was added.\n+\n Post-installation\n =================\n \ndiff --git a/docs/ref/contrib/postgres/fields.txt b/docs/ref/contrib/postgres/fields.txt\nindex 2e6ce4253e7a..34ad06a09a20 100644\n--- a/docs/ref/contrib/postgres/fields.txt\n+++ b/docs/ref/contrib/postgres/fields.txt\n@@ -538,8 +538,8 @@ PostgreSQL. These fields are used to store a range of values; for example the\n start and end timestamps of an event, or the range of ages an activity is\n suitable for.\n \n-All of the range fields translate to :ref:`psycopg2 Range objects\n-<psycopg2:adapt-range>` in Python, but also accept tuples as input if no bounds\n+All of the range fields translate to :ref:`psycopg Range objects\n+<psycopg:adapt-range>` in Python, but also accept tuples as input if no bounds\n information is necessary. The default is lower bound included, upper bound\n excluded, that is ``[)`` (see the PostgreSQL documentation for details about\n `different bounds`_). The default bounds can be changed for non-discrete range\n@@ -553,8 +553,8 @@ the ``default_bounds`` argument.\n \n     Stores a range of integers. Based on an\n     :class:`~django.db.models.IntegerField`. Represented by an ``int4range`` in\n-    the database and a :class:`~psycopg2:psycopg2.extras.NumericRange` in\n-    Python.\n+    the database and a\n+    ``django.db.backends.postgresql.psycopg_any.NumericRange`` in Python.\n \n     Regardless of the bounds specified when saving the data, PostgreSQL always\n     returns a range in a canonical form that includes the lower bound and\n@@ -567,8 +567,8 @@ the ``default_bounds`` argument.\n \n     Stores a range of large integers. Based on a\n     :class:`~django.db.models.BigIntegerField`. Represented by an ``int8range``\n-    in the database and a :class:`~psycopg2:psycopg2.extras.NumericRange` in\n-    Python.\n+    in the database and a\n+    ``django.db.backends.postgresql.psycopg_any.NumericRange`` in Python.\n \n     Regardless of the bounds specified when saving the data, PostgreSQL always\n     returns a range in a canonical form that includes the lower bound and\n@@ -581,8 +581,8 @@ the ``default_bounds`` argument.\n \n     Stores a range of floating point values. Based on a\n     :class:`~django.db.models.DecimalField`. Represented by a ``numrange`` in\n-    the database and a :class:`~psycopg2:psycopg2.extras.NumericRange` in\n-    Python.\n+    the database and a\n+    ``django.db.backends.postgresql.psycopg_any.NumericRange`` in Python.\n \n     .. attribute:: DecimalRangeField.default_bounds\n \n@@ -592,7 +592,7 @@ the ``default_bounds`` argument.\n         default is lower bound included, upper bound excluded, that is ``[)``\n         (see the PostgreSQL documentation for details about\n         `different bounds`_). ``default_bounds`` is not used for\n-        :class:`~psycopg2:psycopg2.extras.NumericRange` inputs.\n+        ``django.db.backends.postgresql.psycopg_any.NumericRange`` inputs.\n \n ``DateTimeRangeField``\n ----------------------\n@@ -601,8 +601,8 @@ the ``default_bounds`` argument.\n \n     Stores a range of timestamps. Based on a\n     :class:`~django.db.models.DateTimeField`. Represented by a ``tstzrange`` in\n-    the database and a :class:`~psycopg2:psycopg2.extras.DateTimeTZRange` in\n-    Python.\n+    the database and a\n+    ``django.db.backends.postgresql.psycopg_any.DateTimeTZRange`` in Python.\n \n     .. attribute:: DateTimeRangeField.default_bounds\n \n@@ -612,7 +612,7 @@ the ``default_bounds`` argument.\n         default is lower bound included, upper bound excluded, that is ``[)``\n         (see the PostgreSQL documentation for details about\n         `different bounds`_). ``default_bounds`` is not used for\n-        :class:`~psycopg2:psycopg2.extras.DateTimeTZRange` inputs.\n+        ``django.db.backends.postgresql.psycopg_any.DateTimeTZRange`` inputs.\n \n ``DateRangeField``\n ------------------\n@@ -621,7 +621,8 @@ the ``default_bounds`` argument.\n \n     Stores a range of dates. Based on a\n     :class:`~django.db.models.DateField`. Represented by a ``daterange`` in the\n-    database and a :class:`~psycopg2:psycopg2.extras.DateRange` in Python.\n+    database and a ``django.db.backends.postgresql.psycopg_any.DateRange`` in\n+    Python.\n \n     Regardless of the bounds specified when saving the data, PostgreSQL always\n     returns a range in a canonical form that includes the lower bound and\n@@ -655,7 +656,7 @@ We will also use the following example objects::\n \n and ``NumericRange``:\n \n-    >>> from psycopg2.extras import NumericRange\n+    >>> from django.db.backends.postgresql.psycopg_any import NumericRange\n \n Containment functions\n ~~~~~~~~~~~~~~~~~~~~~\n@@ -690,7 +691,7 @@ The ``contained_by`` lookup is also available on the non-range field types:\n :class:`~django.db.models.DateField`, and\n :class:`~django.db.models.DateTimeField`. For example::\n \n-    >>> from psycopg2.extras import DateTimeTZRange\n+    >>> from django.db.backends.postgresql.psycopg_any import DateTimeTZRange\n     >>> Event.objects.filter(\n     ...     start__contained_by=DateTimeTZRange(\n     ...         timezone.now() - datetime.timedelta(hours=1),\n@@ -864,9 +865,9 @@ Defining your own range types\n -----------------------------\n \n PostgreSQL allows the definition of custom range types. Django's model and form\n-field implementations use base classes below, and psycopg2 provides a\n-:func:`~psycopg2:psycopg2.extras.register_range` to allow use of custom range\n-types.\n+field implementations use base classes below, and ``psycopg`` provides a\n+:func:`~psycopg:psycopg.types.range.register_range` to allow use of custom\n+range types.\n \n .. class:: RangeField(**options)\n \n@@ -878,7 +879,7 @@ types.\n \n     .. attribute:: range_type\n \n-        The psycopg2 range type to use.\n+        The range type to use.\n \n     .. attribute:: form_field\n \n@@ -895,7 +896,7 @@ types.\n \n     .. attribute:: range_type\n \n-        The psycopg2 range type to use.\n+        The range type to use.\n \n Range operators\n ---------------\ndiff --git a/docs/ref/contrib/postgres/forms.txt b/docs/ref/contrib/postgres/forms.txt\nindex e5d597655f7f..8f9dd449d19e 100644\n--- a/docs/ref/contrib/postgres/forms.txt\n+++ b/docs/ref/contrib/postgres/forms.txt\n@@ -173,7 +173,7 @@ not greater than the upper bound. All of these fields use\n .. class:: IntegerRangeField\n \n     Based on :class:`~django.forms.IntegerField` and translates its input into\n-    :class:`~psycopg2:psycopg2.extras.NumericRange`. Default for\n+    ``django.db.backends.postgresql.psycopg_any.NumericRange``. Default for\n     :class:`~django.contrib.postgres.fields.IntegerRangeField` and\n     :class:`~django.contrib.postgres.fields.BigIntegerRangeField`.\n \n@@ -183,7 +183,7 @@ not greater than the upper bound. All of these fields use\n .. class:: DecimalRangeField\n \n     Based on :class:`~django.forms.DecimalField` and translates its input into\n-    :class:`~psycopg2:psycopg2.extras.NumericRange`. Default for\n+    ``django.db.backends.postgresql.psycopg_any.NumericRange``. Default for\n     :class:`~django.contrib.postgres.fields.DecimalRangeField`.\n \n ``DateTimeRangeField``\n@@ -192,7 +192,7 @@ not greater than the upper bound. All of these fields use\n .. class:: DateTimeRangeField\n \n     Based on :class:`~django.forms.DateTimeField` and translates its input into\n-    :class:`~psycopg2:psycopg2.extras.DateTimeTZRange`. Default for\n+    ``django.db.backends.postgresql.psycopg_any.DateTimeTZRange``. Default for\n     :class:`~django.contrib.postgres.fields.DateTimeRangeField`.\n \n ``DateRangeField``\n@@ -201,7 +201,7 @@ not greater than the upper bound. All of these fields use\n .. class:: DateRangeField\n \n     Based on :class:`~django.forms.DateField` and translates its input into\n-    :class:`~psycopg2:psycopg2.extras.DateRange`. Default for\n+    ``django.db.backends.postgresql.psycopg_any.DateRange``. Default for\n     :class:`~django.contrib.postgres.fields.DateRangeField`.\n \n Widgets\ndiff --git a/docs/ref/databases.txt b/docs/ref/databases.txt\nindex 79b0386e1a14..d62adbe83220 100644\n--- a/docs/ref/databases.txt\n+++ b/docs/ref/databases.txt\n@@ -114,11 +114,21 @@ below for information on how to set up your database correctly.\n PostgreSQL notes\n ================\n \n-Django supports PostgreSQL 12 and higher. `psycopg2`_ 2.8.4 or higher is\n-required, though the latest release is recommended.\n+Django supports PostgreSQL 12 and higher. `psycopg`_ 3.1+ or `psycopg2`_ 2.8.4+\n+is required, though the latest `psycopg`_ 3.1+ is recommended.\n \n+.. _psycopg: https://www.psycopg.org/psycopg3/\n .. _psycopg2: https://www.psycopg.org/\n \n+.. note::\n+\n+    Support for ``psycopg2`` is likely to be deprecated and removed at some\n+    point in the future.\n+\n+.. versionchanged:: 4.2\n+\n+    Support for ``psycopg`` 3.1+ was added.\n+\n .. _postgresql-connection-settings:\n \n PostgreSQL connection settings\n@@ -199,12 +209,12 @@ level`_. If you need a higher isolation level such as ``REPEATABLE READ`` or\n ``SERIALIZABLE``, set it in the :setting:`OPTIONS` part of your database\n configuration in :setting:`DATABASES`::\n \n-    import psycopg2.extensions\n+    from django.db.backends.postgresql.psycopg_any import IsolationLevel\n \n     DATABASES = {\n         # ...\n         'OPTIONS': {\n-            'isolation_level': psycopg2.extensions.ISOLATION_LEVEL_SERIALIZABLE,\n+            'isolation_level': IsolationLevel.SERIALIZABLE,\n         },\n     }\n \n@@ -216,6 +226,10 @@ configuration in :setting:`DATABASES`::\n \n .. _isolation level: https://www.postgresql.org/docs/current/transaction-iso.html\n \n+.. versionchanged:: 4.2\n+\n+    ``IsolationLevel`` was added.\n+\n Indexes for ``varchar`` and ``text`` columns\n --------------------------------------------\n \n@@ -244,7 +258,7 @@ Server-side cursors\n \n When using :meth:`QuerySet.iterator()\n <django.db.models.query.QuerySet.iterator>`, Django opens a :ref:`server-side\n-cursor <psycopg2:server-side-cursors>`. By default, PostgreSQL assumes that\n+cursor <psycopg:server-side-cursors>`. By default, PostgreSQL assumes that\n only the first 10% of the results of cursor queries will be fetched. The query\n planner spends less time planning the query and starts returning results\n faster, but this could diminish performance if more than 10% of the results are\ndiff --git a/docs/releases/1.11.txt b/docs/releases/1.11.txt\nindex 5da81cd739ed..50b78305d41b 100644\n--- a/docs/releases/1.11.txt\n+++ b/docs/releases/1.11.txt\n@@ -256,10 +256,11 @@ Database backends\n * Added the :setting:`TEST['TEMPLATE'] <TEST_TEMPLATE>` setting to let\n   PostgreSQL users specify a template for creating the test database.\n \n-* :meth:`.QuerySet.iterator()` now uses :ref:`server-side cursors\n-  <psycopg2:server-side-cursors>` on PostgreSQL. This feature transfers some of\n-  the worker memory load (used to hold query results) to the database and might\n-  increase database memory usage.\n+* :meth:`.QuerySet.iterator()` now uses `server-side cursors`_ on PostgreSQL.\n+  This feature transfers some of the worker memory load (used to hold query\n+  results) to the database and might increase database memory usage.\n+\n+  .. _server-side cursors: https://www.psycopg.org/docs/usage.html#server-side-cursors\n \n * Added MySQL support for the ``'isolation_level'`` option in\n   :setting:`OPTIONS` to allow specifying the :ref:`transaction isolation level\ndiff --git a/docs/releases/4.2.txt b/docs/releases/4.2.txt\nindex 3abe03ef8581..077762275a5f 100644\n--- a/docs/releases/4.2.txt\n+++ b/docs/releases/4.2.txt\n@@ -26,6 +26,20 @@ and only officially support the latest release of each series.\n What's new in Django 4.2\n ========================\n \n+Psycopg 3 support\n+-----------------\n+\n+Django now supports `psycopg`_ version 3.1 or higher. To update your code,\n+install the `psycopg library`_, you don't need to change the\n+:setting:`ENGINE <DATABASE-ENGINE>` as ``django.db.backends.postgresql``\n+supports both libraries.\n+\n+Support for ``psycopg2`` is likely to be deprecated and removed at some point\n+in the future.\n+\n+.. _psycopg: https://www.psycopg.org/psycopg3/\n+.. _psycopg library: https://pypi.org/project/psycopg/\n+\n Minor features\n --------------\n \ndiff --git a/docs/topics/db/transactions.txt b/docs/topics/db/transactions.txt\nindex d0b67b86f4bc..b41c9fa758e0 100644\n--- a/docs/topics/db/transactions.txt\n+++ b/docs/topics/db/transactions.txt\n@@ -397,8 +397,8 @@ tasks, etc.), this should be fine. If it's not (if your follow-up action is so\n critical that its failure should mean the failure of the transaction itself),\n then you don't want to use the :func:`on_commit` hook. Instead, you may want\n `two-phase commit`_ such as the :ref:`psycopg Two-Phase Commit protocol support\n-<psycopg2:tpc>` and the :pep:`optional Two-Phase Commit Extensions in the\n-Python DB-API specification <249#optional-two-phase-commit-extensions>`.\n+<psycopg:two-phase-commit>` and the :pep:`optional Two-Phase Commit Extensions\n+in the Python DB-API specification <249#optional-two-phase-commit-extensions>`.\n \n Callbacks are not run until autocommit is restored on the connection following\n the commit (because otherwise any queries done in a callback would open an\ndiff --git a/docs/topics/install.txt b/docs/topics/install.txt\nindex 1590da89062c..bbc74bd4e30d 100644\n--- a/docs/topics/install.txt\n+++ b/docs/topics/install.txt\n@@ -79,8 +79,9 @@ databases with Django.\n In addition to a database backend, you'll need to make sure your Python\n database bindings are installed.\n \n-* If you're using PostgreSQL, you'll need the `psycopg2`_ package. Refer to the\n-  :ref:`PostgreSQL notes <postgresql-notes>` for further details.\n+* If you're using PostgreSQL, you'll need the `psycopg`_ or `psycopg2`_\n+  package. Refer to the :ref:`PostgreSQL notes <postgresql-notes>` for further\n+  details.\n \n * If you're using MySQL or MariaDB, you'll need a :ref:`DB API driver\n   <mysql-db-api-drivers>` like ``mysqlclient``. See :ref:`notes for the MySQL\n@@ -111,6 +112,7 @@ database queries, Django will need permission to create a test database.\n .. _PostgreSQL: https://www.postgresql.org/\n .. _MariaDB: https://mariadb.org/\n .. _MySQL: https://www.mysql.com/\n+.. _psycopg: https://www.psycopg.org/psycopg3/\n .. _psycopg2: https://www.psycopg.org/\n .. _SQLite: https://www.sqlite.org/\n .. _cx_Oracle: https://oracle.github.io/python-cx_Oracle/\ndiff --git a/tests/backends/postgresql/tests.py b/tests/backends/postgresql/tests.py\nindex 41d445e6c70b..7e1a2d000d5d 100644\n--- a/tests/backends/postgresql/tests.py\n+++ b/tests/backends/postgresql/tests.py\n@@ -14,6 +14,11 @@\n from django.db.backends.base.base import BaseDatabaseWrapper\n from django.test import TestCase, override_settings\n \n+try:\n+    from django.db.backends.postgresql.psycopg_any import is_psycopg3\n+except ImportError:\n+    is_psycopg3 = False\n+\n \n @unittest.skipUnless(connection.vendor == \"postgresql\", \"PostgreSQL tests\")\n class Tests(TestCase):\n@@ -228,7 +233,7 @@ def test_connect_isolation_level(self):\n         # Since this is a django.test.TestCase, a transaction is in progress\n         # and the isolation level isn't reported as 0. This test assumes that\n         # PostgreSQL is configured with the default isolation level.\n-        # Check the level on the psycopg2 connection, not the Django wrapper.\n+        # Check the level on the psycopg connection, not the Django wrapper.\n         self.assertIsNone(connection.connection.isolation_level)\n \n         new_connection = connection.copy()\n@@ -238,7 +243,7 @@ def test_connect_isolation_level(self):\n         try:\n             # Start a transaction so the isolation level isn't reported as 0.\n             new_connection.set_autocommit(False)\n-            # Check the level on the psycopg2 connection, not the Django wrapper.\n+            # Check the level on the psycopg connection, not the Django wrapper.\n             self.assertEqual(\n                 new_connection.connection.isolation_level,\n                 IsolationLevel.SERIALIZABLE,\n@@ -252,7 +257,7 @@ def test_connect_invalid_isolation_level(self):\n         new_connection.settings_dict[\"OPTIONS\"][\"isolation_level\"] = -1\n         msg = (\n             \"Invalid transaction isolation level -1 specified. Use one of the \"\n-            \"IsolationLevel values.\"\n+            \"psycopg.IsolationLevel values.\"\n         )\n         with self.assertRaisesMessage(ImproperlyConfigured, msg):\n             new_connection.ensure_connection()\n@@ -268,7 +273,7 @@ def test_connect_no_is_usable_checks(self):\n \n     def _select(self, val):\n         with connection.cursor() as cursor:\n-            cursor.execute(\"SELECT %s\", (val,))\n+            cursor.execute(\"SELECT %s::text[]\", (val,))\n             return cursor.fetchone()[0]\n \n     def test_select_ascii_array(self):\n@@ -308,17 +313,18 @@ def test_lookup_cast(self):\n                     )\n \n     def test_correct_extraction_psycopg_version(self):\n-        from django.db.backends.postgresql.base import Database, psycopg2_version\n+        from django.db.backends.postgresql.base import Database, psycopg_version\n \n         with mock.patch.object(Database, \"__version__\", \"4.2.1 (dt dec pq3 ext lo64)\"):\n-            self.assertEqual(psycopg2_version(), (4, 2, 1))\n+            self.assertEqual(psycopg_version(), (4, 2, 1))\n         with mock.patch.object(\n             Database, \"__version__\", \"4.2b0.dev1 (dt dec pq3 ext lo64)\"\n         ):\n-            self.assertEqual(psycopg2_version(), (4, 2))\n+            self.assertEqual(psycopg_version(), (4, 2))\n \n     @override_settings(DEBUG=True)\n-    def test_copy_cursors(self):\n+    @unittest.skipIf(is_psycopg3, \"psycopg2 specific test\")\n+    def test_copy_to_expert_cursors(self):\n         out = StringIO()\n         copy_expert_sql = \"COPY django_session TO STDOUT (FORMAT CSV, HEADER)\"\n         with connection.cursor() as cursor:\n@@ -329,6 +335,16 @@ def test_copy_cursors(self):\n             [copy_expert_sql, \"COPY django_session TO STDOUT\"],\n         )\n \n+    @override_settings(DEBUG=True)\n+    @unittest.skipUnless(is_psycopg3, \"psycopg3 specific test\")\n+    def test_copy_cursors(self):\n+        copy_sql = \"COPY django_session TO STDOUT (FORMAT CSV, HEADER)\"\n+        with connection.cursor() as cursor:\n+            with cursor.copy(copy_sql) as copy:\n+                for row in copy:\n+                    pass\n+        self.assertEqual([q[\"sql\"] for q in connection.queries], [copy_sql])\n+\n     def test_get_database_version(self):\n         new_connection = connection.copy()\n         new_connection.pg_version = 110009\ndiff --git a/tests/backends/tests.py b/tests/backends/tests.py\nindex c3cfa61fdb11..5f11f919583b 100644\n--- a/tests/backends/tests.py\n+++ b/tests/backends/tests.py\n@@ -454,7 +454,7 @@ def test_cursor_contextmanager(self):\n         with connection.cursor() as cursor:\n             self.assertIsInstance(cursor, CursorWrapper)\n         # Both InterfaceError and ProgrammingError seem to be used when\n-        # accessing closed cursor (psycopg2 has InterfaceError, rest seem\n+        # accessing closed cursor (psycopg has InterfaceError, rest seem\n         # to use ProgrammingError).\n         with self.assertRaises(connection.features.closed_cursor_error_class):\n             # cursor should be closed, so no queries should be possible.\n@@ -462,12 +462,12 @@ def test_cursor_contextmanager(self):\n \n     @unittest.skipUnless(\n         connection.vendor == \"postgresql\",\n-        \"Psycopg2 specific cursor.closed attribute needed\",\n+        \"Psycopg specific cursor.closed attribute needed\",\n     )\n     def test_cursor_contextmanager_closing(self):\n         # There isn't a generic way to test that cursors are closed, but\n-        # psycopg2 offers us a way to check that by closed attribute.\n-        # So, run only on psycopg2 for that reason.\n+        # psycopg offers us a way to check that by closed attribute.\n+        # So, run only on psycopg for that reason.\n         with connection.cursor() as cursor:\n             self.assertIsInstance(cursor, CursorWrapper)\n         self.assertTrue(cursor.closed)\ndiff --git a/tests/db_functions/datetime/test_extract_trunc.py b/tests/db_functions/datetime/test_extract_trunc.py\nindex 2d382349818f..b2327931f0ef 100644\n--- a/tests/db_functions/datetime/test_extract_trunc.py\n+++ b/tests/db_functions/datetime/test_extract_trunc.py\n@@ -245,7 +245,7 @@ def test_extract_lookup_name_sql_injection(self):\n         self.create_model(start_datetime, end_datetime)\n         self.create_model(end_datetime, start_datetime)\n \n-        with self.assertRaises((DataError, OperationalError, ValueError)):\n+        with self.assertRaises((OperationalError, ValueError)):\n             DTModel.objects.filter(\n                 start_datetime__year=Extract(\n                     \"start_datetime\", \"day' FROM start_datetime)) OR 1=1;--\"\ndiff --git a/tests/db_utils/tests.py b/tests/db_utils/tests.py\nindex 9c0ec905cc35..a2d9cc7b5e89 100644\n--- a/tests/db_utils/tests.py\n+++ b/tests/db_utils/tests.py\n@@ -62,14 +62,20 @@ def test_nonexistent_alias(self):\n class DatabaseErrorWrapperTests(TestCase):\n     @unittest.skipUnless(connection.vendor == \"postgresql\", \"PostgreSQL test\")\n     def test_reraising_backend_specific_database_exception(self):\n+        from django.db.backends.postgresql.psycopg_any import is_psycopg3\n+\n         with connection.cursor() as cursor:\n             msg = 'table \"X\" does not exist'\n             with self.assertRaisesMessage(ProgrammingError, msg) as cm:\n                 cursor.execute('DROP TABLE \"X\"')\n         self.assertNotEqual(type(cm.exception), type(cm.exception.__cause__))\n         self.assertIsNotNone(cm.exception.__cause__)\n-        self.assertIsNotNone(cm.exception.__cause__.pgcode)\n-        self.assertIsNotNone(cm.exception.__cause__.pgerror)\n+        if is_psycopg3:\n+            self.assertIsNotNone(cm.exception.__cause__.diag.sqlstate)\n+            self.assertIsNotNone(cm.exception.__cause__.diag.message_primary)\n+        else:\n+            self.assertIsNotNone(cm.exception.__cause__.pgcode)\n+            self.assertIsNotNone(cm.exception.__cause__.pgerror)\n \n \n class LoadBackendTests(SimpleTestCase):\ndiff --git a/tests/fixtures/tests.py b/tests/fixtures/tests.py\nindex 9eb2740c90ec..deac1c2d7799 100644\n--- a/tests/fixtures/tests.py\n+++ b/tests/fixtures/tests.py\n@@ -916,15 +916,11 @@ def test_loaddata_error_message(self):\n         with self.assertRaisesMessage(IntegrityError, msg):\n             management.call_command(\"loaddata\", \"invalid.json\", verbosity=0)\n \n-    @unittest.skipUnless(\n-        connection.vendor == \"postgresql\", \"psycopg2 prohibits null characters in data.\"\n-    )\n+    @skipUnlessDBFeature(\"prohibits_null_characters_in_text_exception\")\n     def test_loaddata_null_characters_on_postgresql(self):\n-        msg = (\n-            \"Could not load fixtures.Article(pk=2): \"\n-            \"A string literal cannot contain NUL (0x00) characters.\"\n-        )\n-        with self.assertRaisesMessage(ValueError, msg):\n+        error, msg = connection.features.prohibits_null_characters_in_text_exception\n+        msg = f\"Could not load fixtures.Article(pk=2): {msg}\"\n+        with self.assertRaisesMessage(error, msg):\n             management.call_command(\"loaddata\", \"null_character_in_field_value.json\")\n \n     def test_loaddata_app_option(self):\ndiff --git a/tests/gis_tests/tests.py b/tests/gis_tests/tests.py\nindex d1c93592a834..9da2b4df99e3 100644\n--- a/tests/gis_tests/tests.py\n+++ b/tests/gis_tests/tests.py\n@@ -36,7 +36,7 @@ def _get_postgis_func(self, func):\n                 raise NotImplementedError(\"This function was not expected to be called\")\n \n \n-@unittest.skipUnless(HAS_POSTGRES, \"The psycopg2 driver is needed for these tests\")\n+@unittest.skipUnless(HAS_POSTGRES, \"The psycopg driver is needed for these tests\")\n class TestPostGISVersionCheck(unittest.TestCase):\n     \"\"\"\n     The PostGIS version check parses correctly the version numbers\ndiff --git a/tests/model_fields/test_jsonfield.py b/tests/model_fields/test_jsonfield.py\nindex 05816817ef6a..60357d87b2db 100644\n--- a/tests/model_fields/test_jsonfield.py\n+++ b/tests/model_fields/test_jsonfield.py\n@@ -1007,7 +1007,7 @@ def test_key_sql_injection(self):\n                 False,\n             )\n         self.assertIn(\n-            \"\"\".\"value\" -> 'test'' = ''\"a\"'') OR 1 = 1 OR (''d') = '\"x\"' \"\"\",\n+            \"\"\".\"value\" -> 'test'' = ''\"a\"'') OR 1 = 1 OR (''d') = '\"x\"'\"\"\",\n             queries[0][\"sql\"],\n         )\n \ndiff --git a/tests/postgres_tests/test_apps.py b/tests/postgres_tests/test_apps.py\nindex 7c4cc38183f7..d9fb9622516b 100644\n--- a/tests/postgres_tests/test_apps.py\n+++ b/tests/postgres_tests/test_apps.py\n@@ -19,6 +19,7 @@\n         DateTimeRange,\n         DateTimeTZRange,\n         NumericRange,\n+        is_psycopg3,\n     )\n except ImportError:\n     pass\n@@ -59,6 +60,7 @@ def assertNotSerializable():\n                         MigrationWriter.serialize(field)\n \n         assertNotSerializable()\n+        import_name = \"psycopg.types.range\" if is_psycopg3 else \"psycopg2.extras\"\n         with self.modify_settings(INSTALLED_APPS={\"append\": \"django.contrib.postgres\"}):\n             for default, test_field in tests:\n                 with self.subTest(default=default):\n@@ -68,16 +70,12 @@ def assertNotSerializable():\n                         imports,\n                         {\n                             \"import django.contrib.postgres.fields.ranges\",\n-                            \"import psycopg2.extras\",\n+                            f\"import {import_name}\",\n                         },\n                     )\n                     self.assertIn(\n-                        \"%s.%s(default=psycopg2.extras.%r)\"\n-                        % (\n-                            field.__module__,\n-                            field.__class__.__name__,\n-                            default,\n-                        ),\n+                        f\"{field.__module__}.{field.__class__.__name__}\"\n+                        f\"(default={import_name}.{default!r})\",\n                         serialized_field,\n                     )\n         assertNotSerializable()\ndiff --git a/tests/postgres_tests/test_array.py b/tests/postgres_tests/test_array.py\nindex a3c26fddaeb0..86e9d00b41c4 100644\n--- a/tests/postgres_tests/test_array.py\n+++ b/tests/postgres_tests/test_array.py\n@@ -317,7 +317,7 @@ def test_in_subquery(self):\n     def test_in_including_F_object(self):\n         # This test asserts that Array objects passed to filters can be\n         # constructed to contain F objects. This currently doesn't work as the\n-        # psycopg2 mogrify method that generates the ARRAY() syntax is\n+        # psycopg mogrify method that generates the ARRAY() syntax is\n         # expecting literals, not column references (#27095).\n         self.assertSequenceEqual(\n             NullableIntegerArrayModel.objects.filter(field__in=[[models.F(\"id\")]]),\ndiff --git a/tests/requirements/postgres.txt b/tests/requirements/postgres.txt\nindex f0288c8b74e5..726a08b3e444 100644\n--- a/tests/requirements/postgres.txt\n+++ b/tests/requirements/postgres.txt\n@@ -1 +1 @@\n-psycopg2>=2.8.4\n+psycopg[binary]>=3.1\ndiff --git a/tests/schema/test_logging.py b/tests/schema/test_logging.py\nindex 2821e5f406c6..9c7069c87438 100644\n--- a/tests/schema/test_logging.py\n+++ b/tests/schema/test_logging.py\n@@ -9,9 +9,9 @@ def test_extra_args(self):\n         params = [42, 1337]\n         with self.assertLogs(\"django.db.backends.schema\", \"DEBUG\") as cm:\n             editor.execute(sql, params)\n+        if connection.features.schema_editor_uses_clientside_param_binding:\n+            sql = \"SELECT * FROM foo WHERE id in (42, 1337)\"\n+            params = None\n         self.assertEqual(cm.records[0].sql, sql)\n         self.assertEqual(cm.records[0].params, params)\n-        self.assertEqual(\n-            cm.records[0].getMessage(),\n-            \"SELECT * FROM foo WHERE id in (%s, %s); (params [42, 1337])\",\n-        )\n+        self.assertEqual(cm.records[0].getMessage(), f\"{sql}; (params {params})\")\n"
  },
  {
    "index": 26,
    "filtered_comments": [
      "Hey @carltongibson! \r\nSo I was going trying to work on the coverages of each backend. However, one tests keeps failing. \r\n`cache.tests.CreateCacheTableForDBCacheTests.test_createcachetable_observes_database_router`\r\n```\r\n======================================================================\r\nFAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n    return func(*args, **kwargs)\r\n  File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n    management.call_command('createcachetable', database='other', verbosity=0)\r\n  File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n    self.test_case.assertEqual(\r\nAssertionError: 1 != 5 : 1 queries executed, 5 expected\r\nCaptured queries were:\r\n1. \r\n            SELECT c.relname,\r\n            CASE WHEN c.relispartition THEN 'p' WHEN c.relkind IN ('m', 'v') THEN 'v' ELSE 't' END\r\n            FROM pg_catalog.pg_class c\r\n            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\r\n            WHERE c.relkind IN ('f', 'm', 'p', 'r', 'v')\r\n                AND n.nspname NOT IN ('pg_catalog', 'pg_toast')\r\n                AND pg_catalog.pg_table_is_visible(c.oid)\r\n        \r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nI did not completely understand the reason for this. \r\nMy test settings were\r\n```\r\nDATABASES = {\r\n    'default': {\r\n        'ENGINE': 'django.db.backends.postgresql',\r\n        'NAME': 'mydb_default',\r\n        'USER': 'myuser',\r\n        'PASSWORD': 'password',\r\n        'HOST': 'localhost',\r\n        'PORT': '5432',\r\n    },\r\n    'other': {\r\n        'ENGINE': 'django.db.backends.postgresql',\r\n        'NAME': 'mydb_other',\r\n        'USER': 'myuser',\r\n        'PASSWORD': 'password',\r\n        'HOST': 'localhost',\r\n        'PORT': '5432',\r\n    }\r\n}\r\n\r\nCACHES = {\r\n    \"default\": {\r\n        \"BACKEND\": \"django.core.cache.backends.db.DatabaseCache\",\r\n        \"LOCATION\": \"my_cache_table\",\r\n    },\r\n}\r\n```\r\nTried and tested with SQLite as well, and got the same results. \r\nError with SQLite\r\n```\r\n======================================================================\r\nFAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n    return func(*args, **kwargs)\r\n  File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n    management.call_command('createcachetable', database='other', verbosity=0)\r\n  File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n    self.test_case.assertEqual(\r\nAssertionError: 1 != 5 : 1 queries executed, 5 expected\r\nCaptured queries were:\r\n1. \r\n            SELECT name, type FROM sqlite_master\r\n            WHERE type in ('table', 'view') AND NOT name='sqlite_sequence'\r\n            ORDER BY name\r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nHowever, when I comment out this line, which call `createcachetable`, the test passes.\r\nhttps://github.com/django/django/blob/d270dd584e0af12fe6229fb712d0704c232dc7e5/django/db/backends/base/creation.py#L92\r\n",
      "Hey @carltongibson !\r\nI've just pushed the lastest update that I have. I've adapted the existsing tests for the new backend. The tests are failing at two instance\r\n- Culling\r\n- zero and negative timeout handling : Redis-py does not support 0 or negative timeouts. I have implemented the `get_backend_timeout` similar to the memcache backend but I'm still not sure about how to handle 0 timeout. Ideally it should not store the key in the first place.",
      "> Hey @carltongibson!\r\n> So I was going trying to work on the coverages of each backend. However, one tests keeps failing.\r\n> `cache.tests.CreateCacheTableForDBCacheTests.test_createcachetable_observes_database_router`\r\n> \r\n> ```\r\n> ======================================================================\r\n> FAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n> ----------------------------------------------------------------------\r\n> Traceback (most recent call last):\r\n>   File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n>     return func(*args, **kwargs)\r\n>   File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n>     management.call_command('createcachetable', database='other', verbosity=0)\r\n>   File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n>     self.test_case.assertEqual(\r\n> AssertionError: 1 != 5 : 1 queries executed, 5 expected\r\n> Captured queries were:\r\n> 1. \r\n>             SELECT c.relname,\r\n>             CASE WHEN c.relispartition THEN 'p' WHEN c.relkind IN ('m', 'v') THEN 'v' ELSE 't' END\r\n>             FROM pg_catalog.pg_class c\r\n>             LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\r\n>             WHERE c.relkind IN ('f', 'm', 'p', 'r', 'v')\r\n>                 AND n.nspname NOT IN ('pg_catalog', 'pg_toast')\r\n>                 AND pg_catalog.pg_table_is_visible(c.oid)\r\n>         \r\n> \r\n> ----------------------------------------------------------------------\r\n> ```\r\n> \r\n> I did not completely understand the reason for this.\r\n> My test settings were\r\n> \r\n> ```\r\n> DATABASES = {\r\n>     'default': {\r\n>         'ENGINE': 'django.db.backends.postgresql',\r\n>         'NAME': 'mydb_default',\r\n>         'USER': 'myuser',\r\n>         'PASSWORD': 'password',\r\n>         'HOST': 'localhost',\r\n>         'PORT': '5432',\r\n>     },\r\n>     'other': {\r\n>         'ENGINE': 'django.db.backends.postgresql',\r\n>         'NAME': 'mydb_other',\r\n>         'USER': 'myuser',\r\n>         'PASSWORD': 'password',\r\n>         'HOST': 'localhost',\r\n>         'PORT': '5432',\r\n>     }\r\n> }\r\n> \r\n> CACHES = {\r\n>     \"default\": {\r\n>         \"BACKEND\": \"django.core.cache.backends.db.DatabaseCache\",\r\n>         \"LOCATION\": \"my_cache_table\",\r\n>     },\r\n> }\r\n> ```\r\n> \r\n> Tried and tested with SQLite as well, and got the same results.\r\n> Error with SQLite\r\n> \r\n> ```\r\n> ======================================================================\r\n> FAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n> ----------------------------------------------------------------------\r\n> Traceback (most recent call last):\r\n>   File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n>     return func(*args, **kwargs)\r\n>   File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n>     management.call_command('createcachetable', database='other', verbosity=0)\r\n>   File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n>     self.test_case.assertEqual(\r\n> AssertionError: 1 != 5 : 1 queries executed, 5 expected\r\n> Captured queries were:\r\n> 1. \r\n>             SELECT name, type FROM sqlite_master\r\n>             WHERE type in ('table', 'view') AND NOT name='sqlite_sequence'\r\n>             ORDER BY name\r\n> \r\n> ----------------------------------------------------------------------\r\n> ```\r\n> \r\n> However, when I comment out this line, which call `createcachetable`, the test passes.\r\n> https://github.com/django/django/blob/d270dd584e0af12fe6229fb712d0704c232dc7e5/django/db/backends/base/creation.py#L92\r\n\r\n@carltongibson I was able to figure this one out. I was following the [documentation](https://docs.djangoproject.com/en/3.2/topics/cache/#database-caching) to setup the DatabaseCache.\r\n```\r\nCACHES = {\r\n    \"default\": {\r\n        \"BACKEND\": \"django.core.cache.backends.db.DatabaseCache\",\r\n        \"LOCATION\": \"my_cache_table\",\r\n    },\r\n}\r\n```\r\nHowever, I believe `my_cache_table` was conflicting with this\r\nhttps://github.com/django/django/blob/225d96533a8e05debd402a2bfe566487cc27d95f/tests/cache/tests.py#L1213-L1220\r\n\r\nSetting the \"LOCATION\" to some other table name leads to the test passing. \r\n\r\nMaybe we could mention it in the docs somewhere or update the test to check if duplicate table names exists? This is a little off-topic from this PR. Should I create a separate ticket for this? Or should we let it be for now?\r\n",
      "Hey!\r\nI've made the changes suggested by @pope1ni. I've updated the tests and added\r\n```\r\nredis_excluded_caches = {'cull', 'zero_cull'}\r\n...\r\n@override_settings(CACHES=caches_setting_for_tests(\r\n    base=RedisCache_params,\r\n    exclude=redis_excluded_caches\r\n))\r\nclass RedisCacheTests(BaseCacheTests, TestCase):\r\n    ...\r\n```\r\nNow only on test fails. Handling zero timeout. Redis-py does not support it natively and django expects to no set a key with zero timeout. I'm not sure at which level should this be handled. I was wondering if we perform the check in `get_backend_timeout` method and return a value (eg: None) or raise a suitable exception. \r\n",
      "> If `redis-py` isn't installed, or if Redis isn't running we get a couple of errors:\r\n> \r\n> ```\r\n> django.core.cache.backends.base.InvalidCacheBackendError: Could not find backend 'django.core.cache.backends.redis.RedisCache': No module named 'redis'\r\n> ```\r\n> \r\n\r\nShould I move the `import redis` line inside the `__init__` method of the RedisCache class? All memcache backend do that and the error raised when a binding is not installed is like this\r\n```\r\nModuleNotFoundError: No module named 'pymemcache'\r\n```\r\nIncluding `import redis` at the top leads to an error message as you mentioned above.\r\nI wanted to move the import command like this\r\n```\r\nclass RedisCache(BaseCache):\r\n    def __init__(self, server, params):\r\n        import redis\r\n        super().__init__(params)\r\n        if isinstance(server, str):\r\n            self._servers = re.split('[;,]', server)\r\n        else:\r\n            self._servers = server\r\n\r\n        self._class = RedisCacheClient\r\n        self._options = params.get('OPTIONS') or {}\r\n```\r\nHowever, this would lead to some refactoring of code where `redis.ConnectionPool` etc are used.\r\nWhat do you think about this?\r\n",
      "> Should I move the `import redis` line inside the `__init__` method of the RedisCache class? All memcache backend do that and the error raised when a binding is not installed is like this\r\n\r\nYes, we should do something like this. Although with `pymemcache` it is done in `PyMemcacheCache.__init__()` because `pymemcache` provides the client class. We're writing the client class ourselves, so we want `import redis` in `RedisCacheClient.__init__()`.",
      "Hey @pope1ni and @carltongibson \r\n\r\nAccording to the comments https://github.com/django/django/pull/14437#pullrequestreview-691776531, if we want to use the `redis.incr` or `redis.decr`, we would need to stop serializing the valus which are integers. I think this will create a mess as there would be too much to manual handling of values based on their types. As we are pickling the values, we can not directly use the `redis.incr` or `redis.decr` methods.\r\n\r\nI'm not sure how useful it is to support milli-second timeouts. `django-redis` has migrated to an approach which supports both seconds and milliseconds ( [refs](https://github.com/jazzband/django-redis/pull/508/files) ). \r\n\r\nWe can make the logical databases configurable via the url as well as a parameter in the options. I'll work on it.\r\n\r\n",
      "While using the `from_url` method, we can not provide `username` and `password` in the `OPTIONS`. Even if we pass them in the kwargs, [kwargs.update(...)](https://redis-py.readthedocs.io/en/stable/_modules/redis/connection.html#ConnectionPool.from_url) overrides it with the `username` and `password` from the URL, else it sets it to None. One solution is that we only allow username and password to be set using the \"LOCATION\" key only. Let me know what you feel. @pope1ni @carltongibson ",
      "The master branch of redis-py has updated the implementation of the `from_url` method. \r\nhttps://github.com/andymccurdy/redis-py/blob/627db540acd1f1f36db88290d74cbcd75f6bda0c/redis/connection.py#L951-L955\r\n\r\nHowever, the latest stable branch (3.5.3) still uses the old implementation.\r\n```\r\nif decode_components:\r\n    username = unquote(url.username) if url.username else None\r\n    password = unquote(url.password) if url.password else None\r\n    path = unquote(url.path) if url.path else None\r\n    hostname = unquote(url.hostname) if url.hostname else None\r\nelse:\r\n    username = url.username or None\r\n    password = url.password or None\r\n    path = url.path\r\n    hostname = url.hostname\r\n\r\nif url.scheme == 'unix':\r\n    url_options.update({\r\n                'username': username,\r\n                'password': password,\r\n                'path': path,\r\n                'connection_class': UnixDomainSocketConnection,    \r\n})\r\n\r\nelif url.scheme in ('redis', 'rediss'):\r\n    url_options.update({\r\n                'host': hostname,\r\n                'port': int(url.port or 6379),\r\n                'username': username,\r\n                'password': password,\r\n            })\r\n...\r\nkwargs.update(url_options)\r\n```\r\nThis will always override the username and password in the kwargs. I think for now, we can only support giving the `username` and `password` via the url and once redis-py's latest implementation is stable, we can add supplying  `username` and `password` via the `OPTIONS`.\r\n",
      "Hey @smithdc1! Thank you for the review.\r\n\r\n\r\n> I'm not so confident to comment on the main part with all the different options. I think my main observation there is about the structure, I find it a little hard to follow. There's many options here but here's one idea of how it could be structured.\r\n> \r\n> * Required settings\r\n> * settings which are available to all backends\r\n> * Redis specific items (optional)\r\n\r\nYes we could do that but that'll require restructing the whole documentation as there are some memcached specific items that are currently laid out in the examples section. In the future, I think such a restructuring would be nice where we can talk about cache specific arguements in a separate section.\r\n"
    ],
    "code_diff": "diff --git a/django/core/cache/backends/redis.py b/django/core/cache/backends/redis.py\nnew file mode 100644\nindex 000000000000..16556b1ded5c\n--- /dev/null\n+++ b/django/core/cache/backends/redis.py\n@@ -0,0 +1,224 @@\n+\"\"\"Redis cache backend.\"\"\"\n+\n+import random\n+import re\n+\n+from django.core.cache.backends.base import DEFAULT_TIMEOUT, BaseCache\n+from django.core.serializers.base import PickleSerializer\n+from django.utils.functional import cached_property\n+from django.utils.module_loading import import_string\n+\n+\n+class RedisSerializer(PickleSerializer):\n+    def dumps(self, obj):\n+        if isinstance(obj, int):\n+            return obj\n+        return super().dumps(obj)\n+\n+    def loads(self, data):\n+        try:\n+            return int(data)\n+        except ValueError:\n+            return super().loads(data)\n+\n+\n+class RedisCacheClient:\n+    def __init__(\n+        self,\n+        servers,\n+        serializer=None,\n+        db=None,\n+        pool_class=None,\n+        parser_class=None,\n+    ):\n+        import redis\n+\n+        self._lib = redis\n+        self._servers = servers\n+        self._pools = {}\n+\n+        self._client = self._lib.Redis\n+\n+        if isinstance(pool_class, str):\n+            pool_class = import_string(pool_class)\n+        self._pool_class = pool_class or self._lib.ConnectionPool\n+\n+        if isinstance(serializer, str):\n+            serializer = import_string(serializer)\n+        if callable(serializer):\n+            serializer = serializer()\n+        self._serializer = serializer or RedisSerializer()\n+\n+        if isinstance(parser_class, str):\n+            parser_class = import_string(parser_class)\n+        parser_class = parser_class or self._lib.connection.DefaultParser\n+\n+        self._pool_options = {'parser_class': parser_class, 'db': db}\n+\n+    def _get_connection_pool_index(self, write):\n+        # Write to the first server. Read from other servers if there are more,\n+        # otherwise read from the first server.\n+        if write or len(self._servers) == 1:\n+            return 0\n+        return random.randint(1, len(self._servers) - 1)\n+\n+    def _get_connection_pool(self, write):\n+        index = self._get_connection_pool_index(write)\n+        if index not in self._pools:\n+            self._pools[index] = self._pool_class.from_url(\n+                self._servers[index], **self._pool_options,\n+            )\n+        return self._pools[index]\n+\n+    def get_client(self, key=None, *, write=False):\n+        # key is used so that the method signature remains the same and custom\n+        # cache client can be implemented which might require the key to select\n+        # the server, e.g. sharding.\n+        pool = self._get_connection_pool(write)\n+        return self._client(connection_pool=pool)\n+\n+    def add(self, key, value, timeout):\n+        client = self.get_client(key, write=True)\n+        value = self._serializer.dumps(value)\n+\n+        if timeout == 0:\n+            if ret := bool(client.set(key, value, nx=True)):\n+                client.delete(key)\n+            return ret\n+        else:\n+            return bool(client.set(key, value, ex=timeout, nx=True))\n+\n+    def get(self, key, default):\n+        client = self.get_client(key)\n+        value = client.get(key)\n+        return default if value is None else self._serializer.loads(value)\n+\n+    def set(self, key, value, timeout):\n+        client = self.get_client(key, write=True)\n+        value = self._serializer.dumps(value)\n+        if timeout == 0:\n+            client.delete(key)\n+        else:\n+            client.set(key, value, ex=timeout)\n+\n+    def touch(self, key, timeout):\n+        client = self.get_client(key, write=True)\n+        if timeout is None:\n+            return bool(client.persist(key))\n+        else:\n+            return bool(client.expire(key, timeout))\n+\n+    def delete(self, key):\n+        client = self.get_client(key, write=True)\n+        return bool(client.delete(key))\n+\n+    def get_many(self, keys):\n+        client = self.get_client(None)\n+        ret = client.mget(keys)\n+        return {\n+            k: self._serializer.loads(v) for k, v in zip(keys, ret) if v is not None\n+        }\n+\n+    def has_key(self, key):\n+        client = self.get_client(key)\n+        return bool(client.exists(key))\n+\n+    def incr(self, key, delta):\n+        client = self.get_client(key)\n+        if not client.exists(key):\n+            raise ValueError(\"Key '%s' not found.\" % key)\n+        return client.incr(key, delta)\n+\n+    def set_many(self, data, timeout):\n+        client = self.get_client(None, write=True)\n+        pipeline = client.pipeline()\n+        pipeline.mset({k: self._serializer.dumps(v) for k, v in data.items()})\n+\n+        if timeout is not None:\n+            # Setting timeout for each key as redis does not support timeout\n+            # with mset().\n+            for key in data:\n+                pipeline.expire(key, timeout)\n+        pipeline.execute()\n+\n+    def delete_many(self, keys):\n+        client = self.get_client(None, write=True)\n+        client.delete(*keys)\n+\n+    def clear(self):\n+        client = self.get_client(None, write=True)\n+        return bool(client.flushdb())\n+\n+\n+class RedisCache(BaseCache):\n+    def __init__(self, server, params):\n+        super().__init__(params)\n+        if isinstance(server, str):\n+            self._servers = re.split('[;,]', server)\n+        else:\n+            self._servers = server\n+\n+        self._class = RedisCacheClient\n+        self._options = params.get('OPTIONS', {})\n+\n+    @cached_property\n+    def _cache(self):\n+        return self._class(self._servers, **self._options)\n+\n+    def get_backend_timeout(self, timeout=DEFAULT_TIMEOUT):\n+        if timeout == DEFAULT_TIMEOUT:\n+            timeout = self.default_timeout\n+        # The key will be made persistent if None used as a timeout.\n+        # Non-positive values will cause the key to be deleted.\n+        return None if timeout is None else max(0, int(timeout))\n+\n+    def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.add(key, value, self.get_backend_timeout(timeout))\n+\n+    def get(self, key, default=None, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.get(key, default)\n+\n+    def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        self._cache.set(key, value, self.get_backend_timeout(timeout))\n+\n+    def touch(self, key, timeout=DEFAULT_TIMEOUT, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.touch(key, self.get_backend_timeout(timeout))\n+\n+    def delete(self, key, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.delete(key)\n+\n+    def get_many(self, keys, version=None):\n+        key_map = {self.make_and_validate_key(key, version=version): key for key in keys}\n+        ret = self._cache.get_many(key_map.keys())\n+        return {key_map[k]: v for k, v in ret.items()}\n+\n+    def has_key(self, key, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.has_key(key)\n+\n+    def incr(self, key, delta=1, version=None):\n+        key = self.make_and_validate_key(key, version=version)\n+        return self._cache.incr(key, delta)\n+\n+    def set_many(self, data, timeout=DEFAULT_TIMEOUT, version=None):\n+        safe_data = {}\n+        for key, value in data.items():\n+            key = self.make_and_validate_key(key, version=version)\n+            safe_data[key] = value\n+        self._cache.set_many(safe_data, self.get_backend_timeout(timeout))\n+        return []\n+\n+    def delete_many(self, keys, version=None):\n+        safe_keys = []\n+        for key in keys:\n+            key = self.make_and_validate_key(key, version=version)\n+            safe_keys.append(key)\n+        self._cache.delete_many(safe_keys)\n+\n+    def clear(self):\n+        return self._cache.clear()\ndiff --git a/docs/internals/contributing/writing-code/unit-tests.txt b/docs/internals/contributing/writing-code/unit-tests.txt\nindex 6a5bd5ab8f05..9bba72c451af 100644\n--- a/docs/internals/contributing/writing-code/unit-tests.txt\n+++ b/docs/internals/contributing/writing-code/unit-tests.txt\n@@ -285,6 +285,7 @@ dependencies:\n *  PyYAML_\n *  pytz_ (required)\n *  pywatchman_\n+*  redis_\n *  setuptools_\n *  memcached_, plus a :ref:`supported Python binding <memcached>`\n *  gettext_ (:ref:`gettext_on_windows`)\n@@ -308,8 +309,9 @@ encounter.\n You can also install the database adapter(s) of your choice using\n ``oracle.txt``, ``mysql.txt``, or ``postgres.txt``.\n \n-If you want to test the memcached cache backend, you'll also need to define\n-a :setting:`CACHES` setting that points at your memcached instance.\n+If you want to test the memcached or Redis cache backends, you'll also need to\n+define a :setting:`CACHES` setting that points at your memcached or Redis\n+instance respectively.\n \n To run the GeoDjango tests, you will need to :doc:`set up a spatial database\n and install the Geospatial libraries</ref/contrib/gis/install/index>`.\n@@ -332,6 +334,7 @@ service.\n .. _PyYAML: https://pyyaml.org/wiki/PyYAML\n .. _pytz: https://pypi.org/project/pytz/\n .. _pywatchman: https://pypi.org/project/pywatchman/\n+.. _redis: https://pypi.org/project/redis/\n .. _setuptools: https://pypi.org/project/setuptools/\n .. _memcached: https://memcached.org/\n .. _gettext: https://www.gnu.org/software/gettext/manual/gettext.html\ndiff --git a/docs/ref/settings.txt b/docs/ref/settings.txt\nindex 12d89142d103..0c5b3fe3072d 100644\n--- a/docs/ref/settings.txt\n+++ b/docs/ref/settings.txt\n@@ -153,6 +153,7 @@ The cache backend to use. The built-in cache backends are:\n * ``'django.core.cache.backends.locmem.LocMemCache'``\n * ``'django.core.cache.backends.memcached.PyMemcacheCache'``\n * ``'django.core.cache.backends.memcached.PyLibMCCache'``\n+* ``'django.core.cache.backends.redis.RedisCache'``\n \n You can use a cache backend that doesn't ship with Django by setting\n :setting:`BACKEND <CACHES-BACKEND>` to a fully-qualified path of a cache\n@@ -162,6 +163,10 @@ backend class (i.e. ``mypackage.backends.whatever.WhateverCache``).\n \n     The ``PyMemcacheCache`` backend was added.\n \n+.. versionchanged:: 4.0\n+\n+    The ``RedisCache`` backend was added.\n+\n .. setting:: CACHES-KEY_FUNCTION\n \n ``KEY_FUNCTION``\ndiff --git a/docs/releases/4.0.txt b/docs/releases/4.0.txt\nindex 709363a08f76..540500af4734 100644\n--- a/docs/releases/4.0.txt\n+++ b/docs/releases/4.0.txt\n@@ -65,6 +65,16 @@ The new :ref:`scrypt password hasher <scrypt-usage>` is more secure and\n recommended over PBKDF2. However, it's not the default as it requires OpenSSL\n 1.1+ and more memory.\n \n+Redis cache backend\n+-------------------\n+\n+The new ``django.core.cache.backends.redis.RedisCache`` cache backend provides\n+built-in support for caching with Redis. `redis-py`_ 3.0.0 or higher is\n+required. For more details, see the :ref:`documentation on caching with Redis\n+in Django <redis>`.\n+\n+.. _`redis-py`: https://pypi.org/project/redis/\n+\n Minor features\n --------------\n \ndiff --git a/docs/spelling_wordlist b/docs/spelling_wordlist\nindex bd8785cf67f6..8d9b17a43d1b 100644\n--- a/docs/spelling_wordlist\n+++ b/docs/spelling_wordlist\n@@ -423,6 +423,7 @@ recomputation\n recursed\n redeclare\n redirections\n+redis\n redisplay\n redisplayed\n redisplaying\ndiff --git a/docs/topics/cache.txt b/docs/topics/cache.txt\nindex 0f25260672ce..3bc35fd51d06 100644\n--- a/docs/topics/cache.txt\n+++ b/docs/topics/cache.txt\n@@ -62,7 +62,6 @@ settings file. Here's an explanation of all available values for\n Memcached\n ---------\n \n-The fastest, most efficient type of cache supported natively by Django,\n Memcached__ is an entirely memory-based cache server, originally developed\n to handle high loads at LiveJournal.com and subsequently open-sourced by\n Danga Interactive. It is used by sites such as Facebook and Wikipedia to\n@@ -169,6 +168,71 @@ particularly temporary.\n     some problems and seems to be unmaintained. Use ``PyMemcacheCache`` or\n     ``PyLibMCCache`` instead.\n \n+.. _redis:\n+\n+Redis\n+-----\n+\n+.. versionadded:: 4.0\n+\n+Redis__ is an in-memory database that can be used for caching. To begin you'll\n+need a Redis server running either locally or on a remote machine.\n+\n+__ https://redis.io/\n+\n+After setting up the Redis server, you'll need to install Python bindings for\n+Redis. `redis-py`_ is the binding supported natively by Django. Installing the\n+additional `hiredis-py`_ package is also recommended.\n+\n+.. _`redis-py`: https://pypi.org/project/redis/\n+.. _`hiredis-py`: https://pypi.org/project/hiredis/\n+\n+To use Redis as your cache backend with Django:\n+\n+* Set :setting:`BACKEND <CACHES-BACKEND>` to\n+  ``django.core.cache.backends.redis.RedisCache``.\n+\n+* Set :setting:`LOCATION <CACHES-LOCATION>` to the URL pointing to your Redis\n+  instance, using the appropriate scheme. See the ``redis-py`` docs for\n+  `details on the available schemes\n+  <https://redis-py.readthedocs.io/en/stable/#redis.ConnectionPool.from_url>`_.\n+\n+For example, if Redis is running on localhost (127.0.0.1) port 6379::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': 'redis://127.0.0.1:6379',\n+        }\n+    }\n+\n+Often Redis servers are protected with authentication. In order to supply a\n+username and password, add them in the ``LOCATION`` along with the URL::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': 'redis://username:password@127.0.0.1:6379',\n+        }\n+    }\n+\n+If you have multiple Redis servers set up in the replication mode, you can\n+specify the servers either as a semicolon or comma delimited string, or as a\n+list. While using multiple servers, write operations are performed on the first\n+server (leader). Read operations are performed on the other servers (replicas)\n+chosen at random::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': [\n+                'redis://127.0.0.1:6379', # leader\n+                'redis://127.0.0.1:6378', # read-replica 1\n+                'redis://127.0.0.1:6377', # read-replica 2\n+            ],\n+        }\n+    }\n+\n .. _database-caching:\n \n Database caching\n@@ -422,9 +486,9 @@ behavior. These arguments are provided as additional keys in the\n     On some backends (``database`` in particular) this makes culling *much*\n     faster at the expense of more cache misses.\n \n-  Memcached backends pass the contents of :setting:`OPTIONS <CACHES-OPTIONS>`\n-  as keyword arguments to the client constructors, allowing for more advanced\n-  control of client behavior. For example usage, see below.\n+  The Memcached and Redis backends pass the contents of :setting:`OPTIONS\n+  <CACHES-OPTIONS>` as keyword arguments to the client constructors, allowing\n+  for more advanced control of client behavior. For example usage, see below.\n \n * :setting:`KEY_PREFIX <CACHES-KEY_PREFIX>`: A string that will be\n   automatically included (prepended by default) to all cache keys\n@@ -496,6 +560,27 @@ flag on the connection's socket::\n         }\n     }\n \n+Here's an example configuration for a ``redis`` based backend that selects\n+database ``10`` (by default Redis ships with 16 logical databases), specifies a\n+`parser class`_ (``redis.connection.HiredisParser`` will be used by default if\n+the ``hiredis-py`` package is installed), and sets a custom `connection pool\n+class`_ (``redis.ConnectionPool`` is used by default)::\n+\n+    CACHES = {\n+        'default': {\n+            'BACKEND': 'django.core.cache.backends.redis.RedisCache',\n+            'LOCATION': 'redis://127.0.0.1:6379',\n+            'OPTIONS': {\n+                'db': '10',\n+                'parser_class': 'redis.connection.PythonParser',\n+                'pool_class': 'redis.BlockingConnectionPool',\n+            }\n+        }\n+    }\n+\n+.. _`parser class`: https://github.com/andymccurdy/redis-py#parsers\n+.. _`connection pool class`: https://github.com/andymccurdy/redis-py#connection-pools\n+\n .. _the-per-site-cache:\n \n The per-site cache\ndiff --git a/tests/cache/tests.py b/tests/cache/tests.py\nindex 0e2f5f7d1f04..b88066285801 100644\n--- a/tests/cache/tests.py\n+++ b/tests/cache/tests.py\n@@ -22,6 +22,7 @@\n     caches,\n )\n from django.core.cache.backends.base import InvalidCacheBackendError\n+from django.core.cache.backends.redis import RedisCacheClient\n from django.core.cache.utils import make_template_fragment_key\n from django.db import close_old_connections, connection, connections\n from django.db.backends.utils import CursorWrapper\n@@ -1373,10 +1374,9 @@ def test_lru_incr(self):\n         self.assertEqual(cache.get(9), 9)\n \n \n-# memcached backend isn't guaranteed to be available.\n-# To check the memcached backend, the test settings file will\n-# need to contain at least one cache backend setting that points at\n-# your memcache server.\n+# memcached and redis backends aren't guaranteed to be available.\n+# To check the backends, the test settings file will need to contain at least\n+# one cache backend setting that points at your cache server.\n configured_caches = {}\n for _cache_params in settings.CACHES.values():\n     configured_caches[_cache_params['BACKEND']] = _cache_params\n@@ -1387,6 +1387,11 @@ def test_lru_incr(self):\n # The memcached backends don't support cull-related options like `MAX_ENTRIES`.\n memcached_excluded_caches = {'cull', 'zero_cull'}\n \n+RedisCache_params = configured_caches.get('django.core.cache.backends.redis.RedisCache')\n+\n+# The redis backend does not support cull-related options like `MAX_ENTRIES`.\n+redis_excluded_caches = {'cull', 'zero_cull'}\n+\n \n class BaseMemcachedTests(BaseCacheTests):\n \n@@ -1727,6 +1732,60 @@ def test_empty_cache_file_considered_expired(self):\n             self.assertIs(cache._is_expired(fh), True)\n \n \n+@unittest.skipUnless(RedisCache_params, \"Redis backend not configured\")\n+@override_settings(CACHES=caches_setting_for_tests(\n+    base=RedisCache_params,\n+    exclude=redis_excluded_caches,\n+))\n+class RedisCacheTests(BaseCacheTests, TestCase):\n+\n+    def setUp(self):\n+        import redis\n+        super().setUp()\n+        self.lib = redis\n+\n+    @property\n+    def incr_decr_type_error(self):\n+        return self.lib.ResponseError\n+\n+    def test_cache_client_class(self):\n+        self.assertIs(cache._class, RedisCacheClient)\n+        self.assertIsInstance(cache._cache, RedisCacheClient)\n+\n+    def test_get_backend_timeout_method(self):\n+        positive_timeout = 10\n+        positive_backend_timeout = cache.get_backend_timeout(positive_timeout)\n+        self.assertEqual(positive_backend_timeout, positive_timeout)\n+\n+        negative_timeout = -5\n+        negative_backend_timeout = cache.get_backend_timeout(negative_timeout)\n+        self.assertEqual(negative_backend_timeout, 0)\n+\n+        none_timeout = None\n+        none_backend_timeout = cache.get_backend_timeout(none_timeout)\n+        self.assertIsNone(none_backend_timeout)\n+\n+    def test_get_connection_pool_index(self):\n+        pool_index = cache._cache._get_connection_pool_index(write=True)\n+        self.assertEqual(pool_index, 0)\n+        pool_index = cache._cache._get_connection_pool_index(write=False)\n+        if len(cache._cache._servers) == 1:\n+            self.assertEqual(pool_index, 0)\n+        else:\n+            self.assertGreater(pool_index, 0)\n+            self.assertLess(pool_index, len(cache._cache._servers))\n+\n+    def test_get_connection_pool(self):\n+        pool = cache._cache._get_connection_pool(write=True)\n+        self.assertIsInstance(pool, self.lib.ConnectionPool)\n+\n+        pool = cache._cache._get_connection_pool(write=False)\n+        self.assertIsInstance(pool, self.lib.ConnectionPool)\n+\n+    def test_get_client(self):\n+        self.assertIsInstance(cache._cache.get_client(), self.lib.Redis)\n+\n+\n class FileBasedCachePathLibTests(FileBasedCacheTests):\n     def mkdtemp(self):\n         tmp_dir = super().mkdtemp()\ndiff --git a/tests/requirements/py3.txt b/tests/requirements/py3.txt\nindex af06c4e5f620..893e47a914a7 100644\n--- a/tests/requirements/py3.txt\n+++ b/tests/requirements/py3.txt\n@@ -15,6 +15,7 @@ python-memcached >= 1.59\n pytz\n pywatchman; sys.platform != 'win32'\n PyYAML\n+redis >= 3.0.0\n selenium\n sqlparse >= 0.2.2\n tblib >= 1.5.0\n"
  }
]