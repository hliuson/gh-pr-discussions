[
  {
    "pr_title": "feat(block): Add Ayrshare integration for social media posting",
    "pr_body": "This PR implements a comprehensive Ayrshare social media integration for AutoGPT Platform, enabling users to post content across multiple social media platforms through a unified interface. Ayrshare provides a single API to manage posts across Facebook, Twitter/X, LinkedIn, Instagram, YouTube, TikTok, Pinterest, Reddit, Telegram, Google My Business, Bluesky, Snapchat, and Threads.\r\n\r\nThe integration addresses the need for social media automation and content distribution workflows within AutoGPT ",
    "pr_number": 9946,
    "comments": [
      "Note I have only added loading the profile key on the first post block, whilst I check im doing it the correct way.\r\n\r\nI need to add the frontend button next as well.",
      "plz follow template with bot, once the ai reviewer is back online it'll deny the pr for this",
      "Thank you for this thorough Ayrshare integration PR! The code looks well-structured with comprehensive implementation across both frontend and backend components.\n\nBefore this PR can be merged, there are a few items that need to be addressed:\n\n1. **Missing Checklist**: Please add the standard PR checklist from our template and check off the relevant items. Since this is a significant code change introducing new functionality, we need to ensure you've tested the implementation thoroughly.\n\n2. **Merge Conflicts**: The PR has the 'conflicts' label, indicating there are merge conflicts that need to be resolved before merging.\n\n3. **PR Title Scope**: Consider updating the PR title to use 'platform/blocks' as the scope instead of just 'block' to better align with our conventional commit format and labeled scopes.\n\n4. **Test Plan**: Please provide a test plan detailing how you've verified this integration works correctly. For example:\n   - Connecting to different social media platforms via Ayrshare\n   - Posting content to each supported platform\n   - Handling error cases (e.g., when profile key is missing)\n\nThe implementation itself looks solid, with proper security considerations and a clean architecture. Once the above items are addressed, this PR should be ready for final review.",
      "‚ùå **Preview Environment Deployment Failed**\n\nüö® The preview environment deployment encountered critical errors and has been rolled back.\n\n**Cleanup Completed:**\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema dropped\n- üîå All services terminated\n\n*Please check the workflow logs for details and try again.*",
      "Thanks for this comprehensive PR adding Ayrshare integration for social media posting! The implementation looks solid with good support across multiple platforms.\n\nI've reviewed your changes and have a couple of items that need addressing before this can be merged:\n\n1. **PR Title Format**: The scope in your title should be `blocks` instead of `block` to match our conventional commit format standards. Please update to: `feat(blocks): Added Ayrshare integration for social media posting`\n\n2. **Missing Checklist**: Your PR description is very detailed, which is great, but it's missing the required checklist section. Please add the standard PR checklist that includes items like:\n   - Confirming you've tested your changes\n   - Your test plan\n   - Any configuration changes\n\nThe code implementation itself looks well-structured with comprehensive support for various social media platforms. I particularly like how you've organized the different posting blocks with platform-specific options and validations.\n\nOnce you've addressed these two items, this PR should be ready for another review.",
      "Thanks for the comprehensive Ayrshare integration PR! The implementation looks well-designed and thoroughly documented.\n\n### What Looks Good\n- Great PR description with detailed explanations of all components\n- Clean implementation of both backend and frontend components\n- Good separation of concerns with platform-specific posting blocks\n- Environment variables correctly added to `.env.example`\n- Proper handling of user_id in credential store operations\n- The new AYRSHARE block type is added in the correct alphabetical location\n\n### What Needs Addressing\n- **Missing checklist**: Please add the required checklist to the PR description. As this is a code change, we need a complete checklist including a test plan to verify the functionality works correctly.\n\n### Testing Considerations\nSince this is a complex integration, please ensure your test plan includes:\n- Creating and connecting to Ayrshare accounts\n- Posting to various social media platforms\n- Handling error cases (e.g., invalid credentials, failed posts)\n- Verifying the SSO flow works correctly\n\nOnce you've added the checklist with a proper test plan, this PR should be ready for final review.",
      "Waiting on preview envs to be fixed before this can be comprehensively tested. \r\nAlso there is difficulty testing all platforms as I don't have all the different social account.",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "![Screenshot 2025-06-16 at 11 57 40](https://github.com/user-attachments/assets/3bba19c2-4983-47e8-88f5-9f5a6c6da32f)\r\n![Screenshot 2025-06-16 at 11 55 17](https://github.com/user-attachments/assets/55e3e80c-8106-4b36-8a15-11f86536ac2e)\r\n\r\nI've tested linkedin and twitter both work. The others need testing",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*"
    ],
    "num_comments": 11,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 164852,
    "code_diff": "diff --git a/autogpt_platform/backend/.env.example b/autogpt_platform/backend/.env.example\nindex e223efa52557..21aef0c95fc9 100644\n--- a/autogpt_platform/backend/.env.example\n+++ b/autogpt_platform/backend/.env.example\n@@ -197,6 +197,10 @@ SMARTLEAD_API_KEY=\n # ZeroBounce\n ZEROBOUNCE_API_KEY=\n \n+# A"
  },
  {
    "pr_title": "feat(frontend,backend): Redesign block menu on builder page with supporting API",
    "pr_body": "## Summary\r\n\r\nThis PR introduces a Redesigned Block Menu for the builder and all the relevant backend changes. The implementation includes a complete set of reusable UI components, state management, pagination hooks, and search/filter functionality to enhance the user experience when selecting and adding blocks and agents to their workflows.\r\n\r\n## Screenshot\r\n\r\n![Screenshot 2025-06-06 at 7 15 04‚ÄØPM](https://github.com/user-attachments/assets/90476084-1996-4653-bcc1-d41e09a9dd84)\r\n\r\n## Design fil",
    "pr_number": 9956,
    "comments": [
      "The PR fails to meet some basic requirements but shows a good effort. Issues:\n1. The PR template is not properly filled out - missing test plan and checklist items\n2. While the scope is clear (frontend) and shown in the title correctly, documentation of changes could be clearer\n3. No clear explanation in PR body about 'why' these changes were needed\n4. No test plan or testing notes provided for such a large UI change\n\nHowever, the code itself looks well-structured with clear component organization and documentation. The TODO comments also show good forward planning.",
      "## Feedback for PR Improvement\n\n### Missing PR Template & Checklist\nPlease fill out the complete PR template, including the checklist section. This is required for all PRs and helps reviewers understand what's being changed and how it's been tested.\n\n### Scope Mismatch\nYour PR title mentions only frontend changes (`feat(frontend)`), but the diff shows significant backend additions including:\n- New API routes in `/api/builder/`\n- New DB functions and models\n- Changes to store routes\n\nPlease update your title to reflect both frontend and backend changes, or better explain in the description why backend changes were needed for this frontend redesign.\n\n### User ID Validation\nSome of the new backend functions in `backend/server/v2/builder/db.py` don't appear to validate user permissions via user_id. Please review these functions to ensure proper authorization is in place or provide an explanation for why user_id checks aren't needed.\n\n### Merge Conflicts\nThis PR has merge conflicts that need to be resolved before it can be merged.\n\n### Testing\nPlease include information about how you've tested these changes, particularly since they span both frontend and backend components.\n\nOnce you've addressed these issues, the PR should be ready for re-review.",
      "Thank you for your work on redesigning the block menu. This looks like a substantial change that will improve the builder page experience. \n\nHowever, there are a few things that need to be addressed before this PR can be merged:\n\n1. **Missing checklist**: Please complete the PR template checklist. For code changes like these, we need to know that you've tested your changes with a clear test plan.\n\n2. **Scope discrepancy**: Your PR title mentions only frontend changes, but the diff includes significant backend changes - new routes, endpoints, and database access functions. Please update your PR title to reflect that this is both a frontend and backend feature, perhaps something like `feat(frontend,backend): Redesign block menu on builder page`.\n\n3. **Implementation details**: Your PR description lists the components you've added but doesn't explain how they work together or what specific improvements they bring. Please enhance the description with more details about the implementation and how it improves upon the previous design.\n\n4. **Test plan**: Please add a test plan to your PR description explaining how you've tested these changes.\n\nThe code itself looks well-structured with proper authentication handling and user_id checks. I particularly like how you've organized the new components and the backend routes to support them. Once you address the above points, this PR should be ready for another review.",
      "Thank you for your work on redesigning the block menu! I have a few comments before this can be merged:\n\n## Description and Checklist\n- Please update your PR description to include a more detailed explanation of the changes you've made\n- The required PR checklist is missing. Please add the checklist from the PR template and check off the appropriate items\n\n## Scope Mismatch\nYour PR title indicates this is a frontend feature (`feat(frontend)`), but the changes include substantial backend code, including:\n- New backend routes in `/api/builder`\n- New database functions in `backend/server/v2/builder/db.py`\n- New models in `backend/server/v2/builder/model.py`\n\nConsider updating your PR title to reflect that this involves both frontend and backend changes, perhaps something like `feat(platform): Redesign block menu on builder page with backend support`\n\n## Test Plan\nPlease include a test plan in your PR description to explain how you've tested these changes.\n\nOnce these items are addressed, we can proceed with reviewing the technical aspects of the implementation. The component list and structure looks well-organized!",
      "Thank you for your work on redesigning the block menu! I have a few concerns about this PR that need to be addressed before it can be merged:\n\n1. **PR Description:** While you've listed the components you've added, the PR description should explain why these changes are needed and provide more context about the redesign goals.\n\n2. **Missing Checklist:** Your PR description is missing the required checklist section that outlines your test plan and confirms you've tested the changes appropriately. Please update your PR description to include the complete checklist.\n\n3. **Scope Mismatch:** The PR title indicates this is a frontend change, but the diff shows substantial backend changes including:\n   - New backend routes in `/api/builder`\n   - New database functions and models\n   - Changes to the store API\n\n   Please update either the PR title to reflect the full scope of changes (e.g., `feat(frontend,backend): Redesign block menu and add builder API endpoints`) or separate the backend changes into a different PR.\n\n4. **Dependencies:** You've added several new dependencies (`react-timeago`, `tailwind-scrollbar`, `tailwind-scrollbar-hide`). Please ensure these are necessary and document their purpose in the PR description.\n\n5. **Testing:** Please describe how you've tested these changes, especially the new backend routes and functionality.\n\nAdditionally, I see there's a conflicts label on this PR. Please resolve any merge conflicts before this can be approved.\n\nOnce you've addressed these issues, I'll be happy to review the PR again.",
      "Thank you for your PR to redesign the block menu! The component list you provided is helpful for understanding what's been added. I have some feedback to help get this PR ready for merging:\n\n1. **Missing checklist**: This PR contains significant code changes but is missing the required checklist. Please add the checklist from the PR template and ensure you've tested all changes appropriately.\n\n2. **Scope mismatch**: While your title mentions frontend changes, a significant portion of this PR adds backend functionality (new routes, models, and database operations for the builder). Please update your PR title to reflect both frontend and backend changes, e.g., `feat(frontend,backend): Redesign block menu and add builder API endpoints`.\n\n3. **PR description improvements**: Consider expanding your description to mention the backend API endpoints you've added to support the block menu functionality.\n\nOnce you've addressed these items, we'll be able to review your PR again. The code changes themselves look good - I like the organized approach with the reusable components and the proper handling of user_id in the backend routes.",
      "Thanks for the PR to redesign the block menu! The implementation looks comprehensive with both frontend components and backend support.\n\n### Feedback\n\n1. **PR Description Improvements**:\n   - The PR description is missing the required checklist from the template. Please add the checklist sections for code changes and make sure all items are checked off.\n   - While you've described the frontend components well, please also mention the significant backend changes (new API routes in `/api/builder`, pagination model extensions, etc.) that support this feature.\n\n2. **Implementation Notes**:\n   - The changes include backend routes and models to support the block menu functionality, which is good but should be mentioned in the PR description.\n   - I notice you've added several new npm packages (`react-timeago`, `tailwind-scrollbar`, etc.) - please include these in your description of changes.\n\n3. **Code Quality**:\n   - The backend implementation looks solid with well-structured endpoints and models.\n   - Good job on the TODO markers for future work.\n\nPlease update the PR description to include the missing checklist and mention the backend changes that support this feature. Once these items are addressed, the PR will be ready for another review.",
      "Thank you for your PR to redesign the block menu! I've identified a few issues that need to be addressed before this can be merged:\n\n1. **Scope mismatch**: While your PR title indicates this is a frontend change, approximately half of the changes are backend-related (new API endpoints, models, database functions). The PR title and description should accurately reflect all major components being changed.\n\n2. **Missing checklist**: For substantial code changes like this, the PR template checklist needs to be completed to ensure proper testing and quality checks. Please add the checklist and check off the appropriate items.\n\n3. **Backend changes documentation**: The PR description doesn't mention any of the backend changes. Please update the description to include details about the new backend/server/v2/builder files and endpoints you're adding.\n\nAdditionally, I notice you've marked some items with \"BLOCK MENU TODO:\" for future work. If these are part of this redesign effort but will be addressed in separate PRs, that's fine, but please make that clear in the description.\n\nOnce you've updated the PR title to accurately reflect both frontend and backend changes, completed the required checklist, and expanded the description to cover all major components being changed, we can proceed with the review process.",
      "Thanks for your PR on redesigning the block menu. I noticed a few things that need to be addressed before this can be merged:\n\n1. **Missing Checklist**: Please include the complete PR checklist in your description. Since your changes include significant code modifications, you'll need to add the checklist with appropriate test plans.\n\n2. **Scope Mismatch**: Your PR title only mentions frontend changes (`feat(frontend)`), but the diff shows substantial backend additions including:\n   - New API routes in `/api/builder`\n   - New database functionality\n   - New models and response types\n\n   Please update your PR title to reflect both the frontend and backend changes, perhaps something like `feat(frontend,backend): Redesign block menu on builder page`.\n\n3. **PR Description**: Consider enhancing your description to explain more about the purpose of the backend changes and how they relate to the frontend block menu redesign.\n\nThe changes themselves look promising, but we need to ensure the PR follows our submission guidelines before proceeding with the review of the actual implementation. Please make these updates so we can move forward with the review.",
      "Thank you for your work on redesigning the block menu! I have a few concerns that need to be addressed before this PR can be merged:\n\n1. **Missing Checklist**: The PR description is missing the required checklist. Please update your description to include the standard checklist and fill it out completely.\n\n2. **Scope Mismatch**: The PR title mentions only frontend changes (`feat(frontend)`), but the diff shows significant backend changes as well. Please update the PR title to accurately reflect the scope of your changes. Something like `feat(platform): Redesign block menu on builder page` or `feat(platform/frontend,platform/backend): Redesign block menu on builder page` would be more appropriate.\n\n3. **Backend Changes**: The PR includes substantial backend additions:\n   - New builder routes and endpoints\n   - New database functions and models\n   - Changes to the store API\n\nPlease make sure to include these backend changes in your test plan when you add the checklist.\n\nOnce these items are addressed, we can proceed with the review of the code itself. The overall direction of the redesign looks promising!",
      "Thank you for working on redesigning the block menu. I see you've created several reusable components for this feature.\n\nI have two main concerns with this PR:\n\n1. **Missing Checklist**: Your PR description is missing the required checklist that should cover testing plan and other verification steps. Please update your description to include the standard checklist and fill it out appropriately.\n\n2. **Scope Mismatch**: Your PR title mentions only a frontend redesign, but the changes include significant backend work. I see new backend files for a builder API, including routes, models, and database functionality. The PR title should reflect both frontend and backend changes, something like: `feat(frontend,backend): Redesign block menu and add builder API`\n\nPlease update your PR to address these issues so we can proceed with the review. The code changes themselves look well-structured, but we need to ensure the PR follows our process requirements.\n\nAlso, I noticed you've marked some items with `BLOCK MENU TODO:` for future work. That's a good approach for tracking remaining tasks.",
      "Hi @Abhi1992002, thanks for your work on redesigning the block menu. I have some feedback to help get this PR ready for merging:\n\n## Required Changes\n\n1. **PR Description & Checklist**\n   - Please include the complete PR checklist from our template, including your test plan\n   - The description should explain both the frontend and backend changes\n\n2. **PR Scope**\n   - Your PR title mentions only frontend changes, but there are significant backend changes\n   - Please update the title to reflect both (e.g., `feat(frontend,backend): Redesign block menu on builder page`)\n\n3. **Backend Testing**\n   - The new API endpoints should have proper testing documentation\n   - Please describe how you've tested the new endpoints\n\n4. **User ID Security**\n   - For the new backend routes, please confirm that user_id checks are properly implemented\n\n## Questions\n\n1. What testing have you done for both the frontend components and backend endpoints?\n2. Are there any configuration changes needed for these new components?\n3. How do the new endpoints interact with the redesigned block menu?\n\nOnce these items are addressed, I'd be happy to re-review the PR. Thanks!",
      "Thanks for working on redesigning the block menu for the builder page! This is an important improvement that will enhance the user experience. However, there are a couple of issues that need to be addressed before we can merge this PR:\n\n1. **Missing Checklist**: The PR template requires a filled-out checklist for all code changes. Please update your PR description to include the checklist from the template with all applicable items checked.\n\n2. **Scope Mismatch**: Your PR title indicates this is a frontend change, but the diff contains significant backend changes including:\n   - Adding new backend routes under `/api/builder`\n   - Creating new backend models and database functions\n   - Modifying the store API\n\n   Please either:\n   - Update the PR title to something like `feat(platform): Redesign block menu with backend support` to accurately reflect both frontend and backend changes, or\n   - Split this into separate PRs for frontend and backend changes\n\nAdditionally, while your PR description lists the components you've added, it would be helpful to include a brief explanation of the backend changes as well.\n\nOnce these items are addressed, we can proceed with the review of the implementation details. The component organization looks good, and I appreciate your marking future tasks with the `BLOCK MENU TODO:` tag for easy reference.",
      "Thanks for working on the block menu redesign! This PR adds important components and functionality for the builder page.\n\n### Key Issues to Address\n\n1. **Missing Checklist:** Please add the required checklist to your PR description. This is mandatory for code changes and should include details about your test plan.\n\n2. **Scope Clarification:** Your PR title focuses on frontend changes, but includes significant backend changes (new endpoints, models, DB functions). While these backend changes appear necessary to support the frontend redesign, consider either:\n   - Updating the PR title to reflect both frontend and backend changes (e.g., `feat(platform): Redesign block menu on builder page with supporting API`)  \n   - OR splitting the PR into separate frontend and backend PRs if that makes sense\n\n3. **PR Conflicts:** There's a conflicts label on your PR - please resolve these merge conflicts.\n\n### Additional Notes\n\n- The TODO comments you've marked with `BLOCK MENU TODO:` are fine as placeholders for future work.\n- Good job properly implementing auth middleware on the new API routes.\n- Consider adding more details about what the redesigned block menu includes and what improvements it brings.\n\nOnce you've addressed these issues, particularly adding the required checklist, the PR should be ready for another review.",
      "Thank you for your PR to redesign the block menu! The implementation looks thorough with many new components created for the builder page.\n\nHowever, I've noticed a couple of issues that need to be addressed before this can be merged:\n\n1. **Missing checklist**: The PR description is missing the required checklist from our PR template. Since this is a significant code change, please update your PR description to include a completed checklist with items like:\n   - Confirmation that you've tested your changes\n   - A test plan outlining how you verified your implementation works correctly\n\n2. **Scope mismatch**: Your PR title mentions only frontend changes, but the PR includes substantial backend implementations including:\n   - A new backend/server/v2/builder module with model, db, and routes files\n   - New API endpoints\n   - Modifications to existing store functionality\n\n   Please update your PR title to reflect both frontend and backend changes, perhaps something like: `feat(frontend,backend): Redesign block menu and add builder API endpoints`\n\nOnce these issues are addressed, we can proceed with the review of your implementation. The code itself looks well-structured with appropriate user_id handling in the backend functions.",
      "Thanks for your PR on redesigning the block menu! There are a few items that need to be addressed before this can be merged:\n\n## Missing Required Information\n\n1. **Checklist**: Your PR is missing the required checklist section. Since this PR involves significant code changes, we need to ensure all necessary checks have been completed.\n\n2. **Test Plan**: Please include details on how you've tested these changes.\n\n## Scope Clarification\n\nYour PR title mentions frontend redesign, but includes significant backend changes (new API endpoints, models, etc.). While these backend changes appear to support the frontend redesign, it would be helpful to:\n\n- Update your PR title to reflect both frontend and backend changes, e.g., `feat(platform): Redesign block menu with supporting backend APIs`\n- OR clarify in your description how the backend changes are necessary for the frontend redesign\n\n## Documentation\n\nPlease add some brief documentation on the new backend endpoints you've created to help other developers understand their purpose and how they relate to the block menu redesign.\n\nThe code itself looks good - I see you've properly protected all routes with auth middleware and correctly handle user_id passing where needed. Once you've addressed these items, we can proceed with the review.",
      "Thank you for working on redesigning the block menu! This is an important improvement for the builder page UI. I've identified a couple of issues that need to be addressed before this PR can be merged:\n\n1. The PR description is missing the required checklist. Please update your description to include the checklist from our template, with all applicable items checked off.\n\n2. The PR title mentions only frontend changes (`feat(frontend)`), but the PR includes significant backend additions (new routes, models, and DB functions in `/backend/server/v2/builder/`). Either:\n   - Update the PR title to reflect both frontend and backend changes (e.g., `feat(platform): Redesign block menu with backend support`)\n   - Or split this into separate PRs for frontend and backend changes\n\nYour implementation looks good overall - I can see you've built reusable components and made sure the backend functions properly handle user_id for authentication. Please address the issues above so we can proceed with the review.\n\nNote: There's also a conflict label on this PR that will need to be resolved before merging.",
      "Thank you for your PR to redesign the block menu on the builder page! This looks like a substantial piece of work that adds many components and backend support.  \n\nHowever, I've noticed a few issues that need to be addressed before this can be merged:\n\n1. **Missing Checklist**: Your PR is missing the required checklist. Please add the complete checklist from the PR template and check off the items you've completed. Given the scope of your changes, the checklist is necessary to ensure everything has been properly tested.\n\n2. **PR Title/Scope Mismatch**: Your PR title only mentions frontend changes, but there are significant backend changes as well (new API routes, database functions, models). Please update your PR title to reflect both aspects, perhaps something like: `feat(frontend,backend): Redesign block menu on builder page with API support`\n\n3. **Test Plan**: Please provide a test plan detailing how you've verified that both the frontend components and backend API routes work correctly.\n\n4. **PR Description**: Consider expanding your description to briefly explain the purpose of the backend changes and how they support the frontend redesign.\n\nYour component organization looks good, and I appreciate that you've marked future tasks with `BLOCK MENU TODO:`. Once you address these items, we'll be able to move forward with the review process.",
      "Thanks for your contribution to redesign the block menu! This is a substantial PR with changes across both frontend and backend.\n\n### Missing Required Checklist\n\nPlease add the standard PR checklist to your description and complete it. This is required for all PRs with material code changes. The checklist helps ensure you've tested your changes appropriately and considered all necessary factors.\n\n### Scope Consideration\n\nYour PR title mentions frontend changes, but there are significant backend changes as well (new API endpoints, database functions, models). Consider either:\n1. Updating the PR title to reflect both frontend and backend changes, or\n2. Splitting this into two PRs if the changes are separable (one for backend support, one for frontend implementation)\n\n### TODOs in Code\n\nYou mentioned \"Some tasks are planned for the near future. I've marked them with `BLOCK MENU TODO:` so they can be found easily.\" Please ensure that these TODOs are addressed before the PR is merged, or create follow-up issues to track them.\n\n### Other Notes\n\n- The new backend endpoints look well-structured with appropriate auth middleware\n- The implementation seems comprehensive with search functionality, filtering, and pagination\n- Good job on adding new dependencies in package.json for the enhanced UI components\n\nPlease update your PR description with the required checklist and consider the scope recommendation.",
      "Thank you for your PR! I noticed a few issues that need to be addressed before this can be merged:\n\n## Missing Checklist\nYour PR contains significant code changes but doesn't include the required checklist. Please add the standard checklist to your PR description and ensure all items are checked off.\n\n## Scope Mismatch\nYour PR title mentions only frontend changes (`feat(frontend): Redesign block menu on builder page`), but the actual changes include extensive backend work:\n- New backend routes in `/api/builder/`\n- New database functions and models for builder functionality\n- Changes to store and library endpoints\n\nPlease update your PR title to reflect both frontend and backend changes, for example: `feat(frontend,backend): Redesign block menu and add builder API endpoints`\n\n## PR Description Needs Expansion\nPlease provide more details in your PR description:\n- What problem does this redesign solve?\n- What are the key changes in both frontend and backend?\n- How should reviewers test these changes?\n\nThe current description lists small components but doesn't explain the overall architecture or the backend additions.\n\nOnce these issues are addressed, I'll be happy to review this PR again. Let me know if you need any clarification!",
      "Thanks for your PR on redesigning the block menu! I have a couple of important issues that need to be addressed before this can be merged:\n\n## Required Changes\n\n1. **Missing Checklist**: The PR template requires a checklist to be completed for code changes. Please update your PR description to include the standard checklist and complete it.\n\n2. **PR Title/Scope Mismatch**: Your PR title indicates this is a frontend change (`feat(frontend)`), but the vast majority of changes are actually backend-related. The PR adds several new backend files (db.py, model.py, routes.py) and modifies existing backend code.\n\n   Please update your PR title to accurately reflect the scope of changes, such as `feat(platform): Redesign block menu with backend support` or similar that reflects both the frontend and backend work.\n\n3. **PR Description**: Consider expanding your PR description to explain why this redesign was needed and how it improves the current implementation. \n\n## Additional Notes\n\n- Your backend code changes look good from a security perspective - the functions properly use user_id for access control.\n- I noticed you've added a TODO comment \"BLOCK MENU TODO:\" for future work. Make sure these are tracked separately if they won't be addressed in this PR.\n- You've added some new dependencies in package.json (tailwind-scrollbar, react-timeago, etc.) - it would be helpful to mention these in the PR description.\n\nPlease update your PR with these changes so we can proceed with the review process. Let me know if you need any clarification!",
      "Thanks for your work on redesigning the block menu! I noticed a few issues that need to be addressed before this PR can be merged:\n\n## Description and Scope\n\n- Your PR title indicates only frontend changes, but there are significant backend changes (new API routes, models, etc.). The scope in the title should reflect all areas being modified, such as `feat(platform)` or both frontend and backend.\n- The PR description focuses on frontend components but doesn't mention the backend API work. Please update your description to include details about all changes made.\n\n## Missing Required Checklist\n\n- The PR is missing the required checklist that should be filled out completely. Since this includes substantial code changes, please add the checklist and check off all applicable items.\n- Please include a test plan detailing how you've verified the changes work correctly.\n\n## Suggested Changes\n\n1. Update the PR title to correctly represent the scope of changes (e.g., `feat(platform): Redesign block menu on builder page`)\n2. Expand the PR description to include details about the backend API changes\n3. Add the required checklist template and fill it out completely\n4. Include a test plan that describes how you've tested these changes\n\nOnce these issues are addressed, we can proceed with reviewing the code changes in detail.",
      "Thank you for your PR to redesign the block menu! I've reviewed your changes and have a few concerns that need to be addressed before this can be merged:\n\n## Missing Checklist\nThe PR description is missing the required checklist for code changes. Please update your description to include the standard checklist that confirms you've tested your changes properly.\n\n## Scope Mismatch\nYour PR title indicates this is a frontend change (`feat(frontend): Redesign block menu on builder page`), but the changes include significant backend additions:\n- New builder API endpoints\n- New database functions\n- New models and data structures\n\nThese backend changes appear to be more than 20% of the total changes, which exceeds our scope guidelines. You have two options:\n\n1. Split this into two PRs: one for frontend and one for backend changes\n2. Update the PR title to reflect both areas (e.g., `feat(frontend,backend): Redesign block menu with supporting API endpoints`)\n\nThe new backend code looks good in terms of security and user_id handling, but we need to ensure the PR accurately represents the work being done.\n\nPlease update your PR accordingly, and let me know if you need any help or clarification!",
      "Thanks for the PR to redesign the block menu! The components structure you've listed looks well organized.\n\nI noticed a couple of issues that need to be addressed before this can be merged:\n\n1. **Missing checklist**: The PR template requires a checklist section to be completed for code changes. Please add the checklist from the template and fill it out appropriately, especially including your test plan.\n\n2. **Scope mismatch**: Your PR title indicates this is purely a frontend change (`feat(frontend)`), but the PR contains substantial backend changes including:\n   - New backend API routes in `/api/builder`\n   - New backend models and database methods\n   - Changes to existing backend services\n\nPlease update either:\n   - The PR title to reflect both frontend and backend changes (e.g., `feat(platform): Redesign block menu and add builder API endpoints`)\n   - OR split this into separate PRs for frontend and backend changes\n\nAlso, I notice you've marked some tasks with \"BLOCK MENU TODO\" for future work. It would be helpful to clarify in the PR description which parts are complete in this PR and which parts are planned for future PRs.\n\nLet me know if you need any help with these changes!",
      "Thank you for your work on redesigning the block menu for the builder page. However, there are a couple of issues that need to be addressed before this PR can be merged:\n\n1. **Missing Checklist**: Your PR is missing the required checklist section. Please update your PR description to include the standard checklist and fill it out completely. This helps ensure all necessary steps have been completed before merging.\n\n2. **Scope Mismatch**: Your PR title mentions only frontend changes (`feat(frontend): Redesign block menu on builder page`), but the PR includes extensive backend changes as well. I see new backend routes, models, and database functions being added in:\n   - `backend/server/v2/builder/db.py`\n   - `backend/server/v2/builder/model.py`\n   - `backend/server/v2/builder/routes.py`\n   - And modifications to several other backend files\n\nYou have two options to address this:\n   - Update your PR title to reflect both frontend and backend changes, such as `feat(platform): Redesign block menu with supporting backend APIs`\n   - Or split this into separate PRs - one for frontend and one for backend changes\n\nPlease make these adjustments so we can proceed with the review. The changes themselves look valuable, but we need to ensure the PR follows our standards."
    ],
    "num_comments": 25,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 314421,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/blocks/agent.py b/autogpt_platform/backend/backend/blocks/agent.py\nindex c25d99458d6d..406bff9e06bf 100644\n--- a/autogpt_platform/backend/backend/blocks/agent.py\n+++ b/autogpt_platform/backend/backend/blocks/agent.py\n@@ -23,6 +23,9 @@ class Input(BlockSc"
  },
  {
    "pr_title": "feat(platform): Add Block Development SDK with auto-registration system",
    "pr_body": "## Block Development SDK - Simplifying Block Creation\n\n### Problem\nCurrently, creating a new block requires manual updates to **5+ files** scattered across the codebase:\n- `backend/data/block_cost_config.py` - Manually add block costs\n- `backend/integrations/credentials_store.py` - Add default credentials  \n- `backend/integrations/providers.py` - Register new providers\n- `backend/integrations/oauth/__init__.py` - Register OAuth handlers\n- `backend/integrations/webhooks/__init__.py` - Register we",
    "pr_number": 10074,
    "comments": [
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "FYI I deleted all the comments with a script to clean up all the deployment testing",
      "üßπ **Preview Environment Cleaned Up**\n\nAll resources for PR #10074 have been removed:\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema `pr10074` dropped\n\n*Cleanup completed successfully.*",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "‚ùå **Preview Environment Deployment Failed**\n\nüö® The preview environment deployment encountered critical errors and has been rolled back.\n\n**Cleanup Completed:**\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema dropped\n- üîå All services terminated\n\n*Please check the workflow logs for details and try again.*",
      "Note: I've only reviewed the backend side of this\r\n\r\nFor tests, I'd like to see the full extent of the builder implications tested. \r\nEX: we can do with API key, with OAuth, with extra config, etc, all in one go. It shouldn't be too bad to keep up with due to Claude\r\n\r\nIt's also not clear to me if the builder is order-dependent or if it contains its own internal state machine for stepping through the order as it decides. Do the two examples below behave the same?\r\n```\r\nblah\r\n.withOAuth()\r\n.withApiKey()\r\n.build()\r\n\r\nvs \r\n\r\nblah\r\n.withApiKey()\r\n.withOAuth()\r\n.build()\r\n```\r\n\r\nAlso suggested a few things that I think can make it easier for people / AI to work with",
      "> Note: I've only reviewed the backend side of this\r\n> \r\n> For tests, I'd like to see the full extent of the builder implications tested. EX: we can do with API key, with OAuth, with extra config, etc, all in one go. It shouldn't be too bad to keep up with due to Claude\r\n> \r\n> It's also not clear to me if the builder is order-dependent or if it contains its own internal state machine for stepping through the order as it decides. Do the two examples below behave the same?\r\n> \r\n> ```\r\n> blah\r\n> .withOAuth()\r\n> .withApiKey()\r\n> .build()\r\n> \r\n> vs \r\n> \r\n> blah\r\n> .withApiKey()\r\n> .withOAuth()\r\n> .build()\r\n> ```\r\n> \r\n> Also suggested a few things that I think can make it easier for people / AI to work with\r\n\r\nThe order of builder functions does not matter other than build going at the end",
      "üßπ **Preview Environment Cleaned Up**\n\nAll resources for PR #10074 have been removed:\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema `pr_10074` dropped\n\n*Cleanup completed successfully.*"
    ],
    "num_comments": 8,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 298181,
    "code_diff": "diff --git a/.github/workflows/platform-frontend-ci.yml b/.github/workflows/platform-frontend-ci.yml\nindex 5aed4257879b..38815b54673a 100644\n--- a/.github/workflows/platform-frontend-ci.yml\n+++ b/.github/workflows/platform-frontend-ci.yml\n@@ -148,6 +148,7 @@ jobs:\n           onlyChanged: true\n      "
  },
  {
    "pr_title": "feat(platform/library): Scheduling UX",
    "pr_body": "Complete the implementation of the Agent Run Scheduling UX in the Library.\r\n\r\nDemo:\r\n\r\nhttps://github.com/user-attachments/assets/701adc63-452c-4d37-aeea-51788b2774f2\r\n\r\n### Changes üèóÔ∏è\r\n\r\nFrontend:\r\n- Add \"Schedule\" button + dialog + logic to `AgentRunDraftView`\r\n  - Update corresponding logic on `AgentRunsPage`\r\n  - Add schedule name field to `CronSchedulerDialog`\r\n    - Amend Builder components `useAgentGraph`, `FlowEditor`, `RunnerUIWrapper` to also handle schedule name input\r\n    - Split `Cr",
    "pr_number": 10246,
    "comments": [
      "Thank you for your contribution to improve the scheduling UX in the platform library! I see that you've added several important features like API+backend data paths for scheduling, buttons for creating/deleting schedules, and updated the scheduling API.\n\nHowever, I cannot approve this PR yet because:\n\n1. **The checklist is incomplete** - Please check off all the items or remove sections that aren't applicable. At minimum, we need to see that you've:\n   - Clearly listed your changes (which you have)  \n   - Made and executed a test plan\n\n2. **Missing test plan** - Please add details about how you've tested these scheduling features. For example:\n   - Creating a schedule for an agent\n   - Verifying the schedule executes correctly\n   - Deleting a schedule\n   - Testing the UI components for scheduling\n\nOnce you've addressed these items, we can proceed with the review. The changes themselves look appropriate for the scope described in the PR title.",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "Thank you for working on the scheduling feature implementation! There are a few things that need to be addressed before this PR can be merged:\n\n### Missing Information\n\n1. **Checklist**: Please complete the checklist in the PR description. All items need to be checked off to indicate you've verified them.\n\n2. **Test Plan**: Please add a test plan explaining how you've tested these changes.\n\n3. **Backend Data Changes**: Since there are modifications to backend/data files, please explain how user_id checking is handled in the new/modified functions.\n\n### Scope Alignment\n\nThe PR title mentions \"platform/library\" but the changes span both frontend and backend components (which is also reflected in the PR labels). Consider updating the title to better reflect the scope of changes, perhaps something like: `feat(platform): Scheduling UX implementation`\n\n### Description Enhancements\n\nConsider expanding the description to better capture all the changes made. For example:\n- What specific API endpoints were updated/added?\n- How does the new scheduling flow work from a user perspective?\n- Are there any database changes or migrations needed?\n\nOnce these items are addressed, we'll be happy to review this PR again for merging. Thanks for your contribution to improving the scheduling feature!",
      "Thank you for working on completing the scheduling feature implementation in the Library. There are a few items that need to be addressed before this PR can be merged:\n\n1. **Checklist Completion**: Please complete the checklist in the PR description. Each item needs to be checked off to confirm you've verified these aspects of your changes.\n\n2. **Test Plan**: Include a specific test plan that outlines how you've tested the scheduling functionality. This should cover creating schedules, deleting schedules, and verifying that scheduled runs execute as expected.\n\n3. **PR Scope**: The title mentions only 'platform/library' but your changes span across both frontend and backend components (as evidenced by the file changes and labels). Consider updating the title to better reflect the full scope of changes, such as 'feat(platform): Complete scheduling implementation in Library'.\n\n4. **More Detailed Description**: While your description lists the changes, it would be helpful to provide a bit more context about how these changes improve the scheduling UX and what the expected user experience will be after these changes.\n\nOnce these items are addressed, your PR will be ready for another review.",
      "Thank you for your PR implementing scheduling features in the Library. I have a few points of feedback to help get this ready for merging:\n\n1. **Checklist not completed**: None of the checkboxes in the PR description are checked. Please complete the checklist by checking off all items or removing sections that are not applicable.\n\n2. **Scope in title doesn't match changes**: Your PR title indicates `feat(platform/library)`, but the changes span both frontend and backend components. Consider updating the title to better reflect the scope, perhaps something like `feat(platform): Scheduling UX` or `feat(platform/library,backend): Scheduling UX`.\n\n3. **Backend data modifications**: Since you're modifying files in `backend/data/`, please ensure that any added/changed functions pass the user_id and compare it correctly, or provide an explanation for why this isn't needed.\n\n4. **Test plan**: Please include a test plan that describes how you've tested these scheduling features.\n\nOnce you address these points, we can proceed with the review of the actual code changes.",
      "Thank you for your detailed PR on enhancing the scheduling UX! The description and implementation plan look thorough and well-organized.\n\nHowever, I notice your checklist indicates you haven't completed testing according to your test plan yet. Before this PR can be merged, please:\n\n1. Complete the testing steps you've outlined in your checklist\n2. Check the box to confirm testing has been completed\n\nYour test plan looks comprehensive, covering schedule creation, execution, deletion, and UI state management. Once you've verified all these aspects work correctly, please update the PR.\n\nThe changes themselves look appropriate for the scope defined in your PR title (platform/library scheduling UX improvements), covering both frontend and backend components needed for this feature.",
      "Thanks for this comprehensive PR improving the scheduling functionality! The changes look well-organized and clearly documented.\n\nHowever, I noticed that many of your test plan items are not checked off:\n- You've verified that newly created schedules appear in the list\n- But you haven't confirmed that scheduled runs execute successfully\n- You also haven't verified the deletion functionality works properly\n- And you haven't tested the UI behavior when the last schedule is deleted\n\nBefore we can merge this PR, please complete the testing according to your test plan and update the checklist to reflect the completed tests. This will ensure that all the functionality you've implemented works as expected.\n\nLet me know if you encounter any issues during testing that need to be addressed!",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)"
    ],
    "num_comments": 26,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 91656,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/data/__init__.py b/autogpt_platform/backend/backend/data/__init__.py\nnew file mode 100644\nindex 000000000000..7cbc4487be58\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/data/__init__.py\n@@ -0,0 +1,5 @@\n+from .graph import NodeModel\n+from .integrat"
  },
  {
    "pr_title": "feat(platform, blocks): Webhook-triggered blocks",
    "pr_body": "- Resolves #8352\r\n\r\n## Changes üèóÔ∏è\r\n\r\n- feat(blocks): Add GitHub Pull Request Trigger block\r\n\r\n### feat(platform): Add support for Webhook-triggered blocks\r\n- ‚ö†Ô∏è Add `PLATFORM_BASE_URL` setting\r\n\r\n- Add webhook config option and `BlockType.WEBHOOK` to `Block`\r\n  - Add check to `Block.__init__` to enforce type and shape of webhook event filter\r\n  - Add check to `Block.__init__` to enforce `payload` input on webhook blocks\r\n  - Add check to `Block.__init__` to disable webhook blocks if `PLATFORM_BA",
    "pr_number": 8358,
    "comments": [
      "Ready for review while I iron out the last few details",
      "Convert this to an issue plz?\r\n> Nice-to-have: make a button on webhook blocks to trigger a ping and check its result. The API endpoints for this is already implemented.",
      "![image](https://github.com/user-attachments/assets/f1d2b2a2-7550-456e-af11-1754fe3d1a5a)\r\ncredentials seems non compatible with #8516 \r\n\r\nAlso hit this issue \r\n```\r\nINFO:     127.0.0.1:64414 - \"POST /api/graphs HTTP/1.1\" 400 Bad Request\r\n2024-11-12 19:01:02,046 ERROR  POST /api/graphs failed: Failed to create GitHub webhook: Validation Failed\r\n* url is missing a scheme\r\nTraceback (most recent call last):\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/sentry_sdk/integrations/fastapi.py\", line 143, in _sentry_app\r\n    return await old_app(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/server/routers/v1.py\", line 186, in create_new_graph\r\n    return await do_create_graph(create_graph, is_template=False, user_id=user_id)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/server/routers/v1.py\", line 221, in do_create_graph\r\n    graph = await on_graph_activate(\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\", line 43, in on_graph_activate\r\n    updated_node = await on_node_activate(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\", line 140, in on_node_activate\r\n    new_webhook = await webhooks_manager.get_suitable_webhook(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/base.py\", line 40, in get_suitable_webhook\r\n    return await self._create_webhook(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/base.py\", line 138, in _create_webhook\r\n    provider_webhook_id, config = await self._register_webhook(\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/github.py\", line 122, in _register_webhook\r\n    raise ValueError(f\"Failed to create GitHub webhook: {error_msg}\")\r\nValueError: Failed to create GitHub webhook: Validation Failed\r\n* url is missing a scheme\r\nINFO:     127.0.0.1:64466 - \"POST /api/graphs HTTP/1.1\" 400 Bad Request\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/ebd92341-5b65-4221-8c79-837e8eb934ef)\r\n\r\n\r\nWe should probably have a better error message than that for saying \"set your .env correctly\"",
      "\r\nhttps://github.com/user-attachments/assets/d9a71e7f-f6ae-4483-a770-9e6cf87d045f\r\n\r\nrunning this agent gets me the following \r\n\r\nNote the weird credentials fields",
      "Not sure if related but also can't delete creds",
      "> running this agent gets me the following\r\n> \r\n> [video demonstrating error on run]\r\n\r\nGood catch. I'm not sure how to deal with this. The graph can't be run manually because it relies on a webhook to trigger it. Should we hide the \"Run\" button if a graph can't be run manually? Should it just do nothing?\r\n\r\n> Note the weird credentials fields\r\n\r\nDo you mean it still shows the field title **Credentials** while hiding the actual input? That is a bug, possibly improperly resolved merge conflict. I'll look into it.",
      "Github PR review split the convos, im sorry <3",
      "> Good catch. I'm not sure how to deal with this. The graph can't be run manually because it relies on a webhook to trigger it. Should we hide the \"Run\" button if a graph can't be run manually? Should it just do nothing?\r\n\r\nwhat's toran and john think",
      "> The graph can't be run manually because it relies on a webhook to trigger it. Should we hide the \"Run\" button if a graph can't be run manually? Should it just do nothing?\r\n\r\nSo there needs to be a way of saying that you want your Agent to be \"Running\" - in this case listening for a webhook - or \"Stopped\" - i.e not listening on a webhook. I was thinking the run button would do that here.\r\n\r\nWhat's the current UX for this? Let's sync on this one @Pwuts as it's a lot for a comment.\r\n",
      "Created follow-up ticket #8671",
      "> The advanced button here does nothing?\r\n\r\nThat's because the block has no \"advanced\" inputs, and the toggle doesn't hide when there is nothing to toggle.\r\n\r\n> We probably want these to be able to trivially link\r\n\r\nYes, there are a few ways to do that but most of those are out of scope for this PR and the rest not a sustainable fix imo. We should do a QOL improvement on all of the GitHub blocks to address stuff like this.\r\n\r\n> How do I pass a variable to this block [picture of GitHub webhook trigger block]\r\n\r\nYou don't. Due to the system's architecture, the webhook trigger block can't accept input links and must be the starting node.\r\n\r\n> Why is the output not number type for number\r\n\r\nBecause `NodeHandle` doesn't know what an `integer` is apparently:\r\nhttps://github.com/Significant-Gravitas/AutoGPT/blob/86535b5811f8d1cc0bdde2232693919c4b1115e3/autogpt_platform/frontend/src/components/NodeHandle.tsx#L22-L29\r\n\r\n- [ ] Add `integer` type to `NodeHandle` type list\r\n\r\n> we should probably allow the block to output the repo, URL, etc too for the trigger if its not taking in inputs\r\n\r\nMy idea for a sustainable and scalable fix for that is: allow directly connecting links to nested properties of object outputs. That's way out of scope for this PR.\r\n\r\nDue to the block layout, I don't want to add a large number of outputs because that just fills up the screen very quickly.\r\n\r\n> We need to clarify Payload, Sender and Pull request for normal people\r\n\r\nIf you don't know what a pull request is, why would you be using this block?\r\n\r\n- [x] Improve description of `payload` output\r\n\r\n> I'm not sure the diff in pull request and Payload\r\n\r\ncan't parse, come again?\r\n\r\n> I assume sender is creator?\r\n\r\nSender already has the description *\"Object representing the user who triggered the event\"*. Do you think that output also needs a better name, and if so what?\r\n\r\n> I have no idea (as a dev, not even user) how to debug this via UI. As a dev, I checked the raw output of the block in the logs. It just \"didn't work\" but succeeded from the UI perspective\r\n\r\nI also just debug by looking at the backend logs. Suggestions welcome.\r\n\r\nWe could store all incoming webhook payloads and add a view for that, but that's a significant feature addition. WDYT?\r\n\r\n> If a user does a bad design (ex: leaving out a value on a comparison) the webhook rejects but they should probably know that when saving because it will be an issue they won't be able to diagnose.\r\n\r\nYeah the node needs an indicator for whether a webhook is attached or not. Determining why can usually be done client-side, because it depends directly on whether the user filled out all the required inputs on the node.\r\n\r\n- [x] Create issue for webhook status indicator on webhook-triggered nodes\r\n\r\n> The Run button throws an error when you run (this is better than crashing tho)\r\n\r\nWould you rather hide the button? I'm not sure how to properly fix this.\r\n\r\n> We may want to do something to require platform base URL to be set if someone uses a trigger because currently it just doesn't do anything.\r\n\r\n- [x] Disable webhook-triggered blocks if `PLATFORM_BASE_URL` is not set\r\n- [x] Raise error in `BaseWebhooksManager` on attempt to create webhook if `PLATFORM_BASE_URL` is not set\r\n\r\n> we currently don't actually check platform base URL on inbound webhooks so we just execute from anything lol.\r\n> \r\n> > Replicate by running ngrok and disabling the line in your .env\r\n\r\nWhy would we need to check it on inbound webhook payloads? If it arrives, that's a job done. The `PLATFORM_BASE_URL` is only necessary to configure the webhook in the first place."
    ],
    "num_comments": 11,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 179892,
    "code_diff": "diff --git a/autogpt_platform/backend/.env.example b/autogpt_platform/backend/.env.example\nindex b6d41c25d449..0dd10e838501 100644\n--- a/autogpt_platform/backend/.env.example\n+++ b/autogpt_platform/backend/.env.example\n@@ -28,8 +28,15 @@ SUPABASE_URL=http://localhost:8000\n SUPABASE_SERVICE_ROLE_KEY="
  },
  {
    "pr_title": "New rule: fix missing `git clone` when given a URL",
    "pr_body": "Often when I'm trying to `git clone` a project, I'll paste the URL into my terminal expecting it to also contain the `git clone` part. This rule should allow `thefuck` to detect the missing `git clone` and suggest it when someone pastes an SSH or HTTP/HTTP url that ends in `.git`.\r\n\r\nA rule for correcting for double `git clone git clone [repo]` already exists. This rule addresses the opposite problem.\r\n\r\n![image](https://user-images.githubusercontent.com/31365175/169957706-14e2c30d-5f84-4fb1-b59",
    "pr_number": 1302,
    "comments": [
      "@djh82 would appreciate a re-review. I think I've fixed up all the issues",
      "@scorphus thanks for your feedback! I believe that this rule would be worthwhile as I encountered it enough times to choose to spend my time on creating a rule for it. I tutor a computer science class where we teach git and have observed that I'm far from the only person to make this mistake. Since there's a rule for handling `git clone git clone`, which is useful when I type `git clone` then paste the URL, only for it to include its own `git clone` (Bitbucket does this), I'd say that this particular rule is reasonable enough, since it handles the opposite case.\r\n\r\nRE: your feedback on output matching, it does seem to be a little pointless. If you're interested in merging this PR at some point then I'll absolutely get rid of it.\r\n\r\nPerhaps it could search for `'git'` within the URL as an additional measure to avoid false positives? This would still work for all most of the popular git servers I'm aware of, at least using SSH. Would that change make the addition be worthwhile? Can you think of any other methods I could use to reduce the number of false positives?",
      "Thanks for sharing your testimonial. Let's get this merged.\r\n\r\nRegarding matching the output, I thought about disregarding output altogether and caring only about `output.script` or `output.script_parts`.\r\n\r\nAsserting `git` in the URL would blacklist bitbucket repos.",
      "Thanks again for the feedback! I fixed up things as per your suggestion, and also removed the code that checked the output.\r\n\r\nDouble checking, since I'm not entirely sure, should it check the URL for the presence of `git` somewhere? The only case I can think of that wouldn't work would be cloning using HTTPS on Bitbucket where `.git` has been manually removed from the end of their copied URL. Their SSH username is `git`, and their copy clone button copies the `git clone` part anyway. It would be helpful for preventing false positives such as [YouTube videos](https://www.youtube.com/watch?v=dQw4w9WgXcQ) (I apologise in advance).\r\n\r\n",
      "Pretty sure I've got everything fixed up now! Let me know if there's anything else you'd like me to do!",
      "Just bumping this one again @scorphus \r\nIs there anything else you want me to do on it?",
      "@MiguelGuthridge, please let me know your thoughts about the last change I submitted.",
      "@MiguelGuthridge, thanks for the complete patch! Much appreciated! üôå "
    ],
    "num_comments": 8,
    "repository": "nvbn/thefuck",
    "diff_length": 4096,
    "code_diff": "diff --git a/README.md b/README.md\nindex 0a1060598..d5e161ce4 100644\n--- a/README.md\n+++ b/README.md\n@@ -234,6 +234,7 @@ following rules are enabled by default:\n * `git_branch_0flag` &ndash; fixes commands such as `git branch 0v` and `git branch 0r` removing the created branch;\n * `git_checkout` &nd"
  },
  {
    "pr_title": "Run flake8 in Travis, fix some errors",
    "pr_body": "",
    "pr_number": 563,
    "comments": [
      "[![Coverage Status](https://coveralls.io/builds/8199300/badge)](https://coveralls.io/builds/8199300)\n\nCoverage decreased (-2.8%) to 90.289% when pulling **01c0e800cd2a236d2e00454937409278a8f26d82 on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8199300/badge)](https://coveralls.io/builds/8199300)\n\nCoverage decreased (-2.8%) to 90.289% when pulling **01c0e800cd2a236d2e00454937409278a8f26d82 on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8199300/badge)](https://coveralls.io/builds/8199300)\n\nCoverage decreased (-2.8%) to 90.289% when pulling **01c0e800cd2a236d2e00454937409278a8f26d82 on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8199743/badge)](https://coveralls.io/builds/8199743)\n\nCoverage increased (+0.08%) to 93.2% when pulling **b97830f6f743f7d03839a8e771e2018726692961 on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8199743/badge)](https://coveralls.io/builds/8199743)\n\nCoverage increased (+0.08%) to 93.2% when pulling **b97830f6f743f7d03839a8e771e2018726692961 on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8199743/badge)](https://coveralls.io/builds/8199743)\n\nCoverage increased (+0.08%) to 93.2% when pulling **b97830f6f743f7d03839a8e771e2018726692961 on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8199743/badge)](https://coveralls.io/builds/8199743)\n\nCoverage increased (+0.08%) to 93.2% when pulling **b97830f6f743f7d03839a8e771e2018726692961 on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8199743/badge)](https://coveralls.io/builds/8199743)\n\nCoverage increased (+0.08%) to 93.2% when pulling **b97830f6f743f7d03839a8e771e2018726692961 on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "Cheers @scorphus! It was your comments on #561 that inspired me to do this. I figured it might save some effort going forward.\n",
      "Thanks again for the comments, @scorphus! I made some more changes addressing them :)\n",
      "[![Coverage Status](https://coveralls.io/builds/8215085/badge)](https://coveralls.io/builds/8215085)\n\nCoverage increased (+0.08%) to 93.2% when pulling **31e127da1d47f290c81eac6aa82cddd0b8190ba4 on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "Awesome, @josephfrazier!\n\nWell, I feel I can push this even further üòÉ  As you're in the process of fixing these issues and changes haven't yet made into master, what would you say if I suggested you to remove a couple of commits and squash those which are logically similar/related?\n\nCommit 85a3e34 is a revert of 4d9f177, both could be removed. Also, all EXYZ-related commits could be joined together, leaving one single commit per violation ‚Äì¬†E123, E225, E231, E265, E302, E402, E711 and E731.\n\nWhat do you think about getting a cleaner history?\n",
      "Good call @scorphus, I did a lot more error-fixing and cleaned up the history :D \n",
      "[![Coverage Status](https://coveralls.io/builds/8221807/badge)](https://coveralls.io/builds/8221807)\n\nCoverage increased (+0.08%) to 93.2% when pulling **df8e5ecb40e3e2efa4d2004ad7d54ac6e54652ad on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8221807/badge)](https://coveralls.io/builds/8221807)\n\nCoverage increased (+0.08%) to 93.2% when pulling **df8e5ecb40e3e2efa4d2004ad7d54ac6e54652ad on josephfrazier:flake8** into **ce6b82c92d78ae283cb3db001766b76f6647bc47 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8243763/badge)](https://coveralls.io/builds/8243763)\n\nCoverage increased (+0.08%) to 93.232% when pulling **dda9d55989cd6c499624388ba55f900d0f42d892 on josephfrazier:flake8** into **4d714994a38d8b2f4342ab4f5e331b9db254076b on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8243763/badge)](https://coveralls.io/builds/8243763)\n\nCoverage decreased (-2.8%) to 90.335% when pulling **dda9d55989cd6c499624388ba55f900d0f42d892 on josephfrazier:flake8** into **4d714994a38d8b2f4342ab4f5e331b9db254076b on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/8243763/badge)](https://coveralls.io/builds/8243763)\n\nCoverage decreased (-2.8%) to 90.335% when pulling **dda9d55989cd6c499624388ba55f900d0f42d892 on josephfrazier:flake8** into **4d714994a38d8b2f4342ab4f5e331b9db254076b on nvbn:master**.\n",
      "@nvbn, do you have any thoughts on this? Is there anything else that needs to be done to land it (other than fixing files that have been changed on the `master` branch)?",
      "@josephfrazier, ooops, sorry, I just forgot about this pr.\r\n\r\nI'll check it in the nearest time and will merge it.\r\n\r\nThansk.",
      "Great, thanks! I just pushed some changes that fix the files that had been updated on the master branch, but haven't rebased them yet. If everything looks good to you, let me know and I can `git rebase -i --autosquash master` to combine the `fixup!` commits with their referents and get rid of that `Merge branch 'master' into flake8` commit.",
      "Cheers, hopefully this will make it easier to ensure future contributions adhere to the code style."
    ],
    "num_comments": 22,
    "repository": "nvbn/thefuck",
    "diff_length": 44347,
    "code_diff": "diff --git a/.travis.yml b/.travis.yml\nindex 2798eed99..990a7f004 100644\n--- a/.travis.yml\n+++ b/.travis.yml\n@@ -42,6 +42,7 @@ install:\n   - python setup.py develop\n   - rm -rf build\n script:\n+  - flake8\n   - export COVERAGE_PYTHON_VERSION=python-${TRAVIS_PYTHON_VERSION:0:1}\n   - export RUN_TESTS=\"c"
  },
  {
    "pr_title": "Adding devcontainer for easy Python development",
    "pr_body": "Love the tool, wanted to contribute and here is the result :)\r\n\r\n- Added a [VSCode devcontainer](https://code.visualstudio.com/docs/remote/containers) to help people get up and running with the repo quicker without needing to install any tooling or do any setup of venv\r\n- Added a new rule to help when some repositories use `main` and some use `master` as the primary integration branch\r\n\r\nFeedback welcome and thanks for the great tooling",
    "pr_number": 1184,
    "comments": [
      "Also, the fix for Python 2.7 tests is already on current `master`, would you please rebase on top of that? üôÇ ",
      "Howdy @storey247! Thanks for contributing. I'd really love to read your comments on my suggestions/questions above. I understand that you're probably busy with many stuff. But I thought I'd ping you üôÇ ",
      "hi @scorphus thanks for all the feedback, yes I will action and get this PR split into two :smile: apologies for the delayed response, work is a bit crazy atm",
      "Awesome! I already split the commit into three, so no need to create another PR, I can merge the changes regarding the new rule and we leave this PR for the devcontainer addition. What do you think?",
      "Amazing! Nice work! üëèüëèüëè\r\n\r\nsounds like a plan, I‚Äôll rebase this on Monday when the work is merged and I‚Äôll tidy up the container so it‚Äôs nice and trim. \r\n\r\nThanks @scorphus ",
      "There you go, @storey247. Sorry for the extra noise. To continue your work, I'd suggest you force-resetting to the `main_master` branch on your remote, that way you'll have exactly what this PR currently has. Please don't hesitate to ask me for help with that.",
      "@scorphus I have now rebased my work from latest `master` and cleaned up the comments as discussed.\r\n\r\nI also added some notes into the `Contribute.md` to help people get up and running with the devcontainer setup if required.\r\n\r\nHopefully now this work can be merged in too. Let me know if there is anything else you need me to look at",
      "> Thats super, @storey247! Thanks so much for hanging in! Please consider my suggestion below.\r\n\r\nFeedback actioned, just waiting on the builds and then should be good to merge üëç "
    ],
    "num_comments": 8,
    "repository": "nvbn/thefuck",
    "diff_length": 4374,
    "code_diff": "diff --git a/.devcontainer/Dockerfile b/.devcontainer/Dockerfile\nnew file mode 100644\nindex 000000000..c0682570f\n--- /dev/null\n+++ b/.devcontainer/Dockerfile\n@@ -0,0 +1,10 @@\n+# See here for image contents: https://github.com/microsoft/vscode-dev-containers/tree/v0.163.1/containers/python-3/.devcont"
  },
  {
    "pr_title": "#1282 git misspelled",
    "pr_body": "The function git_support can not be useful if a command has the word \"git\" misspelled.\r\nThe purpose of this rule is to unfuck the commands like: gti commit -m 'message'.\r\nWithout this rule, for example, if you try to unfuck this command the result will be:\r\ngit commit -m message. This means that the quotation marks do not return. #1282\r\n\r\nThis rule is generally useful for every git command, in which the word git is misspelled.\r\n\r\nI have also run some tests!",
    "pr_number": 1292,
    "comments": [
      "I made test_not_match fucntion. I hope that now it is better!",
      "It feels like rather than solving this specifically for the git command, it might be prudent to try and solve the issue in the no_command rule?\r\n\r\nThe underlying issue is that https://github.com/nvbn/thefuck/blob/master/thefuck/shells/generic.py#L87 shlex removes the quotes when operating in posix mode (the default).\r\n\r\nWhat really needs to happen, is we need to use quote for each element of the new command, before calling join, i.e. we need to re-quote the split command.  So something like this:\r\n\r\n```python\r\n    return [' '.join(shell.quote(a) for a in [new_command] + command.script_parts[1:])\r\n            for new_command in new_cmds]\r\n```\r\n\r\nWhere quote is obtained from the current shell (`from thefuck.shells import shell`).  (Also, make my code less ugly, double list comprehension is unsightly!)",
      "Thank you a lot for the review! Is this commit satisfying for the project?",
      "You'll probably want to revert the readme change and add some further tests for no_command? ",
      "Sorry for the too many commits. Is it now okay?\r\nThank you a lot!",
      "@djh82 \r\nIs there any problem with the Ubuntu version 3.9 and what the reason why is not running properly?\r\nIs this PR okay for merging?",
      "Regarding the failure, please check #1310.",
      "@djh82 @scorphus\r\nAre there any other mistakes or missing cases?",
      "I believe that now it is correct!",
      "@NikosKakonas, could you please rebase this on top of current `master`? Thanks!",
      "@scorphus  Is it now okay? Thanks!",
      "@scorphus Is it okay for merging?",
      "@nkakonas does this justify a new release? This has been bugging me for a while.",
      "@scorphus could you cut a new release for this?"
    ],
    "num_comments": 14,
    "repository": "nvbn/thefuck",
    "diff_length": 1668,
    "code_diff": "diff --git a/tests/rules/test_no_command.py b/tests/rules/test_no_command.py\nindex 0df4590b2..96f0f069a 100644\n--- a/tests/rules/test_no_command.py\n+++ b/tests/rules/test_no_command.py\n@@ -21,7 +21,8 @@ def history_without_current(mocker):\n     ('vom file.py', 'vom: not found'),\n     ('fucck', 'fucc"
  },
  {
    "pr_title": "Globalize pyenv rule ",
    "pr_body": "Getting pyenv_no_such_command.py rule as base, I created an env_no_such_command.py script able to hanlde all env related commands. Transforming, in this way, the pyenv rule in a more global one. \r\n\r\nThis commit was originated and fixes #1074 ",
    "pr_number": 1100,
    "comments": [
      "@scorphus Is everything good with my pr, would you like me to enhance anything? :smile: ",
      "Yes I totally agree with all your points. My only thoughts lie on the matter of the global rule. Due to the almost identical commands of the env packages, my solution seemed really convenient. Though, if you think is better to make separate files for each command, I will work on it for sure. Probably the testing process will be more efficient in this way as well. ",
      "The identical parts of the rules could be kept in a single, separate ‚Äú`devenv`‚Äù submodule imported by all of the rules. Such submodule would reside under `thefuck/specific` along with other specific ones. This way there‚Äôs less repetition. What do you think?",
      "Yes, that' s a great idea! I will work on it, as well as the requested changes and come up with a new pr.",
      "Need to check why the tests fail, otherwise I think I managed to fullfill the requested changes and they seem really great!",
      "@scorphus Hello again, everything seems to work pretty great! Unfortunately, your idea of integrating some of the common code of the rules to specific/devenv.py was making the tests to fail, so I decided to simplify it. Hope I find some extra time and manage to integrade more of the common code of the rules in a couple of weeks.\r\n\r\nThank you for your help!",
      "You're on fire! üî• I hope I soon have time to review it more thoroughly. Thanks!",
      "Hello @scorphus I didn't have any time to enhance this commit, as I can understand you haven't found any time to review it in order to merge it as well. Let me know about your status :sunglasses: ",
      "Hey @scorphus,\r\n\r\nIt's okay don't worry! Let me know if you need any help, I will be glad to help üòÉ ",
      "I'd love if you could spare a review üôå",
      "Sorry guys I was pretty busy and saw the updates just now, cheers @divykj for the review! Glad to help @scorphus üôå"
    ],
    "num_comments": 11,
    "repository": "nvbn/thefuck",
    "diff_length": 4841,
    "code_diff": "diff --git a/README.md b/README.md\nindex 57ca6234e..3d965d064 100644\n--- a/README.md\n+++ b/README.md\n@@ -269,13 +269,13 @@ following rules are enabled by default:\n * `npm_wrong_command` &ndash; fixes wrong npm commands like `npm urgrade`;\n * `no_command` &ndash; fixes wrong console commands, for exa"
  },
  {
    "pr_title": "config.fish: improve documentation on creating Fish functions",
    "pr_body": "How about this as solution to #128? Please review and comment. Thanks!\n",
    "pr_number": 130,
    "comments": [
      "This solution works great, thanks.\n",
      "Glad it works for you! It depends on `sed`, though. But I believe it's certain to be available.\n",
      "Can it also include [solution for stdin](https://github.com/nvbn/thefuck/issues/76#issuecomment-96103809)?\n",
      "@nvbn do you mean adopting the workaround as mentioned in https://github.com/nvbn/thefuck/issues/76#issuecomment-96103809?\n",
      "@nvbn nice, 52f4852 does just that. Let's wait and see if anyone confirms it solves #76.\n",
      "@scorphus, it solves both bugs for me. \n",
      "@scorphus One thing on the `$TMPDIR` thing, a lot of Linux distributions don't appear to have it set by default. OS X does but not every Linux distro does so there should be a fallback there to `/tmp` if it's not set.\n",
      "Yeah, absolutely. First I read http://en.wikipedia.org/wiki/TMPDIR and changed it. But then I thought it over and decided to `vagrant up` Ubuntu 14.04, CentOS 6.4 and Debian 7 to test it and, no surprise. Now I'm testing with `mktemp` and it's working just fine in all cases.\n",
      "Ouch, no... too bad. `mktemp`'s implementations differ amongst Linux and OS X\n",
      "So, how about this:\n\n``` fish\nfunction __thefuck_repl --description 'Replace operators into fish-compatible'\n    set -l tmp (echo $argv | sed 's/ && / ; and /g')\n    echo $tmp | sed 's/ || / ; or /g'\nend\n\nfunction fuck --description 'Correct your previous console command'\n    set -l eval_script (mktemp 2>/dev/null ; or mktemp -t 'thefuck')\n    thefuck $history[1] > $eval_script\n    eval (__thefuck_repl (cat $eval_script))\n    rm $eval_script\nend\n```\n",
      "Can I merge it? Or it's not a final solution?\n",
      "I moved aliases to [wiki](https://github.com/nvbn/thefuck/wiki/Shell-aliases), it will be faster to change them there.\n",
      "I think it's good to go. Ping, @daenney, what do you think?\nEm 27/04/2015 02:58, \"Vladimir Iakovlev\" notifications@github.com\nescreveu:\n\n> I moved aliases to wiki\n> https://github.com/nvbn/thefuck/wiki/Shell-aliases, it will be faster\n> to change them there.\n> \n> ‚Äî\n> Reply to this email directly or view it on GitHub\n> https://github.com/nvbn/thefuck/pull/130#issuecomment-96514751.\n",
      "Ya, I think this should work. Needs a rebase though since Github thinks it has a merge conflict.\n",
      "Thanks for the input, @daenney!\n\nThe part of the README.md file regarding shell aliases and/or functions was move over to the wiki, that's why this is conflicting with master. I'll move the solution to the wiki too.\n",
      "One thing to remind ourselves of, though, is that from now on, every single rule that involves logical operators (`&&` and `||` for instance) should enclose them in blank spaces, like the following: `cmd_x && cmd_y` or `cmd_x || cmd_z`.\n"
    ],
    "num_comments": 16,
    "repository": "nvbn/thefuck",
    "diff_length": 658,
    "code_diff": "diff --git a/README.md b/README.md\nindex 9f55b5337..61bec38d2 100644\n--- a/README.md\n+++ b/README.md\n@@ -117,8 +117,16 @@ alias FUCK='fuck'\n Or in `config.fish`:\n \n ```fish\n-function fuck\n-    eval (thefuck $history[1])\n+function __thefuck_repl --description 'Replace operators into fish-compatible'\n"
  },
  {
    "pr_title": "Rule for branch dash 0",
    "pr_body": "fixes fat-fingering `git branch -0` instead of `git branch -v`",
    "pr_number": 942,
    "comments": [
      "Nice idea, maybe it will be a good thing to make that more generic? To just replace any argument that starts with `0` if it appears in `stderr`.",
      "@nvbn When I quickly read your comment on my phone, I thought making this fix more generic sounded like a good idea, but upon reflection, I'm not so sure.\r\n\r\nIn the specific case I coded up, the argument needs to turn into a flag. Would making that assumption more generic (i.e. convert all arguments starting with `0` to `-`) make sense?\r\n\r\nFor that matter, what would the \"undo\" action look like, or would there even need to be an \"undo\" action? (In the specific example here, I need to delete a branch that was just accidentally created...what should the proper response be if `git branch 0v` wasn't the command?",
      "@ProfessorTom you're right, initially I thought that `git branch 0v` prints something, but it just creates a branch.\r\n\r\nI guess the only way to generalize this rule is to also support cases like `git branch 0l`, `git branch 0a` and etc, so just check that the argument after `branch` starts with `0`.",
      "Do you still want me to generalize this feature turning the `0` into a `-` and deleting the branch just created in all cases?\r\n\r\nIs there a case where this would cause more harm than good?",
      "bumping to get a review and hopefully a merge.",
      "@scorphus @nvbn Can I get a review on my latest changes?",
      "What will it take to get this new approach reviewed and merged? cc @nvbn @scorphus @jamtur01 ",
      "Thank you for contributing, @ProfessorTom üëç \r\n\r\nPlease check a new PR, which is probably going to be #1212."
    ],
    "num_comments": 8,
    "repository": "nvbn/thefuck",
    "diff_length": 2935,
    "code_diff": "diff --git a/README.md b/README.md\nindex 838300762..070ee9759 100644\n--- a/README.md\n+++ b/README.md\n@@ -203,6 +203,7 @@ following rules are enabled by default:\n * `git_branch_delete` &ndash; changes `git branch -d` to `git branch -D`;\n * `git_branch_exists` &ndash; offers `git branch -d foo`, `git "
  },
  {
    "pr_title": "Fix Issue #959: breaks after composer require with single package, revamp composer rules",
    "pr_body": "Hello! I'm a day late for Hacktoberfest but I hope you find this PR useful.\r\n\r\nThis pull request aims to fix #959 and add better integration with the [composer](https://getcomposer.org) tool. It turns out the issue of the crash was not because \"there was only one suggestion\" as suggested in the Issue, but rather that the matcher erroneously matches __package not found__ errors instead of just __command not found__ errors. Since the output of the two varies significantly, I have created two separ",
    "pr_number": 1007,
    "comments": [
      "If only GitHub would show the comments in line-number order, other than chronological. Sorry if the review is confusing, please let me know if something is not clear.",
      "wow, thanks for the comprehensive code review! i'm currently occupied with midterm examinations at the moment so it'll take some time for me to follow up on your comments, i'll push my changes at the end of this week.",
      "Sorry for the delay. I've added in your requested changes, and also adopted Black as my code formatter.\r\n\r\nExcept for one problem: Black triggers `E203 whitespace before :` in line 31 of `composer_not_package.py`:\r\n\r\n\r\n```\r\nversion_constraint = offending_script_param[len(wrong_package_name) :]\r\n```\r\n\r\nAccording to [Black themselves](https://github.com/psf/black#slices), E203 should be ignored by flake. Perhaps we should check in a flake8 configuration file that follows their recommendation?",
      "Hey, thanks for keeping up! üëç \r\n\r\nI'm happy that you adopted Black. But maybe we're going too far with it. Like changing lines that are not part of the fix ‚Äì¬†which I've just noticed. Things such as formatting that should be part of a different PR ‚Äì¬†I know it was in the middle of one or two of my suggestions, I didn't notice it back then, sorry for that.\r\n\r\nAlso, I've just noticed that this PR fixes a rule and introduces another. These should be at least two separate commits.\r\n\r\nDo you think you can split the changes in separate commits, undo the lines that are not part of the fix/feature and update this PR? Otherwise, I could do that, if you don't mind. Authorship would be maintained, of course.",
      "This is my first code/feature related PR on a public repository so admittedly I'm not very familiar with best practices ‚Äì are you saying I should add more commits that reverses (removes) the `composer_not_package` rule, then open another PR (perhaps I refork `nvbn:master` and rebase), and then re-add `composer_not_package` and open a separate PR?\r\n\r\nI'm not very sure how to go about reversing selective lines of commits too; I don't use any Git GUI applications that can offer such a functionality.",
      "Sorry for the late reply, @chesnutcase.\r\n\r\nSplitting into two different PRs would be ideal, but as we've come this far, splitting changes into two different commits is more than enough.\r\n\r\nRegarding the changes introduced by the use of Black... On one hand, Black is great because it leaves no space for discussions about how to format code ‚Äì it does it for you. Out of a sudden, you stop wasting time with everything related to code formatting. On the other hand, when it comes to a codebase that's not previously formatted by it, that may generate undesirable noise, such as some parts of this pull request. So it's up to the developer to include only the relevant changes.\r\n\r\nPlease LMK if I can make myself any clearer. (I often fail at that)\r\n\r\nThanks again for contributing and for sticking to it!"
    ],
    "num_comments": 6,
    "repository": "nvbn/thefuck",
    "diff_length": 17954,
    "code_diff": "diff --git a/README.md b/README.md\nindex 386cb6833..c92a1cc14 100644\n--- a/README.md\n+++ b/README.md\n@@ -183,6 +183,7 @@ following rules are enabled by default:\n * `cd_parent` &ndash; changes `cd..` to `cd ..`;\n * `chmod_x` &ndash; add execution bit;\n * `composer_not_command` &ndash; fixes composer "
  },
  {
    "pr_title": "feat: new rule for `nix-shell`",
    "pr_body": "Implementation is similar to the one explained in https://github.com/nvbn/thefuck/issues/912#issue-441679613.\r\n\r\nIn a nutshell, it tries to wrap the user's failed command in a nix-shell call.\r\n\r\n```\r\n$ ponysay moo\r\nThe program 'ponysay' is not in your PATH. You can make it available in an\r\nephemeral shell by typing:\r\n  nix-shell -p ponysay\r\n\r\n$ fuck\r\nnix-shell -p ponysay --run \"ponysay moo\" [enter/‚Üë/‚Üì/ctrl+c]\r\n```\r\n\r\nFurther info on nix-shell: https://thiagowfx.github.io/2022/02/nix-shell-in-a-n",
    "pr_number": 1393,
    "comments": [
      "Been loving this. Hope it gets reviewed and upstreamed for all. Thanks for making this!",
      "I've just discovered `thefuck` and I can't imagine how I didn't stumble into it earlier.\r\n\r\nThis PR would fit like a glove for me that just switched to nixos and haven't yet grown the muscle memory of typing the `nix-shell` whenever my command fails.",
      "i would love to see this merged as well",
      "looks like you [can use this already](https://github.com/NixOS/nixpkgs/compare/master...KiaraGrouwstra:nixpkgs:thefuck-nix-shell) using e.g. an overlay, altho i had a bit of trouble getting it to work out of the box.\r\nspecifically, without adding `doCheck = false;`, i would run into this error:\r\n\r\n```\r\nerror: builder for '/nix/store/rl44gb6qd4x2myclj9i8cpkfrvw6ysqa-thefuck-3.32.drv' failed with exit code 2;\r\n       last 10 log lines:\r\n       > thefuck/system/unix.py:6\r\n       >   /build/source/thefuck/system/unix.py:6: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\r\n       >     from distutils.spawn import find_executable\r\n       >\r\n       > -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\r\n       > =========================== short test summary info ============================\r\n       > ERROR  - ModuleNotFoundError: No module named 'pytest_docker_pexpect'\r\n       > !!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\r\n       > ========================= 1 warning, 1 error in 0.09s ==========================\r\n       > /nix/store/bknngadwym46j65qs14ic2w79rpav888-stdenv-linux/setup: line 1582: pop_var_context: head of shell_variables not a function context\r\n```\r\n\r\ni had tried removing the added test, altho that appeared not to resolve the issue.\r\n",
      "it would seem cool to similarly get an approach using `nix run`, i.e. go from suggesting `nix-shell -p ponysay --run \"ponysay moo\"` to `nix run nixpkgs#ponysay -- moo` - this might eventually help extend beyond just `nixpkgs`.\r\n\r\nedit: https://github.com/KiaraGrouwstra/thefuck/commit/81d6786c80b86f2cc80b3ea90adc214df8266643\r\n",
      "I've been using a custom rule that supports the new [unified CLI](https://zero-to-nix.com/concepts/nix#unified-cli) for a while, and was planning on opening a PR once this one has been merged (I hesitate to update this current PR as it's already tested and ready to be merged). I don't know if that will happen soon, so in the meantime I've pushed the changes to [this](https://github.com/thenbe/thefuck/tree/nix-shell-new) new branch instead, which [builds](https://github.com/thenbe/thefuck/compare/nix-shell...thenbe:thefuck:nix-shell-new) on this here PR. You can use the updated rule by adding it as a [custom rule](https://github.com/nvbn/thefuck?tab=readme-ov-file#creating-your-own-rules) to your config.\r\n\r\nIn the new rule, three variants are suggested. Assuming I run `cowsay hello world`, I am presented with the following:\r\n\r\n1. `nix run nixpkgs#cowsay -- hello world`: This runs my command in a non-interactive shell. Uses the nix unified CLI.\r\n1. `nix shell nixpkgs#cowsay`: This enters an interactive shell with `cowsay` available, but does not run any command. This is useful if you'd rather run the command yourself after entering the shell because your command requires delicate massaging (e.g. running it with `sudo`, prefixing it with environment variable, juggling quote variants, etc).\r\n1. `nix-shell -p cowsay --run \"cowsay hello world\"`. This runs my command in a non-interactive shell. Uses the nix original CLI.\r\n1. `nix shell nixpkgs#cowsay --command cowsay hello world`: Very similar to the first one so I've personally disabled this one.\r\n\r\n### Thoughts on future updates:\r\n\r\n\r\n\r\n- It'd be nice if there was a variant that runs my command and then _keeps me_ in the shell.\r\n  - For the original CLI, we [can](https://nix.dev/manual/nix/2.19/command-ref/nix-shell#options) add a `--command \"echo hello; return\"` to our `nix-shell` invocation.\r\n  - For the unified CLI: not sure yet, we might need to do something like this example from the [docs](https://nix.dev/manual/nix/2.19/command-ref/new-cli/nix3-shell): ` nix shell nixpkgs#gnumake --command sh -c \"cd src && make\"`\r\n- We should expose a couple of flags for users to configure this.\r\n  - `disable_unified_cli` (boolean)\r\n  - `disable_original_cli` (boolean)\r\n- As far as I can tell, the `command-not-found` db doesn't really play nice if you use flakes to configure your system and might return stale results (unless you update it manually?). [`nix-index`](https://github.com/nix-community/nix-index) seems to be the go-to alternative. It'd be great if we could optionally use that instead (perhaps behind a flag `enable_nix_index` for users who have installed `nix-index` (`programs.nix-index.enable = true;` in home manager).",
      "@thenbe i agree integrating with `nix-index`'s `command-not-found` replacement seems cool, as a flake user.\r\ni kinda wish we could have `command-not-found` (and this `thefuck` integration) [extend to flake inputs beyond nixpkgs](https://github.com/nix-community/nix-index/issues/244) as well, such as to packages from NUR for example. preferably this should be dynamic based on your inputs rather than hardcoded to specific ones like nixpkgs, or NUR for that matter.\r\ni'll admit i haven't really figured out how that might work tho.",
      "just tried these with a command like `program_i_have | program_i_dont_have`, seems that may complicate the suggestions a bit",
      "I'm not sure if `thefuck` can handle piping.\r\n\r\nIf I make a typo `git statis` it will correct me to `git status`. But if I do `echo hello | git statis` it does not correct my typo. `thefuck` seems to work mostly on single commands AFAICT.\r\n",
      "@thenbe hm, i'm not sure.\r\n\r\n```\r\nfortune | cowsay\r\nThe program 'cowsay' is not in your PATH. It is provided by several packages.\r\nYou can make it available in an ephemeral shell by typing one of the following:\r\n  nix-shell -p cowsay\r\n  nix-shell -p neo-cowsay\r\n$ fuck\r\nnix run nixpkgs#fortune | cowsay\r\n```\r\n\r\nfeels like it knows about the whole command given it's reproducing it?\r\n",
      "another common nix thing we might be able to address from `thefuck` would be errors about packages being unfree\r\n\r\nedit: https://github.com/KiaraGrouwstra/thefuck/commit/16d838bf6f63117b161a2f1e6572e06108b007eb\r\n",
      "@thenbe what was the argument to favor `nix run` over `nix shell` again? i guess the latter seems a bit more generic in case of handling non-standard binaries at least",
      "If I'm only looking to execute a program (and don't need to be dropped into a shell) then I prefer `nix run` over `nix shell` as the [documentation](https://nix.dev/manual/nix/2.19/command-ref/new-cli/nix3-run) suggests `nix run` specifically for this use case.\r\n\r\nI also recall `nix run` being more performant (perhaps because we forego the overhead of launching a shell?). This last point is not derived from benchmarks, only anecdotal evidence.\r\n\r\n> i guess the latter seems a bit more generic in case of handling non-standard binaries at least\r\n\r\nI've added this variant (the 4th one in my [previous post](https://github.com/nvbn/thefuck/pull/1393#issuecomment-1961487094)), but disabled it after a while when I realized that I never reach for it. Do you find that you still need it over `nix run` (the 1st variant in my previous post)?",
      "> another common nix thing we might be able to address from `thefuck` would be errors about packages being unfree\r\n> \r\n> edit: [KiaraGrouwstra@16d838b](https://github.com/KiaraGrouwstra/thefuck/commit/16d838bf6f63117b161a2f1e6572e06108b007eb)\r\n\r\nThis would be useful. Does it still complain about the `--impure` flag? Or do you use a workaround for that?",
      "i've been using `thefuck` mostly thru its [`zsh` plugin](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/thefuck), which just gets you the top suggestion. i found that failed for me for e.g. `poppler_utils`, which bundles multiple binaries.\r\nto be fair tho, i'm not sure that accounts for a large portion of its invocations, so maybe it could make sense to just actually type out `fuck` in those cases.\r\n\r\nwhat was the `--impure` error? i'm not sure i'd run into that.\r\n\r\nby the way, had you managed to also package your branch for nix? considering i seemed to need that `doCheck = false;` to get our branches to build thru nix.\r\n",
      "I just have it aliased to `f` for extra convenience.\r\n\r\nI opted not to package it for nix separately since `fuck` already exposes a method for easily adding custom rules. Instead, I placed the rule in `~/mydotfiles/thefuck/rules/nix-shell.py` then told home-manager to symlink it to the appropriate place in `.config`:\r\n\r\n```nix\r\n# home.nix\r\nhome.file.\".config/thefuck/rules/nix-shell.py\".source = config.lib.file.mkOutOfStoreSymlink \"${config.home.homeDirectory}/mydotfiles/thefuck/rules/nix-shell.py\";\r\n```\r\n\r\nThis way I don't need to rebuild every time I tweak the rule.\r\n\r\n> what was the --impure error?\r\n\r\nThe unified CLI commands (`nix shell`, `nix run`, etc) will not acknowledge environment variables unless the `--impure` flag is used.\r\n\r\n<details>\r\n  <summary> output </summary>\r\n\r\n```\r\n$ NIXPKGS_ALLOW_UNFREE=1 nix shell nixpkgs#github-copilot-cli\r\n\r\nerror:\r\n       ‚Ä¶ in the condition of the assert statement\r\n\r\n         at /nix/store/xwc3zfc544jg6zhr0wi6k8253s7mwlhi-source/lib/customisation.nix:267:17:\r\n\r\n          266|     in commonAttrs // {\r\n          267|       drvPath = assert condition; drv.drvPath;\r\n             |                 ^\r\n          268|       outPath = assert condition; drv.outPath;\r\n\r\n       ‚Ä¶ while evaluating the attribute 'handled'\r\n\r\n         at /nix/store/xwc3zfc544jg6zhr0wi6k8253s7mwlhi-source/pkgs/stdenv/generic/check-meta.nix:490:7:\r\n\r\n          489|       # or, alternatively, just output a warning message.\r\n          490|       handled =\r\n             |       ^\r\n          491|         (\r\n\r\n       (stack trace truncated; use '--show-trace' to show the full trace)\r\n\r\n       error: Package ‚Äògithub-copilot-cli-0.1.36‚Äô in /nix/store/xwc3zfc544jg6zhr0wi6k8253s7mwlhi-source/pkgs/tools/misc/github-copilot-cli/default.nix:21 has\r\n an unfree license (‚Äòunfree‚Äô), refusing to evaluate.\r\n\r\n       a) To temporarily allow unfree packages, you can use an environment variable\r\n          for a single invocation of the nix tools.\r\n\r\n            $ export NIXPKGS_ALLOW_UNFREE=1\r\n\r\n          Note: When using `nix shell`, `nix build`, `nix develop`, etc with a flake,\r\n                then pass `--impure` in order to allow use of environment variables.\r\n\r\n       b) For `nixos-rebuild` you can set\r\n         { nixpkgs.config.allowUnfree = true; }\r\n       in configuration.nix to override this.\r\n\r\n       Alternatively you can configure a predicate to allow specific packages:\r\n         { nixpkgs.config.allowUnfreePredicate = pkg: builtins.elem (lib.getName pkg) [\r\n             \"github-copilot-cli-0.1.36\"\r\n           ];\r\n         }\r\n\r\n       c) For `nix-env`, `nix-build`, `nix-shell` or any other Nix command you can add\r\n         { allowUnfree = true; }\r\n       to ~/.config/nixpkgs/config.nix.\r\n\r\n\r\n```\r\n```bash\r\n# it wants this instead:\r\n$ NIXPKGS_ALLOW_UNFREE=1 nix shell nixpkgs#github-copilot-cli --impure\r\n```\r\n\r\n</details>\r\n",
      "aah i see! i'd yet to take that into account. üôà\r\n\r\nspecifying the rules rather than doing overlays makes sense - thanks!\r\n"
    ],
    "num_comments": 17,
    "repository": "nvbn/thefuck",
    "diff_length": 6305,
    "code_diff": "diff --git a/README.md b/README.md\nindex 48b4b0fb3..724dc3569 100644\n--- a/README.md\n+++ b/README.md\n@@ -367,6 +367,7 @@ The following rules are enabled by default on specific platforms only:\n * `brew_update_formula` &ndash; turns `brew update <formula>` into `brew upgrade <formula>`;\n * `dnf_no_suc"
  },
  {
    "pr_title": "start work on -y",
    "pr_body": "Fixes #531 \n\n[WIP]\n\n@scorphus:  I have never programmed in Python before, is this where I add the option, or is it somewhere else?\n\nTODO\n- [ ] add flag\n- [ ] make flag set `settings.require_confirmation: False`\n- [ ] make flag execute the rest of the command\n- [ ] add tests\n",
    "pr_number": 532,
    "comments": [
      "To have it implemented the way we discussed earlier ‚Äì at shell alias/function level ‚Äì it would involve rewriting the shell alias ‚Äì i.e. [`Fish.app_alias()`](https://github.com/nvbn/thefuck/blob/51415a5cb1ca6955fb99908e7d0e7bf012a66312/thefuck/shells/fish.py#L21) ‚Äì¬†in order to make it conditionally set `THEFUCK_REQUIRE_CONFIRMATION` to `0` prior to calling `thefuck ...`.\n",
      "Aha, it's there, I interpreted your comment differently!\n",
      "Great!\n\nps.: I'm pondering how this could be done in the Bash alias, for instance.\n",
      "Okay, so @scorphus, I found how to add a fish argument, but where should this be documented, tested?\n\ncontains in `fish`: https://fishshell.com/docs/current/commands.html#contains-example\n",
      "[![Coverage Status](https://coveralls.io/builds/7153857/badge)](https://coveralls.io/builds/7153857)\n\nCoverage remained the same at 92.304% when pulling **ff5372e288c370efd17d8f968daa847e933f23fe on Haroenv:feat/option-y-override-verification** into **51415a5cb1ca6955fb99908e7d0e7bf012a66312 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/7153857/badge)](https://coveralls.io/builds/7153857)\n\nCoverage remained the same at 92.304% when pulling **ff5372e288c370efd17d8f968daa847e933f23fe on Haroenv:feat/option-y-override-verification** into **51415a5cb1ca6955fb99908e7d0e7bf012a66312 on nvbn:master**.\n",
      "For powershell, something like [switch](https://social.technet.microsoft.com/Forums/windowsserver/en-US/2961ae65-d7d9-4c1d-bdf2-505273925ccc/advanced-functions-flags?forum=winserverpowershell) could be used (but I don't know how it finds out what the name of the flag is\n",
      "In `bash` (probably the other shells as well, seeing they're aliases instead of functions too) it'll need to be replaced by a [function](http://apple.stackexchange.com/a/51010/98431)\n",
      "> [‚Ä¶] I found how to add a fish argument, but where should this be documented, tested? [‚Ä¶]\n\nThere are some [funcitonal tests ](https://github.com/nvbn/thefuck/tree/51415a5cb1ca6955fb99908e7d0e7bf012a66312/tests/functional) that could include tests for the new `-y` functionality.\n\n> For powershell, something like [switch](https://social.technet.microsoft.com/Forums/windowsserver/en-US/2961ae65-d7d9-4c1d-bdf2-505273925ccc/advanced-functions-flags?forum=winserverpowershell) could be used (but I don't know how it finds out what the name of the flag is\n\nLooks like it'll work. Maybe @MattKotsenas could chime in and shed some light on the matter?\n\n> In `bash` (probably the other shells as well, seeing they're aliases instead of functions too) it'll need to be replaced by a [function](http://apple.stackexchange.com/a/51010/98431)\n\nYeah, that's the critical change, I think. Not sure how much it impacts on the way TheFuck is set up by users and [tools](https://github.com/Bash-it/bash-it/blob/7415134878b8b01015c9a9fdbc66b784db02ff65/aliases/available/fuck.aliases.bash) alike.\n\nPerhaps there should be a `--function` argument that always creates a `function`. Then we can leave `--alias` functionality untouched. What do you think?\n",
      "I just realised that this would unset the confirmation for everything after once `-y`, will need `else` and setting it back to the default. (or setting it to the default at the end of the command maybe?)\n",
      "Here's [another example](https://github.com/robbyrussell/oh-my-zsh/blob/a7e30b26baa94bac99d9d05cf642bd1942ae1787/plugins/thefuck/thefuck.plugin.zsh#L7) of a tool that uses TheFuck.\n\nThinking it again, on most cases there won't be any problems creating a `function` instead of an `alias`. Let me recall how users used to set them.\n",
      "> [‚Ä¶] setting it back to the default [‚Ä¶]\n\nThat might be unnecessary. It should be an _in-command-variable_, something like:\n\n``` fish\n# middle of Fish alias\nif dash_y\n  env THEFUCK_REQUIRE_CONFIRMATION=0 TF_ALIAS=fuck PYTHONIOENCODING=utf-8 thefuck <cmd>\nelse\n  env TF_ALIAS=fuck PYTHONIOENCODING=utf-8 thefuck <cmd>\nend\n# ‚Ä¶\n```\n",
      "Since [aliases changed in 1.34](https://github.com/nvbn/thefuck#update) and are defined with `eval \"$(thefuck --alias)\"` I think we might be okay changing it to a `function`.\n\n/cc @nvbn any concerns?\n",
      "Oh-My-Fish [TheFuck plugin](https://github.com/oh-my-fish/plugin-thefuck/blob/86f7e1b720f8395964f348f601b4b8fd9a1cf671/functions/fuck.fish), for example, will need to be updated to support this feature, although it would continue to work nonetheless.\n",
      "[![Coverage Status](https://coveralls.io/builds/7156314/badge)](https://coveralls.io/builds/7156314)\n\nCoverage decreased (-0.5%) to 91.828% when pulling **a97416272c65f4fa7aa5a8b4e81e7d77756c1de3 on Haroenv:feat/option-y-override-verification** into **51415a5cb1ca6955fb99908e7d0e7bf012a66312 on nvbn:master**.\n",
      "[![Coverage Status](https://coveralls.io/builds/7161601/badge)](https://coveralls.io/builds/7161601)\n\nCoverage remained the same at 92.304% when pulling **c02357c58f3386dc272f19b55efcfedd57e25756 on Haroenv:feat/option-y-override-verification** into **51415a5cb1ca6955fb99908e7d0e7bf012a66312 on nvbn:master**.\n",
      "@Haroenv: the condition can be written like this:\n\n``` fish\ncontains -- \"-y\" $argv; and set -lx THEFUCK_REQUIRE_CONFIRMATION 0\n```\n- `l`: sets the variable locally ‚Äì it is bound to the funciton scope only\n- `x`: exports the variable to sibling processes ‚Äì¬†`thefuck` will be able to read it\n",
      "[![Coverage Status](https://coveralls.io/builds/7167243/badge)](https://coveralls.io/builds/7167243)\n\nCoverage remained the same at 92.304% when pulling **190db9407cb55a3a40c3a892ba7893cc94d10e37 on Haroenv:feat/option-y-override-verification** into **51415a5cb1ca6955fb99908e7d0e7bf012a66312 on nvbn:master**.\n",
      "Maybe it can be a bit simpler, just a different alias like:\n\n``` bash\nalias fuckit='THEFUCK_REQUIRE_CONFIRMATION=0 fuck'\n```\n\n?\n",
      "That seems like a great idea, should need to check if that saves the variable or if it uses it just once though. Using fuckit as a new alias makes sense\n"
    ],
    "num_comments": 20,
    "repository": "nvbn/thefuck",
    "diff_length": 674,
    "code_diff": "diff --git a/thefuck/shells/fish.py b/thefuck/shells/fish.py\nindex 34fdf7b2c..22d8b3ddd 100644\n--- a/thefuck/shells/fish.py\n+++ b/thefuck/shells/fish.py\n@@ -26,6 +26,7 @@ def app_alias(self, fuck):\n             alter_history = ''\n         # It is VERY important to have the variables declared WITHIN "
  },
  {
    "pr_title": "üåê Add Ukrainian translation for `docs/uk/docs/index.md`",
    "pr_body": "",
    "pr_number": 5178,
    "comments": [
      "üåê Add Ukrainian translation for docs/uk/docs/index.md",
      "> –î—É–∂–µ –∫—Ä—É—Ç–æ, —Ç—ñ–ª—å–∫–∏ —Ö–æ—Ç—ñ–≤ —Å—ñ—Å—Ç–∏ –∑–∞ –ø–µ—Ä–µ–∫–ª–∞–¥ —è–∫ –±–∞—á—É –≤–∏ –≤–∂–µ –ø–æ—á–∞–ª–∏. –î–æ–¥–∞–≤ –∫—ñ–ª—å–∫–∞ –∞–π—Ç–µ–º—ñ–≤ —Å—Ç–æ—Å–æ–≤–Ω–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É. –ù–∞–¥—ñ—é—Å—è –≤–æ–Ω–∏ –±—É–ª–∏ –∫–æ—Ä–∏—Å–Ω–∏–º–∏ —ñ –≤–∏ –ø–æ–≥–æ–¥–∂—É—î—Ç–µ—Å—è. –©–µ —Ä–∞–∑ –¥—è–∫—É—é –∑–∞ –ø—Ä–æ—Ä–æ–±–ª–µ–Ω—É —Ä–æ–±–æ—Ç—É\r\n\r\n–î—è–∫—É—é –∑–∞ –¥–æ–ø–æ–º–æ–≥—É! –ü–æ–≥–æ–¥–∂—É—é—Å—å –∑ –í–∞—à–∏–º–∏ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è–º–∏. –Ø–∫—â–æ –±–∞–∂–∞—î—Ç–µ –ø—Ä–∏—î–¥–Ω–∞—Ç–∏—Å—è –¥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É - —Ü–µ —Å—É–ø–µ—Ä, –º–æ–∂–µ–º–æ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ —ñ —Ä–æ–±–∏—Ç–∏ –ø–µ—Ä–µ–∫–ª–∞–¥ —Ä–∞–∑–æ–º.",
      "> > –î—É–∂–µ –∫—Ä—É—Ç–æ, —Ç—ñ–ª—å–∫–∏ —Ö–æ—Ç—ñ–≤ —Å—ñ—Å—Ç–∏ –∑–∞ –ø–µ—Ä–µ–∫–ª–∞–¥ —è–∫ –±–∞—á—É –≤–∏ –≤–∂–µ –ø–æ—á–∞–ª–∏. –î–æ–¥–∞–≤ –∫—ñ–ª—å–∫–∞ –∞–π—Ç–µ–º—ñ–≤ —Å—Ç–æ—Å–æ–≤–Ω–æ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É. –ù–∞–¥—ñ—é—Å—è –≤–æ–Ω–∏ –±—É–ª–∏ –∫–æ—Ä–∏—Å–Ω–∏–º–∏ —ñ –≤–∏ –ø–æ–≥–æ–¥–∂—É—î—Ç–µ—Å—è. –©–µ —Ä–∞–∑ –¥—è–∫—É—é –∑–∞ –ø—Ä–æ—Ä–æ–±–ª–µ–Ω—É —Ä–æ–±–æ—Ç—É\r\n> \r\n> –î—è–∫—É—é –∑–∞ –¥–æ–ø–æ–º–æ–≥—É! –ü–æ–≥–æ–¥–∂—É—é—Å—å –∑ –í–∞—à–∏–º–∏ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è–º–∏. –Ø–∫—â–æ –±–∞–∂–∞—î—Ç–µ –ø—Ä–∏—î–¥–Ω–∞—Ç–∏—Å—è –¥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É - —Ü–µ —Å—É–ø–µ—Ä, –º–æ–∂–µ–º–æ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ —ñ —Ä–æ–±–∏—Ç–∏ –ø–µ—Ä–µ–∫–ª–∞–¥ —Ä–∞–∑–æ–º.\r\n\r\n–ó–≤—ñ—Å–Ω–æ —è —Ç—ñ–ª—å–∫–∏ –∑–∞. –î–∞–≤–∞–π—Ç–µ –∑–≤—è–∂–µ–º–æ—Å—è —ñ —Ä–æ–∑–¥—ñ–ª–∏–º–æ –ø–µ—Ä–µ–∫–ª–∞–¥. @python4eg –º—ñ–π –Ω—ñ–∫ –≤ —Ç–µ–ª–µ–≥—Ä–∞–º—ñ",
      "@ilkuzmenko Hey, you made a huge work there, are you able to finish this? I could approve this after fixing the comments that were left by reviewers. Or if you are not able or don't want to work on it anymore can I do my own based on your?\r\nP.S. Hope you are okay now",
      "@rostik1410 let me know if you need any help ",
      "As this PR had requested changes to be applied but has been inactive for a while, it's now going to be closed. But if there's anyone interested, feel free to create a new PR."
    ],
    "num_comments": 6,
    "repository": "fastapi/fastapi",
    "diff_length": 26567,
    "code_diff": "diff --git a/docs/uk/docs/index.md b/docs/uk/docs/index.md\nindex 29f92e020a7a8..fa9c9550bc493 100644\n--- a/docs/uk/docs/index.md\n+++ b/docs/uk/docs/index.md\n@@ -1,7 +1,3 @@\n-\n-{!../../../docs/missing-translation.md!}\n-\n-\n <p align=\"center\">\n   <a href=\"https://fastapi.tiangolo.com\"><img src=\"https:/"
  },
  {
    "pr_title": ":globe_with_meridians: Add Turkish translation for `docs/tr/docs/alternatives.md`",
    "pr_body": "Kafami karistiran bir iki farkli yaklasimi ceviri icerisinde denedim, ornegin orijinal dokumanlarda `abbr` tag'ini bazi kavramlar icin bir kac defa kullanip sonrasinda kullanmayi birakiyordu, ben de buna benzer bir yaklasim sergilemeye calistim. Bunun disinda haddimden fazla yerellestirme yapmis olabilirim, incelemenizde lutfen acimayin ki ortaya duzgun bir sey ciksin üòÑ ",
    "pr_number": 10502,
    "comments": [
      "üìù Docs preview for commit 121e6d6a9ed2176615b308489cc8953790ab72ff at: https://88cd7bfd.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit 88c6d84911a4bc17f7fe3723ff57e9f692ddf492 at: https://8d96dfce.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit ced982a7a3f540bbad91a844e4b55bf9b220294f at: https://8e3e519d.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit 03e7b2dfda6779e5c4db649dd11bd52439ae0152 at: https://fd75c3b9.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit 49c0ceb5a3704f6c06b4df44385abc0d0fcda15b at: https://2cdb64e8.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit 7b9dfc3be6b7180d1ead298b4df5030869b7c8d7 at: https://b36663fa.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit add6aa7b5cf12e2d9ba722eb3e84363c13e07309 at: https://8d3d0eae.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit dca64ba1cd69184188f2898bc0aabeb43122cff1 at: https://04ac286b.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit ada79e684085b8f762522a9def6039de04510030 at: https://0140430d.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit 4c871702a100bfe668513ebd1c24762f53c8f910 at: https://01de0dd0.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit 4b2019ee4a07accf2746394cb3f5b16fba011b3d at: https://eef7d8e2.fastapitiangolo.pages.dev",
      "Nice! Thanks @alperiox ü§ìüöÄ \r\n\r\nAnd thanks for your help @hasansezertasan ü•≥üíØ "
    ],
    "num_comments": 12,
    "repository": "fastapi/fastapi",
    "diff_length": 28460,
    "code_diff": "diff --git a/docs/tr/docs/alternatives.md b/docs/tr/docs/alternatives.md\nnew file mode 100644\nindex 0000000000000..9c69503c9812e\n--- /dev/null\n+++ b/docs/tr/docs/alternatives.md\n@@ -0,0 +1,409 @@\n+# Alternatifler, ƒ∞lham Kaynaklarƒ± ve Kar≈üƒ±la≈ütƒ±rmalar\n+\n+**FastAPI**'ya neler ilham verdi? Diƒüer altern"
  },
  {
    "pr_title": "üåê Update Turkish translation for `docs/tr/docs/python-types.md`",
    "pr_body": "Fixes mistranslations and updates outdated doc",
    "pr_number": 10445,
    "comments": [
      "üìù Docs preview for commit 018703a41b7236195f10c1bdefa04b228f387071 at: https://149a3713.fastapitiangolo.pages.dev",
      "Geri d√∂n√º≈üler i√ßin te≈üekk√ºrler.",
      "üìù Docs preview for commit 574596ceaae7cbb1447a4868e518111dab393a15 at: https://6b65633d.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit aba7b4cb9f46728322e1d84f78563990f670ee9a at: https://461f92ef.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit 1a6ae53e896e361c456c19dc8395861c130024f4 at: https://3aad993a.fastapitiangolo.pages.dev",
      "As this PR had requested changes to be applied but has been inactive for a while, it's now going to be closed. But if there's anyone interested, feel free to create a new PR."
    ],
    "num_comments": 6,
    "repository": "fastapi/fastapi",
    "diff_length": 23285,
    "code_diff": "diff --git a/docs/tr/docs/python-types.md b/docs/tr/docs/python-types.md\nindex 3b9ab905079e5..b4bb1fe30ba47 100644\n--- a/docs/tr/docs/python-types.md\n+++ b/docs/tr/docs/python-types.md\n@@ -1,19 +1,19 @@\n # Python Veri Tiplerine Giri≈ü\n \n-Python isteƒüe baƒülƒ± olarak \"tip belirte√ßlerini\" destekler.\n+Pyt"
  },
  {
    "pr_title": "üåê Add Russian translation for `docs/ru/docs/tutorial/response-model.md`",
    "pr_body": "",
    "pr_number": 9675,
    "comments": [
      "üìù Docs preview for commit 1ad46b67e6dc33ffb1178d18a86c922aae69b44a at: https://6489918cc356c5078c7143d3--fastapi.netlify.app",
      "üìù Docs preview for commit cb8277b848f98da3dca93934efbf2e9afcee516a at: https://6489a1b7cb6e8811f92dc3e4--fastapi.netlify.app",
      "üìù Docs preview for commit 40c1afec5cf87a49b12596d35a282d182d679c90 at: https://648c2ed97897085123307603--fastapi.netlify.app",
      "üìù Docs preview for commit e8137f6f049b9b0580269782f05339ef99be56ad at: https://648c39c1546bc455bcbe061a--fastapi.netlify.app",
      "üìù Docs preview for commit 481e4826b1222d63127d1a49cbee546163e6e22e at: https://648c3d3362aae25b60aaa027--fastapi.netlify.app",
      "üìù Docs preview for commit 6aabbb641b1cf022cb863ccb90dbe29dd57edb0c at: https://648c41322c8d36602e0da830--fastapi.netlify.app",
      "üìù Docs preview for commit 7e008fbad029c8948821cdae7cf08889c299d868 at: https://64905a8944734c66b54f1453--fastapi.netlify.app",
      "üìù Docs preview for commit 3159e9fe77a6880746e306c69697e4bc6f6eb63d at: https://64905c2c44734c676c4f14dd--fastapi.netlify.app",
      "üìù Docs preview for commit aa88cafcf0ce37179d67c54413b803e896ec7f03 at: https://64905d8e72e8806ce36e030c--fastapi.netlify.app",
      "üìù Docs preview for commit 410e5a4eb9c1fbdabdebc01c6f010987975e0ac0 at: https://649472f6b121c94bf1ab2b91--fastapi.netlify.app",
      "üìù Docs preview for commit 55b6d628fb9b975c60311c61dc4311c28fd724f8 at: https://649a3167b8e96720081c1f8a--fastapi.netlify.app",
      "Awesome, thanks @glsglsgls! üöÄ \r\n\r\nAnd thanks for the reviews @ivan-abc and @Alexandrhub üôá ‚òï "
    ],
    "num_comments": 12,
    "repository": "fastapi/fastapi",
    "diff_length": 21221,
    "code_diff": "diff --git a/docs/ru/docs/tutorial/response-model.md b/docs/ru/docs/tutorial/response-model.md\nnew file mode 100644\nindex 0000000000000..c5e111790dcb3\n--- /dev/null\n+++ b/docs/ru/docs/tutorial/response-model.md\n@@ -0,0 +1,480 @@\n+# –ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∞ - –í–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–π —Ç–∏–ø\n+\n+–í—ã –º–æ–∂–µ—Ç–µ –æ–±—ä—è–≤–∏—Ç—å —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞, —É"
  },
  {
    "pr_title": "üåê Initialize translations for Traditional Chinese",
    "pr_body": "",
    "pr_number": 10505,
    "comments": [
      "üìù Docs preview for commit 3a0d3836b5a53cc11b6946e60eb9b5d7ef3d75f0 at: https://15e5624d.fastapitiangolo.pages.dev",
      "Hello, @tiangolo \r\n\r\nWe want to create the language for zh-hant (Mandarin), but the language will be limited to 2 letters. What are your recommended approaches for handling languages that have more than two letters?",
      "Thanks @hsuanchi! Up to now, I've avoided language localizations as there aren't yet enough translations of the first language, for example, for `pt-PT`.\r\n\r\nThe first question would be, how different is it from `zh`? Would `zh` not be understandable by people who speak `zh-hant`?\r\n\r\nThe second question is, are there others (at least other two, in total with you at least three) that are willing to help with those translations to `zh-hant`? Because I need to get 2 approvals to merge, and if there are now two variants of `zh`, it's probably gonna be even more difficult to get approvals for both.\r\n\r\nThere are currently 63 PRs for `zh` awaiting for reviewers: https://github.com/tiangolo/fastapi/pulls?q=is%3Apr+is%3Aopen+sort%3Aupdated-desc+label%3Alang-zh\r\n\r\nLet me know what you think!",
      "Hello @tiangolo ,\r\n\r\nThank you for your swift reply. To address your first question, zh is generally considered to be Simplified Chinese (zh-hans), while zh-hant refers to Traditional Chinese. Although the two variants share similarities, they differ significantly in terms of characters and some terminology. People from regions such as Taiwan, Hong Kong, and Macau typically use zh-hant and may find zh less intuitive to read.\r\n\r\nAs for your second question, we have a robust team of over 10 developers who use FastAPI on a daily basis. Some of our members have also contributed to the Traditional Chinese translation of Python documentation ([python-docs-zh-tw](https://github.com/python/python-docs-zh-tw)). We're confident that we'll be able to efficiently review and contribute to the zh-hant localization for FastAPI.\r\n\r\nThank you once again, and looking forward to your reply.",
      "Hi @tiangolo ,\r\n\r\nI would like to kindly inform you that I am part of the same team as @hsuanchi , and we are wholeheartedly dedicated to supporting the translation of the content into `zh-hant`. Let's collaborate to make it even more exceptional!",
      "I'm an avid user of FastAPI and would be thrilled to review @hsuanchi  's translation. Additionally, I'd be honored to contribute to the translation efforts for FastAPI, aiming to make it even more accessible and developer-friendly for the Chinese-speaking community.\"",
      "Fastapi basically consists of my day-to-day life and I'm confident that I can dedicate myself to contributing to the translations to zh-hant as zh-hant is my first language.\r\nIt'd be my pleasure to review @hsuanchi's translation and make the development of Fastapi more friendly to the Chinese-speaking community.",
      "Hello @tiangolo,\r\n\r\nI am in the same team as @hsuanchi. I am very eager to assist in the zh-hant localization to make FastAPI more accessible for the Traditional Chinese community. Looking forward to hearing from you!",
      "I am a user of FastAPI from Taiwan, and I am very eager to promote FastAPI among Traditional Chinese-speaking users. This way, more users can read the documentation in their native language, which would lower the barrier to entry.",
      "Hi @tiangolo,\r\n\r\nIf I could contribute to the translation efforts of FastAPI, making community documents more clear and intuitive for developers using the zh-hant language, I find it highly meaningful.",
      "Dear @tiangolo,\r\n\r\nI am extremely excited and looking forward to seeing FastAPI in Traditional Chinese. The purpose of this initiative is to assist developers in Taiwan in using and understanding FastAPI more easily. We are committed to continuing to support and contribute to FastAPI.",
      "As a software engineer from Hong Kong and Taiwan, I can tell that FastAPI is important for Traditional Chinese software community, we have a large community here working on Python with FastAPI in Taiwan.\r\n\r\nCan't wait to see more Traditional Chinese resources and contributions so FastAPI can be widely promoted.",
      "Traditional Chinese (`zh_hant`) and Simplified Chinese (`zh_hans`) are different *writing systems* of Chinese. Both are categorized under the `zh` code in ISO 639-1, and using this two-character code alone is insufficient to differentiate between the two. (Similarly, I'd say the term \"Mandarin\" in the PR title is not accurate enough.) They have different histories and cultures. Both are official in their regions and are widely used with rich literature and media. Their users generally have a strong preference for one writing system over the other based on their background. Though Mandarin is mostly spoken in both regions that use Traditional and Simplified Chinese characters, regional variations exist between both systems.\r\n\r\nIn the field of software technical document translation, many prominent resources offer both Traditional and Simplified Chinese versions simultaneously. Examples include [Python docs](https://docs.python.org/zh-tw/3/), [MDN docs](https://developer.mozilla.org/zh-TW/), [Microsoft docs](https://learn.microsoft.com/zh-tw/docs/), [AWS docs](https://docs.aws.amazon.com/zh_tw/), [React docs](https://zh-hant.legacy.reactjs.org/), [Angular docs](https://angular.tw/), and more. Although the momentum for zh_hant translation might not be as extensive as zh_hans, it's undeniable that Traditional Chinese is a legitimate and important language version for software documents.\r\n\r\nAs a Python backend engineer and a member of the Python document translation community, I'm more than willing to help with translating the FastAPI doc or reviewing for the translation PRs. (Actually, [I tried once before](https://github.com/tiangolo/fastapi/discussions/9758) lol).\r\n",
      "Hi @tiangolo , \r\n\r\nI am a software engineer who works with @hsuanchi. I understand your concern about the use of Traditional Chinese (`zh-Hant`) for translations. Here's a more detailed explanation of why it's essential to consider using `zh-Hant` for FastAPI translations:\r\n\r\n`zh-Hant`, which represents Traditional Chinese, is crucial for ensuring the accessibility and readability of FastAPI content for a specific audience. Traditional Chinese characters are primarily used in regions like Taiwan, Hong Kong, and among overseas Chinese communities. If your project has users in these areas or if your goal is to be inclusive of these regions, providing translations in `zh-Hant` is a necessity.\r\n\r\nHere's why `zh-Hant` matters:\r\n1. **Cultural Sensitivity**: Using Traditional Chinese characters is a matter of cultural sensitivity and respect. Some users in regions where Traditional Chinese is the norm may find it more comfortable and culturally appropriate when content is presented in `zh-Hant`.\r\n\r\n2. **Audience Reach**: Including `zh-Hant` extends your reach and ensures that FastAPI is accessible to a broader audience. It allows you to engage users who may prefer or are more familiar with Traditional Chinese characters.\r\n\r\n3. **User Experience**: Users tend to have a better user experience when content is in their preferred language variant. It enhances user satisfaction and encourages adoption.\r\n\r\nIn summary, providing translations in `zh-Hant` alongside `zh` is a strategic move to make FastAPI more inclusive, culturally sensitive, and user-friendly for a wider audience. It demonstrates your commitment to engaging with users from different regions and respecting their language and cultural preferences.\r\n\r\nWe hope you consider the importance of using `zh-Hant` as part of the translation efforts for FastAPI. Thank you for your understanding and support.",
      "Hi @tiangolo,  are there any updates? Are we ready to begin work on the Traditional Chinese version?\r\n\r\n\r\n\r\n\r\n\r\n",
      "üìù Docs preview for commit 84b953ba9a18cb60f296ae9bdbdada60937e7562 at: https://3397a9ff.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit 953ad1489ace9cae73f2fdcf3dfa1df4e94aec20 at: https://b5531961.fastapitiangolo.pages.dev",
      "> Here is the initial review of the translation part.\r\n\r\nThanks for helping with the review ü´°ü´°ü´°, all amendments are now complete.",
      "üìù Docs preview for commit 73e95a72238dfc79c8d895ced60692e6fd8839a9 at: https://d5229374.fastapitiangolo.pages.dev",
      "üìù Docs preview for commit 66cba69728d587ee07cbb3b08168e0c252f9e50f at: https://c9489c35.fastapitiangolo.pages.dev",
      "Amazing! If you all work together you could have all the FastAPI docs translated to Traditional Chinese in no time! üéâ \r\n\r\nI updated the script to handle docs to take this into account, and I added this discussion: https://github.com/tiangolo/fastapi/discussions/10949\r\n\r\nWhen there's a new PR for Traditional Chinese and I add the label, it will be automatically posted there. You could subscribe to that discussion to get notified when there's a new PR to review. ü§ì \r\n\r\nThank you all for your help!\r\n\r\nOnce this PR is ready and has two approving reviews I'll merge it. The same with the next future PRs. üöÄ ",
      "üìù Docs preview for commit efc2197589ec94dda13015d4a17d80d9bc4b6334 at: https://be6f65a0.fastapitiangolo.pages.dev",
      "Awesome, thank you @hsuanchi ! üç∞ \r\n\r\nAnd thanks for the reviews @SonnyYou, @mattwang44 ‚òï üç™ "
    ],
    "num_comments": 23,
    "repository": "fastapi/fastapi",
    "diff_length": 16402,
    "code_diff": "diff --git a/docs/en/mkdocs.yml b/docs/en/mkdocs.yml\nindex 92d081aa12333..ab8066aa67b50 100644\n--- a/docs/en/mkdocs.yml\n+++ b/docs/en/mkdocs.yml\n@@ -302,6 +302,8 @@ extra:\n     name: yo - Yor√πb√°\n   - link: /zh/\n     name: zh - Ê±âËØ≠\n+  - link: /zh-hant/\n+    name: zh - ÁπÅÈ´î‰∏≠Êñá\n   - link: /em/\n     name: üòâ"
  },
  {
    "pr_title": "üåê Add Russian translation for `docs/ru/docs/tutorial/security/simple-oauth2.md`",
    "pr_body": "",
    "pr_number": 10599,
    "comments": [
      "Do you happen to know three Russian developers you can ask a [PR review](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/requesting-a-pull-request-review) from? [This](https://github.com/tiangolo/fastapi/pulls?q=is%3Apr+is%3Aopen+Russian) might help.",
      "@Xewus, @s111d –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å —Å —Ä–µ–≤—å—é. –ü–æ–¥–∫–ª—é—á–∞–π—Ç–µ—Å—å!\r\n:wave: :blush: ",
      "Hi @AlertRED, I have updated the documentation with the latest syntax for `includes`.\r\nI will wait for you to review the changes suggested by @alv2017 before merging the PR. \r\nThanks to both of you for the help! :rocket: ",
      "@alejsdev: Thank you for your help! \r\n\r\n@AlertRED: I think we are ready with PR, aren't we? \r\n\r\n:smiley: ",
      "> @alejsdev: Thank you for your help! \n> \n> @AlertRED: I think we are ready with PR, aren't we? \n> \n> :smiley: \n\nYes, we are :)",
      "–ü—Ä–æ–±–ª–µ–º–∞ —Ä–µ—à–µ–Ω–∞, —É –º–µ–Ω—è –Ω–∏–∫–∞–∫–∏—Ö –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π –±–æ–ª—å—à–µ –Ω–µ—Ç :smiley:",
      "Great! Thanks for your work @AlertRED @Xewus @alv2017 :rocket: :sparkles: "
    ],
    "num_comments": 7,
    "repository": "fastapi/fastapi",
    "diff_length": 10975,
    "code_diff": "diff --git a/docs/ru/docs/tutorial/security/simple-oauth2.md b/docs/ru/docs/tutorial/security/simple-oauth2.md\nnew file mode 100644\nindex 0000000000000..9732265ccabef\n--- /dev/null\n+++ b/docs/ru/docs/tutorial/security/simple-oauth2.md\n@@ -0,0 +1,272 @@\n+# –ü—Ä–æ—Å—Ç–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª—É OAuth2 —Å —Ç–æ"
  },
  {
    "pr_title": "Uses MPS (Mac acceleration) by default when available",
    "pr_body": "Currently, Whisper defaults to using the CPU on MacOS devices despite the fact that PyTorch has introduced Metal Performance Shaders framework for Apple devices in the nightly release ([more info](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)).\r\n\r\nWith my changes to __init__.py, torch checks in MPS is available if torch.device has not been specified. If it is, and CUDA is not available, then Whisper defaults to MPS.\r\n\r\nThis way, Mac users can experience speedups from",
    "pr_number": 382,
    "comments": [
      "@dwarkeshsp have you measured any speedups compared to using the CPU?",
      "Doesn't this also require switching FP16 off?",
      "I'm getting this error when try to use MPS\r\n\r\n/Users/diego/.pyenv/versions/3.10.6/lib/python3.10/site-packages/whisper-1.0-py3.10.egg/whisper/decoding.py:629: UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/diego/Projects/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/AppleInternal/Library/BuildRoots/2d9b4df9-4b93-11ed-b0fc-2e32217d8374/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:794: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 23200 bytes\r\n'\r\nAbort trap: 6\r\n/Users/diego/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n\r\nany clues?",
      "@DiegoGiovany Not an expert on this but It looks like PyTorch itself is missing some operators for MPS. See for example\r\nhttps://github.com/pytorch/pytorch/issues/77764#issuecomment-1254352628\r\n(which refers to repeat_interleave)\r\n\r\nand\r\nhttps://github.com/pytorch/pytorch/issues/87219\r\n",
      "Thanks for your work. I just tried this. Unfortunately, it didn't work for me on my m1 max with 32GB.\r\nHere is what I did:\r\npip install git+https://github.com/openai/whisper.git@refs/pull/382/head\r\n\r\nNo errors on install and it works fine when run without mps: whisper audiofile_name --model medium \r\n\r\nWhen I run: whisper audiofile_name --model medium --device mps\r\n\r\nHere is the error I get:\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/810eba08-405a-11ed-86e9-6af958a02716/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x1024x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s). \r\n\r\nWhen I run:  whisper audiofile_name --model medium --device mps --fp16 False\r\n\r\nHere is the error I get:\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: English\r\n/anaconda3/lib/python3.9/site-packages/whisper/decoding.py:633: UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/AppleInternal/Library/BuildRoots/f0468ab4-4115-11ed-8edc-7ef33c48bc85/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:794: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 1007280 bytes\r\n\r\nBasically, same error as @DiegoGiovany.\r\n\r\nAny ideas on how to fix?",
      "+1 for me!  I'm actually using an Intel Mac with Radeon Pro 560X 4 GB...",
      "Related\r\nhttps://github.com/pytorch/pytorch/issues/87351",
      "@dwarkeshsp \r\n\r\nnot workÔºåwith mbp2015 pytorch 1.3 stableÔºåegpu RX580, MacOS 12.3.\r\n\r\nchanged the code as the same as yours.\r\n\r\nchanged  to use --device mps but show error, maybe there is still somewhere to change or modify.\r\n\r\nuse --device cpu, it works.\r\n\r\nwith other pytorch-metal project, MPS works.",
      "I also see the same errors as others mentioned above, on an M1 Mac running arm64 Python. ",
      "On an M1 16\" MBP with 16GB running MacOS 13.0.1, I'm seeing the following with `openai-whisper-20230117`:\r\n\r\nUsing this command:\r\n```(venv) whisper_ai_playground % whisper './test_file.mp3' --model tiny.en --output_dir ./output --device mps```\r\n\r\nI'm encountering the following errors:\r\n\r\n```loc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/810eba08-405a-11ed-86e9-6af958a02716/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x384x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible```\r\n\r\n```LLVM ERROR: Failed to infer result type(s).```\r\n\r\n```zsh: abort      whisper  --model tiny.en --output_dir ./output --device mps```\r\n\r\n```/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '```",
      "Is there any update on this, or did anyone figure out how to get it to work? ",
      "Same problem with osx 13.2 in MacBook Pro M2 max:\r\n\r\n```\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x1280x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\nzsh: abort      whisper audio.wav --language en --model large\r\nm2@Render ~ % /opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```",
      "I'm getting the same error as @renderpci using the M1 Base Model\r\n```bash\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x512x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\n[1]    3746 abort      python3 test.py\r\n```\r\n**test.py:**\r\n```py\r\nimport whisper\r\n\r\nmodel = whisper.load_model(\"base\")\r\nresult = model.transcribe(\"audio.mp3\")\r\nprint(result[\"text\"])\r\n```",
      "FWIW I switched to the C++ port https://github.com/ggerganov/whisper.cpp/ and got a ~15x speedup compared to CPU pytorch on my M1 Pro. (But note that it doesn't have all the features/flags from the official whisper repo.)",
      "> FWIW I switched to the C++ port https://github.com/ggerganov/whisper.cpp/ \r\n\r\nFor us whisper.cpp is not an option:\r\n\r\n> **Should I use whisper.cpp in my project?**\r\n> \r\n> whisper.cpp is a hobby project. It does not strive to provide a production ready implementation. The main goals of the implementation is to be educational, minimalistic, portable, hackable and performant. There are no guarantees that the implementation is correct and bug-free and stuff can break at any point in the future. Support and updates will depend mostly on contributions, since with time I will move on and won't dedicate too much time on the project.\r\n> \r\n> If you plan to use whisper.cpp in your own project, keep in mind the above.\r\n> My advice is to not put all your eggs into the whisper.cpp basket.",
      "The same error as @renderpci using the M2\r\n\r\n\r\nwhisper interview.mp4 --language en --model large --device mps\r\n\r\n```\r\nloc(\"mps_multiply\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/9e200cfa-7d96-11ed-886f-a23c4f261b56/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":228:0)): error: input types 'tensor<1x1280x3000xf16>' and 'tensor<1xf32>' are not broadcast compatible\r\nLLVM ERROR: Failed to infer result type(s).\r\nzsh: abort      whisper interview.mp4 --language en --model large --device mps\r\npac@dd ~ % /opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```",
      "Hey @devpacdd  - this should be fixed in latest pytorch nightly (pip3 install --pre --force-reinstall torch --index-url https://download.pytorch.org/whl/nightly/cpu). Let me know if you still see any issues. Thanks",
      "Still have the same error after updating\r\n\r\nEdit: After adding `--fp16 False` to the command, I now get a new error, as well as the old one:\r\n```\r\n/opt/homebrew/lib/python3.10/site-packages/whisper/decoding.py:633: UserWarning: The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/AppleInternal/Library/BuildRoots/5b8a32f9-5db2-11ed-8aeb-7ef33c48bc85/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:794: failed assertion `[MPSNDArray, initWithBuffer:descriptor:] Error: buffer is not large enough. Must be 1007280 bytes\r\n'\r\nzsh: abort      whisper --model large --language de --task transcribe  --device mps --fp16\r\n/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```",
      "i was able to get it to kinda work: https://github.com/davabase/whisper_real_time/issues/5#issue-1596258783",
      "> The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n>   audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n\r\n@manuthebyte could you please make sure you are on a recent nightly? `repeat_interleave` should be natively supported. If you could try grabbing today's nightly and give a try that would be awesome! (You can get today's nightly with `pip3 install --pre --force-reinstall torch==2.0.0.dev20230224 --index-url https://download.pytorch.org/whl/nightly/cpu`)\r\n\r\n",
      "Wow! \r\n\r\nwhen running:\r\n`Python3 transcribe_demo.py --model medium` (from https://github.com/davabase/whisper_real_time)\r\n\r\nwith the following packages in my pipenv's requirements.txt\r\n```\r\ncertifi==2022.12.7\r\ncharset-normalizer==3.0.1\r\nffmpeg-python==0.2.0\r\nfilelock==3.9.0\r\nfuture==0.18.3\r\nhuggingface-hub==0.12.1\r\nidna==3.4\r\nmore-itertools==9.0.0\r\nmpmath==1.2.1\r\nnetworkx==3.0rc1\r\nnumpy==1.24.2\r\nopenai-whisper @ git+https://github.com/openai/whisper.git@51c785f7c91b8c032a1fa79c0e8f862dea81b860\r\npackaging==23.0\r\nPillow==9.4.0\r\nPyAudio==0.2.13\r\nPyYAML==6.0\r\nregex==2022.10.31\r\nrequests==2.28.2\r\nSpeechRecognition==3.9.0\r\nsympy==1.11.1\r\ntokenizers==0.13.2\r\ntorch==2.0.0.dev20230224\r\ntorchaudio==0.13.1\r\ntorchvision==0.14.1\r\ntqdm==4.64.1\r\ntransformers==4.26.1\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.14\r\n```\r\n\r\nit gets every word! while i was singing! in realtime, with maybe 50%~ gpu usage on the apple M2 Pro Max.",
      "> > The operator 'aten::repeat_interleave.self_int' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\r\n> > audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n> \r\n> @manuthebyte could you please make sure you are on a recent nightly? `repeat_interleave` should be natively supported. If you could try grabbing today's nightly and give a try that would be awesome! (You can get today's nightly with `pip3 install --pre --force-reinstall torch==2.0.0.dev20230224 --index-url https://download.pytorch.org/whl/nightly/cpu`)\r\n\r\nWith my pip3 freeze being:\r\n```\r\nbeautifulsoup4==4.11.2\r\ncertifi==2022.12.7\r\ncharset-normalizer==3.0.1\r\ncolorama==0.4.6\r\ndnspython==2.3.0\r\nffmpeg-python==0.2.0\r\nfilelock==3.9.0\r\nfuture==0.18.3\r\nhuggingface-hub==0.12.1\r\nidna==3.4\r\nmore-itertools==9.0.0\r\nmpmath==1.2.1\r\nnetworkx==3.0rc1\r\nnumpy==1.24.2\r\nopenai-whisper @ git+https://github.com/openai/whisper.git@7858aa9c08d98f75575035ecd6481f462d66ca27\r\npackaging==23.0\r\nprotobuf==4.21.12\r\nPyYAML==6.0\r\nregex==2022.10.31\r\nrequests==2.28.2\r\nsix==1.16.0\r\nsoupsieve==2.4\r\nsympy==1.11.1\r\ntokenizers==0.13.2\r\ntorch==2.0.0.dev20230224\r\ntqdm==4.64.1\r\ntransformers==4.26.1\r\ntyping_extensions==4.4.0\r\nurllib3==1.26.14\r\n```\r\n\r\nIt now seems to use the GPU but I now get these errors:\r\n```\r\n/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py:636: UserWarning: 0MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\r\n  audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\r\n/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py:443: UserWarning: 1MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:143.)\r\n  timestamp_logprob = logprobs[k, self.tokenizer.timestamp_begin :].logsumexp(dim=-1)\r\n/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py:444: UserWarning: 1MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1269.)\r\n  max_text_token_logprob = logprobs[k, : self.tokenizer.timestamp_begin].max()\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/bin/whisper\", line 8, in <module>\r\n    sys.exit(cli())\r\n             ^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py\", line 314, in cli\r\n    result = transcribe(model, audio_path, temperature=temperature, **args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py\", line 183, in transcribe\r\n    result: DecodingResult = decode_with_fallback(segment)\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py\", line 118, in decode_with_fallback\r\n    decode_result = model.decode(segment, options)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 707, in decode\r\n    result = DecodingTask(model, options).run(mel)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 640, in run\r\n    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\r\n                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 609, in _main_loop\r\n    tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/whisper/decoding.py\", line 258, in update\r\n    next_tokens = Categorical(logits=logits / self.temperature).sample()\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/distributions/categorical.py\", line 66, in __init__\r\n    super().__init__(batch_shape, validate_args=validate_args)\r\n  File \"/opt/homebrew/lib/python3.11/site-packages/torch/distributions/distribution.py\", line 62, in __init__\r\n    raise ValueError(\r\nValueError: Expected parameter logits (Tensor of shape (5, 51865)) of distribution Categorical(logits: torch.Size([5, 51865])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\r\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan],\r\n        [nan, nan, nan,  ..., nan, nan, nan]], device='mps:0')\r\n```\r\n\r\nWhen running the command `whisper --model small --language en --task transcribe ***.wav --device mps`",
      "> Hey @devpacdd - this should be fixed in latest pytorch nightly (pip3 install --pre --force-reinstall torch --index-url https://download.pytorch.org/whl/nightly/cpu). Let me know if you still see any issues. Thanks\r\n\r\nGeart! it works!\r\nBut.. In my test the GPU is slow than CPU... ??? \r\n\r\nAudio to transcribe: 1 minute with model large, language catalan\r\n\r\nCPU  : 2m : 33 s\r\nGPU (--device mps): 4m : 54 s\r\n\r\nI tried with different files and the result was the same; +/- double time with GPU enable.\r\n\r\nIt's normal? I expected less time for GPU than CPU.\r\n\r\nBest",
      "I get this error while trying to use MPS\r\n\r\nHere is the command I am running: `whisper --model large --language en --task transcribe test.mp3 --device mps`\r\n\r\n```\r\n$ whisper --model large --language en --task transcribe test.mp3 --device mps\r\nTraceback (most recent call last):\r\n  File \"/Users/mukul/miniconda3/envs/ml/bin/whisper\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/whisper/transcribe.py\", line 433, in cli\r\n    model = load_model(model_name, device=device, download_root=model_dir)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/whisper/__init__.py\", line 159, in load_model\r\n    return model.to(device)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1170, in to\r\n    return self._apply(convert)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 869, in _apply\r\n    self._buffers[key] = fn(buf)\r\n  File \"/Users/mukul/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1168, in convert\r\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\nNotImplementedError: Could not run 'aten::empty.memory_format' with arguments from the 'SparseMPS' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, MPS, Meta, QuantizedCPU, QuantizedMeta, MkldnnCPU, SparseCPU, SparseMeta, SparseCsrCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\r\n\r\nCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31085 [kernel]\r\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMPS.cpp:24065 [kernel]\r\nMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26824 [kernel]\r\nQuantizedCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\r\nQuantizedMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterQuantizedMeta.cpp:105 [kernel]\r\nMkldnnCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMkldnnCPU.cpp:507 [kernel]\r\nSparseCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1379 [kernel]\r\nSparseMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseMeta.cpp:249 [kernel]\r\nSparseCsrCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1128 [kernel]\r\nBackendSelect: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:734 [kernel]\r\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\r\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\r\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\r\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\r\nConjugate: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\r\nNegative: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:23 [kernel]\r\nZeroTensor: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:90 [kernel]\r\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\r\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:17927 [autograd kernel]\r\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16872 [kernel]\r\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\r\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\r\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\r\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\r\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\r\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\r\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\r\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\r\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\r\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\r\n```",
      "> pytorch/pytorch#87351\r\n\r\nI'd love to hear a clear update too. It looks like there will be a lot of demand for this. (Mac M2 myself) Thank you OpenAI people!",
      "@mukulpatnaik \r\nMy device is M1 MacBook Pro, I got the same error with the latest version of whisper([v20230314](https://github.com/openai/whisper/releases/tag/v20230314)), then I switch to [v20230124](https://github.com/openai/whisper/releases/tag/v20230124), every thing works fine. (torch nightly version)\r\n\r\nBut, seems like mps is slower than cpu like @renderpci reported, for my task\r\n* cpu 3.26 s\r\n* mps 5.25 s\r\n* cpu+torch2 compile 3.31 s\r\n* mps+torch2 compile 4.94 s\r\n\r\nü´†",
      "@HFrost0, what's your macOS, PyTorch and Python version? Some versions support different operations, and PyTorch defaults to CPU on those. "
    ],
    "num_comments": 27,
    "repository": "openai/whisper",
    "diff_length": 1323,
    "code_diff": "diff --git a/whisper/__init__.py b/whisper/__init__.py\nindex 2a1fb4ec6..4f45f9969 100644\n--- a/whisper/__init__.py\n+++ b/whisper/__init__.py\n@@ -92,7 +92,12 @@ def load_model(name: str, device: Optional[Union[str, torch.device]] = None, dow\n     \"\"\"\n \n     if device is None:\n-        device = \"cuda\""
  },
  {
    "pr_title": "word-level timestamps in `transcribe()`",
    "pr_body": "",
    "pr_number": 869,
    "comments": [
      "This DTW dependency introduces a licence incompatibility, but an alternative was suggested earlier in the discussions from memory.\r\n\r\nEdit: Alternative library recommended in https://github.com/openai/whisper/discussions/813#discussioncomment-4617447",
      "Hi!\r\nI tried out this branch with ```kwargs['word_level_timestamps'] = True``` but the model performed very slowly. In addition (or rather because of) it started to hallucinate like mad. \r\nIm using chunks of short (couple of seconds) audio data in german produced by a VAD for live transcription.\r\n\r\nMaybe its a problem on my side, maybe anyone can try to reproduce?",
      "Thanks for the comments, all -- this is work in progress and not quite ready for merging. I'm trying to address both hallucination and performance concerns.",
      "Yet another DTW implementation, fyi. Can't vouch for it other than to say that it is Apache licensed, recently updated, has both pure Python and C implementations.\r\n\r\nhttps://github.com/wannesm/dtaidistance",
      "Hi, thanks for the great work! \r\n\r\nI would like to ask if it is safe to swap to a smaller model (e.g. tiny) for world-level alignment to compute attention scores instead of using the same model (e.g. medium or large ) used to generate transcription. I suspect it could improve performance in terms of inference speed if this option would be supported. ",
      "I found an interesting edge case with the `small` model where enabling the word-level timestamps option causes it to repeat the prompt at the end of the audio while also failing to infer the last word.\r\n\r\n```bash\r\n$ ffmpeg -t 29 -i https://audio2.redcircle.com/episodes/6b196013-8672-43d9-be52-4332b3207d93/stream.mp3 test.mp3\r\n\r\n$ whisper --model small test.mp3\r\n.../whisper/transcribe.py:98: UserWarning: FP16 is not supported on CPU; using FP32 instead\r\n  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: English\r\n[00:00.000 --> 00:15.920]  Military veteran Eric Weinstein began 69 Whiskey as a college radio show on 107.7 The\r\n[00:15.920 --> 00:21.720]  Bronx, located on the campus of Ryder University in Lawrenceville, New Jersey.\r\n[00:21.720 --> 00:27.560]  A show once restrained by rules and boundaries now comes straight to you raw, uncensored and\r\n[00:27.560 --> 00:28.960]  unapologetic.\r\n\r\n$ whisper --model small --output_format json --word_timestamps True test.mp3\r\n.../whisper/transcribe.py:98: UserWarning: FP16 is not supported on CPU; using FP32 instead\r\n  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\r\nDetecting language using up to the first 30 seconds. Use `--language` to specify the language\r\nDetected language: English\r\n[00:08.040 --> 00:15.940]  Military veteran Eric Weinstein began 69 Whiskey as a college radio show on 107.7 The\r\n[00:15.940 --> 00:21.320]  Bronx, located on the campus of Ryder University in Lawrenceville, New Jersey.\r\n[00:21.720 --> 00:28.980]  A show once restrained by rules and boundaries now comes straight to you raw, uncensored and\r\n[00:28.960 --> 00:28.960]  Military veteran Eric Weinstein began 69 Whiskey as a college radio show on 107.7 The\r\n[00:28.960 --> 00:28.960]  Bronx, located on the campus of Ryder University in Lawrenceville, New Jersey.\r\n[00:28.960 --> 00:28.960]  A show once restrained by rules and boundaries now comes straight to you raw, uncensored and\r\n[00:28.960 --> 00:28.960]  Military veteran Eric Weinstein began 69 Whiskey as a college radio show on 107.7 The\r\n[00:28.960 --> 00:28.960]  Bronx, located on the campus of Ryder University in Lawrenceville, New Jersey.\r\n```",
      "Hi @jongwook ,\r\nSince you first release the notebook to obtain word-level timestamps I've been working on this to add to whisper process. And I've tried to test other alingment methods than DTW. Have you tried something else and found out that it works better?\r\n\r\nAlso, I've been struggling a lot with alucinations, specially for spanish content. I've create a cleaner function at segmet level, is there any smarter way?",
      "is there any chance to have word level timestamps in Whisper API?",
      "Hi @IgnacioSan22, the custom DTW implementation in this PR was for the license issue as noted by others and also for the speed. An alternative is to use the timestamp predictions from the model, but we found that it's less reliable than using the attention patterns like in this PR. If you have solutions using any other algorithms for alignment, please let me know!\r\n\r\nThe community had some success handling hallucinations by preprocessing the inputs with VAD, like:\r\n\r\n- #679\r\n- #397\r\n- https://github.com/m-bain/whisperX\r\n\r\n---\r\n\r\nHi @ioskevinshah, this feature is still experimental but we do plan to add it to the API as an option, once we're sure that it's reliable enough.",
      "@jongwook is there a way to access it via a beta flag for instance? How can we know when something is/isn't added to the API?",
      "For the API, the [speech-to-text guide](https://platform.openai.com/docs/guides/speech-to-text) and the [audio API reference](https://platform.openai.com/docs/api-reference/audio) provide the full documentation of the available features. These documents will be updated accordingly as we roll out new features.",
      "> Hi @IgnacioSan22, the custom DTW implementation in this PR was for the license issue as noted by others and also for the speed. An alternative is to use the timestamp predictions from the model, but we found that it's less reliable than using the attention patterns like in this PR. If you have solutions using any other algorithms for alignment, please let me know!\r\n> \r\n> The community had some success handling hallucinations by preprocessing the inputs with VAD, like:\r\n> \r\n> * [A possible solution to Whisper hallucination¬†#679](https://github.com/openai/whisper/discussions/679)\r\n> * [Whisper WebUI with a VAD for more accurate non-English transcripts (Japanese)¬†#397](https://github.com/openai/whisper/discussions/397)\r\n> * https://github.com/m-bain/whisperX\r\n> \r\n> Hi @ioskevinshah, this feature is still experimental but we do plan to add it to the API as an option, once we're sure that it's reliable enough.\r\n\r\nHi @jongwook, I've tried the hungarian algorithm and in some cases the results are better, however due to the lack of resources I'm not capable to perform a proper study to find the best alingment algorithm. For hallucinations I've developed a postprocess functions that cleans the segments. It improves quite a lot, but I'll check those references. \r\n\r\nThanks",
      "> For the API, the [speech-to-text guide](https://platform.openai.com/docs/guides/speech-to-text) and the [audio API reference](https://platform.openai.com/docs/api-reference/audio) provide the full documentation of the available features. These documents will be updated accordingly as we roll out new features.\r\n\r\nOne more question: when will this new feature be rolled out?",
      "> Hi @IgnacioSan22, the custom DTW implementation in this PR was for the license issue as noted by others and also for the speed. An alternative is to use the timestamp predictions from the model, but we found that it's less reliable than using the attention patterns like in this PR. If you have solutions using any other algorithms for alignment, please let me know!\r\n> \r\n> The community had some success handling hallucinations by preprocessing the inputs with VAD, like:\r\n> \r\n> * [A possible solution to Whisper hallucination¬†#679](https://github.com/openai/whisper/discussions/679)\r\n> * [Whisper WebUI with a VAD for more accurate non-English transcripts (Japanese)¬†#397](https://github.com/openai/whisper/discussions/397)\r\n> * https://github.com/m-bain/whisperX\r\n> \r\n> Hi @ioskevinshah, this feature is still experimental but we do plan to add it to the API as an option, once we're sure that it's reliable enough.\r\n\r\nany workaround or logic after the API response?",
      "This is awesome! Is there a way to pass in pre-transcribed text that whisper can use for more accurate alignment?"
    ],
    "num_comments": 15,
    "repository": "openai/whisper",
    "diff_length": 43620,
    "code_diff": "diff --git a/.github/workflows/test.yml b/.github/workflows/test.yml\nindex c5b4eeccd..f06bff79a 100644\n--- a/.github/workflows/test.yml\n+++ b/.github/workflows/test.yml\n@@ -21,6 +21,5 @@ jobs:\n       - run: conda install -n test ffmpeg python=${{ matrix.python-version }} pytorch=${{ matrix.pytorch-v"
  },
  {
    "pr_title": "Skip silence around hallucinations",
    "pr_body": "This PR introduces a heuristic that determines if a segment is probably a hallucination. If that \"probable\" hallucination occurs after a period of silence (specified by `--hallucination_silence_threshold` in seconds), then we seek past the silence and reprocess from that point. Eliminating the silence before a hallucination improves the likelihood of getting a correct inference, but since this also requires extra processing time, we only do this when a probable hallucination is detected.\r\n\r\nThe ",
    "pr_number": 1838,
    "comments": [
      "Testing on another example from https://github.com/openai/whisper/discussions/679#discussioncomment-7649183\r\n\r\n<details>\r\n<summary>Output</summary>\r\n\r\n```\r\nv2 runs:\r\n\r\n[00:00.000 --> 00:05.660]  spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo\r\nDETECTED HALLUCINATION:  non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho\r\nDETECTED HALLUCINATION:  no\r\nDETECTED HALLUCINATION:  no\r\n\r\n\r\n\r\n[00:00.000 --> 00:05.660]  spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo\r\nDETECTED HALLUCINATION:  non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho\r\nDETECTED HALLUCINATION:  .....\r\nDETECTED HALLUCINATION:  .....\r\n\r\n\r\n\r\n[00:00.000 --> 00:05.660]  spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo\r\nDETECTED HALLUCINATION:  non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho capito, non ho\r\nDETECTED HALLUCINATION:  uh\r\nDETECTED HALLUCINATION:  uh\r\n\r\n\r\n\r\nv3 run:\r\n\r\n[00:00.000 --> 00:04.240]  Spero che si ripigli un attimo, ho schiacciato qualche tasto che non dovevo.\r\nDETECTED HALLUCINATION:  Grazie a tutti.\r\nDETECTED HALLUCINATION:  E' un attimo che non dovevo.\r\n[00:54.440 --> 00:55.700]  Ehm, ehm.\r\n```",
      "When using `--clip_timestamps`, is there a need to pad before and after the clip? If so, what would be a recommended value for pad duration? ",
      "It should work without padding, but if the VAD is inaccurate then padding might help compensate for that.",
      "Will this PR included in the next release? If so, when it it planned?",
      "Related PR: #2005 (fixes a bug in `--clip_timestamps` if you pass an end timestamp that is after the audio end.)",
      "Just tested out hallucination_silence_threshold and it worked for me\r\nthanks!",
      "@ryanheise Can you show me your transcribe.py with debug stuff?",
      "I don't have the exact code anymore, but you could try temporarily inserting these two lines:\r\n\r\n```python\r\n                if score >= 3 or score + 0.01 >= len(words):\r\n                    print(f\"DETECTED HALLUCINATION: {segment['text']}\")\r\n```\r\n\r\nbefore the return in this function:\r\n\r\n```python\r\n            def is_segment_anomaly(segment: Optional[dict]) -> bool:\r\n                if segment is None or not segment[\"words\"]:\r\n                    return False\r\n                words = [w for w in segment[\"words\"] if w[\"word\"] not in punctuation]\r\n                words = words[:8]\r\n                score = sum(word_anomaly_score(w) for w in words)\r\n                return score >= 3 or score + 0.01 >= len(words)\r\n```",
      "@ryanheise\r\nSometimes `--hallucination_silence_threshold` makes whole non-hallucinating segments or part of segments disappear.\r\n\r\nBelow is example with disappeared `orange pigmentation.` segment.\r\n\r\nI'm using faster-whisper, but you should be able to reproduce it with whisper too as implementation is same.\r\nAudio file -> https://we.tl/t-U5a6Al5bRs\r\n\r\n\r\n`--language en --model=base --beam_size=5 --word_timestamps=True --hallucination_silence_threshold=None`:\r\n\r\n```\r\n[02:06.620 --> 02:11.120]  White tigers carry a mutated version of this gene, which prevents them from producing\r\n  Processing segment at 02:11.120\r\n[02:11.120 --> 02:12.460]  orange pigmentation.\r\n[02:15.360 --> 02:18.340]  Fewer than 4,000 tigers remain in the wild.\r\n```\r\n\r\n`--language en --model=base --beam_size=5 --word_timestamps=True --hallucination_silence_threshold=2`:\r\n\r\n```\r\n[02:06.620 --> 02:11.120]  White tigers carry a mutated version of this gene, which prevents them from producing\r\n  Processing segment at 02:12.380\r\n* HST_1: Skipping silence before possible hallucinations.\r\n* HST_3: DETECTED HALLUCINATION:  oxygen.\r\n  Processing segment at 02:13.380\r\n* HST_1: Skipping silence before possible hallucinations.\r\n[02:14.680 --> 02:18.360]  fewer than 4,000 tigers remain in the wild.\r\n```\r\n\r\nEDIT:\r\nfloat32 was in use",
      "I think, I've noticed a pattern, it happens when `if remaining_duration > threshold:` is not triggered, in there:\r\n`seek = previous_seek + segment_size`\r\n\r\nThen chunk go exactly by 30 secs cutting off the word.\r\n\r\nChunking when `--hallucination_silence_threshold=None`:\r\n\r\n```\r\n  Processing segment at 00:00.000\r\n  Processing segment at 00:26.040\r\n  Processing segment at 00:48.280\r\n  Processing segment at 01:14.400\r\n  Processing segment at 01:42.380\r\n  Processing segment at 02:11.120\r\n  Processing segment at 02:35.400\r\n  Processing segment at 03:05.400\r\n```\r\nChunking by setting high threshold `--hallucination_silence_threshold=40`:\r\n```\r\n  Processing segment at 00:00.000\r\n  Processing segment at 00:30.000\r\n  Processing segment at 01:00.000\r\n  Processing segment at 01:30.000\r\n  Processing segment at 02:00.000\r\n  Processing segment at 02:30.000\r\n  Processing segment at 03:00.000\r\n```",
      "Another thing, this PR affects transcription even if both new parameters are not enabled, I meant comparing vs without this PR.\r\n\r\nThis happens sometimes, but when it happens the discrepancy is always in the last chunk.\r\n\r\nAnd sometimes when discrepancy happens it tries to process additional micro chunk after it which produces some hallucination or fails because no-speech threshold is met, not sure if this is related to PR or to a discrepancy.\r\n\r\nExample of such discrepancy [audio is `05:05.877` long]:\r\n\r\nWithout this PR [perfect transcription]:\r\n```\r\nProcessing segment at 04:48.000\r\n[04:58.120 --> 05:05.260]  I just...\r\n[05:05.260 --> 05:05.760]  I...\r\n```\r\n\r\nWith this PR [all goes exactly same till the last chunk]:\r\n\r\n```\r\n  Processing segment at 04:48.000\r\n* Compression ratio threshold is not met with temperature 0.0 (3.523810 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.2 (3.523810 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.4 (8.038462 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.6 (3.523810 > 2.400000)\r\n* Compression ratio threshold is not met with temperature 0.8 (2.423077 > 2.400000)\r\n[05:01.940 --> 05:02.900]  Okay.\r\n[05:02.900 --> 05:04.000]  I just-\r\n[05:04.940 --> 05:05.740]  I-\r\n[05:05.740 --> 05:05.840]  I-\r\n* Reset prompt. prompt_reset_on_temperature threshold is met 1.000000 > 0.500000\r\n  Processing segment at 05:05.840\r\n* Log probability threshold is not met with temperature 0.0 (-1.105777 < -1.000000)\r\n* No speech threshold is met (0.772002 > 0.600000)\r\n```",
      "> Sometimes `--hallucination_silence_threshold` makes whole non-hallucinating segments or part of segments disappear.\r\n\r\nThis logic is part of the original Whisper strategy of advancing by the full 30 seconds to the next window whenever the current segment is unfinished. So basically, if the segment finishes before the end of the 30 second window, then Whisper will crop the window to the exact end timestamp of the last word in that segment. But if the segment does not finish by the end of the 30 second window, the window is not cropped, the speech is assumed to run all the way to the end of the window.\r\n\r\nThis logic exists whether or not the `hallucination_silence_threshold` is enabled, and I have seen it cause problems in both cases, however the larger models tend to be better at picking up the words across the window boundary.\r\n\r\nIn your case, the sentence in question is:\r\n\r\n> White tigers carry a mutated version of this gene, which prevents them from producing orange pigmentation.\r\n\r\nThis sentence does not fit within the 30 second window, and the word \"orange\" is right on the boundary. In fact, the word \"orange\" is slightly before the boundary and the human ear can pick it up (as can the larger models) but the smaller models fail to pick it up.\r\n\r\nAnd given Whisper's logic in this case, it will assume the speech went right up to the end of the 30 second window and will resume the next window from there.\r\n\r\nSo although yes the large models would probably resolve this, I think it would still be better to change Whisper's strategy and crop the window to the end timestamp of the last word even in this case where we have an unfinished segment.",
      "> This logic is part of the original Whisper strategy of advancing by the full 30 seconds to the next window whenever the current segment is unfinished.\r\n\r\nI can't connect the dots...\r\nThen why it's \"unfinished\" when using `hallucination_silence_threshold` and it's \"finished\" without it?\r\n\r\nHow `remaining_duration <= hallucination_silence_threshold` means an \"unfinished\" segment? The option doesn't read as \"finished/unfinished segment threshold\"....\r\n",
      "Apologies, my explanation of that was around the wrong way. The original Whisper behaviour was that if the last segment in the window is \"complete\", THEN it skips to the end of the full 30 second window. If the last segment is incomplete, then it crops the window to end timestamp of the last word.\r\n\r\nBut when `hallucination_silence_threshold` is set, it still applies this logic in most cases except that it also includes a misfired heuristic that skips to the end of the full 30 second window if the end of the speech is close enough to the end of the window:\r\n\r\n```python\r\n                # skip silence before possible hallucinations\r\n                if hallucination_silence_threshold is not None:\r\n                    threshold = hallucination_silence_threshold\r\n                    if not single_timestamp_ending:\r\n                        last_word_end = get_end(current_segments)\r\n                        if last_word_end is not None and last_word_end > time_offset:\r\n                            remaining_duration = window_end_time - last_word_end\r\n                            if remaining_duration > threshold:  # <--- misfired heuristic\r\n                                seek = round(last_word_end * FRAMES_PER_SECOND)\r\n                            else:\r\n                                seek = previous_seek + segment_size\r\n````\r\n\r\nThe goal was to skip over as much silence as safely possible.\r\n\r\nHowever, in hindsight, this was a bit opportunistic, since after all `single_timestamp_ending` was `False` for good reason. You should find your example will work if you remove that heuristic. i.e. Delete this entire section:\r\n\r\n```python\r\n                    if not single_timestamp_ending:\r\n                        last_word_end = get_end(current_segments)\r\n                        if last_word_end is not None and last_word_end > time_offset:\r\n                            remaining_duration = window_end_time - last_word_end\r\n                            if remaining_duration > threshold:  # <--- misfired heuristic\r\n                                seek = round(last_word_end * FRAMES_PER_SECOND)\r\n                            else:\r\n                                seek = previous_seek + segment_size\r\n```\r\n\r\n(It's OK, the other parts of this code block are already handled elsewhere.)",
      "I've created a PR #2043 incorporating the above fix based on your counter example.",
      "Thanks for explanation, now this part of code makes sense.\r\nDo you have idea why seek in the last window can be affected by PR? -> https://github.com/openai/whisper/pull/1838#issuecomment-1960581637\r\n\r\n> The goal was to skip over as much silence as safely possible.\r\n\r\nImho, skipping to full 30s window is pretty unsafe.  üòÜ\r\nAnd it contradicted the description: \"skip silent periods longer than this threshold (in seconds) **when a possible hallucination is detected**\"",
      "> Do you have idea why seek in the last window can be affected by PR? -> https://github.com/openai/whisper/pull/1838#issuecomment-1960581637\r\n\r\nDo you have an audio file to reproduce?",
      "> Do you have an audio file to reproduce?\r\n\r\nThis file has discrepancy in the last window/chunk:\r\nt-001.mka -> https://we.tl/t-ecd6U1QaZp\r\n`--language en --model=base --beam_size 1 --word_timestamps=True`\r\n\r\nWhisper without this PR:\r\n```\r\n[01:53.920 --> 01:54.500]  I'll give you some advice.\r\n[01:59.500 --> 02:00.080]  I'll give you some advice.\r\n[02:00.080 --> 02:00.080]  I'll give you some advice.\r\n[02:00.080 --> 02:00.980]  Say the word, General.\r\n[02:02.300 --> 02:03.320]  Let him go.\r\n```\r\nWhisper with this PR:\r\n```\r\n[01:53.920 --> 01:55.200]  I'll give you some advice.\r\n[01:59.500 --> 02:00.980]  Say the word, General.\r\n[02:02.280 --> 02:03.320]  Let him go.\r\n```\r\n",
      "I'll test tomorrow, but does this also happen on PR https://github.com/openai/whisper/pull/2043 ?",
      "> I'll test tomorrow, but does this also happen on PR #2043 ?\r\n\r\nYes, because `hallucination_silence_threshold` option is not relevant for the issue.\r\n\r\nCulprit affecting only the last window is found. it happens because of this:\r\n```python\r\n            mel_segment = mel[:, seek : seek + segment_size]\r\n```\r\n\r\nThis is the fix [that's how it was before this PR]:\r\n```python\r\n            mel_segment = mel[:, seek : seek + N_FRAMES]\r\n```\r\n\r\nNot sure why you changed it, on my observation it makes more hallucinations [probably it's random].\r\nAnyway, the fix brings back the previous behavior.",
      "That is changed for `--cilp_timestamps` because parts of the audio that are clipped out should not be included in the mel spectrogram. I'll take a look at your test scenario to see what's going on.",
      "I've confirmed the discrepancy, which seems to be a consequence of slightly different mel spectrograms. Although in the two examples you gave (only the latter of which I have tested with the supplied audio file), the PR actually removed a hallucination on one example and introduced a hallucination on the other example. So on balance, it's hard to say whether this discrepancy it better or worse or about the same.\r\n\r\nSo if it's not clear whether it's better or worse, do you see anything incorrect in the clipping logic? I think the difference is that I am always clipping exactly to the stretch of audio being examined, and then padding it. But originally, there was padding on the end that was added immediately when the mel spectrogram was first generated, and then (in the original code), it is also possible that due to the dynamic shifting of the window starts, it could end up padding the last part of the audio twice, because there is no guarantee that that initial padding Whisper added at the start of the process was enough to reflect where this last window ended up actually starting.\r\n\r\nBut it's possible I've done something wrong which I can't see, so let me know if you do spot something incorrect in the logic.",
      "After plotting the mel spectrograms, I noticed the padding when the audio is first loaded (as a whole) contains all -1.0's, while the padding in the main loop for each 30 second window contains all 0.0's. Not sure why that is, but there are two different padding algorithms in the code, and weirdly they are producing different padding results.\r\n\r\nSo in your example, the PR ends up always using the padding algorithm that pads to 0.0's whereas originally the end of file padding had -1.0's. ",
      "There's still a chance that a hallucination will be produced.\r\nFor me it was:\r\n```\r\n[02:15:58.100 --> 02:16:05.380]  —è –≤–∞–º –≤—ã—à–ª—é. –í—Å–µ–≥–æ –¥–æ–±—Ä–æ–≥–æ, –¥–æ —Å–≤–∏–¥–∞–Ω–∏—è.\r\n[02:16:28.100 --> 02:16:30.100]  –†–µ–¥–∞–∫—Ç–æ—Ä —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –ò.–ë–æ–π–∫–æ–≤–∞\r\n```\r\ni. e.\r\n```\r\n....\r\n[02:16:28.100 --> 02:16:30.100] Subtitle Editor I. Boykova\r\n```\r\nNotably, this timestamp belongs to the end of the audio.\r\n\r\nModel size: small. Also there are some results in google, if you search for this phrase. One of them:\r\n```\r\n[24:26.800 --> 24:30.160]  –°–º–æ—Ç—Ä–∏—Ç–µ —Ç–µ–ª–µ–±–∞—Ä–æ–º–µ—Ç—Ä –Ω–∞ –Ω–∞—à–µ–º —Ç–µ–ª–µ–∫–∞–Ω–∞–ª–µ.\r\n[24:30.160 --> 24:32.160]  –†–µ–¥–∞–∫—Ç–æ—Ä —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –ò.–ë–æ–π–∫–æ–≤–∞\r\n[24:32.160 --> 24:39.160]  –ö–æ—Ä—Ä–µ–∫—Ç–æ—Ä –ê.–ö—É–ª–∞–∫–æ–≤–∞\r\n```\r\nfrom https://storage.googleapis.com/data.gdeltproject.org/blog/2022-tv-news-whisperasr/BELARUSTV_20221005_161500.small.transcribe.run1.txt",
      "That's certainly possible, and unfortunately there is no single choice of parameters that will be perfect in all scenarios. You can tweak the silence threshold, which is exposed on the command line. You can also try tweaking the other thresholds that were built into the code (like how long a word must be before it is flagged as an abnormality). If we can gather a large enough dataset of audio samples that produce hallucinations, we should be able to come up with better default settings that work well across a variety of scenarios and languages.",
      "@ryanheise \r\nI was using a bit tweaked segment anomaly heuristics to reduce false-positives, didn't noticed increase of false-negatives:\r\n\r\nchanged\r\n `if duration < 0.133:`\r\nto:\r\n`if duration < 0.133 and probability < 0.8:`\r\n\r\nchanged\r\n`return score >= 3 or score + 0.01 >= len(words)`\r\nto:\r\n`return score >= 3 or score + 0.001 >= len(words)`\r\n\r\nWhat you think about this tweak?",
      "Unfortunately I'm between computers right now (my old computer died 2 weeks ago, and I'm just in the process of installing everything on the recently arrived replacement...)\r\n\r\n>  return score >= 3 or score + 0.001 >= len(words)\r\n\r\nI don't see any problem with that change.\r\n\r\n>  `if duration < 0.133 and probability < 0.8:`\r\n\r\nDo you have an example audio for this one? I'd be interested to analyse this correlation between duration < 0.133 and probability < 0.8.\r\n\r\nThe alternative is to take into account more observations (like your audio example) and try to fit a new curve to the data. I initially fitted a simple linear curve, and maybe exponential could help because it could model a slower initial gradient."
    ],
    "num_comments": 27,
    "repository": "openai/whisper",
    "diff_length": 13328,
    "code_diff": "diff --git a/whisper/timing.py b/whisper/timing.py\nindex befcf464e..b695ead0a 100644\n--- a/whisper/timing.py\n+++ b/whisper/timing.py\n@@ -299,6 +299,7 @@ def add_word_timestamps(\n     word_durations = np.array([t.end - t.start for t in alignment])\n     word_durations = word_durations[word_durations.n"
  },
  {
    "pr_title": "[Do not land] [RFC] 1.375x speedup - Remove control flow from model, small hacks, enable TorchDynamo + TorchInductor",
    "pr_body": "Obviously not meant to land, this RP is representative of what it would take to get dynamo working.\r\n\r\ntest_me.py takes 4.4 seconds on main branch\r\ntest_me.py takes 3.2 seconds in this PR\r\n\r\n**Overview:**\r\n1) I took some free audio book of chapter 1 of Charles Dickens' David Copperfield\r\n2) I used an mp3 splitting tool to split it into 8 parts, and then used the util in the model to get 10 chunks of 30 seconds each\r\n3) I \"preheated\" the model with audio part 0, and then ran inference on the othe",
    "pr_number": 43,
    "comments": [
      "For those who haven't heard of [TorchDynamo/TorchInductor](https://github.com/pytorch/torchdynamo), it is automatically fusing and mapping PyTorch to [Triton](https://github.com/openai/triton).",
      "This sounds great! I was also wondering how fast it'd be if [Triton's flash attention](https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py) was integrated, but unfortunately it's A100 only.\r\n\r\nImplementation-wise, I think we could subclass the class `PyTorchInference(Inference):` and monkey-patch the attention layers only when TorchDynamo is available, so that the code is still usable in the older PyTorch versions.",
      "> This sounds great! I was also wondering how fast it'd be if [Triton's flash attention](https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py) was integrated, but unfortunately it's A100 only.\r\n> \r\n> Implementation-wise, I think we could subclass the class `PyTorchInference(Inference):` and monkey-patch the attention layers only when TorchDynamo is available, so that the code is still usable in the older PyTorch versions.\r\n\r\nThat sounds awesome. I'd love to do that. I'm in the pytorch slack and the triton slack - would you like to chat there? I also have further questions on getting a little bit of realistic inference data so we can setup a benchmark on our end (As well as to better measure accuracy). The adhoc free audiobook approach isn't scaling super well, hah. ",
      "do you have more benchmarks? for example, on cpu.",
      "> do you have more benchmarks? for example, on cpu.\r\n\r\nNo, I am sorry, I do not. I plan on working with @jongwook to benchmark this properly :) ",
      "This is the RFC - The implementation PR will be here: https://github.com/openai/whisper/pull/115",
      "@voznesenskym I am curious why you guys started with \"torchdynamo\" instead of more widely-adopted \"torchscript\". We are in the process of making this torch.jit compatible, so I was wondering whether torchscript is slower in comparison.",
      "> @voznesenskym I am curious why you guys started with \"torchdynamo\" instead of more widely-adopted \"torchscript\". We are in the process of making this torch.jit compatible, so I was wondering whether torchscript is slower in comparison.\r\n\r\nTorchDynamo and torchscript are fundamentally different projects, and we are investing in TorchDynamo as a next gen core component of our stack. While their surface levels goals (in this case, speed) align, they are rather different. I am happy to go into it more, but the ReadMe in the TorchDynamo project goes into great depths about what the project is. Have you had a chance to read that yet? ",
      "@taylorchu I definitely recommend in going the torchdynamo route than `torch.jit`. it's more aligned with our future plans.",
      "@soumith @voznesenskym  is the torch team plan for torchdynamo or torch.jit written some where? \r\n\r\nI am interested in whether one will choose one over the other in certain use cases. ",
      "not written down anywhere concretely, we'll talk about it in a few months.\r\nBut about dynamo itself, we have quite a few posts here with various updates: https://dev-discuss.pytorch.org/",
      "Just in case. I can provide a large set of data transcribed by whisper so that you guys can validate whether the change affects the text output. ",
      "@voznesenskym I am trying to benchmark your approach with torchdynamo but got some error modules.  do you know which version torchinductor, torchdynamo and triton are used to make your modification work?  ",
      "> @voznesenskym I am trying to benchmark your approach with torchdynamo but got some error modules. do you know which version torchinductor, torchdynamo and triton are used to make your modification work?\r\n\r\nHey, dynamo migrated to latest triton, so we maybe have some new errors here, but the torchdynamo Makefile https://github.com/pytorch/torchdynamo/blob/main/Makefile has the versions of all our deps (usually cutting edge nightlies).\r\n\r\nI plan to revisit this shortly, and will fix up any errors I find. ",
      "Thanks, I'll close this for now, since it doesn't quite yet work \"out of the box\" and relying on nightly versions makes things difficult for me to maintain. I'm hoping to get an easier integration with the stable PyTorch 2 interface once it's out."
    ],
    "num_comments": 15,
    "repository": "openai/whisper",
    "diff_length": 30214,
    "code_diff": "diff --git a/README.md b/README.md\nindex 1beaf359f..df953dc92 100644\n--- a/README.md\n+++ b/README.md\n@@ -28,12 +28,17 @@ It also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be ins\n sudo apt update && sudo apt install ffmpeg\n \n # on MacOS using Homebrew (https://brew.sh/)\n-brew "
  },
  {
    "pr_title": "attempt to fix the repetition/hallucination issue identified in #1046",
    "pr_body": "",
    "pr_number": 1052,
    "comments": [
      "Hi @jongwook Not sure if you saw the comment below, but it includes a reproduction case which might be useful:\r\n\r\nhttps://github.com/openai/whisper/pull/869#issuecomment-1445024980\r\n\r\nThe repetition persists with this PR.",
      "@ryanheise thanks! will look into it...",
      "The problem triggered by the test data from @ryanheise is model sensitive. I see the problem with `small` but using either `small.en` or `medium.en` looks ok although the timing of the last few words is off. Below is the mp3 fragment converted to video to show the English subtitles.\r\n\r\nhttps://user-images.githubusercontent.com/3035114/223597998-74a8ec7f-da0b-4948-9f6a-75712820eb15.mp4\r\n\r\n\r\n",
      "Thanks all! The incorrect zero-padding of Mel spectrograms as identified in #730 and #838 was contributing to this error. The fix in 477f0be appears to fix the repetition issue.",
      "> The fix in https://github.com/openai/whisper/commit/477f0befc76456c37b2d2f484095f803592c90fa appears to fix the repetition issue.\r\n\r\nI can confirm this fixed my example, thanks! :+1:\r\n\r\n> Below is the mp3 fragment converted to video to show the English subtitles.\r\n\r\n@glangford FYI the subtitles didn't show in your video.",
      "@ryanheise Inline, (on Mac at least) you may need to click on the >> on the right to turn on subtitles. Or download and view with VLC, Quicktime, or whatever and enable subtitles in the viewer.",
      "Ah, I see, Firefox doesn't show any options, but downloading it and opening in VLC works. You can also do hard subs this way https://github.com/openai/whisper/discussions/435#discussioncomment-4238872\r\n\r\n> using either `small.en` or `medium.en` looks ok although the timing of the last few words is off.\r\n\r\nHere is the `base` model for comparison, which appears more accurate on the last few words:\r\n\r\nhttps://user-images.githubusercontent.com/19899190/223708621-c4b53230-171c-4e92-871f-4c5f390425a1.mp4",
      "Btw have you guys tried with longer audio, e.g. 5 mins long? I am still getting a lot of repetition even with this fix.\r\nE.g. on the TEDLIUM test set \"AimeeMullins_2009P.wav\"\r\n>[02:10.440 --> 02:14.720]  and needless to say, thank God, I wasn't using a thesaurus back then.\r\n[02:14.720 --> 02:14.720]  and needless to say, thank God, I wasn't using a thesaurus back then.\r\n[02:15.460 --> 02:18.580]  I mean from this entry, it would seem that\r\n[02:18.580 --> 02:22.800]  I was born into a world that perceived someone like me\r\n[02:22.800 --> 02:23.340]  I was born into a world that perceived someone like me\r\n[02:23.340 --> 02:27.540]  to have nothing positive, whatsoever, going for them\r\n[02:27.540 --> 02:27.540]  to have nothing positive, whatsoever, going for them\r\n[02:27.540 --> 02:35.340]  When in fact today, I'm celebrated for the opportunities and adventures my life has procured\r\n[02:35.340 --> 02:35.960]  When in fact today, I'm celebrated for the opportunities and adventures my life has procured\r\n[02:35.960 --> 02:42.140]  So I immediately went to look up the 2009 online edition\r\n[02:42.140 --> 02:42.160]  So I immediately went to look up the 2009 online edition\r\n[02:42.160 --> 02:42.160]  So I immediately went to look up the 2009 online edition\r\n\r\nI was hoping to update word segmentation results for whisper-only word timestamps in our paper https://arxiv.org/abs/2303.00747\r\n\r\nBut currently i am getting better results with our implementation which is similar to https://github.com/linto-ai/whisper-timestamped\r\n",
      "> Btw have you guys tried with longer audio, e.g. 5 mins long? I am still getting a lot of repetition even with this fix.\r\n\r\nI am testing a longer audio now (running on CPU, larger model, transcript+transcribe so it is taking a while). For clarity,\r\n* are you running the 20230307 release version? with, or without `--word_timestamps`?\r\n* the repetitions from \"AimeeMullins_2009P.wav\" above, are they from verbose print to the console? \r\n\r\nIt seems like there are different possible sources of error, in all the different discussions\r\n* model hallucination\r\n* new repetition introduced or magnified by `--word_timestamps True`\r\n* (hand waving) segmentation issues",
      "> are you running the 20230307 release version? with, or without --word_timestamps?\r\n\r\nyes\r\n\r\n> the repetitions from \"AimeeMullins_2009P.wav\" above, are they from verbose print to the console?\r\n\r\nyes\r\n\r\n",
      "@jongwook Note from @m-bain example above the repetition occurring with verbose print. The repetitions in this example are all \"instantaneous\" ; eg same start and end time\r\n> [02:14.720 --> 02:14.720] and needless to say, thank God, I wasn't using a thesaurus back then.\r\n\r\nthey are printed but then immediately cleared by this code, which looks like a bug unique to `--verbose True`\r\n\r\nhttps://github.com/openai/whisper/blob/aac47c98349b98cec5ca7b1be53960fb59f4436b/whisper/transcribe.py#L345",
      "@m-bain Given this could you maybe rerun and see if the formal output formats are messed up or not, using `--verbose False`?",
      "This is not a verbose error, and the start times and end times of repetition are not always instantaneous, see output for the .srt file without verbose:\r\n\r\n271\r\n00:02:14,440 --> 00:02:14,720\r\nand needless to say, thank God, I wasn't using a thesaurus back<u> then.</u>\r\n\r\n272\r\n00:02:14,720 --> 00:02:14,720\r\n\r\n\r\n273\r\n00:02:15,460 --> 00:02:16,180\r\n<u>I</u> mean from this entry, it would seem that\r\n\r\n274\r\n00:02:16,180 --> 00:02:16,360\r\nI<u> mean</u> from this entry, it would seem that\r\n\r\n275\r\n00:02:16,360 --> 00:02:16,960\r\nI mean<u> from</u> this entry, it would seem that\r\n\r\n276\r\n00:02:16,960 --> 00:02:17,220\r\nI mean from<u> this</u> entry, it would seem that\r\n\r\n277\r\n00:02:17,220 --> 00:02:17,620\r\nI mean from this<u> entry,</u> it would seem that\r\n\r\n278\r\n00:02:17,620 --> 00:02:17,800\r\nI mean from this entry, it would seem that\r\n>",
      "So there are at least two problems then\r\n* verbose mode can print cleared segments\r\n* something else triggered by word_timestamps\r\n\r\nGiven how close the start/end times are it feels like something related to `seek_shift` is still off\r\nhttps://github.com/openai/whisper/blob/aac47c98349b98cec5ca7b1be53960fb59f4436b/whisper/transcribe.py#L337\r\n\r\n@m-bain Do the same repetitions happen with `word_timestamps False` or no? ",
      "Update, I realise there is some specific underline formatting in the word_timestamps, was able to get it working in the end. See here for comparison on word-level timestamp accuracy\r\n\r\n![image](https://user-images.githubusercontent.com/36994049/224011580-4782f2ad-a178-4b2d-80c3-4baa8ca54ab9.png)\r\n\r\n@jongwook could you share the evaluation for long-form transcription WER? I am unable to reproduce whisper results, right now I report in the vanilla setting -- greedy/beam5 decoding without the heuristic tricks\r\n"
    ],
    "num_comments": 15,
    "repository": "openai/whisper",
    "diff_length": 6338,
    "code_diff": "diff --git a/whisper/audio.py b/whisper/audio.py\nindex a19b7ab0d..513ab7c9d 100644\n--- a/whisper/audio.py\n+++ b/whisper/audio.py\n@@ -1,6 +1,6 @@\n import os\n from functools import lru_cache\n-from typing import Union\n+from typing import Optional, Union\n \n import ffmpeg\n import numpy as np\n@@ -15,10 +1"
  },
  {
    "pr_title": "Per Token Confidence + Color terminal example",
    "pr_body": "Hello!\r\nI implemented per-token confidence scores and also added a little example under examples/confidence_per_token.py, where you get a fancy colored text output resembling the confidence score of each token:\r\n\r\n\r\n(image incorrect, look at last commented image)\r\n![image](https://user-images.githubusercontent.com/43215895/226175988-e18657f4-589a-422d-a93c-6f3139648eaf.png)\r\nExample WAV: https://www.voiptroubleshooter.com/open_speech/american/OSR_us_000_0010_8k.wav\r\n\r\n\r\nI am aware of the work of",
    "pr_number": 1119,
    "comments": [
      "EDIT: Done\r\n\r\nTODO: correct propability display when supplying prompts (prompt tokens seem to get assigned prob of 0, if anyone can please help, I'd appreciate it)",
      "Fixed probs offset (and prompt offset)\r\n\r\nNew (correct) output:\r\n![image](https://user-images.githubusercontent.com/43215895/227074244-58d15f24-2a25-4302-a1f9-068bd8d01d6f.png)\r\n",
      "This can be really useful for proofing the output via something like Subtitle Edit.  \r\n\r\nWould really need an command line option to output an additional subtitle though, right?  \r\n\r\nI get the impression @jongwook doesn't want to stuff too many features in though, so how does such a useful feature get added without having a fork?",
      "> I get the impression @jongwook doesn't want to stuff too many features in\r\n\r\nAlthough the colour terminal stuff might be questionable, I think adding per-token timestamps and confidence to the raw JSON results could itself be useful to a wider range of use cases. Exposing the raw data from Whisper would then make it possible to write your own external script on top of that to do the colour terminal staff.\r\n\r\nOne example of the wider potential uses of per-token data is that German is known to have very long compound words, and if you wanted to break them up (e.g. to compute a line break timestamp, or for sub-word highlighting, etc.), it would be helpful to have access to the raw data per token.",
      "Hello!\r\n\r\n> Although the colour terminal stuff might be questionable\r\n\r\nI implemented the per-token confidence as is and implemented the colorful CLI output only in an example.\r\nThe main whisper code does not contain anything with color\r\n\r\n@jongwook is there anything I should modify or change for you to accept the PR? ",
      "I'm hesitant to add this because the incremental utility of this compared to the probabilities returned by `word_timestamps=True` is quite niche, versus the added complexity & latency due to the additional GPU operations. The decoding logic is already taking as much as the forward pass, and I'm hoping to reduce this overhead. The subword token probabilities are not very useful anyway, because it's usually influenced more by language modeling than from speech recognition.\r\n\r\nFor the case you need per-token probs, you can add another forward pass without modifying decoding.py (similar to how it's done in [timing.py](https://github.com/openai/whisper/blob/76c901ab8d4558992c44138479c4d69eb52fadcb/whisper/timing.py#L197)) without incurring too much additional latency. It may even be faster than adding GPU operations for every autoregressive step.\r\n\r\nThe example script looks nifty, but i'd prefer it in the [show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) section.",
      "I see, thank you for the comprehensive response!",
      "@SinanAkkoyun thanks for your contribution. Not sure, but seems it works incorrect, \r\nI made distorted speech example https://drive.google.com/file/d/12zGWllJg6edftcnwuHX_ZHMuwk7PlVjg/view?usp=sharing .\r\nIf I don't set the language of decoding i.e. `options = whisper.DecodingOptions() `,  the output is correct in terms of locating mispronounce (I can read this slavic) though it translates it to random language.\r\n![Screenshot from 2023-08-09 10-58-49](https://github.com/openai/whisper/assets/54935496/cf45eaab-1799-45d0-a386-2fc2e3076b1a)\r\n\r\nBut if I set 'en' for decoding  `options = whisper.DecodingOptions(language=\"en\")` the picture is wrong.\r\n![Screenshot from 2023-08-09 10-58-53](https://github.com/openai/whisper/assets/54935496/8c009a46-ec4d-4a47-abe3-786408580857)\r\n The rest of the code is the same as in your PR except I used \"small\" model.\r\n",
      "@Rtut654 Hi, I don't quite understand the issue you are having, the \"I like to play badminton and football.\" seems to be correct, the football especially sounds vague in the audio you provided. Could you please tell me more about your issue?\r\n\r\nDespite that, the PR is not going to get merged, so I stopped working on it and use that modification in my own work which does not include translation\r\n\r\nIf the random translation is the problem you are referring to, I believe that my PR did not modify nor change the output prediction by any means, it just grabbed the logits and displays them as confidence",
      "@SinanAkkoyun \r\nThe issue is in the accuracy of token_probs. The first version (with translation to Ukrainian) gives very accurate result since \"like\" was also mispronounced very much. Also the word \"football \" was mispronounced in the last part which is correctly shown in the first picture. \r\n\r\nI did the same test with other audio, setting language of decoding to English.  The picture was same. Somehow it is lowering the prob of the last word even when it is pronounced correctly. At the same time probs of mispronounciations were high which is strange. So something is wrong in the way it predicts probs when language is set to English.",
      "Hello, @jongwook ! Could you please look at the issue?",
      "@Rtut654 Ah I see, I am sorry but in my testing I was not able to reproduce that behaviour, for me it seemed very fine in regards to unclear spoken words and clearly spoken words, even in german transcriptions.\r\nTo me, the football sounded unclearer than the rest, my code does not do much except that it takes the unmodified logits. Maybe the model has a different sense of misspronunciation than you? It has been trained on many accents.\r\n\r\nI currently am working on other projects, you can take a look at the file changes yourself and work your way through, if you have a question feel free to ask and I will give my best to clarify",
      "In case it's of interest, I created a small web component to view the Whisper JSON file when `--word_timestamps` has been used. Ideas for improving it would be welcome!\r\n\r\nhttps://edsu.github.io/whisper-transcript/"
    ],
    "num_comments": 13,
    "repository": "openai/whisper",
    "diff_length": 9839,
    "code_diff": "diff --git a/.gitignore b/.gitignore\nindex 7ae8fabc6..9f5ed862b 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -9,3 +9,6 @@ thumbs.db\n .DS_Store\n .idea\n \n+.venv/\n+\n+samples/\ndiff --git a/README.md b/README.md\nindex eba82ce22..9ea3a38e5 100644\n--- a/README.md\n+++ b/README.md\n@@ -144,4 +144,4 @@ Please u"
  },
  {
    "pr_title": "add always_use_initial_prompt",
    "pr_body": "    always_use_initial_prompt: bool\r\n        if True, the initial_prompt will be used to all windows, and condition_on_previous_text\r\n        will be ignored. Enabling this may make the text more consistent if the audio is long\r\n        and you set the initial_prompt properly.",
    "pr_number": 1040,
    "comments": [
      "I think some variation on this idea might help it to remember your prompting in long audio, but when a window boundary occurs mid sentence, I think it's also important to have the previous text as the prompt.\r\n\r\nAs a compromise, have you thought about truncating the previous text at a sentence boundary and then prepending the initial prompt before that? It might be the best of both worlds.",
      "> I think some variation on this idea might help it to remember your prompting in long audio, but when a window boundary occurs mid sentence, I think it's also important to have the previous text as the prompt.\r\n> \r\n> As a compromise, have you thought about truncating the previous text at a sentence boundary and then prepending the initial prompt before that? It might be the best of both worlds.\r\n\r\nI agree, but I don't know how to do that.",
      "A really cheap modification might be to add a check here:\r\n\r\n```python\r\n            if not condition_on_previous_text or result.temperature > 0.5:\r\n                # do not feed the prompt tokens if a high temperature was used\r\n                prompt_reset_since = len(all_tokens)\r\n```\r\n\r\nso that you also check if your option is enabled and if the latest token ends with one of these characters `\".„ÄÇ!ÔºÅ?Ôºü\"`, effectively resetting the prompt after every sentence boundary. Then when feeding the prompt:\r\n\r\n```python\r\n            decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\r\n```\r\n\r\nIf your option is enabled you could prepend the initial prompt here.\r\n\r\nBUT, I think it might be more useful to parameterise how many previous sentences back to include in the prompt. For that the code would be a bit more complicated. But you could keep a FIFO buffer, e.g. to remember the last 3 sentences, you have a FIFO of size 3 containing the last 3 sentence boundary positions, which you put into the FIFO under the same condition as that first block of code above. The oldest sentence boundary gets popped out so you never have more than the last 3 in there.",
      "I use whisper ai for audio transcription and translation but since 25 February  it is not transcribing and Translating clearly !!!               I would be happy if anybody help me. plz ",
      "@ryanheise note that this code in `decoding.py:594` truncates the list of all prompt tokens (from the beginning), not the end. That means that simply prepending without checking for prompt window length will not always work. Truncation size depends on the model config.\r\n\r\n`tokens = ( [self.tokenizer.sot_prev] + prompt_tokens[-(self.n_ctx // 2 - 1) :] + tokens )`",
      "the output after this is just amazing\r\n\r\ni dont get why this is still not implemented ",
      "@mercury233 it is hallucinating significantly after this change. anyway to prevent it? other than that it works great. you found a solution for hallucination ? I can use very big beam size and best of but they didnt help. ",
      "> ```python\r\n>  not condition_on_previous_text or result.temperature > 0.5\r\n> ```\r\n\r\ncan you share modified file like this? i would like to test. currently it is having problems ",
      "yes with this way it is skipping 30 second blocks sometimes. we need optimization.  @mercury233 @ryanheise @radurevutchi ",
      "> @mercury233 it is hallucinating significantly after this change. anyway to prevent it? \r\n\r\nSorry, I didn't",
      "I have used the same basic idea of applying the initial prompt to every window to supply a dictionary of obscure words that might be in the transcript. It's very effective at boosting recognition of some words. However, I don't see it as in opposition to condition_on_previous text; the basic idea of using context from the end of the previous window to influence understanding of the beginning of the next window is still valuable."
    ],
    "num_comments": 11,
    "repository": "openai/whisper",
    "diff_length": 3029,
    "code_diff": "diff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex 8e1240bd6..674e450de 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -46,6 +46,7 @@ def transcribe(\n     no_speech_threshold: Optional[float] = 0.6,\n     condition_on_previous_text: bool = True,\n     initial_prompt: O"
  },
  {
    "pr_title": "Replaced 'no' langauge code with 'nb' and use full norwegian language names",
    "pr_body": "The 'no' language code is a obsolete language code that is the union of language codes 'nb' (Norwegian Bokm√•l) and 'nn' (Norwegian Nynorsk), the two written variants in use in Norway.  As 'no' is misleading, and seem to be used by Whisper to mean Norwegian Bokm√•l, I recommend replacing it with 'nb' and using the full names for both of the norwegian written forms.\r\n\r\nUseful references:\r\n\r\n * https://linguistics.stackexchange.com/questions/36784/norwegian-translation-codes-no-nn-nb-which-to-use-on",
    "pr_number": 1250,
    "comments": [
      "From my understanding, this may be similar to Chinese which also has multiple written variants, although Chinese also has multiple spoken dialects. The way Whisper was trained, it was trained on all of these written and spoken variants under the umbrella of \"Chinese\", and so ultimately a single language code has to describe all of these. Since there is only one language code for Chinese, the way you get it to transcribe for a particular variant is therefore not by specifying a different language but by using the same language code and then using a prompt to get it off on the right foot with the variant. It may not be a strictly correct use of language codes, but the training is already done that way.\r\n\r\nI haven't tried transcribing Norwegian, but is it similar in that Whisper contains training data for both writing systems under a single umbrella of \"no\" which can be accessed by using a prompt to set it off in one of those two directions? If so, I'm not sure if changing it to \"nb\" would make sense.",
      "[ryanheise]\n> From my understanding, this may be similar to Chinese which also has\n> multiple written variants, although Chinese also has multiple spoken\n> dialects.\n\nI believed China had several distinct languages with their own written\nlanguage as well as the written Mandarin standardized across the\ncountry, in addition to a lot of dialects of the different languages.\n\n> I haven't tried transcribing Norwegian, but is it similar in that\n> Whisper contains training data for both writing systems under a single\n> umbrella of \"no\" which can be accessed by using a prompt to set it off\n> in one of those two directions? If so, I'm not sure if changing it to\n> \"nb\" would make sense.\n\nThe transcribing I have tested so far gave me Norwegian Bokm√•l.  Not\nsure how to to prompt it to switch to Norwegian Nynorsk.\n\n-- \nHappy hacking\nPetter Reinholdtsen\n",
      "You can try using `--initial_prompt \"Some introductory pre-sentence written in the Norwegian Nynorsk script.\"`\r\n\r\nSo you just write a made-up sentence in the script you want, and you might get better results if it is a sentence that you could plausibly imagine as having been spoken before the first actual sentence in the audio you're transcribing.",
      "The rest of the world uses no and doing this change would result in numerous posts all over the world asking why -no doesn't work. Not to mention the hundreds of guides and copies of documentation the uses -no\r\n\r\nAs a minimum -no should be kept for legacy support and nn and nb should only be implemented if whisper actually knows the difference.",
      "There are for sure quite a few using the Norwegian language codes incorrectly, or confuse the country and language code.  Because of this it is a good idea to keep the incorrect 'no' language code still working as an alias, probably for the W3C recommended 'nb' code.\r\n\r\nIn any case, if Whisper is unable to know the difference between Nynorsk and Bokm√•l, I guess the entire question is moot.",
      "> You can try using `--initial_prompt \"Some introductory pre-sentence written in the Norwegian Nynorsk script.\"`\r\n\r\nBy the way, did you get around to trying this?",
      "\n[ryanheise]\n> By the way, did you get around to trying this?\n\nI did, running this using a random nynorsk piece on youtube,\n<URL: https://yewtu.be/watch?v=s7olTWEIwAI >.\n\n  whisper --model medium Are\\ Kalv√∏\\ -\\ K√•seri\\ om\\ nynorsk\\ \\[s7olTWEIwAI\\].webm --language Nynorsk --initial_prompt \"eg heitar\"\n\nSadly the resulting transcription is of very low quality:\n\n  Den st√∏rste fordelen me √• bruke ny norsk er at det gj√¶r det lett √•\n  framst√• som langt meir intresang enn du faktisk er.  For oss som\n  koserer er det for eksempel helt opplagt enn fordel √• bruke ny norsk.\n\nThere are several typos and inaccuracies here at the start of the\nrecording. :)\n\n-- \nHappy hacking\nPetter Reinholdtsen\n",
      "Normally I would say that \"eg heitar\" isn't actually a pre-sentence, it's only two words, no full stop at the end, etc. and probably not a great prompt to teach Whisper what style to continue in. Although that being said, I don't hold out any hopes that it will work well if you're getting typos. That feels like there's limited training data for that script.",
      "We originally collected the language tags from the [VoxLingua107](https://bark.phon.ioc.ee/voxlingua107/) dataset, but 100% of the transcription data had `no`, and no `nn` label. We had some `nn` labels in the translation data, but I guess that's less relevant when the input is spoken Norwegian and output is English.\r\n\r\nSo the labeling was a bit haphazard, but I think it still makes sense to keep the macrolanguage label `no`, considering that that labels would've contained a mixture of Bokm√•l and Nynorsk. (It appears that most were in Bokm√•l though, as the Nynorsk prompting example above didn't work very well unfortunately.)",
      "\n[Jong Wook Kim]\n> We originally collected the language tags from the\n> [VoxLingua107](https://bark.phon.ioc.ee/voxlingua107/) dataset, but\n> 100% of the transcription data had `no`, and no `nn` label. We had\n> some `nn` labels in the translation data, but I guess that's less\n> relevant when the input is spoken Norwegian and output is English.\n\nThis is not really surprising that Voxlingual07 used 'no', given that it\nstates \"VoxLingua107 is a speech dataset for training spoken language\nidentification models.\"  There is a difference between language codes\nfor spoken language and written language in Norway.  The Norwegian\nBokm√•l and Nynorsk are written languages, while the spoken language is a\ndialect of Norwegian.  So all written Norwegian should use either 'nb'\nor 'nn', and spoken language could use 'no'.  If you only found 'no' in\ntranscription data, the transcriptions are misclassified, and most\nlikely should have been classified as 'nb', the written variant used by\nmost people in Norway.\n\n> So the labeling was a bit haphazard, but I think it still makes sense\n> to keep the macrolanguage label `no`, considering that that labels\n> would've contained a mixture of Bokm√•l and Nynorsk. (It appears that\n> most were in Bokm√•l though, as the Nynorsk prompting example above\n> didn't work very well unfortunately.)\n\nYeah.  Hope someone can do a better job at training a system to write\nNorwegian Bokm√•l and Nynorsk with the correct classicication in the\nfuture.  At least the issue is better known in the Whisper community\nnow.\n\nNote, there are ways to fairly accurately detect if the written text is\nNynorsk or Bokm√•l by looking for marker words like 'jeg'(nb) vs 'eg'(nn)\nog 'en'(nb) vs 'ein'(nn).\n\n-- \nHappy hacking\nPetter Reinholdtsen\n",
      "I was looking into this recently and one approach to work around Whisper's inclination toward Bokm√•l (based on its training data for `no`) would be to just use a separate tool to convert Whisper's Bokm√•l output into Nynorsk in post processing.\r\n\r\nFor example, [Apertium](https://www.apertium.org/) can be used, and can be installed locally (making it also helpful for automated pipelines).\r\n\r\nThe specific Bokm√•l/Nynorsk module for Apertium is [here](https://github.com/apertium/apertium-nno-nob), including some examples of its output.\r\n\r\nFor timestamps, the two scripts are at least textually similar enough to match them up (e.g. [Diff Match Patch](https://neil.fraser.name/software/diff_match_patch/demos/diff.html)). An easier way might be to just embed timestamps of the form 00:24:07 into the source text and see if Apertium preserves them in place without disrupting the translation process. It seems to work, but I haven't fully tested that.",
      "[ryanheise]\r\n> I was looking into this recently and one approach to work around\r\n> Whisper's inclination toward Bokm√•l (based on its training data for\r\n> `no`) would be to just use a separate tool to convert Whisper's Bokm√•l\r\n> output into Nynorsk in post processing.\r\n\r\nThis is a good point, and Apertium is doing a very good job as\r\nconverting Bokm√•l to Nynorsk.  But sadly it also require seriuos proof\r\nreading as it is far from perfect, according to the creator of the\r\nnb->nn Apertium transformer. :)\r\n\r\nIn any case, the main takeaway here is that the 'no' language code is\r\nfor a spoken language, while the 'nb' and 'nn' language codes are for\r\nwritten languages.\r\n\r\n-- \r\nHappy hacking\r\nPetter Reinholdtsen\r\n"
    ],
    "num_comments": 12,
    "repository": "openai/whisper",
    "diff_length": 744,
    "code_diff": "diff --git a/whisper/tokenizer.py b/whisper/tokenizer.py\nindex 4030e15aa..eaef67bd4 100644\n--- a/whisper/tokenizer.py\n+++ b/whisper/tokenizer.py\n@@ -37,7 +37,7 @@\n     \"da\": \"danish\",\n     \"hu\": \"hungarian\",\n     \"ta\": \"tamil\",\n-    \"no\": \"norwegian\",\n+    \"nb\": \"norwegian bokm√•l\",\n     \"th\": \"thai\""
  },
  {
    "pr_title": "Add support for AMD GPU (ROCm Platform)",
    "pr_body": "PyPI name of openAI Triton for ROCm Platform is pytorch-triton-rocm, so that modify setup.py to install correct Triton package for ROCm platform. Also modify README.md to add instruction to install on ROCm Platform. Tested on ROCm Platform with AMD GPUs.",
    "pr_number": 1473,
    "comments": [
      "Instead of using environment variable, i suggest to use `extras_require` as could be see in [here](https://setuptools.pypa.io/en/latest/userguide/dependency_management.html#optional-dependencies). Another option is to automatically detect if it is using ROCm platform.",
      "Based on the suggestion, remove environmental variable and add function to detect ROCm Platform automatically.",
      "Will this work with generic AMD gpus, ie newer integrated gpus?",
      "> Will this work with generic AMD gpus, ie newer integrated gpus?\r\n\r\n@x86Gr This is the list of supported GPUs\r\n\r\nhttps://rocm.docs.amd.com/en/latest/release/gpu_os_support.html",
      "@x86Gr @glangford \r\nThis link should work: https://rocm.docs.amd.com/en/latest/release/gpu_os_support.html#linux-supported-gpus\r\n",
      "Any particular reason the PR only selected a subset of supported AMD GPUs? \r\n\r\ngfx1030 or gfx1100 appear to be missing",
      "@vadimkantorov  @Reviewer  of this PR. Is there any update about review to merge  this PR? Is there  anything I can help to speed up the process?  Thanks!",
      "running `pip install .` on that branch shows me \r\n```\r\n% pip install .            \r\nProcessing <snip>whisper/whisper\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nINFO: pip is looking at multiple versions of openai-whisper to determine which version is compatible with other requirements. This could take a while.\r\nERROR: Could not find a version that satisfies the requirement pytorch-triton-rocm>=2.0.1 (from openai-whisper) (from versions: 0.0.1)\r\nERROR: No matching distribution found for pytorch-triton-rocm>=2.0.1\r\n```\r\nwhat am i missing here?\r\n\r\n--\r\nEdit:\r\nNevermind, `pip install --upgrade --no-deps torch pytorch-triton-rocm --index-url https://download.pytorch.org/whl/rocm6.2` solved it. Maybe we should add that to the readme?"
    ],
    "num_comments": 8,
    "repository": "openai/whisper",
    "diff_length": 404882,
    "code_diff": "diff --git a/.github/workflows/test.yml b/.github/workflows/test.yml\nindex 3796a3973..dffc17c61 100644\n--- a/.github/workflows/test.yml\n+++ b/.github/workflows/test.yml\n@@ -6,8 +6,38 @@ on:\n   pull_request:\n     branches:\n       - main\n+\n jobs:\n+  pre-commit:\n+    runs-on: ubuntu-latest\n+    steps:\n"
  },
  {
    "pr_title": "Add CSV formatted output in transcript, using integer start/end times in milliseconds.",
    "pr_body": "This PR adds CSV output to Whisper transcription similar to the way #102 added SRT subtitle formatted output.\r\n\r\nEach line of the resulting CSV file is formatted like:\r\n` <startTime-in-integer-milliseconds>, <endTime-in-integer-milliseconds>, \"<transcript-including-commas>\"`\r\n\r\nOne of the reasons for using integer millisecond timings is to avoid regional incompatibilities with writing and reading floating point timings across language regions which use different characters - either \".\" or \",\" - ",
    "pr_number": 228,
    "comments": [
      "The CSV seems to just multiply the range by 1000 to get ms resolution.  Could we please have actual ms resolution for the speakers?  If doing text-to-speech of a natural conversation, and trying to combine it with speaker recognition (pytorch for example), two persons may speak in the same second.  We don't know who said what.  Therefore, if we had ms resolution out of whisper, we could easily know who said what sentence in a natural conversation.",
      "To use Whisper for subtitles, Millisecond resolution would be a big plus. ",
      "@ksn-systems that is precisely why I made this change. See #233 for details.",
      "@jongwook  would it be possible to \"approve the workflow\" as I see this PR is stuck at \"1 workflow awaiting approval\".\r\n\r\nIs there anything I should change or clarify in order to get this PR merged or get the \"workflow awaiting approval\" to be satisfied?\r\n\r\nPlease note a similar PR was merged for whisper.cpp  ( https://github.com/ggerganov/whisper.cpp/pull/340 ). Having this functionality in place for whisper allows for easier comparison of results between implementations, via easier importing into spreadsheets and databases supporting the CSV format.",
      "@NielsMayer Thanks for the PR! Would it work for you if I make this `write_tsv` instead? CSV format is not standardized and `csv.reader` and `pandas.read_csv` often create headache parsing quotes.\r\n\r\nI should probably merge #333 first with some modifications as the number of output files is becoming unwieldly.",
      "> @NielsMayer Thanks for the PR! Would it work for you if I make this `write_tsv` instead? CSV format is not standardized and `csv.reader` and `pandas.read_csv` often create headache parsing quotes.\r\n> \r\n> I should probably merge #333 first with some modifications as the number of output files is becoming unwieldly.\r\n\r\nYes, merging #333 makes sense. If you do that first, I will update this PR to conform to the changes made, e.g. add additional csv (or tsv) output format keyword.\r\n\r\nW/r/t changing from CSV to TSV, that is fine by me as it would require a trivial change on my end. The reason why I chose CSV is that it seems more \"standard\";  although most of the programs automatically importing from CSV just as easily handle TSV. \r\n\r\nI'm not familiar with the comma issues you mention in csv.reader or pandas.read_csv, however, do note that I updated the code for compatibility with importing into, e.g. openoffice, where string type is automatically recognized if delimited by '\"' character.  To prevent issues, Internal `\"` in each CSV text line  is replaced by two consecutive single quotes `''` ... Note: ` print('\"' + segment['text'].strip().replace('\"', \"''\") + '\"'`\r\n\r\nChances are, such formatting and lack of special escape character means the existing solution would work with the readers you mention, @jongwook .\r\n\r\nPS: I updated my repo for this PR to the latest head of repository from whisper, so once again, there is a \"workflow awaiting approval\" message...",
      "FYI here's a whisper CSV file read into libreoffice on a linux desktop. Note the \" replaced by '' \r\n(orig source: https://rumble.com/v2619vq-bills-proliferate-to-criminalize-speech-darren-beattie-on-lex-and-brazil-fa.html )\r\n```\r\n3171020 | 3172820 | and he is up right now.\r\n3172820 | 3176820 | [''System Updates,'' main theme music playing.]\r\n3182620 | 3183760 | Great to be here.\r\n```\r\n\r\n@jongwook how on earth did whisper figure out that ` [''System Updates,'' main theme music playing.]` -- I mean how did it figure out the name of the show (\"remembered\" the announcement of the show name at the beginning, sometimes an hour earlier?) \r\n\r\nSometimes however it gets the theme music wrong on the same show, but different episode: ( https://rumble.com/v24mywg-what-really-happened-in-brazil-yesterday-system-update-18.html )\r\n`\"[''The Daily Show Theme'']\"\r\n\r\nLikewise how is whisper figuring out where quotes start and end? It's kind of spooky actually! :-)",
      "Thanks for accommodating the TSV suggestion! I merged a refactored version of #333 and edited this PR accordingly.\r\n\r\nThe issue about CSV is that, although CSV is more widely used and well known, even the simplest possible case like:\r\n\r\n```csv\r\nstart, end, text\r\n1234, 12345, \"hello, world!\"\r\n```\r\n\r\nresults in very inconsistent user experience according to the program because of the lack of standardization around the quotation marks:\r\n\r\n**Apple Quick Look:**\r\n<img width=\"176\" alt=\"image\" src=\"https://user-images.githubusercontent.com/266841/213907134-61c0790b-f92b-4a5b-bdfb-f714835aac50.png\">\r\n\r\n**Apple Numbers:**\r\n<img width=\"200\" alt=\"image\" src=\"https://user-images.githubusercontent.com/266841/213907251-577c1a5b-83b4-47dc-a3dd-25ccff3c69da.png\">\r\n\r\n**`csv.reader()`:**\r\n<img width=\"568\" alt=\"image\" src=\"https://user-images.githubusercontent.com/266841/213907278-ce67bbc7-0331-4ef2-bafd-e1b7860c58a6.png\">\r\n\r\n**pandas.read_csv()**\r\n<img width=\"229\" alt=\"image\" src=\"https://user-images.githubusercontent.com/266841/213907322-a997810e-3a72-4e34-9e0d-0371b83f988c.png\">\r\n\r\nThe latter two are the most common way to read CSV files in Python -- there are some combination of options to read the file as intended, but it's inconvenient and not practical to expect the users to use the \"correct\" configuration for all reader implementations.\r\n\r\n\r\nMeanwhile, TSV doesn't need to deal with quoting because the field values are not allowed to contain tab characters.\r\n\r\n---\r\n\r\nRe: the second comment, because of the way that Whisper was trained, the model must have encountered the exact music and the text `[\"System Updates\" main theme music playing.]` multiple times during training. It's usually an undesired behavior, and we tried to mitigate this (rather hackily) by suppressing the `[` character by default.\r\n"
    ],
    "num_comments": 8,
    "repository": "openai/whisper",
    "diff_length": 2955,
    "code_diff": "diff --git a/whisper/transcribe.py b/whisper/transcribe.py\nindex 02952dfe2..c0404416e 100644\n--- a/whisper/transcribe.py\n+++ b/whisper/transcribe.py\n@@ -260,7 +260,7 @@ def cli():\n     parser.add_argument(\"--model_dir\", type=str, default=None, help=\"the path to save model files; uses ~/.cache/whispe"
  },
  {
    "pr_title": "#22667 replaced occurrences of master/slave terminology with leader/follower",
    "pr_body": "The docs and some tests contain references to a master/slave db configuration.\nWhile this terminology has been used for a long time, those terms may carry racially charged meanings to users.\nThis patch replaces all occurrences of master and slave with 'leader' and 'follower'\n",
    "pr_number": 2692,
    "comments": [
      "Thanks for taking the time to do this!\n",
      "Are you serious ?\nThe meaning of a word is defined by its use, by the context.\nIn this case, _master_/_slave_ is used by every database server, in every documentation ([redis](http://redis.io/topics/replication), [mysql](http://dev.mysql.com/doc/refman/5.0/en/replication-howto.html) ‚Ä¶)\n\nNB: I don't say I'm against this change. Just that I don't see the point of changing two words with two others just because they have been used somewhere else.\nFor example, your avatar is red. Red, like communism. You should use a black and white color. Oh no, that's linked to racism too. Well. Let's remove colors, too, then ? ;)\n",
      "As you can see from https://github.com/django/django/commit/beec05686ccc3bee8461f9a5a02c607a02352ae1 the terminology we have actually used is \"primary/replica\". So thank you for your interest, but there's nothing to see here and you can move along now.\n",
      "Before the flood of white male HN dwellers truly kicks off and obliterates all reasonable discussion, I'd like to thank the Django team for taking the time to do this.\n",
      "i'm just here for the laughs. is IT becoming too stupid? has science gone too far?\n",
      "Discussion on the ticket: https://code.djangoproject.com/ticket/22667\n\"Primary\" and \"Replica\" aren't especially bad choices, but they're also wrong. The correct terms are `master` and `slave`. They've been used in databases, hardware setups, server setups and god knows what else for god knows how long.\n\nI cannot f_ing believe this PR made it through and was given actual man hours when there are massive outstanding PRs and patches on trac that need real attention. And don't be surprised that this does make it on HN and probably later on the usual slashdot/phoronix and what not. This is stupid *and_ controversial which is exactly what tech media loves.\n\nAnd to the HN/whatever crowd, don't post stupid memes here. This isn't the place.\n\nHelp yourselves and revert this, guys. Django docs, or docs in general are not the place to make up new terms for stuff that already exists.\n\nPS: Quick heads up: This made it to 4chan and various other troll places. Do not be surprised if there's suddenly an influx of .. weird comments.\n",
      "This is silly.\nNext we will remove all mention og objects because some people might feel objectified.\nOr classes, because of the poor people that feels they are being discriminated against.\n\nSure, I understand that the use of words can hurt, but words themselves carry no meaning outside of it's use. Saying a car is yellow and calling a person yellow are two very different statements. We seem to have a new round of book burnings going on today...\n",
      "Guys, just ignore this. These are bots who do this automatically for projects, which have been circling around on Github lately. There are also ones about feminism etc. The Linux kernel also had a PR like this a few months ago with a massive amount of responses on it.\n\nDon't feed the trolls, and just continue to use `master` and `slave`.\n",
      "Faith in humanity restored. Thank you guys. I just want to add, that this will also be bad for django itself, since almost every developer who is no aware of this will be confused, and trust me - he will be expecting that this is some bizarre django thing, not the well known pattern. Peace.\n",
      "Thanks so much Django for doing this thing! <3\n",
      "Excellent stuff. Thanks for doing that.\n",
      "I'm very glad for this change because as a PoC I felt very uncomfortable seeing and using this terminology in my code\n",
      "glad to see big projects taking this seriously\n",
      "Thanks Django for making this important change to be more welcoming and inclusive to more members of the tech community. <3 \n",
      "\"they've always been called that\" is a dumb reason to keep doing something, especially something that is hurtful or alienating. Kudos to Django for making this change! \n",
      "The use of the terms `master` and `slave` in relation to databases (and hardware configurations) has always made me uncomfortable. I think the terms `leader` and `follower` are much more appropriate, and are actually more expressive. :heart: to the Django team for making this change!\n",
      "These terms have been in use forever, but that doesn't make them good. Fixing them has to start somewhere; good for Django for taking the lead.\n",
      ":+1: \nGreat job @fcurella, thank you @alex & Django.\n",
      "Awesome change, thanks @fcurella! \"Primary/replica\" sounds much better.\n",
      "Good. The old terminology should be made obsolete.\n",
      "Thank you so much for making this change!\n",
      "Awesome change. Another reason to love the Django project :)\n"
    ],
    "num_comments": 22,
    "repository": "django/django",
    "diff_length": 23188,
    "code_diff": "diff --git a/docs/ref/settings.txt b/docs/ref/settings.txt\nindex 9a0dbd686d1c..7fa7aba64231 100644\n--- a/docs/ref/settings.txt\n+++ b/docs/ref/settings.txt\n@@ -649,10 +649,10 @@ Default: ``None``\n The alias of the database that this database should mirror during\n testing.\n \n-This setting exists to al"
  },
  {
    "pr_title": "Fixed #373 -- Added CompositePrimaryKey.",
    "pr_body": "# Trac ticket number\r\n\r\nticket-373\r\n\r\n# Branch description\r\n\r\nThis branch adds the `CompositePrimaryKey` field. If present, Django will create a composite primary key.\r\n\r\nPlease refer to the [docs](https://github.com/django/django/pull/18056/files#diff-cca8870fcaec19104d999f61553ba925c72e2eb19b4933068c4849f2ce58a6f6) for a more in-depth explanation.\r\n\r\n[Proposal](https://forum.djangoproject.com/t/gsoc-2024-proposal-django-orm-support-for-composite-primary-keys/29146)\r\n[Previous PR](https://githu",
    "pr_number": 18056,
    "comments": [
      "I was trying out this exciting branch and ran into this error when running a test:\r\n```\r\n<...>/lib/python3.12/site-packages/django/db/models/lookups.py:30: in __init__\r\n    self.rhs = self.get_prep_lookup()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = TupleIn(<django.db.models.fields.composite.Cols object at 0x107560980>, <django.db.models.sql.query.Query object at 0x1074e23f0>)\r\n\r\n    def get_prep_lookup(self):\r\n        if not isinstance(self.lhs, Cols):\r\n            raise ValueError(\r\n                \"The left-hand side of the 'in' lookup must be an instance of Cols\"\r\n            )\r\n        if not isinstance(self.rhs, Iterable):\r\n>           raise ValueError(\r\n                \"The right-hand side of the 'in' lookup must be an iterable\"\r\n            )\r\nE           ValueError: The right-hand side of the 'in' lookup must be an iterable\r\n```\r\n\r\nThe issue stems from the use of `isnull` like so:\r\n\r\n```\r\nMyModel.objects.filter(\r\n    type_override__severity__isnull=False\r\n).update(severity=\"high\")\r\n```\r\n\r\nCurious if anyone ran into this as well.\r\n\r\nEdited for traceback:\r\n\r\n```\r\n<...>\r\nlib/python3.12/site-packages/django/db/models/sql/compiler.py:2080: in pre_sql_setup\r\n    self.query.add_filter(\"pk__in\", query)\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1601: in add_filter\r\n    self.add_q(Q((filter_lhs, filter_rhs)))\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1617: in add_q\r\n    clause, _ = self._add_q(q_object, self.used_aliases)\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1649: in _add_q\r\n    child_clause, needed_inner = self.build_filter(\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1563: in build_filter\r\n    condition = self.build_lookup(lookups, col, value)\r\nlib/python3.12/site-packages/django/db/models/sql/query.py:1393: in build_lookup\r\n    lookup = lookup_class(lhs, rhs)\r\nlib/python3.12/site-packages/django/db/models/lookups.py:30: in __init__\r\n    self.rhs = self.get_prep_lookup()\r\n```\r\n\r\nSo, this is part of `SQLUpdateCompiler` and is coming from the `update` code path.",
      "Thanks for testing and reporting the issue @grjones! Indeed, I forgot to handle this use case. I'll look into it this week.",
      "@grjones, FYI I pushed the fix",
      "> @grjones, FYI I pushed the fix\r\n\r\nNice! I hope this gets merged in soon. Your branch has been working great for me.",
      "I may have found one other small issue. When adding a regular `primary_key=True` on a single field, a unique constraint is added. But when using this branch, it becomes an `IntegrityError` instead. Adding a `UniqueConstraint` on the composite fields is a work-a-round but ideally would be captured in this PR. Imo, this PR is sooooo close. I'm excited for it to be merged in.",
      "@grjones , thanks, I appreciate the feedback, I'll look into it. If a model defines `Meta.primary_key`, defining `primary_key=True` on a field should not be possible - could you give me a code example so I know how to reproduce the issue? I didn't know Django added unique constraints to primary keys, I'll check, but isn't that redundant?",
      "> @grjones , thanks, I appreciate the feedback, I'll look into it. If a model defines `Meta.primary_key`, defining `primary_key=True` on a field should not be possible - could you give me a code example so I know how to reproduce the issue? I didn't know Django added unique constraints to primary keys, I'll check, but isn't that redundant?\r\n\r\nI'll see if I can give you a solid failing test. My \"unique constraint\" phrasing might not be exactly right. But ultimately, I believe Django queries the DB first to see if the new object's PK already exists and throws a validation error. The composite key logic doesn't seem to be doing that and so an unhandled IntegrityError is raised instead.",
      "@grjones , sorry for the late reply, I've been busy last week. Could you give me more specifics? What's the error message you expect?",
      "> @grjones , sorry for the late reply, I've been busy last week. Could you give me more specifics? What's the error message you expect?\r\n\r\nActually, I think it's mostly ok. I was using [Django Spanner](https://github.com/googleapis/python-spanner-django) and it's just not quite working with composite keys and will need to be fixed there. I wrote this and it passed. It probably shouldn't say `Id` though?\r\n\r\n```\r\nfrom django.core.exceptions import ValidationError\r\nfrom django.test import TestCase\r\n\r\nfrom .models import Tenant, User\r\n\r\n\r\nclass CompositePKCleanTests(TestCase):\r\n    \"\"\"\r\n    Test the .clean() method of composite_pk models.\r\n    \"\"\"\r\n\r\n    @classmethod\r\n    def setUpTestData(cls):\r\n        cls.tenant = Tenant.objects.create()\r\n\r\n    def test_validation_error_is_raised_when_pk_already_exists(self):\r\n        test_cases = [\r\n            {\"tenant\": self.tenant, \"id\": 2412, \"email\": \"user2412@example.com\"},\r\n            {\"tenant_id\": self.tenant.id, \"id\": 5316, \"email\": \"user5316@example.com\"},\r\n            {\"pk\": (self.tenant.id, 7424), \"email\": \"user7424@example.com\"},\r\n        ]\r\n        expected = \"{'id': ['User with this Id already exists.']}\"\r\n        for fields in test_cases:\r\n            User.objects.create(**fields)\r\n            with self.assertRaisesMessage(ValidationError, expected):\r\n                User(**fields).clean()\r\n```",
      "Thank you so much for taking the time to review my changes @LilyFoote !\r\nI have two questions:\r\n\r\n1. If `Meta.primary_key` is defined, this PR will automatically add a composite field called `primary_key` to the model. What do you think about this approach? I felt like it was easier to handle the composite primary keys this way as we can run checks against the meta class instead of traversing the model's fields for a composite field.\r\n2. I wrote a lot of tests testing the underlying queries made by the ORM. It makes a lot of sense to me, but I haven't seen this type of tests that much in the Django source code - do these tests look okay to you?",
      " \r\n> If `Meta.primary_key` is defined, this PR will automatically add a composite field called `primary_key` to the model. What do you think about this approach?\r\n\r\nI don't feel strongly that this is better or worse than another option here, so happy to go with what you think is best.\r\n\r\n> I wrote a lot of tests testing the underlying queries made by the ORM. It makes a lot of sense to me, but I haven't seen this type of tests that much in the Django source code - do these tests look okay to you?\r\n\r\nI like your tests quite a bit - they're pretty readable and comprehensive. The main issue I have with them is that they're written for specific databases instead of for generic database features. Where possible Django strongly prefers to test based on features because then the tests apply to as many databases as possible (including third party database libraries). I think the asserts of the actual SQL might be a bit tricky to adapt though, so we might need a different way to check what they're checking.\r\n\r\nAlso, after I reviewed yesterday, I thought of some more things:\r\n\r\n* We should add migrations tests to make sure that adding/removing `Meta.primary_key` works correctly and that removing a field that's part of a primary key also does something appropriate.\r\n* We might want tests for composite keys in forms and the admin. Maybe there's other areas too that we need to check the interactions.",
      "Thanks @charettes !\r\n\r\n> Something that came through my mind while reviewing is that we likely want a plan to eventually deprecate `Options.pk` in favor of `Options.primary_key`?\r\n\r\nI'm not sure what you mean by that, I don't think we can, because `Options.pk` refers to the field, while `Options.primary_key` is the list of field names.",
      "So as far as I understand, at the moment `MultiColSource` is used by Django internally to represent `JOIN`s on multiple fields - that's why it has a `sources` field.\r\n\r\nI'm not sure it's the right decision to reuse this for composite fields, which on the other hand don't need `sources`, it just needs to represent a list of `Col`s as an expression.\r\n\r\nLet me know what you think!",
      "> I'm not sure what you mean by that, I don't think we can, because Options.pk refers to the field, while Options.primary_key is the list of field names.\r\n\r\nYou're completely right. In this case is `pk` set to `CompositePrimaryKey` when `Meta.primary_key` is defined and is `primary_key` set when a non-composite primary is used as well?",
      "> > I'm not sure what you mean by that, I don't think we can, because Options.pk refers to the field, while Options.primary_key is the list of field names.\r\n> \r\n> You're completely right. In this case is `pk` set to `CompositePrimaryKey` when `Meta.primary_key` is defined and is `primary_key` set when a non-composite primary is used as well?\r\n\r\nIt would not be set, if it's a regular primary key, `Meta.primary_key` is `None`.",
      "Hey @csirmazbendeguz, thank you for the amazing work out there! I was trying to test this branch on my local with SQLite and realised a few things:\r\n\r\n1. If you run `makemigrations` for a model with a `CompositePrimaryKey`, the resulting migration file has erroneous imports. To fix this, I believe we need to add `django.db.models.fields.composite` path to the `if...elif` block [here](https://github.com/django/django/blob/main/django/db/models/fields/__init__.py#L645).\r\n2. Assume that I have the following models:\r\n\r\n    ```py\r\n    class Author(models.Model):\r\n    name = models.CharField(max_length=100)\r\n\r\n    class Book(models.Model):\r\n        id = models.CompositePrimaryKey(\"author\", \"title\")\r\n        author = models.ForeignKey(Author, on_delete=models.CASCADE, related_name=\"books\")\r\n        title = models.CharField(max_length=255)\r\n    ```\r\n\r\n    With the current implementation, following test fails:\r\n    ```py\r\n    class TestCompositeFks(TestCase):\r\n        def test_composite_fks(self):\r\n            author = Author.objects.create(name=\"Author\")\r\n            book = Book.objects.create(author=author, title=\"Title\")\r\n            list(Author.objects.filter(books__in=[book])) == book\r\n    ```\r\n    with an `OperationalError`, caused by a syntax error. Executed SQL is as following:\r\n    ```SQL\r\n    SELECT\r\n        \"books_author\".\"id\",\r\n        \"books_author\".\"name\"\r\n    FROM\r\n        \"books_author\"\r\n        INNER JOIN \"books_book\" ON (\"books_author\".\"id\" = \"books_book\".\"author_id\")\r\n    WHERE\r\n        \"books_book\".\"author_id\", \"books_book\".\"title\" IN ((1, 'Title'))\r\n    ```\r\n    because LHS in WHERE clause should have been wrapped with parantheses like this:\r\n    ```SQL\r\n    ...\r\n    WHERE\r\n        (\"books_book\".\"author_id\", \"books_book\".\"title\") IN ((1, 'Title'))\r\n    ```\r\n    Unfortunately I didn't have a time to deep-dive to this.\r\n3. Not a big issue but my code editor (VSCode) does not recognize `models.CompositePrimaryKey`, although the import is working fine. This is probably related with Pylance or something that VSCode uses to recognize fields under `models` module.\r\n\r\nAgain thanks for this amazing initiative! üöÄ ",
      "Thanks a lot for the review @omerfarukabaci ! I'll take a look",
      "> Author.objects.filter(books__in=[book])\r\n\r\n@omerfarukabaci , I pushed the changes to support this, but note that filtering on reverse relations is one of those \"gotchas\" in Django, it may not produce the results you expect.\r\n\r\n_EDIT: I mean it might return duplicates, you probably already know this, I'm just mentioning it just in case._",
      "> If you run makemigrations for a model with a CompositePrimaryKey, the resulting migration file has erroneous imports\r\n\r\nYes, I recently changed the API to `CompositePrimaryKey`, the migrations are not 100% yet. I'm working on sorting them out.\r\nI pushed the fix for the issue you mentioned, thanks üëç ",
      "@csirmazbendeguz Thanks for your answers, now the above issues seem like fixed, created migration is correct and reverse relation lookup is working as expected. Thank you! üöÄ\r\n\r\nWhile I was testing it further with the exact [same models](https://github.com/django/django/pull/18056#issuecomment-2158820017), I realized another issue:\r\n\r\n```py\r\nclass TestCompositeFks(TestCase):\r\n    def test_composite_fks(self):\r\n        author = Author.objects.create(name=\"Author\")\r\n        Book.objects.create(author=author, title=\"Title\")\r\n        author = Author.objects.annotate(book_count=Count(\"books\")).get()\r\n        assert author.book_count == 1\r\n```\r\n\r\nThis test fails with the following error:\r\n\r\n```\r\ndjango.db.utils.OperationalError: wrong number of arguments to function COUNT()\r\n```\r\n\r\nExecuted SQL is as following:\r\n\r\n```SQL\r\nSELECT\r\n    \"books_author\".\"id\",\r\n    \"books_author\".\"name\",\r\n    COUNT(\"books_book\".\"author_id\", \"books_book\".\"title\") AS \"book_count\"\r\nFROM\r\n    \"books_author\"\r\n    LEFT OUTER JOIN \"books_book\" ON (\"books_author\".\"id\" = \"books_book\".\"author_id\")\r\nGROUP BY\r\n    \"books_author\".\"id\",\r\n    \"books_author\".\"name\"\r\n```\r\n\r\nIf we could change the parameter we pass to the `COUNT` function to a concatenation as below:\r\n\r\n```SQL\r\nCOUNT(\"books_book\".\"author_id\" || '-' || \"books_book\".\"title\")\r\n```\r\n\r\nit should work fine (if I am not missing something), with the exception that for some databases we need to use `CONCAT` function instead of `||` operator, which might be resolved using the existing `db.models.functions.Concat` function.\r\n\r\nNote: I am not sure if concatenation works between every data type that is allowed to be a primary key, although this could be considered as an edge case.",
      "Thanks @omerfarukabaci , these bug reports are very helpful. Yes, I haven't considered annotations with multi-column pks. I'll look into this.",
      "Thanks @LilyFoote , @sarahboyce for the meeting.\r\nNotes:\r\n1. At the moment, `Count(\"books\")` is not too easy to fix. It could be written as `Count(\"books__author_id\")` and it would work. If we cannot fix it, we should document it. Maybe we could use `*` instead of `pk` for counting?\r\n2. The content types framework will not work with composite pks. This includes everything that depends on the content types framework, e.g. the `contrib.auth` module too.",
      "I squsahed all commits and rebased to latest main branch.",
      "Idea: how about calling the field `PrimaryKey` instead of `CompositePrimaryKey`?",
      "@omerfarukabaci , I thought about the issue of `Count(\"books\")`.\r\n\r\nMy conclusion is we can't support this.\r\n\r\nI don't think concatenating is a good solution. The only way we could support this is if we could get Django to count this with `*` instead of the primary key.\r\n\r\nThis is an edge case that is only needed for `Count` though, and it's not as simple to implement as it is to explain.\r\n\r\nI added a section to the docs about this. This is a case of using a database function with a composite primary key directly, which cannot be expected to work in general.\r\n\r\nIn your case, `Count(\"books__author_id\")` would do the trick instead.",
      "Regarding the issue raised by @sarahboyce last week...\r\n\r\nI think it is okay to merge this without support for generic relations. I added a section to the docs about this not being supported for now.\r\n\r\nThe only impact is some third-party packages using generic relations won't work with composite primary keys (e.g. `django-guardian`).\r\n\r\nLet's have a separate discussion on how to support this. I lean towards storing composite primary keys serialized as JSON in a single CharField.",
      "Btw, semantically it would be nice if it were possible to write:\r\n```python \r\nclass User(models.Model):\r\n    pk = models.CompositePrimaryKey(\"tenant_id\", \"id\")\r\n    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE)\r\n    id = models.IntegerField()\r\n```\r\n\r\nie to let `CompositePrimaryKey` replace the automatically generated `pk`. Would that be possible?",
      "> Btw, semantically it would be nice if it were possible to write:\r\n> \r\n> ```python\r\n> class User(models.Model):\r\n>     pk = models.CompositePrimaryKey(\"tenant_id\", \"id\")\r\n>     tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE)\r\n>     id = models.IntegerField()\r\n> ```\r\n> \r\n> ie to let `CompositePrimaryKey` replace the automatically generated `pk`. Would that be possible?\r\n\r\n@apollo13 , good point! It also came up when we were discussing this with @LilyFoote and @charettes . It seems like a natural thing to do, so it's worth a discussion. Here are a couple ideas that make sense to me:\r\n\r\n1. `pk` at the moment is reserved, users can't add a field named `pk`. We could remove this restriction.\r\n2. If `pk` is defined, it should always set `primary_key=True`.\r\n3. If `pk` is not defined, it should still refer to the `primary_key=True` field (e.g. `id` field). This is required for backwards-compatibility.\r\n4. If `pk` is defined, and it's an `IntegerField`, then a field called `pk` should be created in the database (same as any field, e.g. `id`).\r\n5. If `pk` is defined, and it's a `CompositePrimaryKey`, then a field called `pk` shouldn't be created in the database (same as any field, e.g. `primary_key`).\r\n\r\nMy only issue with this is, it adds extra complexity to how `pk` works. In this case, `pk` can be both a reference to the primary key field, or the primary key field itself.\r\n\r\nSo I'm not sure if it's worth doing this. It doesn't feel like an elegant or consistent solution to me.\r\n\r\n---\r\n\r\nThe other approach @charettes and @LilyFoote mentioned is to always have `pk` be a `CompositePrimaryKey` (could be renamed to `PrimaryKey`):\r\n\r\n1. `pk` cannot be defined explicitly.\r\n2. `CompositePrimaryKey` cannot be used explicitly.\r\n3. `pk` is _always_ added to the model in the background, and it's _always_ an instance of `CompositePrimaryKey`.\r\n4. Consequently, `pk` will cease to be a reference to another field, it will always be a field itself.\r\n5. If field `x` defines `primary_key=True`, `pk` is `CompositePrimaryKey(\"x\")`. `obj.pk` returns the value of `x` for backwards-compatibility (instead of a tuple).\r\n6. If `Meta.primary_key` option is `(\"a\", \"b\", \"c\")`, `pk` is `CompositePrimaryKey(\"a\", \"b\", \"c\")`. `obj.pk` returns a tuple.\r\n7. If `Meta.primary_key` is not set, it could be set to `(\"x\",)` automatically.\r\n\r\nThis is quite an invasive change. It would mean all existing models get a new field called `pk`.\r\n`meta.pk` would return a different field. Instead of `IntegerField`, it would return `CompositePrimaryKey`. Is breaking backwards-compatibility okay here?\r\n\r\nI don't have anything against it other than that. It does feel more intuitive. If the community wants this, I could fork this branch and open another PR.",
      "Today's meeting with @LilyFoote and @charettes :\r\n* enable setting pk to CompositePrimaryKey explicitly. pk is a well-known Django keyword, so it makes sense to reuse it here, even if it somewhat complicates things.\r\n* pk can only be set to CompositePrimaryKey.\r\n* CompositePrimaryKey can't be set to anything else but pk.",
      "@apollo13 , I discussed your suggestion with @LilyFoote and @charettes and they agreed. I pushed the changes. The only way to define a composite pk is with the `pk` field name now."
    ],
    "num_comments": 30,
    "repository": "django/django",
    "diff_length": 140643,
    "code_diff": "diff --git a/django/contrib/admin/sites.py b/django/contrib/admin/sites.py\nindex 3399bd87b85a..201f28ef3744 100644\n--- a/django/contrib/admin/sites.py\n+++ b/django/contrib/admin/sites.py\n@@ -113,6 +113,11 @@ def register(self, model_or_iterable, admin_class=None, **options):\n                     \"Th"
  },
  {
    "pr_title": "[Soc2014] Official meta API",
    "pr_body": "",
    "pr_number": 3114,
    "comments": [
      "Here are some docs edits: http://dpaste.com/1X0TZB4\nLet me know if any changes look questionable.\n",
      "Hi @timgraham thank you so much for the doc edits! I will have a look at them now.\n",
      "Daniel, I'd recommend to finish the docs and then I'll do another review. After that, we can put out a call for anyone else who wants to do a final review. We still have time before 1.8 feature freeze to make changes as needed after merge, but this way you won't have to keep resolving conflicts due to changes in master.\n",
      "Sounds great. I don't have access to a computer this week but will re-write\nall the docs as soon as I come back home.\n\nOn Wednesday, September 17, 2014, Tim Graham notifications@github.com\nwrote:\n\n> Daniel, I'd recommend to finish the docs and then I'll do another review.\n> After that, we can put out a call for anyone else who wants to do a final\n> review. We still have time before 1.8 feature freeze to make changes as\n> needed after merge, but this way you won't have to keep resolving conflicts\n> due to changes in master.\n> \n> ‚Äî\n> Reply to this email directly or view it on GitHub\n> https://github.com/django/django/pull/3114#issuecomment-55829765.\n\n## \n\n---\n\nPirosB3\n\nhttps://github.com/PirosB3\n",
      "@timgraham @freakboy3742 I have done pull from master, and now I am finishing up the docs. I expect this to be all complete in the next 2 days. Then we can do a final review \n",
      "I ran into this issue using the code from the tutorial:\n\n```\n$ python manage.py shell\nTraceback (most recent call last):\n  File \"manage.py\", line 10, in <module>\n    execute_from_command_line(sys.argv)\n  File \"/home/tim/code/django/django/core/management/__init__.py\", line 336, in execute_from_command_line\n    utility.execute()\n  File \"/home/tim/code/django/django/core/management/__init__.py\", line 310, in execute\n    django.setup()\n  File \"/home/tim/code/django/django/__init__.py\", line 23, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File \"/home/tim/code/django/django/apps/registry.py\", line 115, in populate\n    app_config.ready()\n  File \"/home/tim/code/django/django/contrib/admin/apps.py\", line 22, in ready\n    self.module.autodiscover()\n  File \"/home/tim/code/django/django/contrib/admin/__init__.py\", line 24, in autodiscover\n    autodiscover_modules('admin', register_to=site)\n  File \"/home/tim/code/django/django/utils/module_loading.py\", line 73, in autodiscover_modules\n    import_module('%s.%s' % (app_config.name, module_to_search))\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/home/tim/code/django/django/contrib/auth/admin.py\", line 182, in <module>\n    admin.site.register(Group, GroupAdmin)\n  File \"/home/tim/code/django/django/contrib/admin/sites.py\", line 101, in register\n    admin_class.check(model)\n  File \"/home/tim/code/django/django/contrib/admin/options.py\", line 149, in check\n    return cls.checks_class().check(cls, model, **kwargs)\n  File \"/home/tim/code/django/django/contrib/admin/checks.py\", line 492, in check\n    errors = super(ModelAdminChecks, self).check(cls, model=model, **kwargs)\n  File \"/home/tim/code/django/django/contrib/admin/checks.py\", line 32, in check\n    errors.extend(self._check_filter_horizontal(cls, model))\n  File \"/home/tim/code/django/django/contrib/admin/checks.py\", line 245, in _check_filter_horizontal\n    for index, field_name in enumerate(cls.filter_horizontal)\n  File \"/home/tim/code/django/django/contrib/admin/checks.py\", line 253, in _check_filter_item\n    field = model._meta.get_field(field_name)\n  File \"/home/tim/code/django/django/db/models/options.py\", line 434, in get_field\n    \"The Apps registry is still not ready, this means get_field() is not able \"\ndjango.core.exceptions.AppRegistryNotReady: The Apps registry is still not ready, this means get_field() is not able to find related objects that point to this model.\n```\n",
      "There is still some usage of `get_field_by_name()` and other deprecated APIs in the tests. Run the tests with `python -Wall runtests.py` and ensure there are no errors.\n\nThere are also a fair number of flake8 errors -- some appear not related to your changes, but rather like you haven't merged in some commits from master. I think you could probably rebase and squash most of the commits now.\n",
      "@timgraham \nRE \"I ran into this issue using the code from the tutorial: ....\"\nTotally right, I added a fix for it, currently running unit tests. It looks like some admin checks are happening prior to the apps registry being ready. This should never happen actually, so I added a fix for it.\nI'll let you know once all tests pass with -Wall\n",
      "@timgraham I have just pushed the last changes with your comments fixed.\n",
      "There are still many flake8 errors on your branch aren't there? This is what I see:\n\n```\n./django/db/models/manager.py:6:1: F401 'FieldDoesNotExist' imported but unused\n./django/db/models/options.py:13:1: F401 'Field' imported but unused\n./django/db/models/options.py:500:17: E126 continuation line over-indented for hanging indent\n./django/db/models/base.py:1420:9: F401 'FieldDoesNotExist' imported but unused\n./django/db/models/fields/__init__.py:45:1: E302 expected 2 blank lines, found 1\n./django/contrib/contenttypes/fields.py:41:15: W291 trailing whitespace\n./django/contrib/admin/utils.py:462:1: E302 expected 2 blank lines, found 1\n./django/contrib/admin/utils.py:481:1: E302 expected 2 blank lines, found 1\n./tests/prefetch_related/tests.py:723:45: E127 continuation line over-indented for visual indent\n./tests/apps/tests.py:18:1: F401 'AbstractPerson' imported but unused\n./tests/apps/tests.py:18:1: F401 'BasePerson' imported but unused\n./tests/apps/tests.py:18:1: F401 'Relation' imported but unused\n./tests/apps/tests.py:18:1: F401 'new_apps_2' imported but unused\n./tests/test_client_regress/tests.py:997:31: E127 continuation line over-indented for visual indent\n./tests/introspection/tests.py:133:18: E127 continuation line over-indented for visual indent\n```\n",
      "@aaugustin - do you know if admin checks happening prior to the app registry being ready is expected? see https://github.com/django/django/pull/3114#issuecomment-57612835. Seems suspicious to me and the workaround adds more code which isn't ideal: 3a6a7131e020b49ae3c030adddf320b32d43f3a8.\n",
      "buildbot, retest this please.\n",
      "@freakboy3742 @timgraham Latest fixes (documentation + style + implementation) have been pushed. I'm ready for a further review\n",
      "buildbot, retest this please.\n",
      "@collinanderson did the last fixes we spoke about, can you give me another rev?\n",
      "@PirosB3 It looks like most of comments on the PR have now been addressed.\n",
      "Thanks @collinanderson \nLooking forward to hear comments from @timgraham @freakboy3742\n",
      "@timgraham To me it looks like the admin checks should be triggered from `AdminAppConfig.ready()`.\n",
      "# Further API change\n\n### Properties changes\n- many_to_many becomes _many_to_many and is only used internally, as there should be no more external distinction between m2m and forward fields\n- fields, concrete_fields, local_concrete_fields become all internal, (with a _ before and not documented) , as there should be no more external distinction between m2m and forward fields\n- related_objects become reverse_fields, in order to keep the same name convention\n- we add another property called \"forward_fields\"\n- make get_fields() internal, but we don't change the endpoint name for legacy reasons (there was already a get_fields())\n\n### Final _meta API\n- field_names => [\"name\", \"surname\", ...]\n- get_field(field_name) => FieldInstance\n- forward_fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n- reverse_fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n\n### Final internal _meta API\n- _fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n- _concrete_fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n- _local_concrete_fields => [FieldInstance, FieldInstance, FieldInstance, .. ]\n- _many_to_many => [FieldInstance, FieldInstance, FieldInstance, .. ]\n",
      "Assuming we document the `FieldDoesNotExist` exception as part of this (which will be necessary) this PR should also resolve https://code.djangoproject.com/ticket/9104 one way or the other.\n",
      "@tomchristie super correct! I have not documented FieldDoesNotExist yet. Waiting till we sort out the last field flags issues and the _meta API. Once I get the final blessing on these, I will finish the documentation part.\nSaid this, isn't django.db.models.fields the correct place for this Exception?\n",
      "If we make it a public API, I am in favor of moving it to `core.exceptions` (and keeping backwards compatibility where it is now)\n",
      "@timgraham sounds good to me. We can move it there and then alias it back on db.models.fields\n",
      "\"We can move it there and then alias it back on db.models.fields\"\n\nYes, that sounds like the right thing to do.\n\nOptionally we _could_ have the `db.models.fields` version be pushed into the pending deprecation state, but I don't much mind either way on that.\n",
      "A quick question on API correctness:\n`opts.field_names` API can also return more than 1 name for each field, this usually happens with ForeignKeys, where fields can be fetched by property or property_id.\n\nThis is an example where `manager` is a ForeignKey: `{u'id', 'item', 'manager', u'manager_id', 'name'}`\n\nDo you think this is the correct way to go? or shall we exclude duplicates from `field_names`?\n",
      "Gut reaction: I'd certainly expect it to only return the canonical attribute names, and not the `_id` variants.\n\nSo long as the API gives enough information for users to be able to derive the \"_id\" style ones if needed then that would seem sufficient.\n",
      "@tomchristie interestingly Django also uses the *_id stuff internally. I suggest we keep the possibility of Django fetching fields by *_id using get_field(), but we remove duplicates in field_names\n",
      "@PirosB3 What's the hold-up in using `_meta.fields` as the main (and only) entry point? Is that backward compatibility because `fields` doesn't have \"fake\" fields like reverse relations?\n\nIf that's the case I think we have here a unique opportunity to get it right and it's easy enough to provide an upgrade path.\n"
    ],
    "num_comments": 28,
    "repository": "django/django",
    "diff_length": 248741,
    "code_diff": "diff --git a/django/apps/registry.py b/django/apps/registry.py\nindex fe53d965de10..68aa411d91f6 100644\n--- a/django/apps/registry.py\n+++ b/django/apps/registry.py\n@@ -337,7 +337,12 @@ def clear_cache(self):\n \n         This is mostly used in tests.\n         \"\"\"\n+        # Call expire cache on each mo"
  },
  {
    "pr_title": "Fixed #14370 -- Added select2 widget for related object fields in admin.",
    "pr_body": "Adds jQuery Select2 version 4 to support async select inputs\r\nincluding a search feature.\r\n\r\n**I split the PR in two commits, one is vendoring select2, one contains my code.**\r\n\r\n### Links & Discussions\r\n* [djangoproject#14370](https://code.djangoproject.com/ticket/14370)\r\n* https://groups.google.com/forum/#!topic/django-developers/tCNWnLP8jzM\r\n* https://groups.google.com/forum/#!topic/django-developers/Ip63Xqw01IA/discussion\r\n* https://groups.google.com/forum/#!topic/django-developers/jGgZngTq3",
    "pr_number": 6385,
    "comments": [
      "@jpic this is a first draft, I tested it manually, it seems fine\n@timgraham I only added tests for the new json view, I'll add the widget tests in the sprint tomorrow\n",
      "Nice !\n\nDid you try it on dynamically added formset rows ?\n",
      "@jpic no that can't work yet, but I'm on it today!\nI also got a lot of documentation ahead of me, but it really depends where this is going. I'm gonna talk to a couple of people here about it.\n",
      "@codingjoe we use DOMNodeInserted event for dynamically added selects: https://github.com/yourlabs/django-autocomplete-light/blob/master/src/dal/static/autocomplete_light/autocomplete.init.js#L25-L27\n\nWe also have a really small snippet for option renaming using the edit button which you might like: https://github.com/yourlabs/django-autocomplete-light/blob/master/src/dal_select2/static/autocomplete_light/select2.js#L100-L106\n",
      "I might be able to try test this out later this week. Seems to me we should give this a release or two in the real world before deprecating raw_id_fields. Like, make sure it's good enough in practice to replace raw_id_fields.\n",
      "Does `/foreignkey_json/` do permission checks? It seems to me it should require the same permissions as raw_id_fields (so if you're logged in to the admin, you can't just query that table unless you actually have permission, but maybe that's ok)\n\nEdit: Actually, maybe we just need to make sure that you have the change permission for the Model that has the foreign key. That way it follows the permissions of a normal ChoiceField.\n",
      "@collinanderson good point, that's what I thought. Lets see how it performs.\nRegarding the permissions, you are correct, the change permission would the right thing to check. I'll put it on my list, thanks!\n",
      "There is a small typo in the diff of docs/ref/models/fields.txt: you mean `admin` not `admon`.\n",
      "@codingjoe  first: Thank you for this great improvement. Next: It would be nice if the autocomplete would be easily re-usable outside the admin, too. Do you plan to support this?\n",
      "@guettli no, not really. We have `django-select2` and `django-autocomplete-light` for that.\nThe really tricky part is to know which queryset to server as a JSON. `django-select2` solves this by using the cache as a persistent storage shared by all application servers. In `django-autocomplete-light` you'll have to specify that explicitly.\n\nI don't see a way to get this into core. This should remain a admin only feature for now, just like the raw ID field.\n",
      "@codingjoe btw, at some point we'll need to eventually get django-developers mailing list approval for vendoring select2.\n\nAlso, you've been mentioning raw_id_fields. I'm also hoping we can eventually use select2 to replace filter_horizontal/filter_vertical for m2m's.\n",
      "@codingjoe you said:\n\n> The really tricky part is to know which queryset to server as a JSON\n\nYes, this is true, since the django admin interface needs a generic ajax server part.\n\nBut I still think it would be great to have a autocomplete component in django which can be used in django apps and the django admin.\n\nYou can make a BaseWidget available which gets subclassed once in the admin interface and once for the usage in custom apps.\n\nI like small systems and having select2 twice in my static directory gives me a bad feeling. Yes it works and does not hurt, but somehow I think \"less is more - avoid redundancy\".\n",
      "You don't need to have it twice, you can reuse it. For example, this public\nfacing-app reuses admin scripts:\nhttps://github.com/jonashaag/django-addanother/blob/master/django_addanother/widgets.py#L44-L52\n\nCorrect me if I'm wrong but the idea here is to first incorporate\nautocompletion in the admin first and then perhaps extract it into\ndjango.forms.\n",
      "@jpic  nice to hear, that you want it in django.forms, too.\n\nI just ask myself if this is the best order for the steps for the implementation:\n1. in admin\n2. then in django.forms.\n",
      "@guettli @jpic I'd like to end the discussion about a feature beyond Django admin at this point. Please submit another ticket for that, it's beyond the scope of the current ticket and seems to need a longer discussion.\n",
      "@collinanderson I'm not familiar with that process, can you help me out here?\n",
      "Yeah, I also agree adding generally to forms is a lot more work, and it's something we could maybe do down the road. I think the next set is to run this all by django-developer's mailing list. Though maybe we should wait til the vendoring/dependency discussion plays out first. https://groups.google.com/d/topic/django-developers/Ip63Xqw01IA/discussion \n",
      "@collinanderson we'll need to vendor select2 anyways, it can't be a dependency for the sake of offline development and I had to manually add a wrapper to support the jQuery's `noConflict`.\n[I posted into the mailing list.](https://groups.google.com/forum/#!topic/django-developers/tCNWnLP8jzM)\n",
      "I was able to try out a little and added some notes. Super cool. :)\n",
      "@kevin-brown thanks for the review, good the have some feedback form a select2 dev\n@collinanderson thanks to you too of course :)\nI'll try to get your these changes in this weekend and add some of the tests that I'm still missing.\n",
      "Ok, I fixed all the review notes and added support for M2M fields. That was actually a 1 liner :)\n",
      "Awesome. formsets/inlines are working for me, and m2m is also working well. This is so cool.\n\nIt looks like there's just one test failure that needs to be fixed?\n",
      "Also, I think there should be a check to be sure the User has access to change the model containing the field.\n",
      "@timgraham I'm slowly getting there. One question, I want to add system checks, as we have for `raw_id_fields`. Should I add new once or make the one for `raw_id_fields` check the `autocomplete_fields` as well?\nhttps://docs.djangoproject.com/en/1.9/ref/checks/#admin\n",
      "No string opinion but I guess I don't see much harm in reusing the existing error codes for the checks if they work similarly.\n",
      "Ok, I'm done with my own checklist. I need some serious reviews now to get going.\n\n@timgraham since I didn't no one was against vendoring Select2, I don't see anything holding this feature back. I'd like to start writing release notes, when would this be released?\n",
      "1.10 alpha is scheduled for May 16 so there's probably enough time to get this polished by then.\n",
      "Please add tests in `tests/admin_checks` for the new checks.\n",
      "A small bug: after using the pencil icon to open a pop and edit the related object, the representation of the object in the select box isn't updated after clicking the save of the popup.\n\nAlso I'm getting a `NoSuchElementException` failure for (admin_views.test_autocomplete_view.SeleniumTests.test_select `on both Firefox and Chrome.\n",
      "@timgraham they are not updated, because there isn't really a way to do that in select2 üòû \n"
    ],
    "num_comments": 30,
    "repository": "django/django",
    "diff_length": 68339,
    "code_diff": "diff --git a/django/contrib/admin/checks.py b/django/contrib/admin/checks.py\nindex 830a190ff0bd..a9398db7e7a3 100644\n--- a/django/contrib/admin/checks.py\n+++ b/django/contrib/admin/checks.py\n@@ -66,6 +66,7 @@ class BaseModelAdminChecks:\n \n     def check(self, admin_obj, **kwargs):\n         errors = "
  },
  {
    "pr_title": "Schema alteration",
    "pr_body": "To accompany the mailing list thread here: https://groups.google.com/forum/?fromgroups=#!topic/django-developers/esCFLLXwIOY\n",
    "pr_number": 376,
    "comments": [
      "Andrew - it's occurred to me that this may not address the situation when someone starts a project then switches to a swapped user model - I am **NOT** talking about the data migration fore user data (people are own for that) - but raising the issue of what, if anything, needs to be done when a model._meta.swapped goes from False to True from one migration to the next.\n\nNot really being familiar with the core approach here, I don't even know if anything needs to be done, just realizing that this was probably developed before _meta.swapped was introduced and pointing that out. cc @freakboy3742 \n",
      "@ptone - @andrewgodwin is at least peripherally aware of the problem; I've already raised a bug on South's tracker about integration with AUTH_USER_MODEL. \n\nFully swapping the User model is a bit of a nightmare; On the South ticket, I suggested that a perfectly acceptable first pass is to disallow this - i.e., initial sync records the AUTH_USER_MODEL in use, and future migrations won't allow it to be changed. This is what we suggest in the docs anyway, so I don't see any problem with enforcing it.\n",
      "Yes, Russ has filed this in the South tracker already and I've not been able to take a good look at it yet. I suspect the first pass will be that we just moan, though I'd like to do something more intelligent (spit out a skeleton migration for swapping the tables out and preserving data, and letting them finish it off, perhaps).\n",
      "I just created a migration and the file shows:\n\n```\n    dependencies = [(u'testb', '0001_initial')]\n```\n\n-- That doesn't work on 3.2, you might also want to import unicode_literals, depending on whether you want text or bytes for everything.\n\nEDIT:// The fields also have unicode markers from time to time:\n\n```\n            fields = [(u'id', models.AutoField(verbose_name=u'ID', serialize=False, auto_created=True, primary_key=True),), ('char', models.CharField(max_length=256),), ('fk', models.ForeignKey(to=u'testb.TestB', to_field=u'id'),)],\n```\n",
      "Is there any way to not get asked everytime if you want to enable migrations for an app? \n",
      "I get migrations which are getting unapplied without me asking for it:\n\n```\nflorian@apollo13:~/.virtualenvs/522125f0c8c708a8/migrationtest$ ./manage.py migrate\nOperations to perform:\n  Synchronize unmigrated apps: sessions, admin, messages, testc, auth, staticfiles, contenttypes\n  Apply all migrations: testa, testb\nSynchronizing apps without migrations:\n  Creating tables...\n    Creating table django_admin_log\n    Creating table auth_permission\n    Creating table auth_group_permissions\n    Creating table auth_group\n    Creating table auth_user_groups\n    Creating table auth_user_user_permissions\n    Creating table auth_user\n    Creating table django_content_type\n    Creating table django_session\n    Creating table testc_test\n  Installing custom SQL...\n  Installing indexes...\nInstalled 0 object(s) from 0 fixture(s)\nRunning migrations:\n  Applying testb.0001_initial... OK\n  Applying testa.0001_initial... OK\n  Applying testa.0002_auto... OK\n  Unapplying testa.0002_auto... OK\n  Unapplying testa.0001_initial... OK\n\nYou just installed Django's auth system, which means you don't have any superusers defined.\nWould you like to create one now? (yes/no): no\nflorian@apollo13:~/.virtualenvs/522125f0c8c708a8/migrationtest$ ./manage.py migrate\nOperations to perform:\n  Synchronize unmigrated apps: sessions, admin, messages, testc, auth, staticfiles, contenttypes\n  Apply all migrations: testa, testb\nSynchronizing apps without migrations:\n  Creating tables...\n  Installing custom SQL...\n  Installing indexes...\nInstalled 0 object(s) from 0 fixture(s)\nRunning migrations:\n  Applying testa.0001_initial... OK\n  Applying testa.0002_auto... OK\n  Unapplying testa.0002_auto... OK\n  Unapplying testa.0001_initial... OK\n```\n\nPing me in IRC if you need details.\n",
      "@apollo13 That's weird, I'll look into it.\n\nAs for the asking every time, that's something that still needs to be resolved (current solution works, but is REALLY ANNOYING). The proposal I like most is having them enabled in the new app template, and then having this just assume they're disabled unless migrations directory is present.\n",
      "@andrewgodwin more details (from irc):\n\n```\n<apollo13> andrewgodwin: are you around? testa has a fk to testb, testb changes are \"stable\" so to say\n<apollo13> andrewgodwin: also, since django itself asks \"Would you like to create one now? (yes/no): no\"  -- we might wanna change [y/n] to stay consistent? [sry]\n```\n\nand migrating manually works:\n\n```\nflorian@apollo13:~/.virtualenvs/522125f0c8c708a8/migrationtest$ ./manage.py migrate testa 0001_initial\nOperations to perform:\n  Target specific migration: 0001_initial, from testa\nRunning migrations:\n  Applying testa.0001_initial... OK\n```\n",
      "Trying to migrate apps not in INSTALLED_APPS shows a confusing error:\n\n```\nflorian@apollo13:~/.virtualenvs/522125f0c8c708a8/migrationtest$ ./manage.py migrate fsdgdshdsfdsgdfshgds 0001_initial\nCommandError: App 'fsdgdshdsfdsgdfshgds' does not have migrations (you cannot selectively sync unmigrated apps)\n```\n\nSomething like \"this app doesn't exist\" would be better.\n",
      "Almost none of the new files import unicode_literals (and many of the files have string literals).\n",
      "I see different failures from what you described in the ML, and even more failures if I run just the tests for migrations and schema; I'm pretty sure several tests here depend on operations performed in other tests. This probably applies mostly to database systems which do not support transactional DDL, like Oracle and MySQL.\n",
      ":sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: :sparkles: \n",
      "![Thumbs up!](http://georgebrock.com/images/thumbsup.gif)\n",
      "[![LGTM](http://i.imgur.com/azxpEtW.jpg)](http://www.lgtm.in/i/Z2e)\n"
    ],
    "num_comments": 14,
    "repository": "django/django",
    "diff_length": 373926,
    "code_diff": "diff --git a/django/conf/global_settings.py b/django/conf/global_settings.py\nindex ab3cdab59eb7..6dd25e18f9bb 100644\n--- a/django/conf/global_settings.py\n+++ b/django/conf/global_settings.py\n@@ -609,3 +609,10 @@\n     'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n #    'django.contrib.st"
  },
  {
    "pr_title": "Fixed #12990 -- Added JSONField model field.",
    "pr_body": "### This pull request is closed. Please look at #12392 instead.\r\n\r\n---\r\n\r\nTicket [#12990](https://code.djangoproject.com/ticket/12990), as part of the [Google Summer of Code](https://g.co/gsoc) program.\r\n\r\nSome points:\r\n- Currently supports storing and retrieving any valid JSON value (boolean, integer, float, string, object, array) on all supported database backends (SQLite, PostgreSQL, MySQL, MariaDB, Oracle).\r\n  Note: Oracle only supports JSON object and array if `IS JSON` constraint is enable",
    "pr_number": 11452,
    "comments": [
      "Any clue on which version of MariaDB is used on [djangoci.com](//djangoci.com)? The `json` data type is introduced in MariaDB 10.2.7.",
      "> Any clue on which version of MariaDB is used on [djangoci.com](//djangoci.com)? The `json` data type is introduced in MariaDB 10.2.7.\r\n\r\ndjangoci uses MariaDB 10.1.40. I can bump MariaDB version in the next few days. We need to remember that [Django 3.0 supports](https://docs.djangoproject.com/en/dev/ref/databases/#mariadb-notes) MariaDB 10.1 and higher so a new db feature is required e.g. `has_json_field`.",
      "> > Any clue on which version of MariaDB is used on [djangoci.com](//djangoci.com)? The `json` data type is introduced in MariaDB 10.2.7.\r\n> \r\n> djangoci uses MariaDB 10.1.40. I can bump MariaDB version in the next few days. We need to remember that [Django 3.0 supports](https://docs.djangoproject.com/en/dev/ref/databases/#mariadb-notes) MariaDB 10.1 and higher so a new db feature is required e.g. `has_json_field`.\r\n\r\nI see. I'll try to add it later.",
      "> djangoci uses MariaDB 10.1.40. I can bump MariaDB version in the next few days. We need to remember that Django 3.0 supports MariaDB 10.1 and higher so a new db feature is required e.g. has_json_field.\r\n\r\nHow about `supports_json` ? It's not really a separate data type on SQLite or MariaDB.",
      "> > djangoci uses MariaDB 10.1.40. I can bump MariaDB version in the next few days. We need to remember that Django 3.0 supports MariaDB 10.1 and higher so a new db feature is required e.g. has_json_field.\r\n> \r\n> How about `supports_json` ? It's not really a separate data type on SQLite or MariaDB.\r\n\r\nIt is also not a separate field on Oracle, but a feature flag will determine if backend has JSON field or not, so ... :thinking:   ",
      "@laymonage I updated MariaDB to 10.2.24 on Jenkins.",
      "> @laymonage I updated MariaDB to 10.2.24 on Jenkins.\r\n\r\nThanks! As expected, the tests have passed now.\r\n\r\nI have added a `supports_json` feature (can be renamed if desired). Not sure if I should check the SQLite version, though. I don't think there's any way to check if the JSON1 extension is enabled (maybe we could try to do `SELECT json('\"test\"')`, but that's a bit hack-ish).\r\nThe JSON1 extension was introduced with the release of SQLite 3.9.0. However, since it's a loadable extension, it *might* work if it's loaded on older SQLite version(s). I haven't tried.\r\n\r\nAlso, I'm not sure if I should use `check` and extend the list returned by that method instead of raising a `NotSupportedError`. I've seen both examples in the existing codebase.",
      "I've added a form field. It's pretty much the one in `contrib.postgres`, but I omitted the field value in the invalid JSON error message and changed the tests accordingly.",
      "> I have added a supports_json feature (can be renamed if desired). Not sure if I should check the SQLite version, though. I don't think there's any way to check if the JSON1 extension is enabled (maybe we could try to do SELECT json('\"test\"'), but that's a bit hack-ish).\r\n> The JSON1 extension was introduced with the release of SQLite 3.9.0. However, since it's a loadable extension, it might work if it's loaded on older SQLite version(s). I haven't tried.\r\n\r\nI think trying the `json` function and catching the error isn't so bad, as long as it won't break any transactions.\r\n\r\nThe other option is to use `PRAGMA compile_options` and check if the extension is in there, however I am not sure if it's possible to load the `json1` extension without it being built in at compile time...\r\n\r\n```\r\nsqlite> PRAGMA compile_options;\r\nBUG_COMPATIBLE_20160819\r\nCOMPILER=clang-10.0.1\r\nDEFAULT_CACHE_SIZE=2000\r\nDEFAULT_CKPTFULLFSYNC\r\nDEFAULT_JOURNAL_SIZE_LIMIT=32768\r\nDEFAULT_PAGE_SIZE=4096\r\nDEFAULT_SYNCHRONOUS=2\r\nDEFAULT_WAL_SYNCHRONOUS=1\r\nENABLE_API_ARMOR\r\nENABLE_COLUMN_METADATA\r\nENABLE_DBSTAT_VTAB\r\nENABLE_FTS3\r\nENABLE_FTS3_PARENTHESIS\r\nENABLE_FTS3_TOKENIZER\r\nENABLE_FTS4\r\nENABLE_FTS5\r\nENABLE_JSON1\r\nENABLE_LOCKING_STYLE=1\r\nENABLE_PREUPDATE_HOOK\r\nENABLE_RTREE\r\nENABLE_SESSION\r\nENABLE_SNAPSHOT\r\nENABLE_SQLLOG\r\nENABLE_UNKNOWN_SQL_FUNCTION\r\nENABLE_UPDATE_DELETE_LIMIT\r\nHAVE_ISNAN\r\nMAX_LENGTH=2147483645\r\nMAX_MMAP_SIZE=1073741824\r\nMAX_VARIABLE_NUMBER=500000\r\nOMIT_AUTORESET\r\nOMIT_LOAD_EXTENSION\r\nSTMTJRNL_SPILL=131072\r\nTHREADSAFE=2\r\nUSE_URI\r\n```",
      "> I think trying the json function and catching the error isn't so bad, as long as it won't break any transactions.\r\n\r\nI'm not sure where and how to properly put it in Django's source code, though.\r\n\r\n---\r\n\r\nI tried compiling SQLite 3.28.0 without JSON1, compiling JSON1 separately, and loading it with the `.load` command.\r\n`SELECT JSON('\"test\"');` works, but `ENABLE_JSON1` doesn't show up with `PRAGMA compile_options` (which is correct since I didn't build JSON1 along with SQLite).\r\n\r\nOn the other hand, I also tried loading JSON1 (compiled from SQLite 3.28.0 source code) on SQLite 3.8.7.1 (what's available on Debian Jessie). This SQLite version supports extension loading, but I got a segmentation fault when I tried to load JSON1. So, I guess it needs SQLite 3.9.0 and up.\r\n\r\nBy the way... JSON1 is also enabled by default if SQLite is compiled using `make` with the amalgamation and the given configurations.",
      "> I'm not sure where and how to properly put it in Django's source code, though.\r\n\r\nYou can use a `@cached_property` for the feature, for example https://github.com/django/django/blob/master/django/db/backends/mysql/features.py#L110",
      "> > I'm not sure where and how to properly put it in Django's source code, though.\r\n> \r\n> You can use a `@cached_property` for the feature, for example https://github.com/django/django/blob/master/django/db/backends/mysql/features.py#L110\r\n\r\nYeah, I've used it in my `supports_json` DB feature. What I mean is, should I do something like this?\r\n\r\n```python\r\ntry:\r\n    with self.connection.cursor() as cursor:\r\n        cursor.execute(\"SELECT JSON('\\\"test\\\"')\r\nexcept DatabaseError:\r\n    return False\r\nelse:\r\n    return True\r\n```",
      "That looks like what I was thinking of, though you might need a `transaction.atomic` around it to prevent the error from breaking any current transaction - at least that's the way it works with some kinds of error on other databases, I'm no SQLite expert.",
      "~~I'm thinking of adding~~ I added custom encoder and decoder support for the form field.~~, but I'm not sure if it makes sense. Any thoughts?~~",
      "> No, `json.dumps` and `json.loads` take `None` as the default argument for the `cls` parameter. See https://docs.python.org/3/library/json.html#json.dumps.\r\n\r\nActually, I didn't test it but I saw [this part](https://github.com/django/django/blob/698df6a009cb1c4dbd55905264f24f6edf41066e/django/contrib/postgres/fields/jsonb.py#L25) in code.",
      "> I believe the first argument in functions returned by `get_db_converters` in a `Field` should take the field's value. I'm not sure keeping it inside the class and use `@staticmethod` is a better option.\r\n> \r\n> Such functions are mostly found in the backend (https://github.com/django/django/blob/master/django/db/backends/oracle/operations.py#L164). Since there's `get_db_converters` in the field, I decided to use that to avoid modifying the backend too much. If modifying the backend is preferred, I can do it that way.\r\n\r\nI would move under class as instance method.",
      "> Actually, I didn't test it but I saw [this part](https://github.com/django/django/blob/698df6a009cb1c4dbd55905264f24f6edf41066e/django/contrib/postgres/fields/jsonb.py#L25) in code.\r\n\r\nYes, but that's unnecessary since the default argument is `None`.\r\n\r\n> I would move under class as instance method.\r\n\r\nI don't think that would work since the first argument would be the `JSONField` instance, instead of the value?\r\n",
      "@laymonage Thanks for updates :+1: I think that we should currently move all PostgreSQL tests related with JSONField (e.g. `postgres_tests/test_json.py`) to all databases scope and start to work on failures. I would also recommend to remove current implementation from `contrib.postgres` and for backward compatibility leave it only as a reference to a new implementation (probably some workaround should be added to migrations):\r\n\r\n- `django.contrib.postgres.fields.JSONField` -> `django.db.models.JSONField`,\r\n- `django.contrib.postgres.forms.JSONField` -> `django.forms.JSONField`,\r\n\r\nFor example, `django/contrib/postgres/fields/jsonb.py`:\r\n```python\r\nfrom django.db.models import JSONField\r\n\r\n__all__ = ['JSONField']\r\n```\r\nall lookups should be moved from `contrib/postgres/fields/jsonb.py`  to `db/models/lookups.py`.\r\n\r\nWith these changes we will be able to find caveats for each database :male_detective: .",
      "@laymonage It's easier to review new changes when you push more commits instead of force-pushing. We will squash commits at the end (before a final review).",
      "@felixxm I remember some folks saying it'd be better to leave the current implementation in `contrib.postgres` as it is (and add a deprecation message). However, I see your idea is reasonable, as long as we can maintain all of the lookups and transforms. I guess I'll try going down that route and see if we can do that.\r\n\r\nMeanwhile, I've removed some tests in `postgres_tests` and incorporated them into `test_jsonfield.py`.\r\n\r\nSome updates:\r\n\r\n- `JSON_VALID(NULL)` returns `0` (false) on SQLite, while it returns true on MySQL and MariaDB (or maybe the check just doesn't occur). This makes it impossible to store SQL `NULL` even if we specify `blank=True, null=True`. I've updated the SQLite constraint with `OR \"%(column)s\" IS NULL` and now it works correctly.\r\n- Oracle Database stores SQL `NULL` as an empty string `''` on fields that support empty strings. I've updated `JSONField` to accommodate this behavior. Saving empty Python strings would still work, as they would be encoded as `'\"\"'`.\r\n- I've refactored the tests into different classes for cohesiveness.",
      "> @laymonage It's easier to review new changes when you push more commits instead of force-pushing. We will squash commits at the end (before a final review).\r\n\r\nAh, yes. I was wondering if I should do that. Thanks for the reminder.\r\nEdit: Done. I guess I should've done so earlier, but oh well... :sweat_smile: ",
      "I've implemented `HasKey`, `HasAnyKeys`, and `HasKeys` lookups on all database backends.\r\n\r\nI still have a bit of a problem on Oracle, though. It seems that parameterized values in SQL queries aren't quoted on Oracle. The `JSON_EXISTS` function requires the path to be a literal, so it has to be quoted. I tried adding the quote manually into the template, but it didn't work until I formatted the string and left out the path from the SQL params. However, of course, I'm afraid this would allow SQL injections.\r\n\r\nI'm not very familiar with Oracle Database, so any help is highly appreciated.",
      "Apparently, it's not because the values aren't quoted.  \r\ncx_Oracle uses bind variables:  \r\nhttps://www.oracle.com/technetwork/articles/dsl/prez-python-queries-101587.html\r\nhttps://oracle.readthedocs.io/en/latest/plsql/bind/\r\n\r\nBasically, query parameters get passed using variables, so queries look like this on Oracle:\r\n\r\n```sql\r\nSELECT * FROM TABLE WHERE col1 = :arg1 AND col2 = :arg2 ...\r\n```\r\n\r\nand the arguments can be passed using a dictionary, kwargs, or sequence (list, tuple), e.g.\r\n```python\r\nparams = {'arg1': 'hello', 'arg2': 'world'}\r\n```\r\n\r\n(see also: https://github.com/django/django/blob/master/django/db/backends/oracle/base.py#L478)\r\n\r\nThe problem is, `JSON_EXISTS` function on Oracle [doesn't support bind variables](https://stackoverflow.com/questions/48913687/jdbc-prepared-statement-to-query-json-using-json-exists). We can format the arguments directly into the SQL string (which is what I've done), but this opens up the possibility of SQL injections.\r\n\r\nHowever, I do `json.dumps()` on the specified key before formatting it, so the key will be double-quoted. If someone were to execute an SQL injection, they should end the quote first, which I don't think is possible since `\"` will be escaped by `json.dumps()` into `\\\"`. I think the worst that could happen is a `DatabaseError`. I currently can't think of a key string that can be used to perform an SQL injection.\r\n\r\nShould we go through with it, or drop support for these lookups on Oracle?\r\n\r\n",
      "@laymonage I *think* json.dumps is sufficient protection against an SQL attack. Really weird that Oracle doesn't support binding variables for some functions, but hey Oracle seems weird. @felixxm might have an opinion as an ex-Oracle user.",
      "Thanks a lot for the feedback. I started working on the lookups and transforms on MySQL using @adamchainz's and @raphaelm's existing code. It turns out that the code doesn't pass all of the tests from `contrib.postgres`, so I still have to fix things up.\r\n\r\nI also try to simplify or find better ways to implement the lookups and transforms, but fixing one thing tends to break another. It's very confusing, to be honest. Not to mention debugging it isn't so easy since I have to inspect the queries most of the time... :grimacing: \r\n\r\nEdit: on the other hand, `TestQuerying` test cases aren't run by djangoci. What's up with that?",
      "All tests now pass on MySQL.\r\n\r\nI found an issue with MariaDB: there's no function that converts JSON values into their equivalent SQL values. I could work around it for the most part, and now all the tests pass, except one: ordering by JSON values.\r\nSee this dbfiddle for a demonstration: https://dbfiddle.uk/?rdbms=mariadb_10.2&fiddle=2b4cbd4c5106f6a9d701be516c2a315b\r\n\r\nWe could try `CAST`ing the values into `SIGNED` integers, but that doesn't really solve the issue. I guess we could just add a notice on the docs to say that the values would only be ordered by the string representation of the JSON values. I don't think there's any other option.",
      "I've implemented the transforms and lookups on Oracle. Some features aren't supported, so I skipped the tests for those on Oracle.\r\n\r\nSome notes:\r\n- I didn't choose to implement it using the simple dot-notation syntax.\r\n  It's mainly because it requires the tables to be given aliases in the query. I could not find an easy and clean way to do that. The [oracle_json_field](https://github.com/Exscientia/oracle-json-field/blob/master/oracle_json_field/managers.py) package uses a custom Queryset and Manager with forced self-join to make table aliases.\r\n- In effect, I had to use `JSON_QUERY` to retrieve JSON objects and arrays, and `JSON_VALUE` to retrieve scalar values. To combine this, I used `COALESCE`. I probably should use `models.functions.Coalesce` for this, but if that's the case, it would make sense to also write `JSON_QUERY` and `JSON_VALUE` functions. It would probably add a little overhead on the Python-side. I'm not sure if I should do this. If I should, I'll probably also write some JSON `Func`s for all database backends that support them. For now, I'm just writing `COALESCE` directly into the SQL.\r\n- On the upside, using `JSON_QUERY` and `JSON_VALUE` supports querying > 4 KBytes of data while using the dot-notation syntax does not.",
      "I've implemented the transforms and lookups on SQLite. It turns out I can reuse most of the code from MySQL implementation. I only had to handle the case for querying JSON `null` values in JSON objects to differentiate them from missing keys (by using the `JSON_TYPE` function).\r\nSurprisingly, the support is equivalent to MySQL, which is much better than on Oracle.\r\n\r\nI have also added tests for storing JSON `null` scalar values. It is possible to do so by using `Value` during object creation. However, the Python representation of SQL `NULL` and JSON `null` are the same, i.e. `None`.",
      "I think I've found a way to implement `contains` and `contained_by` on SQLite and Oracle. I'll see what I can do.",
      "I managed to get `contains` working on SQLite and Oracle, though it was a bit of a hack since they both don't include a function similar to `JSON_CONTAINS`. It seems to work fine for its intended use (a `dict` rhs, to be checked on the top level of the JSON document). I added more tests, but I cannot guarantee it to work uniformly across all backends, especially for scalar and array rhs.\r\n\r\nEdit: I cannot think of a way to implement `contained_by`. Without a `JSON_CONTAINS` function, one would need to enumerate the JSON document in the database, which I think is impossible to do in one query."
    ],
    "num_comments": 30,
    "repository": "django/django",
    "diff_length": 150948,
    "code_diff": "diff --git a/AUTHORS b/AUTHORS\nindex 4632c66a62e8..03ed3fb7d643 100644\n--- a/AUTHORS\n+++ b/AUTHORS\n@@ -784,6 +784,7 @@ answer newbie questions, and generally made Django that much better:\n     Ryan Rubin <ryanmrubin@gmail.com>\n     Ryno Mathee <rmathee@gmail.com>\n     Sachin Jat <sanch.jat@gmail.com"
  },
  {
    "pr_title": "Checking framework",
    "pr_body": "This branch is part of my Google Summer of Code 2013 project. It's not intended to be merged, it's only to make deep review easier.\n\nSee discussion of checking framework on django-developers: https://groups.google.com/forum/?fromgroups=&hl=en#!topic/django-developers/fEf21dtpqDE\n",
    "pr_number": 1364,
    "comments": [
      "I've made error message single-line so there is no short/long description separation. See https://github.com/chrismedrela/django/commit/1929a8c3565bdd6aa36b8ce3f578f34091105d59.\n",
      "@chrismedrela can you push over the fixes from your main GSOC branch to this review branch - looks like lots of updates have been made, but it will be easier to review the whole branch in one place.\n",
      "It is great to see validation.py get replaced with something far more sane - and I think the overall approach is good.\n\nIn addition to my line level comments - here are some overall thoughts:\n\nThere are a number of places where related field checks are skipped if the the value is a string - I'm assuming for lazy resolution. Shouldn't we be in a position by the time checks are done to have all related fields connected? It seems that potential problems that are checked for are now deferred to runtime leaving users with \"why did this setup pass checks and now bombs?\"\n\nI wish there was a way to test this without the brittle problem of doing essentially string comparisons on the error messages. Any typo fixes or rewording means updating the docs. Unfortunately I don't have any bright ideas. When hitting a similar situation for SuspiciousOperation the solution was to create specific subclasses - but that seems like the wrong type of fix here.\n\nThere is a bit too much opaque use of **kwargs being passed around - it is fine for this, but if the design of this feature were to be much more complex than it is, it would be an ass-biting laying in wait.\n\nAs said in a comment, I think the \"check_all_models\" adds enough enhancement and exposes enough checking API for this feature without also adding the global \"registration\" of custom check functions.\n\nThe docs will need some more polish (I'm willing to help - left no comments yet), and actual deprecations need to be started.\n\nI'm NOT NOT NOT a coverage zealot but I did run my little diff coverage tool on the branch which found the following lines that were added/changed that are not tested:\n\nhttps://gist.github.com/ptone/fa491c101de3bc4fc5c7\nhttp://ptone.com/temp/checks-coverage/ (untested changes have block red line numbers)\n\n100% coverage should not be a blind objective, but it can be helpful for you to see any major untested areas, but overall the tests looked good.\n\nThanks for the tremendous amount of work during your GSOC.\n",
      "Thanks for the review, Preston -- much appreciated to have another set of eyes on the codebase.\n\nRegarding the string resolution of related field names -- that's mostly inherited from the old codebase -- Chris hasn't introduced anything new there. You're completely correct that at the point checks are performed, all the models _should_ be resolved. I'll stand corrected on this, but as I recall, the reason the string exclusions exist is so that when a bad model has been referenced, we can catch the fact that it hasn't been resolved, report that problem, and then perform any other checks that are appropriate. However, some checks will break hard if the foreign key reference hasn't been resolved, so you need to skip over those checks.\n\nRegarding the tests checking string content -- I agree that isn't ideal. A stretch goal for this project is to enable pyflakes-style warning/error suppression -- so you'll be able to register that you don't care about E115, and have errors of that type suppressed. This will also give us a simple constant against which we can perform tests. \n\nThe *_kwargs usage is a 'room for expansion' thing, much like the use of *_kwargs on save(). The idea is that you might be able to pass in specific qualifiers or modifiers to the checks; we don't know exactly what they will be -- one use at the moment is \"the app name\", but there could be others. Requiring **kwargs in the prototype for check methods means any future flags will be silently ignored, but can be specifically catered for when appropriate.\n\nAdding custom check functions was a specific goal for the project, with security checks being the use case validating the need for the feature. \n\nCompletely agreed on docs needing polish before this is merged -- that's true of any project, however. I'll certainly remember to call on you when we get to a merge point :-) \n\nThanks for the hit list on coverage, too. My validation to date has been a line-for-line comparison with the old validation checks; that means we should be at least as covered as we were previously, but doesn't account for previously existing testing holes.\n",
      "Am I correct that this issue implies that a default value should always be set, and that this could be a check added to BooleanField?\n\nhttps://github.com/django/django/pull/1466/files\n",
      "Preston, thank you very much for your review! Your comments helped me to improve the code and I really appreciate it.\n\nI went through list of all untested lines. Some of them were just dead lines (so they had no chance to be executed). The rest was result of lack of tests. I don't have too much time, so I wrote tests only for those lines which were easy to cover.\n\nYes, I agree that the check in #1466 pull request should be moved to `BooleanField`.\n",
      "Regarding the naming, what about \"Runtime checks\"?\n\nFor more inspiration, maybe these two links can help:\n- http://en.wikipedia.org/wiki/Static_program_analysis.\n- http://en.wikipedia.org/wiki/Dynamic_program_analysis.\n",
      "@loic, thank you for your input. I've had a look at these wiki pages, but I think that we will stay with \"system checks\" -- I cannot see any option that is _much_ better.\n",
      "One thing I'm curious about: how should we (whether that's Django or third-parties) decide what validation should be done by this checking process, and what should be done in `__init__()`? A number of fields do checks in `__init__()` and raise exceptions there - for example, `FileField` will raise a `TypeError` from `__init__()` if you try to pass it a `unique` argument, but it will not check `upload_to` until model checking.\n\nSince `__init__()` is always run, while model checking is generally only run in development, it would seem that this distinction matters most in a production environment. Since model checking is skipped there, under the assumption that problems have already been addressed in development, perhaps the distinction should be that `__init__()` only does the validation necessary to make the code actually run, while all correctness checks are done by the checking process.\n\nAny thoughts?\n",
      "@marfire I think you've found an unusual edge case of the old validation design.\n\nThe only examples of exceptions raised in `__init__()` that I can find are:\n- AutoField (rejecting `primary_key=False`), \n- FileField (rejecting `primary_key` and `unique` arguments). \n- ForeignKey/M2M (rejecting references to abstract models, and references that aren't a model or a string)\n\nHistorically, implementing these checks in validation.py would have meant extending the 'type specific' blocks in validation.py. Although these blocks already existed, it's not an especially good design pattern (putting all your validation logic in one place), so those three cases of localized validation have slipped in. \n\nI'm fairly certain that these checks could all be converted into system checks without any real change in behavior; and given that we're now moved to a 'check behavior stored on the field' archictecture, we can avoid the bad architecture. We also get slightly improved error reporting behavior as well -- under the current setup, if you have multiple ForeignKeys pointing to an abstract model, each one would be reported as an individual exception. Using a check-based approach, you'd get a summary of _all_ the bad references at once.\n\nAs for third party fields -- historically, they haven't had a choice. They've had to use assertions in `__init__`, because they didn't have access to validation.py. This is one of the reasons behind a move to a checking framework.\n\nSo - my advice for third parties (once this all lands in trunk) would be to use checks, rather than assertions in `__init__` checks -- and, for backwards compatibility, do both :-)\n",
      "@freakboy3742 Thanks for the clarification. That's good news - it's certainly nicer to do everything in system checks than it is to split the work with `__init__()`. \n",
      "I've rebased this branch. I've also improved documentation. I've also fixed the problem of compatibility checks -- I've added new `is_overridden` method to `Settings` and `UserSettingsHolder`. @ptone, do you have time to review documentation? This is the last thing we need to do in order to merge this branch.\n",
      "@chrismedrela Thanks for rebasing, but at this point, that's not strictly necessary; I've got a copy of this project in a branch of my own that I've been polishing when I get a chance. I'm hoping to get some time this week to take a good stab at it. \n\nOf course, documentation reviews are always welcome :-)\n",
      "Are we still trying to merge this for 1.7 alpha on Monday? Looks like it's at least in need of a rebase to merge cleanly.\n",
      "My intention is to merge for 1.7, yes. I'm hoping to find some time in the evenings this week (nothing quite like the last minute‚Ä¶)\n",
      "I'm going to close this PR now; this one was used as the work-in-progress during the GSoC period. I've completed a final review and rebase against trunk ready for 1.7 alpha. This review includes the comments @timgraham made over the last few days to the documentation.\n\nThe new PR is #2181.\n"
    ],
    "num_comments": 16,
    "repository": "django/django",
    "diff_length": 467197,
    "code_diff": "diff --git a/django/conf/__init__.py b/django/conf/__init__.py\nindex 7a915f1486bf..5a5a40519c01 100644\n--- a/django/conf/__init__.py\n+++ b/django/conf/__init__.py\n@@ -117,6 +117,7 @@ def __setattr__(self, name, value):\n \n class Settings(BaseSettings):\n     def __init__(self, settings_module):\n+\n    "
  },
  {
    "pr_title": "Refs #28643 -- Added math database functions.",
    "pr_body": "Added ACOS, ASIN, ATAN, ATAN2, CEILING, COS, COT, DEGREES, EXP,\r\nFLOOR, LOG, MOD, PI, POWER, RADIANS, ROUND, SIN, SQRT, TAN\r\nABS was added according to the commit by onkruid. \r\nAny suggestion is welcome!",
    "pr_number": 9622,
    "comments": [
      "> It seems that the test_math.py cannot import my math functions. However, it works just fine for me locally. Could anybody help me with that?\r\n\r\nAre you still struggling with that?\r\n\r\nIt seems a bit odd to me, PY2 could have got mixed up between the absolute `math` package and the relative one, but PY3 doesn't have this issue.\r\n\r\nHave you tried clearing `__pycache__`?",
      "@loic I think I found the reason. Thank you very much. ",
      "`.. module:: django.db.models.functions.math` remove this, and try again. The toplevel `..versionadded: 2.1` ought be enough. `PI` should be `Pi` IMO.",
      "@atombrella Thank you very much for the suggestions! I followed your instructions and now it works.",
      "@auvipy It supports sqlite, Mysql, postgresql and oracle.",
      "@pope1ni The test on oracle backend paused because of conflicts of releases/2.1.txt. It seems that the release/2.1.txt has been updated since my last commit. Do I need to resolve the conflict?\r\n\r\nI have tried to fetch the most recent releases/2.1.txt from django/django and update my local file based on this version. However, it doesn't work. There is still conflict after my update. Any suggestion?\r\n\r\nJunyi ",
      "(Sorry - GitHub seemed to break my review into pieces...)",
      "@pope1ni @atombrella Thank you so much for the reviews and suggestions. I am working on them now. Thank you for the detailed explanations and the comments really helps a lot. \r\n",
      "@pope1ni I have revised the code based on the review/comments and finished updating most of them. However, I noticed that there is no tests running for my last two commits. Is there any issue? Did I commit too much that the system prohibited running tests on my code?",
      "You can squash your commits and force push. That should trigger a build.",
      "@timgraham Thanks for the advice. I tried squash commits and force push, but it didn't trigger a build. Any suggestions?",
      "Hi @pope1ni I have resolved the conflicts now. Thank you. The conflicts resolving part is more complicated than I expected.",
      "I am a little bit surprised that none of the automatic tests test database oracle (correct me if I am wrong). @felixxm @pope1ni @atombrella @timgraham ",
      "@pope1ni Totally agreed with all your comments, thanks! :bowing_man: :rocket: ",
      "Thank you @pope1ni @felixxm for helping the oracle functions. I have committed all the changes as you suggested.",
      "Hi @pope1ni @felixxm @timgraham Thank you so much for your comments and review suggestions. I am wondering whether there is anything else to change or not? If not, could you help test on oracle and then merge it?",
      "Thank you for your patience, @JunyiJ. This is coming together well now.\r\n\r\nThere are a couple of things that I can think of outstanding.\r\n\r\n1. As suggested by @felixxm, we can now remove `django_power`:\r\n    ```diff\r\n    diff --git a/django/db/backends/sqlite3/base.py b/django/db/backends/sqlite3/base.py\r\n    index a9753bb094..07b153cd22 100644\r\n    --- a/django/db/backends/sqlite3/base.py\r\n    +++ b/django/db/backends/sqlite3/base.py\r\n    @@ -6,2 +6,3 @@ import decimal\r\n     import math\r\n    +import operator\r\n     import re\r\n    @@ -170,3 +171,2 @@ class DatabaseWrapper(BaseDatabaseWrapper):\r\n             conn.create_function(\"django_format_dtdelta\", 3, _sqlite_format_dtdelta)\r\n    -        conn.create_function(\"django_power\", 2, _sqlite_power)\r\n             conn.create_function('LPAD', 3, _sqlite_lpad)\r\n    @@ -187,3 +187,3 @@ class DatabaseWrapper(BaseDatabaseWrapper):\r\n             conn.create_function('PI', 0, lambda: math.pi)\r\n    -        conn.create_function('POWER', 2, _sqlite_power)\r\n    +        conn.create_function('POWER', 2, operator.pow)\r\n             conn.create_function('RADIANS', 1, math.radians)\r\n    @@ -498,5 +498 @@ def _sqlite_rpad(text, length, fill_text):\r\n         return (text + fill_text * length)[:length]\r\n    -\r\n    -\r\n    -def _sqlite_power(x, y):\r\n    -    return x ** y\r\n    diff --git a/django/db/backends/sqlite3/operations.py b/django/db/backends/sqlite3/operations.py\r\n    index 10b064d966..65796ea8a2 100644\r\n    --- a/django/db/backends/sqlite3/operations.py\r\n    +++ b/django/db/backends/sqlite3/operations.py\r\n    @@ -274,6 +274,6 @@ class DatabaseOperations(BaseDatabaseOperations):\r\n         def combine_expression(self, connector, sub_expressions):\r\n    -        # SQLite doesn't have a power function, so we fake it with a\r\n    -        # user-defined function django_power that's registered in connect().\r\n    +        # SQLite doesn't have a POWER function, so we fake it with a\r\n    +        # user-defined function that's registered in connect().\r\n             if connector == '^':\r\n    -            return 'django_power(%s)' % ','.join(sub_expressions)\r\n    +            return 'POWER(%s)' % ','.join(sub_expressions)\r\n             return super().combine_expression(connector, sub_expressions)\r\n    ```\r\n\r\n2. I raised the issue of input values being `DecimalField` and coming out as `FloatField`.\r\n\r\n    The functions `ACos`, `ASin`, `ATan`, `ATan2`, `Cos`, `Cot`, `Degrees`, `Log`, `Mod`, `Pi`, `Power`, `Radians`, `Sin`, `Sqrt` and `Tan` all declare `output_field = FloatField()`.\r\n\r\n    Firstly, this doesn't seem consistent as some functions are missing this, e.g. `Ln`. It makes sense that `IntegerField` becomes `FloatField`, but I'd expect `DecimalField` to stay `DecimalField`.\r\n\r\n    One way this could be achieved would be via a hack similar to this:\r\n   https://github.com/django/django/blob/d549b8805053d4b064bf492ba90e90db5d7e2a6b/django/db/models/functions/window.py#L49-L51\r\n    But as a mixin so that it can be added easily to each of the math functions that require it:\r\n    ```python\r\n    class MathOutputFieldMixin:\r\n        def _resolve_output_field(self):\r\n            sources = self.get_source_expressions()\r\n            if any(isinstance(s.output_field, DecimalField) for s in sources):\r\n                return DecimalField()\r\n            else:\r\n                return FloatField()\r\n    ```\r\n    I'd like some opinion on this from @felixxm and @timgraham before you spend time implementing it.",
      "Hi @pope1ni, I revised the code based on your suggestions except the output_field(Though I think it is a great idea to use output_field_reolved, I will wait to see whether there is different opinion). There are several issues that I noticed:\r\n1) For the log(a,b)function, different database behaves differently. Some use log(base, num) and some use log(num, base).\r\n2) Some data base doesn't support mod(double precision, double precision). I guess that is the reason why I didn't include the tests on float for the mod function.\r\n3) Some data base doesn't support log(double precision, double precision). I guess that is the reason why I didn't include the tests on float for the mod function.\r\n\r\nFor cases 2) and 3) Do you think it is OK to remove the tests  on float?\r\n\r\nThanks,\r\nJunyi",
      "Hi @JunyiJ,\r\n\r\nI've made a number of fixes that you can pull into your branch @ https://github.com/JunyiJ/django/pull/1:\r\n\r\n- Simplified the type casting for `Log()` and `Mod()` on PostgreSQL.\r\n  *(The issue was that a value is needed for `max_digits` and `decimal_places`.)*\r\n- We no longer cast `DecimalField` to `FloatField` for `output_field`.\r\n- Added type assertions to tests to complement the `output_field` stuff.\r\n- Added missing tests for `Round()`.\r\n- Fixed the problem with `Log()` on the SpatiaLite backend.\r\n- Various other bits of tidying in the tests.\r\n\r\nI've noted that `ATAN2` was added to SpatiaLite v4.3.0 which also seems to exhibit problems but I think Django's Jenkins currently uses < v4.3.0, so `math.atan2` is still being used.\r\n\r\nSee http://www.gaia-gis.it/gaia-sins/spatialite-sql-4.3.0.html#math\r\n\r\nBug filed for Log() on SpatiaLite: https://www.gaia-gis.it/fossil/libspatialite/tktview?name=8f59ddebf0",
      "@pope1ni Thank you very much for fixing these problems. I just merged all your changes to the branch.\r\n\r\nJunyi",
      "So apparently it looks like spatialite handles `ROUND()` differently to everyone else too although I didn't encounter a problem locally.\r\n\r\nJust checking that Oracle is behaving...",
      "@pope1ni In that case, we can either loose the test case criteria to pass the tests or change the Round function in math.py as what you did to Log. What do you think is a better way?",
      "> In that case, we can either loose the test case criteria to pass the tests...\r\n\r\nNo, we shouldn't do this. The behaviour of spatialite is incorrect. Tests should test correct behaviour, not be made lax to satisfy broken implementations. Could you revert that change?\r\n\r\nI'll do some more digging over the next couple of days and try and come up with a correct solution. I have a few ideas.",
      "@pope1ni OK. I have reverted the implementation. \r\n\r\nBased on the definition SpatiaLite round function returns the integer value nearest to x, but it doesn't specify the special case of numbers ended with 0.5. As a result, I am not sure whether SpatiaLite Round function is wrong here or it is designed to be so.",
      "Nick, do you want to look at this any more per https://github.com/django/django/pull/9622#issuecomment-376322325?\r\n\r\nJunyi, please target Django 2.2 and squash the commits.",
      "Tim, will aim to have a look at this over the next few days.",
      "@timgraham Hi Tim, Does 'Targeting Django 2.2' means that I should update the 2.2 in documents folder? \r\n@pope1ni Hi Nick, should I wait until you finish reviewing?\r\n\r\nThanks!\r\nJunyi",
      "So https://github.com/django/django/pull/9622#issuecomment-376322325 was already addressed by Junyi's last commit - it seems that `ROUND()` is unstable around `0.5`."
    ],
    "num_comments": 28,
    "repository": "django/django",
    "diff_length": 71275,
    "code_diff": "diff --git a/django/db/backends/sqlite3/base.py b/django/db/backends/sqlite3/base.py\nindex 7951143fd097..20905c83b85f 100644\n--- a/django/db/backends/sqlite3/base.py\n+++ b/django/db/backends/sqlite3/base.py\n@@ -173,10 +173,28 @@ def get_new_connection(self, conn_params):\n         conn.create_functio"
  },
  {
    "pr_title": "Fixed #33308 -- Added support for psycopg version 3",
    "pr_body": "What did I do? I took\r\nhttps://github.com/dvarrazzo/django-psycopg3-backend and blackified it +\r\nported over most (all?) new commits. I am now opening this on GitHub to\r\nbe able to nicely diff and start a discussion about whether we can\r\nsupport psycopg2 & 3 easiyl from the same codebase (I think we can).\r\n\r\n----\r\nI (i.e. @felixxm) have the following plan to move this forward:\r\n- [x]  merge #16245,\r\n- [x]  squash commits,\r\n- [x]  rebase,\r\n- [x]  revert unnecessary changes, e.g.\r\n  - [x]  https:/",
    "pr_number": 15687,
    "comments": [
      "Looking through the code base there are quite a few areas where it would probably be easier if we just assumed that if psycopg3 is installed that we want to use it; this might get a bit more fun for testing (extra environment, but realistically speaking we want to be on psycopg3 only in the longrun anways‚Ä¶)",
      "We are down to three failures :)",
      "@timgraham I'd love if you could look over this and maybe test your cockroachdb backend against this. It would be great if I don't fully break it :)",
      "Besides the issue in `SchemaLoggerTests`, there's one other regression with psycopg2. (I'll work through the failures with psycopg3 later).\r\n```\r\n======================================================================\r\nFAIL: test_orders_nulls_first_on_filtered_subquery (ordering.tests.OrderingTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/tim/code/django/tests/ordering/tests.py\", line 199, in test_orders_nulls_first_on_filtered_subquery\r\n    self.assertQuerysetEqualReversible(\r\n  File \"/home/tim/code/django/tests/ordering/tests.py\", line 129, in assertQuerysetEqualReversible\r\n    self.assertSequenceEqual(queryset.reverse(), list(reversed(sequence)))\r\nAssertionError: Sequences differ: <QuerySet [<Author: Name 3>, <Author: Name 1>, <Author: Name 2>]> != [<Author: Name 2>, <Author: Name 1>, <Author: Name 3>]\r\n\r\nFirst differing element 0:\r\n<Author: Name 3>\r\n<Author: Name 2>\r\n\r\n- <QuerySet [<Author: Name 3>, <Author: Name 1>, <Author: Name 2>]>\r\n? ----------               ^                                   ^  -\r\n\r\n+ [<Author: Name 2>, <Author: Name 1>, <Author: Name 3>]\r\n?                ^                                   ^\r\n```\r\nThe old SQL:\r\n```sql\r\n SELECT DISTINCT \"ordering_author\".\"id\",\r\n                \"ordering_author\".\"name\",\r\n                \"ordering_author\".\"editor_id\",\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\") AS \"last_date\",\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\") IS NULL,\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\")\r\nFROM   \"ordering_author\"\r\nORDER  BY (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n           FROM   \"ordering_article\" U0\r\n           WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                    AND Upper(U0.\"headline\" :: text) LIKE Upper('%Article%') )\r\n           GROUP  BY U0.\"author_id\") IS NULL,\r\n          (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n           FROM   \"ordering_article\" U0\r\n           WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                    AND Upper(U0.\"headline\" :: text) LIKE Upper('%Article%') )\r\n           GROUP  BY U0.\"author_id\") DESC  \r\n```\r\n\r\n\r\nThe new SQL: \r\n```sql\r\n SELECT DISTINCT \"ordering_author\".\"id\",\r\n                \"ordering_author\".\"name\",\r\n                \"ordering_author\".\"editor_id\",\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\") AS \"last_date\",\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\") IS NULL,\r\n                (SELECT Max(U0.\"pub_date\") AS \"last_date\"\r\n                 FROM   \"ordering_article\" U0\r\n                 WHERE  ( U0.\"author_id\" = ( \"ordering_author\".\"id\" )\r\n                          AND Upper(U0.\"headline\" :: text) LIKE Upper(\r\n                              '%Article%') )\r\n                 GROUP  BY U0.\"author_id\")\r\nFROM   \"ordering_author\"\r\nORDER  BY 5 DESC  \r\n```\r\nIt might be that because CockroachDB has `DatabaseFeatures.nulls_order_largest = False` (unlike PostgreSQL), the loss of the second subquery in the `ORDER BY` is problematic.",
      "> It might be that because CockroachDB has `DatabaseFeatures.nulls_order_largest = False` (unlike PostgreSQL), the loss of the second subquery in the `ORDER BY` is problematic.\r\n\r\nThis is probably a result of https://github.com/django/django/pull/15687/files#diff-f58de2deaccecd2d53199c5ca29e3e1050ec2adb80fb057cdfc0b4e6accdf14fR753-R769 but if it looses the second `ORDER BY` then this might be a problem in the linked code and not in cdb.",
      "@timgraham I can reproduce your query issue when I set `supports_order_by_nulls_modifier = False` (which it probably is on old cockroachdb versions: https://github.com/cockroachdb/django-cockroachdb/blob/master/django_cockroachdb/features.py#L60 -- did you test against an old version?). That said, the combination of `supports_order_by_nulls_modifier = False` & `supports_order_column_alias = True` certainly shows there is a problem in the new code.",
      "> @timgraham I can reproduce your query issue when I set `supports_order_by_nulls_modifier = False` (which it probably is on old cockroachdb versions: https://github.com/cockroachdb/django-cockroachdb/blob/master/django_cockroachdb/features.py#L60 -- did you test against an old version?). That said, the combination of `supports_order_by_nulls_modifier = False` & `supports_order_column_alias = True` certainly shows there is a problem in the new code.\r\n\r\nYes, the failure is only present on older versions of CockroachDB.",
      "wow all tests passing! great work",
      "@timgraham Any chance you could test the latest changes against cockroach db?",
      "Just tested this with CockroachDB on [1043b36](https://github.com/django/django/commit/1043b3655d02d277a5c200788d381d8a9ed078d9). First impressions: Seems to work! Brilliant job.\r\n\r\nForgive my noobiness. This is a really exciting feature to me-- I'm really eager to see a release, so I wanted to post a report.\r\n\r\nhttps://github.com/cockroachdb/django-cockroachdb Needs to be updated with psycopg3 @timgraham. Was able to get it running by changing the following lines:\r\n\r\n* In `django_cockroachdb/base.py`\r\n```python\r\nimport psycopg\r\n#import psycopg2  # noqa\r\n#import psycopg2.extensions  # noqa\r\n#import psycopg2.extras  # noqa\r\n```\r\n* In `django_cockroachdb/operations.py`\r\n```python\r\nfrom psycopg import errors\r\n#from psycopg2 import errorcodes\r\n#.... I'm sure this will break, but I was just trying to get it running.\r\nif (getattr(exc.__cause__, 'pgcode', '') != errors.SERIALIZATION_FAILURE or\r\n#if (getattr(exc.__cause__, 'pgcode', '') != errorcodes.SERIALIZATION_FAILURE or\r\n```\r\n* Commented out version check in `django_cockroachdb/utils.py`\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/24665/184909616-520f48a7-4244-4559-a3dd-b3d2335f794c.png)\r\n",
      "@apollo13 Will you tackle the PostGIS backend? (First issue: `from psycopg2 import Binary` in `django/contrib/gis/db/backends/postgis/adapter.py`)",
      "@timgraham At some point yes, but probably not before I have weeded out the remaining issues with bind params in the core backend.",
      "Great! Your last commit here for EXTRACT params fixed failures on CockroachDB. Most of the remaining failures are in the following categories:\r\n* `could not determine data type of placeholder $1`\r\n* `column \"name\" must appear in the GROUP BY clause or be used in an aggregate function`\r\n* `unsupported binary operator: <decimal> / <float> (desired <decimal>)`\r\n\r\nI'm guessing that first category is what you're referring to in your last comment.",
      "Yes, bullets 1 & 2 are expected to show up currently. I just pushed (a rather ugly) gis support commit.",
      "Okay down to one failure that will probably require some work. Reviews welcome! GIS Tests pass also now.",
      "Hi @apollo13 , what stage this is currently at? Maybe I can help you with something? See, [my project](https://github.com/Bi-Coloured-Python-Rock-Snake/readme) needs this backend badly :)",
      "@pwtail Well code review here and testing your code against this branch with psycopg3 would be a massive help.\r\n\r\nI'll rebase & update the branch over the next days (parts of the required changes have already be merged to main)",
      "Thank you @apollo13 , already using your branch) ",
      "Hi folks,\r\n\r\nI have just rebased this PR to latest master. Now that all prerequisites for this PR are in further rebasing should get easier and not require manual intervention. I think the PR is starting to become solid, there are some areas though where I'd highly appreciate some help:\r\n\r\n * PostGIS in general. The adapter registration there is imo still rather ugly, I am open to creative ideas :)\r\n * Is my new handling of registering extension sound? \r\n * Do the new `as_postgresql` make sense everywhere?\r\n * Ideas on https://github.com/django/django/pull/15687/files#diff-530111bb812ef292d6db3ab0285e128bb585992d02d47dcca872d2e8c9b592daR586 \r\n * Is `compose` used correctly and consistently or are there other variants still around in the codebase?",
      "@apollo13 thanks for the rebase. I'm currently working on addressing the `as_postgresql` and `Text(str)` quirks by having `JSONField.get_db_prep_value` make use of `psycopg.types.json.Jsonb` instead and it's showing great results (down to a few remaining test failures).\r\n\r\nI'll keep you updated!",
      "@apollo13 I confirm that locally tests are working with Python 3.11 , psycopg2 or psycopg version 3, PostgreSQL 14.5",
      "@charettes Uff that is great to hear. Btw I do have a custom branch where I have enabled github actions to test all combinations: https://github.com/apollo13/django/tree/psycopg-dev So once you have something I'll reapply it there to run the tests against all combinations.",
      "We are not 100% there yet, ie Simon's changes will be coming in. But this PR is not in a reviewable state and I'd love to have more people look over it.",
      "I haven't reviewed much of the code, but it's mostly working on CockroachDB. Mostly it's down to `could not determine data type of placeholder` errors which I think will be fixed by adding `as_cockroachdb` to alias the new `as_postgresql` methods.\r\n\r\nOn GIS, there a a bunch of `cannot adapt type 'PostGISAdapter' using placeholder '%s' (format: AUTO)`. Glancing at the code, I haven't spotted the fix for this. (I'll dig in if something obvious doesn't come to mind.)",
      "After digging in, many of the errors about 'could not determine type of placeholder' are actually string arguments like:\r\n```\r\nSELECT \"ordering_article\".\"id\",\r\n       \"ordering_article\".\"author_id\",\r\n       \"ordering_article\".\"second_author_id\",\r\n       \"ordering_article\".\"headline\",\r\n       \"ordering_article\".\"pub_date\",\r\n       '1' AS \"constant\"\r\nFROM \"ordering_article\"\r\nORDER BY \"constant\" ASC,\r\n         \"ordering_article\".\"headline\" DESC;\r\n\r\nargs=('1',);\r\n```\r\nfails with:\r\n```\r\npsycopg.errors.IndeterminateDatatype: could not determine data type of placeholder $1\r\nHINT:  consider adding explicit type casts to the placeholder arguments\r\n```\r\nAs far as I can tell, the same query is generated when running against PostgreSQL. Am I missing some initialization code or something? Could it be a problem with CockroachDB?",
      "@timgraham if you want this to work properly for CockroachDB you'd likely want to override `Value.as_cockroachdb` to add explicit casts based of `self.output_field` so annotating `Value(\"1\")` turns into `('%s::text', '1')`.\r\n\r\nThis might not work properly until a form of https://github.com/apollo13/django/pull/3 lands here though as we've been supporting `Value(json.dumps(1))` and expect the JSON type inference to just work and I believe we'll have to deprecate that.",
      "Thanks for looking. It seems to be some difference between PostgreSQL and CockroachDB then? If so, I'll try to confirm and then report to the Cockroach team to see if they can address it since they try to be compatible with PostgreSQL. If they can't, thanks for the tip about a workaround.",
      "I added a temporary configuration to run tests with `psycopg` 3. You can trigger it with _\"buildbot, test on psycopg3.\"_"
    ],
    "num_comments": 28,
    "repository": "django/django",
    "diff_length": 75535,
    "code_diff": "diff --git a/django/contrib/gis/db/backends/postgis/adapter.py b/django/contrib/gis/db/backends/postgis/adapter.py\nindex 9161e25f1635..c95f9032538a 100644\n--- a/django/contrib/gis/db/backends/postgis/adapter.py\n+++ b/django/contrib/gis/db/backends/postgis/adapter.py\n@@ -1,8 +1,6 @@\n \"\"\"\n  This objec"
  },
  {
    "pr_title": "Fixed #33012 -- Added Redis cache backend.",
    "pr_body": "ticket-33012\r\n\r\nThis PR is in accordance with this [GSoC project](https://summerofcode.withgoogle.com/projects/#6292871491092480)\r\nThe detailed proposal can be found [here](https://docs.google.com/document/d/1_gIa_17uCNlwJTmqiMLkiVtRgTOD2MvHpy4NNFvKBWc/edit?usp=sharing)\r\n\r\nThis PR aims at adding support for Redis to be used as a caching backend with Django. As redis is the most popular caching backend, adding it to django.core.cache module would be a great addition for developers who previously ",
    "pr_number": 14437,
    "comments": [
      "Hey @carltongibson \r\nSo I was able to get a quick and simple implementation up and running. However, there are several decisions that I made for simplicity, like using pickle for serializing data. \r\nI am still not sure how shall be handle multiple servers. Do we setup a sharding based client (like memcached) or should it cater to a replication based setup with a \"Primary and replica\" based setup?\r\n\r\nDo let me know what improvement I should make and how I shall proceed!",
      "Sorry for stepping in. \r\n\r\nI have some questions, isn't this what [jazzband/django-redis](https://github.com/jazzband/django-redis) is doing?\r\n\r\nWill there be some code copying, what will be the faith of it?\r\nBy the way `django-redis` is already somewhat compatible with django api and widely used by the community.\r\n\r\nShould there be any discussion about it? Has it ever been one?",
      "Hey @WisdomPill ‚Äî The proposal is to add a Redis backend to core. \r\n\r\nIt will be simpler than that provided by `django-redis`, for instance customising the serialiser is out-of-scope for the initial pass. \r\n\r\nIt's been [discussed quite a few times on django-developers](https://groups.google.com/g/django-developers/search?q=redis). The swinger was [last year's survey showing approx twice as many users opting for Redis (where we have no backend) over memcached (where we have several)](https://www.djangoproject.com/weblog/2020/jul/28/community-survey-2020/).",
      "> Hi @abbasidaniyal ‚Äî good speedy start! \r\n> \r\n> The initial thing is tests. Can you look at the coverage for the existing backends and adapt the tests for the new backend? Then, that immediately gives us a todo list in terms of API.\r\n\r\nSure. I'll start working on the tests. Will be sharing coverage reports soon!",
      "> Sorry for stepping in.\r\n> \r\n> I have some questions, isn't this what [jazzband/django-redis](https://github.com/jazzband/django-redis) is doing?\r\n> \r\n> Will there be some code copying, what will be the faith of it?\r\n> By the way `django-redis` is already somewhat compatible with django api and widely used by the community.\r\n> \r\n> Should there be any discussion about it? Has it ever been one?\r\n\r\nHey @WisdomPill!\r\nYou can find a link to my proposal [here](https://docs.google.com/document/d/1_gIa_17uCNlwJTmqiMLkiVtRgTOD2MvHpy4NNFvKBWc/edit?usp=sharing). A lot of inspiration is taken from the `django-redis` package as mentioned in the proposal. There might be some code similarities with `django-redis` as the end-goal of remains the same, that is, connecting `redis-py` client with the django caching framework. \r\n",
      "Hey @carltongibson! \r\nSo I was going trying to work on the coverages of each backend. However, one tests keeps failing. \r\n`cache.tests.CreateCacheTableForDBCacheTests.test_createcachetable_observes_database_router`\r\n```\r\n======================================================================\r\nFAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n    return func(*args, **kwargs)\r\n  File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n    management.call_command('createcachetable', database='other', verbosity=0)\r\n  File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n    self.test_case.assertEqual(\r\nAssertionError: 1 != 5 : 1 queries executed, 5 expected\r\nCaptured queries were:\r\n1. \r\n            SELECT c.relname,\r\n            CASE WHEN c.relispartition THEN 'p' WHEN c.relkind IN ('m', 'v') THEN 'v' ELSE 't' END\r\n            FROM pg_catalog.pg_class c\r\n            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\r\n            WHERE c.relkind IN ('f', 'm', 'p', 'r', 'v')\r\n                AND n.nspname NOT IN ('pg_catalog', 'pg_toast')\r\n                AND pg_catalog.pg_table_is_visible(c.oid)\r\n        \r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nI did not completely understand the reason for this. \r\nMy test settings were\r\n```\r\nDATABASES = {\r\n    'default': {\r\n        'ENGINE': 'django.db.backends.postgresql',\r\n        'NAME': 'mydb_default',\r\n        'USER': 'myuser',\r\n        'PASSWORD': 'password',\r\n        'HOST': 'localhost',\r\n        'PORT': '5432',\r\n    },\r\n    'other': {\r\n        'ENGINE': 'django.db.backends.postgresql',\r\n        'NAME': 'mydb_other',\r\n        'USER': 'myuser',\r\n        'PASSWORD': 'password',\r\n        'HOST': 'localhost',\r\n        'PORT': '5432',\r\n    }\r\n}\r\n\r\nCACHES = {\r\n    \"default\": {\r\n        \"BACKEND\": \"django.core.cache.backends.db.DatabaseCache\",\r\n        \"LOCATION\": \"my_cache_table\",\r\n    },\r\n}\r\n```\r\nTried and tested with SQLite as well, and got the same results. \r\nError with SQLite\r\n```\r\n======================================================================\r\nFAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n    return func(*args, **kwargs)\r\n  File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n    management.call_command('createcachetable', database='other', verbosity=0)\r\n  File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n    self.test_case.assertEqual(\r\nAssertionError: 1 != 5 : 1 queries executed, 5 expected\r\nCaptured queries were:\r\n1. \r\n            SELECT name, type FROM sqlite_master\r\n            WHERE type in ('table', 'view') AND NOT name='sqlite_sequence'\r\n            ORDER BY name\r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nHowever, when I comment out this line, which call `createcachetable`, the test passes.\r\nhttps://github.com/django/django/blob/d270dd584e0af12fe6229fb712d0704c232dc7e5/django/db/backends/base/creation.py#L92\r\n",
      "Hey @abbasidaniyal ‚Äî super. That's what we like to see. üòÉ\n\nCan you push your latest and I will take a look hopefully tomorrow. ",
      "> Hey @abbasidaniyal ‚Äî super. That's what we like to see. \r\n> \r\n> Can you push your latest and I will take a look hopefully tomorrow.\r\n\r\nI'm currently running the tests against the `main` branch. \r\nSharing the coverage results as screenshots [here](https://docs.google.com/document/d/1l2NiwZjNPCKYm5QLaQoWOtt7o4nt7nReLceVGF3DYks/edit?usp=sharing)\r\n\r\nI'll start adapting these existing tests for the new backend now!",
      "Hey @carltongibson !\r\nI've just pushed the lastest update that I have. I've adapted the existsing tests for the new backend. The tests are failing at two instance\r\n- Culling\r\n- zero and negative timeout handling : Redis-py does not support 0 or negative timeouts. I have implemented the `get_backend_timeout` similar to the memcache backend but I'm still not sure about how to handle 0 timeout. Ideally it should not store the key in the first place.",
      "> Hey @carltongibson!\r\n> So I was going trying to work on the coverages of each backend. However, one tests keeps failing.\r\n> `cache.tests.CreateCacheTableForDBCacheTests.test_createcachetable_observes_database_router`\r\n> \r\n> ```\r\n> ======================================================================\r\n> FAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n> ----------------------------------------------------------------------\r\n> Traceback (most recent call last):\r\n>   File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n>     return func(*args, **kwargs)\r\n>   File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n>     management.call_command('createcachetable', database='other', verbosity=0)\r\n>   File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n>     self.test_case.assertEqual(\r\n> AssertionError: 1 != 5 : 1 queries executed, 5 expected\r\n> Captured queries were:\r\n> 1. \r\n>             SELECT c.relname,\r\n>             CASE WHEN c.relispartition THEN 'p' WHEN c.relkind IN ('m', 'v') THEN 'v' ELSE 't' END\r\n>             FROM pg_catalog.pg_class c\r\n>             LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\r\n>             WHERE c.relkind IN ('f', 'm', 'p', 'r', 'v')\r\n>                 AND n.nspname NOT IN ('pg_catalog', 'pg_toast')\r\n>                 AND pg_catalog.pg_table_is_visible(c.oid)\r\n>         \r\n> \r\n> ----------------------------------------------------------------------\r\n> ```\r\n> \r\n> I did not completely understand the reason for this.\r\n> My test settings were\r\n> \r\n> ```\r\n> DATABASES = {\r\n>     'default': {\r\n>         'ENGINE': 'django.db.backends.postgresql',\r\n>         'NAME': 'mydb_default',\r\n>         'USER': 'myuser',\r\n>         'PASSWORD': 'password',\r\n>         'HOST': 'localhost',\r\n>         'PORT': '5432',\r\n>     },\r\n>     'other': {\r\n>         'ENGINE': 'django.db.backends.postgresql',\r\n>         'NAME': 'mydb_other',\r\n>         'USER': 'myuser',\r\n>         'PASSWORD': 'password',\r\n>         'HOST': 'localhost',\r\n>         'PORT': '5432',\r\n>     }\r\n> }\r\n> \r\n> CACHES = {\r\n>     \"default\": {\r\n>         \"BACKEND\": \"django.core.cache.backends.db.DatabaseCache\",\r\n>         \"LOCATION\": \"my_cache_table\",\r\n>     },\r\n> }\r\n> ```\r\n> \r\n> Tried and tested with SQLite as well, and got the same results.\r\n> Error with SQLite\r\n> \r\n> ```\r\n> ======================================================================\r\n> FAIL: test_createcachetable_observes_database_router (cache.tests.CreateCacheTableForDBCacheTests)\r\n> ----------------------------------------------------------------------\r\n> Traceback (most recent call last):\r\n>   File \"/home/daniyal/Desktop/django/django/test/utils.py\", line 430, in inner\r\n>     return func(*args, **kwargs)\r\n>   File \"/home/daniyal/Desktop/django/tests/cache/tests.py\", line 1238, in test_createcachetable_observes_database_router\r\n>     management.call_command('createcachetable', database='other', verbosity=0)\r\n>   File \"/home/daniyal/Desktop/django/django/test/testcases.py\", line 86, in __exit__\r\n>     self.test_case.assertEqual(\r\n> AssertionError: 1 != 5 : 1 queries executed, 5 expected\r\n> Captured queries were:\r\n> 1. \r\n>             SELECT name, type FROM sqlite_master\r\n>             WHERE type in ('table', 'view') AND NOT name='sqlite_sequence'\r\n>             ORDER BY name\r\n> \r\n> ----------------------------------------------------------------------\r\n> ```\r\n> \r\n> However, when I comment out this line, which call `createcachetable`, the test passes.\r\n> https://github.com/django/django/blob/d270dd584e0af12fe6229fb712d0704c232dc7e5/django/db/backends/base/creation.py#L92\r\n\r\n@carltongibson I was able to figure this one out. I was following the [documentation](https://docs.djangoproject.com/en/3.2/topics/cache/#database-caching) to setup the DatabaseCache.\r\n```\r\nCACHES = {\r\n    \"default\": {\r\n        \"BACKEND\": \"django.core.cache.backends.db.DatabaseCache\",\r\n        \"LOCATION\": \"my_cache_table\",\r\n    },\r\n}\r\n```\r\nHowever, I believe `my_cache_table` was conflicting with this\r\nhttps://github.com/django/django/blob/225d96533a8e05debd402a2bfe566487cc27d95f/tests/cache/tests.py#L1213-L1220\r\n\r\nSetting the \"LOCATION\" to some other table name leads to the test passing. \r\n\r\nMaybe we could mention it in the docs somewhere or update the test to check if duplicate table names exists? This is a little off-topic from this PR. Should I create a separate ticket for this? Or should we let it be for now?\r\n",
      "Hey!\r\nI've made the changes suggested by @pope1ni. I've updated the tests and added\r\n```\r\nredis_excluded_caches = {'cull', 'zero_cull'}\r\n...\r\n@override_settings(CACHES=caches_setting_for_tests(\r\n    base=RedisCache_params,\r\n    exclude=redis_excluded_caches\r\n))\r\nclass RedisCacheTests(BaseCacheTests, TestCase):\r\n    ...\r\n```\r\nNow only on test fails. Handling zero timeout. Redis-py does not support it natively and django expects to no set a key with zero timeout. I'm not sure at which level should this be handled. I was wondering if we perform the check in `get_backend_timeout` method and return a value (eg: None) or raise a suitable exception. \r\n",
      "@carltongibson I'm starting off with the documentation now. Should post updates in a few days!",
      "> If `redis-py` isn't installed, or if Redis isn't running we get a couple of errors:\r\n> \r\n> ```\r\n> django.core.cache.backends.base.InvalidCacheBackendError: Could not find backend 'django.core.cache.backends.redis.RedisCache': No module named 'redis'\r\n> ```\r\n> \r\n\r\nShould I move the `import redis` line inside the `__init__` method of the RedisCache class? All memcache backend do that and the error raised when a binding is not installed is like this\r\n```\r\nModuleNotFoundError: No module named 'pymemcache'\r\n```\r\nIncluding `import redis` at the top leads to an error message as you mentioned above.\r\nI wanted to move the import command like this\r\n```\r\nclass RedisCache(BaseCache):\r\n    def __init__(self, server, params):\r\n        import redis\r\n        super().__init__(params)\r\n        if isinstance(server, str):\r\n            self._servers = re.split('[;,]', server)\r\n        else:\r\n            self._servers = server\r\n\r\n        self._class = RedisCacheClient\r\n        self._options = params.get('OPTIONS') or {}\r\n```\r\nHowever, this would lead to some refactoring of code where `redis.ConnectionPool` etc are used.\r\nWhat do you think about this?\r\n",
      "> Should I move the `import redis` line inside the `__init__` method of the RedisCache class? All memcache backend do that and the error raised when a binding is not installed is like this\r\n\r\nYes, we should do something like this. Although with `pymemcache` it is done in `PyMemcacheCache.__init__()` because `pymemcache` provides the client class. We're writing the client class ourselves, so we want `import redis` in `RedisCacheClient.__init__()`.",
      "Hi @carltongibson. It's coming on nicely, yes.\r\n\r\nThese are some of the highlights of things addressed:\r\n\r\n- Fixed handling of `default` value with `.get()` allowing for use of a sentinel value in other operations. (Allows stored `None`.)\r\n  - See https://github.com/django/django/pull/14437#discussion_r656105087 for details.\r\n- Fixed handling of `timeout` values, notably delete/expire if `0` and persist if `None`. (Redis treats timeouts as we would expect.)\r\n  - See the thread at https://github.com/django/django/pull/14437#discussion_r658226217 for details.\r\n  - Also see https://github.com/django/django/pull/14437#discussion_r658806487 for more.\r\n- Fixed naming and ordering so things align with other backends, notably the memcached ones. (Easier maintenance.)\r\n- Fixed some bugs where `write=True` wasn't passed. Made `write` a keyword-only argument.\r\n- Implemented optimized `.delete_many()` and `.has_key()` operations instead of relying on the base class methods.\r\n- Used pipelines in `.set_many()` to optimize handling of timeouts.\r\n- Pinned to `redis >= 3.0.0` based on https://github.com/django/django/pull/14437#pullrequestreview-692861563.\r\n- Removed base serializer class - not much point. Can revisit if we need something other than pickle.\r\n- Removed half-baked support for importing client classes from string because:\r\n  - Subclassing the backend class and overriding a class attribute is better.\r\n  - We should be able to avoid adding additional client classes by passing options.\r\n  - We can always add this back if the implementation really needs it.\r\n  - See https://github.com/django/django/pull/14437#discussion_r657927858 for more commentary.\r\n\r\nThere are some outstanding comments that still need to be addressed:\r\n\r\n- [ ] As mentioned in https://github.com/django/django/pull/14437#pullrequestreview-691776531 some operations still need to be implemented:\r\n  - Implement atomic/optimized `.incr()` using `Redis.incr()` which uses the `INCRBY` command.\r\n  - Implement atomic/optimized `.decr()` using `Redis.decr()` which uses the `DECRBY` command.\r\n- [ ] As Redis supports timeouts in milliseconds, do we want to allow that? (What are your thoughts @carltongibson?)\r\n  - It would be nice as it has often come up as a complaint that memcached doesn't support this.\r\n  - We would still keep `timeout` in floating point seconds for compatibility, but scale up in `.get_backend_timeout()`.\r\n  - Then we would switch over from `ex` to `px` and `.expire()` to `.pexpire()`, etc.\r\n- [x] Redis has 16 logical databases, so, although most people will use the default, we should allow configuration of that.\r\n  - See https://github.com/django/django/pull/14437#discussion_r658856381 for more details.\r\n\r\nOther things that need attention:\r\n\r\n- [x] Documentation. I've only made brief comments, but it should be looked at thoroughly.\r\n- [x] Release notes. None have been added yet. This will be a headline feature, obviously.\r\n- [x] Tests. Currently we're piggybacking off the cross-backend tests nicely. But are there any gaps or other specific things to test?\r\n- [ ] How the connection pooling, etc. works. I haven't reviewed this at all yet.\r\n  - For example, there is a lot of parsing stuff, but can't we just use [`redis.Redis.from_url()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis.from_url) or [`redis.from_url()`](https://redis-py.readthedocs.io/en/stable/#redis.from_url)? üòâ \r\n- [ ] What `OPTIONS` do we explicitly want to handle before being passed on to the underlying library?\r\n  - We'll need to forward `**kwargs` at the very least so that users have the flexibility.\r\n  - Is there anything we want to set by default? Check out [`redis.Redis()`](https://redis-py.readthedocs.io/en/stable/#redis.Redis).\r\n\r\nOnce this is done, I think that it would be good to land this as a very simple backend that essentially mirrors what the memcached backends can do. So as long as we have one or multiple servers and that they can support UNIX sockets, IPv6, possibly TLS -- then we have a drop-in replacement.\r\n\r\nThat would be the first phase. Adding sharding, etc. could then be considered in a follow up PR. As long as we have a very simple implementation here and now, extending for other things should be easy. \r\n\r\nSome other thoughts:\r\n\r\n- We don't need to consider culling. That is only necessary for services that do not handle expiry themselves, e.g. databases.\r\n  - *Which is why it is not implemented for the memcached backends.*\r\n\r\nThanks @abbasidaniyal for all your efforts so far. This is coming along nicely.",
      "Thanks @pope1ni. That's super helpful! ",
      "Hey @pope1ni and @carltongibson \r\n\r\nAccording to the comments https://github.com/django/django/pull/14437#pullrequestreview-691776531, if we want to use the `redis.incr` or `redis.decr`, we would need to stop serializing the valus which are integers. I think this will create a mess as there would be too much to manual handling of values based on their types. As we are pickling the values, we can not directly use the `redis.incr` or `redis.decr` methods.\r\n\r\nI'm not sure how useful it is to support milli-second timeouts. `django-redis` has migrated to an approach which supports both seconds and milliseconds ( [refs](https://github.com/jazzband/django-redis/pull/508/files) ). \r\n\r\nWe can make the logical databases configurable via the url as well as a parameter in the options. I'll work on it.\r\n\r\n",
      "While using the `from_url` method, we can not provide `username` and `password` in the `OPTIONS`. Even if we pass them in the kwargs, [kwargs.update(...)](https://redis-py.readthedocs.io/en/stable/_modules/redis/connection.html#ConnectionPool.from_url) overrides it with the `username` and `password` from the URL, else it sets it to None. One solution is that we only allow username and password to be set using the \"LOCATION\" key only. Let me know what you feel. @pope1ni @carltongibson ",
      "The master branch of redis-py has updated the implementation of the `from_url` method. \r\nhttps://github.com/andymccurdy/redis-py/blob/627db540acd1f1f36db88290d74cbcd75f6bda0c/redis/connection.py#L951-L955\r\n\r\nHowever, the latest stable branch (3.5.3) still uses the old implementation.\r\n```\r\nif decode_components:\r\n    username = unquote(url.username) if url.username else None\r\n    password = unquote(url.password) if url.password else None\r\n    path = unquote(url.path) if url.path else None\r\n    hostname = unquote(url.hostname) if url.hostname else None\r\nelse:\r\n    username = url.username or None\r\n    password = url.password or None\r\n    path = url.path\r\n    hostname = url.hostname\r\n\r\nif url.scheme == 'unix':\r\n    url_options.update({\r\n                'username': username,\r\n                'password': password,\r\n                'path': path,\r\n                'connection_class': UnixDomainSocketConnection,    \r\n})\r\n\r\nelif url.scheme in ('redis', 'rediss'):\r\n    url_options.update({\r\n                'host': hostname,\r\n                'port': int(url.port or 6379),\r\n                'username': username,\r\n                'password': password,\r\n            })\r\n...\r\nkwargs.update(url_options)\r\n```\r\nThis will always override the username and password in the kwargs. I think for now, we can only support giving the `username` and `password` via the url and once redis-py's latest implementation is stable, we can add supplying  `username` and `password` via the `OPTIONS`.\r\n",
      "I was working on the documentation and I wanted to mention about customizing the `parser_class`, `pool_class` and `pickle_protocol`. However, I think I can only add examples of these configurations in the [Cache Arguements](https://docs.djangoproject.com/en/3.2/topics/cache/#cache-arguments) section. I was thinking if we can add a sub-section for the redis backend. We can do this for the other backends or can include the arguments section inside each backend's section? ",
      "Hey @abbasidaniyal ‚Äî¬†I think adding a sub-section is fine. _\"The Redis backend accepts the following additional arguments...\"_, or such. ",
      "Hey @carltongibson!\r\nI've added a couple of new sections to the documentation and I've added few tests for the custom methods that we've implemented. I might be missing on few additional tests, do let me know if you find anything. \r\n\r\nAs far as the documentation is concerned, I believe most of the key points are mentioned however, I feel it can be worded a little better. Looking forward to inputs on this!\r\n\r\nLast, I was wondering about how can we add a new ci job on djangoci for the redis cache backend. I couldn't find any documentation / resources about that. Let me know if there are any resources that I can refer too!\r\n\r\nThanks!",
      "Hey @carltongibson !\r\n\r\n> Then, can I ask you to squash this down as it is and rebase, and go through the [Patch review checklist](https://docs.djangoproject.com/en/3.2/internals/contributing/writing-code/submitting-patches/#patch-review-checklist) to make sure all the _Admin_ type things are in place.\r\n\r\nOkay I've squashed the commits down. The checklist seems to be fine. Let me know if you thinking something is missing!\r\n\r\n> I've created a [ticket-33012](https://code.djangoproject.com/ticket/33012), and assigned you. I've ticked all the flags: we can uncheck those as we're happy. (Note the number for commit message(s))\r\n\r\nI didn't understand the commit message(s) part? Could you please elaborate?\r\n\r\n> That leaves us free to finish off the rough edges. I'll keep playing with it. Looking forward to getting this in for Django 4.0! üíÉ\r\n\r\nYes looking forward to it! üòÑ ",
      "> I didn't understand the commit message(s) part? Could you please elaborate?\n\nI'll try answering this. Currently your commit message is `Refs #32508` which would be for this ticket. https://code.djangoproject.com/ticket/32508\n\nThe ticket created for your project is #33012 and so your commit message should be something like... `Fixed #33012 -- Added Redis cache backend` \n\nHope this helps, and great work on this feature. üëç",
      "Hey @smithdc1!\r\nIt seems I mixed up the ticket number with another ticket I was working onüòõ \r\nThanks! üòÉ ",
      "Hi - I work for Redis (the company), maintain aioredis-py, contribute to redis-py, and am a long-time Django user. I'm happy to offer any review help on this PR if needed!",
      "> Hi - I work for Redis (the company), maintain aioredis-py, contribute to redis-py, and am a long-time Django user. I'm happy to offer any review help on this PR if needed!\r\n\r\nYou're most welcome!",
      "> Hi - I work for Redis (the company), maintain aioredis-py, contribute to redis-py, and am a long-time Django user. I'm happy to offer any review help on this PR if needed!\r\n\r\nThat'll be great! Looking forward to your inputs! üòÑ ",
      "Hey @smithdc1! Thank you for the review.\r\n\r\n\r\n> I'm not so confident to comment on the main part with all the different options. I think my main observation there is about the structure, I find it a little hard to follow. There's many options here but here's one idea of how it could be structured.\r\n> \r\n> * Required settings\r\n> * settings which are available to all backends\r\n> * Redis specific items (optional)\r\n\r\nYes we could do that but that'll require restructing the whole documentation as there are some memcached specific items that are currently laid out in the examples section. In the future, I think such a restructuring would be nice where we can talk about cache specific arguements in a separate section.\r\n",
      "Hi all!\r\nI just in corporated the suggestions above. Thank you to everyone who's taking time out for reviewing this PR! \r\n\r\n>For example, I do want to know how to select the db, but I'm not sure I need to be told (here) what happens if I specify multiple >dbs... thinking (I do want to know where to go to find out all the details.)\r\n>\r\n>Does that make sense?\r\n\r\nYes this seems better!\r\n\r\n>Understood. I think I got sucked in by the \"most popular\" comment. Maybe we don't need that either? Could just say...\r\n>\"Redis is an in-memory database...\"\r\n\r\nThoughts on this @carltongibson?\r\n\r\n\r\n\r\n"
    ],
    "num_comments": 30,
    "repository": "django/django",
    "diff_length": 20570,
    "code_diff": "diff --git a/django/core/cache/backends/redis.py b/django/core/cache/backends/redis.py\nnew file mode 100644\nindex 000000000000..16556b1ded5c\n--- /dev/null\n+++ b/django/core/cache/backends/redis.py\n@@ -0,0 +1,224 @@\n+\"\"\"Redis cache backend.\"\"\"\n+\n+import random\n+import re\n+\n+from django.core.cache.bac"
  }
]