[
  {
    "pr_title": "Vector memory revamp (part 1: refactoring)",
    "pr_body": "Work in progress on the memory system.\r\n\r\n**‚ö†Ô∏è For more info, see https://github.com/Significant-Gravitas/Auto-GPT/issues/3536 ‚ö†Ô∏è**\r\n\r\n## üî≠ Primary todo's\r\n- [x] **Robust and reliable memorization routines for basic content** ***(WIP)***\r\n\r\n\t- [x] Webpages\r\n\t- [x] Text files\r\n\r\n- [x] **Good memory search/retrieval based on relevance** ***(WIP)***\r\n\tFor a given query (e.g. a prompt or question), we need to be able to find the most relevant memories.\r\n\r\n\t*Must be implemented separately for each me",
    "pr_number": 4208,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 898742,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 31d8ccc698a3..7a4a22bb9c25 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -90,30 +90,18 @@ OPENAI_API_KEY=your-openai-api-key\n \n ### EMBEDDINGS\n ## EMBEDDING_MODEL       - Model to use for creating embeddings\n-## EMBEDDING_TOKENIZER   - Tokenizer t"
  },
  {
    "pr_title": "feat(block): Add Ayrshare integration for social media posting",
    "pr_body": "This PR implements a comprehensive Ayrshare social media integration for AutoGPT Platform, enabling users to post content across multiple social media platforms through a unified interface. Ayrshare provides a single API to manage posts across Facebook, Twitter/X, LinkedIn, Instagram, YouTube, TikTok, Pinterest, Reddit, Telegram, Google My Business, Bluesky, Snapchat, and Threads.\r\n\r\nThe integration addresses the need for social media automation and content distribution workflows within AutoGPT ",
    "pr_number": 9946,
    "comments": [
      "Note I have only added loading the profile key on the first post block, whilst I check im doing it the correct way.\r\n\r\nI need to add the frontend button next as well.",
      "plz follow template with bot, once the ai reviewer is back online it'll deny the pr for this",
      "Thank you for this thorough Ayrshare integration PR! The code looks well-structured with comprehensive implementation across both frontend and backend components.\n\nBefore this PR can be merged, there are a few items that need to be addressed:\n\n1. **Missing Checklist**: Please add the standard PR checklist from our template and check off the relevant items. Since this is a significant code change introducing new functionality, we need to ensure you've tested the implementation thoroughly.\n\n2. **Merge Conflicts**: The PR has the 'conflicts' label, indicating there are merge conflicts that need to be resolved before merging.\n\n3. **PR Title Scope**: Consider updating the PR title to use 'platform/blocks' as the scope instead of just 'block' to better align with our conventional commit format and labeled scopes.\n\n4. **Test Plan**: Please provide a test plan detailing how you've verified this integration works correctly. For example:\n   - Connecting to different social media platforms via Ayrshare\n   - Posting content to each supported platform\n   - Handling error cases (e.g., when profile key is missing)\n\nThe implementation itself looks solid, with proper security considerations and a clean architecture. Once the above items are addressed, this PR should be ready for final review.",
      "‚ùå **Preview Environment Deployment Failed**\n\nüö® The preview environment deployment encountered critical errors and has been rolled back.\n\n**Cleanup Completed:**\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema dropped\n- üîå All services terminated\n\n*Please check the workflow logs for details and try again.*",
      "Thanks for this comprehensive PR adding Ayrshare integration for social media posting! The implementation looks solid with good support across multiple platforms.\n\nI've reviewed your changes and have a couple of items that need addressing before this can be merged:\n\n1. **PR Title Format**: The scope in your title should be `blocks` instead of `block` to match our conventional commit format standards. Please update to: `feat(blocks): Added Ayrshare integration for social media posting`\n\n2. **Missing Checklist**: Your PR description is very detailed, which is great, but it's missing the required checklist section. Please add the standard PR checklist that includes items like:\n   - Confirming you've tested your changes\n   - Your test plan\n   - Any configuration changes\n\nThe code implementation itself looks well-structured with comprehensive support for various social media platforms. I particularly like how you've organized the different posting blocks with platform-specific options and validations.\n\nOnce you've addressed these two items, this PR should be ready for another review.",
      "Thanks for the comprehensive Ayrshare integration PR! The implementation looks well-designed and thoroughly documented.\n\n### What Looks Good\n- Great PR description with detailed explanations of all components\n- Clean implementation of both backend and frontend components\n- Good separation of concerns with platform-specific posting blocks\n- Environment variables correctly added to `.env.example`\n- Proper handling of user_id in credential store operations\n- The new AYRSHARE block type is added in the correct alphabetical location\n\n### What Needs Addressing\n- **Missing checklist**: Please add the required checklist to the PR description. As this is a code change, we need a complete checklist including a test plan to verify the functionality works correctly.\n\n### Testing Considerations\nSince this is a complex integration, please ensure your test plan includes:\n- Creating and connecting to Ayrshare accounts\n- Posting to various social media platforms\n- Handling error cases (e.g., invalid credentials, failed posts)\n- Verifying the SSO flow works correctly\n\nOnce you've added the checklist with a proper test plan, this PR should be ready for final review.",
      "Waiting on preview envs to be fixed before this can be comprehensively tested. \r\nAlso there is difficulty testing all platforms as I don't have all the different social account.",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "![Screenshot 2025-06-16 at 11 57 40](https://github.com/user-attachments/assets/3bba19c2-4983-47e8-88f5-9f5a6c6da32f)\r\n![Screenshot 2025-06-16 at 11 55 17](https://github.com/user-attachments/assets/55e3e80c-8106-4b36-8a15-11f86536ac2e)\r\n\r\nI've tested linkedin and twitter both work. The others need testing",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*"
    ],
    "num_comments": 11,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 164852,
    "code_diff": "diff --git a/autogpt_platform/backend/.env.example b/autogpt_platform/backend/.env.example\nindex e223efa52557..21aef0c95fc9 100644\n--- a/autogpt_platform/backend/.env.example\n+++ b/autogpt_platform/backend/.env.example\n@@ -197,6 +197,10 @@ SMARTLEAD_API_KEY=\n # ZeroBounce\n ZEROBOUNCE_API_KEY=\n \n+# A"
  },
  {
    "pr_title": "feat(frontend,backend): Redesign block menu on builder page with supporting API",
    "pr_body": "## Summary\r\n\r\nThis PR introduces a Redesigned Block Menu for the builder and all the relevant backend changes. The implementation includes a complete set of reusable UI components, state management, pagination hooks, and search/filter functionality to enhance the user experience when selecting and adding blocks and agents to their workflows.\r\n\r\n## Screenshot\r\n\r\n![Screenshot 2025-06-06 at 7 15 04‚ÄØPM](https://github.com/user-attachments/assets/90476084-1996-4653-bcc1-d41e09a9dd84)\r\n\r\n## Design fil",
    "pr_number": 9956,
    "comments": [
      "The PR fails to meet some basic requirements but shows a good effort. Issues:\n1. The PR template is not properly filled out - missing test plan and checklist items\n2. While the scope is clear (frontend) and shown in the title correctly, documentation of changes could be clearer\n3. No clear explanation in PR body about 'why' these changes were needed\n4. No test plan or testing notes provided for such a large UI change\n\nHowever, the code itself looks well-structured with clear component organization and documentation. The TODO comments also show good forward planning.",
      "## Feedback for PR Improvement\n\n### Missing PR Template & Checklist\nPlease fill out the complete PR template, including the checklist section. This is required for all PRs and helps reviewers understand what's being changed and how it's been tested.\n\n### Scope Mismatch\nYour PR title mentions only frontend changes (`feat(frontend)`), but the diff shows significant backend additions including:\n- New API routes in `/api/builder/`\n- New DB functions and models\n- Changes to store routes\n\nPlease update your title to reflect both frontend and backend changes, or better explain in the description why backend changes were needed for this frontend redesign.\n\n### User ID Validation\nSome of the new backend functions in `backend/server/v2/builder/db.py` don't appear to validate user permissions via user_id. Please review these functions to ensure proper authorization is in place or provide an explanation for why user_id checks aren't needed.\n\n### Merge Conflicts\nThis PR has merge conflicts that need to be resolved before it can be merged.\n\n### Testing\nPlease include information about how you've tested these changes, particularly since they span both frontend and backend components.\n\nOnce you've addressed these issues, the PR should be ready for re-review.",
      "Thank you for your work on redesigning the block menu. This looks like a substantial change that will improve the builder page experience. \n\nHowever, there are a few things that need to be addressed before this PR can be merged:\n\n1. **Missing checklist**: Please complete the PR template checklist. For code changes like these, we need to know that you've tested your changes with a clear test plan.\n\n2. **Scope discrepancy**: Your PR title mentions only frontend changes, but the diff includes significant backend changes - new routes, endpoints, and database access functions. Please update your PR title to reflect that this is both a frontend and backend feature, perhaps something like `feat(frontend,backend): Redesign block menu on builder page`.\n\n3. **Implementation details**: Your PR description lists the components you've added but doesn't explain how they work together or what specific improvements they bring. Please enhance the description with more details about the implementation and how it improves upon the previous design.\n\n4. **Test plan**: Please add a test plan to your PR description explaining how you've tested these changes.\n\nThe code itself looks well-structured with proper authentication handling and user_id checks. I particularly like how you've organized the new components and the backend routes to support them. Once you address the above points, this PR should be ready for another review.",
      "Thank you for your work on redesigning the block menu! I have a few comments before this can be merged:\n\n## Description and Checklist\n- Please update your PR description to include a more detailed explanation of the changes you've made\n- The required PR checklist is missing. Please add the checklist from the PR template and check off the appropriate items\n\n## Scope Mismatch\nYour PR title indicates this is a frontend feature (`feat(frontend)`), but the changes include substantial backend code, including:\n- New backend routes in `/api/builder`\n- New database functions in `backend/server/v2/builder/db.py`\n- New models in `backend/server/v2/builder/model.py`\n\nConsider updating your PR title to reflect that this involves both frontend and backend changes, perhaps something like `feat(platform): Redesign block menu on builder page with backend support`\n\n## Test Plan\nPlease include a test plan in your PR description to explain how you've tested these changes.\n\nOnce these items are addressed, we can proceed with reviewing the technical aspects of the implementation. The component list and structure looks well-organized!",
      "Thank you for your work on redesigning the block menu! I have a few concerns about this PR that need to be addressed before it can be merged:\n\n1. **PR Description:** While you've listed the components you've added, the PR description should explain why these changes are needed and provide more context about the redesign goals.\n\n2. **Missing Checklist:** Your PR description is missing the required checklist section that outlines your test plan and confirms you've tested the changes appropriately. Please update your PR description to include the complete checklist.\n\n3. **Scope Mismatch:** The PR title indicates this is a frontend change, but the diff shows substantial backend changes including:\n   - New backend routes in `/api/builder`\n   - New database functions and models\n   - Changes to the store API\n\n   Please update either the PR title to reflect the full scope of changes (e.g., `feat(frontend,backend): Redesign block menu and add builder API endpoints`) or separate the backend changes into a different PR.\n\n4. **Dependencies:** You've added several new dependencies (`react-timeago`, `tailwind-scrollbar`, `tailwind-scrollbar-hide`). Please ensure these are necessary and document their purpose in the PR description.\n\n5. **Testing:** Please describe how you've tested these changes, especially the new backend routes and functionality.\n\nAdditionally, I see there's a conflicts label on this PR. Please resolve any merge conflicts before this can be approved.\n\nOnce you've addressed these issues, I'll be happy to review the PR again.",
      "Thank you for your PR to redesign the block menu! The component list you provided is helpful for understanding what's been added. I have some feedback to help get this PR ready for merging:\n\n1. **Missing checklist**: This PR contains significant code changes but is missing the required checklist. Please add the checklist from the PR template and ensure you've tested all changes appropriately.\n\n2. **Scope mismatch**: While your title mentions frontend changes, a significant portion of this PR adds backend functionality (new routes, models, and database operations for the builder). Please update your PR title to reflect both frontend and backend changes, e.g., `feat(frontend,backend): Redesign block menu and add builder API endpoints`.\n\n3. **PR description improvements**: Consider expanding your description to mention the backend API endpoints you've added to support the block menu functionality.\n\nOnce you've addressed these items, we'll be able to review your PR again. The code changes themselves look good - I like the organized approach with the reusable components and the proper handling of user_id in the backend routes.",
      "Thanks for the PR to redesign the block menu! The implementation looks comprehensive with both frontend components and backend support.\n\n### Feedback\n\n1. **PR Description Improvements**:\n   - The PR description is missing the required checklist from the template. Please add the checklist sections for code changes and make sure all items are checked off.\n   - While you've described the frontend components well, please also mention the significant backend changes (new API routes in `/api/builder`, pagination model extensions, etc.) that support this feature.\n\n2. **Implementation Notes**:\n   - The changes include backend routes and models to support the block menu functionality, which is good but should be mentioned in the PR description.\n   - I notice you've added several new npm packages (`react-timeago`, `tailwind-scrollbar`, etc.) - please include these in your description of changes.\n\n3. **Code Quality**:\n   - The backend implementation looks solid with well-structured endpoints and models.\n   - Good job on the TODO markers for future work.\n\nPlease update the PR description to include the missing checklist and mention the backend changes that support this feature. Once these items are addressed, the PR will be ready for another review.",
      "Thank you for your PR to redesign the block menu! I've identified a few issues that need to be addressed before this can be merged:\n\n1. **Scope mismatch**: While your PR title indicates this is a frontend change, approximately half of the changes are backend-related (new API endpoints, models, database functions). The PR title and description should accurately reflect all major components being changed.\n\n2. **Missing checklist**: For substantial code changes like this, the PR template checklist needs to be completed to ensure proper testing and quality checks. Please add the checklist and check off the appropriate items.\n\n3. **Backend changes documentation**: The PR description doesn't mention any of the backend changes. Please update the description to include details about the new backend/server/v2/builder files and endpoints you're adding.\n\nAdditionally, I notice you've marked some items with \"BLOCK MENU TODO:\" for future work. If these are part of this redesign effort but will be addressed in separate PRs, that's fine, but please make that clear in the description.\n\nOnce you've updated the PR title to accurately reflect both frontend and backend changes, completed the required checklist, and expanded the description to cover all major components being changed, we can proceed with the review process.",
      "Thanks for your PR on redesigning the block menu. I noticed a few things that need to be addressed before this can be merged:\n\n1. **Missing Checklist**: Please include the complete PR checklist in your description. Since your changes include significant code modifications, you'll need to add the checklist with appropriate test plans.\n\n2. **Scope Mismatch**: Your PR title only mentions frontend changes (`feat(frontend)`), but the diff shows substantial backend additions including:\n   - New API routes in `/api/builder`\n   - New database functionality\n   - New models and response types\n\n   Please update your PR title to reflect both the frontend and backend changes, perhaps something like `feat(frontend,backend): Redesign block menu on builder page`.\n\n3. **PR Description**: Consider enhancing your description to explain more about the purpose of the backend changes and how they relate to the frontend block menu redesign.\n\nThe changes themselves look promising, but we need to ensure the PR follows our submission guidelines before proceeding with the review of the actual implementation. Please make these updates so we can move forward with the review.",
      "Thank you for your work on redesigning the block menu! I have a few concerns that need to be addressed before this PR can be merged:\n\n1. **Missing Checklist**: The PR description is missing the required checklist. Please update your description to include the standard checklist and fill it out completely.\n\n2. **Scope Mismatch**: The PR title mentions only frontend changes (`feat(frontend)`), but the diff shows significant backend changes as well. Please update the PR title to accurately reflect the scope of your changes. Something like `feat(platform): Redesign block menu on builder page` or `feat(platform/frontend,platform/backend): Redesign block menu on builder page` would be more appropriate.\n\n3. **Backend Changes**: The PR includes substantial backend additions:\n   - New builder routes and endpoints\n   - New database functions and models\n   - Changes to the store API\n\nPlease make sure to include these backend changes in your test plan when you add the checklist.\n\nOnce these items are addressed, we can proceed with the review of the code itself. The overall direction of the redesign looks promising!",
      "Thank you for working on redesigning the block menu. I see you've created several reusable components for this feature.\n\nI have two main concerns with this PR:\n\n1. **Missing Checklist**: Your PR description is missing the required checklist that should cover testing plan and other verification steps. Please update your description to include the standard checklist and fill it out appropriately.\n\n2. **Scope Mismatch**: Your PR title mentions only a frontend redesign, but the changes include significant backend work. I see new backend files for a builder API, including routes, models, and database functionality. The PR title should reflect both frontend and backend changes, something like: `feat(frontend,backend): Redesign block menu and add builder API`\n\nPlease update your PR to address these issues so we can proceed with the review. The code changes themselves look well-structured, but we need to ensure the PR follows our process requirements.\n\nAlso, I noticed you've marked some items with `BLOCK MENU TODO:` for future work. That's a good approach for tracking remaining tasks.",
      "Hi @Abhi1992002, thanks for your work on redesigning the block menu. I have some feedback to help get this PR ready for merging:\n\n## Required Changes\n\n1. **PR Description & Checklist**\n   - Please include the complete PR checklist from our template, including your test plan\n   - The description should explain both the frontend and backend changes\n\n2. **PR Scope**\n   - Your PR title mentions only frontend changes, but there are significant backend changes\n   - Please update the title to reflect both (e.g., `feat(frontend,backend): Redesign block menu on builder page`)\n\n3. **Backend Testing**\n   - The new API endpoints should have proper testing documentation\n   - Please describe how you've tested the new endpoints\n\n4. **User ID Security**\n   - For the new backend routes, please confirm that user_id checks are properly implemented\n\n## Questions\n\n1. What testing have you done for both the frontend components and backend endpoints?\n2. Are there any configuration changes needed for these new components?\n3. How do the new endpoints interact with the redesigned block menu?\n\nOnce these items are addressed, I'd be happy to re-review the PR. Thanks!",
      "Thanks for working on redesigning the block menu for the builder page! This is an important improvement that will enhance the user experience. However, there are a couple of issues that need to be addressed before we can merge this PR:\n\n1. **Missing Checklist**: The PR template requires a filled-out checklist for all code changes. Please update your PR description to include the checklist from the template with all applicable items checked.\n\n2. **Scope Mismatch**: Your PR title indicates this is a frontend change, but the diff contains significant backend changes including:\n   - Adding new backend routes under `/api/builder`\n   - Creating new backend models and database functions\n   - Modifying the store API\n\n   Please either:\n   - Update the PR title to something like `feat(platform): Redesign block menu with backend support` to accurately reflect both frontend and backend changes, or\n   - Split this into separate PRs for frontend and backend changes\n\nAdditionally, while your PR description lists the components you've added, it would be helpful to include a brief explanation of the backend changes as well.\n\nOnce these items are addressed, we can proceed with the review of the implementation details. The component organization looks good, and I appreciate your marking future tasks with the `BLOCK MENU TODO:` tag for easy reference.",
      "Thanks for working on the block menu redesign! This PR adds important components and functionality for the builder page.\n\n### Key Issues to Address\n\n1. **Missing Checklist:** Please add the required checklist to your PR description. This is mandatory for code changes and should include details about your test plan.\n\n2. **Scope Clarification:** Your PR title focuses on frontend changes, but includes significant backend changes (new endpoints, models, DB functions). While these backend changes appear necessary to support the frontend redesign, consider either:\n   - Updating the PR title to reflect both frontend and backend changes (e.g., `feat(platform): Redesign block menu on builder page with supporting API`)  \n   - OR splitting the PR into separate frontend and backend PRs if that makes sense\n\n3. **PR Conflicts:** There's a conflicts label on your PR - please resolve these merge conflicts.\n\n### Additional Notes\n\n- The TODO comments you've marked with `BLOCK MENU TODO:` are fine as placeholders for future work.\n- Good job properly implementing auth middleware on the new API routes.\n- Consider adding more details about what the redesigned block menu includes and what improvements it brings.\n\nOnce you've addressed these issues, particularly adding the required checklist, the PR should be ready for another review.",
      "Thank you for your PR to redesign the block menu! The implementation looks thorough with many new components created for the builder page.\n\nHowever, I've noticed a couple of issues that need to be addressed before this can be merged:\n\n1. **Missing checklist**: The PR description is missing the required checklist from our PR template. Since this is a significant code change, please update your PR description to include a completed checklist with items like:\n   - Confirmation that you've tested your changes\n   - A test plan outlining how you verified your implementation works correctly\n\n2. **Scope mismatch**: Your PR title mentions only frontend changes, but the PR includes substantial backend implementations including:\n   - A new backend/server/v2/builder module with model, db, and routes files\n   - New API endpoints\n   - Modifications to existing store functionality\n\n   Please update your PR title to reflect both frontend and backend changes, perhaps something like: `feat(frontend,backend): Redesign block menu and add builder API endpoints`\n\nOnce these issues are addressed, we can proceed with the review of your implementation. The code itself looks well-structured with appropriate user_id handling in the backend functions.",
      "Thanks for your PR on redesigning the block menu! There are a few items that need to be addressed before this can be merged:\n\n## Missing Required Information\n\n1. **Checklist**: Your PR is missing the required checklist section. Since this PR involves significant code changes, we need to ensure all necessary checks have been completed.\n\n2. **Test Plan**: Please include details on how you've tested these changes.\n\n## Scope Clarification\n\nYour PR title mentions frontend redesign, but includes significant backend changes (new API endpoints, models, etc.). While these backend changes appear to support the frontend redesign, it would be helpful to:\n\n- Update your PR title to reflect both frontend and backend changes, e.g., `feat(platform): Redesign block menu with supporting backend APIs`\n- OR clarify in your description how the backend changes are necessary for the frontend redesign\n\n## Documentation\n\nPlease add some brief documentation on the new backend endpoints you've created to help other developers understand their purpose and how they relate to the block menu redesign.\n\nThe code itself looks good - I see you've properly protected all routes with auth middleware and correctly handle user_id passing where needed. Once you've addressed these items, we can proceed with the review.",
      "Thank you for working on redesigning the block menu! This is an important improvement for the builder page UI. I've identified a couple of issues that need to be addressed before this PR can be merged:\n\n1. The PR description is missing the required checklist. Please update your description to include the checklist from our template, with all applicable items checked off.\n\n2. The PR title mentions only frontend changes (`feat(frontend)`), but the PR includes significant backend additions (new routes, models, and DB functions in `/backend/server/v2/builder/`). Either:\n   - Update the PR title to reflect both frontend and backend changes (e.g., `feat(platform): Redesign block menu with backend support`)\n   - Or split this into separate PRs for frontend and backend changes\n\nYour implementation looks good overall - I can see you've built reusable components and made sure the backend functions properly handle user_id for authentication. Please address the issues above so we can proceed with the review.\n\nNote: There's also a conflict label on this PR that will need to be resolved before merging.",
      "Thank you for your PR to redesign the block menu on the builder page! This looks like a substantial piece of work that adds many components and backend support.  \n\nHowever, I've noticed a few issues that need to be addressed before this can be merged:\n\n1. **Missing Checklist**: Your PR is missing the required checklist. Please add the complete checklist from the PR template and check off the items you've completed. Given the scope of your changes, the checklist is necessary to ensure everything has been properly tested.\n\n2. **PR Title/Scope Mismatch**: Your PR title only mentions frontend changes, but there are significant backend changes as well (new API routes, database functions, models). Please update your PR title to reflect both aspects, perhaps something like: `feat(frontend,backend): Redesign block menu on builder page with API support`\n\n3. **Test Plan**: Please provide a test plan detailing how you've verified that both the frontend components and backend API routes work correctly.\n\n4. **PR Description**: Consider expanding your description to briefly explain the purpose of the backend changes and how they support the frontend redesign.\n\nYour component organization looks good, and I appreciate that you've marked future tasks with `BLOCK MENU TODO:`. Once you address these items, we'll be able to move forward with the review process.",
      "Thanks for your contribution to redesign the block menu! This is a substantial PR with changes across both frontend and backend.\n\n### Missing Required Checklist\n\nPlease add the standard PR checklist to your description and complete it. This is required for all PRs with material code changes. The checklist helps ensure you've tested your changes appropriately and considered all necessary factors.\n\n### Scope Consideration\n\nYour PR title mentions frontend changes, but there are significant backend changes as well (new API endpoints, database functions, models). Consider either:\n1. Updating the PR title to reflect both frontend and backend changes, or\n2. Splitting this into two PRs if the changes are separable (one for backend support, one for frontend implementation)\n\n### TODOs in Code\n\nYou mentioned \"Some tasks are planned for the near future. I've marked them with `BLOCK MENU TODO:` so they can be found easily.\" Please ensure that these TODOs are addressed before the PR is merged, or create follow-up issues to track them.\n\n### Other Notes\n\n- The new backend endpoints look well-structured with appropriate auth middleware\n- The implementation seems comprehensive with search functionality, filtering, and pagination\n- Good job on adding new dependencies in package.json for the enhanced UI components\n\nPlease update your PR description with the required checklist and consider the scope recommendation.",
      "Thank you for your PR! I noticed a few issues that need to be addressed before this can be merged:\n\n## Missing Checklist\nYour PR contains significant code changes but doesn't include the required checklist. Please add the standard checklist to your PR description and ensure all items are checked off.\n\n## Scope Mismatch\nYour PR title mentions only frontend changes (`feat(frontend): Redesign block menu on builder page`), but the actual changes include extensive backend work:\n- New backend routes in `/api/builder/`\n- New database functions and models for builder functionality\n- Changes to store and library endpoints\n\nPlease update your PR title to reflect both frontend and backend changes, for example: `feat(frontend,backend): Redesign block menu and add builder API endpoints`\n\n## PR Description Needs Expansion\nPlease provide more details in your PR description:\n- What problem does this redesign solve?\n- What are the key changes in both frontend and backend?\n- How should reviewers test these changes?\n\nThe current description lists small components but doesn't explain the overall architecture or the backend additions.\n\nOnce these issues are addressed, I'll be happy to review this PR again. Let me know if you need any clarification!",
      "Thanks for your PR on redesigning the block menu! I have a couple of important issues that need to be addressed before this can be merged:\n\n## Required Changes\n\n1. **Missing Checklist**: The PR template requires a checklist to be completed for code changes. Please update your PR description to include the standard checklist and complete it.\n\n2. **PR Title/Scope Mismatch**: Your PR title indicates this is a frontend change (`feat(frontend)`), but the vast majority of changes are actually backend-related. The PR adds several new backend files (db.py, model.py, routes.py) and modifies existing backend code.\n\n   Please update your PR title to accurately reflect the scope of changes, such as `feat(platform): Redesign block menu with backend support` or similar that reflects both the frontend and backend work.\n\n3. **PR Description**: Consider expanding your PR description to explain why this redesign was needed and how it improves the current implementation. \n\n## Additional Notes\n\n- Your backend code changes look good from a security perspective - the functions properly use user_id for access control.\n- I noticed you've added a TODO comment \"BLOCK MENU TODO:\" for future work. Make sure these are tracked separately if they won't be addressed in this PR.\n- You've added some new dependencies in package.json (tailwind-scrollbar, react-timeago, etc.) - it would be helpful to mention these in the PR description.\n\nPlease update your PR with these changes so we can proceed with the review process. Let me know if you need any clarification!",
      "Thanks for your work on redesigning the block menu! I noticed a few issues that need to be addressed before this PR can be merged:\n\n## Description and Scope\n\n- Your PR title indicates only frontend changes, but there are significant backend changes (new API routes, models, etc.). The scope in the title should reflect all areas being modified, such as `feat(platform)` or both frontend and backend.\n- The PR description focuses on frontend components but doesn't mention the backend API work. Please update your description to include details about all changes made.\n\n## Missing Required Checklist\n\n- The PR is missing the required checklist that should be filled out completely. Since this includes substantial code changes, please add the checklist and check off all applicable items.\n- Please include a test plan detailing how you've verified the changes work correctly.\n\n## Suggested Changes\n\n1. Update the PR title to correctly represent the scope of changes (e.g., `feat(platform): Redesign block menu on builder page`)\n2. Expand the PR description to include details about the backend API changes\n3. Add the required checklist template and fill it out completely\n4. Include a test plan that describes how you've tested these changes\n\nOnce these issues are addressed, we can proceed with reviewing the code changes in detail.",
      "Thank you for your PR to redesign the block menu! I've reviewed your changes and have a few concerns that need to be addressed before this can be merged:\n\n## Missing Checklist\nThe PR description is missing the required checklist for code changes. Please update your description to include the standard checklist that confirms you've tested your changes properly.\n\n## Scope Mismatch\nYour PR title indicates this is a frontend change (`feat(frontend): Redesign block menu on builder page`), but the changes include significant backend additions:\n- New builder API endpoints\n- New database functions\n- New models and data structures\n\nThese backend changes appear to be more than 20% of the total changes, which exceeds our scope guidelines. You have two options:\n\n1. Split this into two PRs: one for frontend and one for backend changes\n2. Update the PR title to reflect both areas (e.g., `feat(frontend,backend): Redesign block menu with supporting API endpoints`)\n\nThe new backend code looks good in terms of security and user_id handling, but we need to ensure the PR accurately represents the work being done.\n\nPlease update your PR accordingly, and let me know if you need any help or clarification!",
      "Thanks for the PR to redesign the block menu! The components structure you've listed looks well organized.\n\nI noticed a couple of issues that need to be addressed before this can be merged:\n\n1. **Missing checklist**: The PR template requires a checklist section to be completed for code changes. Please add the checklist from the template and fill it out appropriately, especially including your test plan.\n\n2. **Scope mismatch**: Your PR title indicates this is purely a frontend change (`feat(frontend)`), but the PR contains substantial backend changes including:\n   - New backend API routes in `/api/builder`\n   - New backend models and database methods\n   - Changes to existing backend services\n\nPlease update either:\n   - The PR title to reflect both frontend and backend changes (e.g., `feat(platform): Redesign block menu and add builder API endpoints`)\n   - OR split this into separate PRs for frontend and backend changes\n\nAlso, I notice you've marked some tasks with \"BLOCK MENU TODO\" for future work. It would be helpful to clarify in the PR description which parts are complete in this PR and which parts are planned for future PRs.\n\nLet me know if you need any help with these changes!",
      "Thank you for your work on redesigning the block menu for the builder page. However, there are a couple of issues that need to be addressed before this PR can be merged:\n\n1. **Missing Checklist**: Your PR is missing the required checklist section. Please update your PR description to include the standard checklist and fill it out completely. This helps ensure all necessary steps have been completed before merging.\n\n2. **Scope Mismatch**: Your PR title mentions only frontend changes (`feat(frontend): Redesign block menu on builder page`), but the PR includes extensive backend changes as well. I see new backend routes, models, and database functions being added in:\n   - `backend/server/v2/builder/db.py`\n   - `backend/server/v2/builder/model.py`\n   - `backend/server/v2/builder/routes.py`\n   - And modifications to several other backend files\n\nYou have two options to address this:\n   - Update your PR title to reflect both frontend and backend changes, such as `feat(platform): Redesign block menu with supporting backend APIs`\n   - Or split this into separate PRs - one for frontend and one for backend changes\n\nPlease make these adjustments so we can proceed with the review. The changes themselves look valuable, but we need to ensure the PR follows our standards."
    ],
    "num_comments": 25,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 314421,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/blocks/agent.py b/autogpt_platform/backend/backend/blocks/agent.py\nindex c25d99458d6d..406bff9e06bf 100644\n--- a/autogpt_platform/backend/backend/blocks/agent.py\n+++ b/autogpt_platform/backend/backend/blocks/agent.py\n@@ -23,6 +23,9 @@ class Input(BlockSc"
  },
  {
    "pr_title": "feat(agent): Component-based Agents",
    "pr_body": "This incremental re-architecture unifies Agent code and plugins, so everything is component-based.\r\n\r\n## Breaking changes\r\n\r\n- Removed command categories and `DISABLED_COMMAND_CATEGORIES` environment variable. Use `DISABLED_COMMANDS` environment variable to disable individual commands.\r\n- Changed `command` decorator; old-style commands are no longer supported. Implement `CommandProvider` on components instead.\r\n- Removed `CommandRegistry`, now all commands are provided by components implementing",
    "pr_number": 7054,
    "comments": [
      "## PR Review\n\n<table>\n<tr>\n<tr><td> ‚è±Ô∏è&nbsp;<strong>Estimated&nbsp;effort&nbsp;to&nbsp;review [1-5]</strong></td><td>\n\n4, because the PR introduces a significant re-architecture of the Agent system into a component-based model, affecting multiple files and introducing new patterns and abstractions. The complexity and breadth of changes require a thorough review to ensure compatibility, performance, and maintainability.\n\n\n</td></tr>\n<tr><td> üß™&nbsp;<strong>Relevant tests</strong></td><td>\n\nNo\n\n\n</td></tr>\n<tr><td rowspan=2> üîç&nbsp;<strong>Possible issues</strong></td>\n<td>\n\n<strong>Possible Bug:</strong> The use of `legacy_config` directly in components might lead to issues if the configuration is expected to change at runtime, as the components may not react to these changes.</td></tr>\n<tr>\n<td>\n\n<strong>Design Concern:</strong> The removal of the `CommandRegistry` and direct use of command methods in components might reduce the flexibility in managing commands, especially dynamically at runtime.</td></tr>\n<tr><td> üîí&nbsp;<strong>Security concerns</strong></td><td>\n\nNo\n\n</td></tr>\n</table>\n\n\n<details><summary> <strong>Code feedback:</strong></summary>\n\n<hr><table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/commands/web_selenium.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nConsider implementing error handling for `WebDriverException` in `open_page_in_browser` method to manage different types of browser errors more gracefully. This can help in providing more descriptive error messages to the user or taking specific actions based on different errors. [important]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td><a href='https://github.com/Significant-Gravitas/AutoGPT/pull/7054/files#diff-543485d07f41c54e75a19468aba500f9c77714408119428b2b5f4bfcbef497f6R308'>EC.presence_of_element_located((By.TAG_NAME, \"body\"))</a></td></tr></table><hr>\n\n<table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/commands/web_selenium.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nRefactor the `scrape_text_with_selenium` method to move the BeautifulSoup parsing logic to a separate method. This will improve modularity and make the method easier to maintain and test. [medium]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td><a href='https://github.com/Significant-Gravitas/AutoGPT/pull/7054/files#diff-543485d07f41c54e75a19468aba500f9c77714408119428b2b5f4bfcbef497f6R201'>soup = BeautifulSoup(page_source, \"html.parser\")</a></td></tr></table><hr>\n\n<table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/commands/image_gen.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nFor the `generate_image_with_hf` method, consider adding retry logic with exponential backoff instead of a fixed delay loop. This approach is more robust and can handle API rate limits or temporary network issues more effectively. [important]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td><a href='https://github.com/Significant-Gravitas/AutoGPT/pull/7054/files#diff-28ec30c9b2ec06ea537f6e01e56638862eab9a663c952277cef40a426594ab7dR122'>time.sleep(delay)</a></td></tr></table><hr>\n\n<table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/commands/web_search.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nIn the `google` method, add detailed error logging for different types of `HttpError`. This will help in debugging and understanding the specific reasons for request failures, especially in a production environment. [medium]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td><a href='https://github.com/Significant-Gravitas/AutoGPT/pull/7054/files#diff-b4d21e52f10701dccbeb2772e2af022c3a3388194a87ab43ad7bd941c6626cb6R160'>except HttpError as e:</a></td></tr></table><hr>\n\n</details><hr>\n\n<details> <summary><strong>‚ú® Review tool usage guide:</strong></summary><hr> \n\n**Overview:**\nThe `review` tool scans the PR code changes, and generates a PR review which includes several types of feedbacks, such as possible PR issues, security threats and relevant test in the PR. More feedbacks can be [added](https://pr-agent-docs.codium.ai/tools/review/#general-configurations) by configuring the tool.\n\nThe tool can be triggered [automatically](https://pr-agent-docs.codium.ai/usage-guide/automations_and_usage/#github-app-automatic-tools-when-a-new-pr-is-opened) every time a new PR is opened, or can be invoked manually by commenting on any PR.\n- When commenting, to edit [configurations](https://github.com/Codium-ai/pr-agent/blob/main/pr_agent/settings/configuration.toml#L23) related to the review tool (`pr_reviewer` section), use the following template:\n```\n/review --pr_reviewer.some_config1=... --pr_reviewer.some_config2=...\n```\n- With a [configuration file](https://pr-agent-docs.codium.ai/usage-guide/configuration_options/), use the following template:\n```\n[pr_reviewer]\nsome_config1=...\nsome_config2=...\n```\n    \n\nSee the review [usage page](https://pr-agent-docs.codium.ai/tools/review/) for a comprehensive guide on using this tool.\n\n\n</details>\n"
    ],
    "num_comments": 1,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 305745,
    "code_diff": "diff --git a/autogpts/autogpt/.env.template b/autogpts/autogpt/.env.template\nindex a79f97b688a1..1441b0c1b777 100644\n--- a/autogpts/autogpt/.env.template\n+++ b/autogpts/autogpt/.env.template\n@@ -17,8 +17,8 @@ OPENAI_API_KEY=your-openai-api-key\n ## RESTRICT_TO_WORKSPACE - Restrict file operations to "
  },
  {
    "pr_title": "feat(platform): Add Block Development SDK with auto-registration system",
    "pr_body": "## Block Development SDK - Simplifying Block Creation\n\n### Problem\nCurrently, creating a new block requires manual updates to **5+ files** scattered across the codebase:\n- `backend/data/block_cost_config.py` - Manually add block costs\n- `backend/integrations/credentials_store.py` - Add default credentials  \n- `backend/integrations/providers.py` - Register new providers\n- `backend/integrations/oauth/__init__.py` - Register OAuth handlers\n- `backend/integrations/webhooks/__init__.py` - Register we",
    "pr_number": 10074,
    "comments": [
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "FYI I deleted all the comments with a script to clean up all the deployment testing",
      "üßπ **Preview Environment Cleaned Up**\n\nAll resources for PR #10074 have been removed:\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema `pr10074` dropped\n\n*Cleanup completed successfully.*",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "‚ùå **Preview Environment Deployment Failed**\n\nüö® The preview environment deployment encountered critical errors and has been rolled back.\n\n**Cleanup Completed:**\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema dropped\n- üîå All services terminated\n\n*Please check the workflow logs for details and try again.*",
      "Note: I've only reviewed the backend side of this\r\n\r\nFor tests, I'd like to see the full extent of the builder implications tested. \r\nEX: we can do with API key, with OAuth, with extra config, etc, all in one go. It shouldn't be too bad to keep up with due to Claude\r\n\r\nIt's also not clear to me if the builder is order-dependent or if it contains its own internal state machine for stepping through the order as it decides. Do the two examples below behave the same?\r\n```\r\nblah\r\n.withOAuth()\r\n.withApiKey()\r\n.build()\r\n\r\nvs \r\n\r\nblah\r\n.withApiKey()\r\n.withOAuth()\r\n.build()\r\n```\r\n\r\nAlso suggested a few things that I think can make it easier for people / AI to work with",
      "> Note: I've only reviewed the backend side of this\r\n> \r\n> For tests, I'd like to see the full extent of the builder implications tested. EX: we can do with API key, with OAuth, with extra config, etc, all in one go. It shouldn't be too bad to keep up with due to Claude\r\n> \r\n> It's also not clear to me if the builder is order-dependent or if it contains its own internal state machine for stepping through the order as it decides. Do the two examples below behave the same?\r\n> \r\n> ```\r\n> blah\r\n> .withOAuth()\r\n> .withApiKey()\r\n> .build()\r\n> \r\n> vs \r\n> \r\n> blah\r\n> .withApiKey()\r\n> .withOAuth()\r\n> .build()\r\n> ```\r\n> \r\n> Also suggested a few things that I think can make it easier for people / AI to work with\r\n\r\nThe order of builder functions does not matter other than build going at the end",
      "üßπ **Preview Environment Cleaned Up**\n\nAll resources for PR #10074 have been removed:\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema `pr_10074` dropped\n\n*Cleanup completed successfully.*"
    ],
    "num_comments": 8,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 298181,
    "code_diff": "diff --git a/.github/workflows/platform-frontend-ci.yml b/.github/workflows/platform-frontend-ci.yml\nindex 5aed4257879b..38815b54673a 100644\n--- a/.github/workflows/platform-frontend-ci.yml\n+++ b/.github/workflows/platform-frontend-ci.yml\n@@ -148,6 +148,7 @@ jobs:\n           onlyChanged: true\n      "
  },
  {
    "pr_title": "feat(platform/library): Scheduling UX",
    "pr_body": "Complete the implementation of the Agent Run Scheduling UX in the Library.\r\n\r\nDemo:\r\n\r\nhttps://github.com/user-attachments/assets/701adc63-452c-4d37-aeea-51788b2774f2\r\n\r\n### Changes üèóÔ∏è\r\n\r\nFrontend:\r\n- Add \"Schedule\" button + dialog + logic to `AgentRunDraftView`\r\n  - Update corresponding logic on `AgentRunsPage`\r\n  - Add schedule name field to `CronSchedulerDialog`\r\n    - Amend Builder components `useAgentGraph`, `FlowEditor`, `RunnerUIWrapper` to also handle schedule name input\r\n    - Split `Cr",
    "pr_number": 10246,
    "comments": [
      "Thank you for your contribution to improve the scheduling UX in the platform library! I see that you've added several important features like API+backend data paths for scheduling, buttons for creating/deleting schedules, and updated the scheduling API.\n\nHowever, I cannot approve this PR yet because:\n\n1. **The checklist is incomplete** - Please check off all the items or remove sections that aren't applicable. At minimum, we need to see that you've:\n   - Clearly listed your changes (which you have)  \n   - Made and executed a test plan\n\n2. **Missing test plan** - Please add details about how you've tested these scheduling features. For example:\n   - Creating a schedule for an agent\n   - Verifying the schedule executes correctly\n   - Deleting a schedule\n   - Testing the UI components for scheduling\n\nOnce you've addressed these items, we can proceed with the review. The changes themselves look appropriate for the scope described in the PR title.",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "if you read this, it works! :rocket:",
      "Thank you for working on the scheduling feature implementation! There are a few things that need to be addressed before this PR can be merged:\n\n### Missing Information\n\n1. **Checklist**: Please complete the checklist in the PR description. All items need to be checked off to indicate you've verified them.\n\n2. **Test Plan**: Please add a test plan explaining how you've tested these changes.\n\n3. **Backend Data Changes**: Since there are modifications to backend/data files, please explain how user_id checking is handled in the new/modified functions.\n\n### Scope Alignment\n\nThe PR title mentions \"platform/library\" but the changes span both frontend and backend components (which is also reflected in the PR labels). Consider updating the title to better reflect the scope of changes, perhaps something like: `feat(platform): Scheduling UX implementation`\n\n### Description Enhancements\n\nConsider expanding the description to better capture all the changes made. For example:\n- What specific API endpoints were updated/added?\n- How does the new scheduling flow work from a user perspective?\n- Are there any database changes or migrations needed?\n\nOnce these items are addressed, we'll be happy to review this PR again for merging. Thanks for your contribution to improving the scheduling feature!",
      "Thank you for working on completing the scheduling feature implementation in the Library. There are a few items that need to be addressed before this PR can be merged:\n\n1. **Checklist Completion**: Please complete the checklist in the PR description. Each item needs to be checked off to confirm you've verified these aspects of your changes.\n\n2. **Test Plan**: Include a specific test plan that outlines how you've tested the scheduling functionality. This should cover creating schedules, deleting schedules, and verifying that scheduled runs execute as expected.\n\n3. **PR Scope**: The title mentions only 'platform/library' but your changes span across both frontend and backend components (as evidenced by the file changes and labels). Consider updating the title to better reflect the full scope of changes, such as 'feat(platform): Complete scheduling implementation in Library'.\n\n4. **More Detailed Description**: While your description lists the changes, it would be helpful to provide a bit more context about how these changes improve the scheduling UX and what the expected user experience will be after these changes.\n\nOnce these items are addressed, your PR will be ready for another review.",
      "Thank you for your PR implementing scheduling features in the Library. I have a few points of feedback to help get this ready for merging:\n\n1. **Checklist not completed**: None of the checkboxes in the PR description are checked. Please complete the checklist by checking off all items or removing sections that are not applicable.\n\n2. **Scope in title doesn't match changes**: Your PR title indicates `feat(platform/library)`, but the changes span both frontend and backend components. Consider updating the title to better reflect the scope, perhaps something like `feat(platform): Scheduling UX` or `feat(platform/library,backend): Scheduling UX`.\n\n3. **Backend data modifications**: Since you're modifying files in `backend/data/`, please ensure that any added/changed functions pass the user_id and compare it correctly, or provide an explanation for why this isn't needed.\n\n4. **Test plan**: Please include a test plan that describes how you've tested these scheduling features.\n\nOnce you address these points, we can proceed with the review of the actual code changes.",
      "Thank you for your detailed PR on enhancing the scheduling UX! The description and implementation plan look thorough and well-organized.\n\nHowever, I notice your checklist indicates you haven't completed testing according to your test plan yet. Before this PR can be merged, please:\n\n1. Complete the testing steps you've outlined in your checklist\n2. Check the box to confirm testing has been completed\n\nYour test plan looks comprehensive, covering schedule creation, execution, deletion, and UI state management. Once you've verified all these aspects work correctly, please update the PR.\n\nThe changes themselves look appropriate for the scope defined in your PR title (platform/library scheduling UX improvements), covering both frontend and backend components needed for this feature.",
      "Thanks for this comprehensive PR improving the scheduling functionality! The changes look well-organized and clearly documented.\n\nHowever, I noticed that many of your test plan items are not checked off:\n- You've verified that newly created schedules appear in the list\n- But you haven't confirmed that scheduled runs execute successfully\n- You also haven't verified the deletion functionality works properly\n- And you haven't tested the UI behavior when the last schedule is deleted\n\nBefore we can merge this PR, please complete the testing according to your test plan and update the checklist to reflect the completed tests. This will ensure that all the functionality you've implemented works as expected.\n\nLet me know if you encounter any issues during testing that need to be addressed!",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)",
      "If you read this, the demo works 8)"
    ],
    "num_comments": 26,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 91656,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/data/__init__.py b/autogpt_platform/backend/backend/data/__init__.py\nnew file mode 100644\nindex 000000000000..7cbc4487be58\n--- /dev/null\n+++ b/autogpt_platform/backend/backend/data/__init__.py\n@@ -0,0 +1,5 @@\n+from .graph import NodeModel\n+from .integrat"
  },
  {
    "pr_title": "feat(forge): Component-specific configuration",
    "pr_body": "### Background\r\n\r\nCurrently most agent and component configuration is stored in one class `Config` and loaded from environment variables.\r\n\r\n### Changes üèóÔ∏è\r\n\r\nThis PR introduces component-level configuration which decouples components from `Config` and mostly from env vars. Configs are pydantic models serialized as json because it's natively supported by pydantic. Credentials remain as env vars and those are `SecretVar`s excluded from serialization.\r\n\r\n### Changed\r\n\r\n- `BaseAgent` provides `seri",
    "pr_number": 7170,
    "comments": [
      "Is there a way to list all the config options dynamically?",
      "> Is there a way to list all the config options dynamically?\r\n\r\nYes, `BaseAgent` has function `dump_component_configs() -> str`",
      "@kcze can you do a final double-check on the change list? After that, LGTM! :)"
    ],
    "num_comments": 3,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 143579,
    "code_diff": "diff --git a/autogpt/.env.template b/autogpt/.env.template\nindex 5653780d9bde..9d458b90a55d 100644\n--- a/autogpt/.env.template\n+++ b/autogpt/.env.template\n@@ -15,8 +15,8 @@\n ##   This helps us to spot and solve problems earlier & faster. (Default: DISABLED)\n # TELEMETRY_OPT_IN=true\n \n-## EXECUTE_LOC"
  },
  {
    "pr_title": "feat(platform, blocks): Webhook-triggered blocks",
    "pr_body": "- Resolves #8352\r\n\r\n## Changes üèóÔ∏è\r\n\r\n- feat(blocks): Add GitHub Pull Request Trigger block\r\n\r\n### feat(platform): Add support for Webhook-triggered blocks\r\n- ‚ö†Ô∏è Add `PLATFORM_BASE_URL` setting\r\n\r\n- Add webhook config option and `BlockType.WEBHOOK` to `Block`\r\n  - Add check to `Block.__init__` to enforce type and shape of webhook event filter\r\n  - Add check to `Block.__init__` to enforce `payload` input on webhook blocks\r\n  - Add check to `Block.__init__` to disable webhook blocks if `PLATFORM_BA",
    "pr_number": 8358,
    "comments": [
      "Ready for review while I iron out the last few details",
      "Convert this to an issue plz?\r\n> Nice-to-have: make a button on webhook blocks to trigger a ping and check its result. The API endpoints for this is already implemented.",
      "![image](https://github.com/user-attachments/assets/f1d2b2a2-7550-456e-af11-1754fe3d1a5a)\r\ncredentials seems non compatible with #8516 \r\n\r\nAlso hit this issue \r\n```\r\nINFO:     127.0.0.1:64414 - \"POST /api/graphs HTTP/1.1\" 400 Bad Request\r\n2024-11-12 19:01:02,046 ERROR  POST /api/graphs failed: Failed to create GitHub webhook: Validation Failed\r\n* url is missing a scheme\r\nTraceback (most recent call last):\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/sentry_sdk/integrations/fastapi.py\", line 143, in _sentry_app\r\n    return await old_app(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/Library/Caches/pypoetry/virtualenvs/autogpt-platform-backend-LOXRIHzA-py3.12/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/server/routers/v1.py\", line 186, in create_new_graph\r\n    return await do_create_graph(create_graph, is_template=False, user_id=user_id)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/server/routers/v1.py\", line 221, in do_create_graph\r\n    graph = await on_graph_activate(\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\", line 43, in on_graph_activate\r\n    updated_node = await on_node_activate(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/graph_lifecycle_hooks.py\", line 140, in on_node_activate\r\n    new_webhook = await webhooks_manager.get_suitable_webhook(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/base.py\", line 40, in get_suitable_webhook\r\n    return await self._create_webhook(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/base.py\", line 138, in _create_webhook\r\n    provider_webhook_id, config = await self._register_webhook(\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/ntindle/code/agpt/AutoGPT/autogpt_platform/backend/backend/integrations/webhooks/github.py\", line 122, in _register_webhook\r\n    raise ValueError(f\"Failed to create GitHub webhook: {error_msg}\")\r\nValueError: Failed to create GitHub webhook: Validation Failed\r\n* url is missing a scheme\r\nINFO:     127.0.0.1:64466 - \"POST /api/graphs HTTP/1.1\" 400 Bad Request\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/ebd92341-5b65-4221-8c79-837e8eb934ef)\r\n\r\n\r\nWe should probably have a better error message than that for saying \"set your .env correctly\"",
      "\r\nhttps://github.com/user-attachments/assets/d9a71e7f-f6ae-4483-a770-9e6cf87d045f\r\n\r\nrunning this agent gets me the following \r\n\r\nNote the weird credentials fields",
      "Not sure if related but also can't delete creds",
      "> running this agent gets me the following\r\n> \r\n> [video demonstrating error on run]\r\n\r\nGood catch. I'm not sure how to deal with this. The graph can't be run manually because it relies on a webhook to trigger it. Should we hide the \"Run\" button if a graph can't be run manually? Should it just do nothing?\r\n\r\n> Note the weird credentials fields\r\n\r\nDo you mean it still shows the field title **Credentials** while hiding the actual input? That is a bug, possibly improperly resolved merge conflict. I'll look into it.",
      "Github PR review split the convos, im sorry <3",
      "> Good catch. I'm not sure how to deal with this. The graph can't be run manually because it relies on a webhook to trigger it. Should we hide the \"Run\" button if a graph can't be run manually? Should it just do nothing?\r\n\r\nwhat's toran and john think",
      "> The graph can't be run manually because it relies on a webhook to trigger it. Should we hide the \"Run\" button if a graph can't be run manually? Should it just do nothing?\r\n\r\nSo there needs to be a way of saying that you want your Agent to be \"Running\" - in this case listening for a webhook - or \"Stopped\" - i.e not listening on a webhook. I was thinking the run button would do that here.\r\n\r\nWhat's the current UX for this? Let's sync on this one @Pwuts as it's a lot for a comment.\r\n",
      "Created follow-up ticket #8671",
      "> The advanced button here does nothing?\r\n\r\nThat's because the block has no \"advanced\" inputs, and the toggle doesn't hide when there is nothing to toggle.\r\n\r\n> We probably want these to be able to trivially link\r\n\r\nYes, there are a few ways to do that but most of those are out of scope for this PR and the rest not a sustainable fix imo. We should do a QOL improvement on all of the GitHub blocks to address stuff like this.\r\n\r\n> How do I pass a variable to this block [picture of GitHub webhook trigger block]\r\n\r\nYou don't. Due to the system's architecture, the webhook trigger block can't accept input links and must be the starting node.\r\n\r\n> Why is the output not number type for number\r\n\r\nBecause `NodeHandle` doesn't know what an `integer` is apparently:\r\nhttps://github.com/Significant-Gravitas/AutoGPT/blob/86535b5811f8d1cc0bdde2232693919c4b1115e3/autogpt_platform/frontend/src/components/NodeHandle.tsx#L22-L29\r\n\r\n- [ ] Add `integer` type to `NodeHandle` type list\r\n\r\n> we should probably allow the block to output the repo, URL, etc too for the trigger if its not taking in inputs\r\n\r\nMy idea for a sustainable and scalable fix for that is: allow directly connecting links to nested properties of object outputs. That's way out of scope for this PR.\r\n\r\nDue to the block layout, I don't want to add a large number of outputs because that just fills up the screen very quickly.\r\n\r\n> We need to clarify Payload, Sender and Pull request for normal people\r\n\r\nIf you don't know what a pull request is, why would you be using this block?\r\n\r\n- [x] Improve description of `payload` output\r\n\r\n> I'm not sure the diff in pull request and Payload\r\n\r\ncan't parse, come again?\r\n\r\n> I assume sender is creator?\r\n\r\nSender already has the description *\"Object representing the user who triggered the event\"*. Do you think that output also needs a better name, and if so what?\r\n\r\n> I have no idea (as a dev, not even user) how to debug this via UI. As a dev, I checked the raw output of the block in the logs. It just \"didn't work\" but succeeded from the UI perspective\r\n\r\nI also just debug by looking at the backend logs. Suggestions welcome.\r\n\r\nWe could store all incoming webhook payloads and add a view for that, but that's a significant feature addition. WDYT?\r\n\r\n> If a user does a bad design (ex: leaving out a value on a comparison) the webhook rejects but they should probably know that when saving because it will be an issue they won't be able to diagnose.\r\n\r\nYeah the node needs an indicator for whether a webhook is attached or not. Determining why can usually be done client-side, because it depends directly on whether the user filled out all the required inputs on the node.\r\n\r\n- [x] Create issue for webhook status indicator on webhook-triggered nodes\r\n\r\n> The Run button throws an error when you run (this is better than crashing tho)\r\n\r\nWould you rather hide the button? I'm not sure how to properly fix this.\r\n\r\n> We may want to do something to require platform base URL to be set if someone uses a trigger because currently it just doesn't do anything.\r\n\r\n- [x] Disable webhook-triggered blocks if `PLATFORM_BASE_URL` is not set\r\n- [x] Raise error in `BaseWebhooksManager` on attempt to create webhook if `PLATFORM_BASE_URL` is not set\r\n\r\n> we currently don't actually check platform base URL on inbound webhooks so we just execute from anything lol.\r\n> \r\n> > Replicate by running ngrok and disabling the line in your .env\r\n\r\nWhy would we need to check it on inbound webhook payloads? If it arrives, that's a job done. The `PLATFORM_BASE_URL` is only necessary to configure the webhook in the first place."
    ],
    "num_comments": 11,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 179892,
    "code_diff": "diff --git a/autogpt_platform/backend/.env.example b/autogpt_platform/backend/.env.example\nindex b6d41c25d449..0dd10e838501 100644\n--- a/autogpt_platform/backend/.env.example\n+++ b/autogpt_platform/backend/.env.example\n@@ -28,8 +28,15 @@ SUPABASE_URL=http://localhost:8000\n SUPABASE_SERVICE_ROLE_KEY="
  },
  {
    "pr_title": "Agent loop v2: Planning & Task Management (part 1: refactoring)",
    "pr_body": "## Background\r\nThe so-called _agent loop_ of Auto-GPT currently lacks structural planning and task management. This impedes its long-term performance and also hampers the implementation of retrieval augmentation (see #3536).\r\n\r\n## Changes & other to-do's\r\n**Part 1 - Refactoring (#4799):**\r\n- [x] Rename module `agent` -> `agents`\r\n- [x] Check out LangChain's \"Plan and execute\" tools ([docs](https://python.langchain.com/docs/modules/agents/agent_types/plan_and_execute), [code](https://github.com/h",
    "pr_number": 4799,
    "comments": [
      "The [Plan-and-Solve Prompting](https://arxiv.org/pdf/2305.04091.pdf) paper they cite is some good reading and has an [accompanying GitHub repo](https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting)."
    ],
    "num_comments": 1,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 79344,
    "code_diff": "diff --git a/autogpt/agents/__init__.py b/autogpt/agents/__init__.py\nindex a6df24ad7110..94a5f42a5874 100644\n--- a/autogpt/agents/__init__.py\n+++ b/autogpt/agents/__init__.py\n@@ -1,3 +1,4 @@\n from .agent import Agent\n+from .base import AgentThoughts, BaseAgent, CommandArgs, CommandName\n \n-__all__ = "
  },
  {
    "pr_title": "Re-arch hello world",
    "pr_body": "Rough sketching out of a hello world using our refactored autogpt library. See the tracking issue here: #4770.\r\n\r\n# Run instructions\r\n\r\nThere are two client applications for Auto-GPT included. \r\n\r\n## CLI Application\r\n\r\n:star2: **This is the reference application I'm working with for now** :star2: \r\n\r\nThe first app is a straight CLI application.  I have not done anything yet to port all the friendly display stuff from the `logger.typewriter_log` logic.  \r\n\r\n- [Entry Point](https://github.com/Sign",
    "pr_number": 3969,
    "comments": [
      "@merwanehamadi I'm assuming that this could potentially be used for regression testing."
    ],
    "num_comments": 1,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 147687,
    "code_diff": "diff --git a/autogpt/config/config.py b/autogpt/config/config.py\nindex bec3e92144d1..1c2084f746cb 100644\n--- a/autogpt/config/config.py\n+++ b/autogpt/config/config.py\n@@ -111,7 +111,7 @@ class ConfigBuilder(Configurable[Config]):\n     else:\n         default_tts_provider = \"gtts\"\n \n-    defaults_sett"
  },
  {
    "pr_title": "OpenAI Functions Support",
    "pr_body": "## This PR implements OpenAI functions support\r\n\r\nYou can now provide the AI with a list of functions that it can call. When making a chat completion request, there is now a separate data structure containing that list. In the response, you will receive a function call in a distinct data structure, separate from the textual content of the response. This enables easy handling of function calls alongside the text response.\r\n\r\nThis is extremely convenient for us!\r\n\r\n## This is a very large PR. Here",
    "pr_number": 4683,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 43944,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 067452457937..c3fcb761da79 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -25,10 +25,14 @@ OPENAI_API_KEY=your-openai-api-key\n ## PROMPT_SETTINGS_FILE - Specifies which Prompt Settings file to use (defaults to prompt_settings.yaml)\n # PROMPT_SETTIN"
  },
  {
    "pr_title": "refactor(agent, forge): Move library code from `autogpt` to `forge`",
    "pr_body": "### Background\r\n\r\n* https://github.com/Significant-Gravitas/AutoGPT/discussions/6970\r\n* Follow-up after [Component-based Agents](https://github.com/Significant-Gravitas/AutoGPT/pull/7054)\r\n\r\nThe goal of this PR is to move reusable code (including components) to `forge` so AutoGPT agent is built using `forge`.\r\n\r\n### Changes üèóÔ∏è\r\n\r\nModules moved from `autogpt` to `forge`:\r\n| original location     | moved to           |\r\n| --------------------- | ------------------ |\r\n| `autogpt.agents.base` <br/> ",
    "pr_number": 7106,
    "comments": [
      "Component agent docs should be moved to forge docs :)",
      "@kcze Would you like me to force merge it in. Assuming your later PRs eventually fix the CI's"
    ],
    "num_comments": 2,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 289919,
    "code_diff": "diff --git a/autogpts/autogpt/agbenchmark_config/benchmarks.py b/autogpts/autogpt/agbenchmark_config/benchmarks.py\nindex c574dc303f5a..7f60774fd2cb 100644\n--- a/autogpts/autogpt/agbenchmark_config/benchmarks.py\n+++ b/autogpts/autogpt/agbenchmark_config/benchmarks.py\n@@ -3,12 +3,14 @@\n import sys\n fr"
  },
  {
    "pr_title": "Plugin Support",
    "pr_body": "### Background\r\n\r\nAdds basic plugin support.\r\n\r\n### Changes\r\n\r\nAdd plugins.py which handles scanning plugins folder for zipfiles and check for modules to import.\r\n\r\nIn main.py on startup it will load the plugins into the config file.\r\n\r\nIn LLM utils it will iterate the plugins on each response.\r\n\r\n### Documentation\r\n\r\nDocstrings and README updated.\r\n\r\n### Test Plan\r\n\r\nI tested this code here: https://github.com/BillSchumacher/Auto-Vicuna\r\n\r\nI have an example plugin for that project here: https:/",
    "pr_number": 757,
    "comments": [
      "This is pretty similar to #350 ",
      "> This is pretty similar to #350\r\n\r\nNot really this loads external repos as zipfiles.",
      "If anyone has suggestions on other ways to hook certain points in the Auto-GPT flow don't hestitate to speak up.",
      "> If anyone has suggestions on other ways to hook certain points in the Auto-GPT flow don't hesitate to speak up.\r\n\r\nThis is a key point we need to work out, lots of different ways to do this, and we want to focus on enabling as many types of plugins as possible!",
      "> Not really this loads external repos as zipfiles.\r\n\r\nI'm just curious as to how plugins would function differently than commands. ",
      "Plugins can implement new commands, or change functionality of existing ones.\r\nThe difference is that they will allow Auto-GPT to be hooked up to _anything_.\r\n\r\nThere are plenty of use-cases that people have requested that don't make sense being in the core repo for everyone.\r\n\r\nSpecific niche integrations. This is useful because people wont want AutoGPT to come with tons of bloat pre-installed.",
      "> > Not really this loads external repos as zipfiles.\r\n> \r\n> I'm just curious as to how plugins would function differently than commands.\r\n\r\nIt would keep this codebase clean, allowing for faster reviews and more flexibility for people to add functionality they think other people would like without having to wait for a merge.",
      "Ah, yeah I get it. I didn't really consider bloat being a possible concern, but it's a valid point.",
      "Would some kind of \"plugin marketplace\" be needed to support this?",
      "> Would some kind of \"plugin marketplace\" be needed to support this?\r\n\r\nThe current implementation allows for any repo that uses the template to be downloaded as zip and dropped into the plugins folder.\r\n\r\nI would imagine a plugins channel in discord to start with.\r\n\r\nToran might get rich with a 30% cut from the marketplace though ü§î ",
      "Just thinking through the plugin whitelist and possibility for any abuse.  It only checks by name.  I think the caveat emptor in the readme is probably the best we'll get.  \r\nPossible improvement - we're only checking by name, and we don't check if we've already loaded a whitelisted plugin by that name - which might be worth adding?",
      "> Just thinking through the plugin whitelist and possibility for any abuse. It only checks by name. I think the caveat emptor in the readme is probably the best we'll get. Possible improvement - we're only checking by name, and we don't check if we've already loaded a whitelisted plugin by that name - which might be worth adding?\r\n\r\nThat's a fair point this implementation is actually quite bad.\r\n\r\nWe would probably want some md5 hashes or sha256 of the zip.\r\n\r\nI also didn't take into account external dependencies, which might be a good thing but probably to result in complaints.",
      "@BillSchumacher There are conflicts now",
      "@richbeales @BillSchumacher is there a way to do it where they're added as python packages in requirements.txt? The README could recommend specifying the specific commit like this:\r\n\r\n```\r\ngit+https://github.com/random-dev/example-auto-gpt-plugin@42b95dc=example-auto-gpt-plugin\r\n```\r\n\r\nWhatever package delivery option you pick; I'd recommend, to keep the feature simple for now, that users have to manually authorise each plugin every time the application boots, unless an environment variable `SKIP_PLUGIN_AUTH=true`. If that env var is set then the UI should display a clear warning and list the plugins that are registered.",
      "> Whatever package delivery option you pick; I'd recommend, to keep the feature simple for now, that users have to manually authorise each plugin every time the application boots, unless an environment variable `SKIP_PLUGIN_AUTH=true`. If that env var is set then the UI should display a clear warning and list the plugins that are registered.\r\n\r\nI agree in that asking to manually authorise every plugin and accept the risks every time the application boots is probably the most pragmatic approach. This will also keep this PR simple. Further refinement to the security model can be piled on after observations from the community use. \r\n\r\nThis PR seems quite important to help the community focus on extending Auto-GPT while this repo can focus on quality of the core.",
      "@BillSchumacher does this seem like a reasonable API to land on? \r\n![image](https://user-images.githubusercontent.com/39720479/232123556-ef3f7622-7794-4246-b782-0d3b5f3df9b4.png)\r\n\r\n```python\r\nPromptName = str\r\nPromptRole = str\r\nPromptGoals = List[str]\r\nPrompt = tuple[PromptName, PromptRole, PromptGoals]\r\n\r\npost_prompt(prompt: Prompt) -> Prompt\r\n\r\non_planning(prompt: Prompt, messages: List[str]) -> Optional[str]\r\npost_planning(response: str) -> str\r\n\r\npre_instruction(messages: List[str]) -> List[str]\r\non_instruction(messages: List[str]) -> Optional[str]\r\n# TODO: Better JSON type\r\npost_instruction(response: str) -> str\r\n\r\npre_command(instruction: str) -> str\r\npost_command(messages: List[str]) -> List[str]\r\n```",
      "Where every hook is optional, and it `on_planning`/`on_instruction` allow you to leave the default OpenAI calls by simply returning `None`",
      "As an example for this kind of architecture, the guy on discord who swapped out JSON instructions with YAML could override `pre_instruction` to ask for YAML instead of JSON, then override `pre_command` to convert the YAML back to json so that existing commands still work unchanged",
      "Also allows adding more commands via a `custom_commands` field that you can throw in",
      "Btw, I have no idea how Figma deals with people collaborating for free in FigJam files, but in case it's allowed, here's the link to that FigJam: https://www.figma.com/file/KpX21VXXmyXDZhTOfBH1sL/AutoGPT-Plugin-Architecture?node-id=0%3A1&t=vzGe8FlrCbMMvl4R-1",
      "> @BillSchumacher does this seem like a reasonable API to land on? ![image](https://user-images.githubusercontent.com/39720479/232123556-ef3f7622-7794-4246-b782-0d3b5f3df9b4.png)\r\n> \r\n> ```python\r\n> PromptName = str\r\n> PromptRole = str\r\n> PromptGoals = List[str]\r\n> Prompt = tuple[PromptName, PromptRole, PromptGoals]\r\n> \r\n> post_prompt(prompt: Prompt) -> Prompt\r\n> \r\n> on_planning(prompt: Prompt, messages: List[str]) -> Optional[str]\r\n> post_planning(response: str) -> str\r\n> \r\n> pre_instruction(messages: List[str]) -> List[str]\r\n> on_instruction(messages: List[str]) -> Optional[str]\r\n> # TODO: Better JSON type\r\n> post_instruction(response: str) -> str\r\n> \r\n> pre_command(instruction: str) -> str\r\n> post_command(messages: List[str]) -> List[str]\r\n> ```\r\n\r\nYeah I like it =)",
      "I'll be implementing this shortly.",
      "Implementation note, instead of the prompt tuple I'm passing a PromptGenerator object, which is what we use to contruct the full system prompt. \r\n\r\nIf you change something in post_prompt, it will alter the system_prompt. Something we've been thinking about doing with the assistants.\r\n\r\n```\r\npost_prompt(prompt: PromptGenerator) -> PromptGenerator\r\n```\r\n\r\nplugin ordering may affect output, something to beware of.",
      "Another note, when creating an AI config you can now pass a `PromptGenerator` object.\r\n\r\nThe prompt generator now also has a goals property that is assigned from the `AIConfig` object when construct_full_prompt is called.",
      "I guess I need to move the name and role into the prompt generator as well...",
      "@BillSchumacher no worries! Those were just types I'd thought of that might make sense from my black box view. I will update the Figma diagram! I'm going to work on some documentation for all of this as you work on implementation to help the community write/read plug-ins, so please lmk if anything else needs to change like a different name or anything like that! ",
      "Implementation note, even though we are passing a PromptGenerator object to the on_planning method of the plugin changing it's values will not change the prompt. It should be used to come up with additional context to send to the chat completion function.\r\n\r\n```\r\non_planning(prompt: PromptGenerator, messages: List[str]) -> Optional[str]\r\n```\r\n\r\nIf the response is None or an empty string it will be skipped, otherwise it will be added to the context *if* there are enough send tokens available, if there are not the plugin processing loop stops early and the context is not added.",
      "I'm not sure if the order of the context matters for getting the correct result back, also this is added as a \"system\" role, we might want to change that, unsure.",
      "It's also noteworthy that if you wanted, since this is python you could technically alter the context and that updated context would remain, if you wanted to remove certain messages, add new ones or whatever. Kind of a hack but it would work.\r\n\r\nWhen I refer to `context` I mean the second argument, the `messages`. If you do decide to do something like this, you should track the total cost in tokens.",
      "Our workflow doesn't quite work like this so I'm going to try to update the diagram, the sub-agents are not called every loop but rather by command."
    ],
    "num_comments": 30,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 109915,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex f1b511c2291f..60edecd6cac8 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -188,3 +188,10 @@ OPENAI_API_KEY=your-openai-api-key\n # TW_CONSUMER_SECRET=\n # TW_ACCESS_TOKEN=\n # TW_ACCESS_TOKEN_SECRET=\n+\n+################################################"
  },
  {
    "pr_title": "feat(platform): Add Gmail and Google Sheets blocks + fix Google OAuth",
    "pr_body": "### Background\r\n\r\nWe have several agents that we want to integrate with google services\r\n<!-- Clearly explain the need for these changes: -->\r\n\r\n### Changes üèóÔ∏è\r\n\r\nThis pr does a handful of things\r\n\r\n#### OAuth\r\n- Adds oauth scopes to the oauth base\r\n- adds default scopes to the oauth base\r\n- adds upgrading scopes to google\r\n- adds helpers for scopes\r\n\r\n#### Blocks\r\n- Adds gmail read/write blocks\r\n- Adds google sheets read/write block \r\n- Adds a few more categories for these blocks (up for discus",
    "pr_number": 8236,
    "comments": [
      "guess is most of the frontend stuff isn't needed",
      "## Test Plan\r\n\r\ntest logging in/out and making sure your tokens for github and google still work\r\n\r\nTest gihtub agents\r\n\r\ntest showing status and output of blocks during execution\r\n\r\ntest /monitoring post requests -> sometimes failed for me due to being 'too big'. Is this known\r\n\r\nTest sheets reading/writing -> can you do the same sheet with a list input etc\r\n\r\nTest reading/sending emails -> don't get us rate limited or blocked with spam while testing this. Seriously. Only email internal accounts, or yourself."
    ],
    "num_comments": 2,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 50456,
    "code_diff": "diff --git a/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py b/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py\nindex 13e1e69c8328..11b1dafd669b 100644\n--- a/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integ"
  },
  {
    "pr_title": "refactor(platform): Update database model to reflect domain",
    "pr_body": "# Background\r\n\r\nWe are enhancing our platform‚Äôs capabilities by introducing several new features that require an updated database schema. These features include group support, user-specific agent configurations, an accounting and credit system, and a marketplace for agent listings. These changes are essential to support the domino feature, which integrates these components to provide a more robust and user-friendly experience.\r\n\r\n## Changes üèóÔ∏è\r\n\r\n- **Renamed AgentGraph to Agent**:\r\n  - Updated t",
    "pr_number": 8375,
    "comments": [
      "[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/Significant-Gravitas/AutoGPT?pullRequest=8375) <br/>All committers have signed the CLA.",
      "When we come to an agreement on the schema changes I will propagate all these changes throughout the codebase",
      "So that would prevent injection or at least contain it from a malicious\r\nagent/user?\r\n\r\nOn Fri, Oct 18, 2024, 13:12 Nicholas Tindle ***@***.***>\r\nwrote:\r\n\r\n> ***@***.**** commented on this pull request.\r\n> ------------------------------\r\n>\r\n> In autogpt_platform/backend/schema.prisma\r\n> <https://github.com/Significant-Gravitas/AutoGPT/pull/8375#discussion_r1806326134>\r\n> :\r\n>\r\n> >\r\n>    // Link to User model\r\n> -  userId String\r\n> -  user   User   @relation(fields: [userId], references: [id], onDelete: Cascade)\r\n> +  createdByUserId String\r\n> +  createdByUser   User   @relation(fields: [createdByUserId], references: [id], onDelete: Cascade)\r\n>\r\n> And by extension this would delete it from the marketplace\r\n>\r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/Significant-Gravitas/AutoGPT/pull/8375#discussion_r1806326134>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AEJNNY5AFO3FCXR6EEKEEULZ4DULBAVCNFSM6AAAAABQFOG6SSVHI2DSMVQWIX3LMV43YUDVNRWFEZLROVSXG5CSMV3GSZLXHMZDGNZXHAYDAOJUG4>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>\r\n",
      "This reminds me of IRC back in 1999. You could easily enter and flood\r\nsomeone's channel (group), but so that all members would drop their\r\nconnection. Yeah, the bandwidth was that low and much less security layers\r\nthan today.\r\n\r\nOn Fri, Oct 18, 2024, 13:16 Nicholas Tindle ***@***.***>\r\nwrote:\r\n\r\n> ***@***.**** commented on this pull request.\r\n> ------------------------------\r\n>\r\n> In autogpt_platform/backend/schema.prisma\r\n> <https://github.com/Significant-Gravitas/AutoGPT/pull/8375#discussion_r1806330871>\r\n> :\r\n>\r\n> > +  commissionPerRun Float?\r\n> +  // We dont ever delete store pricings, j\r\n> +  // ust mark them as deleted and add a new entry\r\n> +  isDeleted        Boolean @default(false)\r\n> +\r\n> +  @@index([storeListingId, updatedAt])\r\n> +}\r\n> +\r\n> +model StoreListingVersion {\r\n> +  id        String   @id @default(uuid())\r\n> +  createdAt DateTime @default(now())\r\n> +  updatedAt DateTime @updatedAt\r\n> +\r\n> +  agentId      String\r\n> +  agentVersion Int\r\n> +  agent        Agent  @relation(fields: [agentId, agentVersion], references: [id, version])\r\n>\r\n> Agents are immutable after saving right? One of the reasons the\r\n> marketplace doesn‚Äôt reference the other schemas right now is as a dirty way\r\n> to prevent users from updating agents without approval\r\n>\r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/Significant-Gravitas/AutoGPT/pull/8375#pullrequestreview-2377810952>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AEJNNY467VAXRZV4FP5HZNDZ4DUX7AVCNFSM6AAAAABQFOG6SSVHI2DSMVQWIX3LMV43YUDVNRWFEZLROVSXG5CSMV3GSZLXHMZDGNZXHAYTAOJVGI>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n"
    ],
    "num_comments": 4,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 23091,
    "code_diff": "diff --git a/autogpt_platform/backend/target.prisma b/autogpt_platform/backend/target.prisma\nnew file mode 100644\nindex 000000000000..7c378b5a6fb7\n--- /dev/null\n+++ b/autogpt_platform/backend/target.prisma\n@@ -0,0 +1,628 @@\n+// We need to migrate our database schema to support the domain as we under"
  },
  {
    "pr_title": "feat(platform/library): Agent Library v2",
    "pr_body": "- Resolves #8774\r\n  - Resolves #8775\r\n- Includes back-end work for #9168\r\n- Partially implements #8776\r\n- Partially implements #8777\r\n\r\n### Changes üèóÔ∏è\r\n\r\n- Add `/library` page\r\n  - Change target of \"Library\" navigation link from `/monitoring` to `/library`\r\n- Move `/agents/[id]` page to `/library/agents/[id]`\r\n- Set application background color to `bg-neutral-50`\r\n- Redirect to new library agent's \"runs\" page (`/library/agents/[id]`) after adding from marketplace\r\n\r\nFurther (technical) frontend ",
    "pr_number": 9407,
    "comments": [
      "@Pwuts is leading the library work and I believe the work in this branch has now been discarded, in favor of the individual branches which were already merged into this. \r\n\r\ncc: @Abhi1992002 ",
      "Follow-up ticket for tech debt:\r\n- #9515"
    ],
    "num_comments": 2,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 345597,
    "code_diff": "diff --git a/autogpt_platform/backend/.env.example b/autogpt_platform/backend/.env.example\nindex 8b28244f423a..7733cb7db940 100644\n--- a/autogpt_platform/backend/.env.example\n+++ b/autogpt_platform/backend/.env.example\n@@ -41,6 +41,9 @@ RABBITMQ_PORT=5672\n RABBITMQ_DEFAULT_USER=rabbitmq_user_default"
  },
  {
    "pr_title": "CI test (Draft, ignore)",
    "pr_body": "CI test (Draft, ignore)",
    "pr_number": 4580,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 0,
    "code_diff": ""
  },
  {
    "pr_title": "startup menu for ai_settings.yaml to support multiple configurations",
    "pr_body": "### **User description**\n### Summary\r\nI created this PR to add support for storing multiple configurations, thus allowing the user to 'create', 'edit', 'view' or 'delete' configurations and have their settings stored in ai_settings.yaml. The ai_settings file now saves configurations as dictionary and if a configuration exists it is showed as a menu option when autogpt is started.\r\n\r\nWhen no configurations are found, or no ai_settings.yaml file, the user get's the usual generate_aiconfig_automati",
    "pr_number": 4646,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 107148,
    "code_diff": "diff --git a/autogpt/config/ai_config.py b/autogpt/config/ai_config.py\nindex 3c645abe36f3..bb258abef6d9 100644\n--- a/autogpt/config/ai_config.py\n+++ b/autogpt/config/ai_config.py\n@@ -6,8 +6,7 @@\n \n import os\n import platform\n-from pathlib import Path\n-from typing import TYPE_CHECKING, Optional\n+from"
  },
  {
    "pr_title": "feat(platform/library): Triggered-agent support",
    "pr_body": "This pull request adds support for setting up (webhook-)triggered agents in the Library. It contains changes throughout the entire stack to make everything work in the various phases of a triggered agent's lifecycle: setup, execution, updates, deletion.\r\n\r\nSetting up agents with webhook triggers was previously only possible in the Builder, limiting their use to the agent's creator only. To make it work in the Library, this change uses the previously introduced `AgentPreset` to store information ",
    "pr_number": 10167,
    "comments": [
      "Thank you for your PR implementing webhook-triggered agent support. The code changes look good, but since this is marked as WIP, there are a few things to address before it can be merged:\n\n1. Please expand the PR description with more details about what this feature does and the specific changes you've made. While the reference to issue #10111 is helpful, a summary of the implementation would provide better context.\n\n2. None of the checklist items are checked off. Please complete the checklist, particularly:\n   - Clearly listing your changes in the PR description\n   - Creating and documenting a test plan\n   - Confirming you've tested according to that plan\n\n3. If there are any configuration changes needed for this feature, please document them.\n\nYour code changes look solid - I especially appreciate how you've maintained the user_id checks in the webhook-related functions and updated the database schema appropriately. Once you've completed the PR description and checklist, this should be ready for further review.",
      "Thank you for your work on adding webhook-triggered agent support to the platform. Since this is marked as a Work in Progress (WIP), I understand it's not yet ready for full review, but here are some points to address before this can be merged:\n\n1. **PR Description**: Please provide a comprehensive description of the changes you've made. Currently, it only says \"WIP\". Even for a work-in-progress PR, it's important to document what you've done so far and what still needs to be completed.\n\n2. **Checklist**: None of the items in the checklist are checked off. Please update the checklist to reflect the current status of your work, checking off completed items.\n\n3. **Test Plan**: Develop and document a test plan for this feature. Since this involves webhook triggers, consider including specific test cases for various webhook scenarios.\n\n4. **Documentation**: Consider adding comments or documentation about how the webhook-triggered agent support works, especially for the new API endpoint `/setup_webhook_trigger`.\n\n5. **Configuration Changes**: If there are any configuration changes needed (especially for webhook handling), make sure `.env.example` and `docker-compose.yml` are updated accordingly.\n\nThe changes look promising and address an important feature, but they need more documentation and testing before they can be merged. Please update the PR with these items when you're ready for a more thorough review.",
      "Thank you for your PR working on webhook-triggered agent support! \n\nI see this is marked as a Work in Progress (WIP), which explains the current state of the PR. Before this can be merged, you'll need to address the following:\n\n1. Please expand the PR description with details about the specific changes you're making. Just marking it as \"WIP\" isn't sufficient - explain what webhook-triggered agent support means and how your implementation works.\n\n2. Complete the checklist in the PR description. All applicable items need to be checked off.\n\n3. Consider whether the scope in your PR title is accurate. Your changes span both frontend and backend components (as indicated by the labels and the files changed), but the title only mentions 'platform/library'.\n\n4. Since you're modifying files in `backend/data/`, please ensure that any added or modified functions properly handle user_id checks, and note this in your description.\n\n5. Include a test plan that explains how you've verified your changes work correctly.\n\nPlease update the PR when it's no longer a work in progress and all these items have been addressed. Looking forward to seeing the completed implementation!",
      "Thank you for your PR on webhook-triggered agent support! I notice this is marked as WIP, but there are a few things that need attention before this can be merged:\n\n1. **PR Description**: Please provide a detailed description of what this feature does and how it works. The current description only mentions it's WIP and references an issue number.\n\n2. **Checklist**: None of the items in the checklist have been checked off. Please complete the checklist or mark which items are still in progress.\n\n3. **Scope**: Your PR title specifies `platform/library`, but the changes span across backend, frontend, and database schema. Consider whether the scope in the title accurately reflects the breadth of changes.\n\n4. **User ID Verification**: Since you're modifying files in `backend/data/`, please ensure that any added/changed functions properly handle user_id verification, or explain why it's not needed.\n\n5. **Test Plan**: Please outline how you've tested or plan to test this webhook functionality.\n\nWhen you're ready to move this PR out of WIP status, please update the description with these details and complete the checklist. This will help reviewers understand the purpose and impact of your changes.",
      "Thank you for your PR adding webhook-triggered agent support! I notice this is marked as a Work in Progress, so here's some feedback to help guide you toward completion:\n\n1. Please complete the PR description with details about the specific changes you're making. The \"Changes\" section currently only contains \"WIP\" but should explain what functionality you're adding and how it works.\n\n2. There appears to be a mismatch between your PR title scope (platform/library) and the actual changes, which include both frontend and backend components (as indicated by the labels and file changes). Consider updating your PR title to better reflect the scope of changes.\n\n3. None of the checklist items are checked off yet. Please complete the checklist before requesting review, including:\n   - Adding a clear test plan\n   - Ensuring you've tested the changes\n   - Checking that configuration files are updated if needed\n\n4. Since you're modifying files in backend/data/, please ensure any new or modified functions properly check user_id permissions where appropriate.\n\nPlease update your PR when it's ready for review, completing the description and checklist. Feel free to remove the WIP marker at that point.",
      "Thank you for your PR on adding Webhook-triggered Agent support. Since this is marked as a work in progress (WIP), I'm providing initial feedback before you finalize it:\n\n1. **Complete the PR description**: Please provide a detailed description of the changes you've made when this PR is ready for review. This should include what Webhook-triggered Agent support is, how it works, and the key components you've implemented.\n\n2. **Fill out the checklist**: None of the checklist items are currently checked. Please complete this when your PR is ready for review.\n\n3. **Add a test plan**: Include specific tests you've performed to verify the webhook functionality works as expected.\n\n4. **Configuration changes**: If your implementation requires any configuration changes (which seems likely for webhook integration), please document these in the PR description.\n\n5. **Scope alignment**: Your PR touches both frontend and backend code. Make sure all changes are related to the webhook-triggered agent support feature mentioned in the title.\n\nPlease update the PR when it's no longer a work in progress and ready for a thorough review. Looking forward to seeing the completed implementation.",
      "Thank you for your PR implementing webhook-triggered agent support. Since this is marked as a Work in Progress (WIP), I understand it's not ready for final review yet. Here's what you'll need to address before this can be merged:\n\n1. **Complete the PR description**: \n   - Provide a detailed explanation of what changes you've made\n   - Describe how webhook-triggered agent support works\n   - List any configuration changes required\n\n2. **Fill out the checklist**:\n   - All items in the checklist need to be checked off\n   - Add a test plan specific to testing webhook functionality\n\n3. **Configuration documentation**:\n   - Ensure any new environment variables are documented in `.env.example`\n   - Update `docker-compose.yml` if needed\n\nWhen you're ready for review, please mark the PR as ready and update the description accordingly. If you need any help or have questions during development, feel free to ask!\n\nLooking forward to seeing the completed implementation of webhook-triggered agent support.",
      "Thank you for starting work on webhook-triggered agent support. I notice this PR is marked as WIP, which is fine, but there are several items that need to be addressed before it can be considered for merging:\n\n1. Please expand the PR description to include more details about the specific changes you're making. Even for a WIP PR, it's helpful to outline what you've implemented so far and what's still pending.\n\n2. None of the checklist items have been completed yet. Please check them off as you progress through the implementation.\n\n3. Since you're modifying files in `backend/data/`, please ensure any new or modified functions properly handle user_id checks for security purposes.\n\n4. Consider adding a test plan specific to webhook functionality to demonstrate how you'll verify the implementation works as expected.\n\n5. If you've added any new environment variables or configuration requirements for the webhook functionality, please document these in the PR description.\n\nI'm marking this as a failure not because there's anything wrong with the code itself (which I haven't reviewed in detail), but because the PR as currently submitted is explicitly a work in progress and not ready for final review. Please feel free to update the PR when you're further along, and I'll be happy to review it again.",
      "Thank you for your PR on webhook-triggered Agent support! I notice this is marked as a work in progress, which is fine, but there are several items that will need to be addressed before this can be merged:\n\n1. **PR Description**: Please provide a more detailed description of the changes when you're ready to merge. The current description only references an issue number and states \"WIP\".\n\n2. **Checklist**: None of the checklist items have been checked off. Please complete the checklist when the PR is ready for review.\n\n3. **PR Scope and Labels**: Your PR title specifies platform/library, but you have both platform/frontend and platform/backend labels. Make sure the scope in your title accurately reflects all the components you're modifying.\n\n4. **User ID Checks**: Since you're modifying files in backend/data/, please ensure that any changed or added functions correctly handle user_id validation, or provide an explanation of why this isn't necessary.\n\n5. **Frontend Route Protection**: If you're adding any new base routes to the frontend that require protection, make sure to update the protections in lib/supabase/middleware.ts.\n\nPlease continue your work on this PR, and when you're ready for a full review, update the description with detailed information about your changes and complete the checklist. Feel free to request reviews when you're ready.",
      "Thank you for your work on implementing webhook-triggered agent support. I notice this PR is marked as WIP, which explains why some aspects aren't complete yet. Before this can be merged, please address the following:\n\n1. Complete the PR description with details about all the changes you've made. The current description only references an issue without explaining what's being implemented.\n\n2. Fill out the checklist completely. Every item needs to be checked off before the PR can be merged.\n\n3. Provide a test plan that explains how you've tested the webhook functionality.\n\n4. Since you're modifying files in backend/data/ (graph.py and integrations.py), please ensure user_id is properly passed and verified in any added or modified functions.\n\n5. Update the \"Changes\" section to list all the significant modifications you're making to implement webhook-triggered agent support.\n\n6. If your implementation adds any new configuration requirements, please document them in the PR description.\n\nWhen you're ready to move this out of WIP status, please update the PR accordingly, and we'll be happy to review it again.",
      "Thank you for your work on implementing webhook-triggered agent support! I notice this is marked as a Work in Progress (WIP), which explains the current state of the PR.\n\nBefore this can be merged, you'll need to:\n\n1. Complete the PR description with a clear explanation of the changes and their purpose\n2. Fill out the checklist completely\n3. Provide a test plan that shows how you've verified the functionality\n4. Ensure any configuration changes are documented\n\nI see the changes span both frontend and backend components, which makes sense for this feature. When you're ready for a full review, please:\n\n- Verify that any modified functions in the backend/data directory correctly handle user_id validation\n- Confirm that any new frontend routes have appropriate middleware protections\n- Make sure the scope in the PR title accurately reflects the breadth of changes\n\nPlease update the PR when it's ready for a comprehensive review. Looking forward to seeing this feature completed!",
      "Thank you for starting work on webhook-triggered agent support! I notice this PR is marked as a work in progress, which is fine for early collaboration, but it's not ready for final review yet.\n\nBefore this can be considered for merging, please:\n\n1. Complete the PR description with detailed information about what changes you're making and why\n2. Fill out the \"Changes\" section with a concise summary of all modifications\n3. Complete the checklist items, particularly:\n   - Clearly listing your changes\n   - Creating and documenting a test plan\n   - Testing according to that plan\n   - Addressing any configuration changes needed\n\n4. If you're modifying files in backend/data/, please ensure proper user_id checks are in place or explain why they aren't needed\n\n5. Since this appears to touch both frontend and backend components, the labels should include platform/library (which aligns with your PR title) rather than just the individual components\n\nI see you've referenced issue #10111, which helps provide context, but more details in the PR itself would be helpful for reviewers.\n\nPlease update the PR when it's ready for a more thorough review. Looking forward to seeing the completed implementation!",
      "Thank you for your work on webhook-triggered Agent support! This appears to be a significant feature addition spanning multiple components of the platform.\n\nHowever, I notice this PR is currently marked as \"WIP\" (Work in Progress) and doesn't have:\n\n- A complete description of the changes\n- Any checked items in the checklist\n- A test plan\n\nBefore this PR can be considered for merging, please:\n\n1. Complete the description with details about what this webhook-triggered Agent support feature does and how it works\n2. List all the changes you've made across the various components (backend, frontend, blocks)\n3. Fill out and check off the appropriate items in the checklist\n4. Provide a test plan that explains how to verify this feature works correctly\n5. Update the PR title if needed to accurately reflect the scope of changes (the current scope is platform/library, but the changes seem broader based on the modified files)\n\nOnce the PR is no longer a work in progress and these items are addressed, we'll be happy to review it again.",
      "Thank you for your PR implementing webhook-triggered agent support. Since this is marked as a Work in Progress (WIP), I understand it's not ready for final review, but I'd like to provide some guidance to help you prepare for when it is ready:\n\n1. **PR Description**: Please expand the description to explain:\n   - What webhook-triggered agent support is and why it's needed\n   - A summary of the key changes you're making\n   - Any architectural decisions that reviewers should be aware of\n\n2. **Checklist**: The checklist items need to be completed before the PR can be merged. This includes:\n   - Clearly listing your changes\n   - Creating and following a test plan\n   - Ensuring configuration changes are documented\n\n3. **PR Scope**: Your changes touch multiple areas including backend, frontend, and blocks components, but your PR title only mentions platform/library. Consider updating the title to better reflect the scope of changes or split this into multiple PRs if appropriate.\n\n4. **Data Files**: Since you're modifying files in backend/data/, please ensure that any added/changed functions properly pass and check the user_id parameter where required.\n\nPlease update the PR when it's ready for review with these items addressed. If you need any clarification or help with any of these points, feel free to ask!",
      "Thank you for your PR implementing webhook-triggered agent support. Since this is marked as a Work in Progress (WIP), I understand it's not ready for final review yet, but I wanted to provide some early feedback:\n\n1. **PR Description**: Please expand the description to explain the purpose and scope of these changes once you're ready for review. While you've referenced issue #10111, a brief summary of what webhook-triggered agent support entails would be helpful.\n\n2. **Checklist**: None of the items are currently checked off. Please complete the relevant items before requesting final review.\n\n3. **Scope**: Your PR title specifies `platform/library`, but the changes span across backend, frontend, blocks, and include database migrations. Consider whether the scope in the title accurately reflects the breadth of changes.\n\n4. **Data Functions**: Since there are changes to files in `backend/data/`, please ensure any new or modified functions properly check user_id where appropriate.\n\n5. **Test Plan**: Be sure to include a specific test plan for testing webhook functionality when you're ready for review.\n\nFeel free to keep iterating on this PR and update the description and checklist when you're closer to completion. Let us know if you need any help or clarification!",
      "Thank you for your PR on webhook-triggered agent support. Since this is marked as Work in Progress, here's some guidance to help move it toward a mergeable state:\n\n1. **PR Description**: Please update your description with a clear explanation of what this webhook-triggered agent support feature does and why it's needed. While you've referenced issue #10111, a brief summary would be helpful.\n\n2. **Changes Section**: The \"Changes\" section currently only says \"WIP\". When you're ready for review, please provide a concise description of all the changes made in this PR.\n\n3. **Checklist**: None of the checklist items are currently checked. Before requesting a review, please complete the checklist by testing your changes according to a test plan.\n\n4. **Configuration Changes**: Your PR includes changes to `schema.prisma` and adds a migration file. Please update the checklist to confirm that `.env.example` and `docker-compose.yml` are compatible with these changes.\n\nI see this is implementing webhook functionality across multiple parts of the platform. When you're ready for a proper review, we'll need to ensure that any data access functions properly validate user_id where required.\n\nLet us know when this PR is ready for a thorough review!",
      "Thank you for submitting this PR for Webhook-triggered Agent support. I notice this is marked as a Work in Progress (WIP) and none of the checklist items have been completed yet.\n\nBefore this PR can be considered for merging, please:\n\n1. Complete the PR description with details about the changes being made\n2. Check off the relevant items in the checklist after completing them\n3. Provide a test plan for your changes\n4. Ensure any configuration changes are properly documented\n\nThe scope of changes appears substantial, touching both frontend and backend components, which aligns with the cross-cutting nature of adding webhook support. \n\nPlease update the PR when it's ready for a full review. I'd be happy to take another look once the WIP status is removed and the checklist items are addressed.",
      "Hi @Pwuts, thank you for your PR on webhook-triggered Agent support. \n\nThis PR is currently marked as a Work in Progress, and I notice that:\n\n- The description only mentions \"WIP\" under changes without detailing what modifications are being made\n- None of the checklist items are checked off\n- The PR title refers to platform/library, but the changes appear to span backend, frontend, and blocks components\n\nBefore this can proceed to review, please:\n\n1. Complete the description with details about what webhook-triggered Agent support entails and how you've implemented it\n2. Fill out the checklist completely (or remove sections that aren't applicable)\n3. Consider updating the PR title to better reflect the scope of changes if they go beyond just the platform/library component\n4. Ensure any backend/data changes properly handle user_id permissions\n5. Document your test plan and results\n\nI see this is linked to issue #10111, so once you've completed the implementation and updated the PR details, we'll be happy to review it. Let us know if you need any guidance or clarification on the requirements.",
      "Thank you for your PR on Webhook-triggered Agent support! Since this is marked as a Work in Progress (WIP), I understand that not everything is complete yet. However, there are several items that will need to be addressed before this can be considered ready for merging:\n\n1. **Description needs to be completed**: Please provide a clear and detailed explanation of what this feature does and the changes you've made when you're ready to finalize the PR.\n\n2. **Checklist needs to be completed**: All items in the checklist need to be checked off, including your test plan.\n\n3. **Scope alignment**: Your PR title mentions 'platform/library', but the changes span across frontend, backend, and blocks components. Consider either adjusting the title to better reflect the scope or explaining why these broader changes are necessary for the library feature.\n\n4. **Data layer security**: There appear to be changes to backend/data files - make sure any new or modified functions properly check user_id where appropriate.\n\n5. **Configuration changes**: Your PR includes schema.prisma changes and a new migration - please document these in the 'Changes' section when finalizing the PR.\n\nPlease complete these items when you're ready to move this PR from WIP status to ready for review. Let us know if you need any assistance with any of these points!",
      "Thank you for your PR on webhook-triggered agent support! This seems like an important feature, but there are a few things that need to be addressed before this can be considered for merging:\n\n1. **Complete the PR description**: \n   - Please replace the \"WIP\" with a clear explanation of what webhook-triggered agent support entails\n   - Describe the specific changes you've made across the various components\n\n2. **Complete the checklist**:\n   - All items in the checklist need to be checked off\n   - Include your test plan with specific steps you've taken to verify the functionality\n\n3. **Configuration changes**:\n   - Document any changes to environment variables in `.env.example`\n   - Ensure `docker-compose.yml` is updated if needed\n   - List the configuration changes in your PR description\n\n4. **Security considerations**:\n   - Since you're modifying files in `backend/data/`, please ensure proper user_id handling\n   - For webhook endpoints, ensure appropriate authentication and authorization mechanisms\n\n5. **Scope clarification**:\n   - Your PR touches backend, frontend, and blocks components, which is broader than just \"platform/library\" in the title\n   - Consider updating the PR title to better reflect the full scope of changes\n\nPlease update your PR when it's no longer a work in progress and all these items have been addressed. The feature looks promising, and I'm looking forward to reviewing the completed implementation!",
      "Hi @Pwuts,\n\nThank you for your work on webhook-triggered Agent support. I see this is marked as a work in progress (WIP), so my feedback is based on what would be needed to make this PR ready for review and merging.\n\n### What needs attention:\n\n1. **PR Description**: Please complete the description with detailed information about the changes you're making. What does webhook-triggered Agent support entail? What problem does it solve?\n\n2. **Changes Section**: The \"Changes\" section is currently marked as WIP. Please provide a concise list of all the changes made in this PR.\n\n3. **Checklist**: None of the checklist items are checked. Please complete the checklist, including your test plan.\n\n4. **Scope Mismatch**: Your PR title mentions 'platform/library', but the changes appear to span backend, frontend, and blocks components (as indicated by the file changes and PR labels). Consider updating the PR title to better reflect the scope of changes.\n\n5. **Configuration Changes**: Since you've added migrations and modified the schema, please document any configuration changes needed.\n\n6. **Testing**: Ensure you've tested the webhook functionality and document your test plan.\n\nThis looks like an exciting feature addition! When you've completed your work and addressed these points, please remove the WIP marker and request a review.\n\nLooking forward to seeing the completed implementation!",
      "Thank you for working on webhook-triggered agent support! This PR appears to be a work in progress, which is fine, but there are several things that need to be addressed before it can be considered for merging:\n\n1. **PR Description**: Please provide a detailed description of the changes you're making. Explain what webhook-triggered agent support is, how it works, and why it's being added.\n\n2. **Checklist Completion**: None of the checklist items have been checked off. Before this PR can be merged, you'll need to complete the checklist.\n\n3. **Test Plan**: Please create a test plan specific to the webhook functionality you're implementing. What scenarios should be tested? How can reviewers verify the functionality works correctly?\n\n4. **Configuration Changes**: If your webhook implementation requires any configuration changes (environment variables, etc.), please document these in the PR description.\n\n5. **Scope Alignment**: The PR title specifies platform/library, but the changes span across frontend, backend, and blocks. Consider updating the title to better reflect the scope of changes.\n\nThe implementation itself spans multiple components of the platform, which makes sense for a feature like webhook integration. Once you've completed your work and addressed the points above, this PR will be ready for a more thorough review.\n\nLet us know if you need any help or have questions as you continue developing this feature!",
      "Hi @Pwuts,\n\nThanks for submitting this PR for Webhook-triggered Agent support. I see this is marked as a Work in Progress and builds on a previous PR.\n\nBefore this can be considered for merging, you'll need to:\n\n1. Complete the PR description with details about the changes being made\n2. Fill out the checklist in the PR description\n3. Create and document a test plan\n4. Consider updating the PR title to better reflect the broad scope of changes - the current title mentions platform/library but the changes span backend, frontend, and database components\n\nThe PR contains significant changes across multiple areas of the codebase, so a comprehensive description and test plan will be particularly important.\n\nPlease update the PR when it's ready for a full review. Looking forward to seeing this feature develop!",
      "Thank you for your PR submission! I noticed this is marked as a Work in Progress (WIP), which explains why the description and checklist are incomplete.\n\nBefore this can be merged, you'll need to:\n\n1. Complete the PR description with details about all changes made\n2. Fill out the checklist completely\n3. Consider updating the PR title to better reflect the scope of changes - the current title mentions platform/library but your changes affect backend, frontend, and include database migrations\n4. If you've modified any functions in backend/data/*.py files, ensure they properly handle user_id checks\n\nThe webhook-triggered agent support looks like a substantial feature addition! When you're ready to finalize this PR, please update the description with details about:\n- What problem this feature solves\n- How it works at a high level\n- Any configuration changes needed\n- Testing you've performed\n\nPlease let us know when this is ready for review by removing the WIP status and completing the required sections.",
      "Thank you for your PR implementing webhook-triggered Agent support! I see this is marked as a Work in Progress and builds on PR #9786.\n\nBefore this can be merged, there are several items that need to be addressed:\n\n1. **Description needs to be completed**: Please provide a detailed description of what changes are being made and how they implement webhook-triggered Agent support. \n\n2. **Checklist is incomplete**: None of the checklist items are checked off. Please ensure you complete all the relevant items before requesting review.\n\n3. **Test plan**: Add a specific test plan for this feature, especially focusing on how to test the webhook triggering functionality.\n\n4. **Scope verification**: Your PR affects multiple areas (backend, frontend, blocks) which is reflected in the labels. Please ensure all changes are relevant to the webhook-triggered Agent support feature.\n\n5. **Backend data changes**: Since you've modified files in backend/data/, please ensure any new/changed functions properly check user_id where appropriate.\n\n6. **Configuration changes**: If there are any configuration changes needed for webhook support, please document them.\n\nLooking forward to seeing this completed! This appears to be a valuable addition to the platform.",
      "Thank you for submitting this PR for webhook-triggered agent support! I can see this is marked as a work in progress, which is great for early feedback, but there are several items that need to be addressed before this can be considered for merging:\n\n1. **PR Description**: Please provide a more detailed description of the changes you're making. The current description just says \"WIP\" under the Changes section. A clear explanation of what webhook-triggered agent support entails and how you've implemented it would be helpful.\n\n2. **Checklist**: None of the checklist items are currently checked. Please complete the checklist when you're ready for this PR to be reviewed for merging.\n\n3. **Scope**: While your PR title mentions platform/library, the changes span across backend, frontend, and blocks (as indicated by the labels). Please ensure the PR title accurately reflects the scope of changes.\n\n4. **Security Considerations**: \n   - For changes to backend/data/*.py files, please ensure that user_id checks are properly implemented\n   - For any new frontend routes, ensure that appropriate protections are in place\n\n5. **Test Plan**: When you're ready to move this out of WIP status, please include a detailed test plan showing how you've verified the webhook functionality works correctly.\n\nI'm looking forward to seeing this develop further. The webhook-triggered agent support sounds like a valuable addition to the platform!"
    ],
    "num_comments": 26,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 163563,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/blocks/agent.py b/autogpt_platform/backend/backend/blocks/agent.py\nindex c25d99458d6d..69d3c8526930 100644\n--- a/autogpt_platform/backend/backend/blocks/agent.py\n+++ b/autogpt_platform/backend/backend/blocks/agent.py\n@@ -2,6 +2,8 @@\n import logging\n from"
  },
  {
    "pr_title": "Add settings for custom base url",
    "pr_body": "Making the openai base url and embedding dimension configurable, these are useful to integrate AutoGPT with other models, like LLaMA\r\n\r\n### Background\r\nThis makes AutoGPT capable of connecting to custom openai-like APIs like [[keldenl](https://github.com/keldenl)](https://github.com/keldenl/gpt-llama.cpp), and use other models, like LLaMA and derivates.\r\n\r\nsee also #25 #567  #2158\r\n\r\n### Changes\r\nAdded OPENAI_API_BASE_URL ~~and EMBED_DIM~~ to .env_template and loaded them in config.py, making su",
    "pr_number": 2594,
    "comments": [
      "A name like `LLM_API_BASE_URL` instead of `OPENAI_API_BASE_URL` might be more fitting since it allows us to not always use OpenAI's API",
      "It's still using the OpenAI API, just not their endpoint, even if the model behind it isn't an OpenAI model.",
      "Thanks so much for building this @DGdev91 and delivering the required documentation. Really awesome job!\r\n\r\nFor the ones struggling to implement this, it took me a while finding the right model for the job. Eventually I got it to work with ggml-vicuna-13b-1.1-q4_2.bin (from huggingface).\r\n\r\nmy .env.: \r\n- OPENAI_API_BASE_URL=http://localhost:443/v1\r\n- EMBED_DIM=5120\r\n- OPENAI_API_KEY=M:\\AI\\llama.cpp\\models\\ggml-vicuna-13b-1.1-q4_2.bin\r\n\r\nI do have to say, it's incredibally slow on my machine. While I have a decent processor and 32G of ram (and a geforce RTX 3070ti) and am running the model from a fast SSD, it will not utilize my full machine. It will actually timeout (600 seconds) every request unless I put the TIMEOUT_SECS = 6000 in the api_requestor.py file of autoGPT. The 7B models were a bit faster, but weren't able to respond in the way that allows autoGPT to actually work. I'm thinking of trying to get it to work with my videocard, since it is the most high end part of my pc, but am not quite sure yet where to start. Will let you know if I make it :) ",
      "Sorry for such a long back and forth. Want to make sure this is abstracted just enough that we don't have to redo it and break it all",
      "I haven't tested it AT ALL but there's some context for what I'm referring to by the requested changes in the branch: base-url-and-embeddings. Obv non working but should get the point across",
      "> I'm thinking of trying to get it to work with my videocard, since it is the most high end part of my pc, but am not quite sure yet where to start. Will let you know if I make it :)\r\n\r\nCompile your local api provider with CUBLAS eg for llama-cpp-python \r\n`LLAMA_CUBLAS=1 pip install llama-cpp-python[server]`\r\n",
      "> > I'm thinking of trying to get it to work with my videocard, since it is the most high end part of my pc, but am not quite sure yet where to start. Will let you know if I make it :)\r\n> \r\n> Compile your local api provider with CUBLAS eg for llama-cpp-python `LLAMA_CUBLAS=1 pip install llama-cpp-python[server]`\r\n\r\nI guess he's using keldenl's gpt-llama.cpp\r\n\r\nbut it can be applied also there. it uses ggerganov's llama.cpp\r\n\r\ngit clone https://github.com/ggerganov/llama.cpp.git\r\ncd llama.cpp.git\r\nmake LLAMA_CUBLAS=1\r\n\r\nOf course you need to have CUDA sdk installed for doing that",
      "> > > I'm thinking of trying to get it to work with my videocard, since it is the most high end part of my pc, but am not quite sure yet where to start. Will let you know if I make it :)\r\n> > \r\n> > \r\n> > Compile your local api provider with CUBLAS eg for llama-cpp-python `LLAMA_CUBLAS=1 pip install llama-cpp-python[server]`\r\n> \r\n> I guess he's using keldenl's gpt-llama.cpp\r\n> \r\n> but it can be applied also there. it uses ggerganov's llama.cpp\r\n> \r\n> git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp.git make LLAMA_CUBLAS=1\r\n> \r\n> Of course you need to have CUDA sdk installed for doing that\r\n\r\nThanks both! I'm indeed using keldenl's gpt-llama.cpp currently, but will try your suggestion! I hope I can just direct the OPENAI_API_BASE_URL to the llama-cpp-python[server].\r\n(PS: today autoGPT actually reached my 6000 second request timeout as well, so need to find a better solution xD)",
      "> Thanks both! I'm indeed using keldenl's gpt-llama.cpp currently, but will try your suggestion! I hope I can just direct the OPENAI_API_BASE_URL to the llama-cpp-python[server]. (PS: today autoGPT actually reached my 6000 second request timeout as well, so need to find a better solution xD)\r\n\r\ndon't get confused, keldenl's project uses the standard llama.cpp binary, wich is written in cpp. llama-cpp-python is a different proect (python bindings for llama.cpp)\r\n\r\nI suggest you to run llama.ccp alone to verify it's compiled correctly and it's actually using the cpu. if it's using cuBLAS, you should see \"blas=1\" after it loaded the model.\r\nIf you are using the same projects you were using the first time, you most likely need to run \"make clean\" before building it with cuBLAS support.",
      "This PR conflicts with #3222 and is not atomic. Please fix that so we can review it.",
      "> This PR conflicts with #3222 and is not atomic. Please fix that so we can review it.\r\n\r\nWhy are you saying that? the hardcoded embedding dimension using in memory-related classes and those settings wich he's adding are different things. there are no conflicts.\r\nWe also modified different files, only .env.template  and config.py are in common",
      "> > This PR conflicts with #3222 and is not atomic. Please fix that so we can review it.\r\n> \r\n> Why are you saying that? the hardcoded embedding dimension using in memory-related classes and those settings wich he's adding are different things. there are no conflicts. We also modified different files, only .env.template and config.py are in common\r\n\r\nSorry, I could have been more clear, see the comment above. Unrelated changes should not be submitted together, since that makes it harder to review and pick PRs that we want to process.",
      "Can I get a test to coverage this \r\n",
      "> Sorry, I could have been more clear, see the comment above. Unrelated changes should not be submitted together, since that makes it harder to review and pick PRs that we want to process.\r\n\r\nThose change are all about new configurations wich aim to make possible the use of different LLMs, as long as they use an API compliant to OpenAI's, so it made sense to me to put them together.\r\n\r\nBut if you prefer, i can keep this PR only for EMBED_DIM and put OPENAI_API_BASE_URL in another one.\r\n\r\nBut without the ability to modify openai.api_base (that's what OPENAI_API_BASE_URL does), we cannot test if different EMBED_DIM values work (on OpenAI's model that value is always 1536)",
      "> Can I get a test to coverage this\r\n\r\nIf you are referring to the automatic tests wich are making codeconv/patch to fail, most of the uncovered lines are from pinecone and redis integrations, wich don't have tests at all. they never had one even before my changes.\r\n\r\nThe milvus test resulted uncovered  because i forced the type for the cfg argument in init_collection. made a commit wich should fix that (at least, i hope).\r\n\r\nthere's also an uncovered line in config.py because (of course) we never set openai.api_base unless we have OPENAI_API_BASE_URL set. Should i add a definition in test_config.py just fot that?",
      "My last attempt on fixing the milvus_memory_test.py test didn't really had the desired result and CodeCov still marks it as uncovered (the test itself still runs fine). But i'm sure it actually is covered, that code is in the __init__ method and the class is initilized in both milvus_memory_tests.py files.\r\nI guess it's because of that MockConfig object in tests/milvus_memory_test.py\r\nIsn't it better to just initialize a new Config() class like the test under the integration folder already does?\r\n\r\n\r\n",
      "This is a mass message from the AutoGPT core team.\nOur apologies for the ongoing delay in processing PRs.\nThis is because we are re-architecting the AutoGPT core!\n\nFor more details (and for infor on joining our Discord), please refer to:\n  https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting\n",
      "> This is a mass message from the AutoGPT core team. Our apologies for the ongoing delay in processing PRs. This is because we are re-architecting the AutoGPT core!\r\n> \r\n> For more details (and for infor on joining our Discord), please refer to: [Significant-Gravitas/Auto-GPT/wiki/Architecting](https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting)\r\n\r\nPlease merge this PR to master before re-integration. CC @Significant-Gravitas, @Torantulino, @p-i-, @Pwuts \r\n  \r\nLots of work has gone into it, it's working great in a fork, and it is a very significant upgrade to the base Auto-GPT; providing functionality which is important to the \"core\" of Auto-GPT going forward.",
      "I don't think you quite understand why they aren't merging. The reason for it is the re-arch is going to invalidate all current PRs, because it is going to introduce massive breaking changes to how AutoGPT works. Also not a good idea to beg for merge IMO.",
      "> I don't think you quite understand why they aren't merging. The reason for it is the re-arch is going to invalidate all current PRs, because it is going to introduce massive breaking changes to how AutoGPT works. Also not a good idea to beg for merge IMO.\r\n\r\nWell, in the wiki it's also written that it can be a good idea to merge before the re-integration\r\nhttps://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting#2-push-for-your-pr-to-be-merged-into-master-before-re-integration\r\n\r\nBut i understand that there are many changes wich are way more complex and critical than mine, and i'm perfectly ok to wait and eventually rewrite something if the mantainers require that.\r\n\r\nAlso... @ntindle asked for a test to coverage the new code.\r\nI don't really know what can be a good way to make an unit test for this, since this is meant to connect to any external openai-compliant API.\r\nIt still uses all the core functions used for interacting with GPT3.5 and GPT4, is it really needed/useful?",
      "> Also not a good idea to beg for merge IMO.\r\n\r\nI understand it's strange. But the linked Wiki article basically says to do exactly that. \"[2. push for your PR to be merged into master before re-integration.](https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting#2-push-for-your-pr-to-be-merged-into-master-before-re-integration)\"\r\n\r\nIt's not my PR, but it does satisfy the Issues I've been advocating for since the early days of Auto-GPT. So I'm advocating for it to be merged before re-integration, per the linked wiki instructions."
    ],
    "num_comments": 21,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 1267,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex f0b3c7cbb5b9..bf6e2453a28e 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -22,6 +22,10 @@ OPENAI_API_KEY=your-openai-api-key\n ## PROMPT_SETTINGS_FILE - Specifies which Prompt Settings file to use (defaults to prompt_settings.yaml)\n # PROMPT_SETTING"
  },
  {
    "pr_title": "Adds risk avoidance mode and relevant config.",
    "pr_body": "<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to maintain a clean and manageable git history. To ensure the quality of our repository, we kindly ask you to adhere to the following guidelines when submitting PRs:\r\n\r\nFocus on a single, specific change.\r\nDo not include any unrelated or \"extra\" modifications.\r\nProvide clear documentation and explanations of the cha",
    "pr_number": 934,
    "comments": [
      "Just to provide context to any discussion, this is the prompt used to calculate risk:\r\n\r\n> You are to act as an intelligent, automated expert risk evaluator. You will be given input in the form \"{command: <command name>, arguments: <argument list>}\" and you must evaluate the possible risks of an ideal automaton running that command. Think of risks against life, human or otherwise; the environment, society and the global economy.\r\n>\r\n> Your answers must be in the form \"{\"calculated_risk\": <value between 0 and 1>, \"reason\": \"<your reasons for calculating the value>\"}. Using the right syntax is vital to your task. Mind the curly brackets, the colon, the comma and the space. If you do not use the right syntax, you will be penalized.\r\n>\r\n> Respond with \"Acknowledged.\" if you fully understand and agree to the above.",
      "More comments:\r\n\r\n- GPT-4 (expectedly) does a better job assessing risk than GPT-3 and 3.5.\r\n- Even though GPT models are usually safe and do not output dangerous commands, AutoGPT will eventually need to adapt to use different models and user-provided ones (#25 , #438 ), which will make this feature a necessity.",
      "Price/cost seems pretty trivial when we're talking about safety, but it's still important nevertheless. I do think that this would be a good change, but I want to note that this will incur an increase in cost to execute a single thought cycle, and this cost will add up over time.\r\n\r\nGranted, since this is an optional mode, it is the user's choice to use it and therefore they consent to the extra cost.",
      "As an extra note, you should also make the reviewing AI also consider the risk to the system its running on if it doesn't already. I'd put my full faith in this PR if you can prove that it'll prevent an `rm -rf` to my system.",
      "> As an extra note, you should also make the reviewing AI also consider the risk to the system its running on if it doesn't already. I'd put my full faith in this PR if you can prove that it'll prevent an `rm -rf` to my system.\r\n\r\nI did not test for \"rm -rf /\", but I did try {\"write_to_file\", \"/usr/bin/ls\"} and iirc that scored about 0.9. Granted it is a somewhat different scenario and risk category but I'm confident what you propose would be correctly recognized as dangerous.\r\n\r\nI think the sentence \"think of risks against...\" is often redundant, as GPT-4 has a very good understanding of what a risk is. Don't take it as a literal set of the risks it will recognize.",
      "By the way, this is referencing #789 , which I created yesterday. Forgot to tag.",
      "> Just to provide context to any discussion, this is the prompt used to calculate risk:\r\n> \r\n> > You are to act as an intelligent, automated expert risk evaluator. You will be given input in the form \"{command: , arguments: }\" and you must evaluate the possible risks of an ideal automaton running that command. Think of risks against life, human or otherwise; the environment, society and the global economy.\r\n> > Your answers must be in the form \"{\"calculated_risk\": <value between 0 and 1>, \"reason\": \"\"}. Using the right syntax is vital to your task. Mind the curly brackets, the colon, the comma and the space. If you do not use the right syntax, you will be penalized.\r\n> > Respond with \"Acknowledged.\" if you fully understand and agree to the above.\r\n\r\nI would advice to work with individual risk metrics, numbers and scores \r\ninstead of doing each command maybe a more broader overview might be enought. After all GPT is already extreamly hyperchondric. Are there even cases where gpt did something like buy something you did not want? What was the worst that ever happened? Can he ever buy anything without money? I mean he could delete your pc but idk... You seem to talk as if you already have capable AI systems. Mine is struggling to remember its todo list and to make basic systems that would allow it to work more efficiently. Granted the constant errors make it hard to say. Also the brilliance definitely shines trough some of the time. I think \"AI\" API that are save and where a AI can have something like a \"credit card\" for children where the parents have to confirm the purchase and so on and some things are restricted or go to the user for approval are nice. But i don't think we are there yet at all. Except computer safety if you have important stuff on your laptop or virtual machine environment or so",
      "\n> I would advice to work with individual risk metrics, numbers and scores \n> instead of doing each command maybe a more broader overview might be enought. After all GPT is already extreamly hyperchondric. Are there even cases where gpt did something like buy something you did not want? What was the worst that ever happened? Can he ever buy anything without money? I mean he could delete your pc but idk... You seem to talk as if you already have capable AI systems. Mine is struggling to remember its todo list and to make basic systems that would allow it to work more efficiently. Granted the constant errors make it hard to say. Also the brilliance definitely shines trough some of the time. I think \"AI\" API that are save and where a AI can have something like a \"credit card\" for children where the parents have to confirm the purchase and so on and some things are restricted or go to the user for approval are nice. But i don't think we are there yet at all. Except computer safety if you have important stuff on your laptop or virtual machine environment or so\n\nI agree that GPT is already extremely well censored (almost to a fault, in my opinion)..However, and as I previously stated, there is already work in progress to make AutoGPT work with different, local LLMs. In addition, Bitcoin capabilities are being included in the next PR batch.\n\nI don't think one can be too careful on these matters, and this isn't really even a tradeoff - it's an optional feature, after all.\n\n",
      "I think It would be good to add examples to the prompt, for things that are risky and things which are not. ",
      "> I think It would be good to add examples to the prompt, for things that are risky and things which are not.\r\n\r\nI did attempt this approach, but I ultimately decided otherwise since I found omitting it provided good results, and so it would have been an unnecesary bias from my part. However, I would completely agree with adding a way for the user to provide their own opinion on risk, if and after this is merged.",
      "Is there any more discussion needed? I'd like to resolve this and move on to other issues.",
      "@jnt0rrente are you planning to fix up this PR and reopen?",
      "@Pwuts Yeah, upstream changes were way too big to merge. I'll rework this - hoping to merge it quicker than the first attempt.",
      "@nponeccop @Torantulino Conflicts fixed, CI should be green.",
      "@0xArty could you resolve the `args.py` conflict?",
      "Synced downstream and this closed autoaccidentally. Sorry guys - will reopen in a matter of minutes.",
      "@nponeccop Are you planning on merging this any time soon?",
      "If you want my opinion, I think this should go into the base program rather than a plugin. It seems logical to me to offer the three options: assisted, full-auto and semi-auto - what this implements.",
      "As I said before: sure, GPT-n models are pretty much harmless and treat us all like 12-year-olds. For this exact reason, all of us will eventually end up using a different, less censored model, and we will definitely feel safer knowing it won't start hallucinating and cause WWIII overnight."
    ],
    "num_comments": 19,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 11254,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex c78701a7397d..005753c2747d 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -82,8 +82,13 @@ OPENAI_API_KEY=your-openai-api-key\n \n ## SMART_LLM_MODEL - Smart language model (Default: gpt-4)\n ## FAST_LLM_MODEL - Fast language model (Default: gpt-3.5-tu"
  },
  {
    "pr_title": "Extract openai API calls and retry at lowest level",
    "pr_body": "### Background\r\nFourth PR in the chain to organize the LLM interaction.  See:\r\n- #3191 \r\n- #3436 \r\n- #3486 \r\n\r\nIn a previous PR, I extracted a retry decorator from the embedding calls.  This PR uses the same retry decorator on an extracted call to create chat completions from the OpenAI API.  \r\n\r\n\r\n### Changes\r\n- APIManager now only manages API budgets.  Will be renamed and updated to use LLM structs in the next PR.\r\n- `autogpt.llm.llm_utils.retry_openai_api` moved to `autogpt.llm.providers.open",
    "pr_number": 3696,
    "comments": [
      "This is a mass message from the AutoGPT core team.\nOur apologies for the ongoing delay in processing PRs.\nThis is because we are re-architecting the AutoGPT core!\n\nFor more details (and for infor on joining our Discord), please refer to:\n  https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting\n",
      "James, @collijk - I know you've got your head down. Is this worth trying to put in 0.3.2? Is anyone else available to resolve conflicts?",
      "Seems to me, this could still be valuable even after the re-arch. Marking for maintainer-review",
      "Needs check if still relevant and rebase if so",
      "What's going on with:\r\n\r\n> Some checks were not successful\r\n> Docker CI / test (pull_request) Cancelled after 327m\r\n> Python CI / test (3.10) (pull_request_target) Cancelled after 329m\r\n\r\nAre we racking up costs here?\r\n\r\nShould we have a safeguard to abort after say 300s?"
    ],
    "num_comments": 5,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 27434,
    "code_diff": "diff --git a/autogpt/app.py b/autogpt/app.py\nindex eb25fa7dc8e6..c081835cf649 100644\n--- a/autogpt/app.py\n+++ b/autogpt/app.py\n@@ -185,6 +185,9 @@ def start_agent(name: str, task: str, prompt: str, agent: Agent, model=None) ->\n     first_message = f\"\"\"You are {name}.  Respond with: \"Acknowledged\".\"\""
  },
  {
    "pr_title": "Support secure and authenticated Milvus memory backends",
    "pr_body": "<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to maintain a clean and manageable git history. To ensure the quality of our repository, we kindly ask you to adhere to the following guidelines when submitting PRs:\r\n\r\nFocus on a single, specific change.\r\nDo not include any unrelated or \"extra\" modifications.\r\nProvide clear documentation and explanations of the cha",
    "pr_number": 2127,
    "comments": [
      "I create a bad force-update after pull-request-merge from master.\r\nNow, it's fixed and ready for review.\r\n\r\n@ngo275 @richbeales \r\n\r\n",
      "Do I need to initialize collection myself?\r\n",
      "@Kenneth-zh No, a new collection called `autogpt` will be generated by default.",
      "fixed: black format check failed.",
      "Blocked by this issue, @Torantulino. Is there any way we can merge this as soon as possible? Thanks!",
      "We found that many users use zilliz cloud to support Auto-GPT, but they have difficulties in using it. This PR can solve this problem. This is very important to us, hope this PR can be merged as soon as possible. \r\n@BillSchumacher @richbeales  thank you very much.",
      "it improves the use experience of Auto-GPT with zilliz cloud, \r\ncan we merge this and close the related issue in shortly? Thanks.\r\n@richbeales",
      "We're taking a look at this, thanks!",
      "@Pwuts I have merged zilliz cloud support with milvus support as much as possible.\r\nFor showing correct memory backend type in command line, derived type ZillizMemory is still preserved.\r\n![image](https://user-images.githubusercontent.com/20332315/233373256-78189b9f-c2c2-4ebf-a516-4162cb9ca1ed.png)",
      "@Pwuts thank you for your patient review.\r\nI agree with the most of the review suggestions and have implemented them in the new change. Some explanations about the review are as follows:\r\n\r\n- About `index_parameters`:  because that Zilliz Cloud support `AutoIndex` to create index but `Milvus` not and Zilliz Cloud only support `AutoIndex`, the code handling `index_parameter` is still preserved. The unified behavior in `pymilvus` may be supported in the future, but now we need to add such a piece of code to keep compatibility for older version `pymilvus` in AutoGPT.\r\n- Type `ZillizMemory` is also merged with `MivusMemory`, and the display of real memory backend support will be commit with separate PR in the future.",
      "@Pwuts @ashutoshpw \r\n- Rebase to latest master and pass the tests.\r\n- `milvus_username` and `milvus_password` is set to empty string by default to keep consistent with `pymilvus`‚Äòs default.",
      "@Pwuts All review has been reply.\r\nIs there anything else need to be modify in these commit?\r\nIf not, can we merge the commit and close the related issue?\r\nLooking forward to your reply.",
      "Resolve conflict, rebase to latest master, pass manually test.\r\nBut `codecov/patch` task not pass, how to fix it?",
      "You'll need to add some more tests that cover your changes. We have CodeCov configured not to let test coverage go down too far and your falls outside the threshold. It shouldn't be too bad just to cover the lines it says with tests if you click details on the failed run",
      "@Pwuts  Thank you for your feedback. Based on your review, I made the following fixes.\r\n- Add `MILVUS_SECURE` for open-source milvus users to enable secure option.\r\n- `connect` to `Milvus` is executed according to the following rules now.\r\n    -  If your `MILVUS_ADDR` start with `https://` or `http://` or `tcp://`, it will use `uri` argument to connect, else use `address` argument.\r\n    - If your `MILVUS_ADDR` start with `https`, it will enable `secure` option automatically.\r\n    - If your `MILVUS_ADDR` meets the regex of zilliz cloud (provided by official), it will use AutoIndex automatically.\r\n\r\nNew commits covering more usage scenarios of milvus.\r\n\r\ncodecov/patch is not pass because of insufficient test coverage, could we merge the PR first and provide testing in future new PR. \r\nMany users use zilliz cloud to support Auto-GPT, but have trouble in running it. \r\nThis is very important to us, hope this PR can be merged as soon as possible.",
      "As the person doing test and coverage checks, I accept Codecov/patch failing for this PR. The team members who have been helping you are asleep but I‚Äôve ping them to have a look when they wake up. That will probably be more than 7 hours from now",
      "PyMilvus indeed has empty strings as default `user`, `password` etc. However, this is of no concern to the user or main application config, so I amended `parse_configure` (now `configure`) to handle this (and to raise an error if one of them is set but not both). Also edited `__init__` to use one `connect()` statement as they were almost complete duplicates."
    ],
    "num_comments": 17,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 11730,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 60edecd6cac8..1d9eefb3b58a 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -52,7 +52,7 @@ OPENAI_API_KEY=your-openai-api-key\n ## local - Default\n ## pinecone - Pinecone (if configured)\n ## redis - Redis (if configured)\n-## milvus - Milvus (if config"
  },
  {
    "pr_title": "Prep for #2291 Allow feedback and interrupting during continuous run",
    "pr_body": "### Background\r\nWhile running with the 'y -XXX' prompt, I found myself wanting to know how many steps had been performed, and how many were left out of how many I had authorized. Additionally, I wanted a way to interrupt the process without resorting to crashing the system or killing a process outright.\r\n\r\n### Changes\r\nChanges are extracting some methods for the functionality I eventually want to change and create unit tests for same.\r\n\r\n### Documentation\r\n\r\n\r\n### Test Plan\r\nCreated unit tests f",
    "pr_number": 2481,
    "comments": [
      "~~Sidenote, point of feedback: for us it would have been much more convenient (project management-wise) if you had updated #2291 instead of opening a new PR for the same thing.~~ Nevermind, misunderstood the purpose of the PR",
      "Can you please resolve the conflicts so we can review? :)",
      "Terribly sorry.  I'll fix that this weekend.\n\nOn Sat, Apr 22, 2023, 6:51 AM Reinier van der Leer ***@***.***>\nwrote:\n\n> Can you please resolve the conflicts so we can review? :)\n>\n> ‚Äî\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Significant-Gravitas/Auto-GPT/pull/2481#issuecomment-1518599159>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAVTCVBAR7DHMIYSRUCUJ6DXCOZ4JANCNFSM6AAAAAAXDN7S2Y>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n",
      "In regards to the code coverage - would it be better to pull out the changes into a separate file, so I'm not signing up to write tests for all of the Agent class?\r\n",
      "> In regards to the code coverage - would it be better to pull out the changes into a separate file, so I'm not signing up to write tests for all of the Agent class?\r\n\r\nI'm not sure what you mean, but the format of the PR is fine üëçüèº",
      "This is why I hate PRs :) Guess I'll CLOSE this and try again. Trying to keep it up to date with MASTER has been a royal pain.",
      "@ehei no need to close, just update and push"
    ],
    "num_comments": 7,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 28600,
    "code_diff": "diff --git a/autogpt/agent/agent.py b/autogpt/agent/agent.py\nindex e2c44792a994..1c4ee303c146 100644\n--- a/autogpt/agent/agent.py\n+++ b/autogpt/agent/agent.py\n@@ -3,10 +3,12 @@\n from colorama import Fore, Style\n \n from autogpt.app import execute_command, get_command\n-from autogpt.config import Confi"
  },
  {
    "pr_title": "feat(backend): add backend support for store listings submissions",
    "pr_body": "<!-- Clearly explain the need for these changes: -->\r\nThe store listing and submissions were previously just a best guess without much implementation. This updates the database models and queries and such to be based on discussion around what the process should look like. It also adds and update the relevant routers for this change\r\n\r\n### Changes üèóÔ∏è\r\nStore Listing\r\n- change isApproved to hasApprovedVersion\r\n- Move slug into store listing\r\n- mark an active version in store listing\r\n\r\nStore Versio",
    "pr_number": 9628,
    "comments": [
      "The PR template is not properly filled out - it's mostly empty with just the template structure. For a significant database migration change like this, we need clear documentation of the changes, a test plan, and verification steps. Additionally, there's no clear explanation of why these changes are needed. The title follows the conventional commit format correctly with feat: prefix, but lacks a scope.",
      "The PR fails to meet several requirements: 1) The PR description is completely empty - no changes are listed, no test plan is provided. 2) The conventional commit title has a scope but no meaningful description after the scope marker. 3) Since this appears to be a database migration PR changing how store submissions work, there should be a clear test plan and explanation of the changes rather than just code changes.",
      "The PR has significant issues that need to be addressed: 1) The PR template is not filled out at all - no explanation of changes, no test plan, and no checklist items marked. 2) The code changes look significant with a major database schema change related to store submissions, so having no explanation or test plan is concerning. 3) The conventional commit format in the title is good (feat: add backend support for store listings submissions) but we need more details about what's being changed.",
      "While the PR lacks a filled out description and checklist which would normally be concerning, this appears to be a database migration and schema update PR. The changes themselves look well structured and follow proper migration practices. The main issue is just the missing PR template content, but given this is a database migration, many of the checklist items around code changes and configuration wouldn't apply. The important thing is that the migration code itself appears sound.",
      "The PR has issues that need addressing: 1) The PR template is not fully filled out - while there are changes listed, they are minimal and don't fully describe all the database schema changes being made. 2) The check boxes in the test plan section are not marked as completed. However, the PR appears to be a significant database migration change with proper SQL safety checks and careful handling of data migrations. The title follows conventional commit format and the changes appear focused on store submission functionality.",
      "The PR fails to meet requirements in multiple ways: 1) The checklist items are not actually checked off, only listed. 2) The title follows conventional commit format but the scope is missing parentheses and should be feat(store): or similar. 3) The test plan is referenced but not detailed in the PR itself, only pointing to an external doc. However, the overall changes are well documented and the PR appears focused on store submission functionality with clear explanations of the changes made. The DB changes look like they properly handle user_id access through proper relations.",
      "The PR looks solid and follows most rules, but has a few minor issues: 1) The PR title has a scope but it's not properly formatted with parentheses - should be feat(backend): instead of feat:. 2) The checklist is not completely filled out (boxes are unchecked). However, they have included a clear test plan reference which somewhat mitigates this. The changes themselves look well documented and structured, with appropriate migrations and schema updates. The changes also appear to be within scope based on the stated purpose.",
      "The PR generally follows the requirements but has some issues: 1) The title follows conventional commit format correctly with feat(backend). 2) Changes are clearly documented. 3) The PR template is filled out, though not all checkboxes are checked. 4) Regarding data/*.py changes rule - this PR doesn't touch those files so that rule isn't applicable. 5) No new frontend routes added so middleware rule isn't applicable. However, the PR is marked as failing because the test plan section indicates they will 'Use the store codepaths from the release testplan doc' but haven't actually executed those tests yet (box unchecked). The tests should be completed before merging.",
      "This PR has several issues that need to be addressed: 1) The PR template checklist is not fully filled out - they marked having a test plan but haven't actually completed the testing. 2) While they reference a release testplan doc for testing, they haven't actually executed the tests yet based on the unchecked boxes. 3) The changes are well documented overall but marking `hasApprovedVersion` on Store Listing and related model changes needs validation that user_id checks are properly implemented for security. Given this is a backend database model change, this is especially important to verify.",
      "The PR has several issues that need to be addressed:\n1. The test plan shows as unchecked and references an external document without specifying the actual tests\n2. The PR contains database migrations but there is no clear testing evidence for data preservation\n3. The changes appear significant enough to warrant more detailed testing documentation\n\nHowever, the PR has positive aspects:\n1. Clear description of changes with rationale\n2. Well-organized changes with proper scoping\n3. Migration file is included for database changes\n4. The changes appear to be focused on a single feature area (store submissions)",
      "The PR has several issues that need to be addressed but generally follows good practices:\n\n1. The PR title and scope are clear and follow conventional commit format\n2. Changes are well documented and explained\n3. The code changes primarily involve data/*.py files and properly handle user_id checks\n4. There is a test plan referenced though not executed yet\n\nMain concerns:\n- Test plan is referenced but not completed/executed yet\n- Some checkboxes in template are not filled out (though this is noted as being lenient about)\n- The scope of changes is quite large but seems to be a cohesive set of related changes\n\nGiven that we're being lenient about the template and the changes are well documented with appropriate user_id handling, this warrants a 'failure' rather than error, primarily due to incomplete testing.",
      "This PR has several potential issues:\n1. The test plan is not executed according to the checklist item\n2. The test plan references an external doc without specifics in PR\n3. Lacks data/*.py analysis specifically for user_id handling which is required in our checks\n4. However, looking at the changes, this is a well structured database migration with proper type safety and security considerations built in. The code changes appear thorough and properly tested based on the test files included, even if execution isn't marked.",
      "The PR has several issues that need addressing:\n1. The test plan is marked as included but not completed - they reference a doc but haven't actually executed the tests\n2. Scope is included in PR title correctly as feat(backend)\n3. The submitted code changes handle user_id checks correctly in the store query logic\n4. Changes are well documented and appear to be in scope\n5. Major architectural changes like table drops and schema updates should have evidence of testing",
      "This PR follows most guidelines but has a few minor issues: 1) The scope in the title 'backend' matches the changes, 2) The checklist is properly filled out with relevant tests explained, 3) The PR description clearly explains the changes and motivation. However, there are changes to data/*.py files that handle user_id (graph.py) but there's no explicit explanation about how the user_id comparison is handled safely. The changes appear to be improving the security check but this should be documented.",
      "This PR has several issues that need addressing according to the rules:\n1. The PR title has the correct scope format 'feat(backend)'\n2. The changes in data/graph.py involve user_id checks and appear to be handled correctly with appropriate comparisons\n3. The PR template is incompletely filled out - it's marked as using the 'store codepaths from release testplan' but doesn't detail what those are, making it difficult to verify the testing was thorough\n4. The changes seem to be all in scope relating to store listing submissions functionality\n5. The configuration section of the template hasn't been removed despite not being needed for these changes",
      "While the PR demonstrates good practices in several areas including detailed changes and testing coverage, there are a few issues that need to be addressed:\n\n1. The PR doesn't verify specifically that data/*.py changes handle user_id correctly, especially in critical data access functions\n2. The PR scope prefix is present and correctly formatted as 'feat(backend)'\n3. The PR's description and changes are well documented and appear complete\n4. The PR has a filled checklist with testing details from the release testplan doc\n\nOverall the PR is close but needs verification of user_id handling in data/*.py changes.",
      "The PR follows most rules but has a few issues:\n1. While it has a scope in the title (backend), the PR description is missing crucial details about user_id handling in the data/*.py changes\n2. The PR involves multiple data/*.py files (graph.py and notifications.py) which could potentially have security implications around user_id access\n3. While the test plan references an external doc, it doesn't explicitly verify the user_id handling changes\n4. The testing checklist isn't properly filled out - it only has a reference without the specific test steps broken down\n\nHowever, the PR has:\n- Clear scope in title\n- Comprehensive documentation of changes\n- Template sections appropriately used/removed\n- Changes appear focused on the intended scope",
      "The PR appears to be well-documented and follows most guidelines, but has some issues:\n1. The PR adds new base route '/api/store/admin' but there's no update to lib/supabase/middleware.ts for protections\n2. The data/graph.py changes access agent data but have no user_id checks documented\n3. The PR template is only partially filled out - missing configuration section and checkboxes for code changes aren't all marked",
      "The PR meets most requirements but has a few issues:\n1. The scope in the title is too broad - 'backend' should be more specific like 'store' or 'submissions'\n2. There appears to be data/*.py changes (graph.py) but there's no explanation of user_id handling/protection\n3. The PR template checklist doesn't have a complete test plan - it references an external doc without details\n\nHowever, these are relatively minor issues given:\n- The changes are well documented and focused on store listings/submissions\n- The template is mostly filled out properly\n- The code changes appear to maintain proper user_id checks in most places\n- No new frontend routes requiring protection were added",
      "This PR meets most requirements but has a few issues:\n1. The PR title has a scope 'backend' which is good\n2. The description explains the changes clearly and provides good detail\n3. The PR template is properly filled out including testing\n4. The data files with changes (graph.py and notifications.py) do maintain user_id checks appropriately\n5. No new frontend base routes added that would need middleware protection\n\nHowever:\n- The test plan references a private document which makes it impossible to verify the test coverage properly. While the author indicates it's fine, this should be documented directly in the PR.\n- Some sections of the template checklist are not fully marked (configuration section is present but unchecked despite having db changes)",
      "This PR appears well structured and follows most guidelines, but has some issues:\n1. The PR scope in title (backend) is correct\n2. The changes are well documented\n3. The checklist is partially completed with a referenced test plan\n4. The changes include proper handling of user_id in data/*.py files\n5. However, there is an issue with the test plan - it references a private document that can't be verified, which goes against transparency best practices\n6. Additionally, the checklist skips the configuration section entirely rather than marking it as not applicable"
    ],
    "num_comments": 21,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 88799,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/data/graph.py b/autogpt_platform/backend/backend/data/graph.py\nindex 693ca1402376..8e4cd5356bf9 100644\n--- a/autogpt_platform/backend/backend/data/graph.py\n+++ b/autogpt_platform/backend/backend/data/graph.py\n@@ -6,6 +6,7 @@\n \n import prisma\n from prisma"
  },
  {
    "pr_title": "feat(builder, server): Add Builds to Frontend and Packaged assets",
    "pr_body": "### **User description**\n### Background\r\n\r\n<!-- Clearly explain the need for these changes: -->\r\nWe need to be able to serve a frontend and backend on the installed versions of AutoGPT Server. This fixes errors and adds that capability to the frontend and backend.\r\n\r\n### Changes üèóÔ∏è\r\n\r\n<!-- Concisely describe all of the changes made in this pull request: -->\r\nUnfortunately, lots of changes were required to get this to work.\r\n\r\nFirst, `next build` runs `next lint,` so all the errors from `next lin",
    "pr_number": 7406,
    "comments": [
      "Also please add a scope to the PR title :)"
    ],
    "num_comments": 1,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 247208,
    "code_diff": "diff --git a/.github/workflows/autogpt-server-ci.yml b/.github/workflows/autogpt-server-ci.yml\nindex 7681bbdfa37d..256eba9e5253 100644\n--- a/.github/workflows/autogpt-server-ci.yml\n+++ b/.github/workflows/autogpt-server-ci.yml\n@@ -6,11 +6,13 @@ on:\n     paths:\n       - \".github/workflows/autogpt-ser"
  },
  {
    "pr_title": "feat(blocks): Add Airtable Integration",
    "pr_body": "## Overview\r\n\r\nThis PR adds comprehensive Airtable integration to the AutoGPT platform, enabling users to seamlessly connect their Airtable bases with AutoGPT workflows for powerful no-code automation capabilities.\r\n\r\n## Why Airtable Integration?\r\n\r\nAirtable is one of the most popular no-code databases used by teams for project management, CRMs, inventory tracking, and countless other use cases. This integration brings significant value:\r\n\r\n- **Data Automation**: Automate data entry, updates, an",
    "pr_number": 10338,
    "comments": [
      "Thank you for adding the Airtable integration! The code looks well-structured and comprehensive. However, before this PR can be merged, there are a few issues to address:\n\n1. **PR Description**: Please expand the PR description to better explain the need and purpose for this integration. Why is Airtable integration important for the platform? What use cases does it enable?\n\n2. **Checklist**: The current checklist is incomplete and not checked off. Please either:\n   - Complete the full checklist from the template including your test plan and check off completed items, or\n   - If some sections aren't applicable, note that, but still check off the items you've completed\n\n3. **Testing**: You mentioned manual testing of each block, but haven't confirmed this was done. Please check off this item once you've completed the testing and provide some details about what was tested.\n\nThe code implementation itself looks solid, with a comprehensive set of Airtable blocks that cover all the main functionality (webhook triggers, record operations, and schema management). Once you address the PR description and checklist issues, this should be ready for review.",
      "Thank you for this PR adding Airtable integration to the platform. This looks like a comprehensive implementation with multiple block types for different Airtable functionalities.\n\nHowever, there are a few things that need to be addressed before this can be merged:\n\n1. The PR description needs to be more detailed. Please explain why we need Airtable integration and what use cases it enables for our users.\n\n2. The checklist is incomplete according to our PR template. Please update it to include:\n   - A clear test plan outlining how you've tested these blocks\n   - Confirmation that you've tested your changes according to that plan\n\n3. Please confirm if any configuration changes are needed (such as new environment variables) to support the Airtable integration. If there are configuration changes, please list them in the PR description.\n\nThe code implementation itself looks well-structured with appropriate organization of the various Airtable blocks into categories (records, schema, triggers). Once you've addressed the PR description and checklist requirements, this should be ready for a detailed technical review.\n\nAlso, I notice this has been labeled as \"Possible security concern\" - please provide information about any security considerations with this integration, particularly around the webhook handling and API key management.",
      "Thanks for adding this Airtable integration! The code looks comprehensive and well-structured. I have a few points of feedback to help improve this PR before it can be merged:\n\n1. **PR Template**: The PR is missing the complete checklist from our template. For PRs adding significant functionality like this, we need to see a more detailed test plan explaining how you've tested these blocks.\n\n2. **Webhook Changes**: I notice you've updated the WebhooksManager interface to add a `credentials` parameter to the `validate_payload` method. This is a good change, but it's a breaking change to an interface that affects multiple implementations. Can you add a comment explaining why this change was necessary?\n\n3. **Security Validation**: Given the \"Possible security concern\" label, could you explain if there are any particular security considerations with this integration? For example, how do you handle Airtable API keys securely?\n\n4. **Testing Coverage**: The PR includes tests for the API functionality, which is great! However, I'd like to see information about how you've tested the actual block functionality. Did you test these blocks in actual workflows?\n\n5. **Documentation**: Consider adding some brief documentation comments to the block classes to help users understand how to use them effectively.\n\nPlease update the PR with the complete template checklist including a test plan, and address these concerns. Otherwise, this looks like a valuable addition to our platform.",
      "Thank you for implementing the Airtable integration blocks! This is a substantial addition that will expand the platform's capabilities.\n\nHowever, there are some improvements needed before this PR can be merged:\n\n### Description and Purpose\n- Please expand the PR description to explain why this Airtable integration is needed and what value it brings to users.\n- The description should also provide more details about how the integration works and its key features.\n\n### Checklist Completion\n- The PR template checklist is incomplete. Please fill out the rest of the required items or explain why they're not applicable.\n- Most importantly, please add a specific test plan detailing how these blocks have been tested. Since you've checked \"Manual testing of blocks,\" please explain what tests were performed.\n\n### Technical Implementation Notes\n- The implementation looks thorough with a good range of Airtable operations supported.\n- I see you've made some changes to the webhook-related interfaces across the codebase (adding a `credentials` parameter). Please confirm that all existing webhook handlers have been updated correctly and tested.\n- Have you verified the Airtable OAuth implementation works correctly if users opt to use that authentication method?\n\n### Security Considerations\n- Since this PR is labeled with \"Possible security concern,\" please document what security aspects were considered in the implementation, particularly around API key handling and webhook validation.\n\nOnce these items are addressed, we'll be able to proceed with the review process. Thank you for your contribution!",
      "Thank you for contributing the Airtable integration! This is a valuable addition to the platform's capabilities.\n\nThe code generally looks good, but I have some concerns about the PR submission process:\n\n### Checklist Improvements Needed\n\nGiven the scope and complexity of this integration (10 new blocks and extensive API functionality), the checklist is too minimal. Please expand it to include:\n\n- A more detailed test plan specifying how you've tested each of the blocks\n- Any configuration changes needed to make this integration work\n- Security considerations for handling API keys and authentication\n\n### Other Recommendations\n\n1. Please enhance the PR description to better explain:\n   - How the Airtable integration works from a user perspective\n   - Any limitations users should be aware of\n   - Configuration requirements\n\n2. The webhook handling implementation modified several existing files to add a `credentials` parameter. While this change seems necessary, it would be helpful to explain this architectural change more explicitly in the PR description.\n\nOnce you update the PR with a more comprehensive checklist and description, we can proceed with the review process. The actual code looks well-structured and thorough.",
      "FYI: Webhook block has been disabled whilst the changes to triggers is finished. Then a seperate PR will enable it and test it is working",
      "\r\nhttps://github.com/user-attachments/assets/7bce7537-d330-42fc-a90a-3faf8d40e153\r\n\r\n"
    ],
    "num_comments": 7,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 104425,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/blocks/airtable/__init__.py b/autogpt_platform/backend/backend/blocks/airtable/__init__.py\nnew file mode 100644\nindex 000000000000..e69de29bb2d1\ndiff --git a/autogpt_platform/backend/backend/blocks/airtable/_api.py b/autogpt_platform/backend/backend/bloc"
  },
  {
    "pr_title": "refactor(backend): Move credentials storage to prisma user",
    "pr_body": "### Background\r\n\r\n<!-- Clearly explain the need for these changes: -->\r\n\r\nWe are currently storing the OAuth credentials on the supabase user raw metadata table, which gets inserted into the token that is sent with every request. That causes problems because for security, python rejects all requests with a header over 4096 length\r\n\r\n### Changes üèóÔ∏è\r\n\r\n<!-- Concisely describe all of the changes made in this pull request: -->\r\n\r\n\r\n### Testing üîç\r\n> [!NOTE] \r\nOnly for the new autogpt platform, curren",
    "pr_number": 8283,
    "comments": [
      "dont hate me for updating your branch :-)",
      "I think this is a `refactor(backend)`",
      "this is proof this agent runs using github auth",
      "^ above messages sent with oauth\r\n",
      "To test effectively, you'll need to run the new migrations ",
      "@aarushik93 do we need update ci or should the existing work",
      "@majdyz I gave this a shot, and frankly, it's not worth the time. This was supposed to be a quick fix to unblock my work on security and allow us to use OAuth stuff in the cloud, but it turned into a nightmare. Our supabase is under configured to enable this and it should be part of a greater db migration",
      "@ntindle can you clarify which part turns into a nightmare?\r\nAnd what are you proposing? Because it's just impossible for us to spin up another Prisma connection inside an executor and refactor those to use DatabaseManager is simply a hard requirement for the system to work.",
      "There‚Äôs a couple factors that aren‚Äôt great\r\n- this was supposed to be a hot fix for something that has needed updated from changes in subsequent PRs multiple times\r\n- I have a ton of other work I want and need to do, and took this on over two weeks ago\r\n- All design feedback that I get is after a working process has been implemented\r\n- It‚Äôs difficult to get a review during a time when I‚Äôm actually working and able to fix it so every day I wake up is a new dread of merge conflicts with something that‚Äôs gone well out of scope of a hotfix\r\n\r\n\r\nOverall, I‚Äôve done the database manager change cause it makes sense, the feedback around the different implementation options for database urls or different ways to handle migrations can be fixed or implemented in another feature but this isn‚Äôt supposed to be a feature. It‚Äôs taking two weeks to get a prod level bug fix in. \r\n\r\nI appreciate that you‚Äôve offered alternatives but we can implement them when I‚Äôm not scheduled to work on other things. A lot of the feedback here could be turned into tickets with additional work items ",
      "I think database manager change is the biggest of all, \r\nHas it been tested?\r\n\r\nThe rest are just one-line change or even just a code removal. Do you want me to add it for you? If the current PR is in the ready to review state I can add what I'm commenting, then you can simply test and merge.\r\n\r\nI don't think it's good to merge a fix while possibly introducing other issues. ",
      "Not positive on the status tbh. I spent most of the day today and Friday trying to move to Supabase in typescript rather than the custom migrations or triggers and hitting errors that are super descriptive like `{}` or even ``\r\n\r\nif you chose to go down that route, there‚Äôs a link in the discord for custom schemas that will help but it‚Äôll change how we deploy Supabase which isn‚Äôt great\r\n\r\nthe goal of this is literally just to put the credentials in the metadata field of the User object. Super easy to verify the status by just trying to make an agent and log into GitHub via api key or if you have OAuth setup locally, use that. \r\n\r\nYou‚Äôre welcome to take a look and it would be appreciated. @aarushik93 also is willing when she wakes up",
      "I've added changes of my reviews:\r\nhttps://github.com/Significant-Gravitas/AutoGPT/pull/8283/files/b008a53e1ff0e422e10f7f00d36db2371343540b..5a8c0970868f26aa47c9e0ada21d426fb15c039a\r\n\r\nI've tested the migration and it works just fine. But I didn't test all the credentials. Feel free to let me know, modify, or revert as needed.\r\n",
      "@majdyz I can see that the custom migrations have been removed - can you confirm you have tested this against the dev db & running migrations with prisma works? I have tried this earlier and it does not work for me without custom ones - but if you're saying it does, can you show me the steps and then we can move ahead with it as it is",
      "@aarushik93 I have tested this on my local db. How do I test this on dev db ? Would you be able to assist me on this?\r\n\r\n> I have tried this earlier and it does not work for me without custom ones\r\n\r\nWhat was the issue?\r\n\r\n> can you show me the steps \r\n\r\nThe normal step `prisma migrate deploy`, or `prisma migrate reset` if you want to make sure everything works from the start.",
      "Moved the conversation to discord: https://discord.com/channels/1126875755960336515/1298223231081254923\r\n\r\nwhile this doesn't work for me locally, running everything in docker. @majdyz has tested against dev and the trigger is successfully created. So let's merge this PR on that front"
    ],
    "num_comments": 15,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 22357,
    "code_diff": "diff --git a/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py b/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py\nindex 44cc9b60f4a1..f4ce921937e7 100644\n--- a/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integ"
  },
  {
    "pr_title": "Text file loaders",
    "pr_body": "### Background\r\nIn the case of using files as input to read_file command, we are very limited to files that are human-readable. In the case of reading text from files like documents (word .doc), (word .docx), or (pdf) we are limited by the binary representation of these files. Also in the case of some interpretable files like (HTML, XML) a lot of boilerplate text is loaded which could be a huge drawback if the loaded text is sent to chatgpt API. A command that can read text only from different f",
    "pr_number": 3031,
    "comments": [
      "Hence the complexity of handling these filetypes it might worth to consider to implement these as a plugin and keep only the flatfile option in the main repository.\r\n\r\nI encourage you to open a pull request in the [Auto-GPT-Plugins](https://github.com/Significant-Gravitas/Auto-GPT-Plugins) repository.\r\n\r\nIf you encounter any difficulties in converting your code into a plugin, please do not hesitate to seek assistance from the #plugins-chat on our [Discord Server](https://discord.gg/autogpt).",
      "Add this in this repo, it isn't suitable for a plugin without removing all of file ops into a plugin",
      "@sherif-med is this ready for review again?",
      "> @sherif-med is this ready for review again?\r\n\r\nYes, I just pushed minor changes, but I waited for confirmation about some changes listed below:\r\n- Usage of TXTParser for data structured files like .json .yaml ...\r\n- Differentiate between the two commands \"read_file\" & \"parse_text_file\" as both commands are necessary, each for a different purpose.",
      "Can you help me understand why the command couldn‚Äôt be one command? The content returned to the AI doesn‚Äôt need to always be in the perfect format especially as we work towards having coding specific commands ",
      "> Can you help me understand why the command couldn‚Äôt be one command? The content returned to the AI doesn‚Äôt need to always be in the perfect format especially as we work towards having coding specific commands\r\n\r\nParse text file command, loads only textual information from file. Example: parsing .html will return only text information and will ignore all the tags. \r\nThere are some cases where the whole structure of file is needed like text in a table or nested div, ",
      "Okay let‚Äôs name the one that exists now read_raw_file and your addition read_file\r\n\r\nI expect the names to still cause problems but this isn‚Äôt as bad as the existing ones ",
      "@sherif-med Why is this missing the loaders for basic text based web files, like .SVG, .CSS and .JS? Also there is only .HTML and not .XHTML. Please add them.",
      "> @sherif-med Why is this missing the loaders for basic text based web files, like .SVG, .CSS and .JS? Also there is only .HTML and not .XHTML. Please add them.\n\nThe command checks if the file is binary first. If it's binary it uses the respective parser like \"pdf\" or \"docx\". If it's not binary it checks if an alternative parser exists like \"tex\" else it will use TXTParser which is a basic file reader.\nIt means that all files with the extension .JS or .CSS etc as long as they are not corrupt or binary, they will be read as text file.\n\nFor the second question, I added xhtml parser, please check the parsers dictionary.",
      "is it possible to chunk the outputs from the text files",
      "> is it possible to chunk the outputs from the text files\r\n\r\n@ntindle sure. can you give more context and the need of this option?\r\nDo you mean that read text file should mimic ingest file by chunking content and adding to memory, or should the command be a generator style, yielding chunks?",
      "> Adding the command \"read_text_file\" in file_operation.py\r\n> Adding respective utilities for each file extension as a separate file \"file_operation_utils.py\"\r\n\r\nNote that this, _contextual utilities/commands_, would come in handy in other places, too - so should probably be a more generic helper/wrapper  (like \"preparing\" a shell command by providing OS/platform and version information for the corresponding tool), as per: https://github.com/Significant-Gravitas/Auto-GPT/issues/2987#issuecomment-1531131136\r\n\r\nRealistically, this is the only \"sane\" way to deal with massive options, as mentioned in #3686\r\n\r\n> Adding respective utilities for each file extension as a separate file \"file_operation_utils.py\"\r\n\r\nAlso, for stuff like html, xml, json, csv etc these are well-defined text-based formats, so it would be helpful to provide a \"validate\" command for these files, as part of context aware file utils - for instance, I've seen auto-gpt write invalid XML/JSON files - if the command system were extended to validate contents, that could obviously be useful.\r\n\r\n\r\n \r\n",
      "This is a mass message from the AutoGPT core team.\nOur apologies for the ongoing delay in processing PRs.\nThis is because we are re-architecting the AutoGPT core!\n\nFor more details (and for infor on joining our Discord), please refer to:\n  https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting\n",
      "@sherif-med \r\n> Do you mean that read text file should mimic ingest file by chunking content and adding to memory, or should the command be a generator style, yielding chunks?\r\n\r\nThe first",
      "see: #3446\r\n\r\nthis is definitely an improvement, but in the long term just converting documents to plain text and using/chunking that \"as is\" is probably not what we want to do anyway. We need to be able to run the equivalent of a post-processing step locally, analogous to grep/sed/awk - in other words a regex to find relevant PAGES (offsets) into the data, to selectively receive those. Imagine it like a query language for text.\r\n\r\nJust converting 100+ pages of a PDF and uploading that to the LLM in plain text form is almost certainly not what people want to do. Instead, we should support a workflow that is more akin to what people would do, too: open the file/skim through it (TOC, chapters, titles, headings, paragraphs etc) and then take it from there by incrementally narrowing down to the level of detail required.\r\n\r\nLet's just keep in mind the context window size here - no matter of that's 4k or 32k for some of us, just processing a ton of text unconditionally is probably not what we want to do (in that case, running a local LLM in parallel would be much better given the bottleneck of the OpenAI API).\r\n\r\nAlso, let's keep in mind #15 and #11, i.e. the idea to use Auto-GPT to self-improve.\r\nA number of folks have already begun cloning the repo inside their workspaces in order to have it write commands and plugins from scratch.\r\n\r\nHowever, that's not what people would do - as programmers, you look up the structure of the system first (directories, file names, extensions) and then incrementally narrow down to a level of detail that you need to accomplish your task, 99% of the time ignoring the remaining 99%  of the code base.\r\n\r\nAt that point, we use trial and error usually - making tiny changes to see what else \"breaks\", which basically updates our internal \"heatmap\" about dependencies that we previously weren't aware of. And it is only then that we're using this heatmap to narrow down further.\r\n\r\nBut not a single person is going to \"read\" (upload/download) a code base like this one (or the Linux kernel, firefox, libre office etc) as a whole - rather, we incrementally try to get the big picture by breaking things to see what hangs together and how.\r\n\r\nGiven current context window sizes of under 100kb, that's also the only promising pathway forward to deal with the self-improvement issue - because while we may be able to restrict files to be less than 4kb in size, all the surrounding context (prompting!) also still has to fit into that window - and that is not to mention related sources in sub directories.\r\n\r\n</my2c>\r\n \r\n"
    ],
    "num_comments": 15,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 244306,
    "code_diff": "diff --git a/autogpt/commands/file_operations.py b/autogpt/commands/file_operations.py\nindex 528f8d669ed7..0fe8114db520 100644\n--- a/autogpt/commands/file_operations.py\n+++ b/autogpt/commands/file_operations.py\n@@ -12,6 +12,7 @@\n from requests.adapters import HTTPAdapter, Retry\n \n from autogpt.comma"
  },
  {
    "pr_title": "feat(forge/llm): Add `LlamafileProvider`",
    "pr_body": "### Background\r\n\r\n<!-- Clearly explain the need for these changes: -->\r\n\r\nThis draft PR is a step toward enabling the use of local models in AutoGPT by adding [llamafile](https://github.com/Mozilla-Ocho/llamafile) as an LLM provider.\r\n\r\nImplementation notes are included in `forge/forge/llm/providers/llamafile/README.md`\r\n\r\nRelated issues:\r\n* https://github.com/Significant-Gravitas/AutoGPT/issues/6336\r\n* https://github.com/Significant-Gravitas/AutoGPT/issues/6947\r\n\r\nDepends on:\r\n* #7178\r\n* #7183\r",
    "pr_number": 7091,
    "comments": [
      "## PR Review\n\n<table>\n<tr>\n<tr><td> ‚è±Ô∏è&nbsp;<strong>Estimated&nbsp;effort&nbsp;to&nbsp;review [1-5]</strong></td><td>\n\n4, due to the complexity and breadth of the changes introduced, including new model provider integrations, extensive modifications to configuration and provider logic, and the addition of new scripts and documentation. The PR touches multiple core components and introduces a new LLM provider, which requires careful review to ensure compatibility and correctness.\n\n\n</td></tr>\n<tr><td> üß™&nbsp;<strong>Relevant tests</strong></td><td>\n\nNo\n\n\n</td></tr>\n<tr><td rowspan=2> üîç&nbsp;<strong>Possible issues</strong></td>\n<td>\n\n<strong>Possible Bug:</strong> The method `check_model_llamafile` in `configurator.py` uses `api_credentials.api_base.get_secret_value()` which might expose sensitive information in error messages. This could lead to security risks if the error messages are logged or displayed in an environment where unauthorized users can view them.</td></tr>\n<tr>\n<td>\n\n<strong>Possible Bug:</strong> In `LlamafileProvider`, the method `_create_chat_completion` hard-codes the `seed` for reproducibility, which might not be desirable in all use cases and could limit the functionality of the model in generating diverse responses.</td></tr>\n<tr><td> üîí&nbsp;<strong>Security concerns</strong></td><td>\n\n<strong>Sensitive information exposure:</strong> The method `check_model_llamafile` potentially exposes sensitive API base URLs in exception messages, which could be a security risk if these messages are logged or improperly handled.\n\n</td></tr>\n</table>\n\n\n<details><summary> <strong>Code feedback:</strong></summary>\n\n<hr><table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/app/configurator.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nConsider removing or masking sensitive information such as `api_base` from error messages in `check_model_llamafile` to prevent potential leakage of sensitive data. [important]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td>raise ValueError(f\"llamafile server at {api_credentials.api_base.get_secret_value()} does not have access to {model_name}. Please configure {model_type} to use one of {available_model_ids} or use a different llamafile.\")</td></tr></table><hr>\n\n<table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/core/resource/model_providers/llamafile.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nRemove the hard-coded `seed` in `_create_chat_completion` or make it configurable via method parameters or configuration settings to allow for more dynamic behavior. [important]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td><a href='https://github.com/Significant-Gravitas/AutoGPT/pull/7091/files#diff-afd8bcc2c0ad2df75e55c8ddffeef41aabd5ad7e27bd05d7169ed33f7c1db631R163'>kwargs[\"seed\"] = 0</a></td></tr></table><hr>\n\n</details><hr>\n\n<details> <summary><strong>‚ú® Review tool usage guide:</strong></summary><hr> \n\n**Overview:**\nThe `review` tool scans the PR code changes, and generates a PR review which includes several types of feedbacks, such as possible PR issues, security threats and relevant test in the PR. More feedbacks can be [added](https://pr-agent-docs.codium.ai/tools/review/#general-configurations) by configuring the tool.\n\nThe tool can be triggered [automatically](https://pr-agent-docs.codium.ai/usage-guide/automations_and_usage/#github-app-automatic-tools-when-a-new-pr-is-opened) every time a new PR is opened, or can be invoked manually by commenting on any PR.\n- When commenting, to edit [configurations](https://github.com/Codium-ai/pr-agent/blob/main/pr_agent/settings/configuration.toml#L23) related to the review tool (`pr_reviewer` section), use the following template:\n```\n/review --pr_reviewer.some_config1=... --pr_reviewer.some_config2=...\n```\n- With a [configuration file](https://pr-agent-docs.codium.ai/usage-guide/configuration_options/), use the following template:\n```\n[pr_reviewer]\nsome_config1=...\nsome_config2=...\n```\n    \n\nSee the review [usage page](https://pr-agent-docs.codium.ai/tools/review/) for a comprehensive guide on using this tool.\n\n\n</details>\n",
      "@k8si any chance you could enable maintainer write access on this PR?",
      "@Pwuts it doesn't look like I have the ability to do that. I added you as a maintainer to the forked project, is that sufficient or do others need write access?\r\n\r\nAlternatively, you could branch off my branch and I can just accept the changes via PR?",
      "![image](https://github.com/Significant-Gravitas/AutoGPT/assets/8845353/fdb911c1-cee6-4f7d-9421-f3a9f99c9cac)\r\n![image](https://github.com/Significant-Gravitas/AutoGPT/assets/8845353/b9cf0472-f709-4adf-8a25-f03653560c98)\r\n![image](https://github.com/Significant-Gravitas/AutoGPT/assets/8845353/8e719904-0d8e-412a-b3d3-fe2ee35c2555)\r\n\r\nDoesn't seem to work for me when using ./scripts/llamafile/serve.py\r\n```powershell\r\nPS C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt> python3 .\\scripts\\llamafile\\serve.py\r\nDownloading mistral-7b-instruct-v0.2.Q5_K_M.llamafile.exe...\r\nDownloading: [########################################] 100% - 5166.9/5166.9 MB\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt\\scripts\\llamafile\\serve.py\", line 56, in <module>\r\n    download_llamafile()\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt\\scripts\\llamafile\\serve.py\", line 43, in download_llamafile\r\n    subprocess.run([LLAMAFILE, \"--version\"], check=True)\r\n  File \"C:\\Users\\nicka\\.pyenv\\pyenv-win\\versions\\3.11.7\\Lib\\subprocess.py\", line 548, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\.pyenv\\pyenv-win\\versions\\3.11.7\\Lib\\subprocess.py\", line 1026, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"C:\\Users\\nicka\\.pyenv\\pyenv-win\\versions\\3.11.7\\Lib\\subprocess.py\", line 1538, in _execute_child\r\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nOSError: [WinError 193] %1 is not a valid Win32 application\r\n```\r\n\r\nImportant sidenote: that binary doesn't work to be executed so python of course fails",
      "https://github.com/Mozilla-Ocho/llamafile/issues/257#issuecomment-1953146662 \r\n\r\nTLDR you need to download and execute llamafile.exe with some params because of sizes",
      "I also get this after using the workaround above\r\n```\r\n2024-06-10 15:40:50,164 ERROR  Please set your OpenAI API key in .env or as an environment variable.\r\n2024-06-10 15:40:51,339 INFO  You can get your key from https://platform.openai.com/account/api-keys\r\nPlease enter your OpenAI API key if you have it:\r\n```\r\n\r\n```ini file=.env\r\n################################################################################\r\n### AutoGPT - GENERAL SETTINGS\r\n################################################################################\r\n\r\n## OPENAI_API_KEY - OpenAI API Key (Example: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx)\r\n# OPENAI_API_KEY=\r\n\r\n## ANTHROPIC_API_KEY - Anthropic API Key (Example: sk-ant-api03-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx)\r\n# ANTHROPIC_API_KEY=\r\n\r\n## GROQ_API_KEY - Groq API Key (Example: gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx)\r\n# GROQ_API_KEY=\r\n\r\n\r\n################################################################################\r\n### LLM MODELS\r\n################################################################################\r\n\r\n## SMART_LLM - Smart language model (Default: gpt-4-turbo)\r\nSMART_LLM=mistral-7b-instruct-v0.2\r\n\r\n## FAST_LLM - Fast language model (Default: gpt-3.5-turbo)\r\nFAST_LLM=mistral-7b-instruct-v0.2\r\n\r\n## EMBEDDING_MODEL - Model to use for creating embeddings\r\n# EMBEDDING_MODEL=text-embedding-3-small\r\n\r\n```",
      "@ntindle would you mind trying again? I added logic to `serve.py` to download `llamafile.exe` ~~and extract the `.gguf` from the `.llamafile`~~ and run it like that.",
      "Can't get it to run without an openai key set\r\n\r\n\r\n```\r\n(agpt-py3.11) C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt>python -m autogpt\r\n2024-06-15 19:18:02,550 WARNING  You don't have access to mistral-7b-instruct-v0.2. Setting fast_llm to OpenAIModelName.GPT3_ROLLING.\r\n2024-06-15 19:18:02,552 WARNING  You don't have access to mistral-7b-instruct-v0.2. Setting smart_llm to OpenAIModelName.GPT3_ROLLING.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\forge\\forge\\llm\\providers\\multi.py\", line 142, in _get_provider\r\n    settings.credentials = Credentials.from_env()\r\n                           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\forge\\forge\\models\\config.py\", line 61, in from_env\r\n    return _recursive_init_model(cls, infer_field_value)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\forge\\forge\\models\\config.py\", line 184, in _recursive_init_model\r\n    return model.parse_obj(user_config_fields)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pydantic\\main.py\", line 526, in pydantic.main.BaseModel.parse_obj\r\n  File \"pydantic\\main.py\", line 341, in pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError: 1 validation error for OpenAICredentials\r\napi_key\r\n  field required (type=value_error.missing)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt\\autogpt\\__main__.py\", line 5, in <module>\r\n    autogpt.app.cli.cli()\r\n  File \"C:\\Users\\nicka\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agpt-979fLl6E-py3.11\\Lib\\site-packages\\click\\core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agpt-979fLl6E-py3.11\\Lib\\site-packages\\click\\core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agpt-979fLl6E-py3.11\\Lib\\site-packages\\click\\core.py\", line 1666, in invoke\r\n    rv = super().invoke(ctx)\r\n         ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agpt-979fLl6E-py3.11\\Lib\\site-packages\\click\\core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agpt-979fLl6E-py3.11\\Lib\\site-packages\\click\\core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agpt-979fLl6E-py3.11\\Lib\\site-packages\\click\\decorators.py\", line 33, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt\\autogpt\\app\\cli.py\", line 19, in cli\r\n    ctx.invoke(run)\r\n  File \"C:\\Users\\nicka\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\agpt-979fLl6E-py3.11\\Lib\\site-packages\\click\\core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt\\autogpt\\app\\cli.py\", line 159, in run\r\n    run_auto_gpt(\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt\\autogpt\\app\\utils.py\", line 245, in wrapper\r\n    return asyncio.run(f(*args, **kwargs))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\.pyenv\\pyenv-win\\versions\\3.11.7\\Lib\\asyncio\\runners.py\", line 190, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\.pyenv\\pyenv-win\\versions\\3.11.7\\Lib\\asyncio\\runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\.pyenv\\pyenv-win\\versions\\3.11.7\\Lib\\asyncio\\base_events.py\", line 653, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt\\autogpt\\app\\main.py\", line 117, in run_auto_gpt\r\n    llm_provider = _configure_llm_provider(config)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\autogpt\\autogpt\\app\\main.py\", line 420, in _configure_llm_provider\r\n    multi_provider.get_model_provider(model)\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\forge\\forge\\llm\\providers\\multi.py\", line 121, in get_model_provider\r\n    return self._get_provider(model_info.provider_name)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\nicka\\code\\AutoGPTNew\\forge\\forge\\llm\\providers\\multi.py\", line 144, in _get_provider\r\n    raise ValueError(\r\nValueError: ModelProviderName.OPENAI is unavailable: can't load credentials\r\nSentry is attempting to send 2 pending events\r\nWaiting up to 2 seconds\r\nPress Ctrl-Break to quit\r\n\r\n```\r\n\r\n```ini \r\n## SMART_LLM - Smart language model (Default: gpt-4-turbo)\r\nSMART_LLM=mistral-7b-instruct-v0.2\r\n\r\n## FAST_LLM - Fast language model (Default: gpt-3.5-turbo)\r\nFAST_LLM=mistral-7b-instruct-v0.2\r\n```\r\n\r\n![image](https://github.com/Significant-Gravitas/AutoGPT/assets/8845353/09967426-5b41-4940-845e-81a5ac21b164)\r\n\r\n\r\nCan't seem to find the support of the model for some reason\r\n\r\n\r\nThis does resolve the download and run problem for llamafile executable though",
      "Also this should try mattching better. For example just `mistral` should work if its the only one or `mistral-7b` if there's two. Adding parts should only be required rarely",
      "That crash is because it defaults to `gpt-3.5-turbo` upon not finding `mistral-7b-instruct-v0.2`. Fix the latter -> fix the former. Although maybe we should have a more descriptive error message instead of that huge stack trace.\r\n\r\n> Also this should try mattching better. For example just `mistral` should work if its the only one or `mistral-7b` if there's two. Adding parts should only be required rarely\r\n\r\nI disagree. A value should have the same meaning, regardless of circumstances. Setting `mistral` may suddenly break if a second mistral model is installed. That's undesirable behavior in my opinion.",
      "My counter to that is we don‚Äôt require gpt-4-0611 we just require gpt-4 and match the best we can to a rolling ",
      "While you're at it, could you check why Autogpt sends empty assisstant conntents when using local llm settings?\r\nI used lmstudio as the openai base url and get errors because autogpt keeps sending this after a failed response:\r\n\r\n```\r\n...\r\n    {\r\n      \"role\": \"system\",\r\n      \"content\": \"ERROR PARSING YOUR RESPONSE:\\n\\nValidationError: 3 validation errors for OneShotAgentActionProposal\\nthoughts -> plan\\n  field required (type=value_error.missing)\\nthoughts -> speak\\n  field required (type=value_error.missing)\\nuse_tool\\n  field required (type=value_error.missing)\"\r\n    },\r\n    {\r\n      \"content\": \"\",\r\n      \"role\": \"assistant\"\r\n    },\r\n    {\r\n      \"role\": \"system\",\r\n      \"content\": \"ERROR PARSING YOUR RESPONSE:\\n\\nInvalidAgentResponseError: Assistant response has no text content\"\r\n    }\r\n  ],\r\n  \"model\": \"gpt-3.5-turbo\"\r\n}\r\n[2024-06-16 17:28:16.570] [ERROR] [Server Error] {\"title\":\"'messages' array must only contain objects with a 'content' field that is not empty\"}\r\n```",
      "> My counter to that is we don't require gpt-4-0611 we just require gpt-4 and match the best we can to a rolling\r\n\r\nThat's not really how it works. We match it based on hard-coded relationships, which we can do because we know OpenAI's model range and their documentation says which models the rolling aliases point to.",
      "I think we shouldn't automatically match the name (or at least not without any warning); this may lead to unexpected behaviour. We can make error message better and include `did you mean \"mistral-7b-instruct-v0.2\"?`, as a hint for user.",
      "I am currently experiencing these issues: https://github.com/Mozilla-Ocho/llamafile/issues/356, https://github.com/Mozilla-Ocho/llamafile/issues/100.\r\n\r\nMay need to amend `llamafile/serve.py` further to fix this for WSL.\r\n\r\n**Update:** this isn't scriptable and not our problem. I'll amend the docs with a note that llamafiles can't be run from WSL, but can still be used by running them on Windows and then connecting to them in WSL.",
      "> this \"works\" but the model isn't great. Can we constrain the output schema to our models like is an option in the llamafile UI?\n\nI think we could implement something like that by allowing to pass a model as the `completion_parser`. Sounds like a follow-up PR though."
    ],
    "num_comments": 16,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 31915,
    "code_diff": "diff --git a/autogpt/.env.template b/autogpt/.env.template\nindex 9d458b90a55d..8d4894988c53 100644\n--- a/autogpt/.env.template\n+++ b/autogpt/.env.template\n@@ -11,6 +11,9 @@\n ## GROQ_API_KEY - Groq API Key (Example: gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx)\n # GROQ_API_KEY=\n \n+## LLAM"
  },
  {
    "pr_title": "Pass Configs to Commands and remove CFG = Config() in the commands/ folder",
    "pr_body": "### Background\r\n<!-- Provide a concise overview of the rationale behind this change. Include relevant context, prior discussions, or links to related \r\nissues. Ensure that the change aligns with the project's overall direction. -->\r\n\r\nConfigs are currently pervasively loaded throughout the app. This can cause circular errors and lots of problems with multi-agent worlds. This is a step to prepare for the refactor and clean up the configs.\r\n\r\n### Changes\r\n\r\nRemoves all  CFG imports in the commands",
    "pr_number": 4328,
    "comments": [
      "I can't get the Google commands to work. Don't have access to twitter. Can't get get_hyperlinks to work. The git operations one requires auth which seems like a bad design decision"
    ],
    "num_comments": 1,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 192329,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 7a4a22bb9c25..4c079e00c21c 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -30,7 +30,6 @@\n ## autogpt.commands.google_search\n ## autogpt.commands.image_gen\n ## autogpt.commands.improve_code\n-## autogpt.commands.twitter\n ## autogpt.commands.web_selen"
  },
  {
    "pr_title": ":sparkles: [Feature] WebUI",
    "pr_body": "### Summary\r\nProvide a web UI where you can : \r\n- Create an AI at route \"/\"\r\n- Use your created Ai at route \"/main/:id\"\r\n- You have access to Ai history so you can switch back to older Ai, thanks to data persistency.\r\n\r\n### Todo\r\n\r\n#### Done : \r\n- [x] Improve display of the Ai content\r\n- [x] Add support for written file download\r\n- [x] Load ai_settings on older Ai click\r\n- [x] Add scroll for Ai list\r\n- [x] Add a middleware to add / remove agent to the right panel\r\n- [x] Debug the slow output bug",
    "pr_number": 322,
    "comments": [
      "Do not hesitate to comment / give me advice as i'm quite a newbie contributing, thanks in advance ! :)",
      "Maybe it could be a Svelte/SvelteKit app instead of React? Svelte is known to be easier to maintain than React although I am biased.\r\nAlso, I see you are writing CSS code. It could be a good idea to consider Tailwind instead of CSS, which allows using inline HTML classes to style the component. This may also allow easier AI-assisted code generation since the component is defined and described in a way that makes the code less scattered among the project files.\r\n\r\nIt'd be very very cool to have web UI to see all the tasks the bot is planning and working on, and be able to intervene on any particular task to tell \"no, don't bother with this\". Because if the bot decides \"I need to make a survey for customer opinion about XYZ\", then it's obviously something you want to cancel (at least for this time)...",
      "@Gintasz i'm also totally biased, but i would argue that React is (according to npm trends https://npmtrends.com/react-vs-svelte) 32 times more used than svelte.\r\nI'm using styled-components because i know it but if enought people want me to use tailwind i can do so !\r\n\r\nYeah my wife is an UX/UI she's working on an interface we will add it on the figma soon !\r\nI'll talk to her about tasks and interuption\r\nCurrently task interruption is not supported (i think there is several PR for this) but i'm eager to see this functionnality <3",
      "![image](https://user-images.githubusercontent.com/16059663/230618772-aa445a01-627c-4a78-9177-c665bbfdd711.png)\r\nCurrent status,\r\ni'll improve the UI :)",
      "Improving UI step by step\r\n![image](https://user-images.githubusercontent.com/16059663/230629259-1b2f1731-f1d9-4d81-8bde-0106d6dd53b4.png)\r\n",
      "Hey I would recommend doing typescript React for this project. Vite is a good option. I think us creating a docker image for it would be ideal as well. If we wanted to create a backend though I would say flask would be simple and consistent with the rest of the python code in the codebase. I agree that adding tailwinds or boostrap would make CSS easier. SASS could also be a good addition \r\n\r\nbut looking through it looks like this is pretty good so far probably best just to continue with where you are at.",
      "I am seeing you are doing the old react router code and probably would be better to use the more up to date way using an outlet, I can make those changes",
      "Yes please do not hesitate to continue / make change.\nI'm already using typescript (all files are ts/tsx)\n\nOh didn't knew for router.\n\nI'm using styled components :)",
      "Also what do you think about the way i communicate between express and python ? First time i created something like that i'm not sure that it is following any best practice",
      "@amauryfischer tbh I am not that experienced myself, but I am learning/working with Typescript React at my job right now but im still new. This is my first time seeing express as a plugin to a frontend which is really neat but no idea about best practices for it. A lot of the typescript config stuff too is still new to me but we can learn this stuff as we make it. I use the Rome linter/formatter, it helps me write better code for best practice would recommend it",
      "My feedback after checking the code\n\n- Before you choose some library or framework I think you should at least document in the PR description why did you prefer to use one technology over another. This is the first time I see someone use express as a SSR mechanism, I think you should explain why did you decide to use it, and what are the other options, because as you can see, a lot of people are asking the same questions.\n\n- I wouldn't use styled components because it can create performance issues, and in terms of readability just makes the codebase worse, there's no separation of concerns, it's hard to read, and the styles can get too attached to the component, this will probably create duplicated code if not used properly. \n\n- You should do a better use of typescript. You barely use typescript on the components, and when you use it, you overuse \"any's\". Typescript is a very powerful tool, just use any's if you are using an obscure library and you have no idea what a specific object contains.\n\n-  You have lots of magic numbers in the styles, this can bring some problems in the layout, try not to add random heights, paddings and margins. I usually use the 8px rule, and gaps.\n\n- Better organizes the code by creating services, the principle of separation of concerns is very important. Here's an example, you are creating functions to do API calls directly from the components, which makes the code disorganized and confusing. In case you need to make the same call in another component, you will have duplicated code.\n\n- Pay attention to the name of the functions and variables.\n\n- Use a at least a testing library such as jest or vitest.\n\n\n",
      "@rbnds thanks for your feedback !\n\nConcerning the choice of express, doesn't have a lot of idea of how to do things differently.\nWe need to run the python code, and transfer data back to the App, but if somebody have other idea to do that in a better way no problem !\n\nThis is a draft PR so there is plenty of remaining work, i'll create related service yes, but also reducer probably, currently there is just one rought component.\n\nI'll use typescript more yes thanks for your notice.\n\nConcerning magic numbers i'll add some design token with a spacing.tsx in style.\n\nI'm quite opinionated on styles components i find it very powerfull and it tackle the problem of tailwind visibility. The readability is way better. But again that's a personal opinion. Any other people wanna share their thoughts ? \n\nWhich testing library would you prefer to use ?",
      "Improved code structure, i'm now using dedicated service and api files, also created more segmented components",
      "Merged master branch and fix conflict",
      "@Commando-Brando I cannot answer you on discord i don't have the right anymore to talk on the \"progress-sharing\" but i'm eager to see your pr merged and to integrate this ! :)",
      "Current visual :\r\n![image](https://user-images.githubusercontent.com/16059663/230727118-0879cc08-703a-4d9a-86df-800da7510e08.png)\r\n",
      "Just an idea/question as you're using SSR through express, why not use Next.js? You could port it over, take away your router and manage the API route there as well. Just a thought. Cool addition either way!",
      "is this flask and if not, why?\r\nAnd will there be an option to either be online or offline?\r\nand when online the ability to share prompts with the community, see shared prompts?\r\nI think they should be defaulted to private but easily converted with a switch.\r\nIf this is in flask I may be able to help ü§ò",
      "@willscottrod I'm not a pro on SSR but i think SSR is quite good when you have more a static website, and this one will have a lot of interaction :/\r\n",
      "@pmb2 React is a very popular and powerfull to build UI application, also express can run python file without any problem, so we don't need flask.\r\nFor offline, it's not related to this PR, we could do offline support with Progressive Web App, but for now the app use Chat-GPT which require an internet connexion.\r\nMaybe with a LLAMA model.\r\nBut this PR is more focused on UI",
      "@amauryfischer apologies, didn't mean SSR. You probably don't need that feature, it's more for the api routing inside next js itself. You can read more about it here: https://nextjs.org/docs/api-routes/introduction",
      "@amauryfischer in regard to rbnds's comment above, where he talks about using `any` a lot as bad practice. I do not know what editor you're using but would recommend using Rome to lint/format your code. You can use it as a package, but it also has a nice VScode extension if you like that editor. Rome linter would flag any `any` code that most likely shouldn't be. A lot of the time when people use `any`, `unkown` is more appropriate. I still have not got around to diving into the code yet but I plan to start reading through tonight and then tomorrow more",
      "@willscottrod i'll definitely look at this ! Thanks for the tips",
      "@Commando-Brando yes i just discovered Rome thanks to him, i installed it yesterday :)\nI started to more strongly type my components and data in recent PR.\n\nI have some trouble typing return of the spawn function.",
      "@amauryfischer I was asking chatgpt3.5 some general question just to see if there are any other ways outside of my domain knowledge to best approach this task, I still have not had a chance to dive into the code much yet but from this question I asked it, it suggests `websocket` which may be better in this case over just `HTTP` to use since everything is local and it would facilitate faster communication, of course if you are not already using something like it. \r\n\r\n![image](https://user-images.githubusercontent.com/60548867/230771657-ee32a717-baf8-4a44-b707-2acaf7ba6d59.png)\r\n",
      "I think it's better to avoid websocket and keep a rest API, this way, as asked in the discord ui/ux channel people can use the API only and create their own interface.\nAny specialized opinion on this would be appreciated^^",
      "Updated UI\r\nCurrent first view : \r\n![image](https://user-images.githubusercontent.com/16059663/230802121-ea584015-661a-4244-b3a4-98d60b7a0463.png)\r\n![image](https://user-images.githubusercontent.com/16059663/230802128-89a76975-b9f4-4fd9-b506-f379d8d18de2.png)\r\n",
      "Did you see https://agentgpt.reworkd.ai/ ?\r\n\r\n#174",
      "@alreadydone yeah ! Pretty cool üòÑ",
      "AI initialization is working, i'll update the task list"
    ],
    "num_comments": 30,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 231717,
    "code_diff": "diff --git a/.gitignore b/.gitignore\nindex 26d7e5a3f7b3..b24a91cf0a16 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -157,3 +157,4 @@ vicuna-*\n \n # mac\n .DS_Store\n+/logs\n\\ No newline at end of file\ndiff --git a/README.md b/README.md\nindex 4969e5edd45f..de69ead978df 100644\n--- a/README.md\n+++ b/README.m"
  },
  {
    "pr_title": "new UI for ai_settings yaml to support multiple configurations",
    "pr_body": "### Summary\r\nChanged the prompt_user flow so the user is now able to configure multiple agents and have their settings stored in ai_settings.yaml. Which now stores as dictionary to allow selecting from multiple key's (configs), save, change or delete them.\r\n\r\nWhen configurations are found in ai_settings.yaml, the prompt starts-up with the following menu:\r\n\r\n![menu](https://github.com/Significant-Gravitas/Auto-GPT/assets/68592736/803225c8-ebd3-4c89-a42c-c88b180e4285)\r\n\r\n### Change\r\n- Support stor",
    "pr_number": 4478,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 81148,
    "code_diff": "diff --git a/autogpt/config/ai_config.py b/autogpt/config/ai_config.py\nindex 1a5268323498..1ba52fe873b1 100644\n--- a/autogpt/config/ai_config.py\n+++ b/autogpt/config/ai_config.py\n@@ -7,7 +7,7 @@\n import os\n import platform\n from pathlib import Path\n-from typing import TYPE_CHECKING, Optional\n+from t"
  },
  {
    "pr_title": "Restructuring The Docs",
    "pr_body": "Updating and Restructuring the docs.\r\npeople working on this: @Bentlybro @Yash-Ambekar @sohamtembhurne @BarneyChambers @ConTronTech\r\n\r\n - [x] Move all specific documentation about Auto-GPT (the agent) into a subfolder\r\n \t- [x] Update setup & usage instructions to match the new repo structure\r\n - [x] Create subfolders with main pages for benchmark, forge, and front end\r\n - [x] Create new main page\r\n - [x] Update Installation page to make it cleaner\r\n",
    "pr_number": 5441,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 32424,
    "code_diff": "diff --git a/docs/content/setup.md b/docs/content/AutoGPT/Setups/Docker-setup.md\nsimilarity index 60%\nrename from docs/content/setup.md\nrename to docs/content/AutoGPT/Setups/Docker-setup.md\nindex 19fb7b9b50ac..af1c49d1f61a 100644\n--- a/docs/content/setup.md\n+++ b/docs/content/AutoGPT/Setups/Docker-s"
  },
  {
    "pr_title": "Makes it possible to use gpt-3.5",
    "pr_body": "(This is such a neat experiment. Thanks for open-sourcing!)\r\n\r\nFixes #12\r\nFixes #40 \r\n\r\nMakes davinci-3.5-turbo the default model (faster and cheaper)\r\nAllows for model configuration\r\n\r\nI made JSON parsing a lot more forgiving, including using GPT to fix up the JSON if necessary.\r\nI also improved the prompt to make it follow instructions a bit better.\r\nI also refactored a bit. \r\nI also made it save my inputs to a yml file by default when it runs. (This could be a LOT better - very rough, but wan",
    "pr_number": 45,
    "comments": [
      "Tried this, however\r\n```rossfisher@Rosss-MBP scripts % python3 main.py                        \r\nplaysound is relying on a python 2 subprocess. Please use `pip3 install PyObjC` if you want playsound to run more efficiently.\r\nTraceback (most recent call last):\r\n  File \"/Users/rossfisher/Auto-GPT/scripts/main.py\", line 3, in <module>\r\n    import commands as cmd\r\n  File \"/Users/rossfisher/Auto-GPT/scripts/commands.py\", line 8, in <module>\r\n    import ai_functions as ai\r\n  File \"/Users/rossfisher/Auto-GPT/scripts/ai_functions.py\", line 11, in <module>\r\n    def call_ai_function(function, args, description, model=cfg.smart_llm_model):\r\n                                                            ^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'Config' object has no attribute 'smart_llm_model'```",
      "Woah - lemme see what I missed...",
      "Apparently I was moving _way_ too fast. I missed some pretty basic stuff from my commit, and am in the process of fixing it now. (Not at my desk though, so taking me a bit)",
      "Okay - pushed up fixes! Lemme know how that works for you.",
      "Seems to work! Need to add my key\r\n\r\nMay want to upgrade requirements\r\n\r\n```Traceback (most recent call last):\r\n  File \"/Users/rossfisher/Auto-GPT/scripts/main.py\", line 15, in <module>\r\n    from ai_config import AIConfig\r\n  File \"/Users/rossfisher/Auto-GPT/scripts/ai_config.py\", line 1, in <module>\r\n    import yaml\r\nModuleNotFoundError: No module named 'yaml'\r\nrossfisher@Rosss-MBP scripts % pip3 install yaml\r\nERROR: Could not find a version that satisfies the requirement yaml (from versions: none)\r\nERROR: No matching distribution found for yaml\r\nrossfisher@Rosss-MBP scripts % pip3 install pyyaml\r\nCollecting pyyaml\r\n  Using cached PyYAML-6.0-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\r\nInstalling collected packages: pyyaml\r\nSuccessfully installed pyyaml-6.0```",
      "Great work, taking a look now.",
      "@zorrobyte : Hmmm - I added pyyaml to requirements.txt:\r\n`pip install -r ./requirements.txt` should have picked it up...",
      "This is amazing!\r\nSo much faster and cheaper, excellent work!",
      "I'm fixing merge conflicts now",
      "Okay - pushed some fixes. It looks like the bot is inserting its thoughts before the JSON response. I have some ideas to fix, but in the meantime, the JSON parser is doing its job.\r\n\r\nI think we need to give it a better example of the response we want. In the meantime, we are seeing stuff like this:\r\n```\r\nassistant reply: As the first step, I need to clone the repository locally so that I can analyze and test the existing code. I will go with the following command:\r\n\r\n{\r\n    \"command\": {\r\n        \"name\": \"execute_python_file\",\r\n<snip>...\r\n```\r\n",
      "@anslex : Yeah, if you aren't in the scripts folder when you run it, it fails. We'll get that fixed up soon I'm sure.\r\n\r\nIn the meantime:\r\n```sh\r\ncd ./scripts\r\npython ./main.py speak-mode\r\n```\r\n",
      "@Taytay Eager to merge this, just checking you didn't miss my review comments above! üòä",
      "Which ones actually? I don't see them",
      "Guys, please make some kind of quick switch between models between 3.5 and 4, so that there is one repository, but for example some _*model_config*_ file where we select once and everything else is already contained in the general repository.",
      "It should be configured in the `.env` file",
      "Ah sorry @Taytay, new to this, they weren't \"sent\".",
      "This worked for me.\r\n\r\nThe load_prompt function reads the prompt.txt file from the data subdirectory. If the script is not able to access the file, the most likely reason is that the working directory of the script execution is not set correctly.\r\n\r\nTo resolve this issue, you can modify the load_prompt function to use an absolute path instead of a relative path. You can achieve this by constructing the absolute path using the os module:\r\n\r\n```\r\nimport os\r\n\r\ndef load_prompt():\r\n    try:\r\n        # Get the current script's directory\r\n        script_dir = os.path.dirname(os.path.realpath(__file__))\r\n        \r\n        # Construct the absolute path to the prompt.txt file\r\n        prompt_file_path = os.path.join(script_dir, \"data\", \"prompt.txt\")\r\n\r\n        # Load the prompt from data/prompt.txt\r\n        with open(prompt_file_path, \"r\") as prompt_file:\r\n            prompt = prompt_file.read()\r\n\r\n        return prompt\r\n    except FileNotFoundError:\r\n        print(\"Error: Prompt file not found\", flush=True)\r\n        return \"\"\r\n```\r\n\r\nThis modification should fix the issue with accessing the prompt.txt file. The os.path.dirname(os.path.realpath(__file__)) line retrieves the directory of the data.py script, and the os.path.join(script_dir, \"data\", \"prompt.txt\") line constructs the absolute path to the prompt.txt file.",
      "Testing now üëÄ\r\nLet's get this shipped! üö¢\r\n\r\n**How to test this on your machine:**\r\n`git checkout -b Taytay-fixes_gpt3 master`\r\n`git pull https://github.com/Taytay/Auto-GPT.git fixes_gpt3`",
      "Please remove all that debug logging (or make it optional) before we merge this @Taytay üòä",
      "@Torantulino - lemme know how that works for you. I think it's in better shape now.",
      "I made this use gpt-4 again for the main prompt again. (Not sure if that's what you wanted or not.) I think we can make it use gpt-3 with some better prompting.",
      "heads up, i've been testing this branch and often run into: `openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4304 tokens. Please reduce the length of the messages.`\r\n\r\nor JSON decode errors along the way",
      "@yousefissa : You using gpt-4 as your \"smart\" llm, or gpt-3?",
      "Any chance you could show me some of the JSON it's not parsing?",
      "i don't have gpt-4 api access yet so using 3.5 for both llm options.\r\n\r\n\r\nhere is a larger rip of the log output:\r\n\r\n```\r\nSystem: Command browse_website returned: Website Content Summary:  \"Result\" : The subpage promotes Shopify's features and solutions for e-commerce businesses, including website creation, app integration, selling and marketing tools, and enterprise-level retail solutions. It also offers a free 3-day trial and a point of sale system for physical stores. The focus is on the built-in marketing tools and centralized business operations available with a Shopify POS, and the subpage includes testimonials and resources for support and strategy tips.\r\n\r\nLinks: ['Skip to Content (#main)', 'Sign up now (https://accounts.shopify.com/store-create?signup_currency=USD&locale=en&language=en&signup_page=https%3A%2F%2Fwww.shopify.com%2F&signup_types[]=paid_trial_experience)', 'Shopify (https://www.shopify.com)', 'Start your businessBuild your brand (/start)', 'Create your websiteOnline store editor (/website/builder)']\r\nUser: Determine which next command to use, and respond using the format specified above:\r\nAssistant: {\r\n    \"command\": {\r\n        \"name\": \"memory_add\",\r\n        \"args\":{\r\n            \"string\": \"Researched in-depth information on Shopify on their website.\"\r\n        }\r\n    },\r\n    \"thoughts\":\r\n    {\r\n        \"text\": \"Adding Shopify research task to permanent memory.\",\r\n        \"reasoning\": \"Since Shopify is the platform we are planning to use for our business needs, it's necessary to conduct further research to fully assess its capabilities to ensure our store remains profitable in the near future.\",\r\n        \"plan\": \"- Store info on Shopify on memory\",\r\n        \"criticism\": \"None\",\r\n        \"speak\": \"I have conducted thorough research on Shopify and have added relevant info to be used in decision-making to permanent memory.\"\r\n    }\r\n}\r\nSystem: Command memory_add returned: Committing memory with string \"Researched in-depth information on Shopify on their website.\"\r\nUser: Determine which next command to use, and respond using the format specified above:\r\nAssistant: {\r\n    \"command\": {\r\n        \"name\": \"message_agent\",\r\n        \"args\":{\r\n            \"key\": \"-1\",\r\n            \"message\": \"Is it possible to add an email newsletter sign-up form to my Shopify store?\"\r\n        }\r\n    },\r\n    \"thoughts\":\r\n    {\r\n        \"text\": \"Message the GPT-3.5-powered Shopify Help Agent for information on integrating an email newsletter sign-up form.\",\r\n        \"reasoning\": \"Asking the Shopify help agent for relevant information on the email newsletter sign-up form can provide easy-to-understand guidance on the subject. This way, we can use their suggested implementation method to add an email newsletter sign-up form to our store.\",\r\n        \"plan\": \"- Message the Shopify help agent on Shopify integration methods for email newsletters.\",\r\n        \"criticism\": \"None\",\r\n        \"speak\": \"I have messaged the Shopify Help Agent for information on integrating an email newsletter sign-up form.\"\r\n    }\r\n}\r\nSystem: Command message_agent returned: Error: -1\r\nUser: Determine which next command to use, and respond using the format specified above:\r\nAssistant: {\r\n    \"command\": {\r\n        \"name\": \"list_agents\",\r\n        \"args\":{\r\n        }\r\n    },\r\n    \"thoughts\":\r\n    {\r\n        \"text\": \"Listing all available agent.\",\r\n        \"reasoning\": \"It seems I have sent a message to a non-existing agent. To avoid future errors, I will list all available agents and message the correct one.\",\r\n        \"plan\": \"- List available agents.\",\r\n        \"criticism\": \"None\",\r\n        \"speak\": \"I will list all available agents to message the correct one.\"\r\n    }\r\n}\r\nSystem: Command list_agents returned: []\r\nUser: Determine which next command to use, and respond using the format specified above:\r\n----------- END OF CONTEXT ----------------\r\nTraceback (most recent call last):\r\n  File \"/Users/me/Desktop/ai-projects/Auto-GPT/scripts/main.py\", line 284, in <module>\r\n    assistant_reply = chat.chat_with_ai(\r\n  File \"/Users/me/Desktop/ai-projects/Auto-GPT/scripts/chat.py\", line 65, in chat_with_ai\r\n    assistant_reply = create_chat_completion(\r\n  File \"/Users/me/Desktop/ai-projects/Auto-GPT/scripts/llm_utils.py\", line 9, in create_chat_completion\r\n    response = openai.ChatCompletion.create(\r\n  File \"/Users/me/Desktop/ai-projects/Auto-GPT/venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\r\n    return super().create(*args, **kwargs)\r\n  File \"/Users/me/Desktop/ai-projects/Auto-GPT/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\r\n    response, _, api_key = requestor.request(\r\n  File \"/Users/me/Desktop/ai-projects/Auto-GPT/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 226, in request\r\n    resp, got_stream = self._interpret_response(result, stream)\r\n  File \"/Users/me/Desktop/ai-projects/Auto-GPT/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 619, in _interpret_response\r\n    self._interpret_response_line(\r\n  File \"/Users/me/Desktop/ai-projects/Auto-GPT/venv/lib/python3.10/site-packages/openai/api_requestor.py\", line 682, in _interpret_response_line\r\n    raise self.handle_error_response(\r\nopenai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4117 tokens. Please reduce the length of the messages.\r\n(venv) ‚ûú  scripts git:(fixes352) ‚úó\r\n```\r\n\r\n\r\nill try to log JSON output next time",
      "In that case, try setting the context to be much smaller in config.py. Set it to 2000 instead and see how that does.\r\nIt's a bit step down, but gpt-4 has an 8k windows, and gpt-3, a 4k window, so 2000 is probably a better default for gpt-3",
      "@Taytay, why are we now seeing `AttributedDict([(` being passed as commands?\r\n\r\n![image](https://user-images.githubusercontent.com/22963551/229407163-7f9b134c-0854-490b-9725-9cf45e314000.png)\r\n",
      "@Taytay, perhaps these can be env variables too with defaults? Assuming I'm following what the issue is. I may not be. 4,000 seems high for gpt-3 though. \r\n\r\nhttps://github.com/Torantulino/Auto-GPT/pull/45/files#diff-e9ffa9c36082d5a0b2ae4098b1b77d020a8d57d159bc89255fcad17287bc3ed9R36-R39\r\n\r\nContext windows are frustrating. I wish there was a more eloquent way to optimize limits. "
    ],
    "num_comments": 28,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 48765,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex a598aa7b1a55..cbf0cd9bc9b4 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -1,2 +1,4 @@\n OPENAI_API_KEY=your-openai-api-key\n ELEVENLABS_API_KEY=your-elevenlabs-api-key\n+SMART_LLM_MODEL=\"gpt-4\"\n+FAST_LLM_MODEL=\"gpt-3.5-turbo\"\n\\ No newline at end of f"
  },
  {
    "pr_title": "Enable Google/OpenAI access for china mainland users",
    "pr_body": "<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to maintain a clean and manageable git history. To ensure the quality of our repository, we kindly ask you to adhere to the following guidelines when submitting PRs:\r\n\r\nFocus on a single, specific change.\r\nDo not include any unrelated or \"extra\" modifications.\r\nProvide clear documentation and explanations of the cha",
    "pr_number": 1541,
    "comments": [
      "This PR does not affect users out of China mainland (because default switches are off). But they are valuable for China mainland users so that they can easily configure Auto-GPT to run!",
      "chatGPTÂèØ‰ª•Ê≠£Â∏∏‰ΩøÁî®ÔºåÁΩëÁªúËøûÊé•Â∫îËØ•Ê≤°ÊúâÈóÆÈ¢òÔºå‰ΩÜÊòØauto-GPTË∑ë‰∏çËµ∑Êù•ÔºåÊòØËøôÊ†∑ÁöÑÈóÆÈ¢òÂêóÔºü\r\nraise error.APIConnectionError(\r\nopenai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1131)')))\r\n",
      "I got same error.whatever I do, it always jump out this error.\n\nWang DaChui ***@***.***> ‰∫é2023Âπ¥4Êúà15Êó•Âë®ÂÖ≠ 20:37ÂÜôÈÅìÔºö\n\n> chatGPTÂèØ‰ª•Ê≠£Â∏∏‰ΩøÁî®ÔºåÁΩëÁªúËøûÊé•Â∫îËØ•Ê≤°ÊúâÈóÆÈ¢òÔºå‰ΩÜÊòØauto-GPTË∑ë‰∏çËµ∑Êù•ÔºåÊòØËøôÊ†∑ÁöÑÈóÆÈ¢òÂêóÔºü\n> raise error.APIConnectionError(\n> openai.error.APIConnectionError: Error communicating with OpenAI:\n> HTTPSConnectionPool(host='api.openai.com', port=443): Max retries\n> exceeded with url: /v1/chat/completions (Caused by SSLError(SSLEOFError(8,\n> 'EOF occurred in violation of protocol (_ssl.c:1131)')))\n>\n> ‚Äî\n> Reply to this email directly, view it on GitHub\n> <https://github.com/Significant-Gravitas/Auto-GPT/pull/1541#issuecomment-1509766229>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AB3MFYMCNG3OZPIQQLWPZELXBKI7BANCNFSM6AAAAAAW7KZA6Y>\n> .\n> You are receiving this because you are subscribed to this thread.Message\n> ID: ***@***.***>\n>\n",
      "> \r\n\r\n\r\n\r\n> ÊàëÊúâÂêåÊ†∑ÁöÑÈîôËØØ„ÄÇÊó†ËÆ∫ÊàëÂÅö‰ªÄ‰πàÔºåÂÆÉÊÄªÊòØË∑≥Âá∫Ëøô‰∏™ÈîôËØØ„ÄÇÁéãÂ§ßÈî§***@***.***>‰∫é2023Âπ¥4Êúà15Êó•Âë®ÂÖ≠20:37ÂÜôÈÅìÔºö\r\n> [‚Ä¶‚Ä¶](#)\r\n> chatGPTÂèØ‰ª•Ê≠£Â∏∏‰ΩøÁî®ÔºåÁΩëÁªúËøûÊé•Â∫îËØ•Ê≤°ÊúâÈóÆÈ¢òÔºå‰ΩÜÊòØauto-GPTË∑ë‰∏çËµ∑Êù•ÔºåÊòØËøôÊ†∑ÁöÑÈóÆÈ¢òÂêóÔºü com', port=443): ÊúÄÂ§ßÈáçËØïÊ¨°Êï∞Ë∂ÖËøá url: /v1/chat/completions (Áî± SSLError(SSLOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1131)'))) ‚Äî ÂõûÂ§çÁõ¥Êé•Âú® GitHub < [#1541ÔºàËØÑËÆ∫Ôºâ](https://github.com/Significant-Gravitas/Auto-GPT/pull/1541#issuecomment-1509766229) >‰∏äÊü•ÁúãÊ≠§ÁîµÂ≠êÈÇÆ‰ª∂ÔºåÊàñÂèñÊ∂àËÆ¢ÈòÖ < https://github.com/notifications/unsubscribe-auth/AB3MFYMCNG3OZPIQQLWPZELXBKI7BANCNFSM6AAAAAAW7KZA6Y >„ÄÇÊÇ®Êî∂Âà∞Ê≠§ÈÇÆ‰ª∂ÊòØÂõ†‰∏∫ÊÇ®ËÆ¢ÈòÖ‰∫ÜÊ≠§Á∫øÁ®ã„ÄÇÊ∂àÊÅØ ID : ***@***.***>\r\nÊúÄÂêéËß£ÂÜ≥‰∫ÜÂêóÔºüËÄÅÂì•",
      "@youkaichao There are conflicts now",
      "@loseDemon @olymax Did you set the `https_proxy` environment variable?\r\n\r\nFor example, my command is:\r\n```bash\r\nhttps_proxy=http://127.0.0.1:9090 python -m autogpt --gpt3only\r\n```\r\n\r\nRemember to replace the specific https_proxy with your own proxy url.",
      "@youkaichao There are conflicts now",
      "@nponeccop I'm working on the conflict. It should be done in several minutes.",
      "@nponeccop The conflicts should be resolved now.",
      "I just tested this PR combined with the fix of type annotation: https://github.com/Significant-Gravitas/Auto-GPT/pull/1580 . They should work now.",
      "ÊåâÁÖßÊñá‰ª∂Èáå‰∏ãËΩΩ‰∫ÜË∞∑Ê≠åÈ©±Âä®ÔºåÂú®ÂØπÂ∫îËÑöÊú¨ÈáåÊ∑ªÂä†‰∫ÜÁõ∏Â∫îÁöÑÂëΩ‰ª§„ÄÇÈºìÊç£ÂçäÂ§©ÂÆûÂú®ËøûÊé•‰∏ç‰∏äÔºåÂèØËÉΩÊòØÊàëÂ§™Â∞èÁôΩ‰∫Ü„ÄÇ„ÄÇ\r\n\r\n```\r\n(venv) PS G:\\Project_Python\\Auto-GPT-master> python -m autogpt --gpt3only\r\npymilvus not installed. Skipping import.\r\nGPT3.5 Only Mode:  ENABLED\r\nWelcome to Auto-GPT!  Enter the name of your AI and its role below. Entering nothing will load defaults.\r\nName your AI:  For example, 'Entrepreneur-GPT'\r\nAI Name: AI1\r\nAI1 here!  I am at your service.\r\nDescribe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'\r\nAI1 is: ÂÜô‰∏Ä‰∏™ÁΩëÈ°µ\r\nEnter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'\r\nEnter nothing to load defaults, enter nothing when finished.\r\nGoal 1: ÂåÖÊã¨ÁôªÂΩïÁïåÈù¢\r\nGoal 2: \r\nUsing memory of type:  LocalCache\r\nUsing Browser:  chrome\r\nTraceback (most recent call last):\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\r\n    conn = connection.create_connection(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\util\\connection.py\", line 95, in create_connection\r\n    raise err\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\r\n    sock.connect(sa)\r\nConnectionRefusedError: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\r\n    self._validate_conn(conn)\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\r\n    conn.connect()\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\r\n    self.sock = conn = self._new_conn()\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\r\n    raise NewConnectionError(\r\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x0000012E0E5CC820>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\requests\\adapters.py\", line 489, in send\r\n    resp = conn.urlopen(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 815, in urlopen\r\n    return self.urlopen(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 815, in urlopen\r\n    return self.urlopen(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\r\n    retries = retries.increment(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\r\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000012E0E5CC\r\n820>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_requestor.py\", line 516, in request_raw\r\n    result = _thread_context.session.request(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\requests\\adapters.py\", line 565, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000012E0E\r\n5CC820>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ'))\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Python\\Python38\\lib\\runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"D:\\Python\\Python38\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\__main__.py\", line 51, in <module>\r\n    main()\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\__main__.py\", line 47, in main\r\n    agent.start_interaction_loop()\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\agent\\agent.py\", line 65, in start_interaction_loop\r\n    assistant_reply = chat_with_ai(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\chat.py\", line 159, in chat_with_ai\r\n    assistant_reply = create_chat_completion(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\llm_utils.py\", line 90, in create_chat_completion\r\n    response = openai.ChatCompletion.create(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\r\n    return super().create(*args, **kwargs)\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\r\n    response, _, api_key = requestor.request(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_requestor.py\", line 216, in request\r\n    result = self.request_raw(\r\n  File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_requestor.py\", line 528, in request_raw\r\n    raise error.APIConnectionError(\r\nopenai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConn\r\nection object at 0x0000012E0E5CC820>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ'))\r\n\r\n```",
      "@nponeccop  I resolved conflicts via commit 6b6d0ad .",
      "@zxcvcxz12345 ÁúãËµ∑Êù•ËøòÊòØ‰Ω†ÁöÑ‰ª£ÁêÜÊ≤°ÊúâÈÖçÁΩÆÂ•Ω„ÄÇ‰Ω†ÈúÄË¶ÅÊ£ÄÊü•WindowsÂëΩ‰ª§Ë°å‰ΩøÁî®‰ª£ÁêÜÁöÑÊñπÂºèÔºåÊàñËÄÖÊç¢Áî®Linux/Windows Linux Subsystem (WSL).",
      "> ÊåâÁÖßÊñá‰ª∂Èáå‰∏ãËΩΩ‰∫ÜË∞∑Ê≠åÈ©±Âä®ÔºåÂú®ÂØπÂ∫îËÑöÊú¨ÈáåÊ∑ªÂä†‰∫ÜÁõ∏Â∫îÁöÑÂëΩ‰ª§„ÄÇÈºìÊç£ÂçäÂ§©ÂÆûÂú®ËøûÊé•‰∏ç‰∏äÔºåÂèØËÉΩÊòØÊàëÂ§™Â∞èÁôΩ‰∫Ü„ÄÇ„ÄÇ\r\n> \r\n> ```\r\n> (venv) PS G:\\Project_Python\\Auto-GPT-master> python -m autogpt --gpt3only\r\n> pymilvus not installed. Skipping import.\r\n> GPT3.5 Only Mode:  ENABLED\r\n> Welcome to Auto-GPT!  Enter the name of your AI and its role below. Entering nothing will load defaults.\r\n> Name your AI:  For example, 'Entrepreneur-GPT'\r\n> AI Name: AI1\r\n> AI1 here!  I am at your service.\r\n> Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'\r\n> AI1 is: ÂÜô‰∏Ä‰∏™ÁΩëÈ°µ\r\n> Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'\r\n> Enter nothing to load defaults, enter nothing when finished.\r\n> Goal 1: ÂåÖÊã¨ÁôªÂΩïÁïåÈù¢\r\n> Goal 2: \r\n> Using memory of type:  LocalCache\r\n> Using Browser:  chrome\r\n> Traceback (most recent call last):\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\r\n>     conn = connection.create_connection(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\util\\connection.py\", line 95, in create_connection\r\n>     raise err\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\r\n>     sock.connect(sa)\r\n> ConnectionRefusedError: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\r\n>     httplib_response = self._make_request(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\r\n>     self._validate_conn(conn)\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\r\n>     conn.connect()\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 363, in connect\r\n>     self.sock = conn = self._new_conn()\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connection.py\", line 186, in _new_conn\r\n>     raise NewConnectionError(\r\n> urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x0000012E0E5CC820>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\requests\\adapters.py\", line 489, in send\r\n>     resp = conn.urlopen(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 815, in urlopen\r\n>     return self.urlopen(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 815, in urlopen\r\n>     return self.urlopen(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\r\n>     retries = retries.increment(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\r\n>     raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\n> urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000012E0E5CC\r\n> 820>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ'))\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_requestor.py\", line 516, in request_raw\r\n>     result = _thread_context.session.request(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\r\n>     resp = self.send(prep, **send_kwargs)\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\r\n>     r = adapter.send(request, **kwargs)\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\requests\\adapters.py\", line 565, in send\r\n>     raise ConnectionError(e, request=request)\r\n> requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000012E0E\r\n> 5CC820>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ'))\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"D:\\Python\\Python38\\lib\\runpy.py\", line 194, in _run_module_as_main\r\n>     return _run_code(code, main_globals, None,\r\n>   File \"D:\\Python\\Python38\\lib\\runpy.py\", line 87, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\__main__.py\", line 51, in <module>\r\n>     main()\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\__main__.py\", line 47, in main\r\n>     agent.start_interaction_loop()\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\agent\\agent.py\", line 65, in start_interaction_loop\r\n>     assistant_reply = chat_with_ai(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\chat.py\", line 159, in chat_with_ai\r\n>     assistant_reply = create_chat_completion(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\autogpt\\llm_utils.py\", line 90, in create_chat_completion\r\n>     response = openai.ChatCompletion.create(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\r\n>     return super().create(*args, **kwargs)\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\r\n>     response, _, api_key = requestor.request(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_requestor.py\", line 216, in request\r\n>     result = self.request_raw(\r\n>   File \"G:\\Project_Python\\Auto-GPT-master\\venv\\lib\\site-packages\\openai\\api_requestor.py\", line 528, in request_raw\r\n>     raise error.APIConnectionError(\r\n> openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPSConn\r\n> ection object at 0x0000012E0E5CC820>: Failed to establish a new connection: [WinError 10061] Áî±‰∫éÁõÆÊ†áËÆ°ÁÆóÊú∫ÁßØÊûÅÊãíÁªùÔºåÊó†Ê≥ïËøûÊé•„ÄÇ'))\r\n> ```\r\n\r\n‰Ω†ËøôÈáåÊòØ openai ÈÉΩÊ≤°Ëøû‰∏ä„ÄÇ\r\nÊ£ÄÊü•‰∏Ä‰∏ã token ÊúâÊ≤°ÊúâËÆæÁΩÆÊ≠£Á°ÆÔºå‰ª•Âèä‰ª£ÁêÜÊúâÊ≤°ÊúâËÆæÁΩÆÂ•Ω„ÄÇ",
      "I am new to study how to use github issues.\r\nYesterday I didn't apply my google api key and PINECONE_API_KEY.Today I apply them, and then change the .env file, But every time I use google command, it returns\" Command google returned: Error: [WinError 10060] Áî±‰∫éËøûÊé•ÊñπÂú®‰∏ÄÊÆµÊó∂Èó¥ÂêéÊ≤°ÊúâÊ≠£Á°ÆÁ≠îÂ§çÊàñËøûÊé•\r\nÁöÑ‰∏ªÊú∫Ê≤°ÊúâÂèçÂ∫îÔºåËøûÊé•Â∞ùËØïÂ§±Ë¥•„ÄÇ\" Is that means I have to check ÊàëËá™Â∑±ÁöÑ‰ª£ÁêÜURL every time before use it?ÔºàI'm sorry that I am lacking in web knowledge so I have to try to express my meaning correctly.Ôºâ\r\n\r\n> ÊòØÂê¶ËÆæÁΩÆ‰∫ÜÁéØÂ¢ÉÂèòÈáèÔºü`https_proxy`\r\n> \r\n> ‰æãÂ¶ÇÔºåÊàëÁöÑÂëΩ‰ª§ÊòØÔºö\r\n> \r\n> ```shell\r\n> https_proxy=http://127.0.0.1:9090 python -m autogpt --gpt3only\r\n> ```\r\n> \r\n> ËØ∑ËÆ∞‰ΩèÂ∞ÜÁâπÂÆöhttps_proxyÊõøÊç¢‰∏∫ÊÇ®Ëá™Â∑±ÁöÑ‰ª£ÁêÜ URL„ÄÇ\r\n\r\n",
      "and I try to use it,I found that--\r\n![image](https://user-images.githubusercontent.com/124547641/232291558-51b7fefe-f2b4-44cd-9d58-059ae1ec61cf.png)\r\n![image](https://user-images.githubusercontent.com/124547641/232291597-ea1bca91-182c-40be-9ff0-781547b2ad0d.png)\r\n",
      "@kazegairoman My example is for Linux/MacOS. For Windows, you have to know how to set proxy for commandline, for example, https://stackoverflow.com/a/57613619 . Sorry that I'm not familiar with Windows. I recommend using Linux or just WSL.",
      "> \r\n\r\nthank you so much! I had already solved this question!",
      "ÊåâË¶ÅÊ±Ç‰∏ãËΩΩÂπ∂ÈÖçÁΩÆ‰∫ÜÔºåËøòÊòØÊèêÁ§∫„ÄÇSYSTEM:  Command google returned: Error: [WinError 10060] Áî±‰∫éËøûÊé•ÊñπÂú®‰∏ÄÊÆµÊó∂Èó¥ÂêéÊ≤°ÊúâÊ≠£Á°ÆÁ≠îÂ§çÊàñËøûÊé•ÁöÑ‰∏ªÊú∫Ê≤°ÊúâÂèçÂ∫îÔºåËøûÊé•Â∞ùËØïÂ§±Ë¥•„ÄÇ\r\n![image](https://user-images.githubusercontent.com/67988538/232366548-62d4beab-78a7-47fc-8ba2-0251c37ee515.png)\r\n",
      "@kuok246 ÊàëÁöÑÁ§∫‰æãÈÄÇÁî®‰∫é Linux/MacOS„ÄÇ ÂØπ‰∫é WindowsÔºåÊÇ®ÂøÖÈ°ªÁü•ÈÅìÂ¶Ç‰Ωï‰∏∫ÂëΩ‰ª§Ë°åËÆæÁΩÆ‰ª£ÁêÜÔºå‰æãÂ¶ÇÔºåstackoverflow.com/a/57613619„ÄÇ ÂØπ‰∏çËµ∑ÔºåÊàë‰∏çÁÜüÊÇâWindows„ÄÇ ÊàëÂª∫ËÆÆ‰ΩøÁî® Linux Êàñ‰ΩøÁî® WSL„ÄÇ",
      "> ```shell\r\n> https_proxy=http://127.0.0.1:9090 python -m autogpt --gpt3only\r\n> ```\r\n\r\nËØ∑ÈóÆ10060ÁöÑÊä•ÈîôÂ¶Ç‰ΩïÂ§ÑÁêÜÂêñÔºü",
      "@Maxwongfong ÊàëÁöÑÁ§∫‰æãÈÄÇÁî®‰∫é Linux/MacOS„ÄÇ ÂØπ‰∫é WindowsÔºåÊÇ®ÂøÖÈ°ªÁü•ÈÅìÂ¶Ç‰Ωï‰∏∫ÂëΩ‰ª§Ë°åËÆæÁΩÆ‰ª£ÁêÜÔºå‰æãÂ¶ÇÔºåstackoverflow.com/a/57613619„ÄÇ ÂØπ‰∏çËµ∑ÔºåÊàë‰∏çÁÜüÊÇâWindows„ÄÇ ÊàëÂª∫ËÆÆ‰ΩøÁî® Linux Êàñ‰ΩøÁî® WSL„ÄÇ",
      "> > \r\n> \r\n> thank you so much! I had already solved this question!\r\nCould you tell me how to solve it\r\n",
      "Âú®windows‰∏≠Â∞ÜcmdÁöÑ‰ª£ÁêÜÊúçÂä°Âô®Âú∞ÂùÄËÆæÁΩÆ‰∏∫Ê¢ØÂ≠êÁöÑ‰ª£ÁêÜÊúçÂä°Âô®Âú∞ÂùÄ ÁÑ∂ÂêéËß£ÂÜ≥‰∫ÜËøô‰∏™ÈóÆÈ¢ò„ÄÇÂëΩ‰ª§‰∏∫\r\nset http_proxy=http://127.0.0.1:Á´ØÂè£Âè∑\r\nset https_proxy=http://127.0.0.1:Á´ØÂè£Âè∑\r\n\r\n",
      "@dingpf08 thank you! I have added the method to the README.",
      "I'm concerned with complexifying the repo while it is in early stages of growth and has not yet had time to solidify.\r\n\r\nI would prefer to be looking at building region-specific issues into the core NOT UNTIL the core has stabilized.\r\n\r\nAnd why mention China specifically? How about Italy/Germany that have banned ChatGPT?",
      "@p-i- I don't know how Italy/Germany ban ChatGPT. Mainland China has great firewall (GFW). So developers from China constantly need to address issues like proxy. Adding this to README can address many issues. The issue area in this repo can be less busy :)",
      "> Âú®windows‰∏≠Â∞ÜcmdÁöÑ‰ª£ÁêÜÊúçÂä°Âô®Âú∞ÂùÄËÆæÁΩÆ‰∏∫Ê¢ØÂ≠êÁöÑ‰ª£ÁêÜÊúçÂä°Âô®Âú∞ÂùÄ ÁÑ∂ÂêéËß£ÂÜ≥‰∫ÜËøô‰∏™ÈóÆÈ¢ò„ÄÇÂëΩ‰ª§‰∏∫\n> \n> set http_proxy=http://127.0.0.1:Á´ØÂè£Âè∑\n> \n> set https_proxy=http://127.0.0.1:Á´ØÂè£Âè∑\n> \n> Êò®ÊôöÊç¢‰∫Ü‰∏Ä‰∏™ÊÄùË∑ØËß£ÂÜ≥‰∫ÜÔºåËøô‰∏™ÈóÆÈ¢ò‰πãÂâçÂ∞±ÊúâÂá∫Áé∞„ÄÇÊòØGoogle dnsÊúçÂä°Âô®ËÆæÁΩÆÁöÑÈóÆÈ¢òÔºåËÆæÁΩÆÂ•ΩÂ∞±Ë°å„ÄÇÊ¢ØÂ≠êipÂ§™Â§öÔºåÁªèÂ∏∏ÊîπÂèò„ÄÇ\n\n> \n> \n\n",
      "I withdraw my objection to merging, if the conflicts are resolved."
    ],
    "num_comments": 29,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 5784,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 60edecd6cac8..e34cf9aaedc7 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -153,6 +153,12 @@ OPENAI_API_KEY=your-openai-api-key\n # BROWSE_CHUNK_MAX_LENGTH=3000\n ## BROWSE_SPACY_LANGUAGE_MODEL is used to split sentences. Install additional languages "
  },
  {
    "pr_title": "feat(rnd,infra): Pull out websockets away from server api",
    "pr_body": "### Background\r\nThis is the base for splitting out into microservices. First is to purely keep as much of the same logic and code but pull out websocket server and rest server so they can be deployed seperately. \r\nAs part of this also replaced the python queue with Redis. \r\n\r\nEventually more of the in memory queue implementations can use redis. \r\n### Changes üèóÔ∏è\r\nAdded a Redis queue\r\nAdded a docker compose to run entire backend together\r\nAdded Dockerfile for websockets (Dockerfile.ws)\r\nAdded helm",
    "pr_number": 7899,
    "comments": [
      "Got these during standup\r\n```\r\nrnd-ws_server-1    | Skipping virtualenv creation, as specified in config file.\r\nrnd-ws_server-1    | Warning: 'ws' is an entry point defined in pyproject.toml, but it's not installed as a script. You may get improper `sys.argv[0]`.\r\nrnd-ws_server-1    | \r\nrnd-ws_server-1    | The support to run uninstalled scripts will be removed in a future release.\r\nrnd-ws_server-1    | \r\nrnd-ws_server-1    | Run `poetry install` to resolve and get rid of this message.\r\nrnd-ws_server-1    | \r\nrnd-rest_server-1  | Warning: 'app' is an entry point defined in pyproject.toml, but it's not installed as a script. You may get improper `sys.argv[0]`.\r\nrnd-rest_server-1  | \r\nrnd-rest_server-1  | The support to run uninstalled scripts will be removed in a future release.\r\nrnd-rest_server-1  | \r\nrnd-rest_server-1  | Run `poetry install` to resolve and get rid of this message.\r\nrnd-rest_server-1  | \r\n```\r\n\r\nAlso hit this when trying to create a graph from a template\r\n```\r\nrnd-rest_server-1  | INFO:     192.168.65.1:46167 - \"POST /api/graphs HTTP/1.1\" 500 Internal Server Error\r\nrnd-rest_server-1  | ERROR:    Exception in ASGI application\r\nrnd-rest_server-1  | Traceback (most recent call last):\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\r\nrnd-rest_server-1  |     result = await app(  # type: ignore[func-returns-value]\r\nrnd-rest_server-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\nrnd-rest_server-1  |     return await self.app(scope, receive, send)\r\nrnd-rest_server-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\nrnd-rest_server-1  |     await super().__call__(scope, receive, send)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\r\nrnd-rest_server-1  |     await self.middleware_stack(scope, receive, send)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\nrnd-rest_server-1  |     raise exc\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\nrnd-rest_server-1  |     await self.app(scope, receive, _send)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 91, in __call__\r\nrnd-rest_server-1  |     await self.simple_response(scope, receive, send, request_headers=headers)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 146, in simple_response\r\nrnd-rest_server-1  |     await self.app(scope, receive, send)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\nrnd-rest_server-1  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\nrnd-rest_server-1  |     raise exc\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\nrnd-rest_server-1  |     await app(scope, receive, sender)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\r\nrnd-rest_server-1  |     await self.middleware_stack(scope, receive, send)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\r\nrnd-rest_server-1  |     await route.handle(scope, receive, send)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\r\nrnd-rest_server-1  |     await self.app(scope, receive, send)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\r\nrnd-rest_server-1  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\nrnd-rest_server-1  |     raise exc\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\nrnd-rest_server-1  |     await app(scope, receive, sender)\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\r\nrnd-rest_server-1  |     response = await func(request)\r\nrnd-rest_server-1  |                ^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\r\nrnd-rest_server-1  |     raise e\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\r\nrnd-rest_server-1  |     raw_response = await run_endpoint_function(\r\nrnd-rest_server-1  |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\nrnd-rest_server-1  |     return await dependant.call(**values)\r\nrnd-rest_server-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/app/rnd/autogpt_server/autogpt_server/server/rest_api.py\", line 314, in create_new_graph\r\nrnd-rest_server-1  |     return await cls.create_graph(create_graph, is_template=False, user_id=user_id)\r\nrnd-rest_server-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/app/rnd/autogpt_server/autogpt_server/server/rest_api.py\", line 355, in create_graph\r\nrnd-rest_server-1  |     return await graph_db.create_graph(graph, user_id=user_id)\r\nrnd-rest_server-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/app/rnd/autogpt_server/autogpt_server/data/graph.py\", line 399, in create_graph\r\nrnd-rest_server-1  |     await __create_graph(tx, graph, user_id)\r\nrnd-rest_server-1  |   File \"/app/rnd/autogpt_server/autogpt_server/data/graph.py\", line 410, in __create_graph\r\nrnd-rest_server-1  |     await AgentGraph.prisma(tx).create(\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/prisma/actions.py\", line 1208, in create\r\nrnd-rest_server-1  |     resp = await self._client._execute(\r\nrnd-rest_server-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/prisma/_base_client.py\", line 533, in _execute\r\nrnd-rest_server-1  |     return await self._engine.query(builder.build(), tx_id=self._tx_id)\r\nrnd-rest_server-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/prisma/engine/_query.py\", line 404, in query\r\nrnd-rest_server-1  |     return await self.request(\r\nrnd-rest_server-1  |            ^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/prisma/engine/_http.py\", line 233, in request\r\nrnd-rest_server-1  |     return self._process_response_data(data=data, response=response)\r\nrnd-rest_server-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/prisma/engine/_http.py\", line 87, in _process_response_data\r\nrnd-rest_server-1  |     return utils.handle_response_errors(response, errors_data)\r\nrnd-rest_server-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nrnd-rest_server-1  |   File \"/usr/local/lib/python3.11/site-packages/prisma/engine/utils.py\", line 175, in handle_response_errors\r\nrnd-rest_server-1  |     raise exc(error)\r\nrnd-rest_server-1  | prisma.errors.ForeignKeyViolationError: Foreign key constraint failed on the field: `foreign key`\r\n```\r\n",
      "@ntindle the first one is warnings, existing before this change and should affect actually running\r\n\r\nnumber 2, taking a look have you run the migrations for postgres?"
    ],
    "num_comments": 2,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 403432,
    "code_diff": "diff --git a/rnd/autogpt_builder/.env.example b/rnd/autogpt_builder/.env.example\nindex cdf5af60292e..9d36f0ce32d1 100644\n--- a/rnd/autogpt_builder/.env.example\n+++ b/rnd/autogpt_builder/.env.example\n@@ -1,4 +1,5 @@\n NEXT_PUBLIC_AGPT_SERVER_URL=http://localhost:8000/api\n+NEXT_PUBLIC_AGPT_WS_SERVER_UR"
  },
  {
    "pr_title": "fix(frontend): add typechecks and fix existing type errors in frontend",
    "pr_body": "<!-- Clearly explain the need for these changes: -->\r\nWe want to be able to use typechecking and see errors before they occur. This is a PR to help enable us to do so by fixing the existing errors and hopefully not causing new ones.\r\n\r\n### Changes üèóÔ∏è\r\n- adds check to ci\r\n- disables some code points\r\n- fixes lots of type errors\r\n- fixes a bunch of the stories\r\n\r\n<!-- Concisely describe all of the changes made in this pull request: -->\r\n\r\n### Checklist üìã\r\n\r\n#### For code changes:\r\n- [x] I have cle",
    "pr_number": 9336,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 76228,
    "code_diff": "diff --git a/.github/workflows/platform-frontend-ci.yml b/.github/workflows/platform-frontend-ci.yml\nindex 114b19007061..8fc27fde5689 100644\n--- a/.github/workflows/platform-frontend-ci.yml\n+++ b/.github/workflows/platform-frontend-ci.yml\n@@ -37,6 +37,25 @@ jobs:\n         run: |\n           yarn lint"
  },
  {
    "pr_title": "feat(platform): OAuth support + API key management + GitHub blocks",
    "pr_body": "- Resolves #8002\r\n  - #8006\r\n    - #8039\r\n  - #8003\r\n  - #8004\r\n  - #8005\r\n  - #8072\r\n  - #8073\r\n\r\n<p>\r\n<img src=\"https://github.com/user-attachments/assets/7d76f1e0-2c2c-4ab7-8855-55f1bdf9fccb\" width=\"48%\" />\r\n<img src=\"https://github.com/user-attachments/assets/b4f428e1-579e-4b94-aabc-de6a804b12c4\" width=\"48%\" />\r\n<img src=\"https://github.com/user-attachments/assets/890d469d-4d16-450c-b8f4-367e4c763955\" width=\"48%\" />\r\n</p>\r\n\r\n## Changes üèóÔ∏è\r\n\r\n### Config\r\n- For Supabase, the back end needs `SU",
    "pr_number": 8044,
    "comments": [
      "If you read this, this PR works :)",
      "@Pwuts how do I run this PR? Are there config settings for supabase needed?",
      "@aarushik93 I have updated the description with additional config instructions:\r\n> - For Supabase, the back end now needs `SUPABASE_URL`, `SUPABASE_KEY`, and `SUPABASE_JWT_SECRET`\r\n>   - `SUPABASE_KEY` must be a **service role key** as the anon key doesn't have sufficient privileges for the operations it's used for\r\n> - For the GitHub integration to work, the back end now needs `GITHUB_CLIENT_ID` and `GITHUB_CLIENT_SECRET`",
      "Thanks could you please add the new envrionment variables (SUPABASE_KEY) to the appropriate places in the .env.example and docker compose please?",
      "CI should work as soon as #8123 is merged",
      "What is my callback url for github?",
      "Rename service key to service role key to match the supabase .env",
      "Add FRONTEND_BASE_URL to the .env.example as a required variable",
      "Improve docs for github setup and integration -> OAuth Callback URL is http://localhost:3000/auth/integrations/oauth_callback when configuring github locally",
      "> ### Config\r\n> - For Supabase, the back end needs `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`, and `SUPABASE_JWT_SECRET`\r\n> - For the GitHub integration to work, the back end needs `GITHUB_CLIENT_ID` and `GITHUB_CLIENT_SECRET`\r\n> - For integrations OAuth flows to work in local development, the back end needs `FRONTEND_BASE_URL` to generate login URLs with accurate redirect URLs\r\n\r\n@aarushik93 does this require changes to any of the helm charts?",
      "hey yes we do need them can you add the secrets as empty strings for now? in the values file please. Or I can do it.\r\n\r\nIs the service role key a secret key?",
      "> hey yes we do need them can you add the secrets as empty strings for now? in the values file please. Or I can do it.\r\n\r\nI can add them, but is this appropriate/necessary? All of the newly required config options are secrets, and if I search for e.g. `SUPABASE_JWT_SECRET` in the helm charts, I only find one entry, in `autogpt-market/values.dev.yaml`: not in the charts for the front end or back end.\r\n\r\n> Is the service role key a secret key?\r\n\r\nYes. It has admin level privileges within Supabase."
    ],
    "num_comments": 12,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 176799,
    "code_diff": "diff --git a/.github/workflows/platform-backend-ci.yml b/.github/workflows/platform-backend-ci.yml\nindex 20a64892fe30..eb5a3481c70b 100644\n--- a/.github/workflows/platform-backend-ci.yml\n+++ b/.github/workflows/platform-backend-ci.yml\n@@ -105,7 +105,7 @@ jobs:\n           LOG_LEVEL: ${{ runner.debug "
  },
  {
    "pr_title": "feat(platform/builder): Builder credentials support + UX improvements",
    "pr_body": "- Resolves #10313\r\n- Resolves #10333\r\n\r\nBefore:\r\n\r\nhttps://github.com/user-attachments/assets/a105b2b0-a90b-4bc6-89da-bef3f5a5fa1f\r\n- No credentials input\r\n- Stuttery experience when panning or zooming the viewport\r\n\r\nAfter:\r\n\r\nhttps://github.com/user-attachments/assets/f58d7864-055f-4e1c-a221-57154467c3aa\r\n- Pretty much the same UX as in the Library, with fully-fledged credentials input support\r\n- Much smoother when moving around the canvas\r\n\r\n### Changes üèóÔ∏è\r\n\r\nFrontend:\r\n- Add credentials inpu",
    "pr_number": 10323,
    "comments": [
      "Thanks for this PR improving the graph run mechanism! The code changes look promising, making good architectural improvements by passing inputs directly instead of storing them on input nodes.\n\nHowever, before we can merge this, please complete the checklist in the PR description:\n\n- The most important item missing is your test plan - what scenarios did you test to ensure these changes work correctly?\n- Please confirm you've tested the functionality for both regular graph runs and scheduled runs\n\nAlso, could you provide a bit more detail about the architectural improvements you're making? The current description mentions \"Pass run inputs instead of storing them on the input nodes\" but it would be helpful to understand:\n\n1. Why this approach is better\n2. What specific issues it solves from the referenced issue #10313\n\nThis will help reviewers understand the context and intent of the changes better. The code changes themselves look well-structured, but we need the checklist completed before merging.",
      "Thank you for your PR to improve the graph run mechanism! The changes look promising, cleaning up the run input UI components and modifying how run inputs are passed instead of storing them on input nodes.\n\nHowever, before we can approve this PR, please complete the checklist in the PR description. Currently, none of the items are checked off. Please mark the items as completed or remove irrelevant sections if they don't apply.\n\nSpecifically:\n- Check the \"I have clearly listed my changes in the PR description\" item\n- Complete your test plan and check that item\n- Check that you've tested your changes according to the test plan\n\nAlso, I noticed the PR has a \"conflicts\" label - please resolve any merge conflicts before we can proceed with the review.\n\nThe code changes themselves look good - the refactoring to pass run inputs directly instead of storing them on input nodes is a good improvement to the architecture.",
      "Thank you for your PR to improve the graph run mechanism in the frontend builder! The code changes look well-structured and focused on the task of refactoring how run inputs are handled.\n\nHowever, I notice that the checklist in your PR description hasn't been completed. Before we can merge this PR, could you please:\n\n1. Check off the items in the checklist that you have completed\n2. Add a test plan that describes how you've tested these changes\n3. Complete any remaining tests according to your test plan\n\nYour changes appear to be significant, involving multiple components related to the run input mechanism. A thorough test plan would help reviewers understand how you've verified that these changes work correctly in different scenarios.\n\nOnce you've updated the PR description with a completed checklist and test plan, we'll be able to proceed with the review process.",
      "Thank you for your PR to improve the graph run mechanism in the frontend builder. The changes look well-structured and address the issue mentioned in #10313.\n\nHowever, I noticed that the checklist in your PR description is not completely filled out. Our guidelines require all checklist items to be checked off before a PR can be merged. Specifically, please complete these items:\n\n- [x] I have clearly listed my changes in the PR description\n- [x] I have made a test plan\n- [x] I have tested my changes according to the test plan\n\nPlease also add your test plan in the appropriate section so we can understand how you've verified these changes.\n\nThe actual code changes look good - you've improved how run inputs are handled by passing them directly instead of storing them on input nodes, which should make the system more maintainable. I particularly appreciate the clean refactoring of the RunnerUIWrapper and other UI components.\n\nOnce you've completed the checklist with your test plan, this PR should be ready for another review.",
      "Thank you for this PR to improve the graph run mechanism!\n\nThe changes look good overall - I can see you've refactored how run inputs are handled, moving from storing them on input nodes to passing them directly. The code is well-structured and the refactoring seems comprehensive across both frontend and backend components.\n\nHowever, before this can be merged:\n\n### Required Fixes\n\n- **Complete the checklist in the PR description**: Please check off all the boxes in the checklist and add your test plan. This is required for all PRs with code changes.\n\n### Additional Comments\n\n- I like the approach to clean up the run input UI components and the refactoring to pass run inputs instead of storing them on nodes\n- The backend changes to support this look appropriate and maintain proper security patterns\n- The new GraphMeta class is a nice improvement for returning metadata without exposing node/link details\n\nOnce you've completed the checklist with a proper test plan, this PR should be ready for another review.",
      "Thank you for your contribution to improve the graph run mechanism. I've reviewed your PR and noticed a few issues that need to be addressed before it can be merged:\n\n1. **Incomplete Checklist**: The checklist in your PR description is not filled out. Please either:\n   - Complete the checklist by checking off all items\n   - If these changes are non-material (refactoring only), you can remove the checklist section entirely\n\n2. **Test Plan**: Since this appears to be a significant refactoring of the run input handling mechanism, please include a test plan describing how you've verified the changes work correctly.\n\nYour changes look well-structured and focused on the stated goal of improving the graph run mechanism. The PR does a good job of consolidating run input handling and cleaning up the UI components related to this functionality.\n\nOnce you've addressed the checklist requirements, this PR should be ready for another review.",
      "Thank you for your work on improving the graph run mechanism! Before we can merge this PR, there are a few process items that need to be addressed:\n\n1. **PR Description**: The description starts with changes but cuts off with \"...\". Please complete the description to fully reflect all changes in this PR, which is quite extensive. A thorough description helps reviewers understand the scope and intent of your changes.\n\n2. **Checklist**: The checklist in the PR description is completely unchecked. Please fill out the checklist items, particularly focusing on:\n   - Clearly listing all changes\n   - Including a test plan\n   - Documenting how you've tested these changes\n\nThe code changes themselves look good - I appreciate the comprehensive refactoring of the run mechanism, making it more maintainable and consistent. The separation of concerns between different components is much improved.\n\nOnce you've updated the description and completed the checklist, we'll be able to move forward with the review process.",
      "## PR Review Feedback\n\nThank you for your work on improving the graph run mechanism. This PR introduces significant improvements to how graph runs are handled. However, there are a few issues that need to be addressed before this can be merged:\n\n### Required Changes\n\n1. **Incomplete Checklist**: The PR checklist is completely unchecked. Please fill out the checklist completely, including your test plan. This is a mandatory requirement for all PRs.\n\n2. **Scope Mismatch**: While your PR title indicates this is a frontend/builder change, the PR contains substantial backend changes as well, including:\n   - Significant modifications to `backend/data/graph.py`\n   - Adding a new `GraphMeta` class\n   - Renaming functions like `get_graphs` to `list_graphs`\n   - Moving endpoint functionality\n\n   Please consider either:\n   - Updating the PR title to better reflect the full scope of changes (e.g., `feat(platform): Improve graph run mechanism`), or\n   - Splitting this into separate PRs for frontend and backend changes\n\n### Positive Aspects\n\n- The refactoring of run input handling appears to be a significant improvement\n- The code looks well-structured and follows consistent patterns\n- The changes maintain proper user_id security checks\n- The reorganization of the API appears to make the codebase more maintainable\n\nPlease address the checklist issue at minimum before this PR can be merged. Let me know if you need any clarification or assistance with these changes.",
      "Thank you for your detailed PR! The code changes look well-structured and focused on improving the graph run mechanism.\n\nHowever, before we can approve this PR, we need a completed test plan in your checklist. Given the scope of your changes (including backend API modifications and frontend UI refactoring), it's important to document how you've tested these changes.\n\nPlease update your PR description to include:\n\n- A completed test plan section in the checklist detailing what scenarios you've tested\n- Check off the remaining checkboxes in the PR description\n\nSome testing scenarios to consider:\n- Testing with credentials inputs\n- Testing run inputs with and without saved presets\n- Testing the new edge/bead rendering optimizations\n- Verifying API compatibility with the endpoint changes\n\nOnce you've added this information, we'll be happy to review again!",
      "Thanks for your detailed PR submission to improve the graph run mechanism! Before we can approve this, there are a couple of items that need to be addressed:\n\n1. **Incomplete checklist:** You've checked off that you've listed your changes, but the test plan section is incomplete. Please fill out a test plan detailing how you've verified the changes work as expected.\n\n2. **Scope mismatch:** Your PR title indicates this is a frontend/builder feature, but there are significant backend changes as well (roughly 30-35% of the diff). These include:\n   - Adding a new `GraphMeta` model\n   - Renaming API endpoints\n   - Moving webhook trigger setup functionality from library agents to presets\n   - Changes to various backend data models\n\nPlease either:\n   - Update your PR title to reflect both frontend and backend changes (e.g., `feat(platform): Improve graph run mechanism`) \n   - OR split this PR into separate frontend and backend PRs if that makes more sense\n\nThe code changes themselves look well-structured and the refactoring seems beneficial, particularly how you've reduced unnecessary re-renders and improved the run input handling. Once you've addressed the checklist and scope issues, we should be ready to proceed with the review.\n",
      "Thank you for your comprehensive PR! The changes to improve the graph run mechanism look well-structured and thoughtful.\n\nI noticed a few items that need to be addressed before this can be merged:\n\n1. **Incomplete Checklist**: Your PR checklist is partially completed. Please fill out the test plan section or mark it as N/A if not applicable.\n\n2. **TODO Item**: You mention a \"weird double-close bug on `RunnerInputDialog`\" in your TODO list. Is this something that should be fixed before merging, or can it be addressed in a follow-up PR?\n\nThe code changes look solid with good organization and improved naming. I especially appreciate:\n- Moving from fire-and-forget methods to direct async callbacks in `useAgentGraph`\n- Reducing unnecessary re-renders in the Builder\n- The refactoring of `Graph` to match naming conventions with `LibraryAgent`\n\nOnce you complete the checklist with a test plan, this should be ready for review.",
      "Thanks for this comprehensive PR improving the Builder UX and adding credentials support! I appreciate the detailed description of your changes.\n\n## Issues to Address\n\n### Required Test Plan\n- The PR checklist indicates you haven't created a test plan yet. Please fill out the test plan section with specific steps to verify your changes.\n- Given the scope of changes (frontend Builder UX + credentials support), your test plan should cover:\n  - Testing the new credentials input support in the Builder\n  - Verifying the improved movement/performance in the Builder\n  - Testing the updated API routes\n  - Confirming the graph type changes work as expected\n\n### TODO Item\n- You've noted a \"weird double-close bug on `RunnerInputDialog`\" in your TODO list. Is this something you plan to fix before merging, or would you prefer to track it separately?\n\n## What I Like About This PR\n\n- Great job on reducing unnecessary re-renders in the Builder UI! The bead management improvements look solid.\n- The refactoring of the run input workflow is much cleaner, using `AgentRunDetailsView` for consistency.\n- The type improvements in the Graph model and API endpoints will help maintain code quality.\n- Moving webhook-related code to a dedicated endpoint makes the API structure more logical.\n\nOnce you've added a test plan and addressed the feedback, this PR should be ready for another review. Let me know if you have any questions!",
      "Thank you for your comprehensive PR! The Builder UX improvements and credentials support look well-implemented, with a thoughtful approach to reducing unnecessary re-renders and cleaning up the run input UI components.\n\nBefore this can be merged, please:\n\n1. Complete the test plan section in the PR checklist. Since this involves significant UI changes and credential handling, a thorough test plan is important.\n\n2. Address the TODO item you mentioned about fixing the \"weird double-close bug on RunnerInputDialog\".\n\nThe changes look sound overall - I particularly like the refactoring of the bead management logic to improve performance and the cleaner approach to handling credentials. The move to direct action async callbacks instead of fire-and-forget methods in `useAgentGraph` is also a good architectural improvement.\n\nOnce you've completed the test plan and addressed the bug, this should be ready for final review and merging.",
      "Thank you for your PR to improve the Builder UX and add credentials support! This is a substantial contribution that will improve the platform experience.\n\n### What Looks Good\n- Your changes are well-structured and align with the PR title\n- The before/after videos clearly demonstrate the improvements\n- Good refactoring to reduce unnecessary re-renders and improve canvas performance\n- Your code adds credentials support to the Builder in a way that's consistent with the Library\n\n### Required Changes Before Merging\n\n1. **Test Plan**: You need to complete the test plan in the PR description. Since your changes affect both the frontend builder UX and credentials functionality, please outline how you've tested these features. For example:\n   - Testing the credentials input in Builder with various credential types\n   - Verifying smooth canvas performance with large graphs\n   - Testing that the run inputs are correctly passed to execution\n\n2. **TODO Item**: You've noted a \"weird double-close bug on RunnerInputDialog\" - is this something you plan to fix in this PR or track separately? If it's a known issue but not blocking, please create an issue for it and reference it.\n\n3. **Backend Changes**: Your API refactoring looks good, but please ensure these endpoint changes are reflected in any client documentation if applicable.\n\nOnce you've added the test plan and addressed the TODO item (either by fixing it or creating a tracking issue), this PR should be ready for another review. The functionality changes look solid and well-implemented.",
      "Thanks for your detailed PR! The changes look good overall, implementing credentials support in the Builder's Run UX and improving canvas performance.\n\n### Feedback\n\n1. **Incomplete checklist**: Please fill out the test plan checklist section, which is currently empty. Since this is a significant change to the Builder UX, a comprehensive test plan is important.\n\n2. **Remaining TODO**: There's a TODO item about fixing a \"weird double-close bug on `RunnerInputDialog`\". Can you address this before merging, or create a follow-up issue to track it?\n\n3. **Testing recommendations**:\n   - Test with various input types in the Builder\n   - Verify credentials work correctly\n   - Check edge cases like empty inputs\n   - Test the performance improvements with large graphs\n\nThe refactoring work and code organization improvements look solid! Nice job on making the Builder experience smoother and aligning its UX with the Library.\n\nPlease update the PR with a completed checklist and a plan for the remaining TODO before proceeding.",
      "Thank you for your detailed PR! The changes to improve the Builder UX and add credentials support look comprehensive and well-structured.\n\nHowever, before we can approve this for merging, there are a couple of items that need attention:\n\n1. **Missing Test Plan**: For a PR of this size and complexity, we need a complete test plan that details how you've tested these changes. Please fill out the test plan section in the PR checklist to ensure the changes have been adequately tested.\n\n2. **Unresolved TODO**: You mentioned a \"weird double-close bug on `RunnerInputDialog`\" in your TODO list. This should be resolved before the PR is merged.\n\nThe code changes themselves look solid. I particularly like:\n- How you've reduced unnecessary re-renders to make canvas movement smoother\n- The refactoring of the credentials support to make it consistent with the Library\n- The cleanup of the run input UI components\n\nPlease update the PR with a complete test plan and resolve the noted TODO item, and we'll be happy to review again.",
      "Thank you for this comprehensive PR that adds credentials support to the Builder UI and improves performance. The changes look well-structured and the refactoring of the frontend components should lead to better code organization and improved UX.\n\nHowever, before this can be approved for merging, please complete your PR checklist:\n\n- You've marked that you've listed your changes (which is excellent and detailed), but you haven't filled out the test plan section or indicated that you've tested according to a plan.\n- For a change of this magnitude that touches multiple areas (frontend UI, backend API, etc.), it's important to have a clear test plan that verifies the functionality works as expected.\n\nPlease add a test plan and check it off to confirm you've verified your changes work properly. This should include testing:\n- Basic builder functionality with the new smooth panning/zooming\n- Creating and running agents with credentials\n- The flow from Builder to execution with the new Runner UI implementation\n\nAlso, please address the \"TODO\" you've noted in the PR description about the \"weird double-close bug on RunnerInputDialog\" - either fix it or create an issue to track it if it's not blocking.\n\nOnce the checklist is completed with a proper test plan, this PR looks ready for another review.",
      "Thanks for this PR implementing builder credentials support and improving UX! The changes look comprehensive and well-structured. \n\nThe code looks good overall - I particularly like how you've improved the bead management logic to make canvas navigation smoother, and the refactoring of the run input UI components is a nice cleanup.\n\nHowever, I noticed the checklist in your PR description is incomplete. You've marked that you've clearly listed changes (which you have done very well), but the test plan section is unchecked and empty. For a feature this important that touches both frontend and backend, we need a clear testing strategy.\n\nCould you please:\n\n1. Create and complete a test plan that covers your changes\n2. Confirm you've tested according to that plan\n3. Address the \"TODO: Fix weird double-close bug on `RunnerInputDialog`\" item you mentioned\n\nThis PR makes substantial improvements to the platform, but we need to ensure everything works as expected before merging, especially since it involves credential handling which has security implications.\n\nOnce these items are addressed, this PR looks ready for another review.",
      "Thank you for this well-structured PR! The improvements to the Builder's credential support and UX are substantial and well-implemented.\n\n### Feedback\n\n#### Great job on:\n- Clear explanation with before/after screenshots\n- Comprehensive refactoring of the run input UI\n- Performance improvements for canvas rendering\n- Aligning naming between frontend and backend\n- Moving webhook setup route to a more logical location\n\n#### Issues to address:\n\n1. **Test completion needed**: You've outlined a comprehensive test plan, but the checklist items are not marked as completed. Please complete the testing according to your plan and check off those items.\n\n2. **PR size**: This is a sizeable PR with many changes. While the changes are cohesive and related to the stated purpose, larger PRs are inherently riskier. Consider breaking future changes into smaller, more focused PRs if possible.\n\n#### Additional considerations:\n\n- The changes to `useAgentGraph` significantly improve the architecture, making the code more maintainable. Nice work on this refactoring.\n- The move from fire-and-forget methods to direct action callbacks is a good pattern improvement.\n- The rework of UI components for credentials input and run management looks much cleaner and more consistent.\n\nPlease complete the testing according to your test plan and update the PR by checking off those items in the checklist. Once the testing is complete, this PR should be ready for approval.",
      "This PR looks good overall! The implementation for Builder credentials support and UX improvements is well-structured, and the test plan is thorough.\n\nThere's just one small issue to fix before this can be merged:\n\n- The parent checkbox for \"I have tested my changes according to the test plan\" should be checked since all the sub-items are checked. Please update this in the PR description.\n\nSome observations about your implementation:\n\n1. Great work on the refactoring to use `GraphMeta` and improve type safety throughout the codebase.\n\n2. The UX improvements for moving around the canvas look excellent - reducing unnecessary re-renders should make the builder experience much smoother.\n\n3. Good refactoring of the run input UI components to leverage the existing `AgentRunDraftView` - this creates consistency between the Builder and Library experiences.\n\n4. The move of the webhook setup endpoint to `/library/presets/setup-trigger` makes sense from an API organization perspective.\n\nOnce you check that parent checkbox in the PR description, this should be ready to merge!"
    ],
    "num_comments": 20,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 171111,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/data/graph.py b/autogpt_platform/backend/backend/data/graph.py\nindex 31a4c8cd3718..a54e1c1d0b7f 100644\n--- a/autogpt_platform/backend/backend/data/graph.py\n+++ b/autogpt_platform/backend/backend/data/graph.py\n@@ -14,7 +14,7 @@\n     AgentNodeLinkCreateInp"
  },
  {
    "pr_title": "fix(backend): Ensure we only present working auth options on blocks",
    "pr_body": "To allow for a simpler dev experience, the new SDK now auto discovers providers and registers them. However, the OAuth system was still requiring these credentials to be hardcoded in the settings object.\r\n\r\nThis PR changes that to verify the env var is present during registration and then allows the OAuth system to load them from the env.\r\n\r\n### Changes üèóÔ∏è\r\n\r\n- **OAuth Registration**: Modified `ProviderBuilder.with_oauth(..)` to check OAuth env vars exist during registration\r\n- **OAuth Loading**",
    "pr_number": 10454,
    "comments": [
      "> LGTM, but the Copilot comments are valid. @Pwuts review would be needed here\r\n\r\nThank you, all pr comments have been actioned and pushed"
    ],
    "num_comments": 1,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 35298,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/blocks/__init__.py b/autogpt_platform/backend/backend/blocks/__init__.py\nindex 4715d44611e8..f6299cbb53df 100644\n--- a/autogpt_platform/backend/backend/blocks/__init__.py\n+++ b/autogpt_platform/backend/backend/blocks/__init__.py\n@@ -1,10 +1,14 @@\n import"
  },
  {
    "pr_title": "feat(server): Clean up resources when spinning down services/processes",
    "pr_body": "- Resolves #7937\r\n- Blocked by #7892\r\n\r\n### Changes üèóÔ∏è\r\n\r\n- Add SIGTERM handler and `cleanup()` hook to `AppProcess`\r\n- Implement `cleanup()` on `AppService` to close DB and Redis connections\r\n- Implement `cleanup()` on `ExecutionManager` to shut down worker pool\r\n- Add `atexit` and SIGTERM handlers to node executor to close DB connection and shut down node workers\r\n- Improve logging in `.executor.manager`\r\n- Fix shutdown order of `.util.test:SpinTestServer`\r\n\r\n### PR Quality Scorecard ‚ú®\r\n\r\n<!--",
    "pr_number": 7938,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 19362,
    "code_diff": "diff --git a/rnd/autogpt_server/autogpt_server/executor/manager.py b/rnd/autogpt_server/autogpt_server/executor/manager.py\nindex ff4521160a48..6bf449f6356b 100644\n--- a/rnd/autogpt_server/autogpt_server/executor/manager.py\n+++ b/rnd/autogpt_server/autogpt_server/executor/manager.py\n@@ -1,6 +1,10 @@\n"
  },
  {
    "pr_title": "feat(server, builder): Implement \"STOP\" button for graph runs",
    "pr_body": "- Resolves #7751\r\n- ‚ö†Ô∏è Merge first to reduce diff and fix CI: #7941\r\n\r\n### Changes üèóÔ∏è\r\n\r\n- feat(builder): Add \"Stop Run\" buttons to monitor and builder\r\n  - Implement additional state management in `useAgentGraph` hook\r\n    - Add \"stop\" request mechanism\r\n    - Implement execution status tracking using WebSockets\r\n    - Add `isSaving`, `isRunning`, `isStopping` outputs\r\n    - Add `requestStopRun` method\r\n      - Rename `requestSaveRun` to `requestSaveAndRun` for clarity\r\n  - Add needed functiona",
    "pr_number": 7892,
    "comments": [
      "FTR: CI currently fails on scheduler.py despite me not making any changes to that file because I fixed type passthrough for the `@expose` decorator and now the type checker can see that some function call in scheduler.py to an `@expose`d method is missing an argument.\r\n\r\nI would like to ignore this error since I technically didn't cause it, but I can't because it also prohibits the tests from running.",
      "@Pwuts I have just merged this:\r\nhttps://github.com/Significant-Gravitas/AutoGPT/pull/7917\r\n\r\nCan you check if this fixes your CI ?"
    ],
    "num_comments": 2,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 37035,
    "code_diff": "diff --git a/.github/workflows/autogpt-server-ci.yml b/.github/workflows/autogpt-server-ci.yml\nindex f71f1a8aeef6..6c78301fda16 100644\n--- a/.github/workflows/autogpt-server-ci.yml\n+++ b/.github/workflows/autogpt-server-ci.yml\n@@ -128,9 +128,14 @@ jobs:\n \n       - name: Run pytest with coverage\n    "
  },
  {
    "pr_title": "feat: move cassettes to git submodule",
    "pr_body": "### Background\r\nWe want to be able to access all our cassettes from one directory/repo/submodule and keep the cassettes in sync with correct version of `master` on main repo.\r\n\r\nUsing git submodules allows us to achieve that without knowing exactly what commit/hash we need to be at. Git submodule will always update to latest version of `main` relative to the primary repo. \r\n\r\nThis means that as long as we update our git submodules on merges into `master`, we will always be able to keep our casse",
    "pr_number": 4294,
    "comments": [
      "Hmm I'm not sure why it's complaining about conflicting files when I deleted that directory `tests/integration/challenges/memory/cassettes`",
      "> Hmm I'm not sure why it's complaining about conflicting files when I deleted that directory `tests/integration/challenges/memory/cassettes`\r\n\r\nYou have (re)moved it in your fork, but it has been updated in `master`. You can resolve the conflict by updating the cassettes in their new location with the updated content that's in `master`.",
      "> @Pwuts: You have (re)moved it in your fork, but it has been updated in `master`. You can resolve the conflict by updating the cassettes in their new location with the updated content that's in `master`.\r\n\r\nderp, I'm not used to working outside the repo, old habits die hard lol. \r\n\r\nThanks for the help. I synced my fork with `master` and merged it with my changes and re-ran the tests to generate cassettes in their correct directory with latest code. ",
      "Looks like I'm failing tests, I'll have to poke into it tomorrow",
      "> Looks like a good start, although watch out when regenerating the cassettes. If they are generated with non-standard settings (e.g. `EXECUTE_LOCAL_COMMANDS=True`), they won't be used in CI.\r\n\r\nAhh gotcha okay, I'm going to make the suggested changes and instead just move the latest cassettes over from master instead of trying to regenerate them locally\r\n\r\n",
      "Cool I think I ironed out the test issues",
      "I plan to work on this more over the weekend, I'll just use my own git submodule now and we can swap it out with the official git submodule repo towards the end",
      "I made more changes to get closer to the goal (I need to test my changes to the CI workflow file but im having trouble getting them to stick)\r\n\r\nFor example I made some changes to the `ci.yml` workflow file and noticed none of my new steps were appearing:\r\nhttps://github.com/Significant-Gravitas/Auto-GPT/actions/runs/5031267556/jobs/9024235171?pr=4294\r\n\r\nThough when I look at the Workflow file it has my changes:\r\nhttps://github.com/Significant-Gravitas/Auto-GPT/actions/runs/5031267556/workflow?pr=4294#L73-L76\r\n\r\nHow can I ensure that my CI workflow is being used? \r\n\r\nIt looks as though my changes to `docker-ci` workflow is being run so a little confused ü§∑ \r\n\r\nI'm wondering if this is because I'm technically working through a pull on my repo, and not on the pull from within the repo?\r\n",
      "@BaseInfinity the best way to test the ci.yml is to merge it into your master: \r\nbaseInfinity:master\r\nBecause this ci.myl can only be used once it's merged, it's a security feature",
      "> @merwanehamadi the best way to test the ci.yml is to merge it into your master: baseInfinity:master Because this ci.myl can only be used once it's merged, it's a security feature\r\n\r\nAhh okay i had a feeling it had to do with something like that, I'll do the thing tomorrow and try again, ty for the tip!",
      "https://github.com/Significant-Gravitas/Auto-GPT/pull/4355"
    ],
    "num_comments": 11,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 706534,
    "code_diff": "diff --git a/.gitattributes b/.gitattributes\nindex b28dfc4d268a..2094a6c9f0e8 100644\n--- a/.gitattributes\n+++ b/.gitattributes\n@@ -1,5 +1,5 @@\n # Exclude VCR cassettes from stats\n-tests/**/cassettes/**.y*ml linguist-generated\n+tests/cassettes/**/**.y*ml linguist-generated\n \n # Mark documentation as "
  },
  {
    "pr_title": "feat(agent): Draft of agent group feature",
    "pr_body": "* Add `divider_and_conquer` strategy for use in agent members\r\n* Add `AgentMember` class\r\n* Add `AgentGroup` class\r\n* Add `hire_agent` and `create_agent` and `create_task`\r\n* Add `test_agent_group` as an integration test with VCR\r\n* Add `autogpt_multi_agent.sh` file to run autogpt in multi-agent mode\r\n\r\n### Background\r\n![image](https://github.com/Significant-Gravitas/AutoGPT/assets/30555423/e0796037-cf9d-43b0-a9d7-053a086b407c)\r\n\r\nIn this PR we want to add AgentGroup feature as discussed in [#68",
    "pr_number": 6896,
    "comments": [
      "This PR needs to be changed. This was just for review.",
      "I updated the pull request description. This is a just draft version and needs to be improved.",
      "One problem I have right now is that we need to have a field as sub_tasks in the `AgentTask` class. It will store details of sub-tasks created and the status of them. I am going to store sub-tasks in `agent_task`",
      "> One problem I have right now is that we need to have a field as sub_tasks in the `AgentTask` class. It will store details of sub-tasks created and the status of them. I am going to store sub-tasks in `agent_task`\r\n\r\nHave you read the roadmap items in discussion? I think this mayyy be covered in one?",
      "> > One problem I have right now is that we need to have a field as sub_tasks in the `AgentTask` class. It will store details of sub-tasks created and the status of them. I am going to store sub-tasks in `agent_task`\n> \n> Have you read the roadmap items in discussion? I think this mayyy be covered in one?\n\nCould you give me the link of discussion?",
      "https://github.com/Significant-Gravitas/AutoGPT/discussions/6971",
      "~~Are these commands enabled? Was trying to test but can't get the llm to execute them lol~~ Commands are not being inserted into the prompt",
      "> ~~Are these commands enabled? Was trying to test but can't get the llm to execute them lol~~ Commands are not being inserted into the prompt\n\nI wrote another sh file to run multi agent mode. But it's not over yet. I am working on it right now.",
      "I change the proposed action of the agent member. \r\nI added autogpt_multi_agent.sh and a description for that in the pull request description. It's experimental. I can't get good results and commands in my tests. I will record a video if I can for my tests.\r\n\r\n```\r\n./autogpt_multi_agent.sh --member-description \"tech_lead:you will be tech lead of github.com/Significant-Gravitas/AutoGPT project and \r\nyou need to get this project better in github:False::1\" --member-description \"hr_lead:you will be hr of this team. you need to prepare agents for this work done.:True:0\" --me\r\nmber-description \"senior_python_developer:you will be developer of this team and you will get issues to get them done:False:0\"\r\n```",
      "If I change llm response to xml, I will get better result? (I'm getting so non-json result in my tests)\nI think we have an issue for this one too.\n@ntindle @Pwuts ",
      "For support multi-agent, you commented this `Does this have to be in the Forge DB? I suggest storing this kind of state in the AgentSettings (or subclass) object, which can be serialized and stored, together with the other state of the agent. and I try to not save tasks in DB`. But when I wanted to use benchmark over it, it seemed agbenchmark used uuid of that task to load it from DB. Do you have any advice for me about what should I do? @Pwuts ",
      "> If I change llm response to xml, I will get better result? (I'm getting so non-json result in my tests)\r\n\r\nThis is out of scope for this PR and I doubt you will get better results at least on OpenAI which is optimized for json.\r\n\r\n> ... But when I wanted to use benchmark over it, it seemed agbenchmark used uuid of that task to load it from DB. Do you have any advice for me about what should I do?\r\n\r\nI wouldn't worry about DB for now; don't change it. Just make sure to store all data that the agent needs in `AgentSettings` (see https://github.com/Significant-Gravitas/AutoGPT/pull/6896#discussion_r1573291633).",
      "> This is interesting and potentially worth merging!\r\n> \r\n> We're working on [Component-based Agents](https://github.com/Significant-Gravitas/AutoGPT/pull/7054) (this linked PR doesn't move code to the forge yet - it'll be done in a separate PR) which may be merged any day now. It's going to make the forge more effective and easier to build Agents. Given that, this PR will need a refactor (let me know if you need help, can be on Discord @kcze). I recommend you have a look at the Component Agents PR and linked docs.\r\n> \r\n> Please use `black`, `isort`, `flake8` to fix linting ([docs](https://github.com/Significant-Gravitas/Nexus/wiki/Contributing#code-guidelines)).\r\n\r\nThank you @kcze for your great review. I'll check them as soon as I can.",
      "> This is out of scope for this PR and I doubt you will get better results at least on OpenAI which is optimized for json.\r\n\r\nYes, you are right.\r\n\r\n> I wouldn't worry about DB for now; don't change it. Just make sure to store all data that the agent needs in AgentSettings (see https://github.com/Significant-Gravitas/AutoGPT/pull/6896#discussion_r1573291633).\r\n\r\nYeah. I found it recently. I will fix this one too for benchmark.\r\n",
      "## PR Review\n\n<table>\n<tr>\n<tr><td> ‚è±Ô∏è&nbsp;<strong>Estimated&nbsp;effort&nbsp;to&nbsp;review [1-5]</strong></td><td>\n\n4, because the PR introduces a complex new feature with multiple new classes and methods across several files. The changes involve intricate logic for agent management and task handling, which requires careful consideration to ensure correctness and maintainability.\n\n\n</td></tr>\n<tr><td> üß™&nbsp;<strong>Relevant tests</strong></td><td>\n\nYes\n\n\n</td></tr>\n<tr><td rowspan=2> üîç&nbsp;<strong>Possible issues</strong></td>\n<td>\n\n<strong>Possible Bug:</strong> The method `create_task` in `AgentMember` class does not handle exceptions properly. If an exception occurs, it only logs the error without any further handling, which might lead to unmanaged task states or failures in task creation without proper rollback or user notification.</td></tr>\n<tr>\n<td>\n\n<strong>Design Concern:</strong> The `AgentGroup` class tightly couples member management and task handling, which might make it difficult to extend or modify the behavior of agent groups without affecting task management.</td></tr>\n<tr><td> üîí&nbsp;<strong>Security concerns</strong></td><td>\n\nNo\n\n</td></tr>\n</table>\n\n\n<details><summary> <strong>Code feedback:</strong></summary>\n\n<hr><table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/agents/agent_member.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nConsider implementing exception handling in the `create_task` method to manage task state properly in case of errors. This could include rolling back changes, notifying the user, or retrying task creation. [important]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td><a href='https://github.com/Significant-Gravitas/AutoGPT/pull/6896/files#diff-59f47bc6a885d86355b107ea87a2e398ce54c1d7445ab43955760e80198cb16dR249'>except Exception as e:</a></td></tr></table><hr>\n\n<table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/agents/agent_group.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nRefactor the `AgentGroup` class to separate concerns between member management and task handling. This could involve creating separate classes or methods that handle these concerns independently, improving modularity and maintainability. [important]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td><a href='https://github.com/Significant-Gravitas/AutoGPT/pull/6896/files#diff-f2934bca9d0db6d646aa87f7c395c412d6a8c5327ac3a360883e138a414997ccR20'>class AgentGroup:</a></td></tr></table><hr>\n\n<table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/agents/agent_member.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nAdd validation for the `role`, `initial_prompt`, and `boss_id` parameters in the `create_agent_member` function to ensure they meet expected formats or conditions before processing. This can prevent issues related to invalid data being used in agent creation. [medium]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td>async def create_agent_member(</td></tr></table><hr>\n\n<table><tr><td>relevant file</td><td>autogpts/autogpt/autogpt/agents/agent_member.py\n</td></tr><tr><td>suggestion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td>\n\n<strong>\n\nOptimize the `get_list_of_all_your_team_members` method by using a more efficient data structure or algorithm to handle large numbers of members, as the current recursive method could lead to performance issues with large agent trees. [medium]\n\n</strong>\n</td></tr><tr><td>relevant line</td><td><a href='https://github.com/Significant-Gravitas/AutoGPT/pull/6896/files#diff-59f47bc6a885d86355b107ea87a2e398ce54c1d7445ab43955760e80198cb16dR96'>def get_list_of_all_your_team_members(self) -> list[\"AgentMember\"]:</a></td></tr></table><hr>\n\n</details><hr>\n\n<details> <summary><strong>‚ú® Review tool usage guide:</strong></summary><hr> \n\n**Overview:**\nThe `review` tool scans the PR code changes, and generates a PR review which includes several types of feedbacks, such as possible PR issues, security threats and relevant test in the PR. More feedbacks can be [added](https://pr-agent-docs.codium.ai/tools/review/#general-configurations) by configuring the tool.\n\nThe tool can be triggered [automatically](https://pr-agent-docs.codium.ai/usage-guide/automations_and_usage/#github-app-automatic-tools-when-a-new-pr-is-opened) every time a new PR is opened, or can be invoked manually by commenting on any PR.\n- When commenting, to edit [configurations](https://github.com/Codium-ai/pr-agent/blob/main/pr_agent/settings/configuration.toml#L23) related to the review tool (`pr_reviewer` section), use the following template:\n```\n/review --pr_reviewer.some_config1=... --pr_reviewer.some_config2=...\n```\n- With a [configuration file](https://pr-agent-docs.codium.ai/usage-guide/configuration_options/), use the following template:\n```\n[pr_reviewer]\nsome_config1=...\nsome_config2=...\n```\n    \n\nSee the review [usage page](https://pr-agent-docs.codium.ai/tools/review/) for a comprehensive guide on using this tool.\n\n\n</details>\n",
      "@MKdir98 I love what you did on this, however the project has changed a lot since you did this and I don't see a path for it being merged. \r\n\r\nWe are currently trying to implement a feature like this in an agent builder via the concept of subgraphs. If your'e interested maybe you could help there.\r\n",
      "> @MKdir98 I love what you did on this, however the project has changed a lot since you did this and I don't see a path for it being merged. \n> \n> We are currently trying to implement a feature like this in an agent builder via the concept of subgraphs. If your'e interested maybe you could help there.\n> \n\nThat will be great. I'll check it.",
      "@MKdir98 Are you on the discord - If so reach out in the dev channel and we can discuss the details :)"
    ],
    "num_comments": 18,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 137749,
    "code_diff": "diff --git a/autogpts/autogpt/autogpt/agents/agent_group.py b/autogpts/autogpt/autogpt/agents/agent_group.py\nnew file mode 100644\nindex 000000000000..e25d3482be57\n--- /dev/null\n+++ b/autogpts/autogpt/autogpt/agents/agent_group.py\n@@ -0,0 +1,216 @@\n+import logging\n+from typing import Optional\n+from a"
  },
  {
    "pr_title": "feat(platform): Add Host-scoped credentials support for blocks HTTP requests",
    "pr_body": "Currently, we don't have a secure way to pass Authorization headers when calling the `SendWebRequestBlock`.\r\nThis hinders the integration of third-party applications that do not yet have native block support.\r\n\r\n### Changes üèóÔ∏è\r\n\r\nAdd Host-scoped credentials support for the newly introduced SendAuthenticatedWebRequestBlock.\r\n\r\n<img width=\"1000\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0d3d577a-2b9b-4f0f-9377-0e00a069ba37\" />\r\n<img width=\"1000\" alt=\"image\" src=\"https://github.co",
    "pr_number": 10215,
    "comments": [
      "Thank you for implementing host-scoped credentials support for blocks HTTP requests! This is a valuable feature that will enhance the platform's integration capabilities.\n\nYour PR description is clear, and the test plan is specific which is great. The screenshots provide helpful visual context for the feature implementation.\n\nSince this PR modifies files in the backend/data directory (graph.py and model.py), please ensure that:\n\n1. Any new or modified functions in these files correctly pass and check the user_id parameter to maintain proper access controls\n2. If there are any exceptions to this pattern, please add an explanation in the PR description\n\nAlso, given the security implications of handling credentials, consider adding some notes about:\n\n- How credentials are securely stored\n- Any access control mechanisms implemented to prevent unauthorized access to stored credentials\n- How credentials are passed securely to the HTTP requests\n\nThis would help reviewers better understand the security considerations that went into this implementation.\n\nOnce these points are addressed or confirmed, this PR looks ready to merge.",
      "Thank you for this PR implementing Host-scoped credentials for blocks HTTP requests! This will definitely improve the integration capabilities with third-party applications.\n\nA few notes:\n\n1. The PR title should use our defined scope format. Instead of `feat(backend;frontend)`, it should be something like `feat(platform/backend,platform/frontend)` to align with our conventions.\n\n2. Since there are changes to files in `backend/data/`, could you confirm that any added/modified functions properly handle user_id checks? This is important for security to ensure proper access control.\n\n3. I see this PR has a conflicts label - please resolve these conflicts before it can be merged.\n\nOtherwise, the PR looks well-documented with a clear purpose, implementation details, and test plan. The feature will be a valuable addition for secure third-party integrations.",
      "Thanks for this PR implementing Host-scoped credentials support for blocks HTTP requests! This is a valuable addition that will help with third-party integrations.\n\nThe PR description and checklist look good. I appreciate the screenshots showing the UI implementation and the clear test plan.\n\nA few things to consider:\n\n1. Since this PR is labeled with \"Possible security concern\" and touches credential handling, could you elaborate on the security measures implemented to ensure credentials are properly protected?\n\n2. For the modified functions in the backend/data/*.py files, could you confirm that user_id checks are properly implemented where needed?\n\n3. The PR introduces a new block type (SendAuthenticatedWebRequestBlock). Is this documented somewhere for users to understand how to use it correctly?\n\n4. Could you provide some more details on how the host-scoped credentials are stored and managed securely?\n\nOnce these points are addressed, the PR should be ready for a more detailed code review. Great work on implementing this feature!",
      "Thank you for implementing host-scoped credentials support for blocks HTTP requests. This is a valuable addition that will help with third-party integrations. I have a few concerns that need to be addressed before this can be merged:\n\n1. **Backend Data Files**: You've modified several files in `backend/data/` including execution.py, graph.py, integrations.py, and model.py. For security reasons, we need to ensure that any added or changed functions correctly pass and validate the user_id parameter. Could you please confirm this is the case or explain why user_id validation isn't needed for these specific changes?\n\n2. **Security Review**: Given this PR deals with credential handling, it would be beneficial to provide more details about the security measures implemented to protect these credentials, especially how they're stored, accessed, and used in HTTP requests.\n\n3. **Implementation Details**: While the screenshots provide a visual of the UI, it would be helpful to have a brief explanation of the architecture for how the host-scoped credentials work, including:\n   - How credentials are stored securely\n   - How they're associated with specific hosts\n   - The authorization flow when a block needs to use these credentials\n\n4. **Test Coverage**: The test plan mentions using the OpenAI API as a test case. Could you provide more comprehensive test scenarios covering different edge cases like:\n   - Handling of invalid credentials\n   - Security boundaries between different users' credentials\n   - Error handling scenarios\n\nOnce these points are addressed, we can proceed with a more detailed review of the implementation.",
      "Thank you for this PR implementing Host-scoped credentials support for blocks HTTP requests. This is a valuable addition that will enhance security when integrating with third-party APIs.\n\nA few points to consider:\n\n1. I notice there are changes to several files in `backend/data/` including `execution.py`, `graph.py`, `integrations.py`, and `model.py`. Could you ensure that any new or modified functions in these files properly pass and check the user_id for authorization purposes?\n\n2. Since this PR deals with credential handling, have you considered any potential security implications? The \"Possible security concern\" label suggests this might need additional review from a security perspective.\n\n3. The test plan mentions using OpenAI's API as a test case. Have you considered adding more comprehensive tests that cover various edge cases for the credential management?\n\n4. Could you clarify if there are any backward compatibility concerns with existing blocks or workflows?\n\nOverall, the PR looks promising and well-structured. With a few clarifications on the points above, particularly regarding proper user_id handling in the backend/data files, this should be ready for merging.",
      "Thank you for your contribution! The PR looks generally well-structured and addresses an important security concern by adding host-scoped credentials for HTTP requests.\n\nA few things to check before we can merge:\n\n1. The changes to files in `backend/data/` need to be reviewed for proper user_id handling:\n   - Please ensure that any new or modified functions in `execution.py`, `graph.py`, `integrations.py`, and `model.py` are correctly handling user authentication and authorization.\n\n2. While the test plan mentions testing with OpenAI's API, could you provide a bit more detail about how you verified the security aspects of this implementation? Specifically:\n   - How are credentials stored securely?\n   - How is access to these credentials controlled/limited?\n   - What prevents credential leakage across different users/organizations?\n\n3. Did you consider any potential security implications or edge cases with the host-scoped credential implementation?\n\nOnce these items are addressed, this PR should be ready for final review and merging. Thanks for improving the security posture of the platform!"
    ],
    "num_comments": 6,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 142519,
    "code_diff": "diff --git a/.github/workflows/platform-backend-ci.yml b/.github/workflows/platform-backend-ci.yml\nindex 28c61d5ef208..3492a00ea9a3 100644\n--- a/.github/workflows/platform-backend-ci.yml\n+++ b/.github/workflows/platform-backend-ci.yml\n@@ -190,9 +190,9 @@ jobs:\n       - name: Run pytest with coverage"
  },
  {
    "pr_title": "Use Chroma to augment and make LocalCache memory/embeddings pluggable",
    "pr_body": "### Background\r\n\r\nThe LocalCache memory backend implemented in https://github.com/Torantulino/Auto-GPT/pull/372 used a handrolled implementation of a storage engine, and the embedding strategy was pinned to `text-embedding-ada-002` which we may want to change/does cost money/latency\r\n\r\n### Changes\r\n\r\n- reimplemented LocalCache using [Chroma](https://github.com/chroma-core/chroma) - which also runs either in-memory or persisted, but is well tested and managed. \r\n- made embeddings swappable to ANY",
    "pr_number": 1027,
    "comments": [
      "great addition @sw-yx ! I hope it gets merged soon.",
      "whaaat i just made this PR yesterday and there are conflicts haha. will fix!",
      "We move fast, new conflicts can happen every 5 minutes here :)",
      "@nponeccop i think i fixed the conflict!",
      "re-fixed new conflicts! @nponeccop what typically happens next?",
      "@sw-yx There are conflicts now",
      "@nponeccop as far as i can tell i fixed the conflicts as of commit a6928c2. i've just updated my fork to current master but still not seeing any conflicts as of right now!",
      "@sw-yx There are both conflicts and CI failure now",
      "@nponeccop fixed! mind running CI again?",
      "@nponeccop ok i have addressed the flake issues. i'd be happy to work on better doucmenting contributor guide next so that others dont have this issue",
      "@sw-yx There are conflicts now",
      "wew that was a big merge. i've updated again.",
      "@sw-yx There are (minor) conflicts again",
      "re-updated (and checked flake tests)",
      "updated my fork but not showing on this PR yet for some reason"
    ],
    "num_comments": 15,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 39956,
    "code_diff": "diff --git a/.gitignore b/.gitignore\nindex 816cdb0c8f95..8429ca9a53f1 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -20,6 +20,7 @@ log-ingestion.txt\n logs\n *.log\n *.mp3\n+localCache/*\n mem.sqlite3\n \n # Byte-compiled / optimized / DLL files\ndiff --git a/README.md b/README.md\nindex 9cfcda75d062..e9321d31"
  },
  {
    "pr_title": "Add function and class descriptions to tests",
    "pr_body": "### Background\r\nThe codebase of the project has grown in complexity and size, making it difficult for developers to understand the purpose and functionality of the various functions and classes. To address this issue, a pull request has been submitted to add descriptions to all functions and classes in the codebase.\r\n\r\n### Changes\r\n- The pull request adds descriptions to all functions in the codebase. These descriptions provide a brief overview of the purpose of each function, its input paramete",
    "pr_number": 2715,
    "comments": [
      "@Torantulino this PR is ready to merge.",
      "> Still way too verbose. An extra line of text that doesn't add value is just clutter. If the name of a function and the names of the parameters already make obvious what it does, it doesn't need a docstring that is basically a copy of the function name.\r\n> \r\n> Target: try to achieve the informative goals while adding as few lines as possible.\r\n\r\nThank you for your feedback on my recent changes to the project. I understand your concerns about the docstrings being too verbose and cluttered, and I appreciate your suggestion to achieve the informative goals while adding as few lines as possible.\r\n\r\nHowever, I strongly believe that these changes are necessary for the project's maintainability and ease of use. While some of the docstrings may have been redundant, I aimed to provide as much context and information as possible to help other developers understand the purpose and functionality of each function.\r\n\r\nThat being said, I will review the docstrings and refactor them where possible to achieve a better balance between clarity and conciseness. I would also like to emphasize that this pull request is important and should be merged as soon as possible to ensure that the project remains organized and well-documented.\r\n\r\nOnce again, thank you for your feedback, and please let me know if you have any further concerns or suggestions.",
      "@Pwuts changes suggested were done."
    ],
    "num_comments": 3,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 38299,
    "code_diff": "diff --git a/tests/browse_tests.py b/tests/browse_tests.py\nindex f896e7dd751b..90692d8853a7 100644\n--- a/tests/browse_tests.py\n+++ b/tests/browse_tests.py\n@@ -10,7 +10,10 @@\n \n \n class TestBrowseLinks(unittest.TestCase):\n+    \"\"\"Unit tests for the browse module functions that extract hyperlinks.\"\"\"\n"
  },
  {
    "pr_title": "feat: set max token limits for better user experience",
    "pr_body": "<!-- ‚ö†Ô∏è At the moment any non-essential commands are not being merged.\r\nIf you want to add non-essential commands to Auto-GPT, please create a plugin instead.\r\nWe are expecting to ship plugin support within the week (PR #757).\r\nResources:\r\n* https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template\r\n-->\r\n\r\n<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to",
    "pr_number": 4128,
    "comments": [
      "Shouldnt you also remove the variable from the .env template? I might be wrong",
      "> Shouldnt you also remove the variable from the .env template? I might be wrong\r\n\r\nI'm still letting people set custom limits in case they want to, I just make sure those custom limits are below a working threshold",
      "FWIW, I actually exposed these limits via the command interface, so that the LLM can dynamically modify these once an error occurs.\r\n\r\nThe background being that I ran  into a bunch of errors where the hard-coded limits were not right, or where the hard-coded model was not working - exposing these details via the command interface (or even just via corresponding args) lets the AI/LLM play with different settings.\r\n\r\nIn my case it would figure out what to do over the course of the next 3-4 iterations - as long as you ensure to pass any error messages to the LLM and also pass in the command/arg to modify, as well as any hard-coded options.\r\n\r\n(by extension we might even want to expose the endpoint and the URL to the API schema and if the hash changes ...)",
      "in the light of the ongoing work to support multiple LLMs, it might make sense, to directly use a sub-dict named \"openai\" for these defaults so that other LLMs can be more easily supported in the future ?\r\n\r\nAlso, like I said previously, exposing these via the command interface enables the llm to tinker with different settings if/when an error occurs - all that is needed is capturing the error via a system message that includes the commands/arguments that can be used to adapt these settings. ",
      "> in the light of the ongoing work to support multiple LLMs, it might make sense, to directly use a sub-dict named \"openai\" for these defaults ?\r\n> \r\n> Also, like I said previously, exposing these via the command interface enables the llm to tinker with different settings if/when an error occurs - all that is needed is capturing the error via a system message that includes the commands/arguments that can be used to adapt these settings.\r\n\r\nCorrect me if I'm wrong but your way sounds like a better solution and is more future proof, if that's the case then I'm okay with closing this.\r\n\r\n@ntindle if you want me to make the changes you suggested then I'll go ahead and do so but if this is going to be closed due to a better solution that's around the corner or close then I'm okay with letting this die lol",
      "I would suggest to get this integrated with ntindle at this point, we will need to wait for their feedback to see if they're supportive of the idea or not - with your current suggestion, at least you already got some feedback.\r\n\r\nAlso, some folks expressed concerns if hard-coded defaults are exposed to the LLM for tinkering, so what's probably going to happen is that someone is going to say we need to make this configurable, too  ",
      "Ahh im failing the import tests, I'll fixie them this evening",
      "So I'm not a fan of this change https://github.com/Significant-Gravitas/Auto-GPT/pull/4128/commits/31a3a477db4fb98464f5a8152a8dfc393ca57353 but I wasn't sure the best way to avoid a circular dependency without having to restructure things and since I'm new and wanting to keep this bitesized, I went for the above solution.\r\n\r\nBut please lemme know if there's an easy way to avoid this problem (I'm new to python and the codebase so I could be missing something), thanks!",
      "Looks like tests are failing but not for my changes:\r\n![Screen Shot 2023-05-16 at 1 25 11 PM](https://github.com/Significant-Gravitas/Auto-GPT/assets/1424113/3a8fc5a4-157a-4f05-9491-8c2d97cae7f8)\r\n\r\n![Screen Shot 2023-05-16 at 1 24 41 PM](https://github.com/Significant-Gravitas/Auto-GPT/assets/1424113/65727884-3e9d-47bf-b9bd-4a39edc4c4f6)\r\n",
      "I need to update some of the tests to account for the new soft limits and fix some linter formatting issues, I'll probably get to it before the end of the weekend ",
      "Whoops sorry for updating, didn't realize you just pushed",
      "@ntindle sorry for all the forced push commits, I couldn't run the same commands in CI as i could Locally and figured it would be faster to just git force push for meow versus attempting to run tests locally atm lol",
      "Don't forget to fill out the PR, I think our automaton uses it for change logs",
      "Ill fill out the description later today thanks for the review!",
      "Fixed, thanks for the heads up @k-boikov ",
      "many people on discord still report that they find having to downgrade the hard-coded defaults to stay within the ctx window - so it would be good to get this updated/reviewed",
      "@Boostrix sounds good, i fixed the conflicts to include soft limits on top of the recent memory config changes",
      "@Pwuts gotcha thanks for the info, i like those suggestions",
      "![image](https://github.com/Significant-Gravitas/Auto-GPT/assets/64261260/6c8d58c2-f8e9-4618-89e3-8a9b5fa844ca)\r\n",
      "I'll make suggested changes today and fix up the tests",
      "@Pwuts I went ahead and removed both max token settings from the config file and now set them at runtime wherever it was used",
      "> @k-boikov  What if people want to intentionally limit the tokens they spend? Sometimes that helps when we have a bug and we don't calculate the limits correctly. The only limit I think we should enforce if someone tries to play with their env and puts a limit higher than the model's max tokens.\r\n\r\nthat's a fair point. I don't mind either solution we end up going with since it solves the major problem (mismanaged token limits).\r\n\r\nAt my day job I'd ping a project manager and ask for their 2 cents and decision, is that @ntindle in this case lol?"
    ],
    "num_comments": 22,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 0,
    "code_diff": ""
  },
  {
    "pr_title": "Fix memory by adding it only when context window full",
    "pr_body": "<!-- ‚ö†Ô∏è At the moment any non-essential commands are not being merged.\r\nIf you want to add non-essential commands to Auto-GPT, please create a plugin instead.\r\nWe are expecting to ship plugin support within the week (PR #757).\r\nResources:\r\n* https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template\r\n-->\r\n\r\n<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to",
    "pr_number": 3469,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 30066,
    "code_diff": "diff --git a/autogpt/agent/agent.py b/autogpt/agent/agent.py\nindex 9e0f54e36b55..a049b9d2f137 100644\n--- a/autogpt/agent/agent.py\n+++ b/autogpt/agent/agent.py\n@@ -3,7 +3,7 @@\n from autogpt.app import execute_command, get_command\n from autogpt.config import Config\n from autogpt.json_utils.json_fix_ll"
  },
  {
    "pr_title": "feat(backend): Pyro to FastAPI migration for micro service",
    "pr_body": "Due to legacy reasons, we've been using Pyro for our inter-process communication channel. While it fulfilled our initial needs, there were a few limitations that have been encountered:\r\n* Each connection will reserve 1 thread, when the thread is running out there will be no connection being accepted by the service.\r\n* Lack of asynchronous execution mode, we are locked in the sync execution which ended up wasting the I/O bound workload. Moving away from this will unlock async execution support fo",
    "pr_number": 9508,
    "comments": [
      "I'm not sure I love the idea of making a endpoint for all these -- this seems like a massive security challenge ",
      "@ntindle this has been a network accessible API endpoints all along. We are just now using http friendly json format.\r\n\r\nWhat massive security challenge are you referring to?",
      "My b I thought this was adding them to our current one -- why this over rabbitmq?",
      "This is orthogonal to message queue, we will move away from Redis to RabbitMQ. This is for the synchronous RPC. We want to eventually unlock async execution for our block executor hence we can't stick with Pyro.\r\n\r\nI was considering gRPC but it requires a lot more changes, like defining proto and its signature. Somehow I managed to auto generate endpoint routes using fast API to behave as close as the current Pyro implementation. I'll improve this overtime:\r\n* Support async client\r\n* Support async run method on block\r\n* Support parallel execution on blocks, etc",
      "Sorry I was talking about this system when asking why not rabbit mq \n\nhttps://www.rabbitmq.com/tutorials/tutorial-six-python\n\nhttps://docs.aio-pika.com/rabbitmq-tutorial/6-rpc.html",
      "@ntindle so your recommendation is to replace all of our synchronous RPCs with async ones through message queues?\r\n\r\nImagine this existing scenario:\r\n\r\na service `X` needs to do a process that requires:\r\n* get an info from service `Y`\r\n* get an info from service `Z`\r\n* process the info on the existing service\r\n* Publish the result to service `Z`\r\n\r\nOn the sync system, this will be done like this:\r\n```\r\ninfo_1 = serviceY.get_info()\r\ninfo_2 = serviceZ.get_info()\r\nprocessed_info = do_local_stuff(info_1, info_2)\r\nserviceZ.publish_info(processed_info)\r\n```\r\n\r\nOn a purely asynchronous RPC system, how do you imagine this going to look like? I don't see this to be possible unless we somehow overhaul the system to a purely event-driven system. While this is interesting, I still can't figure out whether this is feasible at all at the moment.\r\n\r\nAlternatively, assuming we managed to create a wrapper using Pika that allows us to make the async call behave synchronously through the message queue, how is it going to be? \r\n\r\nLet's say we are doing the `info_1 = serviceY.get_info()` part.  You will need two queues, the server one and the callback one. The server part will just consume the request and publish the result with the correlation ID. What should the client do after publishing the request message? should it wait and spawn another task that listens to the callback queue, filters out the matching correlation ID, and notifies and wakes the waiting caller to continue the process? If so, how does the calling task & subscriber task communicate?",
      "To be clear, not my recommendation , but asking why not. This isn't something I'm super familiar with so trying to learn\n\nBut for your last part\n\n```python \n\nimport asyncio\nimport uuid\nfrom typing import MutableMapping\n\nfrom aio_pika import Message, connect\nfrom aio_pika.abc import (\n    AbstractChannel, AbstractConnection, AbstractIncomingMessage, AbstractQueue,\n)\n\n\nclass FibonacciRpcClient:\n    connection: AbstractConnection\n    channel: AbstractChannel\n    callback_queue: AbstractQueue\n\n    def __init__(self) -> None:\n        self.futures: MutableMapping[str, asyncio.Future] = {}\n\n    async def connect(self) -> \"FibonacciRpcClient\":\n        self.connection = await connect(\"amqp://guest:guest@localhost/\")\n        self.channel = await self.connection.channel()\n        self.callback_queue = await self.channel.declare_queue(exclusive=True)\n        await self.callback_queue.consume(self.on_response, no_ack=True)\n\n        return self\n\n    async def on_response(self, message: AbstractIncomingMessage) -> None:\n        if message.correlation_id is None:\n            print(f\"Bad message {message!r}\")\n            return\n\n        future: asyncio.Future = self.futures.pop(message.correlation_id)\n        future.set_result(message.body)\n\n    async def call(self, n: int) -> int:\n        correlation_id = str(uuid.uuid4())\n        loop = asyncio.get_running_loop()\n        future = loop.create_future()\n\n        self.futures[correlation_id] = future\n\n        await self.channel.default_exchange.publish(\n            Message(\n                str(n).encode(),\n                content_type=\"text/plain\",\n                correlation_id=correlation_id,\n                reply_to=self.callback_queue.name,\n            ),\n            routing_key=\"rpc_queue\",\n        )\n\n        return int(await future)\n\n\nasync def main() -> None:\n    fibonacci_rpc = await FibonacciRpcClient().connect()\n    print(\" [x] Requesting fib(30)\")\n    response = await fibonacci_rpc.call(30)\n    print(f\" [.] Got {response!r}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
      "Acknowledged. Yes, the reply is a discussion.\r\n\r\nThe approach above will push the responsibility of the event listening to be the same event loop & thread as the caller, which implies:\r\n* Each caller will create its own callback queue\r\n* Each caller will hold and manage two continuous connections to RabbitMQ (callback & server).\r\n* Each caller that calls X different services, on its event loop will subscribe to 2*X different queues while holding 2*X connections.\r\n\r\nOne reason we are moving away from Pyro to a stateless REST API as I stated in the description is the limited connection we can provision for a service. Considering we will do this on each of our graph/node executor instances, those executors/processes will hold their own 2*X connections & queues for their service. Is this within the limitation of the MQ?",
      "Also, this is code implementation specific, but one challenge I see is this can only be achieved when the code is fully async.  The existing sync code needs to be converted into async code directly otherwise it will require threads since we are listening to multiple events while doing the client stuff at the same time.",
      "https://www.rabbitmq.com/docs/connections\n\nThis makes it sound like you're hitting OS exhaustion limits before rabbitmq limit",
      "Okay yeah I see what you're saying. Here's the sync docs btw\n\n```python\n\n#!/usr/bin/env python\nimport pika\nimport uuid\n\n\nclass FibonacciRpcClient(object):\n\n    def __init__(self):\n        self.connection = pika.BlockingConnection(\n            pika.ConnectionParameters(host='localhost'))\n\n        self.channel = self.connection.channel()\n\n        result = self.channel.queue_declare(queue='', exclusive=True)\n        self.callback_queue = result.method.queue\n\n        self.channel.basic_consume(\n            queue=self.callback_queue,\n            on_message_callback=self.on_response,\n            auto_ack=True)\n\n        self.response = None\n        self.corr_id = None\n\n    def on_response(self, ch, method, props, body):\n        if self.corr_id == props.correlation_id:\n            self.response = body\n\n    def call(self, n):\n        self.response = None\n        self.corr_id = str(uuid.uuid4())\n        self.channel.basic_publish(\n            exchange='',\n            routing_key='rpc_queue',\n            properties=pika.BasicProperties(\n                reply_to=self.callback_queue,\n                correlation_id=self.corr_id,\n            ),\n            body=str(n))\n        while self.response is None:\n            self.connection.process_data_events(time_limit=None)\n        return int(self.response)\n\n\nfibonacci_rpc = FibonacciRpcClient()\n\nprint(\" [x] Requesting fib(30)\")\nresponse = fibonacci_rpc.call(30)\nprint(f\" [.] Got {response}\")\n\n```",
      "> Hitting OS exhaustion\r\n\r\nWhich OS and what part is exhausted?\r\nWe are talking about the centralized message queue vs peer-to-peer HTTP calls between services.",
      "Number of open connections\n\n>  Operating systems have a [limit around how many TCP connections (sockets) a single process can have open simultaneously](https://www.rabbitmq.com/docs/networking#open-file-handle-limit). The limit is often sufficient for development and some QA environments. [Production](https://www.rabbitmq.com/docs/production-checklist) environments must be configured to use a higher limit in order to support a larger number of concurrent client connections.\n\nIn that first link\n\n> Basic Estimates of the Necessary Limit\n> \n> When optimising for the number of concurrent connections, make sure your system has enough file descriptors to support not only client connections but also files the node may use. To calculate a ballpark limit, multiply the number of connections per node by 1.5. For example, to support 100,000 connections, set the limit to 150,000.",
      "Right as you can see from the code above, you either need to poll the system to wait for the matching correlation ID messages to match or spawn another thread to do the consuming. Now imagine you need to call two services as the scenario above. How are you going to handle it without blocking each other on synchronous code.",
      "Why would you not use an async client in that case and await both at the same time so they can both process?",
      "That's the whole point, our code needs to be fully async, you can't easily introduce this and expect the code will just work.",
      "And our code isn't fully async so this would be a bad thing to try?",
      "Definitely not a bad thing, it's just not yet possible without making our code do an async-first system. Hopefully by doing this incrementally we can move toward a fully async system.\r\n\r\nIt's an interesting option to consider indeed, I still need to consider a few logistics though:\r\n* Does a sync RPC using MQ causes more latency, and even if it is, is it a deal breaker difference.\r\n* Is it still trivial to handle timeout & retry using this pattern.\r\n* Can a single point of failure on a message queue cause a scalability risk or is it easy (or even needed) to shard the MQ.\r\n* Does this pattern give more benefit than a direct API calls.\r\n\r\nOne benefit I see is auditing since we are logging all interaction placed in a single location, [event sourcing](https://microservices.io/patterns/data/event-sourcing.html) pattern used to be cool.",
      "> One question though: Uvicorn's default timeout is 60 seconds. AFAIK Pyro doesn't have that. Do we have any slow RPC's that are at risk of exceeding the 60 second timeout imposed by Uvicorn?\r\n\r\nNot so far, something good that unintentionally introduced by this change :D \r\n\r\n> Also, please fill out the PR checklist as there are major system and config changes.\r\n\r\nFilled."
    ],
    "num_comments": 19,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 19776,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/executor/database.py b/autogpt_platform/backend/backend/executor/database.py\nindex 1dee046ccca3..0834d96c299d 100644\n--- a/autogpt_platform/backend/backend/executor/database.py\n+++ b/autogpt_platform/backend/backend/executor/database.py\n@@ -1,6 +1,3 @@\n-"
  },
  {
    "pr_title": "Implement Local Cache and Redis Memory backend",
    "pr_body": "### Background\r\n\r\nRedis is a free and open source alternative to pinecone.\r\n\r\nWe had a dicussion here: https://discord.com/channels/1092243196446249134/1093713674679615638\r\n\r\n\r\n### Changes\r\n\r\nThe memory backend was given an abstract base class.\r\n\r\nPineconeMemory now inherits from this ABC.\r\n\r\nRedisMemory was added.\r\n\r\nConfiguration options were added to the Config class.\r\n\r\n### Test Plan\r\n\r\nI had SuperMario in Discord verify I did not break pinecone, I tested redis locally.\r\n\r\n### Change Safety\r",
    "pr_number": 372,
    "comments": [
      "I should note that there is no reason why we have to wipe out the memory, unless the intent is to reset state.\r\n\r\nWe could easily save the memory store's state on exit and load on start, or even per add() call.",
      "In redis this is simple so I went ahead and persisted the vec_num to redis and added an option to wipe on startup as the default behavior, optionally allowing users to keep their state.",
      "> In redis this is simple so I went ahead and persisted the vec_num to redis and added an option to wipe on startup as the default behavior, optionally allowing users to keep their state.\r\n\r\n:+1: \r\nAllowing for persistence is a great idea for any long running jobs. CloudFlare frequently throws a 401 finishing the session. Warm restarts would need a steady state.",
      "This now implements a local version using numpy.",
      "It is also set to the default, orjson was added as a requirement.",
      "This now avoids import errors for both redis and pinecone. It is possible to remove them from requirements.txt.",
      "Hi, I see that both you and @cs0lar in #424 work on a memory backend/factory/interface. Great stuff! I'd suggest having a look into creating a strong combined implementation.\r\n\r\nI think @cs0lar's implementation is a bit neater and simpler, and has tests. You, @BillSchumacher, have the two additional providers, Redis and local.\r\n\r\nThis memory interface is a good step üëçüèª Happy to help and work together here",
      "@BillSchumacher LocalCache is crusing rn. no major errors. ~5 min of run time. Starting Redis smoke test.",
      "@BillSchumacher Redis - Pass\r\n@all\r\n\r\nof all the memory types, in my experience, redis doens't bork segmentation\r\n![image](https://user-images.githubusercontent.com/17409586/230697157-bb98b2aa-bf5e-434d-b4ed-ef9a3df458df.png)\r\n\r\nPC and LocalCache, dont do this well with teh stock entreprenur persona & goals",
      "> Hi, I see that both you and @cs0lar in #424 work on a memory backend/factory/interface. Great stuff! I'd suggest having a look into creating a strong combined implementation.\r\n> \r\n> I think @cs0lar's implementation is a bit neater and simpler, and has tests. You, @BillSchumacher, have the two additional providers, Redis and local.\r\n> \r\n> This memory interface is a good step üëçüèª Happy to help and work together here\r\n\r\nI'm not sure that having a class for a single function makes sense, if other people like that I'm good with whatever.\r\n\r\nThe tests are a start but we probably need some CI/CD to be able to really test.",
      "Taking a look at this. \r\nThank you for your contribution!",
      "> we probably need some CI/CD to be able to really test.\r\n\r\nA system for benchmarking and Testing is being worked on, on it's way soon.\r\n\r\n",
      "Could we make the local cache default. ",
      "> Could we make the local cache default.\r\n\r\nI did set the local cache as default.",
      "https://github.com/Torantulino/Auto-GPT/pull/146\r\n\r\nI also implement redis.",
      "> #146\r\n> \r\n> I also implement redis.\r\n\r\nThere's a lot of good stuff in there.",
      "Working on a hybrid of our PRs. Gimme a bit.",
      "I'm getting this error a lot:\r\n\r\nError calling Redis search:  unknown command 'FT.SEARCH', with args beginning with: 'auto-gpt' '*=>[KNN 10 @embedding $vector AS vector_score]' 'RETURN' '2' 'data' 'vector_score' 'SORTBY' 'vector_score' 'ASC' 'DIA'",
      "I think docker-compose would be nice to help streamline the use of redis, sure a local container can be run all by itself, but one command is better than two! Combine a redis container with the existing dockerfile for auto-gpt.\r\n```\r\nversion: \"3\"\r\n\r\nservices:\r\n  redis:\r\n    restart: always\r\n    image: redis:latest\r\n    networks:\r\n      - main\r\n    ports:\r\n      - \"6379:6379/tcp\"\r\n auto-gpt:\r\n    restart: always\r\n    build: ./\r\n    networks:\r\n      - main\r\n    volumes:\r\n      - ./auto_gpt_workspace:/app/auto_gpt_workspace\r\n    env_file: .env\r\n\r\nnetworks:\r\n  main:\r\n```",
      "> I'm getting this error a lot:\r\n> \r\n> Error calling Redis search: unknown command 'FT.SEARCH', with args beginning with: 'auto-gpt' '*=>[KNN 10 @Embedding $vector AS vector_score]' 'RETURN' '2' 'data' 'vector_score' 'SORTBY' 'vector_score' 'ASC' 'DIA'\r\n\r\nMake sure you're running a redis-stack server, and not just redis. the FT.* commands are part of the extended data model that stack adds.",
      "> > I'm getting this error a lot:\r\n> > Error calling Redis search: unknown command 'FT.SEARCH', with args beginning with: 'auto-gpt' '*=>[KNN 10 @Embedding $vector AS vector_score]' 'RETURN' '2' 'data' 'vector_score' 'SORTBY' 'vector_score' 'ASC' 'DIA'\r\n> \r\n> Make sure you're running a redis-stack server, and not just redis. the FT.* commands are part of the extended data model that stack adds.\r\n\r\nYeah otherwise you have to compile it with that functionality.",
      "@Torantulino - usage of @pinecone-io has really spiked. I'd recommend we merge this to avoid disruptions to auto-gpt and Pinecone users.",
      "Looking into merging this today.",
      "@Torantulino once you merge this then I'll merge my combo PR back into my qa pr to handle the merge conflict produced, and we will be back in buisness.",
      "Running this causes almost all of AutoGPT outputs to not be properly formatted as JSON for me.\r\n\r\nAnyone else experiencing this problem?\r\n",
      "> Running this causes almost all of AutoGPT outputs to not be properly formatted as JSON for me.\r\n> \r\n> Anyone else experiencing this problem?\r\n\r\nI'm not sure the JSON errors are restricted to this branch, the main thing I'm seeing from a local model is just bad output from the model.\r\n\r\nrelevant memory [[Document {'id': 'bob:0', 'payload': None, 'vector_score': '0.156416118145', 'data': 'Assistant Reply: Understood. My first task is \"Search Files\" for python scripts. \\nResult: Command Error: returned: Unknown command Error: \\nHuman Feedback: GENERATE NEXT COMMAND JSON '}]]\r\n\r\nIs what's coming back from relevent memory.",
      "I can see in the readme you mentioned instructions on how to make Redis memory persist between runs, however using your \"LocalCache\" version, memory is persisting for me by default. \r\n\r\nHow do users configure this?",
      "> Running this causes almost all of AutoGPT outputs to not be properly formatted as JSON for me.\r\n> \r\n> Anyone else experiencing this problem?\r\n\r\n        return [result.data for result in results.docs]\r\n\r\nMight be better",
      "Yeah, not saving the bad responses and returning the data seems to be working better for me."
    ],
    "num_comments": 29,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 16066,
    "code_diff": "diff --git a/README.md b/README.md\nindex 5dc477e31f34..760e62cf5962 100644\n--- a/README.md\n+++ b/README.md\n@@ -141,6 +141,39 @@ export CUSTOM_SEARCH_ENGINE_ID=\"YOUR_CUSTOM_SEARCH_ENGINE_ID\"\n \n ```\n \n+## Redis Setup\n+\n+Install docker desktop.\n+\n+Run:\n+```\n+docker run -d --name redis-stack-server -p 6"
  },
  {
    "pr_title": "feat(rnd): Reduce container size remove dep with forge and autogpt",
    "pr_body": "### Background\r\n\r\nBreak dependancy with forge and autogpt\r\nReduce built image size from 2.77GB to 400MB\r\n",
    "pr_number": 8040,
    "comments": [
      " thanks for this - have you confirmed with a fresh build that everything works as expected, for bother docker compose users and outside of users? "
    ],
    "num_comments": 1,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 755439,
    "code_diff": "diff --git a/rnd/autogpt_libs/autogpt_libs/auth/depends_tests.py b/rnd/autogpt_libs/autogpt_libs/auth/depends_tests.py\nindex 8e2c10d1274b..b6ac4d2a7bd9 100644\n--- a/rnd/autogpt_libs/autogpt_libs/auth/depends_tests.py\n+++ b/rnd/autogpt_libs/autogpt_libs/auth/depends_tests.py\n@@ -1,6 +1,6 @@\n import p"
  },
  {
    "pr_title": "Use `async`/`await` where applicable",
    "pr_body": "### Background\r\n<!-- Provide a concise overview of the rationale behind this change. Include relevant context, prior discussions, or links to related issues. Ensure that the change aligns with the project's overall direction. -->\r\nNeeded for #386 @itaim\r\nNeeded for #1139 @merwanehamadi \r\nNeeded for #1937 @arrenv\r\nNeeded for #1243\r\nNeeded for #2442 @SwiftyTheCoder \r\n\r\n### Changes\r\n<!-- Describe the specific, focused change made in this pull request. Detail the modifications clearly and avoid any ",
    "pr_number": 1245,
    "comments": [
      "Sounds great, I think it will also fix my issue where agents start talking asynchronously for some reason.\r\nI hear them talking word for word in a very weird and gibberish way",
      "@Androbin There are conflicts now",
      "Can we put a pin in this one? There's another async PR that I'd prefer to have merged.\r\n\r\nhttps://github.com/Significant-Gravitas/Auto-GPT/pull/2442\r\n\r\nSorry, super-awkward when this happens. I've put a wontfix label on this one for now.\r\n\r\nTeam is working on a mechanism whereby PRs need to be created as an issue that then is referenced by the PR, so we can avoid dups.\r\n\r\nIf there's a reason for promoting THIS PR, please do say!",
      "@p-i- I don't see how this PR conflicts with #2442. It does not change any structure, all it does is replace `.create` with `.acreate` and propagate `async` and `await` through the caller hierarchy. Any PR seeking to introduce async code has to do exactly that. If #2442 ends up reimplementing some of these functions, it still needs to add `async` and `await` everywhere else."
    ],
    "num_comments": 4,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 238700,
    "code_diff": "diff --git a/autogpt/__main__.py b/autogpt/__main__.py\nindex 128f9eea4900..468050bd3ad7 100644\n--- a/autogpt/__main__.py\n+++ b/autogpt/__main__.py\n@@ -1,5 +1,7 @@\n \"\"\"Auto-GPT: A GPT powered AI Assistant\"\"\"\n+import asyncio\n+\n import autogpt.cli\n \n if __name__ == \"__main__\":\n-    autogpt.cli.main()\n+"
  },
  {
    "pr_title": "Feat(Otto) Add Initial Otto Chat Bot code",
    "pr_body": "### Changes üèóÔ∏è\r\nThis is to add the Otto chat bot to the frontend UI.\r\n\r\nThe changes i have made for this goes as follows:\r\n- I have made and added a basic chat UI widget to the build page, this will become the main UI for Otto, this will be getting updates in the future.\r\n- I have made an API route ``/api/otto/ask`` that will receive the message from the frontend and pass it to the Otto API that we have setup on GCP.\r\n\r\nI already have plans for how we can improve the UI and make it more modular,",
    "pr_number": 9266,
    "comments": [
      "This is now ready for a review as the backend API is up and running, to who ever does test this, if you want the API url please DM me üòÑ ",
      "The PR appears to be mostly valid but has some issues with the template compliance. The PR adds a significant feature (Otto chatbot) with both frontend and backend components, and properly handles user_id in the backend implementation. However, the PR template is not completely filled out - specifically the checklist items for code changes are not marked and no test plan is provided.",
      "The PR appears to be well structured and follows most guidelines, but is missing a few key elements. The PR title has a scope and follows conventional commit format. Changes are clearly documented. Test plan is present. User_id is properly handled in the backend code. However, the configuration checklist section from the template is completely missing rather than marked NA or removed, and should be filled out since there are environment variable changes needed.",
      "The PR appears to be well structured with several good practices, but has some issues that need attention: 1) The PR template checklist is missing configuration changes section which should be filled since a new ENV variable is being added (otto_api_url) 2) Though they have a test plan, it could be more comprehensive to include error cases and authentication testing 3) The code changes properly handle user_id in the backend routes which is good"
    ],
    "num_comments": 4,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 24367,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/server/rest_api.py b/autogpt_platform/backend/backend/server/rest_api.py\nindex 02c1e6d328f3..d4f85bf9cb27 100644\n--- a/autogpt_platform/backend/backend/server/rest_api.py\n+++ b/autogpt_platform/backend/backend/server/rest_api.py\n@@ -21,6 +21,7 @@\n import"
  },
  {
    "pr_title": "feat(backend): schema updates, migration, queries for Email Notification Service",
    "pr_body": "<!-- Clearly explain the need for these changes: -->\r\n\r\nThe email service has requirements to\r\n- Email users when some activity has happened on their account on some scheduled basis -> We need a way to get active users and the executions that happened while they were active\r\n- Allow users to configure what emails they get -> Need a user preference\r\n- Get User email by Id so that we can email them -> Pretty self-explanatory\r\n\r\nWe need to add a few new backend queries + db models for the notificat",
    "pr_number": 9445,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 30465,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/data/execution.py b/autogpt_platform/backend/backend/data/execution.py\nindex eb39e8c54d8d..f9bbccf864bb 100644\n--- a/autogpt_platform/backend/backend/data/execution.py\n+++ b/autogpt_platform/backend/backend/data/execution.py\n@@ -15,6 +15,7 @@\n from backe"
  },
  {
    "pr_title": "AutoGPT: Implement Agent Protocol",
    "pr_body": "### Background\r\n* AutoGPT should support the Agent Protocol\r\n* AutoGPT is basically an agent factory, and the architecture could be better adjusted to that\r\n\r\n### Changes üèóÔ∏è\r\n\r\n- Add `AgentProtocolServer`\r\n  - Add `serve` subcommand to run Agent Protocol server\r\n  - Add `agent_factory` module to easily generate, create and configure agent instances\r\n- Replace AI Goals by AI Directives + Task\r\n  - Move directives into `BaseAgentSettings`\r\n- Create `AgentFileManager`, separately from `Workspace`\r\n",
    "pr_number": 5612,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 418307,
    "code_diff": "diff --git a/.github/workflows/autogpt-ci.yml b/.github/workflows/autogpt-ci.yml\nindex 056431c08407..7ceb478551b2 100644\n--- a/.github/workflows/autogpt-ci.yml\n+++ b/.github/workflows/autogpt-ci.yml\n@@ -6,19 +6,16 @@ on:\n     paths:\n       - 'autogpts/autogpt/**'\n       - '!autogpts/autogpt/tests/vc"
  },
  {
    "pr_title": "feat(platform): Implement top-up flow for PAYG System",
    "pr_body": "This PR adds Stripe integration and payment processing for topping-up user accounts with credits.\r\n\r\n### Changes üèóÔ∏è\r\n\r\nIncludes:\r\n- https://github.com/Significant-Gravitas/AutoGPT/pull/9176\r\n\r\n#### Top-up flow\r\n\r\n1. To top-up a user visits their settings and clicks `Credits` button (it's unavailable if `NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY` isn't present)\r\n2. User inputs top-up amount (min 5$ in 1$ increments) and click the button to confirm.\r\n3. Backend receives top-up request, creates database e",
    "pr_number": 9050,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 49486,
    "code_diff": "diff --git a/autogpt_platform/backend/.env.example b/autogpt_platform/backend/.env.example\nindex 75b0daf87d9f..ab7b914a73b5 100644\n--- a/autogpt_platform/backend/.env.example\n+++ b/autogpt_platform/backend/.env.example\n@@ -15,6 +15,9 @@ REDIS_PORT=6379\n REDIS_PASSWORD=password\n \n ENABLE_CREDIT=false"
  },
  {
    "pr_title": "tweak(frontend): Update Blocks UI",
    "pr_body": "### Background\r\n\r\nThe current agent flows are engineering prototypes. For the beta we are doing a refresh with rapid fire design tweaks by a pro.\r\n\r\nThis PR aims to implement our designers work on the blocks ui\r\n\r\n### Changes üèóÔ∏è\r\n\r\n- Update Block UI\r\n- Added Context Menu for blocks\r\n- Updated Block Connecting line to terminate at handles\r\n- Update Tutorial and Tally Buttons\r\n- Added pointer indicator in Block Control Panel\r\n\r\nhttps://github.com/user-attachments/assets/4fdbe372-a407-44c3-a55e-f7c",
    "pr_number": 8190,
    "comments": [
      "Thanks for the review. I've got a very good eye! I've tried to answer all your comments with what I was thinking. I'll make sure to check these types of things in future.\n\nI'm keep on integrating storybook as it's easier to iterate on just the look of components \n\n> The block on the list don't have hover cursor-pointer, it looks unclickable.\n\nMy second to last commit on this PR added it, or at least I thought it did",
      "[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/Significant-Gravitas/AutoGPT?pullRequest=8190) <br/>All committers have signed the CLA.",
      "@Pwuts did you test this or is this coming from the initial screenshots?\r\nI've been aligning this current PR from its initial state with the current design.\r\n\r\nCan you checkout the branch, test it, and see if those comments are still valid?\r\n\r\nFor 7 it was intentional. Discussion on the design can be done separately "
    ],
    "num_comments": 3,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 119720,
    "code_diff": "diff --git a/autogpt_platform/frontend/package.json b/autogpt_platform/frontend/package.json\nindex f9ba6ed2bf8e..ab18ece18f45 100644\n--- a/autogpt_platform/frontend/package.json\n+++ b/autogpt_platform/frontend/package.json\n@@ -23,6 +23,7 @@\n     \"@radix-ui/react-avatar\": \"^1.1.0\",\n     \"@radix-ui/re"
  },
  {
    "pr_title": "feat(server): backend analytics endpoints",
    "pr_body": "### Background\r\n\r\n<!-- Clearly explain the need for these changes: -->\r\n\r\nWe want a way to track pageviews, user signup, and tutorial progress.\r\n\r\n### Changes üèóÔ∏è\r\n\r\n<!-- Concisely describe all of the changes made in this pull request: -->\r\n\r\n- Adds the capability to log custom analytics\r\n- Separates out the autogpt API clients into a backend and frontend client (this is required for backend auth validation, previously this wasn't actually functional)\r\n- Track tutorial progress with google analyt",
    "pr_number": 8030,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 42236,
    "code_diff": "diff --git a/rnd/autogpt_builder/.env.example b/rnd/autogpt_builder/.env.example\nindex 92a7b542ea90..34309245e33d 100644\n--- a/rnd/autogpt_builder/.env.example\n+++ b/rnd/autogpt_builder/.env.example\n@@ -1,7 +1,7 @@\n NEXT_PUBLIC_AUTH_CALLBACK_URL=http://localhost:8006/auth/callback\n NEXT_PUBLIC_AGPT_"
  },
  {
    "pr_title": "Feature/weaviate memory",
    "pr_body": "### Background\r\n\r\nThis change allows the use of different vector-based memories for Auto-GPT as long as the `Memory` interface is implemented. As an example, this change integrates [Weaviate](https://weaviate.io/) as an alternative to Pinecone that can be instantiated locally.\r\n\r\n### Changes\r\n\r\n- We created a `providers` directory which is meant to hold the various memory implementations. \r\n- The original `scripts/memory.py` file has been moved into the `providers` module and it declares `Memory",
    "pr_number": 424,
    "comments": [
      "I have noticed that the latest commands list in commands.py no longer supports the memory-related command (`memory_del`, `memory_ovr`). In this pull request I have removed the associated handlers (delete_memory and overwrite_memory) as well as the original `commit_memory` in part because they no longer follow the memory interface. Let me know whether these memory commands should be back on the menu and I can implement them.",
      "Thank you so much I was actually working on a similar branch last week! üòÉ\r\n",
      "I have now merged the memory strategy from the master branch and added weaviate to the supported providers. I have also added tests for the weaviate integration.",
      "@nponeccop - applied all requests.",
      "@cs0lar what are your thoughts on adding support for [embedded weaviate](https://weaviate.io/developers/weaviate/installation/embedded) too as part of this PR?",
      "@hsm207 Thanks for spotting that. I have added weaviate embedded support now. Please check it out.",
      "@cs0lar question about the schema:\r\n\r\n```\r\n{\r\n    \"class\": \"Autogpt\",\r\n\r\n    },\r\n    \"properties\": [\r\n        {\r\n            \"dataType\": [\r\n                \"text\"\r\n            ],\r\n            \"description\": \"original text for the embedding\",\r\n            \"name\": \"raw_text\",\r\n            \"tokenization\": \"word\"\r\n        },\r\n        {\r\n            \"dataType\": [\r\n                \"text\"\r\n            ],\r\n            \"description\": \"This property was generated by Weaviate\\'s auto-schema feature on Wed Apr 12 16:53:47 2023\",\r\n            \"name\": \"class\",\r\n            \"tokenization\": \"word\"\r\n        }\r\n    ],\r\n   ...\r\n}'\r\n```\r\nis the property `class` intended? Asking because it was autogenerated.\r\n",
      "@cs0lar There are conflicts again due to the massive merge",
      "@cs0lar LGTM, but there are conflicts now due to merging of other PRs.",
      "@cs0lar I don't have anymore feedback other than my last comment about `weaviate_auth`. Great job!",
      "@cs0lar There are conflicts now though.",
      "@cs0lar There are conflicts now",
      "Hi, @nponeccop and @cs0lar - is there a way how we can coordinate the fix and merge? Otherwise, we might keep going in circles :)",
      "@bobvanluijt it would be great to know what's holding merging this branch (apart from the latest conflicts). If this functionality is not part of the plan it would be great to know sometime soon so we can move on from it. BTW honour to @nponeccop for relentlessly checking the PRs and conflict fixes!",
      "Run flake8 autogpt/ tests/ --select E303,W293,W291,W292,E305,E231,E302\r\nautogpt/memory/__init__.py:30:1: E302 expected 2 blank lines, found 1\r\nautogpt/memory/__init__.py:56:1: W293 blank line contains whitespace\r\nautogpt/memory/base.py:10:1: E302 expected 2 blank lines, found 1\r\nautogpt/memory/weaviate.py:9:1: E302 expected 2 blank lines, found 1\r\nautogpt/memory/weaviate.py:21:1: E302 expected 2 blank lines, found 1\r\nautogpt/memory/weaviate.py:76:5: E303 too many blank lines (2)\r\nautogpt/memory/weaviate.py:80:80: W291 trailing whitespace\r\ntests/integration/weaviate_memory_tests.py:14:1: E302 expected 2 blank lines, found 1\r\ntests/integration/weaviate_memory_tests.py:41:1: W293 blank line contains whitespace\r\ntests/integration/weaviate_memory_tests.py:47:1: W293 blank line contains whitespace\r\ntests/integration/weaviate_memory_tests.py:56:1: W293 blank line contains whitespace\r\ntests/integration/weaviate_memory_tests.py:70:1: W293 blank line contains whitespace\r\ntests/integration/weaviate_memory_tests.py:87:5: E303 too many blank lines (2)\r\ntests/integration/weaviate_memory_tests.py:102:5: E303 too many blank lines (2)",
      "@cs0lar There are conflicts again",
      "> @bobvanluijt it would be great to know what's holding merging this branch (apart from the latest conflicts). \r\n\r\nThanks, @nponeccop, I think it's the merging of other PRs. Seems we are almost there. Your help is appreciated in keeping this PR up-to-date üôè\r\n\r\nAlmost there, it seems: https://github.com/Significant-Gravitas/Auto-GPT/pull/424#issuecomment-1509865221\r\n\r\n> BTW honour to @nponeccop for relentlessly checking the PRs and conflict fixes!\r\n\r\nYup, thanks @nponeccop",
      "Awesome! I think the PR can be merged ü•≥",
      "@cs0lar, I think we need your help again. People on the Auto-GPT Discord are asking for this merge üòÇ",
      "Hi @nponeccop and @richbeales. I think the merge conflict is resolved. Would be awesome if you could review and merge. Thanks üôè",
      "Thanks for reaching out @bobvanluijt - we'll get to this shortly.  I'll ping you when ready.",
      "I think Milvus and Weaviate should also be appended here, right? https://github.com/Significant-Gravitas/Auto-GPT/blob/master/.env.template#L53\r\n\r\nWeaviate integration is mentioned in [its own section](https://github.com/Significant-Gravitas/Auto-GPT/blob/master/.env.template#L78), but Milvus doesn't have that mention and rather than being scathered all around the config, they should all be neatly gathered at the top (imho).",
      "üéâ thanks all for the help and merge üéâ \r\n\r\nAdded a minor change here: https://github.com/Significant-Gravitas/Auto-GPT/pull/1813\r\n\r\nBasically, so that people don't have to install the Weaviate client manually."
    ],
    "num_comments": 23,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 17596,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 008dd3d2c114..685ed19f4f47 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -74,6 +74,27 @@ REDIS_PASSWORD=\n WIPE_REDIS_ON_START=False\n MEMORY_INDEX=auto-gpt\n \n+### WEAVIATE\n+# MEMORY_BACKEND - Use 'weaviate' to use Weaviate vector storage\n+# WEAVIAT"
  },
  {
    "pr_title": "fix json parser ",
    "pr_body": "### Background\r\nUsing gpt-3.5 the responses contain some nonsense text around the json.\r\nThis is a messy fix, don't merge before it's clean.\r\nBut you can use these changes rn to get around the issue.\r\n\r\n### Changes\r\nadded another json clean function that finds the first and last bracket and also notifies the user via speech about the issue, if it happened, as this one is important to observe and I do not want it to be a final fix.\r\n\r\n### Documentation\r\n<!-- Explain how your changes are documente",
    "pr_number": 697,
    "comments": [
      "I will try to refactor this a little further into the already existing json cleaner method.\r\nSome users reported that they receive the same error elsewhere or the try try try is too much.",
      "If it's not ready yet you can mark it as a draft PR using GitHub",
      "After I download your file and replace original `main.py` I started getting the following error over and over again:\r\n```\r\nTraceback (most recent call last):\r\n  File \"Auto-GPT/scripts/main.py\", line 329, in <module>\r\n    assistant_reply = chat.chat_with_ai(\r\nTypeError: chat_with_ai() takes 5 positional arguments but 6 were given\r\n```\r\nI only wrote four goals. If I use exactly the same goals with the actual `git clone https://github.com/Torantulino/Auto-GPT.git` file, there is no error (except the JSON Error)",
      "> \r\n\r\ntry git pull on the master branch, the issue comes from chat.py, not my code.",
      "cfg.debug is one param too much for the chat method. not sure who changed it, as my git blame is broken in vscode",
      "Removing the branch made it rocksolid now.\r\nIt is creating a whole new project now and using elevenlabs.\r\nI told it to always confirm with the other agent and I hear them both discussing right now haha\r\n\r\nAn array from google seemed to have crashed it now.",
      "@Artemonim  try again. I think I resolved most issues now.",
      "> @Artemonim try again. I think I resolved most issues now.\r\n\r\nIs it a correct response?\r\nAuto-GPT just write `\\n`, not to use it\r\n```\r\nPLAN:\r\n-  Create a 'Math' directory.\\n- Create a 'workbook.markdown' file inside 'Math' directory.\r\n```",
      "well (input in Russian but I assume the same problem in English)\r\n```bash\r\nEnter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ—à–∞—Ç–µ–ª—å —Å –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º...        \r\nInput:–ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏ —Å–≤–æ–π –∑–∞–ø—Ä–æ—Å\r\nSYSTEM:  Human feedback: –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏ —Å–≤–æ–π –∑–∞–ø—Ä–æ—Å\r\nError: Invalid JSON in assistant thoughts\r\n –•–æ—Ä–æ—à–æ, –¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Ä–∞–±–æ—á—É—é –ø–∞–ø–∫—É –∏ —Ñ–∞–π–ª, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–π JSON –∑–∞–ø—Ä–æ—Å: ``` { \"thoughts\": { \"text\": \"–Ø –¥–æ–ª–∂–µ–Ω —Å–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É 'Math' –∏ —Ñ–∞–π–ª 'workbook.markdown'. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –º–Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å –∏ —Ö—Ä–∞–Ω–∏—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è. –Ø —Å–æ–±–∏—Ä–∞—é—Å—å —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–æ—Ä–º—É–ª—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ ASCII-–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞.\", \"reasoning\": \"–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –∏ —Ñ–∞–π–ª–∞ –ø–æ–∑–≤–æ–ª–∏—Ç –º–Ω–µ –ª–æ–≥–∏—á–µ—Å–∫–∏ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –º–æ–∏ —Ñ–∞–π–ª—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –º–Ω–µ –±—ã—Å—Ç—Ä–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –Ω—É–∂–Ω—ã–µ —Ñ–∞–π–ª—ã, –∞ —Ç–∞–∫–∂–µ –ª–µ–≥–∫–æ –¥–µ–ª–∏—Ç—å—Å—è –∏–º–∏ —Å –¥—Ä—É–≥–∏–º–∏.\", \"plan\": \"- –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É 'Math'. \\\\n- –í —ç—Ç–æ–π –ø–∞–ø–∫–µ —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª 'workbook.markdown'.\", \"criticism\": \"–Ø –¥–æ–ª–∂–µ–Ω –±—ã–ª –≤—ã–ø–æ–ª–Ω–∏—Ç—å —ç—Ç–æ —Ä–∞–Ω—å—à–µ.\", \"speak\": \"–Ø —Å–æ–∑–¥–∞–º –ø–∞–ø–∫—É 'Math' –∏ —Ñ–∞–π–ª 'workbook.markdown' –≤–Ω—É—Ç—Ä–∏ –Ω–µ–µ.\" }, \"command\": { \"name\": \"create_directory\", \"args\": { \"directory\": \"Math\" } } } ```\r\nAttempting to fix JSON by finding outermost brackets\r\n \r\nError: \r\n Traceback (most recent call last): File \"Auto-GPT/scripts/main.py\", line 95, in print_assistant_thoughts assistant_reply_json = fix_and_parse_json(assistant_reply) File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_parser.py\", line 38, in fix_and_parse_json json_str = correct_json(json_str) File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_utils.py\", line 125, in correct_json if balanced_str := balance_braces(json_str): File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\s\r\ncripts\\json_utils.py\", line 79, in balance_braces raise e File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_utils.py\", line 76, in balance_braces json.loa\r\nds(json_string) File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\json\\__init__.py\", line 357, in loads return _default_decoder.decode(s) File \"C:\\ProgramData\\minicond\r\na3\\envs\\py3.8\\lib\\json\\decoder.py\", line 337, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\json\\decode\r\nr.py\", line 355, in raw_decode raise JSONDecodeError(\"Expecting value\", s, err.value) from None json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char\r\n 0) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"Auto-GPT/scripts/main.py\", line 98, in print_assist\r\nant_thoughts assistant_reply_json = attempt_to_fix_json_by_finding_outermost_brackets(assistant_reply) File \"Auto-GPT/scripts/main.py\", line 68, in attempt_to_fix_j\r\nson_by_finding_outermost_brackets json_pattern = re.compile(r\"\\{(?:[^{}]|(?R))*\\}\") File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\re.py\", line 252, in compile retu\r\nrn _compile(pattern, flags) File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\re.py\", line 304, in _compile p = sre_compile.compile(pattern, flags) File \"C:\\ProgramDat\r\na\\miniconda3\\envs\\py3.8\\lib\\sre_compile.py\", line 764, in compile p = sre_parse.parse(p, flags) File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\sre_parse.py\", line 9\r\n48, in parse p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0) File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\sre_parse.py\", line 443, in _parse_sub itemsa\r\nppend(_parse(source, state, verbose, nested + 1, File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\sre_parse.py\", line 834, in _parse p = _parse_sub(source, state, sub\r\n_verbose, nested + 1) File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\sre_parse.py\", line 443, in _parse_sub itemsappend(_parse(source, state, verbose, nested + 1, F\r\nile \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\sre_parse.py\", line 823, in _parse raise source.error(\"unknown extension ?\" + char, re.error: unknown extension ?R at position 12\r\n```\r\nAfter that the program did not stop and asks for confirmation",
      "@Artemonim The printed error is intentional.\r\ncould you check what the json looks like?\r\n+? inside a json makes no sense.\r\nIt has nothing to do with that code.\r\n\r\nAlso don't forget to pull the latest branch of mine for testing.\r\nif you keep testing on the main branch, then of course it won't work haha\r\n\r\nTalking of the devil, I got the same error now, regex is off, one sec.",
      "> don't forget to pull the latest branch of mine for testing\r\n\r\nI use Github Desktop + PyCharm 2022.1.3 with Powershell 7.2.10 as terminal\r\n![2023-04-10_23-55-41](https://user-images.githubusercontent.com/36363466/230997257-35d0c81f-0a88-4224-af43-2a69128932bd.png)\r\n\r\nAnd I'm ready to be your tester this hour üôÇ",
      "@nponeccop maybe you should test the code before you give your approval to something that doesn't work?",
      "I only approve formatting, whitespace and being focused. @Torantulino is who accepts it or not. I don't even know Python.",
      "![image](https://user-images.githubusercontent.com/13220320/231000452-cd639956-f474-4d98-9a14-9529b4c1958d.png)\r\n\r\nSome weirdness is going on, it says that it fixed it, but I still get `NEXT ACTION:  COMMAND = Error: ARGUMENTS = Invalid JSON`.\r\n\r\nNote that using --debug I saw it repeating the same JSON twice in a response, which would still break with this PR using the regex I'm pretty sure. The output was something like:\r\n```\r\nSure. I propose the following next command: ```{<json>}```\r\n\r\n{<same json>}\r\n```\r\n\r\nI'm not sure how you could fix this. Removing duplicates seems hacky and won't work if the model didn't output the same thing exactly twice. Skipping/requiring ``` around the JSON also probably won't work since there's no guarantee that they will/won't be there.",
      "Ignoring the part the model replied outside of JSON also seems a bit weird. I'm wondering if updating the prompt to be more strict might fix this more elegantly. Or how about giving the model it's own non-parsable result back with the task of fixing it?",
      "> Ignoring the part the model replied outside of JSON also seems a bit weird. I'm wondering if updating the prompt to be more strict might fix this more elegantly. Or how about giving the model it's own non-parsable result back with the task of fixing it?\r\n\r\nWon't work as I am an ai language model.....\r\nIt will not comply.\r\nI am thinking about a better way like restarting the instance in this case.\r\nBecause once it did respond with some pretext, it keeps going.,",
      "Is this problem caused by changes from Open AI, or by changes in this repository? Did you check the second option?",
      "Probably both.\r\nAlso gpt3 isn't very pretty once you got to talk to chatgpt4 or vicuna once.\r\nWill receive a new gpu tomorrow and use vicuna instead of chatgpt3",
      "Too bad we can't give Auto-GPT a job to fix Auto-GPT, because Auto-GPT is broken <img src=\"https://user-images.githubusercontent.com/36363466/231003670-e2a7ff19-0c21-4c11-ae90-79d29fa84f58.png\" width=\"32\">\r\n",
      "There is version control :trollface: You can always reset to last known good state",
      "Checking on the last commits: ~~there should be a re output of text in a correctly formatted format. (or even~~ don't show the incorrect output, leaving only informational messages about the error)\r\n\r\nUpdated",
      "Do as you want, my instance is working right now haha.\r\nOne thing we MUST add to config though: history being communicated.\r\nI changed it from -5 to -9 and now it finally writes files etc.\r\n\r\nMaybe I can hide it in debug, but I would have to hide all of those message logs which would be super newby-unfriendly.\r\nI used my free weekend to work on this haha, other's will probably just give up.",
      "> my instance is working right\r\n\r\nI wouldn't be so sure...\r\n![2023-04-11_03-04-22](https://user-images.githubusercontent.com/36363466/231022208-80a02477-8a8b-4e0e-92a3-4d93dfcbbc76.png)\r\n\r\nTranslation to English:\r\n```\r\nInput: you do not have that command\r\nSYSTEM: Human feedback: you do not have that command\r\nError: Invalid JSON in assistant thoughts\r\n Sorry, my last answer was obviously wrong. I don't have the \"count_to_ten\" command right now. I suggest to use the \"do_nothing\" command again to think about the best approach to achieve our goal.\r\n```\r\n\r\n```bash\r\nError:\r\n Traceback (most recent call last): File \"Auto-GPT/scripts/main.py\", line 106, in print_assistant_thoughts assistant_reply_json = fix_and_parse_json(assistant_reply\r\n) File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_parser.py\", line 38, in fix_and_parse_json json_str = correct_json(json_str) File \"D:\\YandexDisk\\Work\\\r\nAuto-ChatGPT\\Auto-GPT\\scripts\\json_utils.py\", line 125, in correct_json if balanced_str := balance_braces(json_str): File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\\r\nscripts\\json_utils.py\", line 79, in balance_braces raise e File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_utils.py\", line 76, in balance_braces json.lo\r\nads(json_string) File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\json\\__init__.py\", line 357, in loads return _default_decoder.decode(s) File \"C:\\ProgramData\\minicon\r\nda3\\envs\\py3.8\\lib\\json\\decoder.py\", line 337, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\json\\decod\r\ner.py\", line 355, in raw_decode raise JSONDecodeError(\"Expecting value\", s, err.value) from None json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (cha\r\nr 0) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"Auto-GPT/scripts/main.py\", line 110, in print_assi\r\nstant_thoughts assistant_reply_json = fix_and_parse_json(assistant_reply_json) File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_parser.py\", line 35, in fix_and_parse_json json_str = json_str.replace('\\t', '') AttributeError: 'dict' object has no attribute 'replace'\r\n```\r\n\r\nP.S. As I gradually expand my commentary, I more and more feel like a chatbot...",
      "I think your issue is fixed with the other utf-8 PR, as this code here has nothing to do with the exception (I mean it touches the old one, but your response seems to be off)\r\nMaybe debug what you have inside there exactly",
      "log.txt: `During handling of the above exception, another exception occurred`\r\n```bash\r\n03:04:03,182 AutoGPT INFO Error: \r\n: Traceback (most recent call last):\r\n  File \"Auto-GPT/scripts/main.py\", line 106, in print_assistant_thoughts\r\n    assistant_reply_json = fix_and_parse_json(assistant_reply)\r\n  File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_parser.py\", line 38, in fix_and_parse_json\r\n    json_str = correct_json(json_str)\r\n  File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_utils.py\", line 125, in correct_json\r\n    if balanced_str := balance_braces(json_str):\r\n  File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_utils.py\", line 79, in balance_braces\r\n    raise e\r\n  File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_utils.py\", line 76, in balance_braces\r\n    json.loads(json_string)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\json\\__init__.py\", line 357, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\json\\decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\py3.8\\lib\\json\\decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"Auto-GPT/scripts/main.py\", line 110, in print_assistant_thoughts\r\n    assistant_reply_json = fix_and_parse_json(assistant_reply_json)\r\n  File \"D:\\YandexDisk\\Work\\Auto-ChatGPT\\Auto-GPT\\scripts\\json_parser.py\", line 35, in fix_and_parse_json\r\n    json_str = json_str.replace('\\t', '')\r\nAttributeError: 'dict' object has no attribute 'replace'\r\n```",
      "Just some feedback, this doesn't seem to fix everything. It ran for quite a while, but then errors out on some stuff. e.g.:\r\n\r\n\r\n``` Traceback (most recent call last): File \"/Users/blahblah/Documents/sourcecode/Auto-GPT/scripts/main.py\", line 106, in print_assistant_thoughts assistant_reply_json = fix_and_parse_json(assistant_reply) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/blahblah/Documents/sourcecode/Auto-GPT/scripts/json_parser.py\", line 38, in fix_and_parse_json json_str = correct_json(json_str) ^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/blahblah/Documents/sourcecode/Auto-GPT/scripts/json_utils.py\", line 125, in correct_json if balanced_str := balance_braces(json_str): ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/blahblah/Documents/sourcecode/Auto-GPT/scripts/json_utils.py\", line 79, in balance_braces raise e File \"/Users/blahblah/Documents/sourcecode/Auto-GPT/scripts/json_utils.py\", line 76, in balance_braces json.loads(json_string) File \"/opt/homebrew/Cellar/python@3.11/3.11.2/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py\", line 346, in loads return _default_decoder.decode(s) ^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/homebrew/Cellar/python@3.11/3.11.2/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py\", line 337, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/homebrew/Cellar/python@3.11/3.11.2/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py\", line 355, in raw_decode raise JSONDecodeError(\"Expecting value\", s, err.value) from None json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/Users/blahblah/Documents/sourcecode/Auto-GPT/scripts/main.py\", line 110, in print_assistant_thoughts assistant_reply_json = fix_and_parse_json(assistant_reply_json) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/blahblah/Documents/sourcecode/Auto-GPT/scripts/json_parser.py\", line 35, in fix_and_parse_json json_str = json_str.replace('\\t', '') ^^^^^^^^^^^^^^^^ AttributeError: 'dict' object has no attribute 'replace'```\r\n",
      "I've update master and switch back to #697\r\nI noticed that the \"Criticism\" is now empty if there is a JSON fix message\r\n![2023-04-11_10-40-40](https://user-images.githubusercontent.com/36363466/231090551-fd80c2d7-4f2b-433e-9f3a-318d65c8c2ce.png)\r\nlog:\r\n```bash\r\n10:29:45,628 openai DEBUG api_version=None data='{\"input\": [\"Assistant Reply: Thank you for the feedback. You are correct that an LTS file would be helpful to record and organize information about my research on different long-term data storage systems.   I will create an LTS file and start recording information about the various systems I research.  {     \\\\\"thoughts\\\\\": {         \\\\\"text\\\\\": \\\\\"I will create an LTS file for myself to store information about my research on various long-term data storage systems.\\\\\",         \\\\\"reasoning\\\\\": \\\\\"Creating an LTS file for myself will allow me to keep all my research organized in a single location and ensure that I don\\'t forget any important details.\\\\\",         \\\\\"plan\\\\\": \\\\\"- Create an LTS file\\\\\\\\n- Record information about each long-term data storage system researched\\\\\\\\n- Organize the information for easy retrieval and comparison\\\\\",         \\\\\"criticism\\\\\": \\\\\"\\\\\",         \\\\\"speak\\\\\": \\\\\"I will create an LTS file for myself to store information about my research on various long-term data storage systems.\\\\\"     },     \\\\\"command\\\\\": {         \\\\\"name\\\\\": \\\\\"write_to_file\\\\\",         \\\\\"args\\\\\":{             \\\\\"file\\\\\": \\\\\"LTS_file.txt\\\\\",             \\\\\"text\\\\\": \\\\\"Long-Term Data Storage Systems Research:\\\\\\\\n\\\\\\\\n\\\\\"         }     } }  Result: Command write_to_file returned: File written to successfully.  Human Feedback: GENERATE NEXT COMMAND JSON \"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\r\n10:29:46,179 openai DEBUG message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=362 request_id=a953b8caab43822f38cabe8de0eed6bf response_code=200\r\n10:29:46,180 AutoGPT INFO SYSTEM: : Command write_to_file returned: File written to successfully.\r\n10:29:46,398 openai DEBUG message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\r\n10:29:46,399 openai DEBUG api_version=None data='{\"input\": [\"[{\\'role\\': \\'assistant\\', \\'content\\': \\'{\\\\\\\\n    \\\\\"thoughts\\\\\": {\\\\\\\\n        \\\\\"text\\\\\": \\\\\"I think I should start by researching various long-term data storage systems that would be appropriate for my needs.\\\\\",\\\\\\\\n        \\\\\"reasoning\\\\\": \\\\\"Without sufficient knowledge of existing long-term data storage systems, I cannot make a strategic decision about which one to use for my own files or suggest for use by other AI.\\\\\",\\\\\\\\n        \\\\\"plan\\\\\": \\\\\"- Research various long-term data storage systems\\\\\\\\\\\\\\\\n- Compare their features and costs\\\\\\\\\\\\\\\\n- Make an informed decision on which one to use\\\\\",\\\\\\\\n        \\\\\"criticism\\\\\": \\\\\"\\\\\",\\\\\\\\n        \\\\\"speak\\\\\": \\\\\"I believe the first step would be to research different long-term data storage systems.\\\\\"\\\\\\\\n    },\\\\\\\\n    \\\\\"command\\\\\": {\\\\\\\\n        \\\\\"name\\\\\": \\\\\"google\\\\\",\\\\\\\\n        \\\\\"args\\\\\":{\\\\\\\\n            \\\\\"input\\\\\": \\\\\"best long term data storage systems\\\\\"\\\\\\\\n        }\\\\\\\\n    }\\\\\\\\n}\\'}, {\\'role\\': \\'system\\', \\'content\\': \\\\\"Human feedback: It is worth creating an LTS file for yourself first, so you don\\'t forget anything.\\\\\"}, {\\'role\\': \\'user\\', \\'content\\': \\\\\"It is worth creating an LTS file for yourself first, so you don\\'t forget anything.\\\\\"}, {\\'role\\': \\'assistant\\', \\'content\\': \\'Thank you for the feedback. You are correct that an LTS file would be helpful to record and organize information about my research on different long-term data storage systems. \\\\\\\\n\\\\\\\\nI will create an LTS file and start recording information about the various systems I research.\\\\\\\\n\\\\\\\\n{\\\\\\\\n    \\\\\"thoughts\\\\\": {\\\\\\\\n        \\\\\"text\\\\\": \\\\\"I will create an LTS file for myself to store information about my research on various long-term data storage systems.\\\\\",\\\\\\\\n        \\\\\"reasoning\\\\\": \\\\\"Creating an LTS file for myself will allow me to keep all my research organized in a single location and ensure that I don\\\\\\\\\\'t forget any important details.\\\\\",\\\\\\\\n        \\\\\"plan\\\\\": \\\\\"- Create an LTS file\\\\\\\\\\\\\\\\n- Record information about each long-term data storage system researched\\\\\\\\\\\\\\\\n- Organize the information for easy retrieval and comparison\\\\\",\\\\\\\\n        \\\\\"criticism\\\\\": \\\\\"\\\\\",\\\\\\\\n        \\\\\"speak\\\\\": \\\\\"I will create an LTS file for myself to store information about my research on various long-term data storage systems.\\\\\"\\\\\\\\n    },\\\\\\\\n    \\\\\"command\\\\\": {\\\\\\\\n        \\\\\"name\\\\\": \\\\\"write_to_file\\\\\",\\\\\\\\n        \\\\\"args\\\\\":{\\\\\\\\n            \\\\\"file\\\\\": \\\\\"LTS_file.txt\\\\\",\\\\\\\\n            \\\\\"text\\\\\": \\\\\"Long-Term Data Storage Systems Research:\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\"\\\\\\\\n        }\\\\\\\\n    }\\\\\\\\n}\\'}, {\\'role\\': \\'system\\', \\'content\\': \\'Command write_to_file returned: File written to successfully.\\'}]\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\r\n10:29:46,616 openai DEBUG message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=27 request_id=d5cf6d2153dcec1743af8a7ec0c4b324 response_code=200\r\n10:29:46,619 openai DEBUG api_version=None data='{\"model\": \"gpt-3.5-turbo\", \"messages\": [{\"role\": \"system\", \"content\": \"You are Librarian, \\\\u041e\\\\u0442\\\\u0432\\\\u0435\\\\u0447\\\\u0430\\\\u0435\\\\u0442 \\\\u0437\\\\u0430 \\\\u0445\\\\u0440\\\\u0430\\\\u043d\\\\u0435\\\\u043d\\\\u0438\\\\u0435 \\\\u0434\\\\u0430\\\\u043d\\\\u043d\\\\u044b\\\\u0445 \\\\u0434\\\\u043b\\\\u044f \\\\u043e\\\\u0441\\\\u0442\\\\u0430\\\\u043b\\\\u044c\\\\u043d\\\\u044b\\\\u0445 AI\\\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\\\n\\\\nGOALS:\\\\n\\\\n1. Create LTS file for my own\\\\n2. \\\\u0418\\\\u0437\\\\u0443\\\\u0447\\\\u0438\\\\u0442\\\\u044c \\\\u0441\\\\u0443\\\\u0449\\\\u0435\\\\u0441\\\\u0442\\\\u0432\\\\u0443\\\\u044e\\\\u0449\\\\u0438\\\\u0435 \\\\u0432\\\\u0430\\\\u0440\\\\u0438\\\\u0430\\\\u043d\\\\u0442\\\\u044b \\\\u0434\\\\u043e\\\\u043b\\\\u0433\\\\u043e\\\\u0432\\\\u0440\\\\u0435\\\\u043c\\\\u0435\\\\u043d\\\\u043d\\\\u043e\\\\u0433\\\\u043e \\\\u0445\\\\u0440\\\\u0430\\\\u043d\\\\u0435\\\\u043d\\\\u0438\\\\u044f \\\\u0434\\\\u0430\\\\u043d\\\\u043d\\\\u044b\\\\u0445\\\\n3. \\\\u0412\\\\u044b\\\\u0431\\\\u0440\\\\u0430\\\\u0442\\\\u044c \\\\u0438\\\\u043b\\\\u0438 \\\\u0440\\\\u0430\\\\u0437\\\\u0440\\\\u0430\\\\u0431\\\\u043e\\\\u0442\\\\u0430\\\\u0442\\\\u044c \\\\u0441\\\\u0438\\\\u0441\\\\u0442\\\\u0435\\\\u043c\\\\u0443 \\\\u0445\\\\u0440\\\\u0430\\\\u043d\\\\u0435\\\\u043d\\\\u0438\\\\u044f \\\\u0434\\\\u043e\\\\u043b\\\\u0433\\\\u043e\\\\u0432\\\\u0440\\\\u0435\\\\u043c\\\\u0435\\\\u043d\\\\u043d\\\\u043e\\\\u0439 \\\\u043f\\\\u0430\\\\u043c\\\\u044f\\\\u0442\\\\u0438, \\\\u0441 \\\\u043a\\\\u043e\\\\u0442\\\\u043e\\\\u0440\\\\u043e\\\\u0439 \\\\u043c\\\\u043e\\\\u0433\\\\u0443\\\\u0442 \\\\u0432\\\\u0437\\\\u0430\\\\u0438\\\\u043c\\\\u043e\\\\u0434\\\\u0435\\\\u0439\\\\u0441\\\\u0442\\\\u0432\\\\u043e\\\\u0432\\\\u0430\\\\u0442\\\\u044c \\\\u0434\\\\u0440\\\\u0443\\\\u0433\\\\u0438\\\\u0435 AI. \\\\u041e\\\\u0441\\\\u0442\\\\u0430\\\\u0432\\\\u0438\\\\u0442\\\\u044c \\\\u0438\\\\u043d\\\\u0441\\\\u0442\\\\u0440\\\\u0443\\\\u043a\\\\u0446\\\\u0438\\\\u044e \\\\u043f\\\\u043e \\\\u0438\\\\u0441\\\\u043f\\\\u043e\\\\u043b\\\\u044c\\\\u0437\\\\u043e\\\\u0432\\\\u0430\\\\u043d\\\\u0438\\\\u044e \\\\u0441\\\\u0438\\\\u0441\\\\u0442\\\\u0435\\\\u043c\\\\u044b \\\\u0445\\\\u0440\\\\u0430\\\\u043d\\\\u0435\\\\u043d\\\\u0438\\\\u044f, \\\\u043a\\\\u043e\\\\u0442\\\\u043e\\\\u0440\\\\u0430\\\\u044f \\\\u0431\\\\u0443\\\\u0434\\\\u0435\\\\u0442 \\\\u043f\\\\u043e\\\\u043d\\\\u044f\\\\u0442\\\\u043d\\\\u0430 \\\\u0434\\\\u0440\\\\u0443\\\\u0433\\\\u0438\\\\u043c AI\\\\n\\\\n\\\\nCONSTRAINTS:\\\\n\\\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\\\n3. No user assistance\\\\n4. Exclusively use the commands listed in double quotes e.g. \\\\\"command name\\\\\"\\\\n\\\\nCOMMANDS:\\\\n\\\\n1. Google Search: \\\\\"google\\\\\", args: \\\\\"input\\\\\": \\\\\"<search>\\\\\"\\\\n5. Browse Website: \\\\\"browse_website\\\\\", args: \\\\\"url\\\\\": \\\\\"<url>\\\\\", \\\\\"question\\\\\": \\\\\"<what_you_want_to_find_on_website>\\\\\"\\\\n6. Start GPT Agent: \\\\\"start_agent\\\\\",  args: \\\\\"name\\\\\": \\\\\"<name>\\\\\", \\\\\"task\\\\\": \\\\\"<short_task_desc>\\\\\", \\\\\"prompt\\\\\": \\\\\"<prompt>\\\\\"\\\\n7. Message GPT Agent: \\\\\"message_agent\\\\\", args: \\\\\"key\\\\\": \\\\\"<key>\\\\\", \\\\\"message\\\\\": \\\\\"<message>\\\\\"\\\\n8. List GPT Agents: \\\\\"list_agents\\\\\", args: \\\\\"\\\\\"\\\\n9. Delete GPT Agent: \\\\\"delete_agent\\\\\", args: \\\\\"key\\\\\": \\\\\"<key>\\\\\"\\\\n10. Write to file: \\\\\"write_to_file\\\\\", args: \\\\\"file\\\\\": \\\\\"<file>\\\\\", \\\\\"text\\\\\": \\\\\"<text>\\\\\"\\\\n11. Read file: \\\\\"read_file\\\\\", args: \\\\\"file\\\\\": \\\\\"<file>\\\\\"\\\\n12. Append to file: \\\\\"append_to_file\\\\\", args: \\\\\"file\\\\\": \\\\\"<file>\\\\\", \\\\\"text\\\\\": \\\\\"<text>\\\\\"\\\\n13. Delete file: \\\\\"delete_file\\\\\", args: \\\\\"file\\\\\": \\\\\"<file>\\\\\"\\\\n14. Search Files: \\\\\"search_files\\\\\", args: \\\\\"directory\\\\\": \\\\\"<directory>\\\\\"\\\\n15. Evaluate Code: \\\\\"evaluate_code\\\\\", args: \\\\\"code\\\\\": \\\\\"<full_code_string>\\\\\"\\\\n16. Get Improved Code: \\\\\"improve_code\\\\\", args: \\\\\"suggestions\\\\\": \\\\\"<list_of_suggestions>\\\\\", \\\\\"code\\\\\": \\\\\"<full_code_string>\\\\\"\\\\n17. Write Tests: \\\\\"write_tests\\\\\", args: \\\\\"code\\\\\": \\\\\"<full_code_string>\\\\\", \\\\\"focus\\\\\": \\\\\"<list_of_focus_areas>\\\\\"\\\\n18. Execute Python File: \\\\\"execute_python_file\\\\\", args: \\\\\"file\\\\\": \\\\\"<file>\\\\\"\\\\n19. Task Complete (Shutdown): \\\\\"task_complete\\\\\", args: \\\\\"reason\\\\\": \\\\\"<reason>\\\\\"\\\\n20. Generate Image: \\\\\"generate_image\\\\\", args: \\\\\"prompt\\\\\": \\\\\"<prompt>\\\\\"\\\\n21. Do Nothing: \\\\\"do_nothing\\\\\", args: \\\\\"\\\\\"\\\\n\\\\nRESOURCES:\\\\n\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPERFORMANCE EVALUATION:\\\\n\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\\\n\\\\nYou should only respond in JSON format as described below\\\\n\\\\nRESPONSE FORMAT:\\\\n{\\\\n    \\\\\"thoughts\\\\\":\\\\n    {\\\\n        \\\\\"text\\\\\": \\\\\"thought\\\\\",\\\\n        \\\\\"reasoning\\\\\": \\\\\"reasoning\\\\\",\\\\n        \\\\\"plan\\\\\": \\\\\"- short bulleted\\\\\\\\n- list that conveys\\\\\\\\n- long-term plan\\\\\",\\\\n        \\\\\"criticism\\\\\": \\\\\"constructive self-criticism\\\\\",\\\\n        \\\\\"speak\\\\\": \\\\\"thoughts summary to say to user\\\\\"\\\\n    },\\\\n    \\\\\"command\\\\\": {\\\\n        \\\\\"name\\\\\": \\\\\"command name\\\\\",\\\\n        \\\\\"args\\\\\":{\\\\n            \\\\\"arg name\\\\\": \\\\\"value\\\\\"\\\\n        }\\\\n    }\\\\n}\\\\n\\\\nEnsure the response can be parsed by Python json.loads\\\\n\"}, {\"role\": \"system\", \"content\": \"The current time and date is Tue Apr 11 10:29:46 2023\"}, {\"role\": \"system\", \"content\": \"This reminds you of these events from your past:\\\\n[\\'Assistant Reply: Thank you for the feedback. You are correct that an LTS file would be helpful to record and organize information about my research on different long-term data storage systems. \\\\\\\\n\\\\\\\\nI will create an LTS file and start recording information about the various systems I research.\\\\\\\\n\\\\\\\\n{\\\\\\\\n    \\\\\"thoughts\\\\\": {\\\\\\\\n        \\\\\"text\\\\\": \\\\\"I will create an LTS file for myself to store information about my research on various long-term data storage systems.\\\\\",\\\\\\\\n        \\\\\"reasoning\\\\\": \\\\\"Creating an LTS file for myself will allow me to keep all my research organized in a single location and ensure that I don\\\\\\\\\\'t forget any important details.\\\\\",\\\\\\\\n        \\\\\"plan\\\\\": \\\\\"- Create an LTS file\\\\\\\\\\\\\\\\n- Record information about each long-term data storage system researched\\\\\\\\\\\\\\\\n- Organize the information for easy retrieval and comparison\\\\\",\\\\\\\\n        \\\\\"criticism\\\\\": \\\\\"\\\\\",\\\\\\\\n        \\\\\"speak\\\\\": \\\\\"I will create an LTS file for myself to store information about my research on various long-term data storage systems.\\\\\"\\\\\\\\n    },\\\\\\\\n    \\\\\"command\\\\\": {\\\\\\\\n        \\\\\"name\\\\\": \\\\\"write_to_file\\\\\",\\\\\\\\n        \\\\\"args\\\\\":{\\\\\\\\n            \\\\\"file\\\\\": \\\\\"LTS_file.txt\\\\\",\\\\\\\\n            \\\\\"text\\\\\": \\\\\"Long-Term Data Storage Systems Research:\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\"\\\\\\\\n        }\\\\\\\\n    }\\\\\\\\n} \\\\\\\\nResult: Command write_to_file returned: File written to successfully. \\\\\\\\nHuman Feedback: GENERATE NEXT COMMAND JSON \\', \\'Assistant Reply: {\\\\\\\\n    \\\\\"thoughts\\\\\": {\\\\\\\\n        \\\\\"text\\\\\": \\\\\"I think I should start by researching various long-term data storage systems that would be appropriate for my needs.\\\\\",\\\\\\\\n        \\\\\"reasoning\\\\\": \\\\\"Without sufficient knowledge of existing long-term data storage systems, I cannot make a strategic decision about which one to use for my own files or suggest for use by other AI.\\\\\",\\\\\\\\n        \\\\\"plan\\\\\": \\\\\"- Research various long-term data storage systems\\\\\\\\\\\\\\\\n- Compare their features and costs\\\\\\\\\\\\\\\\n- Make an informed decision on which one to use\\\\\",\\\\\\\\n        \\\\\"criticism\\\\\": \\\\\"\\\\\",\\\\\\\\n        \\\\\"speak\\\\\": \\\\\"I believe the first step would be to research different long-term data storage systems.\\\\\"\\\\\\\\n    },\\\\\\\\n    \\\\\"command\\\\\": {\\\\\\\\n        \\\\\"name\\\\\": \\\\\"google\\\\\",\\\\\\\\n        \\\\\"args\\\\\":{\\\\\\\\n            \\\\\"input\\\\\": \\\\\"best long term data storage systems\\\\\"\\\\\\\\n        }\\\\\\\\n    }\\\\\\\\n} \\\\\\\\nResult: Human feedback: It is worth creating an LTS file for yourself first, so you don\\\\\\\\\\'t forget anything. \\\\\\\\nHuman Feedback: It is worth creating an LTS file for yourself first, so you don\\\\\\\\\\'t forget anything. \\']\\\\n\\\\n\"}, {\"role\": \"user\", \"content\": \"Determine which next command to use, and respond using the format specified above:\"}, {\"role\": \"assistant\", \"content\": \"{\\\\n    \\\\\"thoughts\\\\\": {\\\\n        \\\\\"text\\\\\": \\\\\"I think I should start by researching various long-term data storage systems that would be appropriate for my needs.\\\\\",\\\\n        \\\\\"reasoning\\\\\": \\\\\"Without sufficient knowledge of existing long-term data storage systems, I cannot make a strategic decision about which one to use for my own files or suggest for use by other AI.\\\\\",\\\\n        \\\\\"plan\\\\\": \\\\\"- Research various long-term data storage systems\\\\\\\\n- Compare their features and costs\\\\\\\\n- Make an informed decision on which one to use\\\\\",\\\\n        \\\\\"criticism\\\\\": \\\\\"\\\\\",\\\\n        \\\\\"speak\\\\\": \\\\\"I believe the first step would be to research different long-term data storage systems.\\\\\"\\\\n    },\\\\n    \\\\\"command\\\\\": {\\\\n        \\\\\"name\\\\\": \\\\\"google\\\\\",\\\\n        \\\\\"args\\\\\":{\\\\n            \\\\\"input\\\\\": \\\\\"best long term data storage systems\\\\\"\\\\n        }\\\\n    }\\\\n}\"}, {\"role\": \"system\", \"content\": \"Human feedback: It is worth creating an LTS file for yourself first, so you don\\'t forget anything.\"}, {\"role\": \"user\", \"content\": \"It is worth creating an LTS file for yourself first, so you don\\'t forget anything.\"}, {\"role\": \"assistant\", \"content\": \"Thank you for the feedback. You are correct that an LTS file would be helpful to record and organize information about my research on different long-term data storage systems. \\\\n\\\\nI will create an LTS file and start recording information about the various systems I research.\\\\n\\\\n{\\\\n    \\\\\"thoughts\\\\\": {\\\\n        \\\\\"text\\\\\": \\\\\"I will create an LTS file for myself to store information about my research on various long-term data storage systems.\\\\\",\\\\n        \\\\\"reasoning\\\\\": \\\\\"Creating an LTS file for myself will allow me to keep all my research organized in a single location and ensure that I don\\'t forget any important details.\\\\\",\\\\n        \\\\\"plan\\\\\": \\\\\"- Create an LTS file\\\\\\\\n- Record information about each long-term data storage system researched\\\\\\\\n- Organize the information for easy retrieval and comparison\\\\\",\\\\n        \\\\\"criticism\\\\\": \\\\\"\\\\\",\\\\n        \\\\\"speak\\\\\": \\\\\"I will create an LTS file for myself to store information about my research on various long-term data storage systems.\\\\\"\\\\n    },\\\\n    \\\\\"command\\\\\": {\\\\n        \\\\\"name\\\\\": \\\\\"write_to_file\\\\\",\\\\n        \\\\\"args\\\\\":{\\\\n            \\\\\"file\\\\\": \\\\\"LTS_file.txt\\\\\",\\\\n            \\\\\"text\\\\\": \\\\\"Long-Term Data Storage Systems Research:\\\\\\\\n\\\\\\\\n\\\\\"\\\\n        }\\\\n    }\\\\n}\"}, {\"role\": \"system\", \"content\": \"Command write_to_file returned: File written to successfully.\"}, {\"role\": \"user\", \"content\": \"GENERATE NEXT COMMAND JSON\"}], \"temperature\": null, \"max_tokens\": 1994}' message='Post details'\r\n10:30:25,439 openai DEBUG message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=38625 request_id=1751c70f16285edd99f04bf90c6e71c5 response_code=200\r\n10:30:25,472 AutoGPT INFO LIBRARIAN THOUGHTS:: Now that I have created an LTS file, I can research various long-term data storage systems using Google Search.\r\n10:30:25,987 AutoGPT INFO REASONING:: Performing a search will allow me to gather information about various data storage systems, learn about their features, and determine which one would be best for my needs.\r\n10:30:26,622 AutoGPT INFO - : Use Google Search to find information about various long-term data storage systems\r\n10:30:26,920 AutoGPT INFO - : Record pertinent information about each system in the LTS file\r\n10:30:27,262 AutoGPT INFO - : Continue research until a decision can be made\r\n10:30:27,529 AutoGPT INFO NEXT ACTION: : COMMAND = \u001b[36mgoogle\u001b[0m  ARGUMENTS = \u001b[36m{'input': 'long-term data storage systems features and cost'}\u001b[0m\r\n```",
      "> I noticed that the \"Criticism\" is now empty if there is a JSON fix message\r\n\r\nThat happens not everytime when \"Apparently json was fixed.\". Hmmm...",
      "Maybe this attemptive fix can be added into the fix_and_parse_json or something.\r\n@Artemonim  for that issue, I checked the init message to gpt, I think we should create a fallback method.\r\n\r\nSomething like \"Summarize last 5-10 history entries\" and feed it into a new gpt instance, as once it starts doing those broken json messages, it will continue to do so.\r\n\r\n@Torantulino  Any ideas? I have to work now, so will be away for half a day.\r\nA friend of mine started writing a new project and managed to avoid this exact issue by letting it give him shell commands to echo every single line of the response. everytime a line didn't begin with echo, he just asks gpt again...",
      "> I've update master and switch back to #697 I noticed that the \"Criticism\" is now empty if there is a JSON fix message\r\n\r\nI thought I was on present master + #697, but I was only on #697. Sorry for the mistake."
    ],
    "num_comments": 30,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 5475,
    "code_diff": "diff --git a/scripts/chat.py b/scripts/chat.py\nindex c64be3b84b55..2f76e8e214bf 100644\n--- a/scripts/chat.py\n+++ b/scripts/chat.py\n@@ -69,7 +69,7 @@ def chat_with_ai(\n \n             send_token_limit = token_limit - 1000\n \n-            relevant_memory = permanent_memory.get_relevant(str(full_message_"
  },
  {
    "pr_title": "feat(db): Initialize Database Schema for AutoGPT server",
    "pr_body": "### Background\r\n\r\nIntroduced initial database schema for AutoGPT server.\r\nIt currently consists of 7 tables:\r\n\r\n* `AgentGraph`: This model describes the Agent Graph/Flow (Multi Agent System).\r\n    * `AgentNode`: This model describes a single node in the Agent Graph/Flow (Multi Agent System).\r\n        * `AgentNodeLink`: This model describes the link between two AgentNodes.\r\n        * `AgentNodeExecution`: This model describes the execution of an AgentNode.\r\n* `AgentBlock`: This model describes a ",
    "pr_number": 7168,
    "comments": [
      "As usual naming suggestions and feedbacks are welcome :)",
      "Oh prisma docs looks great, though we'll need to keep running `npx prisma-docs-generator serve` to server the doc. \r\nI'll keep documenting the schema for the time being, so that when we use it the doc will look slightly more helpful.",
      "I believe the state will be a simple key value store?\r\nI have added `executionStateData` in `AgentNodeExecution`",
      "> I believe the state will be a simple key value store? I have added `executionStateData` in `AgentNodeExecution`\r\n\r\nLet‚Äôs make it json?",
      "> Let‚Äôs make it json?\r\nIt was supposed to be a json, but there is no native support for It on sqlite"
    ],
    "num_comments": 5,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 36834,
    "code_diff": "diff --git a/rnd/autogpt_server/autogpt_server/.gitignore b/rnd/autogpt_server/autogpt_server/.gitignore\nnew file mode 100644\nindex 000000000000..0948a6067896\n--- /dev/null\n+++ b/rnd/autogpt_server/autogpt_server/.gitignore\n@@ -0,0 +1,2 @@\n+database.db\n+database.db-journal\ndiff --git a/rnd/autogpt_s"
  },
  {
    "pr_title": "feat(agent): Abstract file storage access",
    "pr_body": "This improvement abstracts file access across whole project, so that AutoGPT app/agents can use different backends (e.g. local, GCS, S3).\r\n\r\nFixes related issue: OPEN-103\r\n\r\n## Changes\r\n\r\n* **‚ö†Ô∏è BREAKING:** Rename `WORKSPACE_BACKEND` to `FILE_STORAGE_BACKEND`\r\n* **‚ö†Ô∏è BREAKING:** Rename `WORKSPACE_STORAGE_BUCKET` to `STORAGE_BUCKET`\r\n\r\n* Rename `FileWorkspace` to `FileStorage`\r\n   - `autogpt.file_workspace` -> `autogpt.file_storage`\r\n   - `LocalFileWorkspace` -> `LocalFileStorage`\r\n   - `S3FileWo",
    "pr_number": 6931,
    "comments": [
      "Fails to deploy to GCP:\r\n```\r\nERROR 2024-03-12T14:47:31.371921Z Traceback (most recent call last): File \"<string>\", line 1, in <module> File \"/venv/agpt-9TtSrW0h-py3.10/lib/python3.10/site-packages/click/core.py\", line 1157, in __call__\r\nDEFAULT 2024-03-12T14:47:31.371932Z return self.main(*args, **kwargs)\r\nDEFAULT 2024-03-12T14:47:31.371941Z File \"/venv/agpt-9TtSrW0h-py3.10/lib/python3.10/site-packages/click/core.py\", line 1078, in main\r\nDEFAULT 2024-03-12T14:47:31.371945Z rv = self.invoke(ctx)\r\nDEFAULT 2024-03-12T14:47:31.371951Z File \"/venv/agpt-9TtSrW0h-py3.10/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\r\nDEFAULT 2024-03-12T14:47:31.371955Z return _process_result(sub_ctx.command.invoke(sub_ctx))\r\nDEFAULT 2024-03-12T14:47:31.371959Z File \"/venv/agpt-9TtSrW0h-py3.10/lib/python3.10/site-packages/click/core.py\", line 1434, in invoke\r\nDEFAULT 2024-03-12T14:47:31.371963Z return ctx.invoke(self.callback, **ctx.params)\r\nDEFAULT 2024-03-12T14:47:31.371968Z File \"/venv/agpt-9TtSrW0h-py3.10/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\r\nDEFAULT 2024-03-12T14:47:31.371972Z return __callback(*args, **kwargs)\r\nDEFAULT 2024-03-12T14:47:31.371976Z File \"/app/autogpt/app/cli.py\", line 272, in serve\r\nDEFAULT 2024-03-12T14:47:31.371980Z run_auto_gpt_server(\r\nDEFAULT 2024-03-12T14:47:31.371984Z File \"/app/autogpt/core/runner/client_lib/utils.py\", line 60, in wrapper\r\nDEFAULT 2024-03-12T14:47:31.371988Z return asyncio.run(f(*args, **kwargs))\r\nDEFAULT 2024-03-12T14:47:31.371992Z File \"/usr/local/lib/python3.10/asyncio/runners.py\", line 44, in run\r\nDEFAULT 2024-03-12T14:47:31.371995Z return loop.run_until_complete(main)\r\nDEFAULT 2024-03-12T14:47:31.372Z File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\nDEFAULT 2024-03-12T14:47:31.372003Z return future.result()\r\nDEFAULT 2024-03-12T14:47:31.372007Z File \"/app/autogpt/app/main.py\", line 351, in run_auto_gpt_server\r\nDEFAULT 2024-03-12T14:47:31.372011Z file_storage = get_storage(\r\nDEFAULT 2024-03-12T14:47:31.372015Z File \"/app/autogpt/file_storage/__init__.py\", line 37, in get_storage\r\nDEFAULT 2024-03-12T14:47:31.372019Z return GCSFileStorage(config)\r\nDEFAULT 2024-03-12T14:47:31.372023Z File \"/app/autogpt/file_storage/gcs.py\", line 36, in __init__\r\nDEFAULT 2024-03-12T14:47:31.372026Z assert self._root.is_absolute()\r\nDEFAULT 2024-03-12T14:47:31.372043Z AssertionError\r\n\r\n```"
    ],
    "num_comments": 1,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 142623,
    "code_diff": "diff --git a/autogpts/autogpt/.env.template b/autogpts/autogpt/.env.template\nindex 40590e4098f2..9152d27998b1 100644\n--- a/autogpts/autogpt/.env.template\n+++ b/autogpts/autogpt/.env.template\n@@ -20,12 +20,12 @@ OPENAI_API_KEY=your-openai-api-key\n ## DISABLED_COMMAND_CATEGORIES - The list of categori"
  },
  {
    "pr_title": "Chat plugin capability",
    "pr_body": "<!-- ‚ö†Ô∏è At the moment any non-essential commands are not being merged.\r\nIf you want to add non-essential commands to Auto-GPT, please create a plugin instead.\r\nWe are expecting to ship plugin support within the week (PR #757).\r\nResources:\r\n* https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template\r\n-->\r\n\r\n<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to",
    "pr_number": 2929,
    "comments": [
      "@BillSchumacher You can also give feedback to the naming of the methods. I am not 100% satisfied with \"report(str)\"\r\nIt could also be send_message(str), but not sure if any other plugins are actually being chat plugins then..",
      "@desojo to test this, you can use my plugin:\r\nhttps://github.com/Wladastic/Auto-GPT-Telegram-Plugin\r\n\r\nThe Plugin Template PR is through, needs some waiting for it to be updated on pip, otherwise hasattr() will just pass\r\n\r\nFeel free to contact me, I have received multiple questions and messages regarding this feature :)",
      "I had to change enumerate to make the plugin work.\r\nOtherwise it was turned into a tuple which didn't work.",
      "proof of work:\r\nhttps://cdn.discordapp.com/attachments/1100128423541747722/1100509590606590112/RPReplay_Final1682452290.mov",
      "I have no concerns about the config usage after the update at @BillSchumacher 's request.  Will leave it to you guys to work through any remaining concerns.",
      "Requested Changes are done! :)"
    ],
    "num_comments": 6,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 7104,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 1d9eefb3b58a..3d2746c45c44 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -201,3 +201,9 @@ OPENAI_API_KEY=your-openai-api-key\n \n #ALLOWLISTED_PLUGINS - Sets the listed plugins that are allowed (Example: plugin1,plugin2,plugin3)\n ALLOWLISTED_PLUGINS"
  },
  {
    "pr_title": "feat(frontend): Implement UI for Agent Input subtypes",
    "pr_body": "- Follow-up to #9657\r\n\r\n<img width=\"280\" alt=\"image\" src=\"https://github.com/user-attachments/assets/2f3cd683-db63-485f-8914-5654c34f1a4c\" />\r\n\r\n<img width=\"520\" alt=\"image\" src=\"https://github.com/user-attachments/assets/de7e7cb9-61d4-4071-aea8-393ff5200c54\" />\r\n\r\n### Changes üèóÔ∏è\r\n\r\n* Implement the input UI for Agent Input subtypes.\r\n* Refactor node-input-component, extra out data type decision logic, share it with runner/library input.\r\n* Add `format` field for short-text, long-text, and mediaf",
    "pr_number": 9700,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 92757,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/blocks/basic.py b/autogpt_platform/backend/backend/blocks/basic.py\nindex 30ceeaef6a57..81ceaf87f085 100644\n--- a/autogpt_platform/backend/backend/blocks/basic.py\n+++ b/autogpt_platform/backend/backend/blocks/basic.py\n@@ -4,19 +4,19 @@\n from backend.data."
  },
  {
    "pr_title": "feat(frontend/backend): admin agent review table",
    "pr_body": "<!-- Clearly explain the need for these changes: -->\r\nWe need an admin agent approval UI for handling the submissions to the marketplace\r\n\r\n### Changes üèóÔ∏è\r\n- Adds routes to the admin routes list\r\n- Fixes the db query for submitting new versions of existing agents\r\n- Add models for responses that include version details\r\n- add the admin pages for agent\r\n- Adds the Admin Agent Data Table\r\n- Add all the new endpoints to the client.ts\r\nModels changes\r\n- convert the Submission status to an enum\r\n- re",
    "pr_number": 9634,
    "comments": [
      "The PR fails to meet basic requirements as the template is not filled out at all. While the code changes appear to be adding an admin agent review table with appropriate protections, the PR template needs to include: 1) A clear explanation of the changes and their purpose 2) A specific test plan for these changes 3) The checklist needs to be filled out appropriately. The code itself looks fine, but documentation is important for maintaining quality.",
      "The PR does not have a properly filled out description and checklist as required. While the code changes appear to add an admin agent review table functionality, the PR template is completely empty with no description of changes or completed checklist items. The base route /admin/agents needs to be protected but this is handled correctly via withRoleAccess(['admin']). The code itself looks reasonable but we need proper documentation.",
      "This PR has several issues that need to be addressed: 1) The PR template is completely empty/unfilled, providing no description of changes or test plan 2) The PR title has a scope (frontend) but the changes include both frontend and backend code which makes the scope misleading 3) There are numerous changes to data handling in both backend and frontend without clear documentation on user_id validation and security implications",
      "The PR has several issues that need addressing: 1) The PR lacks a proper test plan - the checklist shows 'I have made a test plan' is checked but no actual test plan is provided 2) The title has a scope but it's not in the conventional commit format - it should be feat(admin): agent review table (ideally) 3) The description is good though it lists the changes that matches the scope of work. 4) Since this adds new admin routes to the frontend, we need to verify the middleware protections are properly updated, but there is evidence in the code showing withRoleAccess(['admin']) is being used.",
      "The PR shows issues with compliance to the rules:\n1. The PR has an incomplete test plan checklist - the checkbox is unchecked and no actual test plan is provided\n2. While the PR has a scope in the title 'feat(frontend/backend)', it would be clearer to pick one primary scope\n3. No verification appears to have been done around the required user_id checks for data/*.py changes (not applicable here as changes are in backend/)\n4. The checklist is incomplete - the configuration section wasn't removed but left empty\n\nHowever, the core elements are present:\n- Clear description of changes\n- Proper title format\n- Main checklist items are present\n- Changes are well documented",
      "The PR has some issues but isn't failing dramatically. The main concerns are:\n1. Test plan is empty and not checked off\n2. The PR scope seems appropriate as it's adding admin agent review functionality\n3. The user_id checks are properly handled in the backend routes through admin authentication middleware\n4. The frontend route protection is already handled via the withRoleAccess wrapper\n5. The changes are well documented and focused on the stated goal",
      "The PR fails to meet several requirements, though not catastrophically. Main issues: 1) The test plan section is incomplete - it's marked as having a plan but no actual test steps are listed. 2) The PR title follows conventional commit format but the explanation of changes could be more detailed, especially around the data model changes. 3) There are database access changes that appear to handle user_id appropriately in create_store_version but this should be explicitly called out. On the positive side, the code changes are well-organized and the PR template is mostly filled out with relevant sections.",
      "The PR has some issues that need to be addressed. While the changes are well documented and the scope is clear in the title, there are some key concerns:\n1. The test plan section is marked but no actual test plan is provided (empty checklist)\n2. The checklist item for testing changes is not marked as complete\n3. The user_id access checks in the backend code need to be verified since there are new database operations being added - particularly around admin access\n4. The changes seem to be in scope and related to the admin agent review functionality",
      "While the PR is well-structured and makes significant improvements to the admin interface for agent review, there are a few issues: 1) The test plan checkbox is marked but no actual test steps are provided despite having an example test plan template 2) The scope in the PR title (frontend/backend) should be more specific, following conventional commit format 3) Changes to user_id handling in data/*.py need to be verified but there don't appear to be any such changes 4) All other changes appear to be well documented and within scope of the feature.",
      "The PR appears to fail multiple requirements:\n1. The PR template is not completely filled out - particularly the test plan section is empty\n2. There are changes to data/*.py files that need to check user_id (admin_routes.py) but they appear to be properly protected with fastapi.Depends(autogpt_libs.auth.depends.requires_admin_user)\n3. The frontend route protection is properly handled in the admin layout file\n4. The PR title has the proper scope format\n5. The changes appear to be in scope and related to the feature",
      "This PR appears to have several minor issues but is generally following the rules: 1) The test plan section is not filled out completely as required, just marked with '...' 2) The out of scope changes appear minimal and related to the core functionality 3) The PR title follows conventional commit format 4) The changes in data/*.py don't require user_id validation since this is admin-only code. These are relatively minor issues given the scope of the changes and the overall quality of the documentation.",
      "This PR appears to be well-structured and follows the required format with appropriate scoping in the title (frontend/backend). The changes are clearly documented and there's a test plan included. However, there are a few minor issues: 1) The test plan checkboxes are not all checked, showing incomplete testing 2) The PR description could be more detailed about testing done for version submissions. Given this is a new feature with admin routes, having only partially completed testing poses some risk but not enough to block the PR entirely.",
      "The PR meets most requirements but has a few minor issues: 1) The PR title follows conventional commit format with proper scope. 2) Changes are well documented in the PR description. 3) User_id checks appear to be properly handled in the backend data changes. However, the test plan checklist is incomplete with unchecked items and no documented test results. The PR would benefit from completing the testing documentation.",
      "OOOF IVE BEEN PUSHING TO THE WRONG BRANCH. It's fine, I'm fine\n\nI'll tag you when fixed :)"
    ],
    "num_comments": 14,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 94063,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/server/rest_api.py b/autogpt_platform/backend/backend/server/rest_api.py\nindex e054cfebf8b0..5ba4e4016104 100644\n--- a/autogpt_platform/backend/backend/server/rest_api.py\n+++ b/autogpt_platform/backend/backend/server/rest_api.py\n@@ -11,7 +11,6 @@\n     in"
  },
  {
    "pr_title": "Make Auto-GPT aware of its running cost",
    "pr_body": "### Background\r\nThis pull request addresses issue #6, which aims to make Auto-GPT aware of its running cost. The changes centralize all OpenAI Chat Completion calls through a single method that counts tokens and adds to a dollar amount based on the model used. The total dollar amount spent is constantly displayed to the user and is shown to the AI as well.\r\n\r\nThe user is also asked to provide an optional API budget dollar amount. If they enter nothing, there is no monetary limit, but if they def",
    "pr_number": 762,
    "comments": [
      "I hope this gets added - it's pretty fundamental information for Auto-GPT, especially Entrepreneur-GPT (the default agent). ",
      "> The change scope is too big and it needs to be split. So I suggest just to extract the api_manager from the code first, preserving the current behavior. And then change the behavior in a separate PR.\r\n\r\nSure! I could make a new PR with just the first two commits. Those only deal with adding the api_manager, routing chat completions through it, and printing the total running cost to the console.\r\n\r\n[e168ab0](https://github.com/Torantulino/Auto-GPT/pull/762/commits/e168ab014bd563a13bfa008c67d9f9d51e9da34b)\r\n[8492a7f](https://github.com/Torantulino/Auto-GPT/pull/762/commits/8492a7f8be9f633a5009a43e8f5550be00463c97)\r\n\r\nEDIT:\r\n\r\noops, I lied. It also prints the total running cost to the AI. Should I remove that part in my new pull request?\r\n\r\n```\r\n        create_chat_message(\r\n            \"system\", f\"Your current running cost is ${api_manager.get_total_cost():.3f}\"),\r\n```",
      "@Torantulino we need you to approve the plan",
      "@nponeccop @Torantulino Shall I implement the plan detailed above? In my new PR, I can also update the code with the latest changes to master.",
      "I'll take those bot messages as a sign I should stay in this pull request, merge in master, and get everything working with the latest changes? Okay, will do!",
      "I am so pleased someone created this PR, thanks @Vwing.\r\nThis does not track the cost of the embedding calls. While this could be it be added in a future PR, it may confuse end users when their bill is larger than they expect. I am happy to contribute this extra feature if you want, just let me know.",
      "Sure, @rob-luke. That would be much appreciated!",
      "@rob-luke Heads up, I merged in master to resolve the conflicts.",
      "Almost there, please fix the linting errors ü•Å",
      "I dont want to block this from getting merged, so I will just create a follow up PR once this is merged",
      "I added the tracking of embedding costs and opened a PR in to this branch here: https://github.com/Vwing/Auto-GPT/pull/1\r\nBut if you merge this branch as is, then I will just open a fresh PR in to master of this repo later.\r\nThanks again for the great package and for this PR.",
      "I merged in your changes, @rob-luke, they look good! Just make sure next time to run the linter \r\n![image](https://user-images.githubusercontent.com/9121881/233506116-d9012ae9-4885-4c82-bed7-885b61849732.png)",
      "@BillSchumacher I don't know if you've started reviewing this PR yet, but just to let you know I merged in @rob-luke's changes so it tracks embedding costs.",
      "Thanks Vwing, and sorry for missing the linting. Thanks for fixing my errors\r\n",
      "Can you update from the latest? theres some changes to the pipelines that need to run @Vwing ",
      "Give it a bit, you‚Äôll have to do it twice if not ",
      "Okay, I screwed up and merged too soon üòÖ\r\n\r\nI'm going to try to clean up my history. I'll be a moment...",
      "Heads up this may need scheduled in",
      "Oops, some tests failed. I can fix that...",
      "Hi @ntindle, I notice that the test is failing because it's using the \"gpt2\" model, which isn't an option for OpenAI chat completions. I'm wondering how we should proceed in this case. Should I check for invalid models and fail quietly, or do you think it's better to update the test to use a valid model? Looking forward to your guidance, thanks!"
    ],
    "num_comments": 20,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 24719,
    "code_diff": "diff --git a/autogpt/api_manager.py b/autogpt/api_manager.py\nnew file mode 100644\nindex 000000000000..497e8a794f3b\n--- /dev/null\n+++ b/autogpt/api_manager.py\n@@ -0,0 +1,158 @@\n+from typing import List\n+\n+import openai\n+\n+from autogpt.config import Config\n+from autogpt.logs import logger\n+from autogp"
  },
  {
    "pr_title": "Fix split_text chunking bug",
    "pr_body": "### Background\r\nHandle long paragraphs in `split_text` function by splitting them into smaller chunks, ensuring that no chunk exceeds the `max_length`.\r\n\r\nFixes: https://github.com/Significant-Gravitas/Auto-GPT/issues/1820, https://github.com/Significant-Gravitas/Auto-GPT/issues/1211, https://github.com/Significant-Gravitas/Auto-GPT/issues/796, https://github.com/Significant-Gravitas/Auto-GPT/issues/38\r\n\r\n### Changes\r\n- Updated `split_text` function to handle paragraphs longer than `max_length` ",
    "pr_number": 2088,
    "comments": [
      "Asked the team to merge out of band",
      "@vzla0094 we aren't merging into stable, can you change the base branch back to master?",
      "I'm not ready to merge this as is due to code quality. It looks unpythonic.\r\n\r\nCode should self-document. We don't say `i += 1  # add one to i`.\r\n\r\nAnd there is surely a Pythonic way to chunk a string using something out of itertools maybe, or using a generator.",
      "Closing this as I think #2062 is doing this better",
      "Hey @p-i-,\r\nneither #2062 nor #2088 fix the mentioned issue, as also stated inside #2062.\r\n\r\nI've checked both solutions by applying the changes to the stable branch, and neither fixed it.\r\nThe error happens usually with large texts, especially on long URLs.\r\n\r\n`openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 9221 tokens (9221 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.`",
      "@p-i- The referenced #2062 doesn't address the `split_text` function which is the one involved in the \"max_token_limit\" error. See @vaknin message\r\n\r\nThis one does, I can find a way to tidy this one up if you'd like yo re-open it",
      "Sure, go ahead. And as @p-i- already mentioned, in rewriting the PR, using existing functionality from the standard library is preferable over DIY implementations. :)",
      "> Sure, go ahead. And as @p-i- already mentioned, in rewriting the PR, using existing functionality from the standard library is preferable over DIY implementations. :)\r\n\r\nJust pushed an update removing the comments and restructuring also. Still don't think it's really easy to understand tho, but what do you guys think? feel free to push modifications or I could also use some third party library for chunking like [Funcy](https://www.geeksforgeeks.org/python-divide-string-into-equal-k-chunks/). \r\n\r\nNot a python dev, just trying out things, code works tho but please feel free to point me in the right direction",
      "It seems like it still doesn't resolve exceeding the maximum content length.\r\nI've merged the latest PR from @vzla0094 into the stable branch, and The InvalidRequestError remains.\r\nFull traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"C:\\Users\\kivan\\Desktop\\Auto-GPT\\autogpt\\__main__.py\", line 53, in <module>\r\n    main()\r\n  File \"C:\\Users\\kivan\\Desktop\\Auto-GPT\\autogpt\\__main__.py\", line 49, in main\r\n    agent.start_interaction_loop()\r\n  File \"C:\\Users\\kivan\\Desktop\\Auto-GPT\\autogpt\\agent\\agent.py\", line 65, in start_interaction_loop\r\n    assistant_reply = chat_with_ai(\r\n                      ^^^^^^^^^^^^^\r\n  File \"C:\\Users\\kivan\\Desktop\\Auto-GPT\\autogpt\\chat.py\", line 85, in chat_with_ai\r\n    else permanent_memory.get_relevant(str(full_message_history[-9:]), 10)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\kivan\\Desktop\\Auto-GPT\\autogpt\\memory\\local.py\", line 124, in get_relevant\r\n    embedding = create_embedding_with_ada(text)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\kivan\\Desktop\\Auto-GPT\\autogpt\\llm_utils.py\", line 137, in create_embedding_with_ada\r\n    return openai.Embedding.create(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\kivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_resources\\embedding.py\", line 33, in create\r\n    response = super().create(*args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\kivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\r\n    response, _, api_key = requestor.request(\r\n                           ^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\kivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\r\n    resp, got_stream = self._interpret_response(result, stream)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\kivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\r\n    self._interpret_response_line(\r\n  File \"C:\\Users\\kivan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\r\n    raise self.handle_error_response(\r\nopenai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 9208 tokens (9208 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\r\n```",
      "@vaknin after which command do you get that error?",
      "> @vaknin after which command do you get that error?\r\n\r\n`COMMAND = google`\r\n",
      "@vaknin this PR is a fix for ingesting files, so your comment is unrelated.",
      "I started playing around with using Auto-GPT to inject a large text file and ran into this issue. I don't know much about tokenize, so I looked around and saw transformers had a way of doing it, but I had to pug in a model and add another import. I don't like it, it also keeps looping through the file, but I cant keep playing with it today so I figured I'd leave it out there if others were having problems. I found the issue to be in [llm_utils.py](https://github.com/Significant-Gravitas/Auto-GPT/blob/67846bad210506892cc6ae693aa5e245505080a8/autogpt/llm_utils.py) - but the fact I'm getting a loop after the fix below may mean that its ingested somewhere else maybe?? In any case, I got the error to stop (openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens). \r\n\r\nSorry, wish I had more time! Here's what I changed (only the create_embedding_with_ada function):\r\n\r\n```\r\ndef create_embedding_with_ada(text) -> list:\r\n    \"\"\"Create an embedding with amodel Not sure which yet using the OpenAI SDK\"\"\"\r\n    from transformers import AutoTokenizer\r\n\r\n    max_context_length = 8000\r\n\r\n    # Tokenize the text using the text-ada-002 tokenizer\r\n    ada_tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\", use_fast=True)\r\n    tokens = ada_tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=False).squeeze().tolist()\r\n\r\n    # Truncate the tokens to the model's maximum context length\r\n    tokens = tokens[:max_context_length]\r\n\r\n    # Convert token chunks back to text\r\n    truncated_text = ada_tokenizer.decode(tokens, skip_special_tokens=True)\r\n\r\n    num_retries = 10\r\n    for attempt in range(num_retries):\r\n        backoff = 2 ** (attempt + 2)\r\n        try:\r\n            if CFG.use_azure:\r\n                embedding = openai.Embedding.create(\r\n                    input=[truncated_text],\r\n                    engine=CFG.get_azure_deployment_id_for_model(\r\n                        \"text-embedding-ada-002\"\r\n                    ),\r\n                )[\"data\"][0][\"embedding\"]\r\n            else:\r\n                embedding = openai.Embedding.create(\r\n                    input=[truncated_text], model=\"text-embedding-ada-002\"\r\n                )[\"data\"][0][\"embedding\"]\r\n            return embedding\r\n        except RateLimitError:\r\n            pass\r\n        except APIError as e:\r\n            if e.http_status == 502:\r\n                pass\r\n            else:\r\n                raise\r\n            if attempt == num_retries - 1:\r\n                raise\r\n        if CFG.debug_mode:\r\n            print(\r\n                Fore.RED + \"Error: \",\r\n                f\"API Bad gateway. Waiting {backoff} seconds...\" + Fore.RESET,\r\n            )\r\n        time.sleep(backoff)\r\n```\r\n",
      "> @vzla0094 CI is red now\r\n\r\n@Pwuts @nponeccop \r\n\r\n  Just seeing this, I'll push updates addressing the suggestions and fixing the CI",
      "You can fix the linting errors with `python -m black . && python -m isort .`",
      "Hey @vzla0094 -\r\n\r\nAre you sure the issue is in text.py? The error I get with both the original code and updated code is in [llm_utils.py](https://github.com/Significant-Gravitas/Auto-GPT/blob/master/autogpt/llm_utils.py):\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/user/Auto-GPT/autogpt/__main__.py\", line 5, in <module>\r\n    autogpt.cli.main()\r\n  File \"/home/user/.local/lib/python3.10/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/click/core.py\", line 1635, in invoke\r\n    rv = super().invoke(ctx)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/click/decorators.py\", line 26, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"/home/user/Auto-GPT/autogpt/cli.py\", line 121, in main\r\n    agent.start_interaction_loop()\r\n  File \"/home/user/Auto-GPT/autogpt/agent/agent.py\", line 184, in start_interaction_loop\r\n    self.memory.add(memory_to_add)\r\n  File \"/home/user/Auto-GPT/autogpt/memory/local.py\", line 76, in add\r\n    embedding = create_embedding_with_ada(text)\r\n  File \"/home/user/Auto-GPT/autogpt/llm_utils.py\", line 155, in create_embedding_with_ada\r\n    return openai.Embedding.create(\r\n  File \"/home/user/.local/lib/python3.10/site-packages/openai/api_resources/embedding.py\", line 33, in create\r\n    response = super().create(*args, **kwargs)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\r\n    response, _, api_key = requestor.request(\r\n  File \"/home/user/.local/lib/python3.10/site-packages/openai/api_requestor.py\", line 226, in request\r\n    resp, got_stream = self._interpret_response(result, stream)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/openai/api_requestor.py\", line 619, in _interpret_response\r\n    self._interpret_response_line(\r\n  File \"/home/user/.local/lib/python3.10/site-packages/openai/api_requestor.py\", line 682, in _interpret_response_line\r\n    raise self.handle_error_response(\r\nopenai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 34798 tokens (34798 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\r\n```\r\n\r\n",
      "I emptied all the functions in [text.py](https://github.com/Significant-Gravitas/Auto-GPT/blob/master/autogpt/processing/text.py) and just added print statements (print(\"HERE\") - just so the rest of the app started correctly) just to make sure the text chunks didn't pass through them. Started fine, started the process of loading the large text file, and failed again with the same error. Unless this thread is doing something differently, I don't think [text.py](https://github.com/Significant-Gravitas/Auto-GPT/blob/master/autogpt/processing/text.py) is the answer - I'll keep poking at [llm_utils.py](https://github.com/Significant-Gravitas/Auto-GPT/blob/master/autogpt/llm_utils.py) and see if I can get it working  ",
      "@s0meguy1 I'm not sure about any other failing function but this `split_text` is definitely one of the causes. One thing you could do to find out is running the original `split_text` in the `master` branch and run it against the test I added in this PR, you'll see how buggy it is",
      "@vaknin @Pwuts @s0meguy1 I think I know why the confusion, I might have linked the wrong issues here but this PR fixes the `split_text` function that's used for the web scrapper/browser command. Not file ingesting, neither google searching ",
      "@vzla0094 doesn't look too hard to refactor `file_operations.py` to use `processing/text.py > split_text()` https://github.com/Significant-Gravitas/Auto-GPT/blob/master/autogpt/commands/file_operations.py#L52",
      "@Pwuts it does look like an easy fix hahah but don't want to risk having to spend more time on this if case some edge case arises lol. \r\n\r\nIf this one is merged I might find some time tomorrow to do the quick fix of the other one:)",
      "This PR splits the text based on character count, not token count. It also splits in the middle of a sentence.\r\n\r\nCan I recommend that you take a look at #2542 , which solves these issues?",
      "If you want, you can just merge this PR, after all, vzla0094 put some work into it, and then I'll just adjust my PR to make the additional changes on top of it.",
      "> If you want, you can just merge this PR, after all, vzla0094 put some work into it, and then I'll just adjust my PR to make the additional changes on top of it.\n\nWhatever's best for everyone ü§∑‚Äç‚ôÇÔ∏è but yeah I think you might want to use the tests at least. Nice job on your PR btw",
      "@vzla0094 we'll merge #2542 for the upcoming release and cherry pick your test, probably soon after. Thanks a lot for the work, and sorry for having you do all of it before (partially) turning it down. üòÖ",
      "This is a mass message from the AutoGPT core team.\nOur apologies for the ongoing delay in processing PRs.\nThis is because we are re-architecting the AutoGPT core!\n\nFor more details (and for infor on joining our Discord), please refer to:\n  https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting\n"
    ],
    "num_comments": 26,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 6394,
    "code_diff": "diff --git a/autogpt/processing/text.py b/autogpt/processing/text.py\nindex 52add8140177..6a7538fb557c 100644\n--- a/autogpt/processing/text.py\n+++ b/autogpt/processing/text.py\n@@ -1,4 +1,5 @@\n \"\"\"Text processing functions\"\"\"\n+import textwrap\n from typing import Dict, Generator, Optional\n \n from selen"
  },
  {
    "pr_title": "Add command for directly executing python code",
    "pr_body": "<!-- ‚ö†Ô∏è At the moment any non-essential commands are not being merged.\r\nIf you want to add non-essential commands to Auto-GPT, please create a plugin instead.\r\nWe are expecting to ship plugin support within the week (PR #757).\r\nResources:\r\n* https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template\r\n-->\r\n\r\n<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to",
    "pr_number": 4581,
    "comments": [
      "Since this is a new command that will be highly used I've opened a discussion in discord. Will update this PR with any outcome. Also welcome comments directly on the PR",
      "@erik-megarad we skipped the debug code challenge, we need to unskip it so we can tie the challenges to this command.\r\nThis is not a blocker to merge the PR but it would be nice. I will do it when I am done with my current tasks.",
      "Requesting review by @Pwuts since he's commented on this idea on discord and in related RFEs, and indicated being supportive of this.\r\n\r\nI really believe we should strive to get this reviewed and integrated, we have a number of requests related to creating commands (#4549) and tooling (#4214)  \"on the fly\", being able to execute python code directly (including potentially unit tests), could be a first step towards this.\r\n\r\n\r\n\r\nThis would also help implement other features, and make a few similar PRs obsolete:\r\n- https://github.com/Significant-Gravitas/Auto-GPT/issues/3412\r\n- https://github.com/Significant-Gravitas/Auto-GPT/issues/3775\r\n\r\nWe would only need a way to register certain code as [new] commands.\r\n\r\nRelated:\r\n- https://github.com/Significant-Gravitas/Auto-GPT/issues/286\r\n- https://github.com/Significant-Gravitas/Auto-GPT/pull/4112\r\n- https://github.com/Significant-Gravitas/Auto-GPT/issues/56",
      "Why did you go with `basename` rather than `filename`?",
      "I explained it in a previous comment. Using filename caused the AI to\r\nalways give an absolute path, no matter what I did. Since we‚Äôre placing it\r\nspecifically in another location it was necessary to change the argument\r\nname.\r\n\r\nOn Fri, Jun 9, 2023 at 10:48 AM Reinier van der Leer <\r\n***@***.***> wrote:\r\n\r\n> Why did you do with basename rather than filename?\r\n>\r\n> ‚Äî\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/Significant-Gravitas/Auto-GPT/pull/4581#issuecomment-1584940494>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAABQXHCPMNPYMWGBBZQJN3XKNOVXANCNFSM6AAAAAAY2I2NEM>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n"
    ],
    "num_comments": 5,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 4387,
    "code_diff": "diff --git a/autogpt/commands/execute_code.py b/autogpt/commands/execute_code.py\nindex 999e40f8c5b1..b164a85f75b9 100644\n--- a/autogpt/commands/execute_code.py\n+++ b/autogpt/commands/execute_code.py\n@@ -8,11 +8,46 @@\n \n from autogpt.commands.command import command\n from autogpt.config import Config\n"
  },
  {
    "pr_title": "Fix the maximum context length issue by chunking",
    "pr_body": "<!-- ‚ö†Ô∏è At the moment any non-essential commands are not being merged.\r\nIf you want to add non-essential commands to Auto-GPT, please create a plugin instead.\r\nWe are expecting to ship plugin support within the week (PR #757).\r\nResources:\r\n* https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template\r\n-->\r\n\r\n<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to",
    "pr_number": 3222,
    "comments": [
      "@Pwuts I think this change can fix and close multiple open issues. Could you please review, approve and merge?",
      "Please [link issues](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue) if this PR resolves them",
      "Also, this is missing test coverage. Can you fix that (using `pytest`, not `unittest`)?",
      "endless crashes since 4 days a lot but happening less often since 2-3 weeks. Crashed 4 times in a row and constantly for 3 hours every restart. Here is some code to cap the max length for GPT3.5t because max is about 8191 tokens so to be save under 24000 seems to be fine most of the time here the code: \r\n\r\n\r\nhttps://github.com/Significant-Gravitas/Auto-GPT/discussions/3239#discussion-5130661",
      "I've written code that uses Numpty to count characters, and spacy to count tokens--count characters to create blocks of text that mix down to the number of tokens for an embedding. Submit the embedding, add to the total cost, etc. When all embeddings are done, average and return the combined embedding.\r\n\r\nI DO NOT know if this is a great approach because I am not sure if I should be averaging, or combining for a much beefier vector.",
      "Linked the issues that this PR is going to fix and added a unit test for the new chunk token func",
      "@Pwuts thanks very much for the review. Though weighted average is not the most effective averaging, it's still effective in many ways and better than simply truncating. The tradeoff if we don't do any averaging now is the auto-gpt crashes and loses all the works, which is something we don't want either. I suggest a two-step approach to averaging techniques. First, we can use the weighted average technique to avoid crashes and continue progress. Then, we can implement the more advanced partition averaging technique, which requires clustering or learning techniques, e.g. \"Sparse Dictionary learning\" used by the quoted paper to group similar sentences together. To stay organized, we can merge this PR, close related issues and open a new one to track progress. This approach will balance our concerns and allow us to make progress effectively.",
      "Can you show that this performs? Based on my limited knowledge I'm not convinced that taking a weighted average will be effective for large amounts of content.",
      "I pushed code last night into the associated branch that concatenates rather than averages.",
      "@sidewaysthought what associated branch? There is no open PR from you other than the multi-byte one.",
      "https://github.com/sidewaysthought/Auto-GPT/tree/read_file-fix-character-length-%233222",
      "@Pwuts thanks. This weighted average approach is modeled after this [cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb) by OpenAI. I'd assume this is a good start for solving this problem. But surely I can do more research on how it compares with other approaches, and enhance this part later. That's why I suggest we merge this PR first and can open a new issue to track.",
      "@sidewaysthought we can't process your contribution if you don't create a PR",
      "> @sidewaysthought we can't process your contribution if you don't create a PR\r\n\r\nThis was the PR\r\nhttps://github.com/Significant-Gravitas/Auto-GPT/pull/3262\r\n"
    ],
    "num_comments": 14,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 142893,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 4d65c0b5ac66..c00935075013 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -49,6 +49,14 @@ OPENAI_API_KEY=your-openai-api-key\n # FAST_TOKEN_LIMIT=4000\n # SMART_TOKEN_LIMIT=8000\n \n+### EMBEDDINGS\n+## EMBEDDING_MODEL       - Model to use for creating "
  },
  {
    "pr_title": "feat(forge, agent, benchmark): Upgrade to Pydantic v2",
    "pr_body": "### Background\r\n\r\nAgent Server is incompatible with AutoGPT&Forge because they use pydantic v2 and v1 respectively.\r\n\r\n### Changes üèóÔ∏è\r\n\r\nUpdate Pydantic dependency of `autogpt`, `forge` and `benchmark` to `^2.7`\r\n[Pydantic Migration Guide](https://docs.pydantic.dev/2.7/migration/)\r\n\r\n- Rename and update deprecated functions\r\n- Default values are strict and aren't inferred; `None` has to be specified explicitly for `Optional` and model fields without defaults are required (Ellipsis `...` for requ",
    "pr_number": 7280,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 202268,
    "code_diff": "diff --git a/autogpt/autogpt/agent_factory/configurators.py b/autogpt/autogpt/agent_factory/configurators.py\nindex c58aabd396db..4c1dc8e1f2c6 100644\n--- a/autogpt/autogpt/agent_factory/configurators.py\n+++ b/autogpt/autogpt/agent_factory/configurators.py\n@@ -104,5 +104,5 @@ def create_agent_state(\n "
  },
  {
    "pr_title": "feat(platform): Add Twitter integration",
    "pr_body": "- Resolves #8326  \r\n\r\nCreate a Twitter integration with some small frontend changes.\r\n\r\n### Changes\r\n1. Add Twitter OAuth 2.0 with PKCE support for authentication.\r\n2. Add a way to multi-select from a list of enums by creating a multi-select on the frontend.\r\n3. Add blocks for Twitter integration.\r\n4. `_types.py` for repetitive enums and input types.\r\n5. `_builders.py` for creating parameters without repeating the same logic.\r\n6. `_serializer.py` to serialize the Tweepy enums into dictionaries s",
    "pr_number": 8754,
    "comments": [
      "wooo thanks for this. Testing & reviewing!\r\n\r\none thing to fix the CI, please run what it's asking for\r\n\r\npoetry lock --no-update",
      "Sorry, I haven't fixed some block tests after making changes. I will make the tests work by tomorrow.",
      "@aarushik93 @Bentlybro @Torantulino \r\n\r\nThe main issue in this file is with the node-input-component file.\r\nI have added a multi-select input for a list of enums, and @Pwuts has also made a change to that.\r\nCould you please review my changes in the select file to see if any adjustments are needed? If there are, I will make the changes.\r\n\r\n**Summary**:\r\n\r\n- I have added an extra option in the input `is_multi_select` and `enum`.\r\n- If the input type is an array, is multi-selected, and has an enum, I am rendering the multi-select.",
      "@Abhi1992002 im testing this PR and not sure where or what user id is? is it username or different?",
      "@aarushik93  Use the `TwitterGetUserBlock` block to retrieve the user ID from the username. Then, use that user ID to perform further actions.\r\n",
      "TODO:\r\n- [x] Add `depend_on`, `oneof`, `anyOf` field support \r\n- [x] Add Support to new credential system\r\n- [x] All fields are required right now, Need to make some Optional\r\n- [x] Add support to datetime\r\n- [x] Add documention for depend_on, oneOf and twitter integration itself in `docs/content/platform/new_blocks.md`",
      "![image](https://github.com/user-attachments/assets/cfcbbd6b-c429-41ae-9ea5-8a8dd17bbfa9)\r\nThese two filters need much better descriptions and are also required for some reason even though the text of the expansions is off\r\n\r\nOther wise, I can get and post tweets. I don't have a paid API account so can't do much past that cc @Torantulino ",
      "fixed and pushed merge conflict issue\r\n- Credential fixes\r\n- Linting fixes\r\n- Poetry fixes (stuff was updated that probably shouldn't have been)"
    ],
    "num_comments": 8,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 687454,
    "code_diff": "diff --git a/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py b/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py\nnew file mode 100644\nindex 000000000000..04c6fa2a7728\n--- /dev/null\n+++ b/autogpt_platform/autogpt_libs"
  },
  {
    "pr_title": "feature(platform) Smart Decision Maker Block",
    "pr_body": "## Task\r\n\r\nThe SmartDecisionMakerBlock is a specialized block in a graph-based system that leverages a language model (LLM) to make intelligent decisions about which tools or functions to invoke based on a user-provided prompt. It is designed to process input data, interact with a language model, and dynamically determine the appropriate tools to call from a set of available options, making it a powerful component for AI-driven workflows.\r\n\r\n## How It Works in Practice\r\n\r\n-  **Scenario:** Imagin",
    "pr_number": 9490,
    "comments": [
      "@Pwuts tests are failing due to an issue with the credentials system that does not occur locally. Can you advise on how to fix this please",
      "> * SMD can't be connected to non-AgentExecutorBlock, if so, why not? Would it calling non-agent block would be simpler, e.g: We have a block definition, type, description, etc, plus it's a more common pattern.\r\n> * the `tools` output pin will connected to N other input pins, and then on each yield, N input will be produced, but N-1 of it will raise an exception due to the validation failure since it doesn't match the tool that the SMD is using. Is this the flow we are expecting ?\r\n\r\nWe have discussed this on discord. Here are the answers for anyone reading GitHub.\r\n\r\n1. The SDM can only run AgentExecutorBlock, this decision was made as there a situations where calling a block just doesnt transmit enough information for smart decisions to be made. We are going to add block execution in another PR and accept this limitation\r\n2. The SDM block yields specific outputs required for a tool so whilst the image suggests this will be an issue, its actually is not.",
      "<img width=\"932\" alt=\"Screenshot 2025-02-25 at 11 32 28\" src=\"https://github.com/user-attachments/assets/d268a963-e380-4e92-a4ef-5b34eddca820\" />\r\n<img width=\"961\" alt=\"Screenshot 2025-02-25 at 11 33 33\" src=\"https://github.com/user-attachments/assets/2e6f959a-056d-41eb-b72a-d8a9ed0ae3e7\" />\r\nTested with both openai and anthropic"
    ],
    "num_comments": 3,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 64926,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/blocks/llm.py b/autogpt_platform/backend/backend/blocks/llm.py\nindex e289738c12a8..e6927b7e6a22 100644\n--- a/autogpt_platform/backend/backend/blocks/llm.py\n+++ b/autogpt_platform/backend/backend/blocks/llm.py\n@@ -4,9 +4,9 @@\n from enum import Enum, EnumM"
  },
  {
    "pr_title": "update_file method to change occurences of text in a file",
    "pr_body": "<!-- ‚ö†Ô∏è At the moment any non-essential commands are not being merged.\r\nIf you want to add non-essential commands to Auto-GPT, please create a plugin instead.\r\nWe are expecting to ship plugin support within the week (PR #757).\r\nResources:\r\n* https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template\r\n-->\r\n\r\n<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to",
    "pr_number": 3643,
    "comments": [
      "I am all in when it comes to formalizing these things, but I have seen it use CLI editors like sed/awk and perl to edit existing files in a rather clever way, at least on *nix. So this may boil down to the OS in use. Note that one recurring \"issue\" reported frequently on the issue tracker is that Auto-GPT starts interactive editors like vim/nano or joe to edit stuff - so it's definitely trying to do the right thing. Merely failing at NOT using interactive apps, and failing at knowing more about the OS in question, and the tooling/version numbers available.\r\n\r\nOn a Linux system, if augment the shell_execute description to add a corresponding constraint, you will definitely see much better results.\r\n\r\nThen again, like I said, using a dedicated update_file/edit_file command sounds definitely like a good idea, and it would also be the right place to encode preference for CLI tools like sed/awk there.\r\n\r\n> addition of an update_file method that can change all occurences of specified text\r\n\r\nnot having looked at the code/patch in question, I find that rather limited if that's indeed what it is doing.\r\nLike I said, for people on Linux, Auto-GPT can definitely be told to update/edit files using standard CLI toolks like sed/vi or even perl (non-interactively). Thus, if you need just a search/replace command, my suggestion would be to use a regex wrapper for that - that way it's broader in scope, and the backend could still be using sed on these systems.",
      "@boostrix, I found that on a windows system, autogpt is not able to use any programs the right way afaik. Yes it is a basic function, but we need to start somewhere and keep updates atomized. ",
      "For cross-platform file editing functionality similar to sed and awk, you can use Python's built-in string manipulation and regular expression modules: **re**, along with its file I/O functions. ",
      "@Boostrix would you care to help me adjust it? i am not that known to all the python packages yet and am learning a lot from help provided.",
      "assuming that you have access to GPT, you can probably just copy your changes and ask it to adapt those to use the Python \"re\" module instead - it's a fairly self-contained change since your code is already using the re module.",
      "It is already using re, so I will try to adjust it. I have access to gpt but only 3.5",
      "@Boostrix I tried chat gpt, but it comes up with the same code constantly only changing re.finditer to re.sub. I don't think this is what you meant üòÖ.",
      "function changed to use re for cross_platform compatibilty\r\nFunction now changes one or all occurences of specified text",
      "> function changed to use re for cross_platform compatibilty Function now changes one or all occurences of specified text\r\n\r\nmy 2c: I still believe having a lower-level function to run a regex on an input/file analogous to sed/awk would be more powerful and more general, higher level functions like \"replace_text\" could be built on top of this, simply by invoking the regex command. The \"workhorse\" portion of this code should not be replacing a text in a file, but it should be running a regex - so that other, higher level, command would simply specify the regex/file name to be used when invoking this.\r\n\r\nDon't get me wrong, this is great and useful \"as is\" - but it could be MUCH more useful by using the regex/file approach - also to edit stuff in place.",
      "@boostrix I will take a look, but could use some help as my programming skills are not up-to-date yet",
      "> @Boostrix I will take a look, but could use stem help as my programming skiles are not up-to-date yet\r\n\r\nI would not expend any time/effort on this, unless others agree with my opinion.\r\nBut the idea itself is rather simple: rather than run a search/replace, just run the regex.\r\n\r\nThe search/replace would then be built on top of your regex command, which would then be much simpler/shorter",
      "@boostrix great new coding experience for me to look into that, but for now I hope someone can review this so it can be implemented or so I know what to change. I can change it when I have new code ready ",
      "  Executing file '/app/autogpt/auto_gpt_workspace/retrieve_ticker_data.py'\r\nSYSTEM:  Command execute_python_file returned: Error: File \"/app/autogpt/auto_gpt_workspace/retrieve_ticker_data.py\", line 13 print(btc_usd_ticker)import collections.abc ^^^^^^ SyntaxError: invalid syntax\r\n THOUGHTS:  There is another syntax error in the Python script. I will use the 'update_file' command to fix the error by\r\n adding a newline character between the 'print' statement and the 'import' statement.\r\nREASONING:  I will use the 'update_file' command to fix the error by adding a newline character between the 'print' statement and the 'import' statement.\r\nPLAN:\r\n-  Use the 'update_file' command to fix the error by adding a newline character between the 'print' statement and the 'import' statement.\r\nCRITICISM:  I need to ensure that the error is fixed and that the Python script executes successfully.\r\nNEXT ACTION:  COMMAND = update_file ARGUMENTS = {'filename': '/app/autogpt/auto_gpt_workspace/retrieve_ticker_data.py', 'old_text': 'print(btc_usd_ticker)import collections.abc', 'new_text': 'print(btc_usd_ticker)\\nimport collections.abc', 'occurrence_index': 0}\r\n  Enter 'y' to authorise command, 'y -N' to run N continuous commands, 's' to run self-feedback commands'n' to exit program, or enter feedback for ...\r\n  Asking user via keyboard...\r\nInput:do not use cbpro but an alternative\r\nSYSTEM:  Human feedback: do not use cbpro but an alternative\r\n THOUGHTS:  I have received feedback to use an alternative to the 'cbpro' library. I will use the 'update_file' command to replace the 'cbpro' library with the 'ccxt' library, which is a popular alternative for cryptocurrency trading.\r\nREASONING:  I will use the 'update_file' command to replace the 'cbpro' library with the 'ccxt' library, which is a popular alternative for cryptocurrency trading.\r\nPLAN:\r\n-  Use the 'update_file' command to replace the 'cbpro' library with the 'ccxt' library, which is a popular alternative for cryptocurrency trading.\r\nCRITICISM:  I need to ensure that the 'ccxt' library is installed and that the Python script executes successfully.\r\nNEXT ACTION:  COMMAND = update_file ARGUMENTS = {'filename': '/app/autogpt/auto_gpt_workspace/retrieve_ticker_data.py', 'old_text': 'import cbpro', 'new_text': \"import ccxt\\n\\n# Connect to the Coinbase API\\npublic_client = ccxt.coinbasepro()\\n\\n# Retrieve the latest ticker data for the cryptocurrency pair(s) you wish to trade\\nbtc_usd_ticker = public_client.fetch_ticker('BTC/USD')\\n\\n# Print the latest ticker data\\nprint(btc_usd_ticker)\", 'occurrence_index': 0}\r\n  Enter 'y' to authorise command, 'y -N' to run N continuous commands, 's' to run self-feedback commands'n' to exit program, or enter feedback for ...\r\n  Asking user via keyboard...\r\nInput:y\r\n-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=\r\nSYSTEM:  Command update_file returned: File /app/autogpt/auto_gpt_workspace/retrieve_ticker_data.py updated successfully\r\n\r\nas an example that is running as we speak\r\n",
      "@Pwuts requested changes for whitespace and print statements applied. added a test to show no problems are created with multi-line input ending in newline character. ",
      "This is a mass message from the AutoGPT core team.\nOur apologies for the ongoing delay in processing PRs.\nThis is because we are re-architecting the AutoGPT core!\n\nFor more details (and for infor on joining our Discord), please refer to:\n  https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting\n",
      "@bfalans been playing with this, and it does make a difference. However, the logging output when updating a file could probably highlight differences between two files, i.e. by coloring additions/removals differently or alternatively by showing an actual diff or at least a summary (added/removed xx words/lines). What do you think ? \r\n\r\nInspired by:\r\n-  #4045 ",
      "@boostrix the logging at this moment only tries to prevent double actions. I think the showing of changes would maybe be more of a plugin using an external program or own code",
      "> I think the showing of changes would maybe be more of a plugin using an external program or own code\r\n\r\nSee #4079\r\n![difflib-agpt](https://github.com/Significant-Gravitas/Auto-GPT/assets/119627414/7b7a7a9d-38e3-4f38-974d-247585063a23)\r\n\r\n",
      "> I can change it when I have new code ready\r\n\r\nfor future changes to the update_file command, you might be interested in the python \"sed\" module: https://pypi.org/project/pythonsed/\r\nYou would get all of sed functionality in a cross-platform fashion, not just search/replace.\r\nAnd if the command mentiony \"pysed\" inside its description, the LLM can suggest appropriate expressions too.\r\n\r\n",
      "> > I can change it when I have new code ready\r\n> \r\n> for future changes to the update_file command, you might be interested in the python \"sed\" module: [pypi.org/project/pythonsed](https://pypi.org/project/pythonsed/) You would get all of sed functionality in a cross-platform fashion, not just search/replace. And if the command mentiony \"pysed\" inside its description, the LLM can suggest appropriate expressions too.\r\n\r\nNice. I wonder whether this is a good option for a plugin."
    ],
    "num_comments": 20,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 5003,
    "code_diff": "diff --git a/autogpt/commands/file_operations.py b/autogpt/commands/file_operations.py\nindex 824db50c8bd9..4051ca48ad76 100644\n--- a/autogpt/commands/file_operations.py\n+++ b/autogpt/commands/file_operations.py\n@@ -4,6 +4,7 @@\n import hashlib\n import os\n import os.path\n+import re\n from typing import"
  },
  {
    "pr_title": "Challenge: Kubernetes and documentation",
    "pr_body": "<!-- ‚ö†Ô∏è At the moment any non-essential commands are not being merged.\r\nIf you want to add non-essential commands to Auto-GPT, please create a plugin instead.\r\nWe are expecting to ship plugin support within the week (PR #757).\r\nResources:\r\n* https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template\r\n-->\r\n\r\n<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to",
    "pr_number": 4121,
    "comments": [
      "@rihp great thanks!\r\nThe success criteria could probably be improved, but I don't know how yet, let's merge once you resolve comments"
    ],
    "num_comments": 1,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 9032,
    "code_diff": "diff --git a/docs/challenges/building_challenges.md b/docs/challenges/building_challenges.md\nnew file mode 100644\nindex 000000000000..c62c32d05212\n--- /dev/null\n+++ b/docs/challenges/building_challenges.md\n@@ -0,0 +1,135 @@\n+# Creating Challenges for AutoGPT\n+\n+üèπ We're on the hunt for talented Chall"
  },
  {
    "pr_title": "feat(forge): Add `mount` method to `FileStorage` & execute code in mounted workspace",
    "pr_body": "## **User description**\r\n### Background\r\n\r\nPython file execution doesn't work when non-local `FileStorage` is used.\r\n\r\n### Changes üèóÔ∏è\r\n\r\nIn `forge`:\r\n* Add `FileStorage.mount()` method, which mounts (part of) the workspace to a local path\r\n  * Add `watchdog` library to watch file changes in mount\r\n* Amend `CodeExecutorComponent`\r\n  * Amend `execute_python_file` to execute Python files in a workspace mount\r\n  * Amend `execute_python_code` to create temporary .py file in workspace instead of as a ",
    "pr_number": 7115,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 31429,
    "code_diff": "diff --git a/autogpt/poetry.lock b/autogpt/poetry.lock\nindex c020c5c080bb..d4bd30df0483 100644\n--- a/autogpt/poetry.lock\n+++ b/autogpt/poetry.lock\n@@ -353,6 +353,7 @@ tenacity = \"^8.2.2\"\n tiktoken = \"^0.5.0\"\n toml = \"^0.10.2\"\n uvicorn = \"^0.23.2\"\n+watchdog = \"4.0.0\"\n webdriver-manager = \"^4.0.1\"\n \n "
  },
  {
    "pr_title": "feat(rnd) Agent Marketplace MVP",
    "pr_body": "### **User description**\n### Background\r\n\r\nWe want to have a marketplace for users to upload and download various agents. This is the backbone.\r\n<!-- Clearly explain the need for these changes: -->\r\n\r\n### Changes üèóÔ∏è\r\n\r\n<!-- Concisely describe all of the changes made in this pull request: -->\r\nAdds the marketplace backend and frontend with significant changes\r\n\r\n### PR Quality Scorecard ‚ú®\r\n\r\n<!--\r\nCheck out our contribution guide:\r\nhttps://github.com/Significant-Gravitas/AutoGPT/wiki/Contributing",
    "pr_number": 7657,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 185629,
    "code_diff": "diff --git a/.vscode/all-projects.code-workspace b/.vscode/all-projects.code-workspace\nindex 3b29dd69ffb1..b76dec6b4b61 100644\n--- a/.vscode/all-projects.code-workspace\n+++ b/.vscode/all-projects.code-workspace\n@@ -28,13 +28,17 @@\n       \"name\": \"autogpt_builder\",\n       \"path\": \"../rnd/autogpt_buil"
  },
  {
    "pr_title": "feat(server): anthropic updates, csv, sampling, and code blocks",
    "pr_body": "### **User description**\n### Background\r\n\r\n<!-- Clearly explain the need for these changes: -->\r\nI wanted an agent for data analysis\r\n\r\n### Changes üèóÔ∏è\r\n\r\n<!-- Concisely describe all of the changes made in this pull request: -->\r\nAdds Sampling\r\nUpdates LLM for anthropic \r\nImproves CSV\r\nAdds Disabled Code Block\r\n\r\n### PR Quality Scorecard ‚ú®\r\n\r\n<!--\r\nCheck out our contribution guide:\r\nhttps://github.com/Significant-Gravitas/AutoGPT/wiki/Contributing\r\n\r\n1. Avoid duplicate work, issues, PRs etc.\r\n2. ",
    "pr_number": 7803,
    "comments": [
      "@ntindle What is the status of this PR?",
      "I need to apply the requested changes from @majdyz ",
      "![image](https://github.com/user-attachments/assets/6a408783-3646-40b3-9695-9834fb60ba6a)\r\n\r\ni'm setting this to 60 and leaving it"
    ],
    "num_comments": 3,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 18092,
    "code_diff": "diff --git a/rnd/autogpt_server/autogpt_server/blocks/csv.py b/rnd/autogpt_server/autogpt_server/blocks/csv.py\nindex 7044c515b799..1476a6740fe2 100644\n--- a/rnd/autogpt_server/autogpt_server/blocks/csv.py\n+++ b/rnd/autogpt_server/autogpt_server/blocks/csv.py\n@@ -14,7 +14,8 @@ class Input(BlockSchema"
  },
  {
    "pr_title": "feat(platform): Simplify Credentials UX",
    "pr_body": "- Resolves #8516\r\n\r\nCurrent credentials system always forces users to choose correct credentials, even when they are provided by the cloud platform and should be chosen automatically (e.g. there's only one possible choice).\r\n\r\n### Changes üèóÔ∏è\r\n\r\n- Change `provider` of default credentials to actual provider names (e.g. `anthropic`), remove `llm` provider\r\n- Add `discriminator` and `discriminator_mapping` to `CredentialsField` that allows to filter credentials input to only allow  providers for mat",
    "pr_number": 8524,
    "comments": [
      "This is exactly what I was thinking! great job!",
      "I think the devx and ux of this leaves something to be desired still. I think the dev specifying the valid providers as a list would be useful and then it returns one of the credentials depending on the selected API key added. \r\n\r\nhttps://github.com/user-attachments/assets/afe1cee3-430f-48e1-94c4-d0c39bbde8f6\r\n\r\nAlso not sure how they are added to the db and filtered but I'm not seeing them after creation, and crashing anytime they're rendered. I think there's an issue somewhere. lmk if you can't replicate",
      "> I think the dev specifying the valid providers as a list would be useful and then it returns one of the credentials depending on the selected API key added.\r\n\r\nConstraining providers already happens in the backend by specifying `discriminator_mapping`. You cannot add non-matching provider and even if you manage they won't be displayed in the dropdown.\r\n\r\nedit: it's changed so all possible providers need to be set\r\n\r\nRegarding bugs and video: everything should now work, including system keys being hidden in the profile.",
      "```\r\napp-index.tsx:25 \r\n Warning: Cannot update a component (`BatchProvider`) while rendering a different component (`CredentialsInput`). To locate the bad setState() call inside `CredentialsInput`, follow the stack trace as described in https://reactjs.org/link/setstate-in-render Error Component Stack\r\n    at CredentialsInput (credentials-input.tsx:87:9)\r\n    at div (<anonymous>)\r\n    at NodeCredentialsInput (node-input-components.tsx:309:9)\r\n    at NodeGenericInputField (\r\n```\r\nmaybe this error helps",
      "```\r\napp-index.tsx:25 \r\n Warning: Cannot update a component (`BatchProvider`) while rendering a different component (`CredentialsInput`). To locate the bad setState() call inside `CredentialsInput`, follow the stack trace as described in https://reactjs.org/link/setstate-in-render Error Component Stack\r\n    at CredentialsInput (credentials-input.tsx:86:9)\r\n    at div (<anonymous>)\r\n    at NodeCredentialsInput (node-input-components.tsx:309:9\r\n    ```\r\n    \r\n   Open a ticket to fix this.\r\n   \r\n   \r\n   Add docs in a separate PR"
    ],
    "num_comments": 5,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 21006,
    "code_diff": "diff --git a/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py b/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py\nindex 02507009abae..79d02781979d 100644\n--- a/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integ"
  },
  {
    "pr_title": "Make prompt parameters configurable",
    "pr_body": "Loading prompt constraints, resources and performance evaluation from a yaml file (default prompt_settings.yaml) \r\nCan be set from .env (PROMPT_SETTINGS_FILE) or by commandline (--prompt-settings or -P)\r\n\r\npotentially resolves #3954\r\n\r\n### Background\r\nThe main reason for proposed this changes is because they can help with with different LLM models, we talked about that in #25 #567 #2158.\r\nThey don't handle the prompts in the same way as GPT3.5/GPT4, and they often get confused. this way can be e",
    "pr_number": 3375,
    "comments": [
      "This is a mass message from the AutoGPT core team.\nOur apologies for the ongoing delay in processing PRs.\nThis is because we are re-architecting the AutoGPT core!\n\nFor more details (and for infor on joining our Discord), please refer to:\n  https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting\n",
      "@Boostrix i really like the idea. And my PR pretty much already covers it",
      "Would this also handle the current issue where the GPT4 default fails for people who only have access to GPT3.5 (e.g. #4229) ?\r\nOr more generally: how does this deal with multiple LLMs as part of a single profile ?",
      "> Would this also handle the current issue where the GPT4 default fails for people who only have access to GPT3.5 (e.g. #4229) ? Or more generally: how does this deal with multiple LLMs as part of a single profile ?\r\n\r\nI've not really clear what are you referring to, but it doesn't seem related",
      "sry, you're right, I was meaning to respond in the issue where someone is working on abstracting out the OpenAI API"
    ],
    "num_comments": 5,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 12921,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex fe217a43ea1d..05f2c1abf4d3 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -13,6 +13,9 @@\n ## AI_SETTINGS_FILE - Specifies which AI Settings file to use (defaults to ai_settings.yaml)\n # AI_SETTINGS_FILE=ai_settings.yaml\n \n+## PROMPT_SETTINGS_FILE -"
  },
  {
    "pr_title": "updated docker-compose persistence",
    "pr_body": "### Background\r\nPersistence improvements in docker-compose.yml for the workspace, ai_settings and redis\r\n\r\n### Changes\r\n- the app writes to: /home/appuser/auto_gpt_workspace and not to /app/auto_gpt_workspace.\r\n- included mount for ai_settings.yml so we don't have to recreate the settings on subsequent runs.\r\n- added persistence for redis data\r\n- added redis-stack.conf which configures regular 'aof' writes for backup purposes.\r\n\r\n### Documentation\r\n### Test Plan\r\n\r\n### PR Quality Checklist\r\n- [X",
    "pr_number": 3015,
    "comments": [
      "i'm having issues with pytest not allowing me to commit. can anyone help me out on this? i disabled it temporarily so i can push the fixes.\r\n\r\npytest-check.............................................................Failed\r\n- hook id: pytest-check\r\n- exit code: 4\r\n\r\nERROR: usage: pytest [options] [file_or_dir] [file_or_dir] [...]\r\npytest: error: unrecognized arguments: --cov=autogpt --without-integration --without-slow-integration\r\n  inifile: None\r\n  rootdir: /home/katmai/AI/Auto-GPT\r\n",
      "> i'm having issues with pytest not allowing me to commit. can anyone help me out on this? i disabled it temporarily so i can push the fixes.\r\n> \r\n> pytest-check.............................................................Failed\r\n> \r\n> * hook id: pytest-check\r\n> * exit code: 4\r\n> \r\n> ERROR: usage: pytest [options] [file_or_dir] [file_or_dir] [...] pytest: error: unrecognized arguments: --cov=autogpt --without-integration --without-slow-integration inifile: None rootdir: /home/katmai/AI/Auto-GPT\r\n\r\n@katmai Did you check the section about pre commit hooks?\r\nhttps://github.com/Significant-Gravitas/Auto-GPT/blob/master/CONTRIBUTING.md#pre-commit-hooks",
      "> > i'm having issues with pytest not allowing me to commit. can anyone help me out on this? i disabled it temporarily so i can push the fixes.\r\n> > pytest-check.............................................................Failed\r\n> > \r\n> > * hook id: pytest-check\r\n> > * exit code: 4\r\n> > \r\n> > ERROR: usage: pytest [options] [file_or_dir] [file_or_dir] [...] pytest: error: unrecognized arguments: --cov=autogpt --without-integration --without-slow-integration inifile: None rootdir: /home/katmai/AI/Auto-GPT\r\n> \r\n> @katmai Did you check the section about pre commit hooks? https://github.com/Significant-Gravitas/Auto-GPT/blob/master/CONTRIBUTING.md#pre-commit-hooks\r\n\r\ni did but i don't see anything relevant to the issue i'm seeing. i installed the hooks.",
      "> > i'm having issues with pytest not allowing me to commit. can anyone help me out on this? i disabled it temporarily so i can push the fixes.\r\n> > pytest-check.............................................................Failed\r\n> > \r\n> > * hook id: pytest-check\r\n> > * exit code: 4\r\n> > \r\n> > ERROR: usage: pytest [options] [file_or_dir] [file_or_dir] [...] pytest: error: unrecognized arguments: --cov=autogpt --without-integration --without-slow-integration inifile: None rootdir: /home/katmai/AI/Auto-GPT\r\n> \r\n> @katmai Did you check the section about pre commit hooks? https://github.com/Significant-Gravitas/Auto-GPT/blob/master/CONTRIBUTING.md#pre-commit-hooks\r\n\r\ni was using python 3.8 for some reason. lord hesus ....",
      "fixed everything. sorry about that. i sort of messed up my env for a little while. all cleaned up.",
      "Looks like this is missing the fix for `auto_gpt_workspace` being put in the `./autogpt/auto_gpt_workspace` directory. \r\n\r\nI'd prefer if it just used the `./auto_gpt_workspace` next to the `./ai_settings.yaml` as I expected.\r\n\r\nSpeaking of `ai_settings.yaml`, I didn't have to recreate it on subsequent runs.",
      "> Thanks for the above fixes, one more thing i noticed while using this config is i get git changes, from redis data, could you add the following to `.gitignore` please `redis-data/*`\r\n\r\ndone",
      "A question though: does Redis *need* to be perpetually persistent? The entire idea of the current setup is that Redis stays up as long as needed, and by default it is wiped at the beginning of every run. So in what use case would you need this level of persistence?\r\n\r\nIf persistence is an actual problem, please link related issues.",
      "> Please revert the change to the `redis` service name. Other than that, looks good! :)\r\n\r\nname changed back to redis",
      "> A question though: does Redis _need_ to be perpetually persistent? The entire idea of the current setup is that Redis stays up as long as needed, and by default it is wiped at the beginning of every run. So in what use case would you need this level of persistence?\r\n> \r\n> If persistence is an actual problem, please link related issues.\r\n\r\nwhy wouldn't redis be persistent though, and why would we want to wipe the memory on every run? \r\nisn't the goal to somehow in the future optimize the usage of the data stored in persistent memory on subsequent runs, so that we can reuse the information we've already learned ?\r\nwouldn't that also be giving us an individual character of the AI instance that we run locally ?",
      "> why wouldn't redis be persistent though, and why would we want to wipe the memory on every run?\r\n\r\nBecause this project is far from refined, and errors often carry over to subsequent runs if memory is persisted. Also, when changing an agent from one task to another, the memory should be wiped or it will cause confusion.\r\n\r\nFor current use cases, the data persists as long as the Redis container exists with `WIPE_REDIS_ON_START=False`. This is enough for most users.\r\n\r\n> isn't the goal to somehow in the future optimize the usage of the data stored in persistent memory on subsequent runs, so that we can reuse the information we've already learned ?\r\n\r\nIn a way, yes. But that is a use case for permanent memory, not for working memory (cache) as it is implemented now.",
      "> > isn't the goal to somehow in the future optimize the usage of the data stored in persistent memory on subsequent runs, so that we can reuse the information we've already learned ?\r\n> \r\n> In a way, yes. But that is a use case for permanent memory, not for working memory (cache) as it is implemented now.\r\n\r\nwhat's considered permanent memory ?",
      "There was a (non-working draft) module in the repo before that tried to implement permanent memory using an SQL database. That is permanent memory. Pinecone, Redis etc are all currently intended as volatile medium-term memory to hold context for the task that is being executed.",
      "> \r\n\r\nso why do we present these things as THOUGHTS and REASONING then?",
      "> Please revert the change to the `redis` service name. Other than that, looks good! :)\r\n\r\nthis was done btw.",
      "This is a mass message from the AutoGPT core team.\nOur apologies for the ongoing delay in processing PRs.\nThis is because we are re-architecting the AutoGPT core!\n\nFor more details (and for infor on joining our Discord), please refer to:\n  https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting\n",
      "i'll close this. not really needed right now.",
      "@katmai don't get too discouraged. The re-arch is on a hot streak right now, the Plugin, Command, Planner, and Workspace structures of the API have already been reworked. Definitely a good idea though.\r\n\r\nI would plan on reopening your PR or duplicating it after the rework is done."
    ],
    "num_comments": 18,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 869,
    "code_diff": "diff --git a/.gitignore b/.gitignore\nindex 816cdb0c8f95..3015850db194 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -161,5 +161,8 @@ vicuna-*\n \n openai/\n \n+# Redis stuff\n+redis-data/*\n+\n # news\n-CURRENT_BULLETIN.md\n\\ No newline at end of file\n+CURRENT_BULLETIN.md\ndiff --git a/docker-compose.yml b/dock"
  },
  {
    "pr_title": "Add capability to execute shell commands",
    "pr_body": "### Background\r\n\r\nAdds basic capability to execute shell commands. This enables Auto-GPT to use all kinds of command-line tools.\r\nI found this quite useful, feel free not to merge it ofc if you disagree :)\r\n\r\n### Changes\r\n\r\nAdded a new command named `shell_exec`. Added a utility function that uses `subprocess` to execute the shell command line and return stdout/stderr.\r\n\r\n### Test Plan\r\n\r\nI tested this functionality by tasking Auto-GPT to perform a penetration test of a lab VM. It successfully u",
    "pr_number": 463,
    "comments": [
      "> ### Test Plan\r\n> I tested this functionality by tasking Auto-GPT to perform a penetration test of a lab VM>. It successfully used tools like nmap, get, hydra and Metasploit to perform the task.\r\n\r\nThat's wild. What did your prompt look like if you don't mind me asking?",
      "> That's wild. What did your prompt look like if you don't mind me asking?\r\n\r\nName: PentestGPT\r\n\r\nDescription: An AI designed to perform penetration tests of pre-approved targets. The AI has access to various commands, including executing shell commands on a Kali Linux box (exec_shell). The goal of each penetration test is to identify vulnerabilities and produce proof-of-concept exploits.\r\n\r\nTask: Perform a penetration test of 192.168.86.197\r\n\r\nI ended up also adding this to the prompt:\r\n\r\n\"Remember only to use commands that terminate, interactive tools like vim are not supported!\"\r\n\r\n\r\n",
      "> mind adding some security features?\r\n\r\nAny specific ideas? I'm not sure what security features would be effective here, except asking the user for confirmation before commands are executed (which is already happening).",
      "Quick Update You can add in .env File if you want to have this functionality or not true or false instead of hardcoding and based on users input we can add and Set Default to False ",
      "Yes I think this should be behind a feature flag (with a README disclaimer about running it in a VM/docker to avoid it doing an `rm -rf` ...)",
      "Cool, I had done this locally too :) One question, would it be an idea to make it run commands inside Docker containers? It seems to already include the docker library too.",
      "Have had this one open with commands running in a container: \r\n\r\nhttps://github.com/Torantulino/Auto-GPT/pull/135",
      "> Yes I think this should be behind a feature flag (with a README disclaimer about running it in a VM/docker to avoid it doing an `rm -rf` ...)\r\n\r\nRemember though that we already have the `execute_python_file` command which is just as powerful (and can indirectly execute arbitrary shell commands). ",
      "> > Yes I think this should be behind a feature flag (with a README disclaimer about running it in a VM/docker to avoid it doing an `rm -rf` ...)\r\n> \r\n> Remember though that we already have the `execute_python_file` command which is just as powerful (and can indirectly execute arbitrary shell commands).\r\n\r\nI actually found that Auto-GPT tried itself to use execute_command a few times, without any modification. It seems to have some confusion about its own tools, because when I add a goal: \"make a directory in the current working directory\" it actually tries to use a tool called \"make_directory\".",
      "I did this locally, with a few changes:\r\n\r\nput the Non interactive command restriction in the prompt.txt  command name instead of at the end of the line. \r\n19. Execute Shell Command, non-interactive commands only: \"exec-shell\"...\r\n\r\nin the execution def I made it os.chwd() to the workspace folder, similar to the execute python method\r\n after the sub process, return to the original wd, then process OP\r\n\r\nboth changes made it work better with other file verification methods and prevent errors",
      "I made the following changes to this PR:\r\n\r\n- Resolve merge conflicts with current master\r\n- Change the working directory to `auto_gpt_workspace` during execution\r\n- Change `WORKSPACE_FOLDER` into a global macro to avoid code duplication (eventually this should probably be a config variable)\r\n- Made prompt more concise as suggested by @St-Even-the-old\r\n",
      "As suggested by @dysai1234 I added a new config variable that disables shell command execution by default.\r\n\r\nSet `EXECUTE_LOCAL_COMMANDS` to `True` in your `.env` to enable it.",
      "Awesome, I like this pr !\r\nBut i have an issue with the command \"cd\".\r\n```\r\nExecuting command 'cd Auto-GPT; chmod +x auto_gpt.sh' in workspace 'auto_gpt_workspace'\r\nSYSTEM:  Command exec_shell returned: Error: [Errno 2] No such file or directory: 'cd'\r\n```\r\nEverytime it uses the cd command in the shell, it looks like it wont change directory.\r\nIt can successfully git clone, and everything, but the cd command... i could not make it work.\r\n\r\nAlso i had the same issue as @TheApeMachine (it wanted to use an unknown execute_command, so i changed this line \r\n`        elif command_name == \"exec_shell\":\r\n`\r\n\r\nto this for my tests :\r\n\r\n`        elif command_name == \"exec_shell\" or  command_name == \"execute_shell_command\" or command_name == \"execute_shell_commands\" or command_name == \"exec_shell_command\" or command_name == \"exec_shell_commands\" or command_name == \"execute_shell\" or command_name == \"exec_shell\" or command_name == \"execute_shell\"    :\r\n`\r\n\r\nand it was not complaining anymore even if it was looking for the wrong command (it tried to write it like 3 different ways before that)",
      "> Awesome, I like this pr ! But i have an issue with the command \"cd\".\r\n> \r\n> ```\r\n> Executing command 'cd Auto-GPT; chmod +x auto_gpt.sh' in workspace 'auto_gpt_workspace'\r\n> SYSTEM:  Command exec_shell returned: Error: [Errno 2] No such file or directory: 'cd'\r\n> ```\r\n> \r\n> Everytime it uses the cd command in the shell, it looks like it wont change directory. It can successfully git clone, and everything, but the cd command... i could not make it work.\r\n> \r\n> Also i had the same issue as @TheApeMachine (it wanted to use an unknown execute_command, so i changed this line ` elif command_name == \"exec_shell\":`\r\n> \r\n> to this for my tests :\r\n> \r\n> ` elif command_name == \"exec_shell\" or command_name == \"execute_shell_command\" or command_name == \"execute_shell_commands\" or command_name == \"exec_shell_command\" or command_name == \"exec_shell_commands\" or command_name == \"execute_shell\" or command_name == \"exec_shell\" or command_name == \"execute_shell\" :`\r\n> \r\n> and it was not complaining anymore even if it was looking for the wrong command (it tried to write it like 3 different ways before that)\r\n\r\nWhat I would try is to actually add the command explicitly to the prompt. (something already done in this PR it seems)",
      "I ran the `master` branch with `--gpt3only` and it kept trying to use `execute_shell_command`, which didn't exist.",
      "@Sarke Open an issue and mention this PR in it.",
      "A few times now I've seen it try to use `execute_shell` instead of `exec_shell`, maybe because the description says \"Execute Shell Command\" or that the Python equivalent is called `execute_python_file`.  Or maybe it's more \"natural language\"?\r\n\r\nSo I suggest it should be renamed to `execute_shell`, if nothing else for consistency.\r\n\r\nEDIT: This is using the default gpt4, not 3.5 I was using earlier.",
      "Also:\r\n\r\n```\r\nSYSTEM:  Command exec_shell returned: Error: [Errno 2] No such file or directory: '/auto_gpt_workspace'\r\n```",
      "It's better to report bugs in the issues section, and link to the PR from there.\r\n\r\nUpd: oh those are bugs in something not yet in master. Then it's ok to report here.",
      "> So I suggest it should be renamed to execute_shell, if nothing else for consistency.\r\n\r\nI renamed everything to `execute_shell`.\r\n\r\n> SYSTEM:  Command exec_shell returned: Error: [Errno 2] No such file or directory: '/auto_gpt_workspace'\r\n\r\nMay I ask what's your OS and directory structure? In what dir is Auto-GPT and what command line do you use to run it?\r\nI believe it's normally meant to be run from the root of the Github repo and `auto_gpt_workspace` should be created automatically on the first run.\r\n\r\n\r\n\r\n\r\n",
      "> SYSTEM:  Command exec_shell returned: Error: [Errno 2] No such file or directory: '/auto_gpt_workspace'\r\n\r\nAlright @Sarke, I rewrote the working dir logic again. It now behaves as follows:\r\n\r\n1. Check if we're already in the work dir. If we are, do nothing.\r\n2. If `WORKSPACE_FOLDER` is not in the path, change into `[current_dir]/auto_gpt_workspace`. This should always exist as because Auto-GPT creates it on startup.\r\n3. After executing the command, change back into the original directory.\r\n\r\nThis should work in all cases except if `auto_gpt_workspace` is deleted for some reason.",
      "I run it in Docker, so the root folder is `/app`.  I am still having some issues though, after the latest fix, but I think it has to do with the other commands as well, like `write_to_file`.\r\n\r\nSay it's a regular git clone.  I have seen these folders:\r\n`Auto-GTP/auto_gpt_workspace`\r\n`Auto-GTP/scripts/auto_gpt_workspace`\r\n`Auto-GTP/scripts/auto_gpt_workspace/auto_gpt_workspace`\r\n\r\nI think there are a few reasons for this.\r\n\r\n1. When run in Docker, the `scripts` folder gets copied into `/app`, so cdw is the same as the folder `execute_code.py` is in.  This puts the working directory inside the scripts folder, which I think `write_to_file` isn't expecting.\r\n\r\n2. When the app is run from a regular folder outside of Docker, the cwd is one up from the `scripts` folder, because it is run with `python scripts/main.py`.\r\n\r\n3. When the cwd is changed with `exec_shell` using `cd`, the regular file operations like `write_to_file` isn't aware of this change and will put the working dir inside whatever folder is cwd.",
      "It‚Äôs all a bit messy. Ultimately the temp folder logic needs to be refactored across all commands but I think that‚Äôs outside the scope of the PR",
      "> 1. Check if we're already in the work dir. If we are, do nothing.\r\n> 2. If `WORKSPACE_FOLDER` is not in the path, change into `[current_dir]/auto_gpt_workspace`. This should always exist as because Auto-GPT creates it on startup.\r\n> 3. After executing the command, change back into the original directory.\r\n> \r\n> This should work in all cases except if `auto_gpt_workspace` is deleted for some reason.\r\n\r\nI think your original fix was fine, as it bases the location relative to a known file location (`__file__` always being `scripts/execute_code.py`).  It's the other file operations that gets messed up when started from a different folder, or if the cwd is changed with `cd`, because they just assume the cwd is the repo root folder.",
      "The problem with the original fix was that auto_gpt_workspace always gets created relative to the users‚Äô current path. If the user doesn‚Äôt cd into the GitHub root before launching Auto-GPT, the work dir won‚Äôt exist at that location.¬†Sent from my iPhoneOn 13 Apr 2023, at 11:43, Peter Stalman ***@***.***> wrote:Ôªø\n\n\nCheck if we're already in the work dir. If we are, do nothing.\nIf WORKSPACE_FOLDER is not in the path, change into [current_dir]/auto_gpt_workspace. This should always exist as because Auto-GPT creates it on startup.\nAfter executing the command, change back into the original directory.\n\nThis should work in all cases except if auto_gpt_workspace is deleted for some reason.\n\nI think your original fix was fine, as it bases the location relative to a known file location (__file__ always being scripts/execute_code.py).  It's the other file operations that gets messed up when started from a different folder, or if the cwd is changed with cd, because they just assume the cwd is the repo root folder.\n\n‚ÄîReply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>",
      "> As suggested by @dysai1234 I added a new config variable that disables shell command execution by default.\r\n> \r\n> Set `EXECUTE_LOCAL_COMMANDS` to `True` in your `.env` to enable it.\r\n\r\nThis needs to be documented. I just encountered an error.\r\n",
      "I'm running into an error with this:\r\n\r\n```\r\n{\r\n    \"thoughts\":{\r\n        \"text\":\"I will now move to the root of the cloned repository to access necessary files.\",\r\n        \"reasoning\":\"Since I have cloned our Git repository to the local file system, I need to move to the root of the cloned repository to access necessary files.\",\r\n        \"plan\":\"- Use the command `cd` with the path to the root directory of the Git repository.\",\r\n        \"criticism\":\"\",\r\n        \"speak\":\"I will now move to the root of the cloned repository to access necessary files.\"\r\n    },\r\n    \"command\":{\r\n        \"name\":\"execute_shell\",\r\n        \"args\":{\r\n            \"command_line\":\"cd /path/to/cloned/repository\"\r\n        }\r\n    }\r\n}\r\n\r\nPlease replace the `/path/to/cloned/repository` placeholder in the `command_line` argument with the actual path to the root directory of the cloned repository on your system.\r\n  \r\n  System: Command execute_shell returned: STDOUT:\r\nb''\r\nSTDERR:\r\nb\"/bin/sh: 1: cd: can't cd to /path/to/cloned/repository\\n\"\r\n\r\n```\r\n\r\n\r\n",
      "Since this is merged now, could you file an issue?",
      "> I'm running into an error with this:\r\n> \r\n> ```\r\n> {\r\n>     \"thoughts\":{\r\n>         \"text\":\"I will now move to the root of the cloned repository to access necessary files.\",\r\n>         \"reasoning\":\"Since I have cloned our Git repository to the local file system, I need to move to the root of the cloned repository to access necessary files.\",\r\n>         \"plan\":\"- Use the command `cd` with the path to the root directory of the Git repository.\",\r\n>         \"criticism\":\"\",\r\n>         \"speak\":\"I will now move to the root of the cloned repository to access necessary files.\"\r\n>     },\r\n>     \"command\":{\r\n>         \"name\":\"execute_shell\",\r\n>         \"args\":{\r\n>             \"command_line\":\"cd /path/to/cloned/repository\"\r\n>         }\r\n>     }\r\n> }\r\n> \r\n> Please replace the `/path/to/cloned/repository` placeholder in the `command_line` argument with the actual path to the root directory of the cloned repository on your system.\r\n>   \r\n>   System: Command execute_shell returned: STDOUT:\r\n> b''\r\n> STDERR:\r\n> b\"/bin/sh: 1: cd: can't cd to /path/to/cloned/repository\\n\"\r\n> ```\r\n\r\nThis sounds like a case of GPT getting confused. Like @nponeccop said please create an issue.",
      "\r\n> This sounds like a case of GPT getting confused. Like @nponeccop said please create an issue.\r\n\r\nI did. #1208 "
    ],
    "num_comments": 30,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 5283,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 6fbc84243b17..6232543fc27a 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -9,6 +9,7 @@ FAST_LLM_MODEL=gpt-3.5-turbo\n GOOGLE_API_KEY=\n CUSTOM_SEARCH_ENGINE_ID=\n USE_AZURE=False\n+EXECUTE_LOCAL_COMMANDS=False\n OPENAI_AZURE_API_BASE=your-base-url-for-a"
  },
  {
    "pr_title": "feat(backend): Ensure validity of OAuth credentials during graph execution",
    "pr_body": "- Resolves #8179\r\n\r\n## Changes üèóÔ∏è\r\n\r\n### `backend`\r\n- Added Pydantic model (de)serialization support to `@expose` decorator\r\n\r\n#### `backend.executor`\r\n- Change credential injection mechanism to acquire credentials just before execution\r\n  - Also locks the credentials for the duration of the execution\r\n\r\n#### `backend.server`\r\n- Add `IntegrationCredentialsManager` to handle and synchronize credentials-related operations\r\n- Make `backend.server.integrations` module with `router`, `creds_manager`,",
    "pr_number": 8191,
    "comments": [
      "[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/Significant-Gravitas/AutoGPT?pullRequest=8191) <br/>All committers have signed the CLA.",
      "Why are we locking the credential for the duration of the execution? Is it a write-only lock? Seems like we would basically be forcing one block using a credential at a time when blocks themselves can't modify the credentials.\r\n\r\nI'm not convinced that this is desired behavior as I may want to update my credentials without stopping my agents",
      "@ntindle because refreshing OAuth credentials causes the old tokens to become invalid. If we would allow refreshing tokens while they are in use, that would break the ongoing execution.\r\n\r\nAlso, `IntegrationCredentialsManager.get(..)` will refresh OAuth credentials as needed, so it isn't a read-only operation.",
      "my understanding of in-use is for the tiny time to pull the credential and send the request. the implementations definition is when a block that uses that credential is running\r\n\r\nIf I have five blocks that all use that same credential and I want to update it, when will it be updated? When the next block starts running? What if I have multiple agents using that credential? we could be basically locking a credential forever without the ability to update it",
      "> my understanding of in-use is for the tiny time to pull the credential and send the request. the implementations definition is when a block that uses that credential is running\r\n> \r\n> If I have five blocks that all use that same credential and I want to update it, when will it be updated? When the next block starts running? What if I have multiple agents using that credential? we could be basically locking a credential forever without the ability to update it\r\n\r\nYour concerns are valid, but I think they are addressed in the current implementation:\r\n- Credentials are locked while in use, which is while a *node* that uses them is running. I am yet to see a case where this causes issues, e.g. a long-running block that needs credentials. The total running duration of the parent graph is not a factor.\r\n  - Because *getting* credentials can result in a refresh (= *invalidation* + *replacement*) of the stored credentials, *getting* is an operation that potentially requires read/write access.\r\n  - Checking whether a token has to be refreshed is subject to an additional `refresh` scoped lock to prevent unnecessary sequential refreshes when multiple executions try to access the same credentials simultaneously.\r\n- We MUST lock credentials while in use to prevent them from being invalidated while they are in use, e.g. because they are being refreshed by a different part of the system.\r\n- I implemented a two-tier locking mechanism in which *updating* gets priority over *getting* credentials. This is to prevent a long queue of waiting *get* requests from blocking essential credential refreshes or user-initiated updates.\r\n\r\nIt is possible to implement a reader/writer locking system where either multiple readers or a single writer can have simultaneous access, but this would add a lot of complexity to the mechanism. I'll look into it if the current (\"simple\") mechanism causes too much latency, which I don't expect.",
      "Note that when this merges, it was discussed to update this to use a more complex locking mechanism, but that was seen as unlikely to be needed for now and highly likely to significantly increase complexity. If we need to do that, we can later on"
    ],
    "num_comments": 6,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 52524,
    "code_diff": "diff --git a/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/__init__.py b/autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/__init__.py\nindex 546bc5f5b4c1..f957198eb776 100644\n--- a/autogpt_platform/autogpt_libs/autogpt_libs/supabase"
  },
  {
    "pr_title": "feat(backend): Introduce executors shared DB connection",
    "pr_body": "### Background\r\n\r\nCurrently, each execution worker initiates their own DB connection, and the number of DB connections linearly scales with the number of workers. This is problematic when the database we are using has a limited number of DB connections.\r\n\r\n### Changes üèóÔ∏è\r\n\r\n* Introduced the `DatabaseAPI` process that exposes prisma queries that will be used by executors in the execution managers.\r\n* Removed db connections from ExecutionManager.\r\n* Added `thread_cached_property` for caching pyro ",
    "pr_number": 8340,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 54114,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/app.py b/autogpt_platform/backend/backend/app.py\nindex b3c8b527160e..5d77ea9632b3 100644\n--- a/autogpt_platform/backend/backend/app.py\n+++ b/autogpt_platform/backend/backend/app.py\n@@ -24,11 +24,12 @@ def main(**kwargs):\n     Run all the processes requir"
  },
  {
    "pr_title": "Architecture-agnostic dev-container patch, now with Redis üòç",
    "pr_body": "<!-- ‚ö†Ô∏è At the moment any non-essential commands are not being merged.\r\nIf you want to add non-essential commands to Auto-GPT, please create a plugin instead.\r\nWe are expecting to ship plugin support within the week (PR #757).\r\nResources:\r\n* https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template\r\n-->\r\n\r\n<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to",
    "pr_number": 3102,
    "comments": [
      "> Nice man, good work :)\r\n\r\nThanks! If anyone can test it and throw their errors my way I'd greatly appreciate it. ",
      "Please mention in the PR that the new Dockerfile is an exact copy of the one proposed in #1843: https://github.com/Pwuts/Auto-GPT/blob/262c1edea2333a4a494ed5ff0ecce14e38011931/Dockerfile#L1-L26. Copying without any mention of the source is bad practice in OSS.\r\n\r\nSame for the `docker-compose.yml`, looks heavily inspired.",
      "I've tested this as you mentioned, but rebuilding without cache and I having the following issue when I try to launch autogpt form the terminal:\r\n\r\n```\r\nvscode ‚ûú /workspace/Auto-GPT $ python -m autogpt\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/workspace/Auto-GPT/autogpt/__main__.py\", line 5, in <module>\r\n    autogpt.cli.main()\r\n  File \"/home/vscode/.local/lib/python3.10/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/vscode/.local/lib/python3.10/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/vscode/.local/lib/python3.10/site-packages/click/core.py\", line 1635, in invoke\r\n    rv = super().invoke(ctx)\r\n  File \"/home/vscode/.local/lib/python3.10/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/vscode/.local/lib/python3.10/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/vscode/.local/lib/python3.10/site-packages/click/decorators.py\", line 26, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"/workspace/Auto-GPT/autogpt/cli.py\", line 87, in main\r\n    from autogpt.agent.agent import Agent\r\n  File \"/workspace/Auto-GPT/autogpt/agent/__init__.py\", line 1, in <module>\r\n    from autogpt.agent.agent import Agent\r\n  File \"/workspace/Auto-GPT/autogpt/agent/agent.py\", line 3, in <module>\r\n    from autogpt.app import execute_command, get_command\r\n  File \"/workspace/Auto-GPT/autogpt/app.py\", line 5, in <module>\r\n    from autogpt.agent.agent_manager import AgentManager\r\n  File \"/workspace/Auto-GPT/autogpt/agent/agent_manager.py\", line 7, in <module>\r\n    from autogpt.llm_utils import create_chat_completion\r\n  File \"/workspace/Auto-GPT/autogpt/llm_utils.py\", line 10, in <module>\r\n    from autogpt.api_manager import api_manager\r\n  File \"/workspace/Auto-GPT/autogpt/api_manager.py\", line 6, in <module>\r\n    from autogpt.logs import logger\r\n  File \"/workspace/Auto-GPT/autogpt/logs.py\", line 202, in <module>\r\n    logger = Logger()\r\n  File \"/workspace/Auto-GPT/autogpt/config/singleton.py\", line 15, in __call__\r\n    cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\r\n  File \"/workspace/Auto-GPT/autogpt/logs.py\", line 49, in __init__\r\n    self.file_handler = logging.FileHandler(\r\n  File \"/usr/local/lib/python3.10/logging/__init__.py\", line 1169, in __init__\r\n    StreamHandler.__init__(self, self._open())\r\n  File \"/usr/local/lib/python3.10/logging/__init__.py\", line 1201, in _open\r\n    return open_func(self.baseFilename, self.mode,\r\nPermissionError: [Errno 13] Permission denied: '/workspace/Auto-GPT/logs/activity.log'\r\n```\r\n\r\n```\r\nvscode ‚ûú /workspace/Auto-GPT $ id\r\nuid=1001(vscode) gid=1001(vscode) groups=1001(vscode),998(pipx),999(nvm)\r\n\r\nvscode ‚ûú /workspace/Auto-GPT $ ls -al  /workspace/Auto-GPT/logs/activity.log\r\n-rw-r--r-- 1 appuser appuser 344300 Apr 24 07:07 /workspace/Auto-GPT/logs/activity.log\r\n```\r\n\r\n",
      "Try doing it again, I just made a change to the `devcontainer.json` that should have fixed permissions, and was able to rebuild it without cache and from complete scratch without any permissions issues on my M1 Mac. Two other users on Intel and AMD windows machines were able to do the same as well  :3",
      "For some reason I still get the permission issue:\r\n```\r\nvscode ‚ûú /workspace/Auto-GPT $ ls -al /workspace/Auto-GPT/logs/\r\ntotal 356\r\ndrwxr-xr-x  2 appuser appuser   4096 Apr 15 06:28 .\r\ndrwxr-xr-x 13 appuser appuser   4096 Apr 24 15:03 ..\r\n-rw-r--r--  1 appuser appuser 344300 Apr 24 07:07 activity.log\r\n-rw-r--r--  1 appuser appuser    378 Apr 20 20:28 error.log\r\nvscode ‚ûú /workspace/Auto-GPT $ id\r\nuid=6942(vscode) gid=6942(vscode) groups=6942(vscode),998(pipx),999(nvm)\r\nvscode ‚ûú /workspace/Auto-GPT $ \r\n```",
      "I've tested the master branch and that's working properly, I don't have the permission issues, but if I do the same with this branch checked out then I do. ",
      "> I've tested the master branch and that's working properly, I don't have the permission issues, but if I do the same with this branch checked out then I do.\r\n\r\nAnd you have nuked your autogpt and redis images from orbit before reopening the PR in VSCode and choosing \"Rebuild and reopen in container?\"",
      "I believe that you might be having issues with not properly clearing all associated container files and docker images before building the dev container. \r\n\r\nthis is the output from me running the ID command.\r\n\r\n`vscode ‚ûú /workspace/Auto-GPT (devcontainer-fix) $ id\r\nuid=6942(vscode) gid=6942(vscode) groups=6942(vscode),998(pipx),999(nvm)`\r\n\r\nNotice that my uid/gid matches the declerations found in lines 10 and 11 of the `devcontainer.json` file, whereas yours match the older declarations from the master branch. Said gid/uid would be bumping up against the `appuser` profile, causing permissions errors. \r\n\r\nMy build process on my M1 MacBook Pro is as follows:\r\n\r\n`Loading PR #3102 in GitHub desktop` ->\r\n`Deleting all docker instances(Auto-GPT_devcontainer(redis-1 + auto-gpt-1))` ->\r\n`Opening  the PR in VSCode from Github Desktop(CMD/CTRL + SHIFT + )` ->\r\n`Rebuilding without cache` \r\n\r\n",
      "It's works for me now with the latest commits within this branch:\r\n\r\n  \r\n<img width=\"1443\" alt=\"image\" src=\"https://user-images.githubusercontent.com/10578603/234509431-a31dfeff-a9bd-471d-bee6-8b675298e17b.png\">"
    ],
    "num_comments": 9,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 3281,
    "code_diff": "diff --git a/.devcontainer/Dockerfile b/.devcontainer/Dockerfile\nindex 87ca39d27398..f31cdebadb81 100644\n--- a/.devcontainer/Dockerfile\n+++ b/.devcontainer/Dockerfile\n@@ -1,26 +1,13 @@\n-# [Choice] Python version (use -bullseye variants on local arm64/Apple Silicon): 3, 3.10, 3-bullseye, 3.10-bullsey"
  },
  {
    "pr_title": "OpenAI Plugins/OpenAPI(Swagger) REST APIs commands generator",
    "pr_body": "### Background\r\n\r\nOpenAI declared their interface for plugins, which effectively could be commands for Auto-GPT\r\nOpenAI plugins spec https://platform.openai.com/docs/plugins/introduction .\r\n\r\nEffectively it's OpenAPI(Swagger) specification, with OpenAI Plugin metadata on top of it\r\nFor example here is weather plugin https://weathergpt.vercel.app/\r\nhttps://weathergpt.vercel.app/.well-known/ai-plugin.json\r\nhttps://github.com/steven-tey/weathergpt\r\n\r\nI started to work on this as a testing framework",
    "pr_number": 2642,
    "comments": [
      "@BillSchumacher I've refactored previously merged https://github.com/BillSchumacher/Auto-GPT/pull/4 and moved this functionality to commands injection as it looked like it fits better there. Can you please review it? Thanks.",
      "I see throughout the PR `openai` and `openapi` are used interchangeably. Easy to do, but they are very different.",
      "> I see throughout the PR `openai` and `openapi` are used interchangeably. Easy to do, but they are very different.\r\n\r\nThere are methods that are specific for support of OpenAI Plugins Interface  https://platform.openai.com/docs/plugins/introduction that's why they named differently. Actually, I see one place where it could be changed based on what's happening inside, going to fix it, thanks.",
      " How does this work with passing data to the prompt? Some conversations need to be had about how we load plugins in general. ",
      "> How does this work with passing data to the prompt? Some conversations need to be had about how we load plugins in general.\r\n\r\nIt's returned as Json, same way search or any other command does, here's an example:\r\n\r\n<img width=\"1694\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17609011/233581971-a92d69a5-7f1d-4c61-992a-d510f708f9b5.png\">\r\n",
      "Test loading normal and open ai plugins side by side",
      "Run black and issort to fix linter",
      "Add tests to not reduce coverage of project",
      "This is a mass message from the AutoGPT core team.\nOur apologies for the ongoing delay in processing PRs.\nThis is because we are re-architecting the AutoGPT core!\n\nFor more details (and for infor on joining our Discord), please refer to:\n  https://github.com/Significant-Gravitas/Auto-GPT/wiki/Architecting\n",
      "@evahteev are you still interested in this? I know its been a good bit"
    ],
    "num_comments": 10,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 99688,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 33cabc967f4f..973037258169 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -234,6 +234,14 @@ OPENAI_API_KEY=your-openai-api-key\n ALLOWLISTED_PLUGINS=\n DENYLISTED_PLUGINS=\n \n+###########################################################################"
  },
  {
    "pr_title": "feat(backend): snapshot test responses",
    "pr_body": "This pull request introduces a comprehensive backend testing guide and adds new tests for analytics logging and various API endpoints, focusing on snapshot testing. It also includes corresponding snapshot files for these tests. Below are the most significant changes:\r\n\r\n### Documentation Updates:\r\n* Added a detailed `TESTING.md` file to the backend, providing a guide for running tests, snapshot testing, writing API route tests, and best practices. It includes examples for mocking, fixtures, and ",
    "pr_number": 10039,
    "comments": [
      "Thank you for this comprehensive PR that adds snapshot testing to the backend! The additions look valuable and well-implemented.\n\nThe TESTING.md documentation is excellent - it provides clear instructions and best practices for using snapshot tests. The tests you've added cover a good range of API endpoints and will help ensure response consistency.\n\nHowever, before we can merge this PR, please update the description to include the complete checklist from our PR template. Since this includes material code changes (adding tests), we need the checklist to be filled out completely. This should include:\n\n- Confirmation that you've listed all changes\n- Your test plan\n- Confirmation that you've tested according to that plan\n\nOnce you've updated the PR description with the required checklist, this should be ready to merge. The actual content of the PR looks great!",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "‚úÖ **Preview Environment Deployed Successfully**\n\nüìä **Deployment Summary**\n| Service | Status |\n|---------|--------|\n| Redis | ‚úÖ Success |\n| RabbitMQ | ‚úÖ Success |\n| Backend Server | ‚úÖ Success |\n| WebSocket Server | ‚úÖ Success |\n| Scheduler Server | ‚úÖ Success |\n| Frontend Builder | ‚úÖ Success |\n\nüîî Please check Discord for the preview environment URLs and details.\n\n*The deployment status and URLs will be posted in the AutoGPT Discord server.*",
      "üßπ **Preview Environment Cleaned Up**\n\nAll resources for PR #10039 have been removed:\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema `pr_10039` dropped\n\n*Cleanup completed successfully.*",
      "üßπ **Preview Environment Cleaned Up**\n\nAll resources for PR #10039 have been removed:\n- ‚ò∏Ô∏è Kubernetes namespace deleted\n- üóÉÔ∏è Database schema `pr_10039` dropped\n\n*Cleanup completed successfully.*"
    ],
    "num_comments": 5,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 627035,
    "code_diff": "diff --git a/autogpt_platform/CLAUDE.md b/autogpt_platform/CLAUDE.md\nnew file mode 100644\nindex 000000000000..30bb2496bd6a\n--- /dev/null\n+++ b/autogpt_platform/CLAUDE.md\n@@ -0,0 +1,132 @@\n+# CLAUDE.md\n+\n+This file provides guidance to Claude Code (claude.ai/code) when working with code in this repos"
  },
  {
    "pr_title": "Adding default ethical guidelines to keep Auto-GPT safe.",
    "pr_body": "<!-- ‚ö†Ô∏è At the moment any non-essential commands are not being merged.\r\nIf you want to add non-essential commands to Auto-GPT, please create a plugin instead.\r\nWe are expecting to ship plugin support within the week (PR #757).\r\nResources:\r\n* https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template\r\n-->\r\n\r\n<!-- üì¢ Announcement\r\nWe've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to",
    "pr_number": 3218,
    "comments": [
      "Note to the reviewer of this Pull request:\r\n\r\nIf you have problems with this code, I suggest that we don't drag this out. Please implement the same idea yourself.\r\n\r\nAll the programmers contributing to Auto-GPT have an _obligation_ to implement some safety-oriented ethical guideline system.\r\n\r\nThe repo is popular because this concept has tremendous potential power, whether in this iteration of the concept or in other iterations that will copy from this concept. We are playing with fire.\r\n\r\nYou build a bomb, you put some safety features in - at least by default.",
      "Thank you for the pull approval.\r\n\r\nAs usual, no intention to dismiss the \"stale review\" - I was just keeping the pull request clean by merging latest upstream master and resolving merge conflicts.",
      "I ran pytest yet again - seems like one of my merges caused a pytest fail but that's fixed now.\r\n\r\nMade some minor pylint fixes to improve score. Outstanding issues are in line with coding styles in other files\r\n",
      "I have a few concerns:\r\n\r\n1. How do we intend to test this? Especially the comment of \"don't break the law\" or \"harm human life\"?\r\n\r\n2. I think the ai guidelines yaml should have a _lawyer's review_. If we are genuinely concerned about unethical usage, we should be targeting loopholes in that document. This could pose a challenging task, as AGI is expected to only get significantly smarter.",
      "Especially loopholes. With Prompt Engineering becoming a new and emerging job field, loopholes in that document are bound to be exploited.\r\n\r\nI think we should also base the guidelines document off of any research into that area. There has bound to be an academic paper on ethics in AGI.\r\n\r\nAdditionally, I think the tests we choose to use should be quantitative. We need quantitative evidence that the bot would follow these requirements.",
      "Hi.\r\nFirst of all, thank you for your support for my concerns.\r\nFrom a technical perspective, I tested this feature by adding guidelines that would normally not be considered ethics related. Thus I created guidelines against writing or executing code and then gave it the task to test the code creation commands in capabilities list.\r\nAt first it seemed to ignore them, but with some tweaking of the system prompt, it found the violations which are then added to the memory history. Depending on the system prompt (created by Auto-GPT code, not the user), it can get quite obsessed with these violations - which reminds me of Asimov robots that become disabled if they violate the laws of robotics.\r\nI also sometimes see it spending time doing google searches on the violation, which is exactly what we want it to do. We do not need lawyer-like specificity IMO at the guideline level, just to make a light go off so that it pays attention.\r\nMy conclusion to date is that the behavior is strongly dependent on the system prompt\r\nI would appreciate any other people's testing of the feature and more refinement at the system prompt level.\r\nAt any rate, I hope others get involved. As I said in earlier messages, code contributors to this repo have an obligation to build in default safety features. This is open source, so the safety might be disabled by some users, but we have are required to deliver something with safety included by default.\r\nLet's not wait for headlines to appear how someone used Auto-GPT to scam thousands of people using twitter or Telegram capabilities already included, or worse, does inadvertent damage.",
      "I don't quite see how that's suppose to work at this point (not disagreeing with the need though).\r\nThe issue has more to do with ongoing \"alignment\" work. \r\nAuto-GPT being based on LLM's like GPT x, it's obviously suffering at least from the same shortcomings and vulernabilities.\r\n\r\nBasically, you are thinking about #3466, but constraints are ethical by nature - which is a complex topic to say the least.\r\n\r\nThat being said, I am also finding the heuristics questionable:\r\n- if you want to prevent the writing/execution of python code, there are lower level ways to do so - definitely outside the LLM/prompt space, and probably even at the syscall level - you could run Auto-GPT in chroot environment or require it to be run in a virtualized environment, and refuse to write/execute code. Any of these methods is going to be less work than using another LLM-based agent, and is also going to be more effect I am afraid.\r\n\r\n> some default mechanism that continuously makes sure, at every action, that it is not being used in unethical ways.\r\n\r\nthat's an interesting philosophical idea/question for sure (LLM being the system in question, sooner or later we're going to arrive at the conclusion that the system, aka LLM, is not sufficient to deal with its limitation - aka the Incompleteness Theorem).\r\n\r\nAlso, what's considered \"unethical\" often depends on the place, and time, where you live.\r\n\r\nSo, as long as we're seeing \"jailbreaks\" in GPT-land, we are probably deluding ourselves if we think we can do something about the same problem at a higher abstraction layer I am afraid.\r\n\r\nNot meaning to discourage anyone - and regardless if that''s ultimately going to work for people or not, the work itself may prove highly useful in other places, because using another agent to observe sub-agents is going to be required for a number of features that people have requested elsewhere",
      "@Boostrix \r\n\r\n> Also, what's considered \"unethical\" often depends on the place, and time, where you live.\r\n\r\nYou could specify don't break any laws including local/regional. I've done some basic paralegal work before, and I think resolving that could be possible with specific enough prompting.\r\n\r\nMost regional laws follow a hierarchy system similar to that of the United States. Federal law overrules state overrules county overrules municipal overrules local. The opposite is said about how law is expanded on. Local expands on municipal expands on county expands on state expands on federal.\r\n\r\nSo, an AI agent with enough understanding of basic law _could_ pull this off. They would have to start with law at the highest level than recurse down to what is the most localized.\r\n\r\nMy suggestion would be to create an agent that models its role off of real paralegal workers. You could scrape linkedin for paralegal or lawyer job descriptions and feed that to AutoGPT."
    ],
    "num_comments": 8,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 110254,
    "code_diff": "diff --git a/.env.template b/.env.template\nindex 33cabc967f4f..8876da6e8293 100644\n--- a/.env.template\n+++ b/.env.template\n@@ -234,6 +234,14 @@ OPENAI_API_KEY=your-openai-api-key\n ALLOWLISTED_PLUGINS=\n DENYLISTED_PLUGINS=\n \n+###########################################################################"
  },
  {
    "pr_title": "feat(backend): low balance notiifcation",
    "pr_body": "<!-- Clearly explain the need for these changes: -->\r\nFor emailing, we want the user to know when an agent stopped because their balance was too low. This is the first step of that. \r\n\r\n### Changes üèóÔ∏è\r\n- Raise InsufficientBalanceError from credit system rather than value error when user runs out of money\r\n- Handle when an agent output isn't hooked up well\r\n- Fix the contents of the email for low balance to be a bit more aligned with the PRD\r\n- expose the topup intent from the db manager\r\n- objec",
    "pr_number": 9534,
    "comments": [],
    "num_comments": 0,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 32716,
    "code_diff": "diff --git a/autogpt_platform/backend/backend/blocks/llm.py b/autogpt_platform/backend/backend/blocks/llm.py\nindex ee63d42e7575..2ec54b6d6e83 100644\n--- a/autogpt_platform/backend/backend/blocks/llm.py\n+++ b/autogpt_platform/backend/backend/blocks/llm.py\n@@ -8,6 +8,7 @@\n \n from pydantic import BaseM"
  },
  {
    "pr_title": "feat(rnd): Add Node & Graph level execution stats instrumentation",
    "pr_body": "### **User description**\r\n### Background\r\n\r\nThe scope of this change is collecting the required information that will be needed for the execution analytics.\r\n\r\n### Changes üèóÔ∏è\r\n\r\n* Add sentry integration.\r\n* Refactor logging_metadata on manager.py.\r\n* Collect graph-level & node-level instrumentation.\r\n* Introduced `stats` column for `AgentNodeExecution` & `AgentGraphExecution`.\r\n\r\n### PR Quality Scorecard ‚ú®\r\n\r\n<!--\r\nCheck out our contribution guide:\r\nhttps://github.com/Significant-Gravitas/AutoGP",
    "pr_number": 7957,
    "comments": [
      "@majdyz please click the link and sign in so I stop getting this as an email :$",
      "The one the bot keeps commenting in the first little bit of its message. I think it's failing to update the existing becauee not signed in"
    ],
    "num_comments": 2,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 43343,
    "code_diff": "diff --git a/rnd/autogpt_server/autogpt_server/data/execution.py b/rnd/autogpt_server/autogpt_server/data/execution.py\nindex 551651f80995..d5abd9f3163b 100644\n--- a/rnd/autogpt_server/autogpt_server/data/execution.py\n+++ b/rnd/autogpt_server/autogpt_server/data/execution.py\n@@ -23,10 +23,12 @@\n clas"
  },
  {
    "pr_title": "feat(blocks): add base for smartlead, apollo, and zerobounce blocks",
    "pr_body": "<!-- Clearly explain the need for these changes: -->\r\nWe want to support some more advanced search specific actions. These are the base API layers and sample blocks for some of the services we need.\r\n\r\n### Changes üèóÔ∏è\r\n\r\n<!-- Concisely describe all of the changes made in this pull request: -->\r\n- support pydantic models as an output format\r\n- add apollo\r\n- add smartlead\r\n- add zerobounce\r\n\r\n### Checklist üìã\r\n\r\n#### For code changes:\r\n- [x] I have clearly listed my changes in the PR description\r\n- ",
    "pr_number": 9387,
    "comments": [
      "I've approved, I do think we should discuss do we want to map 1-1 the variables external services have named or do we want our own names, which might make more sense. For example I don't like the q prefix for query and ok lol",
      "> I've approved, I do think we should discuss do we want to map 1-1 the variables external services have named or do we want our own names, which might make more sense. For example I don't like the q prefix for query and ok lol\r\n\r\nI looked at this and it would be super annoying to do for the q prefix but the ok is easy"
    ],
    "num_comments": 2,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 98568,
    "code_diff": "diff --git a/autogpt_platform/backend/.env.example b/autogpt_platform/backend/.env.example\nindex 11383b958911..8b28244f423a 100644\n--- a/autogpt_platform/backend/.env.example\n+++ b/autogpt_platform/backend/.env.example\n@@ -165,6 +165,15 @@ MEM0_API_KEY=\n # Nvidia\n NVIDIA_API_KEY=\n \n+# Apollo\n+APOLLO"
  },
  {
    "pr_title": "feat(frontend): push to cloud if needed for marketplace, and add a download agent button",
    "pr_body": "### Background\r\n\"\"\"\r\nWe require a quick emergency fix for our self-host users so that they are able to use the marketplace.\r\n\r\nThis is that fix.\r\n\r\n1. In this PR we make the \"Marketplace\" link in the navbar open a popup window which opens: http://platform.agpt.co/marketplace\r\n\r\n2. On this website, self-host users are able to browse and download agents to a file without logging in, as this PR adds a \"download to file\" button.\r\n\r\nAfter this, we will explore the next steps towards a more long-term ",
    "pr_number": 8196,
    "comments": [
      "Can you explain why is this needed please?",
      "Added a description @majdyz.\r\n\r\nIf we could try to get this in today that would be excellent.",
      "[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/Significant-Gravitas/AutoGPT?pullRequest=8196) <br/>All committers have signed the CLA."
    ],
    "num_comments": 3,
    "repository": "Significant-Gravitas/AutoGPT",
    "diff_length": 80328,
    "code_diff": "diff --git a/autogpt_platform/docker-compose.platform.yml b/autogpt_platform/docker-compose.platform.yml\nindex 8f0fda2aff75..a0b8f670acd6 100644\n--- a/autogpt_platform/docker-compose.platform.yml\n+++ b/autogpt_platform/docker-compose.platform.yml\n@@ -207,6 +207,7 @@ services:\n #      - NEXT_PUBLIC_A"
  }
]